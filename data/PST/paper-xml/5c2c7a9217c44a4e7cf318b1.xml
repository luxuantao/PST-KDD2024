<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature Denoising for Improving Adversarial Robustness</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">https://github.com/facebookresearch/ ImageNet</orgName>
								<orgName type="institution">Adversarial-Training</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Feature Denoising for Improving Adversarial Robustness</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Adversarial attacks to image classification systems present challenges to convolutional networks and opportunities for understanding them. This study suggests that adversarial perturbations on images lead to noise in the features constructed by these networks. Motivated by this observation, we develop new network architectures that increase adversarial robustness by performing feature denoising. Specifically, our networks contain blocks that denoise the features using non-local means or other filters; the entire networks are trained end-to-end. When combined with adversarial training, our feature denoising networks substantially improve the state-of-the-art in adversarial robustness in both white-box and black-box attack settings. On ImageNet, under 10-iteration PGD white-box attacks where prior art has 27.9% accuracy, our method achieves 55.7%; even under extreme 2000-iteration PGD white-box attacks, our method secures 42.6% accuracy. Our method was ranked first in Competition on Adversarial Attacks and Defenses (CAAD) 2018 -it achieved 50.6% classification accuracy on a secret, ImageNet-like test dataset against 48 unknown attackers, surpassing the runner-up approach by ∼10%. </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Adversarial attacks to image classification systems <ref type="bibr" target="#b19">[20]</ref> add small perturbations to images that lead these systems into making incorrect predictions. While the perturbations are often imperceptible or perceived as small "noise" in the image, these attacks are highly effective against even the most successful convolutional network based systems <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref>. The success of adversarial attacks leads to security threats in real-world applications of convolutional networks, but equally importantly, it demonstrates that these networks perform computations that are dramatically different from those in human brains.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows a randomly selected feature map of a ResNet <ref type="bibr" target="#b8">[9]</ref> applied on a clean image (top) and on its ad- * Work done during an internship at Facebook AI Research.  <ref type="bibr" target="#b8">[9]</ref> applied on a clean image (top) and on its adversarially perturbed counterpart (bottom). The adversarial perturbation was produced using PGD <ref type="bibr" target="#b15">[16]</ref> with maximum perturbation ǫ =16 (out of 256). In this example, the adversarial image is incorrectly recognized as "space heater"; the true label is "digital clock".</p><p>versarially perturbed counterpart (bottom). The figure suggests that adversarial perturbations, while small in the pixel space, lead to very substantial "noise" in the feature maps of the network. Whereas the features for the clean image appear to focus primarily on semantically informative content in the image, the feature maps for the adversarial image are activated across semantically irrelevant regions as well. Figure <ref type="figure" target="#fig_1">2</ref> displays more examples with the same pattern.</p><p>Motivated by this observation, we explore feature denoising approaches to improve the robustness of convolutional networks against adversarial attacks. We develop new convolutional network architectures equipped with building blocks designed to denoise feature maps. Our networks are trained end-to-end on adversarially generated samples, allowing them to learn to reduce feature-map perturbations.</p><p>Empirically, we find that the best performance is achieved by networks using non-local means <ref type="bibr" target="#b1">[2]</ref> for feature denoising, leading to models that are related to selfattention <ref type="bibr" target="#b22">[23]</ref> and non-local networks <ref type="bibr" target="#b23">[24]</ref>. Our ablation studies show that using mean filters, median filters, and bilateral filters <ref type="bibr" target="#b20">[21]</ref> for feature denoising also improves adversarial robustness, suggesting that feature denoising is a good design principle. Our models outperform the state-of-the-art in adversarial robustness against highly challenging white-box attacks on ImageNet <ref type="bibr" target="#b17">[18]</ref>. Under 10-iteration PGD attacks <ref type="bibr" target="#b15">[16]</ref>, we report 55.7% classification accuracy on ImageNet, largely surpassing the prior art's 27.9% <ref type="bibr" target="#b9">[10]</ref> with the same attack protocol. Even when faced with extremely challenging 2000-iteration PGD attacks that have not been explored in other literature, our model achieves 42.6% accuracy. Our ablation experiments also demonstrate that feature denoising consistently improves adversarial defense results in white-box settings.</p><p>Our networks are also highly effective under the blackbox attack setting. The network based on our method won the defense track in the recent Competition on Adversarial Attacks and Defenses (CAAD) 2018, achieving 50.6% accuracy against 48 unknown attackers, under a strict "all-ornothing" criterion. This is an 10% absolute (20% relative) accuracy increase compared to the CAAD 2018 runner-up model. We also conduct ablation experiments in which we defend against the five strongest attackers from CAAD 2017 <ref type="bibr" target="#b12">[13]</ref>, demonstrating the potential of feature denoising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Adversarial training <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16]</ref> defends against adversarial perturbations by training networks on adversarial images that are generated on-the-fly during training. Adversarial training constitutes the current state-of-the-art in adversarial robustness against white-box attacks; we use it to train our networks. Adversarial logit paring (ALP) <ref type="bibr" target="#b9">[10]</ref> is a type of adversarial training that encourages the logit predictions of a network for a clean image and its adversarial counterpart to be similar. ALP can be interpreted as "denoising" the logit predictions for the adversarial image, using the logits for the clean image as the "noise-free" reference.</p><p>Other approaches to increase adversarial robustness include pixel denoising. Liao et al. <ref type="bibr" target="#b14">[15]</ref> propose to use high-level features to guide the pixel denoiser; in contrast, our denoising is applied directly on features. Guo et al. <ref type="bibr" target="#b7">[8]</ref> transform the images via non-differentiable image preprocessing, like image quilting <ref type="bibr" target="#b3">[4]</ref>, total variance minimization <ref type="bibr" target="#b16">[17]</ref>, and quantization. While these defenses may be effective in black-box settings, they can be circumvented in white-box settings because the attacker can approximate the gradients of their non-differentiable computations <ref type="bibr" target="#b0">[1]</ref>. In contrast to <ref type="bibr" target="#b7">[8]</ref>, our feature denoising models are differentiable, but are still able to improve adversarial robustness against very strong white-box attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Feature Noise</head><p>Adversarial images are created by adding perturbations to images, constraining the magnitude of perturbations to be small in terms of a certain norm (e.g., L ∞ or L 2 ). The perturbations are assumed to be either imperceptible by humans, or perceived as noise that does not impede human recognition of the visual content. Although the perturbations are constrained to be small at the pixel level, no such constraints are imposed at the feature level in convolutional networks. Indeed, the perturbation of the features induced by an adversarial image gradually increases as the image is propagated through the network <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b7">8]</ref>, and non-existing activations in the feature maps are hallucinated. In other words, the transformations performed by the layers in the network exacerbate the perturbation, and the hallucinated activations can overwhelm the activations due to the true signal, which leads the network to make wrong predictions.</p><p>We qualitatively demonstrate these characteristics of adversarial images by visualizing the feature maps they give rise to. Given a clean image and its adversarially perturbed counterpart, we use the same network (here, a ResNet-50 <ref type="bibr" target="#b8">[9]</ref>) to compute its activations in the hidden layers. Figure <ref type="figure" target="#fig_1">1  and 2</ref> show typical examples of the same feature map on clean and adversarial images, extracted from the middle of the network (in particular, from a res 3 block). These figures  reveal that the feature maps corresponding to adversarial images have activations in regions without relevant visual content that resemble feature noise. Assuming that strong activations indicate the presence of semantic information about the image content (as often hypothesized <ref type="bibr" target="#b26">[27]</ref>), the activations that are hallucinated by adversarial images reveal why the model predictions are altered.</p><p>In this study, we attempt to address this problem by feature denoising. In Figure <ref type="figure" target="#fig_3">3</ref>, we visualize the feature maps of adversarial images, right before and right after a feature denoising operation (see the next section for details). The figure shows that feature denoising operations can successfully suppress much of the noise in the feature maps, and make the responses focus on visually meaningful content. In the next sections, we present empirical evidence showing that models that perform feature denoising operations, indeed, improve adversarial robustness.</p><p>Before we move on to describing our methods, we note that although the feature noise can be easily observed qualitatively, it is difficult to quantitatively measure this noise. We found it is nontrivial to compare feature noise levels between different models, in particular, when the network architecture and/or training methods (standard or adversarial) change. E.g., adding a denoising block in a network, end-to-end trained, tends to change the magnitudes/distributions of all features. Nevertheless, we believe the observed noisy appearance of features reflects a real phenomenon associated with adversarial images.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Denoising Feature Maps</head><p>Motivated by the empirical observations above, we propose to improve adversarial robustness by adding denoising blocks at intermediate layers of convolutional networks. The denoising blocks are trained jointly with all layers of the network in an end-to-end manner using adversarial training. The end-to-end adversarial training allows the resulting networks to (partly) eliminate feature map noise that is data-dependent, i.e., noise that is generated by the attacker. It also naturally handles the noise across multiple layers by considering how changes in earlier layers may impact the feature/noise distributions of later layers.</p><p>Empirically, we find that the best-performed denoising blocks are inspired by self-attention transformers <ref type="bibr" target="#b22">[23]</ref> that are commonly used in machine translation and by non-local networks <ref type="bibr" target="#b23">[24]</ref> that are used for video classification. In this study, we focus on the design of denoising blocks and study their denoising effects. Besides non-local means, we also experiment with simpler denoising operations such as bilateral filtering, mean filtering, and median filtering inside our convolutional networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Denoising Block</head><p>Figure <ref type="figure" target="#fig_5">4</ref> shows the generic form of our denoising block. The input to the block can be any feature layer in the convolutional neural network. The denoising block processes the input features by a denoising operation, such as non-local means or other variants. The denoised representation is first processed by a 1×1 convolutional layer, and then added to the block's input via a residual connection <ref type="bibr" target="#b8">[9]</ref>. <ref type="foot" target="#foot_0">1</ref>The design in Figure <ref type="figure" target="#fig_5">4</ref> is inspired by self-attention <ref type="bibr" target="#b22">[23]</ref> and non-local blocks <ref type="bibr" target="#b23">[24]</ref>. However, only the non-local means <ref type="bibr" target="#b1">[2]</ref> operation in the denoising block is actually doing the denoising; the 1×1 convolutions and the residual connection are mainly for feature combination. While various operations can suppress noise, they can also impact signal. The usage of the residual connection can help the network to retain signals, and the tradeoff between removing noise and retaining signal is adjusted by the 1×1 convolution, which is learned end-to-end with the entire network. We will present ablation studies showing that both the residual connection and the 1×1 convolution contribute to the effectiveness of the denoising block. The generic form of the denoising block allows us to explore various denoising operations, as introduced next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Denoising Operations</head><p>We experiment with four different instantiations of the denoising operation in our denoising blocks.</p><p>Non-local means. Non-local means <ref type="bibr" target="#b1">[2]</ref> compute a denoised feature map y of an input feature map x by taking a weighted mean of features in all spatial locations L:</p><formula xml:id="formula_0">y i = 1 C(x) ∀j∈L f (x i , x j ) • x j ,<label>(1)</label></formula><p>where f (x i , x j ) is a feature-dependent weighting function and C(x) is a normalization function. We note that the weighted average in Eqn. ( <ref type="formula" target="#formula_0">1</ref>) is over x j , rather than another embedding of x j , unlike <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> -denoising is directly on the input feature x, and the correspondence between the feature in y and x is kept. Following <ref type="bibr" target="#b23">[24]</ref>, we consider two forms:</p><formula xml:id="formula_1">• Gaussian (softmax) sets f (x i , x j ) = e</formula><p>where θ(x) and φ(x) are two embedded versions of x (obtained by two 1×1 convolutions), d is the number of channels, and C = ∀j∈L f (x i , x j ). By noticing that f /C is the softmax function, this version is shown in <ref type="bibr" target="#b23">[24]</ref> to be equivalent to the softmax-based, self-attention computation of <ref type="bibr" target="#b22">[23]</ref>.</p><p>• Dot product sets f (x i , x j ) = x T i x j and C(x) = N , where N is the number of pixels in x. Unlike the Gaussian non-local means, the weights of the weighted mean do not sum up to 1 in dot-product non-local means. However, qualitative evaluations suggest it does suppress noise in the features. Experiments also show this version improves adversarial robustness. Interestingly, we find that it is unnecessary to embed x in the dot-product version of non-local means for the model to work well. This is unlike the Gaussian nonlocal means, in which embedding is essential. The dotproduct version provides a denoising operation with no extra parameters (blue box in Figure <ref type="figure" target="#fig_6">5</ref>). Bilateral filter. It is easy to turn the non-local means in Eqn. (1) into a "local mean". Doing so leads to the classical bilateral filter <ref type="bibr" target="#b20">[21]</ref> that is popular for edge-preserving denoising. Formally, it is defined as:</p><formula xml:id="formula_2">y i = 1 C(x) ∀j∈Ω(i) f (x i , x j ) • x j .<label>(2)</label></formula><p>This equation only differs from Eqn. (1) in the neighborhood, Ω(i), which is a local region (e.g., a 3×3 patch) around pixel i. In Eqn.</p><p>(2), we consider the Gaussian and dot product implementations of the weights as before.</p><p>Mean filter. Perhaps the simplest form of denoising is the mean filter (average pooling with a stride of 1). Mean filters reduce noise but also smooth structures, so it is reasonable to expect them to perform worse than the above weighted means. However, somewhat surprisingly, experiments show that denoising blocks using mean filters as the denoising operation can still improve adversarial robustness.</p><p>Median filter. Lastly, we consider an interesting denoising filter that has rarely been used in deep networks: median filtering. The median filter is defined as:</p><formula xml:id="formula_3">y i = median{∀j ∈ Ω(i) : x j },<label>(3)</label></formula><p>where the median is over a local region, Ω(i), and is performed separately for each channel. Median filters are known to be good at removing salt-and-pepper noise and outliers of similar kind. Training convolutional networks that contain median filters is an open problem, but we find experimentally that using median filters as a denoising operation can also improve adversarial robustness.</p><p>In summary, our study explores a rich collection of denoising operations. Sec. 6 reports the results for all the denoising operations described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Adversarial Training</head><p>We show the effectiveness of feature denoising on top of very strong baselines. Our strong experimental results are partly driven by a successful implementation of adversarial training <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref>. In this section, we describe our implementation of adversarial training, which is used for training both baseline models and our feature denoising models.</p><p>The basic idea of adversarial training <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref> is to train the network on adversarially perturbed images. The adversarially perturbed images can be generated by a given white-box attacker based on the current parameters of the models. We use the Projected Gradient Descent (PGD) 2 <ref type="bibr" target="#b15">[16]</ref> as the white-box attacker for adversarial training.</p><p>PGD attacker. PGD is an iterative attacker. At each iteration, it performs a gradient descent step in the loss function w.r.t. the image pixel values, based on an adversarially selected output target. Next, it projects the resulting perturbed images into the feasible solution space -within a maximum per-pixel perturbation of ǫ of the clean image (that is, subject to an L ∞ constraint). The hyper-parameters of the PGD attacker during adversarial training are: the maximum perturbation for each pixel ǫ = 16, the attack step size α = 1, and the number of attack iterations n = 30. For this PGD in adversarial training, we can initialize the adversarial image by the clean image, or randomly within the allowed ǫ <ref type="bibr" target="#b15">[16]</ref>. We randomly choose from both initializations in the PGD attacker during adversarial training: 20% of training batches use clean images to initialize PGD, and 80% use random points within the allowed ǫ.</p><p>Distributed training with adversarial images. For each mini-batch, we use PGD to generate adversarial images for that mini-batch. Then we perform a one-step SGD on these perturbed images and update the model weights. Our SGD update is based exclusively on adversarial images; the minibatch contains no clean images.</p><p>Because a single SGD update is preceded by n-step PGD (with n = 30), the total amount of computation in adversarial training is ∼n× bigger than standard (clean) training. To make adversarial training practical, we perform distributed training using synchronized SGD on 128 GPUs. Each mini-batch contains 32 images per GPU (i.e., the total mini-batch size is 128×32 = 4096). We follow the training recipe of <ref type="bibr" target="#b6">[7]</ref> 3 to train models with such large minibatches. On ImageNet, our models are trained for a total of 110 epochs; we decrease the learning rate by 10× at the 35th, 70-th, and 95-th epoch. A label smoothing <ref type="bibr" target="#b18">[19]</ref> of 0.1 is used. The total time needed for adversarial training on 128 Nvidia V100 GPUs is approximately 38 hours for the baseline ResNet-101 model, and approximately 52 hours for the baseline ResNet-152 model. 2 Publicly available: https://github.com/MadryLab/cifar10_challenge 3 Implemented using the publicly available Tensorpack framework <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We evaluate feature denoising on the ImageNet classification dataset <ref type="bibr" target="#b17">[18]</ref> that has ∼1.28 million images in 1000 classes. Following common protocols <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref> for adversarial images on ImageNet, we consider targeted attacks when evaluating under the white-box settings, where the targeted class is selected uniformly at random; targeted attacks are also used in our adversarial training. We evaluate top-1 classification accuracy on the 50k ImageNet validation images that are adversarially perturbed by the attacker (regardless of its targets), also following <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>In this paper, adversarial perturbation is considered under L ∞ norm (i.e., maximum difference for each pixel), with an allowed maximum value of ǫ. The value of ǫ is relative to the pixel intensity scale of 256.</p><p>Our baselines are ResNet-101/152 <ref type="bibr" target="#b8">[9]</ref>. By default, we add 4 denoising blocks to a ResNet: each is added after the last residual block of res 2 , res 3 , res 4 , and res 5 , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Against White-box Attacks</head><p>Following the protocol of ALP <ref type="bibr" target="#b9">[10]</ref>, we report defense results against PGD as the white-box attacker. <ref type="foot" target="#foot_2">4</ref> We evaluate with ǫ =16, a challenging case for defenders on ImageNet.</p><p>Following <ref type="bibr" target="#b15">[16]</ref>, the PGD white-box attacker initializes the adversarial perturbation from a random point within the allowed ǫ cube. We set its step size α = 1, except for 10iteration attacks where α is set to ǫ/10 = 1.6. We consider a numbers of PGD attack iterations ranging from 10 to 2000.</p><p>Main results. Figure <ref type="figure">6</ref> shows the main results. We first compare with ALP <ref type="bibr" target="#b9">[10]</ref>, the previous state-of-the-art. ALP was evaluated under 10-iteration PGD attack in <ref type="bibr" target="#b9">[10]</ref>, on Inception-v3 <ref type="bibr" target="#b18">[19]</ref>. It achieves 27.9% accuracy on ImageNet validation images (Figure <ref type="figure">6</ref>, purple triangle).</p><p>ResNet-101 and ResNet-152 in Figure <ref type="figure">6</ref> are our baseline models (without any denoising blocks) trained using our adversarial training implementation. Even with the lowercapacity model of R-101, our baseline is very strong -it has 49.7% accuracy under 10-iteration PGD attacks, considerably better than the ALP result. This shows that our adversarial training system is solid; we note that the comparison with ALP is on the system-level as they differ in other aspects (backbone networks, implementations, etc.).</p><p>"R-152, denoise" in Figure <ref type="figure">6</ref> is our model of ResNet-152 with four denoising blocks added. Here we show the bestperforming version (non-local with Gaussian), which we ablate next. There is a consistent performance improvement introduced by the denoising blocks. Under the 10-iteration PGD attack, it improves the accuracy of ResNet-152 baseline by 3.2% from 52.5% to 55.7% (Figure <ref type="figure">6</ref>, right). Our results are robust even under 2000-iteration PGD attacks. To our knowledge, such a strong attack has not been previously explored on ImageNet. ALP <ref type="bibr" target="#b9">[10]</ref> was only evaluated against 10-iteration PGD attacks (Figure <ref type="figure">6</ref>), and its claimed robustness is subject to controversy <ref type="bibr" target="#b4">[5]</ref>. Against 2000-iteration PGD attacks, our ResNet-152 baseline has 39.2% accuracy, and its denoising counterpart is 3.4% better, achieving 42.6%. We also observe that the attacker performance diminishes with 1000∼2000 attack iterations.</p><p>We note that in this white-box setting, the attackers can iteratively back-propagate through the denoising blocks and create adversarial perturbations that are tailored to the denoisers. Recent work <ref type="bibr" target="#b0">[1]</ref> reports that pixel denoising methods can be circumvented by attackers in the white-box settings. By contrast, feature denoising leads to consistent improvements in white-box settings, suggesting that feature denoising blocks make it more difficult to fool networks.</p><p>Variants of denoising operations. Next, we evaluate variants of denoising operations in Sec. <ref type="bibr" target="#b3">4</ref>. In these ablations, we add blocks of different kinds to baseline ResNet-152.</p><p>We consider the following denoising operations: 3×3 mean filtering, 3×3 median filtering, 3×3 bilateral filtering (Eqn. ( <ref type="formula" target="#formula_2">2</ref>)), and non-local filtering. In our ablation study, we further consider a "null" version of the denoising block: the block in Figure <ref type="figure" target="#fig_5">4</ref> becomes trivially a residual block with a single 1×1 convolution. Further, we also compare with adding 4 standard bottleneck <ref type="bibr" target="#b8">[9]</ref> blocks -essentially, ResNet-164. All models are trained by adversarial training. Figure <ref type="figure" target="#fig_7">7</ref> shows the white-box attacks results; for simplicity, we show PGD attacker with up to 100 attack iterations in this ablation.</p><p>All of these denoising operations have better accuracy than: (i) ResNet-152 baseline, (ii) adding 4 standard bottleneck blocks, and (iii) adding 4 "null" denoising blocks. It is worth noticing that the 1×1 null version has the exact  same number of extra parameters as the mean filtering, median filtering, and bilateral/non-local filtering's dot product versions (which have no embedding). The null version is worse than all of them (Figure <ref type="figure" target="#fig_7">7</ref>). Also, while adding standard bottleneck blocks is helpful, adding denoising blocks of any version is more accurate. These results suggest that the extra parameters are not the main reason for our accuracy improvements; feature denoising appears to be a general approach particularly useful for adversarial robustness.</p><p>Our best-performing model is given by the non-local (Gaussian) version, which we use by default in other parts of the paper unless noted. Interestingly, this Gaussian version is just marginally better than the dot product version. Design decisions of the denoising block. The denoising block in Figure <ref type="figure" target="#fig_5">4</ref> has a 1×1 layer and a residual connection. Although both components do not perform denoising, they are important for the denoising blocks to work well. Next, we ablate the behavior of the 1×1 and residual connection. This ablation is in Table <ref type="table" target="#tab_3">1</ref>. We investigate ResNet-152 with four non-local, Gaussian denoising blocks. All models are all trained by adversarial training. When removing the 1×1 convolution in the denoising block, the accuracy drops considerablye.g., decreasing from 45.5% to 36.8% under 100-iteration PGD attacks. On the other hand, removing the residual connection makes training unstable, and its loss does not decrease in our adversarial training.</p><p>These results suggest that denoising features in itself is not sufficient. As suppressing noise may also remove useful signals, it appears essential to properly combine the denoised features with the input features in denoising blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Against Black-Box Attacks</head><p>Next, we evaluate defending against black-box attacks.</p><p>To have an unbiased yet challenging set of attackers, we study the 5 best attackers of the NIPS 2017 CAAD competition <ref type="bibr" target="#b12">[13]</ref>, for which code is publicly available. We use the latest CAAD 2018 evaluation criterion, which we call "all-or-nothing": an image is considered correctly classified only if the model correctly classifies all adversarial versions of this image created by all attackers. This is a challenging evaluation scenario for the defender. Following the CAAD black-box setting, the maximum perturbation for each pixel is ǫ = 32, which also makes defense more difficult. Note that our models are trained with ǫ = 16.</p><p>Table <ref type="table" target="#tab_4">2</ref> shows the results of defending against black-box attacks on ImageNet validation images. To highlight the difficulty of the new "all-or-nothing" criterion, we find that the CAAD 2017 winner <ref type="bibr" target="#b14">[15]</ref> has only 0.04% accuracy under this criterion. We find that it is mainly vulnerable to two of the five attackers 5,6 . If we remove these two attackers, <ref type="bibr" target="#b14">[15]</ref> has 13.4% accuracy in the "all-or-nothing" setting.</p><p>With the "all-or-nothing" criterion, our ResNet-152 baseline has 43.1% accuracy against all five attackers. This number suggests that a successful implementation of adversarial training is critical for adversarial robustness. On top of our strong ResNet-152 baseline, adding four non-local denoising blocks improves the accuracy to 46.4% (Table <ref type="table" target="#tab_4">2</ref>). Interestingly, both the Gaussian and dot product versions perform similarly (46.4% vs. 46.2%), although the Gaussian version has more parameters due to its embedding. Furthermore, the null version has 44.1% accuracythis is worse than the non-local, dot product version, even though they have the same number of parameters; this null version of 1×1 is 1.0% better than the ResNet-152 baseline.</p><p>We have also studied the local variants of denoising blocks, including mean, median, and bilateral filters. They have 43.6% ∼ 44.4% accuracy in this black-box setting. Their results are not convincingly better than the null version's results. This suggests that non-local denoising is more important than local denoising for robustness against these black-box attackers.</p><p>Pushing the envelope. To examine the potential of our model, we add denoising blocks to all residual blocks (one denoising block after each residual block) in ResNet-152. We only study the non-local Gaussian version here. To make training feasible, we use the sub-sampling trick in <ref type="bibr" target="#b23">[24]</ref>: the feature map of x j in Eqn. ( <ref type="formula" target="#formula_0">1</ref>) is subsampled (by a 2×2 max pooling) when performing the non-local means, noting that the feature map of x i is still full-sized. We only use sub-sampling in this case. It achieves a number of 49.5%. This is 6.4% better than the ResNet-152 baseline's 43.1%, under the black-box setting (Table <ref type="table" target="#tab_4">2</ref>).</p><p>CAAD 2018 challenge results. Finally, we report the results from the latest CAAD 2018 competition. The 2018 defense track adopts the "all-or-nothing" criterion mentioned above -in this case, every defense entry needs to defend against 48 unknown attackers submitted to the same challenge (in contrast to 5 attackers in our above black-box ablation). The test data is a secret, ImageNet-like dataset. The maximum perturbation for each pixel is ǫ = 32. The first-place entry is based on our method. We only show the 5 winning submissions here, out of more than 20 submissions.</p><p>Figure <ref type="figure" target="#fig_8">8</ref> shows the accuracy of the 5 best entries in the CAAD 2018 defense track. The winning entry, shown in the blue bar, was based on our method by using a ResNeXt-101-32×8 backbone <ref type="bibr" target="#b25">[26]</ref> with non-local denoising blocks added to all residual blocks. This entry only uses singlecrop, single-model testing. It achieves 50.6% accuracy against 48 unknown attackers. This is ∼10% absolute (20% relative) better than the second place's 40.8% accuracy.</p><p>We also reported the white-box performance of this winning entry on ImageNet. Under 10-iteration PGD attacks and 100-iteration PGD attacks, it achieves 56.0% accuracy and 40.4% accuracy, respectively. These results are slightly worse than the robustness of ResNet-152 based models reported in Section 6.1. We note that this white-box robustness comparison is on the system-level as the winning entry was trained with a slightly different parameter setting.</p><p>We emphasize that the CAAD 2018 defense task is very challenging because of the "all-or-nothing" criterion and many unknown (potentially new state-of-the-art) attackers. Actually, except for the two leading teams, all others have &lt;10% accuracy and many of them have &lt;1% accuracy. This highlights the significance of our 50.6% accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Denoising Blocks in Non-Adversarial Settings</head><p>Thus far we have been focusing on denoising blocks for improving adversarial defense. Because our denoising blocks are components of the convolutional networks, these networks can also be trained without adversarial training for the classification of "clean" images (i.e., the original ImageNet dataset task). We believe that studying the nonadversarial setting can help us better understand the behavior of denoising blocks.</p><p>Table <ref type="table" target="#tab_5">3</ref> presents the clean image performance of models that were not adversarially trained. We compare the baseline R-152, adding standard bottleneck blocks, adding "null" (1×1) denoising blocks, and adding denoising blocks of various types. In the clean setting, these denoising blocks have no obvious advantage over the baseline R-152, adding standard bottleneck blocks, or adding "null" denoising blocks. Actually, all results are in the range of about ±0.2% of the baseline R-152's result -which have no significant difference if we also consider the natural variance between separate training runs of the same model (see baseline R-152 in Table <ref type="table" target="#tab_5">3</ref>). We also find that adding non-local denoising blocks to the shallower ResNet-50 can moderately improve accuracy by 0.7% in the non-adversarial setting, but doing so on ResNet-152 has diminishing gain. This, however, is not the case for the adversarial images.</p><p>These results suggest that the denoising blocks could have special advantages in settings that require adversarial robustness. This observation matches our intuition that denoising blocks are designed to reduce feature noise, which only appears when classifying adversarial images.</p><p>Finally, we report that our ResNet-152 baseline with adversarial training has 62.32% accuracy when tested on clean images, whereas its counterpart with "clean" training has 78.91%. For the denoising version (non-local, Gaussian), the accuracy of an adversarially trained network is 65.30% on clean images, whereas its cleanly trained counterpart obtains 79.08%. This tradeoff between adversarial and clean training has been observed before (e.g., in <ref type="bibr" target="#b21">[22]</ref>); we expect this tradeoff to be the subject of future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>Motivated by the noisy appearance of feature maps from adversarial images, we have demonstrated the potential of feature denoising for improving the adversarial robustness of convolutional networks. Interestingly, our study suggests that there are certain architecture designs (viz., denoising blocks) that are particularly good for adversarial robustness, even though they do not lead to accuracy improvements compared to baseline models in "clean" training and testing scenarios. When combined with adversarial training, these particular architecture designs may be more appropriate for modeling the underlying distribution of adversarial images. We hope our work will encourage researchers to start designing convolutional network architectures that have "innate" adversarial robustness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure1. Feature map in the res3 block of an ImageNet-trained ResNet-50<ref type="bibr" target="#b8">[9]</ref> applied on a clean image (top) and on its adversarially perturbed counterpart (bottom). The adversarial perturbation was produced using PGD<ref type="bibr" target="#b15">[16]</ref> with maximum perturbation ǫ =16 (out of 256). In this example, the adversarial image is incorrectly recognized as "space heater"; the true label is "digital clock".</figDesc><graphic url="image-4.png" coords="1,348.22,283.83,70.88,70.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. More examples similar to Figure 1. We show feature maps corresponding to clean images (top) and to their adversarial perturbed versions (bottom). The feature maps for each pair of examples are from the same channel of a res3 block in the same ResNet-50 trained on clean images. The attacker has a maximum perturbation ǫ = 16 in the pixel domain.</figDesc><graphic url="image-7.png" coords="2,63.49,145.06,71.35,71.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Adversarial images and their feature maps before (left) and after (right) the denoising operation (blue box in Figure 4). Here each pair of feature maps are from the same channel of a res3 block in the same adversarially trained ResNet-50 equipped with (Gaussian) non-local means denoising blocks. The attacker has a maximum perturbation ǫ = 16 for each pixel.</figDesc><graphic url="image-28.png" coords="3,52.34,220.81,71.32,71.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. A generic denoising block. It wraps the denoising operation (e.g., non-local means, bilateral, mean, median filters) with a 1×1 convolution and an identity skip connection [9].</figDesc><graphic url="image-36.png" coords="3,202.31,145.03,71.32,71.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 ,</head><label>5</label><figDesc>Figure5, adapted from<ref type="bibr" target="#b23">[24]</ref>, shows the implementation of the denoising block based on non-local means.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Ablation: denoising variants for defending against white-box attacks on ImageNet. On the ResNet-152 baseline, all other models add 4 blocks to it. The attacker is PGD under different attack iterations, with ǫ = 16. All denoising models are better than the R-152 baseline and the "null" version.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. CAAD 2018 results of the adversarial defense track.The first-place entry is based on our method. We only show the 5 winning submissions here, out of more than 20 submissions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Figure 5. A block with non-local means as its denoising operation. The blue part illustrates the implementation of non-local means in Eqn. (1). The shapes of the feature tensors are noted, with corresponding reshaping/transposing performed: here, H and W are the height and width of the feature maps, and we use 256 channels as an example. If softmax is used, it is the Gaussian version (with appropriate 1×1 convolution embeddings used; omitted in this figure); if softmax is not used, it is the dot product version.</figDesc><table><row><cell></cell><cell></cell><cell>H×W×256</cell></row><row><cell></cell><cell></cell><cell>1×1 conv</cell></row><row><cell></cell><cell></cell><cell>H×W×256</cell></row><row><cell cols="2">(softmax)</cell><cell>HW×256</cell></row><row><cell>HW×HW</cell><cell></cell><cell>HW×256</cell></row><row><cell>HW×256</cell><cell cols="2">256×HW</cell></row><row><cell></cell><cell></cell><cell>H×W×256</cell></row><row><cell></cell><cell>x</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Defense against white-box attacks on ImageNet. The left plot shows results against a white-box PGD attacker with 10 to 2000 attack iterations. The right plot zooms in on the results with 10 to 100 attack iterations. The maximum perturbation is ǫ = 16.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>55.7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>55</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">ALP, Inception-v3 ours, R-101 baseline</cell><cell></cell><cell>55</cell><cell>52.5</cell><cell>53.3</cell><cell></cell><cell></cell><cell></cell><cell>ALP, Inception-v3 ours, R-101 baseline</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">ours, R-152 baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ours, R-152 baseline</cell></row><row><cell>accuracy (%)</cell><cell>40 45 50</cell><cell>45.5 41.7 38.7 45.5 41.7 38.7</cell><cell>44.4 40.4 37.2 44.4 40.4 37.2</cell><cell>43.3 39.6 36.4 43.3 39.6 36.4</cell><cell></cell><cell></cell><cell>42.8 38.9 35.9 42.8 38.9 35.9</cell><cell></cell><cell></cell><cell cols="3">42.6 39.2 35.8 2000-iter PGD attack 42.6 39.2 35.8 2000-iter PGD attack ours, R-152 denoise</cell><cell>accuracy (%)</cell><cell>40 45 50</cell><cell>49.7</cell><cell>47.3 50.0</cell><cell>43.2 46.1 49.9</cell><cell>42.0 44.9 48.6</cell><cell>43.8 40.8 47.9</cell><cell>42.8 40.0 46.8</cell><cell>42.1 39.3 46.0 ours, R-152 denoise 41.9 39.6 39.0 38.7 41.7 42.4 45.8 45.5 46.4</cell></row><row><cell></cell><cell>35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>27.9 27.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>27.9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">ALP ALP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ALP</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">10 100 200</cell><cell>400</cell><cell>600</cell><cell>800</cell><cell>1000</cell><cell>1200</cell><cell>1400</cell><cell>1600</cell><cell>1800</cell><cell>2000</cell><cell></cell><cell></cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>60</cell><cell>70</cell><cell>80</cell><cell>90</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">attack iterations</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">attack iterations</cell></row><row><cell cols="3">Figure 6.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Ablation: denoising block design for defending against white-box attacks on ImageNet. Our networks have four (Gaussian) non-local means denoising blocks. We indicate the performance of models we were unable to train by "NaN".</figDesc><table><row><cell>attack iterations</cell><cell>10</cell><cell>100</cell></row><row><cell>non-local, Gaussian</cell><cell>55.7</cell><cell>45.5</cell></row><row><cell>removing 1×1</cell><cell>52.1</cell><cell>36.8</cell></row><row><cell>removing residual</cell><cell>NaN</cell><cell>NaN</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Defense against black-box attacks on ImageNet. We show top-1 classification accuracy on the ImageNet validation set. The attackers are the 5 best attackers in CAAD 2017. We adopt the CAAD 2018 "all-or-nothing" criterion for defenders. The 2017 winner has 0.04% accuracy under this strict criterion, and if we remove the 2 attackers that it is most vulnerable to, it has 13.4% accuracy under the 3 remaining attackers.</figDesc><table><row><cell>5 https://github.com/pfnet-research/nips17-adversarial-attack</cell></row><row><cell>6 https://github.com/toshi-k/kaggle-nips-2017-adversarial-attack</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Accuracy on clean images in the ImageNet validation set when trained on clean images. All numbers except the first row are reported as the accuracy difference comparing with the first R-152 baseline result. For R-152, we run training 3 times independently, to show the natural random variation of the same architecture. All denoising models show no significant difference, and are within ±0.2% of the baseline R-152's result.</figDesc><table><row><cell>model</cell><cell>accuracy (%)</cell></row><row><cell>R-152 baseline</cell><cell>78.91</cell></row><row><cell>R-152 baseline, run 2</cell><cell>+0.05</cell></row><row><cell>R-152 baseline, run 3</cell><cell>-0.04</cell></row><row><cell>+4 bottleneck (R-164)</cell><cell>+0.13</cell></row><row><cell>+4 denoise: null (1×1 only)</cell><cell>+0.15</cell></row><row><cell>+4 denoise: 3×3 mean filter</cell><cell>+0.01</cell></row><row><cell>+4 denoise: 3×3 median filter</cell><cell>-0.12</cell></row><row><cell>+4 denoise: bilateral, Gaussian</cell><cell>+0.15</cell></row><row><cell>+4 denoise: non-local, Gaussian</cell><cell>+0.17</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">In our terminology, a "denoising operation" refers to the computation that only performs denoising (blue box in Figure4), and a "denoising block" refers to the entire block (all of Figure4).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">√ d θ(xi) T φ(xj ) ,</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">We have also evaluated other attackers, including FGSM<ref type="bibr" target="#b5">[6]</ref>, iterative FGSM<ref type="bibr" target="#b11">[12]</ref>, and its momentum variant<ref type="bibr" target="#b2">[3]</ref>. Similar to<ref type="bibr" target="#b9">[10]</ref>, we found that PGD is the strongest white-box attacker among them.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement: This work was partially supported by a gift grant from YiTu to Cihang Xie and Alan Yuille.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Boosting adversarial attacks with momentum</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image quilting for texture synthesis and transfer</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
				<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Evaluating and understanding the robustness of adversarial logit pairing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.10272</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch SGD: Training ImageNet in 1 hour</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Countering adversarial images using input transformations</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2006">2016. 1, 2, 3, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06373</idno>
		<title level="m">Adversarial logit pairing</title>
				<imprint>
			<date type="published" when="2006">2018. 2, 5, 6</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adversarial attacks and defences competition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00097</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Backpropagation applied to handwritten zip code recognition. Neural computation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Defense against adversarial attacks using high-level representation guided denoiser</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nonlinear total variation based noise removal algorithms</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D: nonlinear phenomena</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bilateral filtering for gray and color images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manduchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">There is no free lunch in adversarial robustness</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12152</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>but there are unexpected benefits</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2007">2018. 1, 3, 4, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="https://github.com/tensorpack/,2016.5" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
