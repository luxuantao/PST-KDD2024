<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Joint Training Framework for Robust Automatic Speech Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhong-Qiu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering at</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<settlement>Columbus</settlement>
									<region>Ohio</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE D</roleName><forename type="first">Deliang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering at</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<settlement>Columbus</settlement>
									<region>Ohio</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Joint Training Framework for Robust Automatic Speech Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">73047C2C26F2C3529A4E47222E38588F</idno>
					<idno type="DOI">10.1109/TASLP.2016.2528171</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2016.2528171, IEEE/ACM Transactions on Audio, Speech, and Language Processing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CHiME-2</term>
					<term>deep neural networks (DNN)</term>
					<term>joint training</term>
					<term>robust automatic speech recognition</term>
					<term>speech separation</term>
					<term>time-frequency masking</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Robustness against noise and reverberation is critical for ASR systems deployed in real-world environments. In robust ASR, corrupted speech is normally enhanced using speech separation or enhancement algorithms before recognition. This paper presents a novel joint training framework for speech separation and recognition. The key idea is to concatenate a deep neural network (DNN) based speech separation frontend and a DNN-based acoustic model to build a larger neural network, and jointly adjust the weights in each module. This way, the separation frontend is able to provide enhanced speech desired by the acoustic model and the acoustic model can guide the separation frontend to produce more discriminative enhancement. In addition, we apply sequence training to the jointly trained DNN so that the linguistic information contained in the acoustic and language models can be back-propagated to influence the separation frontend at the training stage. To further improve the robustness, we add more noise-and reverberation-robust features for acoustic modeling. At the test stage, utterance-level unsupervised adaptation is performed to adapt the jointly trained network by learning a linear transformation of the input of the separation frontend. The resulting sequence-discriminative jointly-trained multi-stream system with run-time adaptation achieves 10.63% average word error rate (WER) on the test set of the reverberant and noisy CHiME-2 dataset (task-2), which represents the best performance on this dataset and a 22.75% error reduction over the best existing method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>NN-HMM hybrid methods become the dominant approach in automatic speech recognition. Different from traditional GMM-HMM methods, which use GMMs for acoustic modeling, the DNN-HMM approach uses DNN to predict senone states based on acoustic inputs with a large context window <ref type="bibr" target="#b18">[19]</ref>. Compared with traditional GMM-HMM Manuscript received Aug. <ref type="bibr" target="#b12">13</ref> (E-mail: dwang@cse.ohio-state.edu). approaches, large improvement has been observed. Since speech recognition is itself a sequence classification problem, recently, different neural network architectures that can capture sequential dependencies, such as the convolutional neural networks (CNNs) <ref type="bibr" target="#b36">[37]</ref>, recurrent neural networks (RNNs) <ref type="bibr" target="#b11">[12]</ref>, long-short term memory (LSTM) <ref type="bibr" target="#b14">[15]</ref> and time-delayed neural networks <ref type="bibr" target="#b32">[33]</ref>, are introduced to improve the performance of ASR systems in addition to commonly used feed-forward DNNs. Although a lot of progress has been made in ASR on clean speech, the performance still drops sharply in the presence of reverberation, mismatched noises and low SNR conditions. Improving the robustness of ASR systems in such environments remains a challenge.</p><p>Although DNN-based acoustic models are robust to noisy input with small variations <ref type="bibr" target="#b56">[57]</ref>, speech separation algorithms are able to significantly improve recognition performance even when deep neural networks are used for acoustic modeling <ref type="bibr" target="#b3">[4]</ref>. Recently, different DNN-based speech separation methods, such as the time-frequency (T-F) masking <ref type="bibr" target="#b54">[55]</ref> <ref type="bibr" target="#b45">[46]</ref> <ref type="bibr" target="#b47">[48]</ref>, spectral mapping <ref type="bibr" target="#b55">[56]</ref>[14] <ref type="bibr" target="#b0">[1]</ref>, and signal approximation <ref type="bibr" target="#b53">[54]</ref>[8] <ref type="bibr" target="#b52">[53]</ref>, are developed and shown to perform surprisingly well even in highly adverse environments.</p><p>When incorporating speech separation into ASR, there are three commonly used strategies. The first strategy is to conduct acoustic modeling on clean speech. At the test stage, a separation frontend is used to enhance noisy speech before recognition <ref type="bibr" target="#b28">[29]</ref> <ref type="bibr" target="#b5">[6]</ref>. A disadvantage would occur when the separation frontend introduces distortions unseen by the acoustic model trained on clean speech <ref type="bibr" target="#b28">[29]</ref>. Nonetheless, this strategy is still useful from a practical perspective as it allows modular research on noise-robust ASR. The second strategy avoids the distortion problem to some extent by using a separation frontend to enhance both training and test set first, and conducts acoustic modeling on the enhanced training set. It may be able to improve the recognition performance since the features may become cleaner after enhancement. However, the performance of this approach is highly dependent on the performance of the separation frontend <ref type="bibr" target="#b3">[4]</ref> <ref type="bibr" target="#b40">[41]</ref>. As suggested in <ref type="bibr" target="#b39">[40]</ref>, it might be better to let the acoustic model see enough input variations at the training stage. The third strategy performs acoustic modeling on noisy speech and at the test stage, noisy features are fed to the acoustic model for decoding directly or feature enhancement first. The resulting multi-condition training strategy is shown to be very effective <ref type="bibr" target="#b42">[43]</ref> but gives unimpressive performance in matched conditions <ref type="bibr" target="#b24">[25]</ref>. In addition, when the performance of speech separation is good, using the acoustic model trained on noisy data for decoding may not sound like a reasonable idea. Different strategies have their own advantages and disadvantages, and which strategy should be adopted is highly dependent on the situation.</p><p>Speech separation and recognition are not two independent tasks and they can clearly benefit from each other. Many studies in robust ASR focus on improving the performance of speech separation <ref type="bibr" target="#b25">[26]</ref>. In other studies, first-pass recognition results <ref type="bibr" target="#b26">[27]</ref>[53] <ref type="bibr" target="#b2">[3]</ref> or language models <ref type="bibr" target="#b17">[18]</ref> are utilized to help speech separation. In our previous studies <ref type="bibr">[30][31]</ref>[50], we proposed to integrate speech separation and acoustic modeling via joint adaptive training. In this study, we further develop this approach and propose various techniques to elevate the performance. The present work mainly makes the following four contributions.</p><p>First, we concatenate a DNN-based speech separation frontend, a trainable mel-filterbank and a DNN-based acoustic model together to build a larger and deeper DNN, and jointly adjust the weights in each module via the back-propagation algorithm. Note that mel-filtering can be represented as one layer in a neural network <ref type="bibr" target="#b35">[36]</ref> since it is just a linear transformation of power spectrogram. The separation frontend is trained to reconstruct noise-free power spectrogram via time-frequency masking. Acoustic modeling is done in the mel-spectrogram domain. With joint training, the separation frontend and filterbank are able to provide enhanced features expected by the acoustic model. In addition, the linguistic information contained in the acoustic model is allowed to flow back to influence the separation frontend and filterbank. Furthermore, the filterbank can be trained according to the separation frontend and acoustic model <ref type="bibr" target="#b35">[36]</ref>. Second, concatenating the separation frontend and acoustic model to form a bigger DNN naturally leads us to sequence-discriminative training applied to the jointly trained DNN for further improvement. This way, at the training stage, the information from language models can be flowed back to influence not only the acoustic model but also the separation frontend by optimizing sequence-discriminative criterion. Third, utterance-level unsupervised adaptation is performed at the test stage to adapt the jointly trained DNN to potentially mismatched conditions or new speakers to some extent. Fourth, we find that adding more robust features for acoustic modeling can significantly improve the robustness of ASR systems. Traditionally, log mel-spectrogram is widely used as the only feature for DNN-based acoustic models <ref type="bibr" target="#b28">[29]</ref>[41][52] <ref type="bibr" target="#b23">[24]</ref>[10] <ref type="bibr" target="#b34">[35]</ref>, partly because DNN is believed to be capable of extracting highly nonlinear discriminative information from relatively raw input. However, DNN robustness against reverberation and noise is limited. In this study, we incorporate additional robust features, such as AMS <ref type="bibr" target="#b22">[23]</ref>, RASTA-PLP <ref type="bibr" target="#b16">[17]</ref>, MRCG <ref type="bibr" target="#b1">[2]</ref>, and PNCC <ref type="bibr" target="#b20">[21]</ref>, for acoustic modeling. This multi-stream strategy improves recognition rate substantially.</p><p>The proposed sequence-discriminative jointly-trained multi-stream approach achieves 10.63 percent average WER on the test set of the noisy and reverberant CHiME-2 dataset (task-2) <ref type="bibr" target="#b42">[43]</ref>. To our knowledge, this represents the best result on this dataset to date.</p><p>The rest of this paper is organized as follows. We describe our joint training approach in Section II, followed by experiments and evaluations in Section III. We conclude this paper in Section IV. A preliminary version of this work is presented in <ref type="bibr" target="#b49">[50]</ref>. There are major differences from this preliminary work, including the use of sequence training and unsupervised adaptation in the present study, as well as the Kaldi toolkit to give better baseline ASR systems and clean alignments. These methodological differences lead to a large performance improvement over <ref type="bibr" target="#b49">[50]</ref>. Also, more systematic comparisons are provided in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. SYSTEM DESCRIPTION</head><p>Our system is built in a step-by-step way. We first train a separation frontend and an acoustic model separately, both using DNNs. Then we concatenate the separation frontend, mel-filterbank and acoustic model together to construct a deeper and larger DNN, and jointly adjust the weights in all modules. After that, we replace the cross-entropy criterion used at the joint training stage with sequence-discriminative criterion for sequence training. Finally, we perform utterance-level unsupervised adaptation at the test stage. The overall framework of our system is shown in Fig. <ref type="figure" target="#fig_1">1</ref>. More details are provided in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Speech Separation</head><p>Originated in computational auditory scene analysis <ref type="bibr" target="#b44">[45]</ref>, T-F masking has shown considerable potential for removing additive noise in noisy speech. The key idea of this method is to estimate the ideal binary mask (IBM) <ref type="bibr" target="#b43">[44]</ref> that identifies speech dominant and noise dominant T-F units, or the ideal ratio mask (IRM) <ref type="bibr" target="#b31">[32]</ref>, which represents the ratio of speech energy to the sum of speech energy and noise energy within each T-F unit. In this framework, speech separation is formulated as a supervised mask estimation problem. Different learning machines, such as GMMs, support vector machines (SVMs) and multi-layer perceptrons (MLPs), have been used for mask estimation. Recently, DNN is employed for mask estimation, and achieves very promising separation performance in both matched and un-matched test conditions <ref type="bibr" target="#b54">[55]</ref>. Recent listening tests show that DNN based IBM estimation produces substantial speech intelligibility improvements of noisy utterances for both hearing-impaired and normal-hearing listeners <ref type="bibr" target="#b15">[16]</ref>. In addition, different training targets are carefully analyzed recently <ref type="bibr" target="#b48">[49]</ref>, and it is suggested that the IRM is likely to be a better training target for supervised speech separation. Therefore, we utilize DNNs to estimate the IRM in this study.</p><p>The ideal mask can be defined in different T-F representation domains, such as cochleagram domain, mel-spectrogram domain or power spectrogram domain. In line with later joint training, the IRM in this study is defined in the power spectrogram domain <ref type="bibr" target="#b48">[49]</ref>:</p><formula xml:id="formula_0">𝑀(𝑡, 𝑓) = 𝑆(𝑡, 𝑓) 𝑆(𝑡, 𝑓) + 𝑁(𝑡, 𝑓) (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where 𝑀 is the IRM of a noisy signal created by mixing a noise-free utterance with a noise signal at a specific SNR level, 𝑆 represents the power spectrogram of the noise-free utterance, 𝑁 stands for the power spectrogram of the noise signal, and 𝑡 and 𝑓 index time and frequency respectively. At the test stage, the IRM must be estimated from noisy utterances. We employ a DNN as the discriminative learning machine for mask estimation. The DNN has four hidden layers each with 1024 rectified linear units (ReLUs) <ref type="bibr" target="#b10">[11]</ref>. There are 161 sigmoidal units in the output layer, corresponding to the dimension of each frame in the power spectrogram. No pre-training is performed. Starting from random initialization, the network is trained for a maximum of 50 epochs to minimize the cross-entropy loss function within each T-F unit using stochastic gradient descent with momentum and Adagrad <ref type="bibr" target="#b6">[7]</ref>.</p><p>The loss function is defined as:</p><formula xml:id="formula_2">𝐿(𝑀 * ) = - 1 𝑇 ∑[𝑀(𝑡, 𝑓) log 𝑀 * (𝑡, 𝑓) + 𝑡,𝑓 (1 -𝑀(𝑡, 𝑓))log (1 -𝑀 * (𝑡, 𝑓))]<label>(2)</label></formula><p>where 𝑀 * (𝑡, 𝑓) is the estimated mask at time 𝑡 and frequency 𝑓 , and 𝑇 is the total number of frames in the dataset. The momentum is linearly increased from 0.1 to 0.5 in the first 5 epochs and fixed at 0.9 afterward. The learning rate is fixed at 0.005 in the first 20 epochs and linearly decreased to 0.0005 in the following 30 epochs. The dropout rates of the input layer and all hidden layers are set to 0.3. The maximum 𝐿 2 norm of the incoming weights of each neuron in the hidden layers is set to 1. The mini-batch size is set to 256. A development dataset is used for parameter tuning and early stopping. The feature used for mask estimation is log-compressed power spectrogram. We splice a large context window of 19 frames centered at the current frame as the input to DNN. In this study, the frame length is 20 ms and frame shift is 10 ms. Therefore, for a signal with 16 kHz sampling rate, the input dimension corresponding to one frame is 3059 (161* <ref type="bibr" target="#b18">19)</ref>. Note that the log power spectrogram feature is globally mean-variance normalized before splicing.</p><p>At the test stage, after obtaining the estimated IRM from a noisy utterance using the trained DNN, we multiply it point-wisely with the power spectrogram of the noisy utterance to get the enhanced power spectrogram, i.e.</p><formula xml:id="formula_3">𝑋 * = 𝑀 * ⊗ 𝑋 (3)</formula><p>where 𝑋 * is the resulting enhanced power spectrogram, 𝑀 * is the estimated IRM, 𝑋 is the noisy power spectrogram, and ⊗ represents point-wise matrix multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Acoustic Modeling</head><p>The DNN-HMM hybrid approach is dominant in ASR today. We utilize a DNN with 7 hidden layers each with 2048 ReLUs for acoustic modeling. The DNN is trained to estimate the posterior probability of each senone state by minimizing cross-entropy at the training stage. All the other training recipes follow the DNN training for mask estimation presented in the previous section.</p><p>Log mel-spectrogram is widely used as the only feature for acoustic modeling. However, mel-spectrogram itself is not robust to noise and reverberation. To improve the robustness of ASR systems, we add more robust features for acoustic modeling as different features contain different and perhaps complementary information for senone state discrimination. In this study, we use a subset of the following features for acoustic modeling.  40-dimensional log mel-spectrogram together with its delta and double delta components (MEL). We perform sentence level mean normalization before splicing an 11-frame context window;  256-dimensional multi-resolution cochleagram (MRCG) <ref type="bibr" target="#b1">[2]</ref> with its delta and double deltas. The recently proposed MRCG is shown to be relatively robust to additive noise for mask estimation;  31-dimensional power-normalized cepstral coefficients (PNCC) <ref type="bibr" target="#b20">[21]</ref> together with their deltas and double deltas. Sentence level mean normalization is performed before splicing an 11-frame context window. The PNCC feature is shown to be robust to reverberation and additive noise;  13-dimensional RASTA-PLP <ref type="bibr" target="#b16">[17]</ref>. The context window is set to 7;  15-dimensional amplitude modulation spectrogram (AMS) <ref type="bibr" target="#b22">[23]</ref> extracted from each of 26 channels;  31-dimensional narrowband mel-frequency cepstral coefficients (MFCC) with the analysis window of 20 ms. The context window is set to 7;  31-dimensional wideband MFCC with the analysis window of 200 ms. The context window size is set to 7. The last four features, denoted as Fset, are shown to have complementary power for mask estimation <ref type="bibr" target="#b30">[31]</ref>[47] <ref type="foot" target="#foot_0">1</ref> . In this study, we directly use Fset features for acoustic modeling. The frequency ranges are all set 64 to 8000 Hz. Following common practice, we use 9 frames centered at the current frame to calculate delta and double delta components. With the features mentioned above for acoustic modeling, the input dimension is 4026 <ref type="bibr">(</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Joint Training</head><p>As illustrated in Fig. <ref type="figure" target="#fig_1">1</ref>, the key idea for joint training is to concatenate an acoustic model DNN and a speech separation DNN to form a larger and deeper neural network, and jointly adjust the weights in all modules. The link for concatenating the separation frontend and the acoustic model is a trainable filterbank layer and a set of layers with fixed operations, which basically represent the extraction of the enhanced MEL features (with delta and double deltas and an 11-frame context window) (see also <ref type="bibr" target="#b29">[30]</ref>[31] <ref type="bibr" target="#b49">[50]</ref>). More specifically, after obtaining the estimated IRM from the separation frontend based on the log power spectrogram of a noisy utterance, we multiply it point-wisely with the noisy power spectrogram to get the enhanced power spectrogram. The enhanced power spectrogram is then fed into the trainable filterbank layer to get the enhanced filterbank feature. Afterwards, we compress it logarithmically, add delta and double deltas, perform sentence-level mean normalization, conduct global mean-variance normalization, and splice 11 frames to yield the enhanced MEL features. The enhanced MEL features, together with other robust features, are finally passed to the acoustic model to estimate state posterior probabilities. Interestingly, the joint training framework can be performed in a single neural network because the point-wise multiplication, filtering, sentence-and global-level normalization, adding delta and double deltas are all linear transformations. In addition, the derivatives of the logarithmic function can be easily computed. Therefore, we are able to flow the error signal from the acoustic model back to the filterbank layer and the separation frontend, and jointly train all modules using the back-propagation algorithm.</p><p>A similar frontend and backend joint training approach was presented by <ref type="bibr">Gao et al. [9]</ref>, where feature mapping is employed as the frontend. It has been suggested that masking is likely a better approach than mapping for speech separation <ref type="bibr" target="#b48">[49]</ref>. In addition, the output dimension of their frontend is equal to the input dimension, which consists of many consecutive frames and is large. In contrast, we obtain enhancement results per single frame. Furthermore, their frontend obtains enhanced MEL features by direct mapping instead of using a trainable filterbank layer and fixed layers to transform the enhanced power spectrogram.</p><p>In our approach, parameter initialization is critical before joint training. Randomly initializing all the parameters is unlikely to be effective considering the size of the network.</p><p>Here we use the weights in a separately trained acoustic model and a separately trained separation frontend to initialize the corresponding parts of the DNN for joint training. Following <ref type="bibr" target="#b35">[36]</ref>, we initialize the parameters in the trainable filterbank (FB) layer using</p><formula xml:id="formula_4">𝑊 𝐹𝐵 = exp (𝑊 * ) (4)</formula><p>where 𝑊 * is initialized to</p><formula xml:id="formula_5">𝑊 * = log (max (𝑀𝑒𝑙_𝐹𝐵, 𝑒𝑝𝑠))<label>(5)</label></formula><p>Here 𝑀𝑒𝑙_𝐹𝐵 denotes the standard 40-dimensional mel-filterbank and 𝑒𝑝𝑠 is a small constant (10 -3 in this study). With (4), every time 𝑊 * is updated, all the parameters in the filterbank are ensured to be non-negative. With ( <ref type="formula" target="#formula_5">5</ref>), all the parameters in the filterbank can be updated. Using an 𝑒𝑝𝑠 term instead of 0 in the mel-filterbank for initialization is found to consistently improve the performance of our system. The whole network is trained for 15 epochs to minimize the cross-entropy criterion from the acoustic model alone. In preliminary studies, we tried to put a weight between the loss of the acoustic model and the loss of the separation frontend, expecting the performance of both tasks to improve. However, no clear improvement on the ASR performance was observed. The learning rate is fixed at 0.0005 for the first 8 epochs and linearly decreased to 10 -5 for the next 7 epochs. Note that the learning rate of 0.0005 is the smallest learning rate used in the previous sections 2 . The momentum is fixed at 0.9 for all the epochs. The mini-batch size is set to 512. No dropout and weight normalization is performed at the filterbank layer and fixed layers. The sentence-level mean of each utterance and global mean and variance are updated at the beginning of each epoch in the forward pass. All the other network setup and training strategies follow the DNN training in the previous sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Sequence-discriminative Training</head><p>The previous sections describe how the DNN-based acoustic models are trained to minimize the cross-entropy criterion at the frame level. As automatic speech recognition is itself a sequence classification problem, it is sensible to optimize the sequence-discriminative criterion to better capture the essence of this problem. It is widely known that sequence training is helpful for GMM-HMM systems. In recent studies, sequence training is also found to be useful for DNN-HMM hybrid systems <ref type="bibr" target="#b41">[42]</ref> <ref type="bibr" target="#b30">[31]</ref>. Here, we investigate the effectiveness of sequence training criterion on the joint training system. As suggested in <ref type="bibr" target="#b41">[42]</ref>, different sequence training criterion gives similar performance on recognition rates. In this study, we replace the frame-wise cross-entropy criterion with the state-level minimum Bayes risk (sMBR) <ref type="bibr" target="#b21">[22]</ref> and back-propagate the error signal from this criterion to influence 2 The optimization processes of the separately trained acoustic model and separation frontend have already converged long before reaching the 0.0005 learning rate. Therefore, the improvement from joint training is not simply because of using a very small learning rate on un-converged models. the weights in the acoustic model, filterbank and separation frontend. This method is expected to improve the performance of speech recognition. We believe that this method may also benefit mask estimation since the error signal from the sequence training criterion contains information from language models, which is rarely exploited in speech separation research.</p><p>To speed up the sMBR training, we re-generate the lattices after the first epoch, and further train the network for six epochs. The learning rate is linearly decreased from 10 -5 , which is the smallest learning rate used at the joint training stage, to 10 -6 . The acoustic scaling factor is fixed at 0.1. The mini-batch size is variable corresponding to the length of each utterance. The sentence-level mean of each utterance is updated dynamically in the forward pass for each mini-batch. All the other network setup and training recipes follow the DNN training at the joint training stage. Most of the times, the performance on the development set converges in 3-5 epochs after re-generating the lattices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Unsupervised Adaptation</head><p>Adaptation is commonly performed on well-trained acoustic models to compensate the differences between training and test conditions. It can be done in a supervised or unsupervised way, depending on whether the labels of adaptation data are available. Many adaptation methods have been proposed for DNN based acoustic models, such as linear transformation <ref type="bibr" target="#b38">[39]</ref>[29], conservative training <ref type="bibr" target="#b57">[58]</ref>, and subspace based methods <ref type="bibr" target="#b37">[38]</ref>. In <ref type="bibr" target="#b27">[28]</ref>, it is suggested that the linear input network (LIN) and linear hidden network based approaches are better than linear output network, factorization and KL-divergence based adaptation.</p><p>We perform unsupervised adaptation to our jointly trained acoustic models following the LIN approach. At the test stage, given a single test utterance, we first use the un-adapted jointly-trained sequence-discriminative model to generate initial decoding results. The first-pass decoded state sequence is then used as the labels for learning a linear transformation of the input features of the separation frontend by minimizing the cross-entropy criterion calculated from the acoustic model, with all the other parameters fixed. The linear transformation is defined as follows:</p><p>𝑥 ̂𝑡,𝑓 = 𝑤 𝑓 𝑥 𝑡,𝑓 + 𝑏 𝑓 <ref type="bibr" target="#b5">(6)</ref> where 𝑥 𝑡,𝑓 denotes the globally mean-variance normalized log power spectrogram, corresponding to the un-adapted input of the separation frontend, 𝑥 ̂𝑡,𝑓 denotes the adapted features, and 𝑤 𝑓 and 𝑏 𝑓 are the parameters to be learned. For a test utterance, the number of parameters to be learned is 322 (161+161), which is approximately in the same range of the number of frames in the test utterance.</p><p>For each utterance, the adaptation process is run for 20 epochs with a mini-batch size equal to the length of the utterance. The learning rate is linearly decreased from 0.005 to 10 -5 . We simply adopt the learned parameters at the last epoch due to the lack of a development set. All the other training recipes and network setup follow the DNN training described in the previous sections. Note that we also perform dropout on the adapted features, which consistently improves the performance due to alleviated overfitting. After we obtain all the linear transformation for each test utterance, we re-generate the likelihood and run a second-pass decoding to obtain the final results.</p><p>It may be argued that the cross-entropy loss function used at the adaptation stage would counteract the effect of the sequence-discriminative criterion used at the joint training stage. We note that this would not be a problem since the cross-entropy criterion will make posterior estimates closer to the initial decoding results generated from the sequence-discriminative model.</p><p>A similar adaptation method was proposed in <ref type="bibr" target="#b28">[29]</ref>. One key difference is that we perform adaptation on the input of the separation frontend rather than on the output of the separation frontend. We think that our strategy is better since, if we perform adaptation on the input of the separation frontend, the enhancement results would be changed in a highly non-linear way rather than in a simple linear fashion.</p><p>Finally, we believe that this unsupervised adaptation technique with the learned linear transformation can also adapt a well-trained separation frontend to new test environments to some extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EVALUATIONS AND COMPARISONS</head><p>We evaluate our method on the recently proposed reverberant and noisy CHiME-2 dataset (task-2) <ref type="bibr" target="#b42">[43]</ref> <ref type="foot" target="#foot_1">3</ref> . The CHiME-2 dataset is created by first convolving clean utterances in the WSJ0-5k dataset with time-varying binaural room impulse responses (BRIRs) and then mixing with reverberant noises at six SNR levels equally spaced from -6 dB to 9 dB. The BRIRs and reverberant noises are recorded with the same microphone and living room setup. The recorded noises contain major noise sources in a typical kitchen or living room, such as competing speakers, electronic devices, footsteps, laughter, and distant noises. The multi-conditional training set (si_tr_s) contains 7138 utterances (~14.5h) in total. The development set (si_dt_05) contains 409 utterances at each SNR level (~4.5h). The test set (si_et_05) contains 330 utterances at each SNR level (~4h). The CHiME-2 dataset also provides reverberant noises, and reverberant noise-free utterances corresponding to the multi-conditional training set. With the noises, clean speech, reverberant noise-free utterances, and noisy-reverberant utterances available, we can readily evaluate the recognition performance together with speech separation performance of our system.</p><p>Our system is monaural in nature, and we simply average the signals from the left and right channel before extracting features. In our experiments, this technique is much better than only using one of these two channels. A GMM-HMM system is built using the Kaldi toolkit <ref type="bibr" target="#b33">[34]</ref> on the clean utterances in the WSJ0-5k to get the senone state for each frame of the corresponding noisy-reverberant utterances. Following the common pipeline in the Kaldi toolkit, the GMM-HMM system is first built using the MFCC feature. Then we concatenate 13-dimensional MFCC feature within a 7-frame context window, and utilize linear discriminant analysis (LDA) to compress the concatenated feature to 40 dimensions. After that, we de-correlate it via maximum likelihood linear transform (MLLT) and use feature-space maximum likelihood linear regression (fMLLR) to reduce speaker variance, which is estimated by speaker adaptive training. The resulting cross-word tied-state tri-phone GMM-HMM system contains 1965 senone states in total. The initial clean alignments are obtained by performing forced alignment on the clean utterances. To refine the initial clean alignments, we further train a DNN-based acoustic model using the MEL features of the clean utterances, and re-generate clean alignments. Such clean alignments are used as the labels for training all the acoustic models in this study. Note that the DNN-HMM hybrid system built on the clean utterances is a powerful recognizer. It achieves 2.15% WER on the clean test set of the WSJ0-5k dataset. Therefore, we believe that these high-quality labels can guide the DNN-based acoustic model to perform well on discriminating different senone states even when the input features are very noisy and the input SNR is very low. We use the CMU pronunciation dictionary and the official 5k close-vocabulary tri-gram language model in our experiments. Note that this language model is used for decoding at the test stage and generating the lattices of the training utterances at the sequence training stage.</p><p>The training data for mask estimation is obtained from parallel noisy-reverberant and reverberant noise-free data. The mixed noise signals can be obtained by direct subtraction. With these datasets available, we train a separation frontend using the method detailed in Section II.A. The frontend is trained to remove additive noise in noisy-reverberant utterances. Note that the noisy-reverberant dataset, i.e. the multi-conditional training data, is used for both mask estimation and acoustic modeling.</p><p>Our experiments are done in an incremental manner. We first build our acoustic models using feature subsets selected according to the performance on the development set. Then we jointly train the acoustic models with the separation frontend. Afterwards, we perform sequence training on the jointly trained DNN. Finally, we perform unsupervised adaptation to the sequence-discriminative jointly-trained DNN at the test stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Expanded Feature Set for Acoustic Modeling</head><p>We first report the results of incorporating more robust features for acoustic modeling. In this experiment, no speech enhancement or separation is performed. We simply train acoustic models multi-conditionally by adding more robust features and do not tune the network structure or training recipes for each feature set. To push up the baselines, we perform sequence training on the multi-conditionally trained acoustic models, which is followed by unsupervised adaptation at the test stage. The WER results are presented in Table <ref type="table" target="#tab_1">I</ref>.</p><p>If we only train our acoustic models using the cross-entropy criterion, with the commonly used MEL features alone, we are able to obtain 16.16% average WER on the test set. Note that if we just use the default DNN code for the CHiME-2 dataset in the Kaldi toolkit, we can only obtain 17.49% average WER on the test set. This is consistent with the results obtained in <ref type="bibr" target="#b12">[13]</ref>. The major differences are that we use ReLUs, dropout and Adagrad for training, while the default DNN code uses sigmoidal units, pre-training and stochastic gradient descent. By adding the PNCC feature, the average WER can be reduced to 14.74%. After appending the MRCG feature, the WER is brought down to 13.97%. The performance is further pushed to 13.46% average WER after we add the Fset features. Note that this result is already better than our previous best result <ref type="bibr" target="#b49">[50]</ref> using the same set of features on this dataset, mainly because better clean alignments are generated using the Kaldi toolkit.</p><p>We then apply sequence training to the multi-conditionally trained acoustic models. The training recipes follow the sequence training described in Section II.D. We observe that sequence training leads to large improvement for all the input features, and the relative improvement becomes smaller if more features are used for acoustic modeling.</p><p>Finally, we apply utterance-level unsupervised adaptation to the sequence-discriminative acoustic models. Similar to Section II.E, given a test utterance, we first decode it to obtain a hypothesized state sequence, from which we learn a linear transformation of the input features. To reduce the number of parameters to be learned and make a fair comparison with later experiments, we only learn a linear transformation for the MEL features. Learning linear transformations for other features may decrease the performance, simply because too many parameters are learned. Thus, the total number of parameters to be learned is 240 (40*3+40*3) for each test utterance. From Table <ref type="table" target="#tab_1">I</ref>, we can see that unsupervised adaptation leads to consistent improvement, while the relative improvement for acoustic models with more features becomes smaller as well.</p><p>Compared with only using the MEL features, adding all the extra robust features for acoustic modeling reduces the average WER by 2.7 (16.16% to 13.46%), 1.75 (13.92% to 12.17%), and 1.34 percentage points (13.20% to 11.86%) without sequence training or adaptation, with sequence training but no adaptation, and with sequence training and adaptation, respectively. These considerable improvements occur probably because features are extracted from different domains using different filterbanks, compression operations and environmental compensations, and therefore they likely complement each other for acoustic modeling on multi-conditional data. This suggests that relying on the DNN to learn optimal non-linear features from relatively raw input, e.g. the MEL features, may not be the optimal strategy for robust ASR. Combining the feature learning ability of DNNs and domain knowledge may be a better way for improving the robustness of ASR systems.</p><p>As shown in Table <ref type="table" target="#tab_1">I</ref>, the average WER on the development set keeps decreasing as we add more and more features. Therefore, in the following experiments, we add the PNCC, MRCG and Fset features for acoustic modeling. Note that we do not perform any kind of enhancement on these extra features since they are considered to be inherently robust in our study. To facilitate comparisons, we also report the results based on the MEL features alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Plug-and-Play and Re-training Approaches</head><p>Before presenting the results of the joint training approach, we explore two alternative strategies when incorporating speech separation into ASR systems.</p><p>The first strategy, denoted as plug-and-play, is to train our acoustic models using the MEL features alone or the MEL+PNCC+MRCG+Fset features. In the test stage, we use the trained separation frontend to get the enhanced power spectrogram which is then passed to the mel-filterbank to get the enhanced MEL features. Finally, together with other robust features, the enhanced MEL features are passed to the acoustic model for decoding. As shown in the first entry of Table <ref type="table" target="#tab_1">II</ref>, if we only use the MEL features for acoustic modeling, the frontend can lead to 1.39% (16.16% to 14.77%), 0.5% (13.92% to 13.42%), and 0.58% (13.20% to 12.62%) absolute improvement without sequence training or adaptation, with sequence training but no adaptation, and with sequence training and adaptation, respectively. We can see that the relative   Comparing the results from plug-and-play and re-training approach, we find that the former strategy typically scores higher. One possible reason is that, when re-training is used, the separation frontend significantly reduces the variations seen by the acoustic model at the training stage <ref type="bibr" target="#b40">[41]</ref>. In addition, the distortion it introduces for the training utterances may be different from that for the test utterances. Another possible explanation is related to overfitting. Since in this study <ref type="foot" target="#foot_2">4</ref> , the separation frontend is also trained on the multi-conditional training data. We can reasonably assume that the separation frontend performs better on the training set than on the development and test set. Therefore, if the enhanced training data is subsequently used to re-train the acoustic models, overfitting would likely happen. This is exactly what we encountered in our experiments. For the re-training approach, the loss of the acoustic model on the development set is much better than that of the plug-and-play or the direct multi-condition training approach; however it gives us worse performance after decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Joint Training</head><p>Considering that more variations would be seen by the acoustic models trained on noisy-reverberant utterances and the plug-and-play approach normally gets better performance on the development set as shown in Table <ref type="table" target="#tab_1">II</ref> and III, we use the parameters in the acoustic models from this approach, together with the separation frontend, to initialize the corresponding parameters in the joint-training DNN, and then perform joint training. When joint training is done, sMBR training and run-time adaptation are conducted. Note that for the run-time adaptation, we learn a linear transformation of the input of the separation frontend. The number of parameters to be learned for each utterance is 322 (161+161).</p><p>As reported in Table <ref type="table" target="#tab_1">II</ref>, after joint training, the performance can be improved from 14.77% to 13.99% average WER. After sMBR training, the performance is improved to 12.07%. The performance is further pushed up to 11.23% after run-time unsupervised adaption, which is helpful especially in low SNR conditions. For example, when the input SNR is -6 dB, the WER is reduced from 20.44% to 18.72%.</p><p>If we do not use extra robust features for acoustic modeling, compared with plug-and-play, we reduce the average WER by absolute 0.78% or relative 5.3% (14.77% to 13.99%) if only the cross-entropy criterion is used for joint training. The performance gap is enlarged to absolute 1.35% or relative 10.06% (13.42% to 12.07%) after sequence training is applied. If we further perform unsupervised adaptation at the test stage, the performance difference is further increased to absolute 1.39% or relative 11.01% (12.62% to 11.23%). Interestingly, the relative improvement becomes larger after sequence training and unsupervised adaptation are applied to the joint-training DNN. This trend can also be observed by comparing the first entry with the fourth entry in Table <ref type="table" target="#tab_1">III</ref>, where more features are used for acoustic modeling. This is desirable since, in joint modeling, the noise compensation module can be seamlessly combined with other ASR techniques, such as sequence training and adaptation, to obtain further improvement.</p><p>As presented in the fourth and fifth entry of Table <ref type="table" target="#tab_1">II</ref>, co-adapting the filterbank with the separation frontend and acoustic model can give us slightly better results. If the parameters in the filterbank are co-adapted, the performance is 0.24% (14.23% to 13.99%) average WER better after joint training, 0.12% (12.19% to 12.07%) better after sMBR training, and 0.1% (11.33% to 11.23%) better after run-time adaptation.</p><p>These results clearly demonstrate the effectiveness of joint training. We think that it is due to the reduction of the distortion problem and the linguistic information back-propagated from the acoustic model to the separation frontend. In addition, the separation frontend used in this study treats all the frames and time-frequency units equally important, without considering the underlying linguistic information that is critical for senone states discrimination. In contrast, with joint modeling, the separation frontend can be somehow informed by the acoustic model to produce more discriminative enhancement results.</p><p>The best performance we obtained on the test set is 11.23% average WER if no extra robust features are used. With extra robust features, the performance can be further improved to 10.63%. With more sophisticated training and adaptation techniques, the effectiveness of extra features is reduced. This would be welcome as using a small number of features, such as log mel-spectrogram, is favored in industry. On the other hand, incorporating more robust features for acoustic modeling is a simple and effective technique towards improved robustness of ASR systems.</p><p>It might be argued that the joint training approach just performs acoustic modeling multi-conditionally by training a very deep and large DNN on a combination of features. To address this possibility, we train a DNN with 12 (4+1+7) hidden layers, each with 1600 ReLUs, on the combination of the log power spectrogram and MEL features (without robust features) using multi-condition training directly. Note that the number of parameters in this new DNN is almost the same as that in the joint training DNN. The performance, shown in the last entry of Table <ref type="table" target="#tab_1">II</ref>, is much worse than that of joint training. This is likely because the joint training approach has better network architecture and better parameter initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with Other Studies</head><p>In Table <ref type="table" target="#tab_1">IV</ref>, we list the results of several other studies that report competitive results on the same dataset. All of them use the DNN-HMM hybrid approach and clean alignments from clean utterances as the labels to train their acoustic models. The system described in <ref type="bibr" target="#b51">[52]</ref> employs an RNN to perform acoustic modeling on the noisy-reverberant training data and does not use any speech enhancement or separation. Chen et al. <ref type="bibr" target="#b2">[3]</ref> utilize LSTM for both speech separation and acoustic modeling. Their ASR systems follow the re-training approach, and an iterative strategy using alignment information from their ASR system is proposed to improve speech separation and recognition simultaneously. Weninger et al. <ref type="bibr" target="#b52">[53]</ref> build their frontend by training an RNN with the LSTM activation function to predict a phase-sensitive spectrum approximation objective function. They also use re-training and additional alignment information from ASR systems to boost the performance of speech separation. Their DNN based acoustic models are built in a way similar to the standard recipes in the Kaldi toolkit. Both enhanced and unenhanced log mel-filterbank features without delta components are utilized for acoustic modeling, and no extra robust features are used in their study. Han et al. <ref type="bibr" target="#b12">[13]</ref> use a spectral mapping based separation frontend to enhance both the training and test set first, and perform acoustic modeling on the enhanced training set using the standard DNN training recipes in the Kaldi toolkit. Their overall WER is 15.6%, which is slightly worse than obtained by Narayanan and Wang <ref type="bibr" target="#b30">[31]</ref>. To our knowledge, the results by Weninger et al. <ref type="bibr" target="#b52">[53]</ref> are the best on the CHiME-2 dataset reported in the literature so far. As shown in Table <ref type="table" target="#tab_1">IV</ref>, we have now pushed the performance to 10.63% average WER. This represents a 22.75% relative error reduction over <ref type="bibr" target="#b52">[53]</ref>, and the best result to date.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUDING</head><p>Moving forward, we plan to employ sequence models, such as the RNN or LSTM, for speech separation and acoustic modeling since they have been shown to capture temporal dynamics well <ref type="bibr" target="#b19">[20]</ref>. How to effectively perform joint training on two RNNs is an open question.</p><p>Speech separation and recognition are two closely related problems. In this study, a joint training strategy is presented to integrate speech separation and acoustic modeling at the training stage. By further applying sequence training and run-time adaptation, the performance advantage of the joint modeling approach becomes even larger. Still, speech separation is done in a bottom-up fashion at the test stage. How to leverage top-down information, such as the knowledge from language models, to help speech separation at the test stage is an interesting direction for future research. We think that the joint modeling approach presented in this paper is an important step towards this goal, because language models are about the relations among words, or in a wider sense, among phonemes or states, while speech separation is commonly done in the time-frequency domain or at the signal level <ref type="bibr" target="#b50">[51]</ref>. There is clearly a gap between them. The joint modeling approach utilizes acoustic models to bridge these two modules so that the information can be potentially flowed back and forth.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>40*3*11+256*3+31*3*11+13*7+15*26+31*7+31*7). They are globally mean-variance normalized before DNN training. To facilitate comparison, we always include the MEL features for acoustic modeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Schematic diagram of the proposed joint training framework. The layer shown in gray means that the weights or operations of that layer are fixed. Solid and dotted arrows indicate the directions of forward pass and backward pass, respectively. See text for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, 2015; revised Dec. 1, 2015; revised Jan. 31, 2015; accepted Feb. 3, 2015. Date of publication xx; date of current version xx. This research was supported in part by an NSF grant (IIS-1409431), an AFOSR grant (FA9550-12-1-0130), and the Ohio Supercomputer Center. Zhong-Qiu Wang is with the Department of Computer Science and Engineering, The Ohio State University, Columbus, OH 43210-1277 USA. (E-mail: wangzhon@cse.ohio-state.edu). DeLiang Wang is with the Department of Computer Science and Engineering and Center for Cognitive and Brain Sciences, The Ohio State</figDesc><table><row><cell>University,</cell><cell>Columbus,</cell><cell>OH</cell><cell>43210-1277</cell><cell>USA.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I PERFORMANCE</head><label>I</label><figDesc>(% WER) USING MULTI-CONDITION TRAINING WITH MORE ROBUST FEATURES FOR ACOUSTIC MODELING</figDesc><table><row><cell cols="2">Features for Acoustic Modeling</cell><cell cols="2">dev. set Average -6dB</cell><cell>-3dB</cell><cell>0dB</cell><cell cols="2">test set 3dB</cell><cell>6dB</cell><cell>9dB</cell><cell>Average</cell></row><row><cell>MEL</cell><cell></cell><cell>19.40</cell><cell cols="6">26.77 20.49 16.14 12.80 10.67 10.11</cell><cell>16.16</cell></row><row><cell>+sMBR</cell><cell></cell><cell>17.24</cell><cell cols="4">23.87 17.35 13.64 11.30</cell><cell></cell><cell>9.10</cell><cell>8.28</cell><cell>13.92</cell></row><row><cell>+adaptation</cell><cell></cell><cell>16.81</cell><cell cols="4">22.64 16.85 12.78 10.44</cell><cell></cell><cell>8.69</cell><cell>7.79</cell><cell>13.20</cell></row><row><cell>MEL+PNCC</cell><cell></cell><cell>18.54</cell><cell cols="4">25.13 18.57 14.94 11.73</cell><cell></cell><cell>9.51</cell><cell>8.57</cell><cell>14.74</cell></row><row><cell>+sMBR</cell><cell></cell><cell>16.52</cell><cell cols="4">23.22 16.59 12.46 10.52</cell><cell></cell><cell>8.24</cell><cell>7.49</cell><cell>13.09</cell></row><row><cell>+adaptation</cell><cell></cell><cell>16.10</cell><cell cols="4">22.03 16.33 12.22 10.29</cell><cell></cell><cell>7.66</cell><cell>7.36</cell><cell>12.65</cell></row><row><cell cols="2">MEL+PNCC+MRCG</cell><cell>17.99</cell><cell cols="4">23.33 17.92 14.20 11.36</cell><cell></cell><cell>8.95</cell><cell>8.05</cell><cell>13.97</cell></row><row><cell>+sMBR</cell><cell></cell><cell>15.97</cell><cell cols="4">22.01 15.62 12.18 10.59</cell><cell></cell><cell>8.18</cell><cell>7.12</cell><cell>12.62</cell></row><row><cell>+adaptation</cell><cell></cell><cell>15.57</cell><cell cols="4">21.17 15.21 11.83 10.55</cell><cell></cell><cell>7.77</cell><cell>6.80</cell><cell>12.22</cell></row><row><cell cols="2">MEL+PNCC+MRCG+Fset</cell><cell>17.93</cell><cell cols="4">23.09 17.17 13.32 10.41</cell><cell></cell><cell>8.71</cell><cell>8.07</cell><cell>13.46</cell></row><row><cell>+sMBR</cell><cell></cell><cell>15.63</cell><cell cols="3">21.17 14.96 12.24</cell><cell>9.83</cell><cell></cell><cell>7.68</cell><cell>7.14</cell><cell>12.17</cell></row><row><cell>+adaptation</cell><cell></cell><cell>15.48</cell><cell cols="3">20.51 14.68 11.77</cell><cell>9.70</cell><cell></cell><cell>7.49</cell><cell>7.02</cell><cell>11.86</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE II</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">PERFORMANCE (% WER) COMPARISON OF THE PROPOSED APPROACH WITHOUT EXTRA ROBUST FEATURES</cell></row><row><cell>Approaches</cell><cell></cell><cell cols="2">Acoustic Model</cell><cell cols="3">dev. set Average -6dB</cell><cell cols="2">-3dB</cell><cell>0dB</cell><cell>test set 3dB</cell><cell>6dB</cell><cell>9dB Average</cell></row><row><cell></cell><cell></cell><cell>MEL</cell><cell></cell><cell cols="2">18.22</cell><cell cols="3">23.58 18.53 14.85 12.42</cell><cell>9.68</cell><cell>9.56</cell><cell>14.77</cell></row><row><cell>Plug-and-Play</cell><cell></cell><cell>+sMBR</cell><cell></cell><cell cols="2">16.63</cell><cell cols="3">22.72 16.12 13.81 10.84</cell><cell>8.61</cell><cell>8.39</cell><cell>13.42</cell></row><row><cell></cell><cell></cell><cell>+adaptation</cell><cell></cell><cell cols="2">16.05</cell><cell cols="3">21.18 15.82 12.16 10.54</cell><cell>8.14</cell><cell>7.88</cell><cell>12.62</cell></row><row><cell></cell><cell></cell><cell cols="2">Enhanced MEL</cell><cell cols="2">18.67</cell><cell cols="3">25.85 19.20 15.93 12.52</cell><cell>9.96</cell><cell>9.21</cell><cell>15.45</cell></row><row><cell>Re-training</cell><cell></cell><cell>+sMBR</cell><cell></cell><cell cols="2">17.08</cell><cell cols="3">24.38 17.19 13.66 11.10</cell><cell>8.69</cell><cell>8.20</cell><cell>13.87</cell></row><row><cell></cell><cell></cell><cell>+adaptation</cell><cell></cell><cell cols="2">16.59</cell><cell cols="3">23.54 16.40 12.76 10.55</cell><cell>8.37</cell><cell>7.66</cell><cell>13.21</cell></row><row><cell></cell><cell cols="3">Enhanced MEL + MEL</cell><cell cols="2">18.31</cell><cell cols="3">25.31 18.83 15.69 11.94</cell><cell>9.23</cell><cell>8.89</cell><cell>14.98</cell></row><row><cell>Re-training</cell><cell></cell><cell>+sMBR</cell><cell></cell><cell cols="2">16.50</cell><cell cols="3">24.10 16.68 14.18 10.42</cell><cell>8.63</cell><cell>7.88</cell><cell>13.65</cell></row><row><cell></cell><cell></cell><cell>+adaptation</cell><cell></cell><cell cols="2">16.07</cell><cell cols="3">22.70 16.14 13.32</cell><cell>9.96</cell><cell>7.88</cell><cell>7.40</cell><cell>12.9</cell></row><row><cell></cell><cell cols="3">Jointly enhanced MEL</cell><cell cols="2">17.63</cell><cell cols="3">22.55 17.65 14.42 11.36</cell><cell>9.23</cell><cell>8.74</cell><cell>13.99</cell></row><row><cell>Jointly training frontend, AM and filterbank</cell><cell></cell><cell>+sMBR</cell><cell></cell><cell cols="2">15.28</cell><cell cols="3">20.44 14.66 12.39</cell><cell>9.81</cell><cell>7.73</cell><cell>7.38</cell><cell>12.07</cell></row><row><cell></cell><cell></cell><cell>+adaptation</cell><cell></cell><cell cols="2">14.56</cell><cell cols="3">18.72 13.77 11.36</cell><cell>9.32</cell><cell>7.32</cell><cell>6.86</cell><cell>11.23</cell></row><row><cell></cell><cell cols="3">Jointly enhanced MEL</cell><cell cols="2">17.62</cell><cell cols="3">23.15 17.69 14.72 11.38</cell><cell>9.30</cell><cell>9.15</cell><cell>14.23</cell></row><row><cell>Jointly training frontend and AM</cell><cell></cell><cell>+sMBR</cell><cell></cell><cell cols="2">15.30</cell><cell cols="3">20.61 14.89 12.48</cell><cell>9.81</cell><cell>7.85</cell><cell>7.49</cell><cell>12.19</cell></row><row><cell></cell><cell></cell><cell>+adaptation</cell><cell></cell><cell cols="2">14.60</cell><cell cols="3">19.13 13.67 11.40</cell><cell>9.19</cell><cell>7.51</cell><cell>7.08</cell><cell>11.33</cell></row><row><cell>Directly training a large DNN</cell><cell cols="3">Log power spectrogram + MEL</cell><cell cols="2">19.06</cell><cell cols="3">24.88 18.91 15.15 12.57 10.44 9.25</cell><cell>15.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Similar observations can be found in the first entry of TableIII, in which we use the MEL+PNCC+MRCG+Fset features for acoustic modeling.The second alternative, denoted as re-training, is to train our acoustic models using the enhanced MEL features alone or the enhanced MEL+PNCC+MRCG+Fset features. At the test stage, after we get the enhanced MEL features, together with other robust features, we feed all of them to the acoustic model for decoding. Note that, again, the Fset, MRCG and PNCC features are directly extracted from the original noisy-reverberant utterances. The results are shown in the second entry in TableIIand TableIII, respectively. Note that, adaptation is performed only on the enhanced MEL features. Motivated by deep stacking<ref type="bibr" target="#b4">[5]</ref>[53], the unenhanced MEL features are additionally incorporated for acoustic modeling. The results are reported in the third entry of TableIIand III, without and with extra robust features, respectively. We can see that adding the unenhanced MEL features for acoustic modeling brings some gains for the re-training approach.</figDesc><table><row><cell>improvement of using our frontend becomes much smaller if</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>the acoustic model has been sequence-trained. Note that for</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>unsupervised adaptation, we learn a linear transformation of the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>enhanced MEL features. The first-pass decoding results for</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>adaptation are obtained by applying the plug-and-play</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>approach to the sequence-discriminative acoustic model.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Again, the number of parameters to be learned is 240</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(40*3+40*3). Performing unsupervised adaptation on the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>enhanced MEL features can lead to 0.8% (13.42% to 12.62%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>average WER reduction.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">TABLE IV</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">PERFORMANCE (% WER) COMPARISON OF THE PROPOSED APPROACH WITH OTHER STUDIES</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Study</cell><cell cols="2">dev. set Average -6dB</cell><cell>-3dB</cell><cell>0dB</cell><cell>test set 3dB</cell><cell>6dB</cell><cell>9dB</cell><cell>Average</cell></row><row><cell>Weng et al. [52]</cell><cell>-</cell><cell cols="6">38.11 29.07 22.98 17.92 14.96 13.60</cell><cell>22.77</cell></row><row><cell>Chen et al.[3]</cell><cell>20.11</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>16.04</cell></row><row><cell>Narayanan-Wang [31]</cell><cell>-</cell><cell>25.1</cell><cell>19.2</cell><cell>15.1</cell><cell>12.8</cell><cell>10.5</cell><cell>9.5</cell><cell>15.4</cell></row><row><cell>Weninger et al. [53]</cell><cell>17.87</cell><cell cols="4">23.48 17.02 13.71 10.72</cell><cell>8.95</cell><cell>8.67</cell><cell>13.76</cell></row><row><cell>sMBR+joint training+multi-stream+run-time adaptation (proposed)</cell><cell>13.81</cell><cell cols="3">18.23 13.02 10.39</cell><cell>8.67</cell><cell>6.86</cell><cell>6.61</cell><cell>10.63</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We tried to use this feature set to estimate the IRM defined in the power spectrogram domain and in the mel-spectrogram domain as well, but the ASR performance is not as good as using the log power spectrogram directly.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Available at http://spandh.dcs.shef.ac.uk/chime_challenge/chime2013/WSJ0/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>This underlying problem also exists in many other studies.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>We would like to thank Yanzhang He for providing Kaldi training recipes and anonymous reviewers for their constructive comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Combining spectral feature mapping and multi-channel model-based source separation for noise-robust automatic speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bagchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fosler-Lussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A feature study for classification-based speech separation at low signal-to-noise ratios</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1993" to="2002" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Speech enhancement and recognition using multi-task learning of long short-term memory recurrent neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Is speech enhancement pre-processing still relevant when using deep neural networks for acoustic modeling?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kubo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2992" to="2996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scalable stacking and learning for building deep architectures</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2133" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust speech recognition with speech enhanced deep neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="616" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="708" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Joint training of front-end and back-end deep neural networks for robust speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4375" to="4379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Memory-enhanced neural networks and nmf for robust ASR</title>
		<author>
			<persName><forename type="first">J</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wollmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1037" to="1046" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep neural network based spectral feature mapping for robust speech recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bagchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fosler-Lussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2484" to="2488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning spectral mapping for speech dereverberation and denoising</title>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Merks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="982" to="992" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deepspeech: scaling up end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5567</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An algorithm to improve speech recognition in noise for hearing-impaired listeners</title>
		<author>
			<persName><forename type="first">E</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3029" to="3038" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">RASTA processing of speech</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="578" to="589" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Super-human multi-talker speech recognition: a graphical modeling approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Olsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Kristjansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: the shared views of four research groups</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Power-normalized cepstral coefficients (PNCC) for robust speech recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="4101" to="4104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="3761" to="3764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Speech enhancement based on physiological and psychoacoustical models of modulation perception and binaural interaction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kollmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1593" to="1602" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A spectral masking approach to noise-robust speech recognition using deep neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1296" to="1305" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A long, deep and wide artificial neural net for robust speech recognition in unknown noise</title>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nidadavolu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An overview of noise-robust automatic speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="745" to="777" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep autoencoders augmented with phone-class feature for reverberant speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4365" to="4369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A study on deep neural network acoustic model adaptation for robust far-field speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mirsamadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2430" to="2434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Investigation of speech separation as a front-end for noise robust speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="826" to="835" />
			<date type="published" when="2014-04">Apr. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Joint noise adaptive training for robust automatic speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2504" to="2508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving robustness of deep neural network acoustic models via speech separation and joint adaptive training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="92" to="101" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ideal ratio mask estimation using deep neural networks for robust speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="7092" to="7096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A time delay neural network architecture for efficient modeling of long temporal contexts</title>
		<author>
			<persName><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3214" to="3218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Annealed dropout training of deep networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Spoken Language Technology Workshop</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="159" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning filter banks within a deep neural network framework</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="297" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for large-scale speech tasks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="39" to="48" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Speaker adaptation of neural network acoustic models using i-vectors</title>
		<author>
			<persName><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="55" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Feature engineering in context-dependent deep neural networks for conversational speech transcription</title>
		<author>
			<persName><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="24" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robustness is dead! Long live robustness!</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Reverb. Challenge Workshop</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An investigation of deep neural networks for noise robust speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="7398" to="7402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sequence-discriminative training of deep neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Veselý</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2345" to="2349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The second &apos;CHiME&apos; speech separation and recognition challenge: an overview of challenge systems and outcomes</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nesta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matassoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="162" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On ideal binary mask as the computational goal of auditory scene analysis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech Separation by Humans and Machines</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Divenyi</surname></persName>
		</editor>
		<imprint>
			<publisher>Spinger</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="181" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Computational auditory scene analysis: principles, algorithms, and applications</title>
		<editor>D.L. Wang and G.J. Brown</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Press</publisher>
			<pubPlace>Hoboken, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Deep neural network based supervised speech segregation generalizes to novel noises through large-scale training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<idno>OSU-CISRC-3/15-TR02</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Exploring monaural features for classification-based speech segregation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="270" to="279" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Time-frequency masking for large scale robust speech recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2469" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">On training targets for supervised speech separation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1849" to="1858" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Joint training of speech separation, filterbank and acoustic model for robust automatic speech recognition</title>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2839" to="2843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Phoneme-specific speech separation</title>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Recurrent deep neural networks for robust speech recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-H</forename><forename type="middle">F</forename><surname>Juang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="5532" to="5536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Speech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR</title>
		<author>
			<persName><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Latent Variable Analysis and Signal Separation</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Discriminatively trained recurrent neural networks for single-channel speech separation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Global Conference on Signal and Information Processing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="577" to="581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Towards scaling up classification-based speech separation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1381" to="1390" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A regression approach to speech enhancement based on deep neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="19" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Feature learning in deep neural networks -studies on speech recognition tasks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3605</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">KL-divergence regularized deep neural network adaptation for improved large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="7893" to="7897" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
