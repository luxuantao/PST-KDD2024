<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rival Penalized Competitive Learning for Clustering Analysis, RBF Net, and Curve Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lei</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Adam</forename><surname>Krzyzak</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harvard Robotics Laboratory</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<address>
									<postCode>02138</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Concordia University</orgName>
								<address>
									<postCode>H3G 1M8</postCode>
									<settlement>Montreal</settlement>
									<region>Quebec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Information Technology</orgName>
								<orgName type="laboratory">IEEE Log Number</orgName>
								<orgName type="institution">Lappeenranta University of Technology</orgName>
								<address>
									<postCode>SF-53851, 9204279</postCode>
									<settlement>Lappeenranta</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Rival Penalized Competitive Learning for Clustering Analysis, RBF Net, and Curve Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">42CB547526B8663A9F103BCA2490E67C</idno>
					<note type="submission">received December 12, 1991; revised August 17, 1992.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It is well known that the classical k-means clustering algorithm has a problem of selecting an appropriate k, the number of clusters. It is a hard problem and affects the performance of k-means strongly. When used for clustering analysis, the conventional competitive learning (CL) algorithms also have a similar crucial problem: the selection of an appropriate number of neural units, although the problem remains largely unrevealed (or ignored) in the CL literature. As shown in this paper, frequency sensitive competitive learning (FSCL), one version of the recently improved CL algorithms, also significantly deteriorates its performance when the number of units is inappropriately selected. This paper proposes a new algorithm called rival penalized competitive learning (RPCL). The basic idea is that for each input not only the winner unit is modified to adapt to the input, but also its rival (the 2nd winner) is delearned by a smaller learning rate. RPCL can be regarded as an unsupervised extension of Kohonen's supervised LVQ2. RPCL has the ability of automatically allocating an appropriate number of units for an input data set. The experimental results show that RPCL outperforms FSCL when used for unsupervised classification, for training a radial basis function (RBF) network, and for curve detection in digital images. I. INTRODUCTION S an adaptive version of the classical k-means clustering A algorithm, competitive learning (CL) has a number of applications. First, it can function as an adaptive method for clustering analysis problems encountered in statistical data analysis or unsupervised pattern recognition [I]. Second, it can be used for vector quantization which is widely used in image processing and speech signal processing for compressing data and message coding [2], [3]. Third, it has also been recently incorporated into some supervised learning methods for training multilayer feedforward nets more effectively, e.g., into the radial basis function (RBF) nets for locating the centers of Gaussian receptive fields [4]-[6]. However, it was found by Rumelhart et al. [7], Grossberg [8], and Hecht-Nielsen [8] that the simple classical CL algorithm has the so called under-utilized or dead unit problem. Many efforts have been made to solve the problem. Grossberg's ART series [7], [lo]-[13] and Kohonen Map [14], [15] are two main developments of the classical competitive learning. ART provides a stable model for unsupervised pattern Manuscript</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>recognition and associative memory. However, as pointed out by Lippmann [16], the ART method does not correspond to k-means algorithm for clustering analysis and vector quantization in the global optimization sense. Although Kohonen Map is related to adaptive k-means, its main purpose is to form a topographic feature map which is a more complex task than just clustering analysis. It is therefore also computationally more complicated.</p><p>Focusing on improving the dead units problem of the classical CL, there are also several other techniques proposed, e.g., leaky learning by <ref type="bibr">Rumelhart [7]</ref> and Grossberg [8], and convex bridge by Hecht-Nielsen <ref type="bibr">[9]</ref>. A notable improvement is the strategy of reducing the winning rate of the frequent winners <ref type="bibr">[8]</ref>, <ref type="bibr">[ 171-[ 193, sometimes called conscience [ 181.</ref> Frequency Sensitive Competitive Learning <ref type="bibr">(FSCL)</ref> [19] is a good example which uses this strategy. The idea is fairly straightforward but the method does improve the classical CL significantly.</p><p>In this paper, we will show that there is another critical problem with the present CL algorithms: the selection of an appropriate number of the units needed. It is well known that one key problem with the k-means algorithm is that Ic (the number of clusters) should be appropriately pre-selected, otherwise the algorithm will perform badly. In a competitive learning net, k directly corresponds to the number of the neural units used. This number should also be externally preselected appropriately, since the number of units corresponds to the number of resulted clusters when CL is used for solving a clustering problem, or to the the number of hidden units when CL is used in a RBF network. As will be shown later, inappropriately selected k will result in a poor clustering result that will in turn affect the performances of FSCL for unsupervised classification, RBF net, and curve detection significantly.</p><p>For tackling this problem, we propose a new version of competitive learning algorithm called rival penalized competitive learning <ref type="bibr">(RPCL)</ref>, which is developed by adding a new mechanism into FSCL. The basic idea is that for each input, not only the weights of the winner unit are modified to adapt to the input, but also the weights of its rival (the 2nd winner) are delearned by a smaller learning rate. The idea can be regarded as an unsupervised extension of <ref type="bibr">Kohonen's LVQ2 [15]</ref> which is a supervised vector quantization algorithm, and closer in effect to Bayes decision theory. We have applied RPCL to the problems of unsupervised classification, RBF network training, and curve detection, and compared its performance with FSCL. The experimental results show that significant improvements have been obtained by RPCL.</p><p>In the sequel, the problem of selecting the number of units in a competitive learning net will be addressed in Section 11. Then RPCL is proposed and analyzed in Section 111. In Section IV, FSCL, and RPCL are experimentally compared in the problems of unsupervised classification and RBF net's training. In Section V, the curve detection problem is modeled into a problem of competitive learning, and both FSCL and RPCL are tested with a problem of detecting four lines in a diamond-shaped frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">A CRUCIAL PROBLEM FOR COMPETITIVE LEARNING</head><p>Given a layer of units with the output of each unit denoted by ui and its weight vector by G i for i = 1, .... IC, the classical CL algorithm consists of the following two steps.</p><p>Step 1: Randomly take a sample 2 from a data set D, and for i = l , . -. , k , let if i = c such that 113-GC,1l2 = minj 112-GjIl2, U ; = 1, 0, otherwise.</p><p>( 1 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{</head><p>Step 2: Update the weight vectors Gi by</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aiii; = C Y ~U ; ( Z -Gi)</head><p>Thus in practice only the winning unit or best matching unit dc is updated. In (lb), the parameter CY, with 0 5 a, 5 1 is the learning rate which is usually a small real number, or it starts from a reasonable initial value and then reduces to zero in some way <ref type="bibr">[14]</ref>, <ref type="bibr">[15]</ref>, e.g., in the way used in the Robbins-Monro stochastic approximation procedure <ref type="bibr">[21]</ref>. The explicit dependence of CY, on time is not shown above. Note: Equations (la) and (lb) are often called the Winner-Take-All rule. In addition, there are also some other versions of the classical CL <ref type="bibr" target="#b22">[26]</ref>. However, the basic idea is the same; the weight vector of the neural unit which tunes to an input most strongly is adjusted most strongly to tune to the input even stronger.</p><p>The implementation of the above algorithm is quite easy; each weight vector Gi is randomly initialized, and then the above two steps are iterated until the iteration converges or freezes as the learning rate a, becomes zero or very small, or until the number of iterations reaches a prespecified value. the initial weight vectors. W'l,t&amp; will move to cluster centers when they are initialized at points A I , B1, respectively; but when W'l is initialized at A2 and 122 at B2, $1 will be located around the middle point of the two clusters, while Gz remains unmoved as a dead unit. (c) A typical result of using the conscience strategy and FSCL when the number k of used units is larger than the number of clusters in a data set, where the two extra units have also been moved to some boundary points between different clusters, and may confuse quite a large portion of samples from different clusters. u1 = 1 and u2 = 0 during the implementation of the above algorithm. Thus $1 is always moving and eventually oscillates around the mean vector of the two clusters of samples, while dz is always the loser and remains fixed during the whole learning process. Such a loser unit is usually called a dead unit.</p><p>Unfortunately, the situation of dead units happens quite often for the above simple CL algorithm unless the weight vectors are initialized near the cluster centers which are the eventual points of attraction. As mentioned previously in Section I, many efforts have been made to solve the problem. An often used strategy, sometimes called conscience, is to reduce the winning rate of the frequent winners. For example, in <ref type="bibr">Fig. l(b)</ref>, when GI initialized at A2 wins a certain number of times, we force u1 = 0 and u2 = 1 to bring G2 according to (lb) towards its point of attraction. By doing so, G2 will be gradually brought towards the right-side cluster while GI keeps oscillating between the two clusters. Finally W; will stop oscillating and the two weight vectors will converge to the two cluster centers. The so called FSCL <ref type="bibr">[19]</ref> is a recent algorithm using this strategy. It is a straightforward extension of CL, obtained by modifying (la) into the following.</p><formula xml:id="formula_0">-ycllS -Gc112 = min, y, 1 1 2 -G, 112, 1, 0, otherwise. if i = c such that u, = [ (2) k</formula><p>where y, = n,/ n, and n, is the cumulative number of the occurrences of U , = 1.</p><p>The conscience strategy and FSCL do solve the problem of dead units well. Unfortunately, they also bring a new problem. As shown in <ref type="bibr">Fig. l(c)</ref>, when the number k of units used in a CL net is larger than the number of clusters in the input data set, all the k weight vectors will be finally moved to some places in the data set and some weight vectors (at least one) will no longer be located at the centers of the clusters but either at some boundary points between different clusters or at points biased from some cluster centers (the phenomenon will be further verified by experiments shown in the next section).</p><p>The new problem is quite crucial, since it raises some serious problems for the CL applications. For example, some problems are as follows.</p><p>1) For an unsupervised classification application, we expect results like the ones shown in Fig. <ref type="figure">l(a)</ref>, where we obtained three weight vectors located at the mean vectors of the three classes or clusters. In this case, the recognition rate will be the highest when all the samples are classified by distance classifiers based on the three weight vectors. However, in the case shown in Fig. <ref type="figure">l(c</ref>), the recognition rate will significantly decrease since there are two disturbing units located at the boundary points of different clusters and the two units will draw quite a large portion of samples from three of the four clusters to form two mixture groups which give an incorrect clustering result.</p><p>2) For the application of training the hidden units of a RBF net, we would like to expect that in Fig. <ref type="figure">l(c</ref>) only four weight vectors of the hidden units will be located at the centers of the four clusters, while the weights of the other units should be driven away from the input data set so that the linear units in the output layer of the RBF net become capable to perform further classifications. But the results presented in Fig. <ref type="figure">l(c</ref>) show two hidden units becoming disturbing units which may confuse quite a large portion of input samples from different clusters. In the case of supervised learning, a class may consist of more than one cluster, so there may be no problem if the confused clusters belong to the same class. However, if the confused clusters do not belong to the same class, the two disturbing units will not only increase the difficulty of learning for the units in the output layer, but will also reduce the recognition rate considerably since the linear output units may not be able to separate the samples confused by the two disturbing hidden units.</p><p>The above addressed problem may not notably deteriorate the performance of CL on an application of vector quantization, where the goal is not to find any clusters or classes. However, even in this case the input data density is usually not uniform, and results like the one shown in Fig. <ref type="figure">l</ref>(a) are desired. For the codebook we would now have three weight vectors, each located at the center of one of the clusters. Results like that shown in Fig. <ref type="figure">l(c</ref>) will increase the number of vectors in the codebook, without much improvement on the coding performance due to the fact that the code vectors which are located at the boundary points between clusters can contribute only a little in reducing the distortion.</p><p>Therefore, we see that the conscience strategy and FSCL work well only when the number of clusters in the input data set is known in advance so that we can let our CL net have the same number of units. This is not an easy task since we usually do not know the number of clusters in the input data a priori. The same problem exists in the conventional k-means clustering method: if the number of clusters k is selected inappropriately, we may obtain very poor clustering results. Unfortunately, the selection of k is a hard problem. It could only be solved heuristically by some prior knowledge, or by enumerating a number of different values and doing clustering for each of these values so that a better value could be obtained according to some rule, e.g., finding the value with a sharp change on the curve of the average least square error versus the values of IC <ref type="bibr">[l]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">RIVAL PENALIZED COMPETITIVE LEARNING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Algorithm</head><p>For tackling the crucial problem described above, we propose here an new version of CL-RPCL, by adding a new mechanism into FSCL. The basic idea is that for each input not only the weight vector d, of the unit which wins the competition is modified to adapt to the input, but also the weight vector G,. of its rival (i.e., the second winner) is delearned by a learning rate smaller than that used by Gc. Specifically, we modify the algorithm given in the beginning of Section I1 into the following one.</p><p>Step 1: Randomly take a sample S from a data set D, and Step 2: Update the weight vector G i by</p><formula xml:id="formula_1">for i = l , . . . , k , let 1, if i = c such that -1, if i = T such that 0, otherwise. ycllZ -$ , [ I 2 = minj yjllS -Gj1I2, yrllS- = minj+ yjllS-tZjj1I2, (<label>3 4</label></formula><formula xml:id="formula_2">ac(Z-&amp;), if U* = 1 otherwise. Gi), if ui = -1,<label>(3b)</label></formula><p>where 0 5 a,, a,. 5 1 are the learning rates for the winner and rival unit, respectively. In practice they may depend on time and usually at each iteration step t it holds a,(t) &gt;&gt; a,(t).</p><p>Moreover, -yj = n j / E:=, ni is the same parameter as that in the FSCL algorithm introduced earlier.</p><p>As shown in Fig. <ref type="figure" target="#fig_1">2</ref>(a), the rival penalized mechanism tries to push its rival far away from the cluster towards which the winner is moving, thus implicitly producing a force which attempts to make sure that each cluster is learned by only one weight vector. This force is just a balance to the force generated by the conscience strategy of FSCL, which encourages both weight vectors to share one cluster. This balancing role can be more clearly seen from Fig. <ref type="figure" target="#fig_1">2@</ref>). Assuming that three weight vectors have already been brought somewhere between two classes, the rival penalized force will gradually drive away the weight vector 5 3 along a zig-zag path as the input samples come randomly and alternatively from both classes. Similarly, we can also imagine that the two disturbing units given in Fig. <ref type="figure">l(c</ref>) can be driven away by this force.</p><p>So, we see that the key point of using the rival penalized mechanism is that the appropriate number of units will be selected automatically for representing an input data set by gradually driving extra units far away from the distribution of data set in the case that the number of units in a competitive learning net is larger than the number of clusters in the input data set. Thus the crucial problem described in Section I can be tackled. In addition, the extra units become now spare units which are ready to learn some new clusters if some additional data are input in the future.</p><p>Another important point is that the rival penalized mechanism may sometimes speed up the learning process: as shown in Fig. <ref type="figure" target="#fig_1">2(c</ref>), the de-learning of t i &amp; caused by the learning of $1, G2 will push $3 toward its correct cluster. It may be also interesting to note that the basic idea of RPCL can be regarded as a kind of unsupervised extension of Kohonen's supervised learning vector quantization algorithm LVQ2 <ref type="bibr">[15]</ref> which can give a result closer in effect to Bayes decision theory by simultaneously modifying the weight vectors of both the winner and its rival when the winner is in a wrong class but the rival is in a correct class for an input vector.</p><p>In the sequel, the characteristics of RPCL will be further illustrated through some experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Simulation Results</head><p>The data set used here is given in Fig. <ref type="figure" target="#fig_2">3</ref>. There are four clusters of samples, and each cluster has 100 samples from four Gaussian distributions with variance 0.1 and centered at (-l,O), (l,O), (O,l), (0,-1), marked by an asterisk. At each learning step, one sample 3 is randomly selected from the four clusters with anyone of 400 samples being chosen with equal probability. For simplicity, in all the experiments below, we fixed the learning rates at a, = 0.05, a,. = 0.002, although it is usually assumed that better results may be obtained by some specific schedule for changing the rates like that used by the Robbins-Monro stochastic approximation procedure <ref type="bibr">[21]</ref>. In addition, we always initialize the weight vectors by random numbers in the interval between 3.0 and 4.0.</p><p>First, we choose the number of units in our CL net as 4, the same as the number of clusters in the data set. Fig. <ref type="figure">4</ref>(a) shows the learning traces (i.e., trajectories of weight vectors during the learning process) obtained by the classical CL algorithm. Obviously there are three dead units, and only the weight vector d4 of one unit quickly moves towards a cluster center point (0,O) in less than 50 learning steps and then oscillates around it, which can be observed from the learning curves (i.e., the changes of the component variables in every weight vector versus the learning steps) given in Fig. <ref type="figure">4(b)</ref>.</p><p>The fluctuations are due to the fact that the learning rate a, is fixed at 0.05. If the rate is gradually reduced to zero, then the fluctuations will vanish.</p><p>The results obtained by FSCL are given in Figs. <ref type="figure">4(c</ref>) and (d). It can be seen that after about 160 learning steps (see the corresponding learning curves in Fig. <ref type="figure">4(d)</ref>) each of the four weight vectors has smoothly moved into one of the four cluster centers. This result again confirms that FSCL solves the "dead unit" problem <ref type="bibr">[19]</ref>.</p><p>Fig. <ref type="figure">4</ref>(e) shows the learning traces obtained by using our RPCL. Comparing it with Fig. <ref type="figure">4</ref>(c), we see that the performances of RPCL and FSCL are almost the same in this case. Moreover, the learning curves of RPCL are also almost the same as those in Fig. <ref type="figure">4</ref>(d), thus we omit them here.</p><p>Second, we choose the number of units in our CL net larger than the number of clusters in the data set. Fig. <ref type="figure">5</ref>(a) and (b) gives the learning traces (curves) obtained by FSCL with five units in the CL net. As argued in Section 11, there now occurs the problem that all the five units are brought among the four clusters (it will be more clear by comparing the coordinates given in Fig. <ref type="figure" target="#fig_2">3</ref> and Fig. <ref type="figure">5(a)</ref>). In addition to the four units located around the four cluster centers (-1,O) (l,O), (0,-l), (O,l), there is also one disturbing unit located around (-0.4, 0. 4) -a boundary point between the two clusters centered at (-l,O), (0,l). Moreover, due to the effect of the disturbing unit, the other two units, although located around the cluster centers (-1,O) and (O,l), are somewhat biased from these points. As expected, such problems are absent in Fig. <ref type="figure">5(c),</ref><ref type="figure"></ref> showing the results obtained by RPCL. There are only four units moved to the four cluster centers, while the extra unit has been dragged towards the data points only for a while, and then the rival penalized mechanism has driven it back and far away from the four clusters as a spare unit. This phenomenon can also be clearly observed from the dashed learning curve of d2 in Fig. <ref type="figure">5(d)</ref>.</p><p>The advantage of RPCL over FSCL can also be observed in Fig. <ref type="figure" target="#fig_7">6</ref>(a) and (b), showing results obtained with six units in the CL networks. Fig. <ref type="figure" target="#fig_7">6</ref>(a) shows the learning traces obtained by FSCL. Again, all the six units were brought among the four clusters, with four units located around the centers of the clusters, one disturbing unit at a boundary point between the two clusters centered at (-l,O), (OJ), and the other disturbing unit near (0.2,-1). Fig. <ref type="figure" target="#fig_7">6(b</ref>) is the result obtained by RPCL. Similar to the case in Fig. <ref type="figure">5</ref>(c), again only four units moved to each of the four cluster centers, while the two extra units moved towards the clusters only for a while, and then were driven back far away as spare units.</p><p>Furthermore, by comparing Fig. <ref type="figure">5</ref>(b) and (d), one can observe that the learning curves of FSCL became nearly stabilized after about 150 steps, while those of RPCL became near stabilized only after 80 steps. So as we mentioned earlier, the rival penalized mechanism introduced some speedup to the learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iv. COMPARISONS ON THE PERFORMANCES OF RPCL AND FSCL FOR UNSUPERVISED AND SUPERVISED CLASSIFICATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Unsupervised Classification</head><p>Given a set of data consisting of unlabeled samples from several classes, the task of unsupervised classification (or clustering analysis) is to label every sample in the same   if i = argminjllZ-Gill2, then label 2 by index i. (4)</p><p>As a result, all the samples with the same index i constitute a class, and the number of indices is just the number of classes contained in the data set. Each index has been assigned to at least one sample (or at least m samples, m being a predefined number; samples less than this number are regarded as forming a cluster of noise points).</p><p>In the following experiments, the data set is again the same as in Fig. <ref type="figure" target="#fig_2">3</ref>. The learning rates Q , , Q ~ are also the same as those used in the experiments of Fig. <ref type="figure">4(c)-(e</ref>). In the sequel, for convenience we denote the classes with centers located at (0,-l), (O,l), &amp;O), and (-1,O) by class 1, 2, 3, and 4, respectively.</p><p>We first consider a simpler case of IC = 4. The classification results obtained by FSCL and RPCL are given in Tables <ref type="table" target="#tab_0">I</ref> and<ref type="table">11</ref>, respectively.</p><p>It is seen from the tables that both FSCL and RPCL perform similarly in this case. A recognition rate of about 97% is obtained. This rate is already very good since the four classes in Fig. <ref type="figure" target="#fig_2">3</ref> are not separated and one cannot expect a zero error rate. This suggests that FSCL can work equally well as RPCL if we know the number of classes in the data set in advance.</p><p>Second we consider the cases of k larger than 4. Tables <ref type="table" target="#tab_0">I11</ref> and IV present the classification results obtained when IC = 5, and Tables V and VI show the results obtained when IC = 6. Again, the learning processes of FSCL and RPCL are similar to those given in Fig. <ref type="figure">S(a)-(d</ref>) and Fig. <ref type="figure" target="#fig_7">6</ref>(a) and (b), respectively.</p><p>It can be seen from the above tables that now the performances of FSCL and RPCL are significantly different. RPCL can still obtain recognition rates of (96 + 98 + 97 + 97)/400 = 97% when b = 5 and (98 + 98 + 98 + 95)/400 = 97.25% when k = 6, which are as good as in the case when k = 4.</p><p>However, the recognition rates obtained by FSCL have been considerably reduced to (96 + 69 + 85 + 93)/400 = 85.75% when IC = 5 and (54 + 75 + 97 + 84)/400 = 77.5% when IC = 6, due to the influences of the disturbing units. By using FSCL, it follows from Table <ref type="table" target="#tab_2">111 that</ref>       Thus we see that for the case when k is larger than the number of classes in the data set, the performance of the FSCL deteriorates, while the RPCL is able to maintain a good performance. Furthermore, RPCL can automatically find the number of classes in the data set (e.g., in both Tables <ref type="table" target="#tab_0">IV</ref> and<ref type="table" target="#tab_3">VI</ref>, there are only four clusters obtained by four units) and leave the extra units unused as spare units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Supervised Classification through RBF Net</head><p>The RBF network has recently become popular in supervised classification due to its good performance and fast training [4]-[6]. An RBF net consists of one hidden layer of units with the Gaussian radial basis activation functions given by and an output layer of linear units given by oi = w:jgj, where d: = [w,ol, . -, w i l t is the weight vector of the ith  A competitive learning algorithm, especially FSCL, has the same function as the k-means algorithm, and thus can take the same role in training the RBF net. However, here we should point out that FSCL as well as the k-means algorithm work well only when the hidden unit number k is appropriately selected to be the same as the number of clusters in the input data set, otherwise the performance of the RBF net will deteriorate considerably. The problem is again that one usually does not know the number of clusters in the data set in advance.</p><p>In the sequel, we will show through the results of experiments how this problem affects the recognition performance of FSCL, and how RPCL can automatically solve the problem so that good performances are insensitive to the selection of k.</p><p>The data set is given in Fig. <ref type="figure">7</ref>. It consists of two classes which have in total four clusters which are produced in the same way as the data shown in Fig. <ref type="figure" target="#fig_2">3</ref>. However, here two clusters form a class. Specifically, the two clusters centered at (O,l), (0,l) form class 1, and the the two centered at (l,O), (-1,O) form class 2. In fact, we confront a "noisy" XOR problem. Tko sets of such data are generated. One is used as a training set which consists of 400 samples. They are the same as those in Fig. <ref type="figure" target="#fig_2">3</ref>, with each cluster having 100 samples and thus each class having 200 samples. The other is used as a testing set, the 400 samples of which come from the same four Gaussian distributions as the training set, but they are  The first step of training, i.e., locating the weight vectors of hidden RBF units, is performed in the same way as we did earlier in the case of unsupervised classification. Then these trained weight vectors are fixed, and we train the weight vectors of the top linear units by (5b). The Gaussian activation functions g3 of (5a) are calculated under the parameter a2 = 0.1. The same parameter is used in the four Gaussian distributions when we generate the data set. Again, we first record the recognition results obtained in the case k = 4, in the following Tables <ref type="table" target="#tab_3">VI1</ref> and<ref type="table" target="#tab_3">VIII</ref>.</p><p>We see that FSCL and RPCL work similarly well in this case, with a recognition rate around 94% and an error rate below 3% for both the training and testing sets.</p><p>However, we find that the performance changes considerably in the cases when the number of hidden units k is larger than 4 or the number of clusters in the data set. Tables <ref type="table" target="#tab_6">IX</ref> and<ref type="table" target="#tab_6">X</ref> present the classification results obtained when k = 5, and Tables <ref type="table" target="#tab_7">XI</ref> and<ref type="table" target="#tab_7">XI1</ref> show the results for k = 6.</p><p>RPCL can still maintain a recognition rate around 93% and an error rate below 3% for both the training and testing sets for both cases k = 5 and Ic = 6. However, the recognition rates obtained by FSCL have been reduced down to around 85% in the cases of k = 5 and k = 6, while the error rate is still around 3%. That is, the performance of FSCL deteriorates considerably.</p><p>Before closing this section, we would like to further note that in fact the extra units could be discarded from the RBF net after the weight vectors of the hidden units have been trained by using RPCL. These extra units can be detected simply by doing unsupervised classification as indicated in the first part of this section. The exclusion of the extra units can save some computation cost of the RBF net. Moreover, it may also help to keep a good generalization ability. By comparing the results given in Tables VIII, X, and XII, we can see that as the number of extra units increases, the generalization ability is slightly reduced although the good recognition rates prevail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CURVE DETECTION BY RPCL</head><p>Detecting curves (straight line, circle, ellipse, etc.) from an image is one of the basic tasks in image processing and machine vision. In the traditional pattern recognition literature, a lot of methods have been proposed for solving this problem. However, the study of using neural network techniques for such problems may cast some new light on this classical but important task. In [22], motivated by an investigation into the learning characteristics of the Kohonen self-organizing map [14], we developed a new neural network technique, a Houghlike technique called Randomized Hough Transform (RHT), for detecting curves. It has been shown [22] that RHT has several advantages over the conventional Hough transform (HT) and can improve the performance of HT significantly. In the sequel, we further study the possibility of using a competitive learning model for the curve detection tasks.</p><p>Let us assume a binary image (which is usually obtained from a gray-level image by some conventional or neural techniques, , e.g., the Canny operator [23] or the ). Let F(a',.') = 0 be the parametric equation of a curve with a' = [ a l , . . . , a, ] a vector of parameters and 2 = <ref type="bibr">[XI,</ref><ref type="bibr">x2]</ref> the coordinates of a pixel in the image.</p><p>If the image contains one curve expressible by the equation, then there must be a number of "white" or "on" pixels &amp;, i = 1, e . , n which all satisfy the equation &amp;'(a', Z;) = 0.</p><p>When n 2 m, the parameter vector a' can often be solved. This problem is usually called a curve fitting problem and it is not difficult to solve. However, if the image contains a number of groups of pixels (e.g., a number of straight line segments or a number of circles) with each group lying on its own curve expressible by the parametric equation F ( a',.') = 0, with the parameter a' differing from curve to curve, we need to classify every pixel into one of the groups. Thus it is a type of clustering analysis problem. However the clusters here are more complex than what we have studied in the earlier section.</p><p>Let's try to express the problem as a competitive learning problem. Assume we have a number k of neural units with their outputs denoted by ui, i = 1, . . . , k and weight vectors by a';, i = 1, . . . , IC. For an input pixel 2, we let the output of each unit be given by Now each unit stands for one possible parametric curve in the image. Our basic idea is to use competitive learning to modify the weight vectors of these units so that after training the output of each unit can selectively become the minimum for the pixels drawn from a specific curve. Then the weight vectors of the units would directly give estimates for the parameter vectors of the curves. The implementation of this idea is quite simple-just the repeated use of the following two steps.</p><p>Step 1: Randomly take a pixel . ' from D, compute ui, i = 1, . . . , IC by (6a) and then find the index c with U, = mini U;.</p><p>Step 2: Update the weight vector a', by ZC := ZC -There D is a set containing all the "on" pixels of the image, a , is the learning rate, and each a'i is initialized randomly.</p><p>This simple procedure is just what was proposed earlier by two of the authors in [23]. Several further improvements are possible as suggested in the following. ac(a€,2/aa',). 0 First, E; given in (6a) can be replaced by where Ir;I is the orthogonal distance from 2 to the curve, yielding a better error criterion than E ; . Minimization of E: actually corresponds to the minimization of the conventional least square (LS) fitting errors, while the minimization of r: corresponds to the minimization of the so called total least square (TLS) fitting errors [20]. We have shown that the results of curve fitting obtained from TLS are considerably better than those from LS [20].</p><p>0 Second, FSCL can be used to replace the simple competitive learning, i.e., we can replace the above two learning steps by the following.</p><p>Step 1: Randomly take a pixel 2 from D, compute U ; = r: by (6b) and then find the index c with ycuc = minj ~j u j .</p><p>Here yj = n j / ni is the same frequency parameter as the one used for FSCL in the earlier sections of this paper.</p><p>Step 2: Update the weight vector a', by 2, := a',ac(ar,2/aZc). 0 Third, it is better to use RPCL to replace FSCL, i.e.,</p><p>Step 1: Randomly take a pixel Z from D, compute ui = r; by (6b) and then find the index c with ycu, = minj y j u j and index r with yTur = minjZcyjuj. Step 2: Update the weight vectors Z,, 8,. by Z, := 2,a!,dr:/dZ, and 2,. := 2,. + a,dr:/dZ,..</p><p>That is, the winner U, is awarded using a learning rate aC, and the rival U,. of U, is penalized using a smaller learning rate a!,. &lt; &lt; a!,.</p><p>In the above, the curve is considered in its general form F(Z,?) = 0. For a specific curve, the learning rules can be written more precisely. For example, for detecting a line $2 + c = 0, we have replace it by and let us replace 12,. := Z,. + a!,dr:/dZ,. in RPCL by Now the scalar factor in (7a) has been replaced by 1/11Z11. Mathematically, this is equivalent to redefining the learning rates a!, and a,.. The reason is to let the learning rate be adjusted according to input 3, resulting in more stable learning under noise. The details are similar to those given in [20].</p><p>In the sequel, we compare the performances of FSCL and RPCL on a four-line detection problem through several experiments.</p><p>As shown in Fig. <ref type="figure" target="#fig_9">8</ref>, our data consists of the "on" pixels (in this case, black pixels) in an image containing four line segments which form a diamond. The theoretical equations of the four lines are L1 : 2 1 -2 2 = 1, L2 : 2 1 + 2 2 = 1, L3 : -2 1 + 2 2 = 1, L4 : -21 -2 2 = 1, and each line segment in the actual digital binary image consists of 101 pixels. Now the problem is to detect the two parameters of each line under the line equation F ( Z , 2 ) = alxl + a 2 2 2 -1 = 0 in an unsupervised manner, using only the available pixels. In all the experiments, we let for simplicity the number of neural units be IC = 4, which is the same as the number of line segments. The weight vectors are all initialized by random numbers between [0,1]. The learning rates are a!, = 0.01, a, = 0.001.</p><p>Fig. <ref type="figure">9</ref>(a) and (b) shows the learning traces and learning curves for the four weight vectors obtained by using the classical CL. The result is very poor although the learning is quickly stabilized (see Fig. <ref type="figure">9(b)</ref>). Now only one vector $2 reached a correct point (l,l), which is the parameter pair of line L2. Two other vectors converged to wrong points, and another vector was simply dead from the beginning.</p><p>Fig. <ref type="bibr">lO(a)</ref> and (b) give results obtained by FSCL. Now, some improvement is obtained. Two vectors $1 and $4 reached two parameter pairs (1,-1) and (-1,l) of lines L1, L3. However, there are still two vector $2, ti&amp; which moved towards the wrong points (-3,3), (3, -3). So, we see that the four-line problem seems much harder than the four-cluster problem given in Fig. <ref type="figure">5</ref>, where FSCL can very easily produce and RPCL when k = 3. The data set used here is still the four clusters given in Fig. <ref type="figure" target="#fig_2">3</ref>, and thus k is too small. It can be seen from Fig. <ref type="figure" target="#fig_12">12</ref>(a) and more clearly from Fig. <ref type="figure" target="#fig_12">12@</ref>) that all the three weight vectors oscillate strongly between different clusters and the learning can not stabilize with the fixed learning rate a, = 0.05. Moreover, if we reduce ac to zero as the learning proceeds, then each of the three weight vectors will be finally stabilized at some point which is either away from a cluster center at a certain distance or is a boundary point of two different clusters, as marked by the small black rectangles in Fig. <ref type="figure" target="#fig_12">12</ref>(a). Clearly, when this kind of learning behavior occurs for unsupervised classification or RBF net, the performance will not be good, since each of three weight vectors will draw samples from two different classes. Thus, like the k-means algorithm, both FSCL and RPCL will work poorly if k is too small. Fortunately, it is quite easy for RPCL to avoid such situations, e.g., just by the simple method of choosing a large number as k such that it is surely larger than the number N, of clusters in the data set. This may not be desirable, however, since a large k &gt;&gt; N, will increase the training time.</p><p>A better way is the following: we can start with a small number or a guess for k and perform unsupervised classification as was done in Section IV. By checking whether there are spare units left, we would know if k &gt; N,. If yes, then take the present results as the solutions; if not, increase k by adding a positive increment, or double it and then perform unsupervised classification again. The same procedure can be repeated until k &gt; N,. Actually, few iterations are needed for a moderately large number as the starting value of k, especially when there are not too many clusters in the data set.</p><p>VII. CONCLUSIONS Similar to the problem of selecting an appropriate k in the classical k-means algorithm, the present Competitive Learning algorithms have also a similar crucial problem: the selection of an appropriate number of neural units. It has been shown that FSCL significantly deteriorates its performance when the number of units is inappropriately chosen. A new algorithm called RPCL has been developed from the basic idea that for each input not only the winner unit is modified to adapt to the input, but also its rival is de-learned by a smaller learning rate. RPCL can automatically allocate an appropriate number of units for an input data set. The experimental results have shown that RPCL outperforms FSCL when they are used for unsupervised classification, for training a RBF network, and for curve detection in digital images. Erkki Oja  was born in Helsinki, Finland, in 1948. He received the M.Sc. degree and the Dr.Tech. degree (with distinction) from the Helsinki University of <ref type="bibr">Technology in 1972 and</ref><ref type="bibr">1977, respectively.</ref> He was a visiting scientist at Brown Univeristy in <ref type="bibr">1977</ref><ref type="bibr">-1978</ref><ref type="bibr">, and at the Tokyo Institute of Technology, Japan, in 1983</ref><ref type="bibr">-1984</ref><ref type="bibr">. For the academic year 1990</ref><ref type="bibr">-1991, he</ref>  Adam Knyiak (M'86), biography not available at the time of publication.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig. 1. A crucial problem for competitive learning. (a) The desired result of using CL: each of weight vectors moved to a cluster center regardless of the initial values of these weight vectors. (b) The actual result highly depends on</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Rival penalized competitive learning. (a)The rival is pushed away from the cluster that the winner is learning. @) The rival 4 of both $1 and G2 is driven out along a zig-zag path. (c) Learning of G3 is faster because it is pushed towards its correct cluster by the two rivals $1 and ti$.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The data set used in the experiments. Each of four clusters has 100 samples from four Gaussian distributions with variance 0.1 and centered at (-l,O), (l,O), (OJ), (0,-l), marked by an asterisk.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Experimental results obtained in the case IC = 4, i.e., the number of units is equal to the number of clusters in the data. (a) The learning traces, i.e., trajectories of weight vectors during the learning process, obtained by the classical CL algorithm. In this figure, as well as in all the following figures showing learning traces, each of the target locations is marked by an asterisk (in the present figure, only two of the four target locations appear). (b) The learning curves, i.e., the changes of the component variables in every weight vector versus the learning step in the classical CL algorithms. (c) The learning traces obtained by FSCL. (d) The learning curves obtained by FSCL. (e) The learning traces obtained by RPCL.class by the same symbol such that the data set is divided into several clusters (classes) each associated with a different symbol. The task is harder than supervised classification for two reasons: we have no training samples with known labels, and usually we have no knowledge of the number of classes contained in the data set. In the sequel, we compare the performances of RPCL and FSCL in carrying out the task of unsupervised classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>unit number 4 becomes a disturbing unit which forms a mixture group with 48 samples FSCL. @) The learning traces obtained by RPCL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Experimental results obtained in the case k = 6 &gt; 4. (a) The learning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Fig. 7. The data set used for testing a RBF net. There are clusters which are produced in the same way as the data shown in Fig. 3. The two clusters centered at (O,l), (0,l) form class 1, and the two centered at (l,O), (-1,O) form class 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The data set used for testing curve detection by FSCL and RPCL. It consists of pixels lying on four line segments which constitute the diamond shape.ThefourlinesareL1 : 11-zz = 1, L 2 : 21+12 = 1, L 3 : -zi+zz = 1, L4 : -11 -12 = 1, and on each line there are 101 pixels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Zc := Zca!cdT,"/aZc in both FSCL and RPCL could be computed by direct substitution. However, let us Fig. 9. The results obtained by the classical CL with k = 4. (a) The learning traces. (h) The learning curves. Only one curve was detected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. The results obtained by FSCL with k = 4. (a) The learning traces, @I) The learning curves. Two curves were detected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Experimental results by both FSCL and RPCL on the data set given in Fig. 3, with k = 3. In this case, all three weight vectors oscillate strongly between different clusters and the learning cannot stabilize with the fixed learning rate a, = 0.05. (a) The learning traces, @) The learning curves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>both in pattern recognition and signal processing, from TsinghuaUniversity in 1984 and 1987, respectively,  From June 1987  to June 1988, he was a postdoctoral fellow at he Department of Mathematics, Peking University, where he is currently an Associate Professor. He visited the Department of Information Technology, Lappeenranta University of Technology, Finland, and the Department of Computer Science, Concordia University, Canada, as a Senior Researcher and Research Associate, each for one year, respectively. From September 1991 to September 1992, he was a Visiting Scientist in the Division of Applied Sciences, Harvard University. From September 1992 to May 1993, he was a postdoctoral Research Associate in the Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology. He has published several dozen papers in the areas of neural networks, computer vision/pattern recognition, artificial intelligence, and signal processing. He has served as a reviewer for a number of international journals and conferences on neural networks, and as a Guest Editor of a special issue of the Journal of Artificial Neural Networks and as the organizer of one of the NIPS92 Postconference Workshops. Dr. Xu was one of the winners of the First Fok Ying Tung Education Foundation Prize for young university teachers of the People's Republic of China, 1988. He was also the second of the winners of the 2nd Beijing Young Scientists Prize awarded by the Beijing Association for Science and Technology in 1988, and the first of the winners of the excellent paper award for young researchers in the 1988 National Conference of the Chinese Automation Society, May 1988.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>held the Toshiba Visiting Professor's Chair at the Department of Computer Science, Tokyo Institute of Technology. Since. 1987, he has been Professor of Computer Science at the Lappeenranta University of Technology, Finland. He is the author of a number of journal papers and book chapters on pattern recognition, computer vision, and neural computing, and the book Subspace Methods of Pattern Recognition, which has been translated into Chinese and Japanese. His present research interests are in applying neural networks to computer vision and the study of subspace, PCA, and self-organizing networks. He has lectured on neural computation and pattern recognition in universities, and for industry, in Europe and Japan. Dr. Oja has served in the scientific and organization committees of a number of conferences, recently including 1°C Pans '90, Cognitiva '90, ICANN '91, IJCNN '91, ECAI '91, ICPR '92, IEEE ICNN '93, WCNN '93, and IJCNN '93. He was a General Chairman of the 6th Scandinavian Conference on Image Analysis in 1989. He is a member of ACM, INNS, ENNS, Finnish Academy of Sciences, and the Finnish Academy of Technical Sciences. He is a past Chairman of the Finnish Pattern Recognition Society, past Vice President of the IEEE Finland Section, a member of the Governing Board of the International Association of Pattern Recognition , and a member of the editorial boards of the International Journal of Neural Systems, Neural Networks, Neural Computation, and the IEEE TRANSACTIONS ON NEURAL NETWORKS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I THE</head><label>I</label><figDesc>CONFUSION MATRIX OBTAINED BY FSCL WHEN k = 4.</figDesc><table><row><cell></cell><cell>unit1</cell><cell>unit2</cell><cell>unit3</cell><cell>unit4</cell></row><row><cell>class 1</cell><cell>0</cell><cell>1</cell><cell>3</cell><cell>96</cell></row><row><cell>class 2</cell><cell>98</cell><cell>1</cell><cell>1</cell><cell>0</cell></row><row><cell>class 3</cell><cell>1</cell><cell>97</cell><cell>0</cell><cell>2</cell></row><row><cell>class 4</cell><cell>2</cell><cell>0</cell><cell>97</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 111 THE</head><label>111</label><figDesc>CONFUSION MATRIX OBTAINED BY FSCL WHEN k = 5</figDesc><table><row><cell></cell><cell>unit,</cell><cell>unit?</cell><cell cols="2">unit-</cell><cell>unit"</cell><cell>unit&lt;</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>~~</cell><cell></cell></row><row><cell>class 1</cell><cell>0</cell><cell>0</cell><cell>3</cell><cell></cell><cell>1</cell><cell>96</cell></row><row><cell>class 2</cell><cell>69</cell><cell>0</cell><cell>1</cell><cell></cell><cell>30</cell><cell>0</cell></row><row><cell>class 3</cell><cell>1</cell><cell>85</cell><cell>0</cell><cell></cell><cell>12</cell><cell>2</cell></row><row><cell>class 4</cell><cell>1</cell><cell>0</cell><cell>93</cell><cell></cell><cell>5</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE IV</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">THE CONFUSION MAW OBTAINED BY RPCL WHEN k = 5</cell><cell></cell></row><row><cell></cell><cell>unit]</cell><cell>unit2</cell><cell cols="2">unit3</cell><cell>unit4</cell><cell>units</cell></row><row><cell>class 1</cell><cell>0</cell><cell>96</cell><cell>3</cell><cell></cell><cell>0</cell><cell>1</cell></row><row><cell>class 2</cell><cell>98</cell><cell>0</cell><cell>1</cell><cell></cell><cell>0</cell><cell>1</cell></row><row><cell>class 3</cell><cell>1</cell><cell>2</cell><cell>0</cell><cell></cell><cell>0</cell><cell>97</cell></row><row><cell>class 4</cell><cell>2</cell><cell>1</cell><cell>97</cell><cell></cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE V</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">THE CONFUSION MATRIX OBTAINED BY FSCL WHEN k = 6</cell></row><row><cell></cell><cell>unit1</cell><cell>unit2</cell><cell>unit3</cell><cell>unit4</cell><cell>units</cell><cell>units</cell></row><row><cell>class 1</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>45</cell><cell>54</cell><cell>0</cell></row><row><cell>class 2</cell><cell>0</cell><cell>1</cell><cell>75</cell><cell>0</cell><cell>0</cell><cell>24</cell></row><row><cell>class 3</cell><cell>0</cell><cell>97</cell><cell>1</cell><cell>0</cell><cell>2</cell><cell>0</cell></row><row><cell>class 4</cell><cell>84</cell><cell>0</cell><cell>0</cell><cell>4</cell><cell>0</cell><cell>12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE VI THE</head><label>VI</label><figDesc>CONFUSION M.mux OBTAINED BY RPCL WHEN k = 6</figDesc><table><row><cell></cell><cell>unit]</cell><cell>unit2</cell><cell>unit3</cell><cell>unit4</cell><cell>units</cell><cell>units</cell></row><row><cell>class 1</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>98</cell><cell>1</cell><cell>0</cell></row><row><cell>class 2</cell><cell>1</cell><cell>0</cell><cell>98</cell><cell>1</cell><cell>1</cell><cell>0</cell></row><row><cell>class 3</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>98</cell><cell>98</cell><cell>0</cell></row><row><cell>class 4</cell><cell>95</cell><cell>0</cell><cell>2</cell><cell>0</cell><cell>0</cell><cell>0</cell></row></table><note><p>driven far away from the other four units. It is just a spare unit which draws none of the samples. Similarly, units 2 and 6 in Table VI also become spare units.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IX THE CONFUSION MATRIX OBTAINED BY FSCL WHEN k = 5</head><label>IX</label><figDesc></figDesc><table><row><cell></cell><cell>Training</cell><cell></cell><cell cols="2">Set Testing</cell><cell></cell><cell>Set</cell></row><row><cell></cell><cell>class 1</cell><cell>class 2</cell><cell>reiect</cell><cell>class 1</cell><cell>class 2</cell><cell>reiect</cell></row><row><cell>class 1</cell><cell>174</cell><cell>4</cell><cell>22</cell><cell>167</cell><cell>2</cell><cell>31</cell></row><row><cell>class 2</cell><cell>16</cell><cell>170</cell><cell>14</cell><cell>13</cell><cell>170</cell><cell>17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE X THE CONFUSION MATRIX OBTAINED BY RPCL WHEN</head><label>X</label><figDesc></figDesc><table><row><cell></cell><cell>Trainine</cell><cell></cell><cell>Set</cell><cell>Testing</cell><cell></cell><cell>Set</cell></row><row><cell></cell><cell>class 1</cell><cell>class 2</cell><cell>reiect</cell><cell>class 1</cell><cell>class 2</cell><cell>reiect</cell></row><row><cell>class 1</cell><cell>189</cell><cell>6</cell><cell>5</cell><cell>189</cell><cell>5</cell><cell>6</cell></row><row><cell>class 2</cell><cell>6</cell><cell>187</cell><cell>7</cell><cell>5</cell><cell>190</cell><cell>5</cell></row><row><cell cols="7">obtained by different random samplings. Again, each cluster</cell></row><row><cell cols="7">has 100 samples, and thus each class has 200 samples.</cell></row></table><note><p>k = 5</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE XI THE CONFUSION MATRIX OBTAINED BY FSCL WHEN k = 6</head><label>XI</label><figDesc></figDesc><table><row><cell></cell><cell>Trainine</cell><cell></cell><cell>Set</cell><cell>Testine</cell><cell></cell><cell>Set</cell></row><row><cell></cell><cell>class 1</cell><cell>class 2</cell><cell>reject</cell><cell>class 1</cell><cell>class 2</cell><cell>reject</cell></row><row><cell>class 1</cell><cell>171</cell><cell>3</cell><cell>26</cell><cell>171</cell><cell>1</cell><cell>28</cell></row><row><cell>class 2</cell><cell>10</cell><cell>170</cell><cell>20</cell><cell>12</cell><cell>172</cell><cell>17</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TABLE XI1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">THE CONFUSION MATRIX OBTAINED BY RPCL WHEN k = 6</cell><cell></cell></row><row><cell></cell><cell>Training</cell><cell></cell><cell>Set</cell><cell>Testing</cell><cell></cell><cell>Set</cell></row><row><cell></cell><cell>class 1</cell><cell>class 2</cell><cell>reject</cell><cell>class 1</cell><cell>class 2</cell><cell>reject</cell></row><row><cell>class 1</cell><cell>190</cell><cell>4</cell><cell>6</cell><cell>185</cell><cell>6</cell><cell>9</cell></row><row><cell>class 2</cell><cell>7</cell><cell>189</cell><cell>4</cell><cell>3</cell><cell>185</cell><cell>12</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_0"><p>' I   </p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Devijver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<title level="m">Pattern Recognition: A Statistical Approach</title>
		<meeting><address><addrLine>London, U.K.</addrLine></address></meeting>
		<imprint>
			<publisher>prentice-Hall</publisher>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vector quantization in speech coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Makhoul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rpucos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ZEEE, vo1</title>
		<meeting>ZEEE, vo1</meeting>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="1551" to="1558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast leaming in networks of locally tuned processing units</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nasrabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="281" to="294" />
			<date type="published" when="1988">1988. 1989</date>
		</imprint>
	</monogr>
	<note>Neural Computation</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Regularization algorithms for learning that are equivalent to multilayer networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Girosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing System</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Lippmann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Moody</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Touretzky</surname></persName>
		</editor>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufman</publisher>
			<date type="published" when="1990">1990. 1991</date>
			<biblScope unit="volume">247</biblScope>
			<biblScope unit="page" from="774" to="780" />
		</imprint>
	</monogr>
	<note>Evaluation of adaptive mixtures of competing experts</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Feature discovery by competitive learning</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="75" to="112" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Competitive learning: from iterative activation to adaptive resonance</title>
		<author>
			<persName><forename type="first">S</forename><surname>Grossberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="23" to="63" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Counterpropagation networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hecht-Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Opt</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">49794984</biblScope>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive patten classification and universal recording: I. parallel development and coding of neural feature detectors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Grossberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol. Cybern</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="121" to="134" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A massively parallel architecture for a self-organizing neural pattern recognition machine</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grossberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision, Graphics, and Image Processing</title>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="54" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ART 2: Self-organization of stable category recognition codes for analog output patterns</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grossberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Opt</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ART 3: Hierarchical searching using chemical transmitters in self-organizing pattern recognition architectures</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grossberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="129" to="152" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
		<title level="m">Selforganization and Associative Memory</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The self-organizing map</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="1464" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An introduction to computing with neural nets</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Lippmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ASSP Mag</title>
		<imprint>
			<biblScope unit="page" from="4" to="22" />
			<date type="published" when="1987-04">Apr. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Bienenstock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Munro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neuroscience</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="32" to="48" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adding a conscience to competitive learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Desieno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ZEEE In?. Con$ Neural Networks</title>
		<meeting>ZEEE In?. Con$ Neural Networks</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="117" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Competitive learning algorithms for vector quantization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Ahalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Krishnamurty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Melton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="441" to="457" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A stochastic approximation method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Monro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Stat</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="400" to="407" />
			<date type="published" when="1951">1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kultanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A new curve detection methods: Randomized Hough Transform (RHT)</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="331" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Extended self-organizing map for curve detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research Report</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<date type="published" when="1989-09">Sept. 1989</date>
		</imprint>
		<respStmt>
			<orgName>Dept. Information Technology, Lappeenranta University of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Patt. Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="277" to="291" />
			<date type="published" when="1986">1986. 1987. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Theory of edge detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hildreth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<publisher>Roy. Soc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Palmer</surname></persName>
		</author>
		<title level="m">Introduction to Theory of Neural</title>
		<meeting><address><addrLine>London, B</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1980">1980</date>
			<biblScope unit="volume">207</biblScope>
			<biblScope unit="page" from="187" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Lei Xu received the master&apos;s and doctoral degrees</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Computation</publisher>
			<pubPlace>New York Addison-Wesley</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
