<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Map-Reduce System with an Alternate API for Multi-Core Environments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wei</forename><surname>Jiang</surname></persName>
							<email>jiangwei@cse.ohio-state.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Ohio State University Columbus OH</orgName>
								<address>
									<postCode>43210</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vignesh</forename><forename type="middle">T</forename><surname>Ravi</surname></persName>
							<email>raviv@cse.ohio-state.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Ohio State University Columbus OH</orgName>
								<address>
									<postCode>43210</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gagan</forename><surname>Agrawal</surname></persName>
							<email>agrawal@cse.ohio-state.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Ohio State University Columbus OH</orgName>
								<address>
									<postCode>43210</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Map-Reduce System with an Alternate API for Multi-Core Environments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">73746C51C6822CF226A055621766BA77</idno>
					<idno type="DOI">10.1109/CCGRID.2010.10</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>data-intensive computing</term>
					<term>Multi-Core Architectures</term>
					<term>Middleware</term>
					<term>Map-Reduce</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Map-reduce framework has received a significant attention and is being used for programming both large-scale clusters and multi-core systems. While the high productivity aspect of map-reduce has been well accepted, it is not clear if the API results in efficient implementations for different subclasses of data-intensive applications. In this paper, we present a system MATE (Map-reduce with an AlternaTE API), that provides a high-level, but distinct API. Particularly, our API includes a programmer-managed reduction object, which results in lower memory requirements at runtime for many dataintensive applications. MATE implements this API on top of the Phoenix system, a multi-core map-reduce implementation from Stanford. We evaluate our system using three data mining applications and compare its performance to that of both Phoenix and Hadoop. Our results show that for all the three applications, MATE outperforms Phoenix and Hadoop. Despite achieving good scalability, MATE also maintains the easy-to-use API of mapreduce. Overall, we argue that, our approach, which is based on the generalized reduction structure, provides an alternate highlevel API, leading to more efficient and scalable implementations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Multi-core and many-core environments are becoming ubiquitous. Programmers can no longer rely on existing sequential implementations to run faster simply because of increased clock frequency of processors. This makes writing parallel programs inevitable. To enable parallel programming, a number of paradigms have been developed, with tradeoffs in terms of expressive power, programmer productivity, and efficiency. For example, models like explicit thread programming and message-passing offer highest level of flexibility, but involve a steep learning curve and potentially low programmer productivity.</p><p>An approach that is growing in popularity is based on application-class-specific programming models. Such models can enable both high programmer productivity and provide sufficient expressive power for a particular class of applications, while potentially achieving efficient execution. Mapreduce paradigm, developed by Google, is such an example, targeting data-intensive applications <ref type="bibr" target="#b5">[6]</ref>. Over the last few years, multiple projects have focused on improving the API or implementations <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. At the same time, there are studies focusing on the suitability of map-reduce model for a variety of applications across different platforms <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Map-reduce was originally proposed for large scale clusters and data center environments, but its high-level API has been used for developing dataintensive applications on multi-cores as well <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b17">[18]</ref>.</p><p>While the high productivity aspect of map-reduce API has been very well understood, performance or efficiency has been much less understood. At the same time, the class of data-intensive applications is very broad. It ranges from datacenter type of operations like scanning data, which require very little computation, to machine learning or model building on large datasets, which can involve significant amount of computations. It is not yet clear if the current map-reduce API is best suited for all of the different sub-classes of dataintensive applications. This paper describes support for an alternate API in a map-reduce implementation. This API has been implemented in a system we refer to as Map-reduce with AlternaTE API (MATE). This system has been developed on top of Phoenix, a multi-core implementation of map-reduce <ref type="bibr" target="#b17">[18]</ref>.</p><p>The key distinctive aspect of our API is that it allows the programmers to explicitly declare a reduction object. The map and reduce functions are replaced by a single reduction function that updates such a reduction object, as opposed to emitting (key, value) pairs, and then reducing the values that have the same key. Race conditions can be avoided by replicating the reduction object. A combination function combines different copies of the reduction object to create the final results. The runtime system automatically maps the Reduction/Combination tasks to threads that are spawned on processor cores. Also, the runtime scheduler dynamically partitions the dataset and assigns splits to processor cores. approaches like Pthreads or OpenMP. At the same time, it reduces the memory requirements associated with the large number of (key, value) pairs that can be generated in the original map-reduce API for several applications.</p><p>Our implementation on top of the Phoenix system allows us to carefully evaluate the performance advantages of the new API. We have extensively evaluated our system using three data mining applications and compare it with both Phoenix (with original map-reduce API) and Hadoop on two sharedmemory platforms. Our experimental results show that our system outperforms Phoenix for all the three applications and also leads to reasonable speedups in both environments. Hadoop is much slower than both the systems. Overall, we observe that while map-reduce eases the burden of programmers, its inherent API structure may cause performance losses for some applications due to the rigid two-stage computation style. Our approach, which is based on the generalized reduction, offers an alternate API, potentially useful for several sub-classes of data-intensive applications.</p><p>The rest of the paper is organized as follows. In Section II we give an overview of the map-reduce and generalized reduction, with a case study showing the use of APIs from both MATE and Phoenix. The design and the implementation of our system are discussed in Section III. We report the results from our experimental study in Section IV. We give a brief overview of the related efforts in this area in Section V and conclude in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MAP-REDUCE AND GENERALIZED REDUCTION</head><p>This section describes the map-reduce model and the alternate API implemented in our MATE system, which we will refer to as the generalized reduction API. We will further explain the API usage of the two models using apriori association mining, a popular data mining algorithm <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Map-Reduce Model</head><p>Map-reduce <ref type="bibr" target="#b5">[6]</ref> was created by Google for application development on data-centers with thousands of computing nodes. It is a programming model that enables easy development of applications that process vast amounts of data on large clusters. Through a simple interface of two functions, map and reduce, this model facilitates parallel implementations of many realworld tasks, ranging from data processing for search engine support to machine learning <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>.</p><p>The map-reduce programming model can be summarized as follows <ref type="bibr" target="#b5">[6]</ref>. The computation takes a set of input points, and produces a set of output {key, value} pairs. The user of the map-reduce library expresses the computation as two functions: map and reduce. Map, written by the user, takes an input point and produces a set of intermediate {key, value} pairs. The map-reduce library groups together all intermediate values associated with the same key, and passes them to the reduce function. The reduce function, also written by the user, accepts a key and a set of values associated with that key. It merges together these values to form a possibly smaller set of values. Typically, just zero or one output value is produced per reduce invocation.</p><p>The main benefits of this model are in its simplicity and robustness. It allows programmers to write functional-style code that is easily parallelized and scheduled in a distributed computing environment. For a highly parallel system, one can view map-reduce offering two important components <ref type="bibr" target="#b17">[18]</ref>: a practical programming model that allows users to develop applications at a high level and an efficient runtime system that deals with the low-level details. Parallelization, concurrency control, resource management, fault tolerance and many other issues are handled by the map-reduce runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Generalized Reduction Structure</head><p>The API offered by our MATE system is quite similar to the map-reduce API, with some subtle, but important differences. First, the MATE API allows developers to explicitly declare a reduction object, and perform updates to its elements directly. On the other hand, map-reduce maintains an implicit reduction object unexposed to the user. Another important distinction is that, in map-reduce, all data elements are processed in the Map step and the intermediate results are then combined in the Reduce step. In MATE, both map and reduce steps are combined into a single step called Reduction, where each data element is processed and reduced before next data element is processed. This choice of design avoids the overheads due to high memory requirements of intermediate results, sorting, grouping, and shuffling, which can be significant costs in the map-reduce implementation.</p><p>The MATE system's generalized reduction based API is motivated by our earlier work on a system called FREERIDE (FRamework for Rapid Implementation of Datamining Engines) <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. The FREERIDE system was motivated by the difficulties in implementing and performance tuning parallel versions of data mining algorithms. FREERIDE is based upon the observation that parallel versions of several well-known data mining techniques share a relatively similar structure, which is that of a generalized reduction. Though the Map-Reduce paradigm is built on a similar observation <ref type="bibr" target="#b5">[6]</ref>, it should be noted that the first work on FREERIDE was published in 2001 <ref type="bibr" target="#b13">[14]</ref>, prior to the map-reduce paper by Dean and Ghemawat in 2004.  The API supported by MATE (and FREERIDE) exploits the similarity among parallel versions of several data mining and scientific data processing algorithms. The following are some important functions, which need to be written by the application developer. Reduction: A reduction function specifies how, after processing one data instance, a reduction object (initially declared by the programmer), is updated. The result of this processing must be independent of the order in which data instances are processed on each processor. The order in which data instances are processed is determined by the runtime system. Combination: In this function, final results from multiple copies of a reduction object are combined into a single reduction object. A user can choose from one of the several common combination functions already implemented in the system, or could provide one of their own. This API can be used to parallelize applications on multicores and clusters of multi-cores. Throughout the execution of the application, the reduction object is maintained in the main memory, with a private copy for each core. After every iteration where all data instances are processed, the results from multiple threads in a node are combined locally to obtain the final results. The local combination can be handled internally by the middleware or the programmers can write one of their own. Fig. <ref type="figure" target="#fig_1">1</ref> further illustrates the distinction in the processing structure enabled by MATE and map-reduce. The function Reduce is an associative and commutative function. Thus, the iterations of the for-each loop can be performed in any order. The data-structure RObj is referred to as the reduction object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. A Case Study</head><p>We will now use the apriori association mining algorithm, to show the similarities and differences between map-reduce and the generalized reduction APIs. Apriori is a well known algorithm for association mining, which also forms the basis for many newer algorithms <ref type="bibr" target="#b1">[2]</ref>. Association rule mining is a  <ref type="figure">2</ref>. Pseudo-code for apriori using Generalized Reduction API process of analyzing a set of transactions to extract association rules and is a commonly-used and well-studied data mining problem. Given a set of transactions, each of them being a set of items, the problem involves finding subsets of items that appear frequently in these transactions. Formally, let L i represent the set consisting of frequent itemsets of length i and C i denote the set of candidate itemsets of length i. In iteration i, C i is generated at the beginning of this stage and then used to compute L i . After that, L i is used as C i+1 for iteration (i + 1). This process will iterate until the candidate itemsets become empty. Fig. <ref type="figure">2</ref> shows the pseudo-code of apriori using the generalized reduction API. Using this API, in iteration i, first, for (i = 0; i &lt; length; i + +){ temp candidates[j + +] = reduce data out-&gt; key; } candidates = temp candidates; } Fig. <ref type="figure">3</ref>. Pseudo-code for apriori using MapReduce API the programmer is responsible for creating and initializing the reduction object. The reduction object is allocated to store the object ids for each frequent-i itemset candidate with associated counts of support. Then, the reduction operation takes a block of input transactions, and for each transaction, it will scan through the frequent-i itemset candidates, which are the frequent-(i -1) itemsets generated from last iteration. During the scan, if some frequent-i itemset candidate is found in the transaction, the object id for this itemset candidate is retrieved and its corresponding count of support is incremented by one. After the reduction operation is applied on all the transactions, the update frequent candidates operation is invoked to remove the itemset candidates whose count of support is below the support level that is defined by the programmer. Fig. <ref type="figure">3</ref> gives the pseudo-code of apriori using the map-reduce API. The map function is quite similar to the reduction given in Fig. <ref type="figure">2</ref> and the only difference is that the emit intermediate function is used instead of the accumulate operation for updating the reduction object. In iteration i, the map function will produce the itemset candidate as the key and the count one as the value, if this itemset can be found in the transaction. After the Map phase is done, for each distinct itemset candidate, the reduce function will sum up all the one's associated with this itemset. If its total count of occurrences is not less than the support level, this itemset and the total count will be emitted as the reduce output pair. The reduce output will be used in the update frequent candidates to compute L i for the current iteration and use it as the C i+1 for next iteration, the same as in Generalized Reduction.</p><p>By comparing the implementations, we can make the following observations. In Generalized Reduction, the reduction object is explicitly declared to store the number of transactions that own each itemset candidate. During each iteration, the reduction object has to be created and initialized first as the candidate itemsets are dynamically growing. For each itemset candidate, the reduction object is updated in the reduction operation, if it exists in a transaction. When all transactions are processed, the update frequent candidates operation will compute the frequent itemsets and use it as the new candidate itemsets for next iteration.</p><p>In map-reduce, the map function checks the availability of each itemset candidate in a transaction and emits it with the count one if applicable. The reduce function will gather the counts of occurrences with the same itemset candidate and compare the total count with the support level. To summarize, for each distinct itemset candidate, the generalized reduction sums up the number of transactions it belongs to and checks whether the total number is not less than the support level. In comparison, map-reduce examines whether the itemset candidate is in the transaction in the Map phase, without performing the sum. It then completes the accumulation and compares the total count with the support level in the Reduce phase. Therefore, the reduction can be seen as a combination of map and reduce connected by the reduction object. Besides, the intermediate pairs produced by the Map tasks, will be stored in the file system or main memory and accessed by the Reduce tasks. The number of such intermediate pairs can be very large. In apriori, processing each transaction can produce several such pairs. In comparison, the reduction object only has a single count for each distinct itemset. Therefore, it requires much less memory. In addition, map-reduce requires extra sorting, grouping, and shuffling of the intermediate pairs, since each reduce task accumulates pairs with the same key value. In comparison, the generalized reduction API does not have any of these costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SYSTEM DESIGN</head><p>In this section, we discuss the design and implementation of the MATE system. This system has been implemented on top of Phoenix, a shared memory implementation of map-reduce developed at Stanford <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The System API</head><p>The current implementation of our system is written in C. The system includes one set of system functions that are transparent to the programmers, in addition to the two sets of APIs, which are summarized in Table <ref type="table" target="#tab_4">I</ref>. The first API set are the functions that are simply invoked in the application code to initialize the system, perform the computation, and produce the final output. The second API set includes the functions that have to be defined by the programmers, specific to an application.</p><p>The most important operation is the reduction function that processes one split of the data and updates the reduction object. Besides, the programmers can also define applicationspecific splitter function to split the data before each stage. Note that the splitter, reduction, and combination functions can be different for each iteration, if necessary. In other words, multiple such functions may be defined. Also, to maintain the computation state, the reduction object after each iteration's processing is stored as intermediate results and can be reused by calling the get intermediate result function in future stages. The descriptions of other functions are listed in Table <ref type="table" target="#tab_4">I</ref>. Note that C pointers can provide flexibility in handling different data types. Thus, the function arguments are declared as void pointers at several places to utilize this feature. Apart from the API functions, the data structure used to communicate between the application code and the runtime system is of type scheduler args t. Table <ref type="table" target="#tab_3">II</ref> shows the basic and optional fields of this structure. The basic fields should be appropriately set in the application code since they provide the data pointers to both the input/output data buffers and the user- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Runtime System</head><p>As stated above, our system is built on Phoenix runtime, which in turn is based on P-threads <ref type="bibr" target="#b17">[18]</ref>. We use the same scheduling strategy, where the data is partitioned and the splits are assigned to threads dynamically. Execution Overview: Fig. <ref type="figure" target="#fig_2">4</ref> gives an overview of the execution of an application. Some of the key aspects are as follows: Scheduler Initialization: To initialize the scheduler, the programmer needs to specify all required data pointers and functions. Particularly, the programmer should pass the input data pointers and implement the splitter, reduction, and combination functions. Also, the programmer has to declare a reduction object. Reduction: After initialization, the scheduler checks the availability of processor cores, and for each core, it creates one worker thread. Before the Reduction phase starts, the splitter function is used to partition the data into equal-sized splits. Each worker thread will then invoke the splitter to retrieve one split and process it with the reduction function.</p><p>To avoid load imbalance, we try to assign Reduction tasks to worker threads dynamically instead of statically partitioning the data. Using this approach, each thread worker will repeatedly retrieve one split of the data and then process it as specified in the reduction function until no more splits are available. The split size, however, must be set appropriately to balance the lower overheads (few larger splits) and the load balancing (more smaller splits). To make use of temporary locality, by default, the runtime adjusts the split size such that the input data for a Reduction task can fit in the L1 cache. The programmer can also vary this parameter to achieve better performance given the knowledge of a specific application.</p><p>The reduction object is updated correspondingly in the reduction function as specified by the user code. Each thread will repeat this step until all data instances have been processed. At this point, the Reduction stage is over. The scheduler must </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Functions Provided by the Runtime int mate init(scheduler args t * )</head><p>Initializes the runtime system. The arguments provides the needed functions and data pointers int mate scheduler(void * )</p><p>Schedules the Reduction tasks on the input dataset int mate finalize(void * )</p><p>Executes the Finalizing task to generate a final output if needed void reduction object pre init() Allocates a default size of space for the reduction object int reduction object alloc() Assigns a unique object id representing a group of consecutive elements in the reduction object void reduction object post init()</p><p>Clones the reduction object for all threads in the full replication technique void accumulate(int, int, void * value)</p><p>Updates a particular element of the reduction object by addition. The arguments are the object id, the offset, and the associated value void reuse reduction object()</p><p>Reuses the reduction object for future stages. Memsets all fields of the reduction object to be zero void process next iteration() Notifies the runtime that the computation is proceeding to next stage void * get reduction object(int)</p><p>Retrieves the reduction object copy belonging to one thread. The argument is the thread number void * get intermediate result(int, int, int)</p><p>Retrieves a particular element of the reduction object in the combination buffer. The arguments are the stage number, the object id, and the offset Functions Defined by the User int ( * splitter t)(void * , int, reduction args t * )</p><p>Splits the input dataset across Reduction tasks void ( * reduction t)(reduction args t * )</p><p>The Reduction function. Each Reduction task executes this function on its input split. This function must be implemented by the user void ( * combination t)(void * )</p><p>The Combination function. In the full replication model, this function is used to combine results from all threads void ( * finalize t)(void * )</p><p>The Finalize function. The runtime will execute this function when all computation is done wait for all reduction workers to finish before initializing the Combination stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combination:</head><p>The processing structure of Generalized Reduction enables us to develop general but efficient sharedmemory parallelization schemes. Consider again the loop in Fig. <ref type="figure" target="#fig_1">1</ref>. The main correctness challenge in parallelizing a loop like this on a shared-memory machine arises because of possible race conditions (or access conflicts) when multiple processors update the same element of the reduction object. Two obvious approaches for avoiding race conditions are: full replication and full locking. In the full replication approach, each processor can update its own reduction object and these copies are merged together later. In the full locking approach, one lock or latch is associated with each aggregated value.</p><p>Currently, we use the full replication approach to implement the parallelization. So a Combination phase is required to merge the reduction-objects of multiple threads when all data is processed. The full locking and its optimized schemes are being considered and will be supported in the future.</p><p>In the Combination phase, the scheduler will spawn a combination-worker thread to merge the reduction object copies of all Reduction threads. The result is a single copy of the final reduction object after every iteration. If the application involves multiple iterations, this object is stored by the scheduler and can be accessed by the future stages.</p><p>Finalize: This phase occurs after all other stages of the computation have been performed, i.e., each Combination stage has its own copy of the reduction object, which is combined from multiple threads. The programmer is then able to perform a manipulation on the reduction-objects to summarize the results specific to an application. The scheduler also needs to free the allocated space in this phase, but this is transparent to the programmer.</p><p>Buffer Management: The runtime system handles two types of temporary buffers. One is the reduction-object buffer allocated for each thread to do its own computation over different splits. The other is the combination buffer created for storing the intermediate output results of each stage. The combination buffer is of type reduction object in our implementation. The buffers are initially sized to a default value and then grown dynamically if needed.</p><p>Fault Tolerance: Fault detection and recovery have been an important aspect of map-reduce implementations, as dataintensive applications can be very long running, and/or may need to be executed on a large number of nodes or cores. Our current support for fault-tolerance is based on the Phoenix system. It supports fault-recovery for the Reduction tasks. If a Reduction worker does not finish within a reasonable time limit, the scheduler assumes that a failure of this task has occurred. The failed task is then re-executed by the runtime . In our implementation, separate buffers are allocated for the new task to avoid data access conflicts.</p><p>The alternate API implemented in the MATE system also allows a different approach for fault-tolerance. This will be a low-cost checkpointing approach, where a copy of the reduction object can be cached at another location after processing of each split. If a thread fails while processing a particular split, we can restart computation using the cached reduction object. We only need to process the current split assigned to this thread, and not the splits that had been already processed. We plan to support this approach in future versions of the MATE system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>In this section, we evaluate the MATE system on multi-core machines by comparing its performance against Phoenix and Hadoop.</p><p>Since our main focus is on data mining applications, we choose three popular data mining algorithms. They are kmeans clustering, apriori association mining, and principal components analysis (PCA). Among these, we described apriori association mining earlier in Section II. K-means clustering is one of the commonly used data mining algorithms <ref type="bibr" target="#b11">[12]</ref>. The clustering problem is as follows. We consider data instances as representing points in a high-dimensional space. Proximity within this space is used as the criterion for classifying the points into clusters. Four steps in the sequential version of k-means clustering algorithm are as follows: 1) start with k given centers for clusters; 2) scan the data instances, for each data instance (point), find the center closest to it and assign this point to the corresponding cluster, 3) determine the k centroids from the points assigned to the corresponding centers, and 4) repeat this process until the assignment of points to clusters does not change. PCA is a popular dimensionality reduction method that was developed by Pearson in 1901. Its goal is to compute the mean vector and the covariance matrix for a set of data points that are represented by a matrix.</p><p>The datasets and the application parameters we used are as follows. For k-means, the dataset size is 1.2 GB, and the points are 3-dimensional. The number of clusters, k, was set to be 100. With apriori, we used a dataset that has 1,000,000 transactions, with each one having at most 100 items, and the average number of items in each transaction is 10. The support and confidence levels were 3% and 9%, respectively, and the application ran for 2 passes. In PCA, the number of rows and columns used in our experiments were 8,000 and 1,024, respectively.</p><p>Our experiments were conducted on two distinct multicore machines. One system uses Intel Xeon CPU E5345, comprising two quad-core CPUs (8 cores in all). Each core has a clock frequency of 2.33GHz and the system has a 6 GB main memory. The other system uses AMD Opteron Processor 8350 with 4 quad-core CPUs (16 cores in all). Each core has a clock frequency of 1.00GHz and the system has a 16 GB main memory.</p><p>We executed the aforementioned three applications with both MATE and Phoenix systems on the two multi-core machines. For two of the applications, k-means and apriori, we also do a comparison with Hadoop. We could not compare the performance of PCA on Hadoop, as we did not have an implementation of PCA on Hadoop. Hadoop implementations of k-means and apriori were carefully tuned by optimizing various parameters <ref type="bibr" target="#b12">[13]</ref>.</p><p>Figures 5 through 10 show the comparison results for these three applications as we scale the number of cores used. Since our system uses the same scheduler as Phoenix, the main performance difference is likely due to the different processing structure, as we had discussed in earlier sections. Note that for Hadoop, we only report results from the maximum number of cores available on each machine <ref type="bibr">(8 and 16)</ref>. This is because the number of tasks in Hadoop cannot be completely controlled, as there was no mechanism available to make it use only a subset of the available cores.</p><p>Figures <ref type="figure" target="#fig_3">5</ref> and<ref type="figure">6</ref> show the comparison for k-means. Our Hadoop is much slower than both MATE and Phoenix, which we believe is due to the high overheads in data initialization, file system implementation, and use of Java for computations. As compared to MATE, it also has the overhead of grouping, sorting, and shuffling of intermediate pairs <ref type="bibr" target="#b12">[13]</ref>. We also see a very good scalability with increasing number of threads or cores with MATE. On the 8-core machine, the speedup with MATE in going from 1 to 8 threads is about 7.8, whereas, Phoenix can only achieve a speedup of 5.4. The result is similar on the 16-core machine: our system can get a speedup of about 15.0 between 1 and 16 threads, whereas, it is 5.1 for Phoenix. With more than 100 million points in the k-means dataset, Phoenix was slower because of the high memory requirements associated with The results from PCA are shown in Figures <ref type="figure">7</ref> and<ref type="figure">8</ref>. PCA does not scale as well with increasing number of cores. This is because some segments of the program are either not parallelized, or do not have regular parallelism. However, we can still see that, with 8 threads on the 8-core machine and 16 threads on the 16-core machine, our system was twice as fast as Phoenix. Besides the overheads associated with the intermediate pairs, the execution-time breakdown analysis for Phoenix shows that the Reduce and Merge phases account for a non-trivial fraction of the total time. This overhead is non-existent for the MATE system, because of the use of the reduction object.</p><p>Finally, the results for apriori is shown in Figures <ref type="figure">9</ref> and<ref type="figure" target="#fig_6">10</ref>. For 1 and 2 threads, MATE is around 1.5 times as fast as Phoenix on both the machines. With 4 threads and more, the performance of Phoenix was relatively close to MATE. One of the reasons is that, compared to k-means and PCA, the Reduce and Merge phases take only a small fraction of time with the apriori dataset. Thus, the dominant amount of time is spent on the Map phase, leading to good scalability for Phoenix. For the MATE system, the reduction object can be of large size and needs to be grown and reallocated for all threads as the new candidate itemsets are generated during the processing. This, we believe, introduced significant overheads for our system. Hadoop was much slower than our system, by a factor of more than 25 on both the machines.</p><p>Overall, our experiments show that the MATE system has between reasonable to significant speedups for all the three datamining applications. In most of the cases, it outperforms Phoenix and Hadoop significantly, while slightly better in only one case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RELATED WORK</head><p>The topics of data-intensive computing and map-reduce have received much attention within the last 2-3 years. Efforts underway include projects that have used and evaluated mapreduce implementations, as well as those that are trying to improve its performance and programmability in different computing environments.</p><p>Projects in both academia and industry are working towards improving map-reduce. CGL-MapReduce <ref type="bibr" target="#b6">[7]</ref> uses streaming for all the communications, and thus improves the performance to some extent. Mars <ref type="bibr" target="#b9">[10]</ref> is the first attempt to harness GPU's power for map-reduce. Farivar et al. introduced an architecture named MITHRA <ref type="bibr" target="#b7">[8]</ref> and integrated the Hadoop map-reduce with the power of GPGPUs in the heterogeneous environments. Zaharia et al. <ref type="bibr" target="#b20">[21]</ref> improved Hadoop response times by designing a new scheduling algorithm in a virtualized data center. Seo et al. <ref type="bibr" target="#b18">[19]</ref> proposed two optimization schemes, prefetching and pre-shuffling, to improve Hadoop's overall performance in a shared map-reduce environment. Ranger et al. <ref type="bibr" target="#b17">[18]</ref> have implemented Phoenix that we have used as the basis for our implementation.</p><p>Facebook uses Hive <ref type="bibr" target="#b19">[20]</ref> as the warehousing solution to support data summarization and ad-hoc querying on top of Hadoop. Yahoo has developed Pig Latin <ref type="bibr" target="#b15">[16]</ref> and Map-Reduce-Merge <ref type="bibr" target="#b3">[4]</ref>, both of which are extensions to Hadoop, with the goal being to support more high-level primitives and improve the performance. Google has developed Sawzall <ref type="bibr" target="#b16">[17]</ref> on top of map-reduce to provide higher-level API. Microsoft has built Dryad <ref type="bibr" target="#b10">[11]</ref>, which is more flexible than map-reduce, since it allows execution of computations that can be expressed as DAGs.</p><p>Chu et al. <ref type="bibr" target="#b4">[5]</ref> have used Google's map-reduce for a variety of learning algorithms. Gillick et al. <ref type="bibr" target="#b8">[9]</ref> evaluate Hadoop in a distributed environment with multi-node clusters using both a set of benchmark applications and machine learning algorithms. They also offer improvements possibilities to Hadoop's implementation. Disco <ref type="bibr" target="#b0">[1]</ref> is an open source mapreduce runtime which is similar to Google's and Hadoop's architectures, but does not support a distributed file system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>This paper has described a system MATE, which provides an alternate API to the original map-reduce model for developing data-intensive applications. Our API takes advantage of a user-declared reduction object, which substantially reduces an overhead arising because of the original map-reduce API. These overheads are caused by high memory requirements of intermediate results and the need for data communication between Map and Reduce stages. MATE supports this alternate API on top of Phoenix, a multi-core map-reduce implementation from Stanford.</p><p>We have evaluated our system with three popular data mining algorithms, which are k-means clustering, apriori association mining, and principal component analysis. We also compared our system with Phoenix and Hadoop on two distinct multi-core platforms. Our results show that MATE achieves good scalability and outperforms Phoenix and Hadoop for all three applications. Overall, our work suggests that, despite an easy to program API support, map-reduce may not fit naturally into some data-intensive applications and results in non-trivial performance losses. An alternate API, which is based on the generalized reduction, offers better performance</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>The distinct API in MATE still offers a high-level interface, leading to much simpler implementations than alternate 2010 10th IEEE/ACM International Conference on Cluster, Cloud and Grid Computing 978-0-7695-4039-9/10 $26.00 © 2010 IEEE DOI 10.1109/CCGRID.2010.10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Processing Structure: MATE (left) and Map-Reduce (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. One-Stage Execution Overview</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. K-means: Comparison between MATE, Phoenix and Hadoop on 8 cores</figDesc><graphic coords="7,330.71,62.33,207.62,156.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. K-means: Comparison between MATE, Phoenix and Hadoop on 16 cores</figDesc><graphic coords="8,71.39,63.29,207.62,156.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Fig. 8. PCA: Comparison between MATE and Phoenix on 16 cores</figDesc><graphic coords="8,331.07,63.77,207.62,156.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Apriori: Comparison between MATE, Phoenix and Hadoop on 16 cores</figDesc><graphic coords="9,71.87,63.29,207.74,156.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II THE</head><label>II</label><figDesc>SCHEDULER ARGS T DATA STRUCTURE TYPE.</figDesc><table><row><cell>Field</cell><cell>Description</cell></row><row><cell>Basic Fields</cell><cell></cell></row><row><cell>Input data</cell><cell>Data pointer to the input dataset</cell></row><row><cell>Data size</cell><cell>Size of the input dataset</cell></row><row><cell>Data type</cell><cell>Type of the data instance</cell></row><row><cell>Stage num</cell><cell>Computation-Stage number</cell></row><row><cell>Splitter</cell><cell>Pointer to the Splitter function</cell></row><row><cell>Reduction</cell><cell>Pointer to the Reduction function</cell></row><row><cell>F inalize</cell><cell>Pointer to the Finalize function</cell></row><row><cell>Optional Fields</cell><cell></cell></row><row><cell>Unit size</cell><cell># of bytes of one data instance</cell></row><row><cell cols="2">L1 cache size # of bytes of L1 data cache</cell></row><row><cell>Model</cell><cell>Shared-memory parallelization model</cell></row><row><cell>Num threads</cell><cell>Max # of threads for Reduction workers</cell></row><row><cell>Num procs</cell><cell>Max # of processors cores available</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE I DESCRIPTIONS</head><label>I</label><figDesc>OF THE FUNCTIONS IN THE SYSTEM API.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors thank Christos Kozyrakis and his research group at Stanford for making the Phoenix system open source.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>and scalability while still maintaining easy programmability.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="http://discoproject.org/" />
		<title level="m">Disco project</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast algorithms for mining association rules</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1994 Int. conf. Very Large DataBases (VLDB&apos;94)</title>
		<meeting>1994 Int. conf. Very Large DataBases (VLDB&apos;94)<address><addrLine>Santiago,Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-09">September 1994</date>
			<biblScope unit="page" from="487" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mining association rules between sets of items in large databases</title>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Rakesh Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Imielinski</surname></persName>
		</author>
		<author>
			<persName><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1993 ACM SIGMOD Conference</title>
		<meeting>the 1993 ACM SIGMOD Conference</meeting>
		<imprint>
			<date type="published" when="1993-05">May 1993</date>
			<biblScope unit="page" from="207" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Map-reduce-merge: simplified relational data processing on large clusters</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Hung Chih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Dasdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruey-Lung</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Stott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGMOD Conference</title>
		<meeting>SIGMOD Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1029" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Map-reduce for machine learning on multicore</title>
		<author>
			<persName><forename type="first">Cheng-Tao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang</forename><forename type="middle">Kyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-An</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanyuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><forename type="middle">R</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunle</forename><surname>Olukotun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the Twentieth Annual Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="281" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mapreduce: Simplified data processing on large clusters</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of OSDI</title>
		<meeting>OSDI</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="137" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mapreduce for data intensive scientific analyses</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ekanayake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pallickara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Fourth International Conference on e-Science</title>
		<imprint>
			<date type="published" when="2008-12">Dec 2008</date>
			<biblScope unit="page" from="277" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multiple data independent tasks on a heterogeneous resource architecture</title>
		<author>
			<persName><forename type="first">Reza</forename><surname>Farivar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellick</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><surname>Mithra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 IEEE Cluster</title>
		<meeting>the 2009 IEEE Cluster</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Mapreduce: Distributed computing for machine learning</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arlo</forename><surname>Faria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mars: a mapreduce framework on graphics processors</title>
		<author>
			<persName><forename type="first">Bingsheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Naga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuyong</forename><surname>Govindaraju</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of PACT 2008</title>
		<meeting>PACT 2008</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="260" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dryad: distributed data-parallel programs from sequential building blocks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Budiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Birrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Fetterly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 EuroSys Conference</title>
		<meeting>the 2007 EuroSys Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="59" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Algorithms for Clustering Data</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Dubes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Comparing map-reduce and freeride for data-intensive applications</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vignesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gagan</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 IEEE Cluster</title>
		<meeting>the 2009 IEEE Cluster</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A middleware for developing parallel data mining implementations</title>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gagan</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the first SIAM conference on Data Mining</title>
		<meeting>the first SIAM conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2001-04">April 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Shared Memory Parallelization of Data Mining Algorithms: Techniques, Programming Interface, and Performance</title>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gagan</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second SIAM conference on Data Mining</title>
		<meeting>the second SIAM conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2002-04">April 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pig latin: a not-soforeign language for data processing</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Olston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utkarsh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tomkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGMOD Conference</title>
		<meeting>SIGMOD Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1099" to="1110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interpreting the data: Parallel analysis with sawzall</title>
		<author>
			<persName><forename type="first">Rob</forename><surname>Pike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Dorward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Griesemer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Programming</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="277" to="298" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Evaluating mapreduce for multi-core and multiprocessor systems</title>
		<author>
			<persName><forename type="first">Colby</forename><surname>Ranger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramanan</forename><surname>Raghuraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Penmetsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><forename type="middle">R</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 13th HPCA</title>
		<meeting>13th HPCA</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hpmr: Prefetching and pre-shuffling in shared mapreduce computation environment</title>
		<author>
			<persName><forename type="first">Sangwon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingook</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyungchang</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inkyo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin-Soo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungryoul</forename><surname>Maeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 IEEE Cluster</title>
		<meeting>the 2009 IEEE Cluster</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hive -a warehousing solution over a map-reduce framework</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Thusoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Joydeep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Namit</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasad</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Chakka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghotham</forename><surname>Wyckoff</surname></persName>
		</author>
		<author>
			<persName><surname>Murthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1626" to="1629" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving mapreduce performance in heterogeneous environments</title>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Konwinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randy</forename><forename type="middle">H</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of OSDI</title>
		<meeting>OSDI</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="29" to="42" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
