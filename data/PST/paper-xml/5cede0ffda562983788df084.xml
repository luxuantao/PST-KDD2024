<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2020 SINGLE PATH ONE-SHOT NEURAL ARCHITECTURE SEARCH WITH UNIFORM SAMPLING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Under review as a conference paper at ICLR 2020 SINGLE PATH ONE-SHOT NEURAL ARCHITECTURE SEARCH WITH UNIFORM SAMPLING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We revisit the one-shot Neural Architecture Search (NAS) paradigm and analyze its advantages over existing NAS approaches. Existing one-shot method <ref type="bibr" target="#b1">(Bender et al., 2018)</ref>, however, is hard to train and not yet effective on large scale datasets like ImageNet. This work propose a Single Path One-Shot model to address the challenge in the training. Our central idea is to construct a simplified supernet, where all architectures are single paths so that weight co-adaption problem is alleviated. Training is performed by uniform path sampling. All architectures (and their weights) are trained fully and equally. Comprehensive experiments verify that our approach is flexible and effective. It is easy to train and fast to search. It effortlessly supports complex search spaces (e.g., building blocks, channel, mixed-precision quantization) and different search constraints (e.g., FLOPs, latency). It is thus convenient to use for various needs. It achieves start-of-the-art performance on the large dataset ImageNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep learning automates feature engineering and solves the weight optimization problem. Neural Architecture Search (NAS) aims to automate architecture engineering by solving one more problem, architecture design. Early NAS approaches <ref type="bibr" target="#b31">(Zoph et al., 2018;</ref><ref type="bibr">Zhong et al., 2018a;</ref><ref type="bibr">b;</ref><ref type="bibr">Liu et al., 2018a;</ref><ref type="bibr" target="#b14">Real et al., 2018;</ref><ref type="bibr" target="#b18">Tan et al., 2018)</ref> solves the two problems in a nested manner. A large number of architectures are sampled and trained from scratch. The computation cost is unaffordable on large datasets.</p><p>Recent approaches <ref type="bibr">(Wu et al., 2018a;</ref><ref type="bibr" target="#b3">Cai et al., 2018;</ref><ref type="bibr" target="#b10">Liu et al., 2018b;</ref><ref type="bibr" target="#b22">Xie et al., 2018;</ref><ref type="bibr" target="#b13">Pham et al., 2018;</ref><ref type="bibr">Zhang et al., 2018c;</ref><ref type="bibr" target="#b2">Brock et al., 2017;</ref><ref type="bibr" target="#b1">Bender et al., 2018)</ref> adopt a weight sharing strategy to reduce the computation. A supernet subsuming all architectures is trained only once. Each architecture inherits its weights from the supernet. Only fine-tuning is performed. The computation cost is greatly reduced.</p><p>Most weight sharing approaches use a continuous relaxation to parameterize the search space <ref type="bibr">(Wu et al., 2018a;</ref><ref type="bibr" target="#b3">Cai et al., 2018;</ref><ref type="bibr" target="#b10">Liu et al., 2018b;</ref><ref type="bibr" target="#b22">Xie et al., 2018;</ref><ref type="bibr">Zhang et al., 2018c)</ref>. The architecture distribution parameters are jointly optimized during the supernet training via gradient based methods. The best architecture is sampled from the distribution after optimization. There are two issues in this formulation. First, the weights in the supernet are deeply coupled. It is unclear why inherited weights for a specific architecture are still effective. Second, joint optimization introduces further coupling between the architecture parameters and supernet weights. The greedy nature of the gradient based methods inevitably introduces bias during optimization and could easily mislead the architecture search. They adopted complex optimization techniques to alleviate the problem.</p><p>The one-shot paradigm <ref type="bibr" target="#b2">(Brock et al., 2017;</ref><ref type="bibr" target="#b1">Bender et al., 2018)</ref> alleviates the second issue. It defines the supernet and performs weight inheritance in a similar way. However, there is no architecture relaxation. The architecture search problem is decoupled from the supernet training and addressed in a separate step. Thus, it is sequential. It combines the merits of both nested and joint optimization approaches above. The architecture search is both efficient and flexible.</p><p>The first issue is still problematic. Existing one-shot approaches <ref type="bibr" target="#b2">(Brock et al., 2017;</ref><ref type="bibr" target="#b1">Bender et al., 2018)</ref> still have coupled weights in the supernet. Their optimization is complicated and involves sensitive hyper parameters. They have not shown competitive results on large datasets.</p><p>This work revisits the one-shot paradigm and presents a new approach that further eases the training and enhances architecture search. Based on the observation that the accuracy of an architecture using inherited weights should be predictive for the accuracy using optimized weights, we propose that the supernet training should be stochastic. All architectures have their weights optimized simultaneously. This gives rise to a uniform sampling strategy. To reduce the weight coupling in the supernet, a simple search space that consists of single path architectures is proposed. The training is hyperparameter-free and easy to converge. This work makes the following contributions.</p><p>1. We present a principled analysis and point out drawbacks in existing NAS approaches that use nested and joint optimization. Consequently, we hope this work will renew interest in the one-shot paradigm, which combines the merits of both via sequential optimization. 2. We present a single path one-shot approach with uniform sampling. It overcomes the drawbacks of existing one-shot approaches. Its simplicity enables a rich search space, including novel designs for channel size and bit width, all addressed in a unified manner. Architecture search is efficient and flexible. Evolutionary algorithm is used to support real world constraints easily, such as low latency.</p><p>Comprehensive ablation experiments and comparison to previous works on a large dataset (Ima-geNet) verify that the proposed approach is state-of-the-art in terms of accuracy, memory consumption, training time, architecture search efficiency and flexibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">REVIEW OF NAS APPROACHES</head><p>Without loss of generality, the architecture search space A is represented by a directed acyclic graph (DAG). A network architecture is a subgraph a ? A, denoted as N (a, w) with weights w.</p><p>Neural architecture search aims to solve two related problems. The first is weight optimization,</p><formula xml:id="formula_0">w a = arg min w L train (N (a, w)) ,<label>(1)</label></formula><p>where L train (?) is the loss function on the training set.</p><p>The second is architecture optimization. It finds the architecture that is trained on the training set and has the best accuracy on the validation set, as</p><formula xml:id="formula_1">a * = arg max a?A ACC val (N (a, w a )) ,<label>(2)</label></formula><p>where ACC val (?) is the accuracy on the validation set.</p><p>Early NAS approaches perform the two optimization problems in a nested manner <ref type="bibr" target="#b30">(Zoph &amp; Le, 2016;</ref><ref type="bibr" target="#b31">Zoph et al., 2018;</ref><ref type="bibr">Zhong et al., 2018a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b0">Baker et al., 2016)</ref>. Numerous architectures are sampled from A and trained from scratch as in Eq. ( <ref type="formula" target="#formula_0">1</ref>). Each training is expensive. Only small dataset (e.g., CIFAR 10) and small search space (e.g, a single block) are affordable.</p><p>Recent NAS approaches adopt a weight sharing strategy <ref type="bibr" target="#b3">(Cai et al., 2018;</ref><ref type="bibr" target="#b10">Liu et al., 2018b;</ref><ref type="bibr">Wu et al., 2018a;</ref><ref type="bibr" target="#b22">Xie et al., 2018;</ref><ref type="bibr" target="#b1">Bender et al., 2018;</ref><ref type="bibr" target="#b2">Brock et al., 2017;</ref><ref type="bibr">Zhang et al., 2018c;</ref><ref type="bibr" target="#b13">Pham et al., 2018)</ref>. The architecture search space A is encoded in a supernet<ref type="foot" target="#foot_0">1</ref> , denoted as N (A, W ), where W is the weights in the supernet. The supernet is trained once. All architectures inherit their weights directly from W . Thus, they share the weights in their common graph nodes. Fine tuning of an architecture is performed in need, but no training from scratch is incurred. Therefore, architecture search is fast and suitable for large datasets like ImageNet.</p><p>Most weight sharing approaches convert the discrete architecture search space into a continuous one <ref type="bibr">(Wu et al., 2018a;</ref><ref type="bibr" target="#b3">Cai et al., 2018;</ref><ref type="bibr" target="#b10">Liu et al., 2018b;</ref><ref type="bibr" target="#b22">Xie et al., 2018;</ref><ref type="bibr">Zhang et al., 2018c)</ref>. Formally, space A is relaxed to A(?), where ? denotes the continuous parameters that represent the distribution of the architectures in the space. Note that the new space subsumes the original one, A ? A(?). An architecture sampled from A(?) could be invalid in A.</p><p>An advantage of the continuous search space is that gradient based methods <ref type="bibr" target="#b10">(Liu et al., 2018b;</ref><ref type="bibr" target="#b3">Cai et al., 2018;</ref><ref type="bibr">Wu et al., 2018a;</ref><ref type="bibr" target="#b19">V?niat &amp; Denoyer, 2018;</ref><ref type="bibr" target="#b22">Xie et al., 2018;</ref><ref type="bibr">Zhang et al., 2018c)</ref> is feasible. Both weights and architecture distribution parameters are jointly optimized, as</p><formula xml:id="formula_2">(? * , W ? * ) = arg min ?,W</formula><p>L train (N (A(?), W )).</p><p>(3)</p><p>After optimization, the best architecture a * is sampled from A(? * ). Note that it could be invalid in A. If so, it is validated (e.g., by binarization of ? <ref type="bibr" target="#b10">(Liu et al., 2018b)</ref>). It then inherits the weights from W ? * and is fine-tuned.</p><p>Optimization of Eq. ( <ref type="formula">3</ref>) is challenging. First, the weights of the graph nodes in the supernet depend on each other and become deeply coupled during optimization. For a specific architecture, it inherits certain node weights from W . While these weights are decoupled from the others, it is unclear why they are still effective.</p><p>Second, joint optimization of architecture parameter ? and weights W introduces further complexity. Solving Eq. ( <ref type="formula">3</ref>) inevitably introduces bias to certain areas in ? and certain nodes in W during the progress of optimization. The bias would leave some nodes in the graph well trained and others poorly trained. With different level of maturity in the weights, different architectures are actually non-comparable. However, their prediction accuracy is used as guidance for sampling in A(?) (e.g., used as reward in policy gradient <ref type="bibr" target="#b3">(Cai et al., 2018)</ref>). This would further mislead the architecture sampling. This problem is in analogy to the "dilemma of exploitation and exploration" problem in reinforcement learning. To alleviate such problems, existing approaches adopt complicated optimization techniques (see Table <ref type="table">7</ref> for a summary). Nevertheless, there lacks a comprehensive evaluation of their effectiveness <ref type="bibr" target="#b8">(Li &amp; Talwalkar, 2019)</ref>.</p><p>Task constraints Real world tasks usually have additional requirements on a network's memory consumption, FLOPs, latency, energy consumption, etc. These requirements only depends on the architecture a, not on the weights w a . Thus, they are called architecture constraints in this work. A typical constraint is that the network's latency is no more than a preset budget, as Latency(a * ) ? Lat max .</p><p>(4)</p><p>Note that it is challenging to satisfy Eq. (2) and Eq. ( <ref type="formula">4</ref>) simultaneously for most previous approaches. Some works augment the loss function L train in Eq. (3) with soft loss terms that consider the architecture latency <ref type="bibr" target="#b3">(Cai et al., 2018;</ref><ref type="bibr">Wu et al., 2018a;</ref><ref type="bibr" target="#b22">Xie et al., 2018;</ref><ref type="bibr" target="#b19">V?niat &amp; Denoyer, 2018)</ref>. However, it is hard, if not impossible, to guarantee a hard constraint like Eq. (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR SINGLE PATH ONE-SHOT APPROACH</head><p>As analyzed above, the coupling between architecture parameters and weights is problematic. This is caused by joint optimization of both. To alleviate the problem, a natural solution is to decouple the super net training and architecture search in two sequential steps. This leads to the so called one-shot approaches <ref type="bibr" target="#b2">(Brock et al., 2017;</ref><ref type="bibr" target="#b1">Bender et al., 2018)</ref>.</p><p>In general, the two steps are formulated as follows. Firstly, the supernet weight is optimized as</p><formula xml:id="formula_3">W A = arg min W L train (N (A, W )) .<label>(5)</label></formula><p>Compared to Eq. ( <ref type="formula">3</ref>), the continuous parameterization of search space is absent. Only weights are optimized.</p><p>Secondly, architecture searched is performed as</p><formula xml:id="formula_4">a * = arg max a?A ACC val (N (a, W A (a))) .<label>(6)</label></formula><p>During search, each sampled architecture a inherits its weights from W A as W A (a). The key difference of Eq. ( <ref type="formula" target="#formula_4">6</ref>) from Eq. ( <ref type="formula" target="#formula_0">1</ref>) and ( <ref type="formula" target="#formula_1">2</ref>) is that architecture weights are ready for use. Evaluation of ACC val (?) only requires inference. Thus, the search is very efficient.</p><p>The search is also flexible. Any adequate search algorithm is feasible. The architecture constraint like Eq. ( <ref type="formula">4</ref>) can be exactly satisfied. Search can be repeated many times on the same supernet once  trained, using different constraints (e.g., 100ms latency and 200ms latency). These properties are absent in previous approaches. These make the one-shot paradigm attractive for real world tasks.</p><p>One problem in Sec. 2 still remains. The graph nodes' weights in the supernet training in Eq.( <ref type="formula" target="#formula_3">5</ref>) are coupled. It is unclear why the inherited weights W A (a) are still good for an arbitrary architecture a.</p><p>The recent one-shot approach <ref type="bibr" target="#b1">(Bender et al., 2018)</ref> attempts to decouple the weights using a "path dropout" strategy. During an SGD step in Eq. ( <ref type="formula" target="#formula_3">5</ref>), each edge in the supernet graph is randomly dropped. The random chance is controlled via a dropout rate parameter. In this way, the coadaptation of the node weights is reduced during training. Experiments in <ref type="bibr" target="#b1">(Bender et al., 2018)</ref> indicate that the training is very sensitive to the dropout rate parameter. This makes the supernet training hard. A carefully tuned heat-up strategy is used. In our implementation of this work, we also found that the validation accuracy is very sensitive to the dropout rate parameter.</p><p>Single Path Supernet and Uniform Sampling. Let us restart to think about the fundamental principle behind the idea of weight sharing. The key to the success of architecture search in Eq. ( <ref type="formula" target="#formula_4">6</ref>) is that, the accuracy of any architecture a on a validation set using inherited weight W A (a) (without extra fine tuning) is highly predictive for the accuracy of a that is fully trained. Ideally, this requires that the weight W A (a) to approximate the optimal weight w a as in Eq. ( <ref type="formula" target="#formula_0">1</ref>). The quality of the approximation depends on how well the training loss L train (N (a, W A (a))) is minimized. This gives rise to the principle that the supernet weights W A should be optimized in a way that all architectures in the search space are optimized simultaneously. This is expressed as</p><formula xml:id="formula_5">W A = arg min W E a??(A) [L train (N (a, W (a)))] ,<label>(7)</label></formula><p>where ?(A) is a prior distribution of a ? A. Note that Eq. ( <ref type="formula" target="#formula_5">7</ref>) is an implementation of Eq. ( <ref type="formula" target="#formula_3">5</ref>). In each step of optimization, an architecture a is randomly sampled. Only weights W (a) are activated and updated. So the memory usage is efficient. In this sense, the supernet is no longer a valid network. It behaves as a stochastic supernet <ref type="bibr" target="#b19">(V?niat &amp; Denoyer, 2018)</ref>. This is different from <ref type="bibr" target="#b1">(Bender et al., 2018)</ref>.</p><p>To reduce the co-adaptation between node weights, we propose a supernet structure that each architecture is a single path, as shown in Fig. <ref type="figure">3</ref>. Compared to the path dropout strategy in <ref type="bibr" target="#b1">(Bender et al., 2018)</ref>, the single path strategy is hyperparameter-free. We compared the two strategies within the same search space (as in this work). Note that the original drop path in <ref type="bibr" target="#b1">(Bender et al., 2018)</ref> may drop all operations in a block, resulting in a short cut of identity connection. In our implementation, it is forced that one random path is kept in this case since our choice block does not have an identity branch. We randomly select sub network and evaluate its validation accuracy during the training stage. Results in Fig. <ref type="figure">1</ref> show that drop rate parameters matters a lot. Our single path strategy corresponds to using drop rate 1. It works the best, which also verifies the benefits of weight decoupling by our single path strategy.</p><p>The prior distribution ?(A) is important. In this work, we empirically find that uniform sampling is good. This is not much of a surprise. A recent work also finds that purely random search is competitive to several SOTA NAS approaches <ref type="bibr" target="#b8">( Li &amp; Talwalkar (2019)</ref>). We also experimented with a variant that samples the architectures uniformly according to their constraints, named uniform constraint sampling. Specifically, we randomly choose a range, and then sample the architecture repeatedly until the FLOPs of sampled architecture falls in the range. This is because a real task usually expects to find multiple architectures satisfying different constraints. In this work, we find the uniform constraint sampling method is slightly better. So we use it by default in this paper.</p><p>We note that sampling a path according to architecture distribution during optimization is already used in previous weight sharing approaches <ref type="bibr" target="#b13">(Pham et al., 2018;</ref><ref type="bibr" target="#b19">V?niat &amp; Denoyer, 2018;</ref><ref type="bibr">Wu et al., 2018a;</ref><ref type="bibr" target="#b3">Cai et al., 2018;</ref><ref type="bibr" target="#b22">Xie et al., 2018;</ref><ref type="bibr">Zhang et al., 2018c;</ref><ref type="bibr" target="#b23">Yao et al., 2019;</ref><ref type="bibr" target="#b5">Dong &amp; Yang, 2019;</ref><ref type="bibr" target="#b17">Stamoulis et al., 2019)</ref>. The difference is that, the distribution ?(A) is a fixed priori during our training (Eq. ( <ref type="formula" target="#formula_5">7</ref>)), while it is learnable and updated (Eq. ( <ref type="formula">3</ref>)) in previous approaches (e.g. RL <ref type="bibr" target="#b13">(Pham et al., 2018)</ref>, policy gradient <ref type="bibr" target="#b19">(V?niat &amp; Denoyer, 2018;</ref><ref type="bibr" target="#b3">Cai et al., 2018)</ref>, Gumbel Softmax <ref type="bibr">(Wu et al., 2018a;</ref><ref type="bibr" target="#b22">Xie et al., 2018)</ref>, APG <ref type="bibr">(Zhang et al., 2018c)</ref>). As analyzed in Sec. 2, the latter makes the supernet weights and architecture parameters highly correlated and optimization difficult.</p><p>Comprehensive experiments in Sec. 4 show that our approach achieves better results than the SOTA methods. Note that there is no such theoretical guarantee that using a fixed prior distribution is inherently better than optimizing the distribution during training. Our better result likely indicates that the joint optimization in Eq. ( <ref type="formula">3</ref>) is too difficult for the existing optimization techniques.</p><p>Supernet Architecture and Novel Choice Block Design. Choice blocks are used to build a stochastic architecture. Fig. <ref type="figure">3</ref> illustrates an example case. A choice block consists of multiple architecture choices. For our single path supernet, each choice block only has one choice invoked at the same time. A path is obtained by sampling all the choice blocks.</p><p>The simplicity of our approach enables us to define different types of choice blocks to search various architecture variables. Specifically, we propose two novel choice blocks to support complex search spaces.  <ref type="figure">4</ref>. The main idea is to preallocate a weight tensor with maximum number of channels, and the system randomly selects the channel number and slices out the corresponding subtensor for convolution. With the weight sharing strategy, we found that the supernet can converge quickly.</p><p>In detail, assume the dimensions of preallocated weights are (max c out, max c in, ksize). For each batch in supernet training, the number of current output channels c out is randomly sampled. Then, we slice out the weights for current batch with the form Weights[: c out, : c in, :], which is used to produce the output. The optimal number of channels is determined in the search step.</p><p>Mixed-Precision Quantization Search. In this work, We design a novel choice block to search the bit widths of the weights and feature maps, as shown in Fig. <ref type="figure">5</ref>. We also combine the channel search space discussed earlier to our mixed-precision quantization search space. During supernet training, for each choice block feature bit width and weight bit width are randomly sampled. They are determined in the evolutionary step. See Sec. 4 and Fig. <ref type="figure">5</ref> for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evolutionary Architecture Search.</head><p>For architecture search in Eq. ( <ref type="formula" target="#formula_4">6</ref>), previous one-shot works <ref type="bibr" target="#b2">(Brock et al., 2017;</ref><ref type="bibr" target="#b1">Bender et al., 2018)</ref> use random search. This is not effective for a large search space. This work uses an evolutionary algorithm. Note that evolutionary search in NAS is used in <ref type="bibr" target="#b14">(Real et al., 2018)</ref>, but it is costly as each architecture is trained from scratch. In our search, each architecture only performs inference. This is very efficient.</p><p>The algorithm is elaborated in Algorithm 1. For all experiments, population size P = 50, max iterations T = 20 and k = 10. For crossover, two randomly selected candidates are crossed to produce a new one. For mutation, a randomly selected candidate mutates its every choice block with probability 0.1 to produce a new candidate. Crossover and mutation are repeated to generate enough new candidates that meet the given architecture constraints. Before the inference of an architecture, the statistics of all the Batch Normalization (BN) <ref type="bibr" target="#b7">(Ioffe &amp; Szegedy, 2015)</ref> operations are recalculated on a random subset of training data (20000 images on ImageNet). It takes a few seconds. This is because the BN statistics from the supernet are usually not applicable to the candidate nets. This is also referred in <ref type="bibr" target="#b1">(Bender et al., 2018)</ref>. Fig. <ref type="figure" target="#fig_0">2</ref> plots the validation accuracy over generations, using both evolutionary and random search methods. It is clear that evolutionary search is more effective. Experiment details are in Sec. 4.</p><p>The evolutionary algorithm is flexible in dealing with different constraints in Eq. ( <ref type="formula">4</ref>), because the mutation and crossover processes can be directly controlled to generate proper candidates to satisfy the constraints. Previous RL-based <ref type="bibr" target="#b18">(Tan et al., 2018)</ref> and gradient-based <ref type="bibr" target="#b3">(Cai et al., 2018;</ref><ref type="bibr">Wu et al., 2018a;</ref><ref type="bibr" target="#b19">V?niat &amp; Denoyer, 2018</ref>) methods design tricky rewards or loss functions to deal with such constraints. For example, <ref type="bibr">(Wu et al., 2018a</ref>) uses a loss function CE(a, w a ) ? ? log(LAT(a)) ? to balance the accuracy and the latency. It is hard to tune the hyper parameter ? to satisfy a hard constraint like Eq. ( <ref type="formula">4</ref>).</p><p>Summary. The combination of single path supernet, uniform sampling training strategy, evolutionary architecture search, and rich search space design makes our approach simple, efficient and flexible. Table <ref type="table">7</ref> in Appendix performs a comprehensive comparison of our approach against previous weight sharing approaches on various aspects. Ours is the easiest to train, occupies the smallest memory, best satisfies the architecture (latency) constraint, and easily supports large datasets. Extensive results in Sec. 4 verify that our approach is the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset.</head><p>All experiments are performed on ImageNet <ref type="bibr" target="#b15">(Russakovsky et al., 2015)</ref>. We randomly split the original training set into two parts: 50000 images for validation (50 images for each class exactly) and the rest as the training set. The original validation set is used for testing, on which all the evaluation results are reported, following <ref type="bibr" target="#b3">(Cai et al., 2018)</ref>.</p><p>Training. For the training of the supernet and retraining of the best architecture (after evolutionary search) from scratch, we use the same settings (including data augmentation strategy, learning rate schedule, etc.) as <ref type="bibr" target="#b12">(Ma et al., 2018)</ref>. The batch size is 1024. Supernet is trained for 120 epochs (150000 iterations) and the best architecture for 240 epochs (300000 iterations). Training uses 8 NVIDIA GTX 1080Ti GPUs.</p><p>Search Space: Building Blocks. First, we evaluate our method on the task of building block selection, i.e. to find the optimal combination of building blocks under a certain complexity constraint. Our basic building block design is inspired by a state-of-the-art manually-designed network -ShuffleNet v2 <ref type="bibr" target="#b12">(Ma et al., 2018)</ref>. Table <ref type="table" target="#tab_2">1</ref> shows the overall architecture of the supernet. There are 20 choice blocks in total. Each choice block has 4 candidates, namely "choice 3", "choice 5", "choice 7" and "choice x" respectively (see Fig. <ref type="figure" target="#fig_1">6</ref> in Appendix for details). They differ in kernel sizes and the number of depthwise convolutions. The size of the search space is 4 20 .</p><p>We use FLOPs ? 330M as the complexity constraint, as the FLOPs of a plenty of previous networks lies in <ref type="bibr">[300,</ref><ref type="bibr">330]</ref>, including manually-designed networks <ref type="bibr" target="#b6">(Howard et al., 2017;</ref><ref type="bibr" target="#b16">Sandler et al., 2018;</ref><ref type="bibr" target="#b21">Zhang et al., 2018b;</ref><ref type="bibr" target="#b12">Ma et al., 2018)</ref> and those obtained in NAS <ref type="bibr" target="#b3">(Cai et al., 2018;</ref><ref type="bibr">Wu et al., 2018a;</ref><ref type="bibr" target="#b18">Tan et al., 2018)</ref>.</p><p>Table <ref type="table" target="#tab_3">2</ref> shows the results. For comparison, we set up a series of baselines as follows: 1) select a certain block choice only (denoted by "all choice *" entries); note that different choices have different FLOPs, thus we adjust the channels to meet the constraint. 2) Randomly select some candidates from the search space. 3) Replace our evolutionary architecture optimization with random search used in <ref type="bibr" target="#b2">(Brock et al., 2017;</ref><ref type="bibr" target="#b1">Bender et al., 2018)</ref>. Results show that random search equipped with our single path supernet finds an architecture only slightly better that random select (73.8 vs. 73.7). It does no mean that our single path supernet is less effective. This is because the random search is too naive to pick good candidates from the large search space. Using evolutionary search, our approach finds out an architecture that achieves superior accuracy (74.3) over all the baselines.</p><p>Search Space: Channels. Based on our novel choice block for channel number search, we first evaluate channel search on the baseline structure "all choice 3" (refer to Table <ref type="table" target="#tab_3">2</ref>): for each building block, we search the number of "mid-channels" (output channels of the first 1x1 conv in each building block) varying from 0.2x to 1.6x (with stride 0.2), where "k-x" means k times the number of default channels. Same as Sec. 4, we set the complexity constraint FLOPs ? 330M . Table <ref type="table" target="#tab_5">3</ref> (first part) shows the result. Our channel search method has higher accuracy (73.9) than the baselines.</p><p>To further boost the accuracy, we search building blocks and channels jointly. There are two alternatives: 1) running channel search on the best building block search result of Sec. 4; or 2) searching on the combined search space directly. In our experiments, we find the results of the first pipeline is slightly better. As shown in Table <ref type="table" target="#tab_5">3</ref>, searching in the joint space achieves the best accuracy (74.7% acc.), surpassing all the previous state-of-the-art manually-designed <ref type="bibr" target="#b12">(Ma et al., 2018;</ref><ref type="bibr" target="#b16">Sandler et al., 2018)</ref> or automatically-searched models <ref type="bibr" target="#b18">(Tan et al., 2018;</ref><ref type="bibr" target="#b31">Zoph et al., 2018;</ref><ref type="bibr">Liu et al., 2018a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b3">Cai et al., 2018;</ref><ref type="bibr">Wu et al., 2018a)</ref> under the complexity of ? 300M FLOPs.  Comparison with State-of-the-arts. Results in Table <ref type="table" target="#tab_5">3</ref> shows our method is superior. Nevertheless, the comparisons could be unfair because different search spaces and training methods are used in previous works <ref type="bibr" target="#b3">(Cai et al., 2018)</ref>. To make direct comparisons, we benchmark our approach to the same search space of <ref type="bibr" target="#b3">(Cai et al., 2018;</ref><ref type="bibr">Wu et al., 2018a)</ref>. In addition, we retrain the searched models reported in <ref type="bibr" target="#b3">(Cai et al., 2018;</ref><ref type="bibr">Wu et al., 2018a)</ref> under the same settings to guarantee the fair comparison.</p><p>The search space and supernet architecture in ProxylessNAS <ref type="bibr" target="#b3">(Cai et al., 2018)</ref> is inspired by MobileNet v2 <ref type="bibr" target="#b16">(Sandler et al., 2018)</ref> and Mnas-Net <ref type="bibr" target="#b18">(Tan et al., 2018)</ref>. It contains 21 choice blocks; each choice block has 7 choices (6 different building blocks and one skip layer). The size of the search space is 7 21 . FBNet <ref type="bibr">(Wu et al., 2018a</ref>) also uses a similar search space.</p><p>Table <ref type="table" target="#tab_6">4</ref> reports the accuracy and complexities (FLOPs and latency on our device) of 5 models searched by <ref type="bibr" target="#b3">(Cai et al., 2018;</ref><ref type="bibr">Wu et al., 2018a)</ref>, as the baselines. Then, for each baseline, our search method runs under the constraints of same FLOPs or same latency, respectively. Results shows that for all the cases our method achieves comparable or higher accuracy than the counterpart baselines. We also point out that since the target devices in <ref type="bibr" target="#b3">(Cai et al., 2018;</ref><ref type="bibr">Wu et al., 2018a)</ref> are different from ours, the reported results may be sub-optimal on our platform.</p><p>Furthermore, it is worth noting that all our 10 architectures in Table <ref type="table" target="#tab_6">4</ref> are searched on the same supernet, justifying the flexibility and efficiency of our approach to deal with different complexity constraints: supernet is trained once and searched multiple times. In contrast, previous methods <ref type="bibr">(Wu et al., 2018a;</ref><ref type="bibr" target="#b3">Cai et al., 2018)</ref>  Table <ref type="table">5</ref>: Results of mixed-precision quantization search. "kWkA" means k-bit quantization for all the weights and activations. DNAS <ref type="bibr" target="#b21">(Wu et al., 2018b)</ref>.</p><p>Application: Mixed-Precision Quantization.</p><p>We evaluate our method on ResNet-18 and ResNet-34 as common practice in previous quantization works (e.g. <ref type="bibr" target="#b4">(Choi et al., 2018;</ref><ref type="bibr" target="#b21">Wu et al., 2018b;</ref><ref type="bibr">Liu et al., 2018c;</ref><ref type="bibr" target="#b29">Zhou et al., 2016;</ref><ref type="bibr">Zhang et al., 2018a)</ref>). Following <ref type="bibr" target="#b29">(Zhou et al., 2016;</ref><ref type="bibr" target="#b4">Choi et al., 2018;</ref><ref type="bibr" target="#b21">Wu et al., 2018b)</ref>, we only search and quantize the res-blocks, excluding the first convolutional layer and the last fully-connected layer. In the search space, choices of weight and feature bit widths include {(1, 2), (2, 2), (1, 4), (2, 4), (3, 4), (4, 4)}. As for channel search, we search the number of "bottleneck channels" (i.e. the output channels of the first convolutional layer in each residual block) in {0.5x, 1.0x, 1.5x}, where "k-x" means k times the number of original channels. The size of the search space is (3 ? 6) N = 18 N , where N is the number of choice blocks (N = 8 for ResNet-18 and N = 16 for ResNet-34). Note that for each building block we use the same bit widths for the two convolutions. We use PACT <ref type="bibr" target="#b4">(Choi et al., 2018)</ref> as the quantization algorithm.</p><p>Table <ref type="table">5</ref> reports the results. The baselines are denoted as kWkA (k = 2, 3, 4), which means uniform quantization of weights and activations with k-bits. Then, our search method runs under the constraints of the corresponding BitOps. We also compare with a recent mixed-precision quantization search approach <ref type="bibr" target="#b21">(Wu et al., 2018b)</ref>. Results shows that our method achieves superior accuracy in most cases. Also note that all our results for ResNet-18 and ResNet-34 are searched on the same supernet. This is very efficient.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Evolutionary vs. random architecture search.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Choice blocks used in Sec. 4. From left to right : Choice 3, Choice 5, Choice 7, Choice x.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Searched architectures of joint searching channel size and bit width under BitOPs constraints, see Table 5 for details. (a) -(c) are searched based on Resnet18. (d) -(f) are searched based on Resnet34.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Channel NumberSearch. We propose a new choice block based on weight sharing, as shown in Fig.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">current input channels (c_in)</cell><cell>Quantizer</cell><cell>Feature Bit Width</cell></row><row><cell>Choice Block</cell><cell></cell><cell></cell><cell></cell><cell>Convolutional Layer</cell><cell>output channels (c_out)</cell><cell>Weights</cell><cell>max output channels</cell><cell>Convolutional Layer</cell><cell>Quantizer</cell><cell>Kernel Weights</cell></row><row><cell>Choice Block</cell><cell>Choice 1</cell><cell>Choice 2</cell><cell>Choice 3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Choice Block</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Weights[:c_out, :c_in, :]</cell><cell>max input channels</cell><cell>kernel size</cell><cell>Weight Bit Width</cell></row><row><cell cols="4">Figure 3: Choice blocks for our</cell><cell cols="4">Figure 4: Choice block for</cell><cell>Figure 5: Choice block for</cell></row><row><cell cols="3">single path supernet.</cell><cell></cell><cell cols="3">channel number search.</cell><cell></cell><cell>mixed-precision quantization</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>search.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Supernet architecture. CB -choice block. GAP -global average pooling. The "stride" column represents the stride of the first block in each repeated group.</figDesc><table><row><cell>input shape</cell><cell>block</cell><cell cols="3">channels repeat stride</cell><cell>model</cell><cell cols="2">FLOPs top-1 acc(%)</cell></row><row><cell>224 2 ? 3 112 2 ? 16 56 2 ? 64 28 2 ? 160 14 2 ? 320 7 2 ? 640 7 2 ? 1024</cell><cell>3 ? 3 conv CB CB CB CB 1 ? 1 conv GAP</cell><cell>16 64 160 320 640 1024 -</cell><cell>1 4 4 8 4 1 1</cell><cell>2 2 2 2 2 1 -</cell><cell cols="2">all choice 3 all choice 5 all choice 7 all choice x random select (5 times) ?320M 324M 321M 327M 326M SPS + random search 323M</cell><cell>73.4 73.5 73.6 73.5 ?73.7 73.8</cell></row><row><cell>1024</cell><cell>fc</cell><cell>1000</cell><cell>1</cell><cell>-</cell><cell>ours (fully-equipped)</cell><cell>319M</cell><cell>74.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results of building block search. SPS -single path supernet.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results of channel search. * Performances are reported in the form "x (y)", where "x" means the accuracy retrained by us and "y" means accuracy reported by the original paper.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>have to train multiple supernets under various constraints. According to Table6, searching is much cheaper than supernet training. Compared with state-of-the-art NAS methods(Wu et al., 2018a;<ref type="bibr" target="#b3">Cai et al., 2018)</ref> using the same search space. The latency is evaluated on a single NVIDIA Titan XP GPU, with batchsize = 32. Accuracy numbers in the brackets are reported by the original papers; others are trained by us. All our architectures are searched from the same supernet via evolutionary architecture optimization.</figDesc><table><row><cell>baseline network</cell><cell>FLOPs</cell><cell cols="3">latency top-1 acc(%)</cell><cell cols="2">top-1 acc(%)</cell><cell>top-1 acc(%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">baseline</cell><cell cols="2">ours (same FLOPs) ours (same latency)</cell></row><row><cell>FBNet-A (Wu et al., 2018a)</cell><cell>249M/4.3M</cell><cell>13ms</cell><cell cols="2">73.0 (73.0)</cell><cell cols="2">73.2</cell><cell>73.3</cell></row><row><cell>FBNet-B (Wu et al., 2018a)</cell><cell>295M/4.5M</cell><cell>17ms</cell><cell cols="2">74.1 (74.1)</cell><cell cols="2">74.2</cell><cell>74.8</cell></row><row><cell>FBNet-C (Wu et al., 2018a)</cell><cell>375M/5.5M</cell><cell>19ms</cell><cell cols="2">74.9 (74.9)</cell><cell cols="2">75.0</cell><cell>75.1</cell></row><row><cell cols="2">Proxyless-R (mobile) (Cai et al., 2018) 320M/4.0M</cell><cell>17ms</cell><cell cols="2">74.2 (74.6)</cell><cell cols="2">74.5</cell><cell>74.8</cell></row><row><cell>Proxyless (GPU) (Cai et al., 2018)</cell><cell>465M/5.3M</cell><cell>22ms</cell><cell cols="2">74.7 (75.1)</cell><cell cols="2">74.8</cell><cell>75.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Method BitOPs top1-acc(%) Method BitoPs top1-acc(%)</cell></row><row><cell></cell><cell></cell><cell cols="3">ResNet-18 float point</cell><cell>70.9</cell><cell>ResNet-34 float point</cell><cell>75.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2W2A</cell><cell>6.32G</cell><cell>65.6</cell><cell>2W2A</cell><cell>70.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ours</cell><cell>6.21G</cell><cell>66.4</cell><cell>ours</cell><cell>13.11G</cell><cell>71.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>3W3A</cell><cell>14.21G</cell><cell>68.3</cell><cell>3W3A</cell><cell>29.72G</cell><cell>72.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>DNAS</cell><cell>15.62G</cell><cell>68.7</cell><cell>DNAS</cell><cell>38.64G</cell><cell>73.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ours</cell><cell>13.49G</cell><cell>69.4</cell><cell>ours</cell><cell>28.78G</cell><cell>73.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>4W4A</cell><cell>25.27G</cell><cell>69.3</cell><cell>4W4A</cell><cell>52.83G</cell><cell>73.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>DNAS</cell><cell>25.70G</cell><cell>70.6</cell><cell>DNAS</cell><cell>57.31G</cell><cell>74.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ours</cell><cell>24.31G</cell><cell>70.5</cell><cell>ours</cell><cell>51.92G</cell><cell>74.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Search Cost. Gds -GPU days GPUs. The Table6shows that our approach clearly uses less memory than other two methods because of the single path supernet. And our approach is much more efficient overall although we have an extra search step that costs less than 1 GPU day. Note Table6only compares a single run. In practice, our approach is more advantageous and more convenient to use when multiple searches are needed. As summarized in Table7, it guarantees to find out the architecture satisfying constraints within one search. Repeated search is easily supported.</figDesc><table><row><cell>Search Cost Analysis. The search cost is a</cell></row><row><cell>matter of concern in NAS methods. So we an-</cell></row><row><cell>alyze the search cost of our method and previ-</cell></row><row><cell>ous methods (Wu et al., 2018a; Cai et al., 2018)</cell></row><row><cell>(reimplemented by us). We use the search space</cell></row><row><cell>of our building blocks to measure the memory</cell></row><row><cell>cost of training supernet and overall time cost.</cell></row><row><cell>All the supernets are trained for 150000 iter-</cell></row><row><cell>ations with a batch size of 256. All models</cell></row><row><cell>are trained with 8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Structures of searched architectures under FLOPs constraints by using ProxylessNAS search space, see Table 4 for details. Searched result with same latency of Proxyless(GPU) Figure 9: Structures of searched architectures under GPU latency constraints by using ProxylessNAS search space, see Table 4 for details.</figDesc><table><row><cell>MB3 5x5</cell><cell>MB3 3x3</cell><cell>Identity</cell><cell>MB3 5x5</cell><cell>Identity</cell><cell>Identity</cell><cell>MB3 3x3 1.0x</cell><cell>Identity</cell><cell>MB3 7x7 1.0x MB3 3x3</cell><cell>MB3 3x3 1.5x MB3 3x3</cell><cell>MB3 3x3 Identity 1.5x</cell><cell>Identity Identity 1.5x</cell><cell>MB6 7x7 MB3 3x3 1.5x</cell><cell cols="2">Identity MB3 7x7 1.5x</cell><cell>MB3 7x7 Identity 1.5x</cell><cell>MB3 5x5 MB3 3x3</cell><cell>MB3 5x5 MB3 5x5</cell><cell>MB3 5x5 MB3 3x3</cell><cell>Identity MB3 5x5</cell><cell>MB3 3x3 MB3 5x5</cell><cell>MB6 5x5 MB6 3x3</cell><cell>MB3 3x3 MB6 5x5</cell><cell></cell><cell>MB3 7x7 MB3 7x7</cell><cell>MB3 7x7 MB3 3x3</cell><cell>MB3 7x7 MB6 5x5</cell></row><row><cell cols="24">MB3 3x3 MB3 3x3 Identity MB3 3x3 Figure 8: (a) Searched result with same latency of FBNet-A MB3 3x3 Identity MB6 5x5 MB3 3x3 MB3 5x5 MB6 3x3 MB6 7x7 MB3 5x5 MB3 3x3 MB6 7x7 MB3 7x7 MB6 5x5 MB3 5x5 MB3 5x5 MB3 5x5 MB3 5x5 MB3 3x3 Identity MB3 3x3 MB3 3x3 MB3 5x5 MB3 5x5 MB6 7x7 MB3 7x7 MB3 3x3 MB3 7x7 MB6 3x3 MB3 5x5 MB3 5x5 MB3 5x5 MB3 7x7 Identity Identity MB6 7x7 Identity Identity MB3 3x3 MB6 7x7 MB3 7x7 Identity MB3 7x7 MB6 3x3 MB3 5x5 MB3 5x5 MB3 3x3 (a) Searched result with same FLOPs of FBNet-A (b) Searched result with same FLOPs of FBNet-B (c) Searched result with same FLOPs of FBNet-C (d) Searched result with same FLOPs of Proxyless-R(mobile) MB6 7x7 MB3 7x7 MB6 7x7 MB3 7x7 MB6 7x7 MB6 7x7 (e) Searched result with same FLOPs of Proxyless (GPU) MB6 5x5 MB6 5x5 MB6 5x5 MB3 5x5 Identity Identity MB3 7x7 MB3 3x3 MB3 5x5 MB3 5x5 Identity MB3 3x3 MB3 7x7 MB3 3x3 MB3 7x7 MB3 5x5 MB3 3x3 (b) Searched result with same latency of FBNet-B and Proxyless-R(mobile) (c) Searched result with same latency of FBNet-C MB3 3x3 MB3 3x3 Identity Identity MB3 3x3 MB6 3x3 MB3 5x5 MB3 5x5 MB6 3x3 MB6 7x7 MB3 3x3 MB6 5x5 MB6 5x5 MB3 7x7 MB6 3x3 MB3 5x5 MB6 5x5 MB6 7x7 MB3 3x3 MB3 7x7 MB3 3x3 Identity MB6 3x3 MB3 5x5 MB3 3x3 MB6 3x3 MB3 7x7 MB3 5x5 MB3 3x3 MB3 7x7 MB6 5x5 MB6 7x7 MB6 3x3 MB6 5x5 MB6 5x5 MB6 7x7 MB3 3x3 Identity Identity Identity Identity Identity MB3 7x7 MB3 7x7 MB6 7x7 MB6 7x7 MB3 7x7 Identity MB6 5x5 MB6 3x3 MB3 3x3 Identity MB6 7x7 MB3 5x5 2W4A 3W4A 4W4A (a) Searched result with 6.21 GBitOPs 1.5x 1.5x 1.5x 0.5x 0.5x 1.0x 1.0x 1.0x (b) Searched result with 13.49 GBitOPs 1.5x 1.0x 1.5x 1.5x 1.5x 1.5x 1.5x 1.5x (c) Searched result with 24.31 GBitOPs 0.6x 1.0x 1.5x 1.5x 1.5x 1.5x 1.5x 0.5x 1.0x 1.0x 1.0x 1.0x 1.0x 1.5x 1.0x 2W2A 1W4A (d) Searched result with 13.11 GBitOPs (e) Searched result with 28.78 GBitOPs 1.5x 1.5x 1.5x 1.0x 1.0x 1.5x 1.0x 0.5x 1.5x 0.5x 1.0x 1.5x 1.5x 1.0x (d) 1W2A 1.5x 1.5x 1.5x 1.5x 1.5x 1.0x 1.5x 1.0x 1.0x 1.0x 1.5x 1.5x 1.5x 1.5x</cell><cell>MB3 7x7 MB3 7x7 MB3 5x5 MB3 7x7 MB3 7x7 MB3 3x3 MB3 7x7</cell><cell>1.5x 1.5x 1.5x</cell><cell>MB3 5x5 MB3 5x5 Identity MB3 5x5 MB3 3x3 MB3 7x7 MB6 3x3</cell><cell>1.5x 1.5x 1.5x</cell><cell>MB6 7x7 MB6 7x7 MB6 7x7 MB6 7x7 MB6 7x7 MB6 5x5 MB6 5x5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="12">(f) Searched result with 51.92 GBitOPs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">(a) Block search result</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Choice_7</cell><cell></cell><cell>Choice_x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0x</cell><cell></cell><cell>0.6x</cell><cell>1.6x</cell><cell>1.2x</cell><cell>1.0x</cell><cell>1.0x</cell><cell>0.8x</cell><cell>1.6x</cell><cell>1.2x</cell><cell>1.0x</cell><cell>1.6x</cell><cell>1.0x</cell><cell>1.2x</cell><cell>1.6x</cell><cell>0.8x</cell><cell>0.8x</cell><cell>1.2x</cell><cell>1.2x</cell><cell></cell><cell>1.4x</cell><cell>1.0x</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">(b) Channel search result</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Choice_3</cell><cell></cell><cell>Choice_5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.4x</cell><cell></cell><cell>1.2x</cell><cell>0.8x</cell><cell>1.2x</cell><cell>0.6x</cell><cell>1.4x</cell><cell>0.8x</cell><cell>1.0x</cell><cell>0.6x</cell><cell>1.2x</cell><cell>1.6x</cell><cell>1.2x</cell><cell>1.0x</cell><cell>1.4x</cell><cell>1.6x</cell><cell>1.0x</cell><cell>1.0x</cell><cell>1.2x</cell><cell></cell><cell>1.0x</cell><cell>0.8x</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">(c) Block search + channel search result</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>Figure 7: Structures of searched architectures in Sec. 4. (a) Result of building block search. (b) Result of channel search on all choice 3 structure. (c) Result of channel search on best building block search structure.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>"Supernet" is used as a general concept in this work. It has different names and implementation in previous approaches.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>Algorithm 1: Evolutionary Architecture Search 1 Input: supernet weights WA, population size P, architecture constraints C, max iteration T , validation dataset D val 2 Output: the architecture with highest validation accuracy under architecture constraints 3 P0 := Initialize population(P, C); Topk := ?; 4 n := P/2;</p><p>Crossover number 5 m := P/2;</p><p>Mutation number 6 prob := 0.1; Mutation probability 7 for i = 1 : T do 8 ACCi-1 := Inf erence(WA, D val , Pi-1); 9 Topk := U pdate T opk(Topk, Pi-1, ACCi-1); </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02167</idno>
		<title level="m">Nikhil Naik, and Ramesh Raskar. Designing neural network architectures using reinforcement learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Smash: one-shot model architecture search through hypernetworks</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodore</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05344</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00332</idno>
		<title level="m">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Pact: Parameterized clipping activation for quantized neural networks</title>
		<author>
			<persName><forename type="first">Jungwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swagath</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I-Jen</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijayalakshmi</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kailash</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><surname>Gopalakrishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06085</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1761" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Andrew G Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07638</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm</title>
		<author>
			<persName><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="722" to="737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melody</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03268</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01548</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Stamoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruizhou</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Lymberopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bodhi</forename><surname>Priyantha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Marculescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02877</idno>
		<title level="m">Single-path nas: Designing hardware-efficient convnets in less than 4 hours</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Mnasnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11626</idno>
		<title level="m">Platformaware neural architecture search for mobile</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning time/memory-efficient deep architectures with budgeted super networks</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>V?niat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3492" to="3500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03443</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Mixed precision quantization of convnets via differentiable neural architecture search</title>
		<author>
			<persName><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00090</idno>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hehui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09926</idno>
		<title level="m">Snas: stochastic neural architecture search</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ju</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Wei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13577</idno>
		<title level="m">Differentiable neural architecture search via proximal iterations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lq-nets: Learned quantization for highly accurate and compact deep neural networks</title>
		<author>
			<persName><forename type="first">Dongqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongqiangzi</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="365" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">You only search once: Single shot neural architecture search via direct sparse optimization</title>
		<author>
			<persName><forename type="first">Xinbang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01567</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Practical block-wise neural network architecture generation</title>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2423" to="2432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05584</idno>
		<title level="m">Blockqnn: Efficient block-wise neural network architecture generation</title>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zekun</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuheng</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06160</idno>
		<title level="m">Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
