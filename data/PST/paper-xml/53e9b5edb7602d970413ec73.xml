<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Scalable Multiprocessor Virtual Machines</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Volkmar</forename><surname>Uhlig</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">System Architecture Group</orgName>
								<orgName type="institution">Universität Karlsruhe</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joshua</forename><surname>Levasseur</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">System Architecture Group</orgName>
								<orgName type="institution">Universität Karlsruhe</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Espen</forename><surname>Skoglund</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">System Architecture Group</orgName>
								<orgName type="institution">Universität Karlsruhe</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Uwe</forename><surname>Dannowski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">System Architecture Group</orgName>
								<orgName type="institution">Universität Karlsruhe</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Scalable Multiprocessor Virtual Machines</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A98AAEDB0BA03DC0EB0954030DD3D558</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A multiprocessor virtual machine benefits its guest operating system in supporting scalable job throughput and request latency-useful properties in server consolidation where servers require several of the system processors for steady state or to handle load bursts.</p><p>Typical operating systems, optimized for multiprocessor systems in their use of spin-locks for critical sections, can defeat flexible virtual machine scheduling due to lock-holder preemption and misbalanced load. The virtual machine must assist the guest operating system to avoid lock-holder preemption and to schedule jobs with knowledge of asymmetric processor allocation. We want to support a virtual machine environment with flexible scheduling policies, while maximizing guest performance.</p><p>This paper presents solutions to avoid lock-holder preemption for both fully virtualized and paravirtualized environments. Experiments show that we can nearly eliminate the effects of lock-holder preemption. Furthermore, the paper presents a scheduler feedback mechanism that despite the presence of asymmetric processor allocation achieves optimal and fair load balancing in the guest operating system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A recent trend in server consolidation has been to provide virtual machines that can be safely multiplexed on a single physical machine <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">24]</ref>. Coupling a virtual machine environment with a multiprocessor system furthers the trend of untrusted server consolidation.</p><p>A multiprocessor system offers many advantages for a virtualized environment. The hypervisor, the controlling agent of the virtual machine environment, can distribute the physical processors to guest operating sys-This paper was originally published in the Proceedings of the 3rd Virtual Machine Research &amp; Technology Symposium (VM'04), San Jose, CA, <ref type="bibr">May 6-7 2004</ref> tems (OS) to support arbitrary policies, and reassign the processors in response to varying load conditions. The allocation policy may support concurrent execution of guests, such that they only ever access a fraction of the physical processors, or alternatively time-multiplex guests across a set of physical processors to, e.g., accommodate for spikes in guest OS workloads. It can also map guest operating systems to virtual processors (which can exceed the number of physical processors), and migrate between physical processors without notifying the guest operating systems. This allows for, e.g., migration to other machine configurations or hotswapping of CPUs without adequate support from the guest operating system. It is important to recognize that allowing arbitrary allocation policies offers much more flexibility than schemes where one can only configure a virtual machine to either have an arbitrary share of a single processor <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">24]</ref>, or have uniform shares over multiple physical processors <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">24]</ref>.</p><p>Isolating commodity operating systems within virtual machines can defeat the assumptions of the guest operating system. Where the guest operating system expects constant resource configurations, critical timing behavior, and unrestrained access to the platform, the virtual machine provides illusionary access as it sees fit. Several methods exist to attempt to satisfy (a subset of) the assumptions of the guest operating system. The solutions may focus on the issues of instruction set emulation, such as trapping on system instructions <ref type="bibr" target="#b22">[22]</ref>, or they may focus on the behavior of the guest operating system algorithms, such as dynamic allocation of physical memory <ref type="bibr" target="#b25">[25]</ref>.</p><p>This paper presents solutions to two problems that arise with scheduling of virtual machines which provide a multi-processor environment for guest operating systems. Both problems limit scalability and performance. First, guest operating systems often use spin-locks as a means to offer exclusive access to code or data. Such spin-locks are, by design, only held for a short period of time, but if a virtual machine is preempted while holding the lock this assumption no longer holds. The crux of the problem is that the same virtual machine may still be running on a different processor, waiting for the lock to be released, thus wasting huge amounts of processor cycles (often several milliseconds).</p><p>The second problem is due to the ability of virtual processors to offer the illusion of varying speed. Today's operating systems cannot react well to multi-processor systems where otherwise identical CPUs have asymmetric and varying clock speeds. The problem manifests itself in suboptimal scheduling of CPU intensive workloads and burst-load handling.</p><p>To address the first problem, we have devised two techniques for avoiding preemption of lock holders, one requiring modifications of the locking primitives in the guest OS, and one in which the hypervisor attempts to determine when it is safe to preempt the virtual machine (i.e., without modifying the guest OS). Initial results suggest that our lock holder preemption avoidance schemes can increase high-load web server performance by up to 28% compared to an approach where the virtualization layer does not take guest OS spin-locks into account.</p><p>To handle asymmetric CPU speeds we propose a technique called time ballooning where the hypervisor coerces the guest OS to adapt scheduling metrics to processor speed. The coercion may manifest as the introduction of ghost processes into the scheduling queues of the guest OS, or as balloon processes which donate their cycles back to the virtualization layer when scheduled by the guest OS. By artificially increasing the load on a virtual CPU we pressure the guest OS into migrating processes to other virtual CPUs with more available resources.</p><p>The remainder of the paper is structured as follows. Section 2 elaborates on the problem of lock-holder preemption. Sections 3 and 4 describe our solutions with lock-holder preemption avoidance and time ballooning, followed by experimental results in Section 5. The implications of our solution and future work are discussed in Section 6. Section 7 presents related work, and finally Section 8 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Case for Lock-holder Preemption Avoidance</head><p>Many of the commodity operating systems used in server consolidation have optimized support for multiple processors. A primary function of the multiprocessor support is to guarantee atomic and consistent state changes within the kernel's data structures. Typical kernels use memory barriers to ensure in-order memory updates, and they craft critical sections protected by locks to enforce atomic updates. The critical sections may be associated with a region of code, or as with more fine grained locking, they may be associated with a particular piece of data.</p><p>When the number of processors in a system increases, more processors will be competing for access to the critical sections. To achieve multiprocessor scalability it is important that the time a processor spends in a critical section is short. Otherwise, the processors trying to acquire the lock for the critical section can experience long waiting times. Designing a system for short lockholding times makes it feasible to poll for a lock to be released (i.e., using spin-locks). Short lock-holding times may also obviate the need to implement more expensive locking primitives to enforce fair lock access, since the kernel may achieve such fairness statistically.</p><p>A very different approach to achieve multi-processor scalability in operating systems has been to avoid locking altogether by using non-blocking synchronization primitives. Although an operating system kernel can in theory be made lock free using atomic compare-andswap instructions supported by many hardware architectures, it has been shown that special hardware support is needed to make lock free kernels feasible <ref type="bibr" target="#b10">[11]</ref>. Such special hardware support has been used to implement lock-free versions of Synthesis <ref type="bibr" target="#b19">[19]</ref> and the Cachekernel <ref type="bibr" target="#b5">[6]</ref>, but is not applicable to commodity operating systems in general, both because of the hardware requirements and the tremendous task of rewriting large parts of the kernel internal data structures and algorithms. Some form of locking therefore seems unavoidable.</p><p>When running a commodity operating system in a virtual machine, the virtual machine environment may violate some of the premises underlying the guest operating system's spin-locks. The virtual machine can preempt the guest kernel, and thus preempt a lock holder, which can result in an extension of the lock holding time. For example, in Linux, the typical lock holding time is under 20 microseconds (see Figure <ref type="figure">2</ref>), which a preemption can easily extend by several time slices, often in the order of tens of milliseconds.</p><p>Consequently, the main effect of lock preemption is the potential for wasting a guest operating system's time slice. If a guest kernel spins on a preempted lock, it could live out the remainder of its time slice spinning and accomplishing no work. Thus spin-times are amplified. A concomitant effect is the violation of the original statistical fairness properties of the lock.</p><p>The side effects of lock holder preemption could be avoided with coscheduling <ref type="bibr" target="#b21">[21]</ref>. In coscheduling (or gang scheduling), all virtual processors of a virtual machine are simultaneously scheduled on physical processors, for an equal time slice. The virtual machine could preempt lock holders without side effects, since the coschedule guarantees that another processor will not spin on a preempted lock. However, coscheduling introduces several problems for scalability and flexibility. Coscheduling activates virtual processors whether or not they will accomplish useful work, easily leading to underutilized physical processors. Further, coscheduling precludes the use of other scheduling algorithms, such as multiplexing multiple virtual processors on the same physical processor (e.g., in response to fault recovery of a failed processor, or load balancing).</p><p>Alternative lock wait techniques to spinning, such as reschedule or spin-then-reschedule, have successfully been applied to user applications <ref type="bibr" target="#b16">[16]</ref>, but these techniques are generally not applicable to traditional operating systems code because the kernel does not always enjoy the option of preempting its current job (e.g., if within a low-level interrupt handler). To conclude, we see that spin-locks are and will be used in commodity operating systems, and preempting lock-holders may as such pose a significant performance problem.</p><p>To determine whether the frequency of lock-holder preemption really merits consideration, we instrumented Linux 2.4.20 with a lock tracing facility to inspect locking statistics. Table <ref type="table" target="#tab_0">1</ref> shows the results for three workloads with varying characteristics from the application spectrum. We measured the time for when a CPU holds at least one lock, on a machine with four 700MHz Intel Xeon processors. With CPU-intensive applications (povray, a ray-tracing application) we found locks being held for an average of 3.0µs and a maximum of 103µs. With an I/O-intensive workload like the Apache 2 web server under stress these numbers were 2.2µs and 473µs respectively.</p><p>From our tracing experiments we observe that the probability of preempting a virtual CPU while holding a lock lies between 0.04% for CPU-intensive workloads and 39% for I/O-intensive workloads. These numbers indicate that scheduling a virtual machine running an I/O-intensive workload without regard for guest OS spin-locks can severely impact the performance of the virtual machine. Some scheme for dealing with lockholder preemption is therefore deemed necessary.</p><p>There are two approaches to deal with lock-holder preemption. The first approach is to detect contention on a lock and to donate the wasted spinning time to the lock holder. Also known as helping locks, this approach requires substantial infrastructure to donate CPU time between virtual CPUs (provided donation is possible at all) <ref type="bibr" target="#b14">[14]</ref>. The second approach is to avoid preempting lock-holders altogether. Instead, soon to become lockholders are preempted before acquiring a lock, or preemption is delayed until after the last lock has been released.</p><p>Depending on the level of virtual machine awareness in the guest operating systems, different methods of lock-holder preemption avoidance can be used. We discuss these methods in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Lock-holder Preemption Avoidance</head><p>Lock holder preemption avoidance can be achieved by either modifying the guest operating system to give hints to the virtual machine layer (intrusive), or have the virtual machine layer detect when the guest operating system is not holding a lock (non-intrusive). The former approach is well suited for systems where the virtualized architecture is not identical with the underlying hardware; also called paravirtualization <ref type="bibr" target="#b28">[28]</ref>. The latter approach is well suited for fully virtualized systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Intrusive Lock-holder Preemption Avoidance</head><p>For the intrusive approach we use a similar scheme as implemented in Symunix II <ref type="bibr" target="#b7">[8]</ref> and described by Kontothanassis et. al. <ref type="bibr" target="#b17">[17]</ref>; the main difference being that the guest OS itself is here the application giving scheduling hints to the lower layer (the hypervisor).</p><p>Intrusive lock-holder preemption avoidance in our system is achieved by augmenting Linux (our guest OS) with a delayed preemption mechanism. Before acquiring a lock, the guest OS indicates that it should not be preempted for the next n microseconds. After releasing the lock, the guest OS indicates its willingness to be preempted again. The virtualization layer will not preempt the lock-holder if it has indicated that it wishes no preemption, but rather set a flag and delay the preemption by n microseconds. When the guest OS releases the lock, it is required to check the flag, and if set, immediately yield its processing time back to the virtualization layer. Failure to give back the processing time will be caught by the virtualization layer after n microseconds, and the guest operating system will be penalized by a subsequent reduction in its processing time and the possibility of untimely preemption. The value to choose for n depends on how long the guest OS expects to be holding a lock and will as such rely heavily upon the operating system used and the workload being run. We have run a number of lockintensive workloads on a version 2.4.20 Linux kernel and found that more than 98% of the times the Linux kernel holds one or more locks, the locks are held for less than 20µs. These numbers suggest that setting n any higher than 20µs will not substantially decrease the probability of preempting lock holders in the Linux kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Non-intrusive Lock-holder Preemption Avoidance</head><p>It is not always the case that one has the possibility of modifying the guest operating system, in particular if the kernel is only distributed in binary form. We therefore need non-intrusive means to detect and prevent lockholders from being preempted. Utilizing the fact that the operating system will release all kernel locks before returning to user-level, the virtualization layer can monitor all switches between user-level and kernel-level, <ref type="foot" target="#foot_0">1</ref> and determine whether it is safe to preempt the virtual machine without preempting lock-holders. This gives us a first definition of safe and unsafe preemption states: safe state -Virtual machine is currently executing at user-level. No kernel locks will be held.</p><p>unsafe state -Virtual machine is currently executing at kernel-level. Kernel locks may be held.</p><p>The safe state can be further refined by monitoring for when the guest OS executes the equivalent of the IA-32 HLT instruction to enter a low-latency power saving mode (while in the idle loop). Since the operating system can be assumed to hold no global kernel locks while suspended, it is safe to treat the HLT instruction as a switch to safe state. A switch back into unsafe state will occur next time the virtual CPU is rescheduled (e.g., due to an interrupt).</p><p>With the safe/unsafe scheme it is still possible that the virtual machine will be preempted while a userlevel application is holding a spin-lock. We ignore this fact, however, because user-level applications using spin-locks or spin-based synchronization barriers are generally aware of the hardware they are running on, and must use some form of coscheduling to achieve proper performance. Section 6.4 deals with such workloads in more detail.</p><p>In order to substantiate the accuracy of the safe/unsafe model for approximating lock-holding times, we augmented the lock tracing facility described in Section 2 with events for entering and leaving safe states. Our measurements are summarized in Table <ref type="table" target="#tab_1">2</ref>. Figure <ref type="figure">1</ref> shows more detailed excerpts of the traces for 1% of the execution time is spent in a safe state). The large amount of unsafe time can be attributed to Apache's use of the sendfile system call to offload all file-to-network transfer into the kernel (i.e., avoid copying into user-level).</p><p>For the Povray and kernel-build workloads we observe that the unsafe times reasonably approximate their lock holding times. Figure <ref type="figure">2</ref> shows that the unsafe times for these workloads are generally less than 10µs longer than the lock-holding times. For the Apache 2 workload, however, the unsafe times are on average three orders of magnitude longer than the locking times. We observe an average unsafe time of more than 1.4ms. This is in spite of locking times staying around 2µs, and can, as mentioned above, be attributed to the Apache server offloading file-transfer work to the kernel by using the sendfile system call. Now, observing that the standard deviation for the unsafe times in the Apache 2 workload is rather high (7.5ms), we might be tempted to attribute the high disparity between average lock-holding times and unsafe times to a number of off-laying measurements. Looking at Figure <ref type="figure" target="#fig_3">3</ref>, however, we see that the Apache 2 workload has substantially longer unsafe times even for the lower end of the axis. For example, the Apache 2 workload only has about 91% of the unsafe times below 200µs, while the povray workload has close to 100% of its unsafe times below 20µs. These numbers suggest that the unsafe state approximation to lock-holding times is not good enough for workloads like Apache 2. We want a better approximation.</p><p>Having some knowledge of the guest OS internals, it is often possible to construct points in time, so called safe-points, when the virtual machine's guest OS is guaranteed to hold no spin-locks.</p><p>One example of how safe-point injection can be achieved is through targeted device drivers installed in the guest OS, designed to execute in a lock free context. An example (compatible with Linux) is the use of a network protocol handler, added via a device driver. The virtualization layer could craft packets for the special protocol, hand them to the virtual NIC of Linux, from where they would propagate to the special protocol handler. When the guest OS invokes the protocol handler, it will hold no locks, and so the protocol handler is safe to yield the time slice if a preemption is pending.</p><p>In an experimental Linux setup we measured as little as 8000 cycles for such a packet to travel from the virtual NIC to our protocol driver under high load. With time slice lengths in the millisecond range, this enables very precise injection of preemption points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Locking-Aware Virtual Machine Scheduling</head><p>In the following section we describe a mechanism for efficient scheduling of multiple multi-processor virtual machines, leveraging the techniques described in the previous two sections.</p><p>Virtual CPUs can be modeled as threads in the virtualization layer which are then subject to scheduling, each one in turn receiving its time slice to execute. Our goal is to preempt a virtual machine (i.e., a thread in the virtualization layer) as close to the end of its time slice as possible, or before that if the virtual machine decides not to consume the whole time slice. In addition, we want to guarantee fairness so that a virtual machine will aggregately obtain its fair share of the CPU time.</p><p>Figure <ref type="figure" target="#fig_2">4</ref> illustrates the basis of our virtual machine scheduling algorithm. We define a preemption window, w, around the end of the virtual machine's time slice. For the n th time slice, this preemption window starts at time s n , E n is the actual end of the allocated time slice, and P n is the time when the hypervisor finds it safe to preempt the virtual machine (the last spin-lock was released or the VM entered a safe state). If no safe preemption point occurs before the end of the window, the hypervisor will enforce a preemption.</p><p>Our goal with the scheduling algorithm is to choose the window start, s n , so that the preemption point, P n , on average coincides with the actual end of time slice, E n (i.e., n i=0 E i -P i = 0). In doing so we achieve fair access to desired processor time. Now, assume that in the past the average distance between our start points and preemption points equaled an offset, o n . In order to keep this property for our next time slice we must calculate the next offset, o n+1 , so that it takes into account the current distance between preemption point and window start (P ns n ). This is a simple calculation to perform, and the result is used to determine the next window start point:</p><formula xml:id="formula_0">s n+1 = E n+1 -o n+1 .</formula><p>The consequence of our algorithm is that a preemption that occurs before the end of time slice will cause the preemption window to slide forwards, making premature preemptions less likely (see lower part of Figure <ref type="figure" target="#fig_2">4</ref>). Conversely, a preemption after end of time slice will cause the preemption window to slide backwards, making premature preemptions more likely.</p><p>The scheduling algorithm ensures that any preemptions before or after the end of time slice will eventually be evened out so that we achieve fairness. However, since the algorithm keeps an infinite history of previous preemptions it will be slow to adapt to changes in the virtual machine workload. To mitigate this problem we can choose to only keep a history of the last k preemptions. The formula for calculating the next window offset then becomes:</p><formula xml:id="formula_1">o n+1 = o n (k -1) + (P n -s n ) k</formula><p>A further improvement of the algorithm is to detect when preemptions have to be forced at the end of the preemption window-a result of no safe state encountersand remedy the situation by injecting safe points into subsequent preemption windows.</p><p>There are two tunable variables in our VM scheduling algorithm. Changing the window length, w, will decrease or increase the accepted variance in time slice lengths, at the expense of having the virtual machines being more or less susceptible to lock-holder preemptions. Changing the history length, k, will dictate how quickly the hypervisor adapts to changes in a virtual machine's workload.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Time Ballooning</head><p>The scheduling algorithm of a multiprocessor OS distributes load to optimize for some system wide performance metric. The algorithm typically incorporates knowledge about various system parameters, such as processor speed, cache sizes, and cache line migration (a) Five virtual machines running on four physical processors.</p><formula xml:id="formula_2">A A A B C C D D D D E E E p 0 p 1 p 2 p 3</formula><p>(b) Processing time for each processor is apportioned evenly among all virtual machines running on that processor.</p><formula xml:id="formula_3">A A A B C C D D D D E E E p 0 p 1 p 2 p 3</formula><p>(c) Balloon processes are started to even out the differences in processor execution speed as seen by the virtual machines. Rather than consuming CPU cycles, the balloon processes donate their CPU time to other virtual machines.</p><p>Figure <ref type="figure">5</ref>. Time ballooning costs. It furthermore tries to perform a reasonable prediction about future workloads incorporating previous workload patterns.</p><p>By multiplexing multiple virtual processors on one physical CPU the virtualization layer modifies the fundamental system parameters which are used to derive the process distribution, e.g., linear time, identical processor speeds and assumptions about cache working sets due to scheduling order. Based on its wrong assumptions the guest OS's scheduling algorithm will distribute processes among physical processors in a sub-optimal way leading to over-commitment or under-utilization</p><p>To perform correct scheduling decisions the guest scheduler has to be made aware of the virtualization layer and incorporate the additional system parameters. Instead of distributing load equally between all processors it should distribute based on the percentage of physical resource allocation.</p><p>Using ideas similar to memory ballooning in the VMware ESX Server <ref type="bibr" target="#b25">[25]</ref>, we propose a new mechanism, time ballooning, that enables the hypervisor to partake in the load balancing decisions without requiring modification of the scheduling code itself. A balloon module is loaded into the guest OS as a pseudo-device driver. Periodically, the balloon module polls the virtualization layer to determine how much processing time the virtual machine is allotted on the different CPUs. It then generates virtual workload making the guest OS believe that it is busy executing a process during the time of no physical CPU allocation. The virtual workload levels out imbalances of physical processor allocation (see Figure <ref type="figure">5</ref> (c)). The time balloon is used to correct the scheduler's assumption that all processors have the same processing speed, leading to the anticipated distribution.</p><p>The method for generating virtual workload depends on the specific load balancing scheme of the guest OS and cannot be generalized. We investigated two Linux scheduling algorithms, the latest, better-scalable O(1) scheduler and the original, sampling-based work stealing algorithm. Since our focus was on non-intrusive solutions, we explicitly avoided modifications of the guest OS' scheduling code that may have resulted in a more efficient solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Linux O(1) Scheduler</head><p>Linux's O(1) scheduler uses processor local run queues. Linux bases load balancing decisions on two parameters, the run queue length, using the minimum of two sampling points, and cache working-set estimates based on last execution time. When the balancing algorithm finds a length difference of more than 25% between the current and the busiest run queue, or when the current processor falls idle, it starts migrating processes until the imbalance falls below the 25% threshold.</p><p>To reflect differences in virtual machine CPU resource allocations, the balloon module has to manipulate the run queue length. This can be achieved nonintrusively by generating low priority virtual processes with a fixed CPU affinity. In the balloon module we calculate the optimal load distribution using the total number of ready-to-run processes and the allocated CPU share for each virtual CPU. Based on the calculated optimal distribution we add balloon processes to each run queue until all have equal length to the longest run queue, i.e., the virtual CPU with the largest physical processor allocation.</p><p>For a guest OS with n virtual CPUs, a total of p run-ning processes, and a CPU specific processor share s cpu , the number of balloon processes b on a particular virtual processor is</p><formula xml:id="formula_4">b cpu = max(s) -s cpu n i=1 s i • p</formula><p>Rounding up to the next full balloon results in at least one balloon process on all but those virtual CPUs with the largest s cpu and thereby avoids aggressive rebalancing towards slower virtual CPUs that fall idle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Linux Work-Stealing Scheduler</head><p>The second scheduling algorithm, the work-stealing algorithm, migrates processes under two conditions. The first condition responds to newly runnable processes. When a processor P 1 must schedule a newly woken task, it can suggest to idle processor P 2 to acquire the task. The migration completes only when P 2 chooses to execute the task. In the second migration condition, which takes place during general scheduling events such as end-of-timeslice, a processor can choose to steal any process from the centralized task list which is not hardbound to another processor.</p><p>Linux bases load balancing decisions purely on the characteristics of a process, calculated as a "goodness" factor. The decision to steal a process is independent of the status of other processors, and thus doesn't consider factors such as the number of processes associated with other processors.</p><p>To influence load distribution, a balloon module has to give the impression that while the virtual processor is preempted it is not idle (i.e., it is executing a virtual process), to avoid migration attempts from other processors. The module could add balloon threads to the task list, bound to a particular processor, and which yield the virtual machine to the hypervisor when scheduled. But the module is unable to guarantee that Linux will schedule the balloon threads at appropriate times. The likelihood of scheduling balloon tasks can be increased by adjusting their priorities.</p><p>An alternative to balloon processes is possible. The work-stealing algorithm stores the inventory of running tasks in a central list, and thus if these tasks possess the property of cache affinity, then their task structures are likely to possess a field to represent preferred processor affinity (as is the case for Linux). The balloon module can periodically calculate an ideal load distribution plan, and update the tasks' preferred processor. Thus, as processors perform scheduling decisions, they'll find jobs in the task list biased according to feedback from the hypervisor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Virtualization Architecture</head><p>For our experiments we used a paravirtualization approach running a modified Linux 2.4.21 kernel on top of a microkernel-based hypervisor <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b23">23]</ref>. With our approach the management and device access parts of the hypervisor run unprivileged in user-space; interaction with the virtual machine manager, including device access, takes place via the microkernel's inter-process communication (IPC) facility.</p><p>We modified the Linux kernel to utilize the hypervisor's virtual memory management and scheduling primitives. All device access was wrapped into virtual device drivers that communicate with the real device drivers in the hypervisor.</p><p>The hypervisor supports all core device classes: hard disk, Gigabit Ethernet, and text console. Furthermore, it manages memory and time allocation for all virtual machines. To reduce device virtualization overhead we export optimized device interfaces using shared memory communication and IPC.</p><p>Currently, the hypervisor partitions memory and processors statically, i.e., no paging of virtual machines is taking place and virtual processors do not migrate between physical CPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Lock Modeling</head><p>Our paravirtualization approach permitted us to reimplement the Linux kernel locks to study the benefits of intrusive lock-holder preemption avoidance. We implemented delayed preemption locks, pessimistic yield locks, and optimistic yield locks (with brief spinning).</p><p>The delayed preemption locks were constructed to inhibit preemption whenever at least one lock was held. Each virtual CPU (VCPU) maintained a count of acquired locks. Upon acquiring its first lock, a VCPU enabled its delayed preemption flag to prevent preemption by the hypervisor. The flag was cleared only after all locks were released. Setting and clearing the flag was a low cost operation, and only involved manipulating a bit in a page shared between the hypervisor and the VCPU. If the hypervisor signaled that it actually delayed a preemption, via another bit in the shared page, then the Linux kernel would voluntarily release its time slice immediately after releasing its last lock.</p><p>The yield locks were pessimistic and assumed that any spinning was due to a preempted lock, thereby immediately yielding the time slice if unable to acquire the lock. We also used an optimistic yield lock, which first spun on the lock for 20µs (as suggested from lock holding times in Figure <ref type="figure">2</ref>), and then yielded the time slice with the assumption that the lock was preempted.  To model the performance of lock-holder preemption avoidance with a fully virtualized virtual machine, and thus an unmodified guest OS, we had to create estimates using our paravirtualization approach. Given that paravirtualization can outperform a fully virtualized VM <ref type="bibr" target="#b2">[3]</ref>, our results only estimate the real benefits of lock preemption avoidance in a fully virtualized VM. Our safestate implementation precisely models the behavior we describe in Section 3.2. We model safe-state detection by using the hypervisor's delayed preemption flag. When the guest OS executes kernel code we enable the delayed preemption flag, thus compelling the hypervisor to avoid preemption at the end of normal time slice. After finishing kernel activity (upon resuming user code or entering the idle loop), we clear the preemption flag. If Linux kernel code exceeds a one millisecond preemption grace period, it is preempted.</p><p>We observed spin-lock performance by using standard Linux spin-locks. The spin-lock data apply to the case of using spin-locks for paravirtualization, and they approximate the case of a fully virtualized VM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Execution Environment</head><p>Experiments were performed with a Dell PowerEdge 6400, configured with four Pentium III 700 MHz Xeon processors, and an Intel Gigabit Ethernet card (using an 82540 controller). Memory was statically allocated, with 256 MB for the hypervisor, and 256 MB per virtual machine.</p><p>The guest operating system was a minimal Debian 3.0, based on a modified Linux 2.4.21 kernel. Most Debian services were disabled.</p><p>The hard disk was unused. Instead, all Linux instances utilized a 64 MB RAM disk. Linux 2.4.21 intelligently integrates the RAM disk with the buffer cache which makes this setup comparable to a hot buffer cache scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Synthesized Web Benchmark</head><p>The synthesized web benchmark was crafted to tax the Linux network and VFS subsystems, in effect, to emulate a web server under stress. It used Apache 2, for its support of the Linux sendfile system call. The sendfile method not only offloads application processing to the kernel, but it also supports network device hardware acceleration for outbound checksum calculations and packet linearization.</p><p>The benchmark utilized two virtual machines, each hosting a web server to serve static files. The two virtual machines competed for the network device, and each had an equal amount of processor time. A third virtual machine provided an adjustable processor load, to absorb processor time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Lock-Holder Preemption Avoidance Data</head><p>For studying the effect of our lock-holder preemption schemes, we instrumented Linux's synchronization primitives, and measured the throughput of the synthetic web benchmark described in the previous section.</p><p>To capture lock scaling in terms of number of processors, the Linux instances were configured to activate two, three, or four processors of our test system. Further, for each processor configuration we configured the virtual machines hosting the web servers to consume 15%, 20%, 25% or 33% of the total CPU time. Figure <ref type="figure">6</ref> summarizes our results, showing the aggregate bandwidth of both web servers for the different virtual machine and locking scheme configurations.</p><p>The results often reveal a substantial performance difference between the various locking techniques. In order  <ref type="figure">6</ref>). The bars show the extended lock-holding time for one of the web server VMs, per processor, expressed as a percentage of the run time. An extended lock-hold is one which exceeds 1 ms.  <ref type="figure">6</ref>). The bars show the extended time spent waiting for locks on one of the web server VMs, per processor, expressed as a percentage of the run time. An extended lock-wait is one which exceeds 1 ms.</p><p>to explain these differences, we measured lock holding times in each experiment. More precisely, we measured the amount of wall-clock time that a lock was held by the virtual machine running one of the web servers. Figure <ref type="figure">7</ref> shows the extended lock-holding time as a percentage of the virtual machine's real execution time during the benchmark. To distinguish between normal locking activity and lock-preemption activity, we show only extended lock-holding time. A lock-hold time is considered extended if it exceeds one millisecond. We also measured the total time spent acquiring locks in each of the experiments (i.e., the lock spin-time or wait-time). These lock-acquire times are presented in Figure <ref type="figure">8</ref>. Again, the time is measured relative to the real execution time of the benchmark. The data include only wait-times which exceeded one millisecond, to distinguish between normal behavior and lock-wait activity due to preempted locks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Time Ballooning</head><p>Figure <ref type="figure" target="#fig_5">9</ref> presents the result of an experiment designed to show the effectiveness of the O(1) time ballooning algorithm with asymmetric, static processor allocation. We ran two virtual machines, A and B, each configured to run on two physical processors. The goal was to fairly distribute physical processor time between all processes. Both VMs were running a Linux 2.4.21 kernel with the O(1) scheduler patch and our balloon module. Virtual machine A was configured with 30% of the processing time and ran ten CPU intensive tasks. Virtual machine B was configured with the remaining 70% of the processing time, but only ran one CPU intensive task for a short period of time; the other CPU was idle.</p><p>Before VM B started using its processing time, VM A used all processing time on both CPUs. Once VM B started processing (at about 7 seconds into the experiment), virtual machine A's processing share on CPU 0 immediately dropped to 30%. Shortly thereafter, VM A detected the imbalance in processing time, and attempted to mitigate the problem by inserting balloon processes on the slower CPU. This in turn caused Linux to start migrating processes to the other CPU.</p><p>When virtual machine B ceased using its processing time (at about 35 seconds into the experiment), virtual machine A again got the full CPU share, causing the balloon processes to be removed, and Linux to perform another load-balancing.</p><p>6 Discussion and Future Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Lock-holder Preemption Behavior</head><p>The web server benchmark provided complicated system dynamics. The processor utilization at almost every data point fell below the allocated processor share (we used a 5 ms time slice), suggesting that the workload was I/O-bound. On the other hand, the bandwidth is (non-linearly) proportional to the amount of processor share, suggesting a processor-bound workload. The sequencing of virtual machine scheduling, compared to packet arrival and transmit periods, probably causes this behavior. When a guest OS has access to the physical processor it won't necessarily be saturated with network events, since the network card may be sending and receiving packets for a competing virtual machine. Other factors can also regulate web server performance. For example, when a virtual machine sees little concurrent access to both the network device and the processor, the role of kernel buffers and queues becomes important.</p><p>The buffers and queues must hold data until a resource is available, but buffer overruns lead to work underflow, and thus under-utilization of the time slice. The sequencing of events may also lead to high packet processing latency, causing TCP/IP packet retransmits, which increases the load on the limited resources.</p><p>During the experiments involving our lock-holder preemption avoidance techniques we set the size of the preemption window (the value of w in Section 3.3) to 1 ms. Since more than 96% of the unsafe times for an Apache 2 workload fall below the 1 ms boundary (see Figure <ref type="figure" target="#fig_3">3</ref>), the non-intrusive technique did not suffer from excessive amounts of lock-holder preemptions. As such, the web-server throughput achieved with the nonintrusive technique was on par with the intrusive technique.</p><p>The hypervisor uses a proportional share stridescheduling algorithm <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b27">27]</ref>, which ensures that every virtual machine receives its allocated share of the processor. However, the scheduler doesn't impose a ceiling on a virtual machine's processor utilization. If one virtual machine under-utilizes its processor share for a given unit of time, then another virtual machine can pilfer the unused resource, thus increasing its share. With I/O bound workloads, we witness processor stealing (because we have time slice under-utilization). In essence, the scheduling algorithm permits virtual machines to rearrange their time slices. The concept of time-stealing also applies to yield-style locks. When a virtual processor yields its time slice, rather than wait on a lockacquire, it may avoid many milliseconds of spinning. Yielding the time slice, though, enables the hypervisor to schedule another virtual machine sooner, and thus return to scheduling the yielding virtual machine earlier, at which point the lock should be released from the original lock holder. Time slice yielding, as an alternative to spinning on preempted locks, will improve the overall efficiency of the system, as it avoids processor waste. And via the rearrangement of time slices, yield-style locks may improve their processor/packet sequencing, and improve performance.</p><p>Increasing parallelism by adding more processors increases the likelihood of lock contention. Thus preemption of a lock holder can lead to spinning by more processors. As Figure <ref type="figure">8</ref> shows, the lock-wait time increases with the number of processors.</p><p>In the graphs which represent lock-holding time (Figure <ref type="figure">7</ref>) and lock-waiting time (Figure <ref type="figure">8</ref>), the mechanisms which avoid lock-holder preemption (delayedpreemption locks and safe-state detection) have under 1% time involved with locking activity. In contradistinction, the spin-locks lead to severe locking times and spinning times due to preempted locks. The yield-style locks are susceptible to lock preemption, as they only focus on recovering from lock spinning, and also suffer from long lock holding times. And due to releasing their time slices for use by other virtual machines, yieldstyle locks suffer from long lock-wait times as well. The tactic of avoiding spin-time by yielding doesn't help the benchmark results, and often leads to performance worse than spin-locks. Spin-locks may spin for a long time, but they have a chance of acquiring the lock before the end of time slice, and thus to continue using the time slice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Time Ballooning</head><p>Our time ballooning mechanism is targeted towards processor allocations with infrequent changes. Highly dynamic processor reconfigurations introduce load-dependent properties, and are a topic of future work. To support dynamic load balancing in response to adjustments of the processor allocations requires attention to several variables, such as the rates at which the guest OS can rebalance and migrate tasks, allocation sampling rates, sampling overhead, cache migration costs, sample history window size, allocation prediction, and attenuation of thrashing. Likewise, response to burst loads is another dynamic situation for future work. For example, to achieve optimal use of the available physical processor resources, web servers distributed across asymmetric processors may require load balancing of incoming wake-up requests, or load balancing of the web server processes after they wake/spawn.</p><p>The time ballooning mechanism is designed to be installed in unmodified guest operating systems via device drivers. Where it is possible to modify the guest operating system, one can construct feedback scheduling algorithms optimized for the OS and workload and virtualization environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Dealing with Lock-Holder Preemption</head><p>An alternative to avoiding preemption of lock holders in the first place is to deal with the effects of the preemption: Have the hypervisor detect extensive spinning and schedule other, more useful work instead.</p><p>Techniques to detect spinning include instruction pointer sampling and trapping on instructions that are used in the back-off code of spin-locks. Descheduling a virtual CPU immediately after a failed acquire operation is expected to show the same behavior as pessimistic yield locks. To reduce the yield time, one could look for a lock release operation following a failed lock acquire. Lock release operations can be detected by writeprotecting pages containing a lock, using debug registers to observe write accesses, or, with additional hardware support, through extensions of the cache snooping mechanism.</p><p>However, preempting a virtual CPU due to a remote release operation on a monitored lock may preempt another lock holder. Most likely, release operations on a contended lock occur with much higher frequency than preemptions due to regular VM scheduling. Instead of offering a solution, chances are that this approach could amplify the problem. The potential costs and complexity of detecting spinning on a lock and executing another virtual CPU instead could outweigh the benefits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Coscheduling Dependent Workloads</head><p>In this paper we have discussed application workloads that do not possess strong cross-processor scheduling requirements. However, some workloads in the parallel application domain do rely on spin-based synchronization barriers or application spin-locks, and thus necessitate some form of coscheduling in order to perform efficiently. Coscheduling can only be achieved on physical resources (processors and time), and the coscheduling requirements in the guest OS must therefore be communicated to the virtualization layer so that they can be processed on a physical processor basis.</p><p>Making the virtualization layer aware of an application's coscheduling requirements and devising algorithms for fulfilling these requirements is future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>The problems that may result from preempting parts of a parallel application while holding a lock are well-known <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b30">30]</ref> and have been addressed by several researchers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b29">29]</ref>. Proposed solutions include variants of scheduler-aware synchronization mechanisms and require kernel extensions to share scheduling information between the kernel's scheduler and the applications. This translates well to our paravirtualization approach where the guest OS kernel provides information to the hypervisor. To our knowledge no prior work has applied this technique in the context of virtual machines and their guest OSs.</p><p>Only very few virtual machine environments offer multiprocessor virtual machines, and they either avoid lock-holder preemption completely through strict use of gang scheduling <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref>, use gang scheduling whenever they see fit <ref type="bibr" target="#b24">[24]</ref>, or they choose flexible scheduling but don't address lock-holder preemption <ref type="bibr" target="#b15">[15]</ref>.</p><p>Load balancing across non-uniformly clocked, but otherwise identical CPUs is a standard technique in cluster systems. However, we know of no commodity operating system for tightly coupled multiprocessors that would explicitly support such a configuration. The few existing virtual multiprocessor VM environments either implicitly enforce equal virtual CPU speeds (through gang scheduling) or do not consider speed imbalances. As such, we know of no prior art where a virtual machine environment would coerce its guest operating system to adapt to changing, or at least statically different, virtual CPU speeds. Our solution was inspired by the ballooning technique for reclaiming memory from a guest OS <ref type="bibr" target="#b25">[25]</ref>.</p><p>Performance isolation is a quite well-researched principle <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">18]</ref> in the server consolidation field. For CPU time our hypervisor enforces resource isolation using a proportional share stride-scheduling algorithm <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b27">27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>Virtual machine based server consolidation on top of multiprocessor systems promises great flexibility with respect to application workloads, while providing strong performance isolation guarantees among the consolidated servers. However, coupling virtual machine environments with multiprocessor systems raises a number of problems that we have addressed in this paper.</p><p>First, our schemes for avoiding preemption of lockholders in the guest operating systems prevents excessive spinning times on kernel locks, resulting in noticeable performance improvements for workloads exhibiting high locking activity. Our lock-holder preemption avoidance techniques are applicable to both paravirtualized and fully virtualized systems.</p><p>Second, the allocation of physical processing time to virtual machines can result in virtual machines experiencing asymmetric and varying CPU speeds. Our time ballooning technique solves the problem by creating artificial load on slower CPUs, causing the guest OS to migrate processes to the CPUs with more processing time.</p><p>Combined, our solutions enable scalable multiprocessor performance with flexible virtual processor scheduling.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 Figure 1 .</head><label>21</label><figDesc>Figure 1. Locked and unsafe times for three different locking scenarios. Povray spends most of the time executing at user-level. Linux kernel build over NFS spends a considerable amount of time at user-level, and moderately stresses the VFS and network layer of the kernel. Apache 2 utilizes the sendfile system call which offloads large amounts of work to the kernel itself.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 Figure 2 .</head><label>22</label><figDesc>Figure 2. Lock-hold and unsafe state time distributions for three different locking scenarios. Histograms are for readability reasons truncated to a probability of 20%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Virtual machine scheduling. A window of size w indicates when a VM may be preempted. E n indicates the n th end of time slice, P n indicates the n th actual preemption point, and s n indicates the start of the n th preemption window.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>p 1 p 2 p 3</head><label>3</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Time ballooning induced load balancing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Lock-holding times for various Linux workloads. Hold times are measured while at least one kernel lock is being held by the CPU.</figDesc><table><row><cell></cell><cell cols="3">Povray Kbuild Apache 2</cell></row><row><cell>total</cell><cell cols="2">0.04% 15.3%</cell><cell>39.2%</cell></row><row><cell>average</cell><cell>3.0µs</cell><cell>1.8µs</cell><cell>2.2µs</cell></row><row><cell>max</cell><cell cols="2">103µs 293µs</cell><cell>473µs</cell></row><row><cell>std.dev.</cell><cell>5.3µs</cell><cell>6.7µs</cell><cell>7.7µs</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Lock-hold and unsafe times</figDesc><table><row><cell></cell><cell>Povray</cell><cell cols="2">Kbuild Apache 2</cell></row><row><cell>Locked:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>total</cell><cell cols="2">0.04% 15.3%</cell><cell>39.2%</cell></row><row><cell>average</cell><cell>3.0µs</cell><cell>1.8µs</cell><cell>2.2µs</cell></row><row><cell>max</cell><cell cols="2">103µs 293µs</cell><cell>473µs</cell></row><row><cell>std.dev.</cell><cell>5.3µs</cell><cell>6.7µs</cell><cell>7.7µs</cell></row><row><cell>Unsafe:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>total</cell><cell cols="2">0.09% 26.6%</cell><cell>98.9%</cell></row><row><cell>average</cell><cell cols="2">6.9µs 17.8µs</cell><cell>1.4ms</cell></row><row><cell>max</cell><cell>1.4ms</cell><cell>2.0ms</cell><cell>47.6ms</cell></row><row><cell cols="3">std.dev. 28.7µs 52.4µs</cell><cell>7.5ms</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Figure 6. Bandwidth measurement for Apache 2 benchmark. Two virtual machines are configured to run on 2, 3 and 4 CPUs. For each CPU configuration the virtual machines are further configured to consume 15%, 20%, 25% or 33% of the total CPU time. A third virtual machine with a CPU intensive workload consumes the remaining CPU time. The bars show the aggregate bandwidth of the two VMs running the web servers.</figDesc><table><row><cell></cell><cell>15% CPU time</cell><cell>20% CPU time</cell><cell>25% CPU time</cell><cell>33% CPU time</cell><cell></cell></row><row><cell>Agg. Bandwidth (KB/s)</cell><cell>per Apache VM</cell><cell>per Apache VM</cell><cell>per Apache VM</cell><cell>per Apache VM</cell><cell>4 CPUs 3 CPUs 2 CPUs</cell></row><row><cell></cell><cell>a t e p e s s . y i e l d o p t . y i e l d</cell><cell>s a f e s t a t e p e s s . y i e l d o p t . y i e l d</cell><cell>s a f e s t a t e p e s s . y i e l d o p t . y i e l d</cell><cell>s a f e s t a t e p e s s . y i e l d o p t . y i e l d</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The mechanism used for monitoring depends heavily on the hardware architecture emulated and the virtualization approach used. One can for instance catch privileged instructions, or insert monitoring code where the hypervisor adds or removes protection for the guest OS kernel memory.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Åge Kvalnes for his insightful feedback. Further thanks to the anonymous reviewers for their valuable comments. The work presented in this paper was in part funded through a grant by Intel Corporation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scheduler Activations: Effective Kernel Support for the User-Level Management of Parallelism</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">E</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">N</forename><surname>Bershad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">D</forename><surname>Lazowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><forename type="middle">M</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="79" />
			<date type="published" when="1992-02">February 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Resource containers: A new facility for resource management in server systems</title>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Banga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Druschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">C</forename><surname>Mogul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 3rd Symposium on Operating Systems Design and Implementation</title>
		<meeting>of the 3rd Symposium on Operating Systems Design and Implementation<address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">February 22-25 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Xen and the art of virtualization</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Dragovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keir</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rolf</forename><surname>Neugebauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Warfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 19th ACM Symposium on Operating Systems Principles</title>
		<meeting>of the 19th ACM Symposium on Operating Systems Principles<address><addrLine>Bolton Landing, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">October 19-22 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scheduling Support for Concurrency and Parallelism in the Mach Operating System</title>
		<author>
			<persName><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="35" to="43" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Disco: Running commodity operating systems on scalable multiprocessors</title>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Bugnion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Devine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mendel</forename><surname>Rosenblum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 16th Symposium on Operating Systems Principles</title>
		<meeting>of the 16th Symposium on Operating Systems Principles<address><addrLine>Saint Malo, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-08">October 5-8 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A caching model of operating system kernel functionality</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Cheriton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><forename type="middle">J</forename><surname>Duda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 1st Symposium on Operating Systems Design and Implementation</title>
		<meeting>of the 1st Symposium on Operating Systems Design and Implementation<address><addrLine>Monterey, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">November 14-17 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Virtual PC for Windows, Version 5.1, User Guide, second edition</title>
		<author>
			<persName><surname>Connectix</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002-07">July 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Process management for highly parallel UNIX systems</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Edler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jim</forename><surname>Lipkis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edith</forename><surname>Schonberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the USENIX Workshop on Unix and Supercomputers</title>
		<meeting>of the USENIX Workshop on Unix and Supercomputers<address><addrLine>Pittsburg, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988">September 26-27 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Survey of virtual machine research</title>
		<author>
			<persName><forename type="first">P</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Magazine</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="34" to="45" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cellular disco: resource management using virtual clusters on shared-memory multiprocessors</title>
		<author>
			<persName><forename type="first">Kingshuk</forename><surname>Govil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Teodosiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mendel</forename><surname>Rosenblum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="262" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The synergy between non-blocking synchronization and operating system structure</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Greenwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Cheriton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2nd Symposium on Operating Systems Design and Implementation</title>
		<meeting>of the 2nd Symposium on Operating Systems Design and Implementation<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">October 28-31 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">User-level management of kernel memory</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Haeberlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Elphinstone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 8th</title>
		<meeting>of the 8th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Aizu-Wakamatsu City</title>
		<imprint>
			<date type="published" when="2003">September 23-26 2003</date>
			<pubPlace>Japan</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Asia-Pacific Computer Systems Architecture Conference</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The performance of microkernel-based systems</title>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Härtig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hohmuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jochen</forename><surname>Liedtke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Schönberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 16th ACM Symposium on Operating System Principles</title>
		<meeting>of the 16th ACM Symposium on Operating System Principles<address><addrLine>Saint Malo, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-08">October 5-8 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pragmatic nonblocking synchronization for Real-Time systems</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hohmuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Härtig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2001 USENIX Annual Technical Conference</title>
		<meeting>of the 2001 USENIX Annual Technical Conference</meeting>
		<imprint>
			<date type="published" when="2001">June 25-30 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ibm</forename><surname>Vm Performance</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003-08">August 2003</date>
		</imprint>
	</monogr>
	<note>Version 4 Release 4.0, fourth edition</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Empirical studies of competitive spinning for a shared-memory multiprocessor</title>
		<author>
			<persName><forename type="first">Anna</forename><forename type="middle">R</forename><surname>Karlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">S</forename><surname>Manasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Owicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 13th ACM Symposium on Operating Systems Principles</title>
		<meeting>of the 13th ACM Symposium on Operating Systems Principles<address><addrLine>Pacific Grove, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="13" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scheduler-conscious synchronization</title>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">I</forename><surname>Kontothanassis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">W</forename><surname>Wisniewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="40" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Vortex: an event-driven multiprocessor operating system supporting performance isolation</title>
		<author>
			<persName><forename type="first">Åge</forename><surname>Kvalnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dag</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robbert</forename><surname>Van Renesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Audun</forename><surname>Arnesen</surname></persName>
		</author>
		<idno>2003-45</idno>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>University of Tromsø</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A lock-free multiprocessor OS kernel</title>
		<author>
			<persName><forename type="first">Henry</forename><surname>Massalin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calton</forename><surname>Pu</surname></persName>
		</author>
		<idno>CUCS-005-91</idno>
		<imprint>
			<date type="published" when="1991">1991</date>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Columbia University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Relative performance of preemption-safe locking and non-blocking synchronization on multiprogrammed shared memory multiprocessors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Maged</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 11th International Parallel Processing Symposium</title>
		<meeting>of the 11th International Parallel essing Symposium<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-05">April 1-5 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scheduling techniques for concurrent systems</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">K</forename><surname>Ousterhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 3rd International Conference on Distributed Computing Systems</title>
		<meeting>of the 3rd International Conference on Distributed Computing Systems<address><addrLine>Ft. Lauderdale, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1982">October 18-22 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Virtualizing I/O devices on VMware workstation&apos;s hosted virtual machine monitor</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Sugerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Venkitachalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beng-Hong</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2001 USENIX Annual Technical Conference</title>
		<meeting>of the 2001 USENIX Annual Technical Conference<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">June 25-30 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m">The L4Ka::Pistachio microkernel. White paper</title>
		<imprint>
			<date type="published" when="2003-05-01">May 1 2003</date>
		</imprint>
		<respStmt>
			<orgName>System Architecture Group ; Universität Karlsruhe</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<ptr target="http://vmware.com/" />
		<title level="m">VMware ESX Server online documentation</title>
		<imprint>
			<publisher>VMware Inc</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Memory resource management in VMware ESX Server</title>
		<author>
			<persName><forename type="first">Carl</forename><forename type="middle">A</forename><surname>Waldsburger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 5th Symposium on Operating Systems Design and Implementation</title>
		<meeting>of the 5th Symposium on Operating Systems Design and Implementation<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-11">December 9-11 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lottery scheduling: Flexible proportional-share resource management</title>
		<author>
			<persName><forename type="first">Carl</forename><forename type="middle">A</forename><surname>Waldspurger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">E</forename><surname>Weihl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the First Symposium on Operating Systems Design and Implementation</title>
		<meeting>of the First Symposium on Operating Systems Design and Implementation<address><addrLine>Monterey, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">November 14-17 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Stride scheduling: Deterministic proportional-share resource management</title>
		<author>
			<persName><forename type="first">Carl</forename><forename type="middle">A</forename><surname>Waldspurger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">E</forename><surname>Weihl</surname></persName>
		</author>
		<idno>MIT/LCS/TM-528</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>MIT Laboratory for Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scale and Performance in the Denali Isolation Kernel</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Whitaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marianne</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">D</forename><surname>Gribble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 5th Symposium on Operating Systems Design and Implementation</title>
		<meeting>of the 5th Symposium on Operating Systems Design and Implementation<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-11">December 9-11 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">High Performance Synchronization Algorithms for Multiprogrammed Multiprocessors</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">W</forename><surname>Wisniewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">I</forename><surname>Kontothanassis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</title>
		<meeting>5th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming<address><addrLine>Santa Barbara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">July 19-21 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The effect of scheduling discipline on spin overhead in shared memory parallel systems</title>
		<author>
			<persName><forename type="first">John</forename><surname>Zahorjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">D</forename><surname>Lazowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">L</forename><surname>Eager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="180" to="198" />
			<date type="published" when="1991-04">April 1991</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
