<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revisiting Approximate Metric Optimization in the Age of Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sebastian</forename><surname>Bruch</surname></persName>
							<email>bruch@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Masrour</forename><surname>Zoghi</surname></persName>
							<email>mzoghi@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
							<email>najork@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Revisiting Approximate Metric Optimization in the Age of Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3331184.3331347</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Direct Ranking Metric Optimization</term>
					<term>Deep Neural Networks for IR</term>
					<term>Learning to Rank</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning-to-Rank is a branch of supervised machine learning that seeks to produce an ordering of a list of items such that the utility of the ranked list is maximized. Unlike most machine learning techniques, however, the objective cannot be directly optimized using gradient descent methods as it is either discontinuous or flat everywhere. As such, learning-to-rank methods often optimize a loss function that either is loosely related to or upper-bounds a ranking utility instead. A notable exception is the approximation framework originally proposed by Qin et al. [14]  that facilitates a more direct approach to ranking metric optimization. We revisit that framework almost a decade later in light of recent advances in neural networks and demonstrate its superiority empirically. Through this study, we hope to show that the ideas from that work are more relevant than ever and can lay the foundation of learningto-rank research in the age of deep neural networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Given the LTR task and the corresponding evaluation metrics, the first proposal would be to train a LTR model by directly optimizing a metric like NDCG. As explained in Section 2, it is known, however, that ranking metrics including NDCG are non-differentiable and therefore impossible to optimize using gradient descent methods. Moreover, in the regions where the metrics are smooth, infinitesimal perturbations of our model parameters will almost surely leave the ranked list unperturbed, which in turn implies that whatever gradients we compute will be identically zero almost everywhere. Faced with this stumbling block, the LTR community has produced a plethora of schemes to improve metrics like NDCG, including metric smoothing methods such as SoftRank <ref type="bibr" target="#b14">[15]</ref> and indirect boosting methods like LambdaMART <ref type="bibr" target="#b16">[17]</ref>.</p><p>A more direct approach to LTR metric optimization was proposed by Qin et al. <ref type="bibr" target="#b13">[14]</ref>, where the rank variable in the definition of metrics like NDCG was approximated by a sum of sigmoids, thereby allowing for gradient computations. However, this idea happened to be proposed at a time when tree-based LTR models were making great strides, as demonstrated for instance by LambdaMART's winning of the Yahoo! Learning to Rank Challenge <ref type="bibr" target="#b4">[5]</ref>, and since regression trees cannot be optimized globally, such differentiable approximations of the metrics offered no immediate advantage. Recent hardware and software advances in the training of neural networks, however, make the work in <ref type="bibr" target="#b13">[14]</ref> relevant again and potentially allow us to harvest the effectiveness and the scalability of deep neural networks in LTR.</p><p>In this paper, we make the following contribution: we demonstrate that directly optimizing NDCG, rather than a surrogate loss, using deep neural networks can give results that are comparable with those obtained using existing state-of-the-art LTR algorithms such as LambdaMART. We give an overview of LTR and in particular <ref type="bibr" target="#b13">[14]</ref> in Section 2. We discuss experimental results in Section 3 and conclude the paper in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK AND METHODOLOGY</head><p>In this section, we formulate the problem of LTR and provide an overview of the literature. We also provide a self-contained summary of the core idea behind the ApproxNDCG [14] method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview of Learning-to-Rank</head><p>LTR methods are supervised techniques and the story naturally begins with a description of the training set. Consider a set of training samples Ψ = {(x, y) ∈ X n × R n + }, where x is a vector of n items x i , 1 ≤ i ≤ n, y is a real vector of n nonnegative relevance labels y i , 1 ≤ i ≤ n, and X is the space of all items. Each item x i could generally take any form but throughout this paper we define it to be a vector of features representing a query-document pair. The objective is to learn a function that produces an ordering of items in any x in such a way that the utility of the ordered list is maximized.</p><p>Most LTR algorithms reformulate the problem to that of learning a scoring function that computes a score for every item. Scores usually represent relevance-for some notion of relevance-and induce an ordering of the items by sorting them in decreasing order of relevance to form a ranked list. As such, the goal of LTR often boils down to finding a parameterized ranking function f (•; Θ) : X n → R n , where Θ denotes the set of parameters, that minimizes the empirical loss:</p><formula xml:id="formula_0">L(f ) = 1 |Ψ| (x ,y)∈Ψ ℓ(y, f (x)),<label>(1)</label></formula><p>where ℓ(•) is a local loss function. The function f is often univariate and can be rewritten as follows:</p><formula xml:id="formula_1">f (x)| i = u(x i ), 1 ≤ i ≤ n,<label>(2)</label></formula><p>where f (•)| i denotes the i th dimension of f , and u : X → R computes a relevance score for each item independently of other items. LTR algorithms differ primarily in how they parameterize f and how they define ℓ. Tried and tested parameterization methods include linear functions <ref type="bibr" target="#b8">[9]</ref>, boosted weak learners <ref type="bibr" target="#b18">[19]</ref>, gradientboosted trees <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref>, support vector machines <ref type="bibr" target="#b8">[9]</ref>, and neural networks <ref type="bibr" target="#b1">[2]</ref>. In this paper, we model f using the latter.</p><p>The loss function, ℓ, is ideally derived from a utility of interest such as NDCG <ref type="bibr" target="#b7">[8]</ref>, a popular ranking metric. However, most ranking metrics are either discontinuous or flat. Take NDCG as an example:</p><formula xml:id="formula_2">NDCG(π f , y) = DCG(π f , y) DCG(π * , y) ,<label>(3)</label></formula><p>where π f is a ranked list induced by the ranking function f on x, π * is the ideal ranked list (where x is sorted by y), and DCG is defined as follows:</p><formula xml:id="formula_3">DCG(π , y) = n i=1 2 y i − 1 log 2 (1 + π (i)) ,<label>(4)</label></formula><p>where π (i) is the rank of x i . In Eq. 4, small perturbations of the scores would not change the ranks for generic scores, and therefore NDCG is locally constant almost everywhere. Also, when the item ranks do change, NDCG becomes discontinuous. The non-differentiability of ranking metrics has given rise to a body of research that attempts to find differentiable surrogate losses that either are loosely related to or upper-bound ranking metrics <ref type="bibr">[2-4, 9, 18]</ref>. There exist a few notable exceptions that attempt to directly maximize a ranking metric by using coordinate ascent <ref type="bibr" target="#b10">[11]</ref>, smoothing scores <ref type="bibr" target="#b14">[15]</ref>, boosting <ref type="bibr" target="#b18">[19]</ref>, and approximating the metric <ref type="bibr" target="#b13">[14]</ref>. It is the latter that can tightly bound any ranking metric such as NDCG <ref type="bibr" target="#b13">[14]</ref> and can be easily optimized with gradient descent.</p><p>Surprisingly, despite its attractive theoretical properties, the framework in <ref type="bibr" target="#b13">[14]</ref> has received little attention in LTR studies in the decade since the original publication. In this paper, we revisit that work in light of recent advances in deep neural networks and the availability of powerful optimizers. With significantly more computing power at our disposal today, we set out to study the hyperparameters of that work and reproduce experiments to optimize NDCG-referred to as ApproxNDCG. Our results show that  the theoretical guarantees in <ref type="bibr" target="#b13">[14]</ref> materialize in practice. Before we go any further, we give a brief overview of ApproxNDCG in the next section for completeness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Summary of ApproxNDCG</head><p>As shown in Equation <ref type="formula" target="#formula_3">4</ref>, to compute DCG, all that is required is the rank of items in the final ranked list as ordered by relevance scores. Moreover, the rank of an item i can be computed as follows:</p><formula xml:id="formula_4">π f (i) ≜ 1 + j i I f (x ) | i &lt;f (x ) | j ,<label>(5)</label></formula><p>where f (•) is the scoring function from Equation 2, and I s &lt;t is the indicator which is 1 if s &lt; t and 0 otherwise. <ref type="bibr">Qin et al.</ref> propose in <ref type="bibr" target="#b13">[14]</ref> a smooth approximation of Equation <ref type="formula" target="#formula_4">5</ref>where I is estimated by a sigmoid as follows:</p><formula xml:id="formula_5">I s &lt;t = I t −s &gt;0 ≈ σ (t − s) ≜ 1 1 + e −α (t −s) ,<label>(6)</label></formula><p>where α &gt; 0 is a knob that controls how tightly the sigmoid fits the indicator. As α becomes larger, σ approximates the indicator more closely as shown in Figure <ref type="figure" target="#fig_1">1</ref>.</p><p>Unlike the indicator function, the approximation in Equation 6 is smooth and differentiable. Plugging this approximation into Equation 4 yields ApproxNDCG, an approximation of NDCG. Because NDCG is a utility, we define the loss ℓ in Equation 1 to be negative ApproxNDCG and minimize the loss using gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>We are largely interested in two research questions alluded to earlier: (1) What is the impact of the hyperparameter α on the learned model? (2) Can directly optimizing the ranking metric with deeper networks and a much larger number of training iterations lead to higher quality models? In this section, we describe the experiments we designed to study those questions and analyze the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We conduct exhaustive experiments on two publicly available LTR datasets: MSLR-WEB30K <ref type="bibr" target="#b12">[13]</ref> and Yahoo! LTR Challenge <ref type="bibr" target="#b4">[5]</ref>. Both datasets contain roughly 30,000 queries. Web30K has an average of 120 documents per query, each represented by a vector of 136 numeric features. Yahoo! Set 1 has 24 documents per query and 519 features per document. Documents in both datasets are labeled with graded relevance from 0 to 4 with larger labels indicating a higher relevance. We report our findings on Fold 1 of Web30K and Set 1 of the Yahoo! dataset. It is important to note that in both datasets, queries with no relevant documents are discarded during evaluation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Models</head><p>We have compared our results with existing ranking models including ListMLE <ref type="bibr" target="#b17">[18]</ref>, RankNet and LambdaMART <ref type="bibr" target="#b2">[3]</ref>. To train LambdaMART models, we used the recent open-source Light-GBM <ref type="bibr" target="#b9">[10]</ref> implementation (denoted by λMART GBM ). We also used the legacy RankLib implementation (λMART RankLib ). We implemented ListMLE and RankNet in Tensorflow <ref type="bibr" target="#b0">[1]</ref>, a deep learning framework. In all of our experiments, we run 10 trials of each experiment and report mean metrics and 95% confidence intervals.</p><p>The hyperparameters for LambdaMART models are based on those reported in previous work (e.g., <ref type="bibr" target="#b15">[16]</ref>) and further fine-tuned on the validation set. Specifically, we train λMART RankLib models by setting the hyperparameter values as follows: number of leaves per tree to 10, learning rate to 0.1, minimum leaf support to 1. We were unable to train larger trees as larger parameter settings lead to a substantial and prohibitive rise in memory usage and training time. LightGBM, on the other hand, is an efficient implementation and as such we set the hyperparameters for λMART GBM as follows: learning rate is 0.1, number of leaves is 200, min_data_in_leaf is 50, and min_sum_hessian_in_leaf is set to 100. We use NDCG@5 as the main metric to select the best models on validation sets.</p><p>Our proposed method is a fully-connected feedforward network with ReLU activation (ReLU(t) = max(t, 0)) using ApproxNDCG as the loss function: henceforth, we will also use ApproxNDCG to refer to this type of model. The models are trained as follows: similar to baseline models, the hyperparameters of the ApproxNDCG models are selected based on NDCG@5 on the validation set; training batch size is set to 128; and we use a learning rate of 0.005. We further use batch normalization <ref type="bibr" target="#b6">[7]</ref> between consecutive layers, including over the input layer to, in effect, normalize input features. We describe the architecture of our networks in more detail in upcoming sections.</p><p>We have released our implementation of ApproxNDCG in Tensorflow in the open-source Tensorflow Ranking library <ref type="bibr" target="#b11">[12]</ref>. <ref type="foot" target="#foot_0">1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Effect of the Sigmoid Exponent</head><p>As stated earlier, the first factor we examine in this work is the effect of α in Equation <ref type="formula" target="#formula_5">6</ref>on the trained model. To that end, we  train networks with a single hidden layer to limit the parameter space, but use different values of α. Figure <ref type="figure" target="#fig_3">2</ref> illustrates the results on Web30K. Experiments on Yahoo! yield a similar trend.</p><p>The results are interesting but not surprising. When α is small, the sigmoid approximates the indicator function less accurately as shown in Figure <ref type="figure" target="#fig_1">1</ref>. As such, the model optimizes a loss that is only loosely related to NDCG. On the other hand, when α is too large, the sigmoid becomes flatter closer to the origin. As a result, the gradients tend to vanish which impedes learning. It appears, however, that a relatively small value of α offers a compromise between the two extremes: it is a close-enough approximation of the indicator while also enabling gradient descent.</p><p>Note that the difference between models trained using α ∈ {5, 10, 20, 40} is not statistically significant. We choose α = 10 as the configuration of choice in the remainder of this paper purely based on its relatively superior NDCG@5 on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Effect of Deeper Networks</head><p>Now that we have found an optimal value for α, we focus on the second question to study the effect of deeper networks on model quality. We start with a small network with 3 hidden layers with 64, 32, and 16 hidden units each. We refer to that as B64. We then construct progressively deeper models by adding layers that grow by a factor of 2. As an example, B128 will have 128 units in the first hidden layer, and 64, 32, and 16 units in subsequent layers. As stated earlier, layers are fully connected with batch normalization and ReLU nonlinearity in between.</p><p>Figure <ref type="figure" target="#fig_5">3</ref> plots model quality as measured by NDCG at various ranks on the Web30K validation set. Results on Yahoo! exhibit a similar trend. From Figure <ref type="figure" target="#fig_5">3</ref>, it is clear that deeper models generally lead to improved quality. We note that the differences in NDCG@1 are not statistically significant, but that adding more and wider layers yield NDCG@5 and NDCG@10 measurements that statistically significantly improve upon shallower networks. The largest network exhibits signs of overfitting but we note that no regularization was employed in these experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Comparison with Baseline Models</head><p>Based on the experiments conducted in previous sections, we use the B1024 model with α = 10 and compare its performance with  <ref type="table" target="#tab_0">1</ref>, we observe that ApproxNDCG significantly outperforms λMART RankLib on both datasets, but does not do as well as λMART GBM . Note that our NDCG measurements for λMART GBM are lower than those reported in previous work (e.g., <ref type="bibr" target="#b15">[16]</ref>). This is because LightGBM computes an NDCG of 1.0 for queries with no relevant documents. In this work, we exclude such queries from the evaluation set to facilitate a fair comparison of scores.</p><p>By comparing ApproxNDCG with ListMLE and RankNet, we conclude that the success of ApproxNDCG is not simply due to the use of deeper networks: the loss function itself is a more appropriate choice than the losses used in RankNet or ListMLE. We omit RankNet and ListMLE results on Yahoo! due to space constraints, but the findings are similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSION AND FUTURE WORK</head><p>Deep neural networks have enabled a significant leap forward in many applications of machine learning such as NLP and Image Processing. Our ability to train scalable deep networks that handle sparse features such as text are among the factors that place neural networks at the vanguard of machine learning research. Harvesting these abilities in LTR, however, remains a challenge due to the discontinuous nature of ranking utility functions.</p><p>In this work, we set out to revisit the work of Qin et al. <ref type="bibr" target="#b13">[14]</ref> which formulates a smooth approximation to any ranking metric such as NDCG. Unlike many other existing surrogate LTR losses, the framework in <ref type="bibr" target="#b13">[14]</ref> offers a way to directly optimize ranking metrics. Because the objective is differentiable, it is also a good fit for gradient descent algorithms.</p><p>We studied ApproxNDCG, an approximation to NDCG, and examined its hyperparameter. We demonstrated empirically that Ap-proxNDCG greatly benefits from deep network architectures and, despite the little attention it received in the LTR literature, is a competitive algorithm for ranking.</p><p>Through this study, we hope to convey that (a) it is not just plausible but more appropriate to directly optimize ranking metrics rather than loosely related surrogate losses; and (b) that the approximation framework in <ref type="bibr" target="#b13">[14]</ref> could lay out the foundation of deep neural networks in LTR. We wish to encourage research in this direction by open sourcing our implementation of ApproxNDCG in the Tensorflow Ranking library.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sigmoid approximation of the indicator function with different values of hyperparameter α.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Effect of the sigmoid exponent, α, on NDCG at different ranks on the Web30K validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Effect of depth on quality as measured by NDCG at different ranks on the Web30K validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>A comparison of ranking models on the Web30K and Yahoo! test sets.</figDesc><table><row><cell></cell><cell></cell><cell>Web30K</cell><cell></cell></row><row><cell>Model</cell><cell>NDCG@1</cell><cell>NDCG@5</cell><cell>NDCG@10</cell></row><row><cell>ListMLE</cell><cell cols="3">41.90 (±0.34) 42.56 (±0.20) 44.91 (±0.17)</cell></row><row><cell>RankNet</cell><cell cols="3">42.18 (±0.35) 43.23 (±0.14) 45.70 (±0.10)</cell></row><row><cell cols="4">λMART RankLib 45.35 (±0.06) 44.59 (±0.04) 46.46 (±0.03)</cell></row><row><cell>λMART GBM</cell><cell cols="3">50.33 (±0.22) 49.20 (±0.07) 51.05 (±0.02)</cell></row><row><cell>ApproxNDCG</cell><cell cols="3">46.64 (±0.22) 45.38 (±0.11) 47.31 (±0.10)</cell></row><row><cell></cell><cell></cell><cell>Yahoo! Set 1</cell><cell></cell></row><row><cell cols="4">λMART RankLib 68.52 (±0.09) 70.27 (±0.05) 74.58 (±0.05)</cell></row><row><cell>λMART GBM</cell><cell cols="3">72.07 (±0.22) 74.16 (±0.14) 78.40 (±0.10)</cell></row><row><cell>ApproxNDCG</cell><cell cols="3">69.63 (±0.17) 72.32 (±0.10) 76.77 (±0.06)</cell></row><row><cell cols="4">baseline methods on both Web30K and Yahoo! held-out test sets.</cell></row><row><cell cols="4">We further fine-tune the "momentum" hyperparameter of batch</cell></row><row><cell cols="4">normalization-used in the estimation of population statistics-and</cell></row><row><cell cols="4">set it to 0.8 and 0.99 in the Web30K and Yahoo! experiments respec-</cell></row><row><cell cols="4">tively. We use the same network architecture and hyperparameters</cell></row><row><cell cols="4">for ListMLE and RankNet methods to facilitate a fair comparison.</cell></row><row><cell cols="2">Table 1 summarizes our findings.</cell><cell></cell><cell></cell></row><row><cell>From Table</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Available at http://github.com/tensorflow/ranking</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ACKNOWLEDGEMENTS</head><p>This work would not be possible without the support provided by the TF-Ranking team. We thank Donald Metzler for his input on an early draft of this work, and the anonymous reviewers for their feedback. The first author's deepest gratitude goes to Katherine for her invaluable encouragement and wholehearted support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 12th USENIX Symposium on Operating Systems Design and Implementation</title>
				<meeting>of the 12th USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to rank using gradient descent</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erin</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Lazier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Deeds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Hullender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 22nd International Conference on Machine Learning</title>
				<meeting>of the 22nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">From RankNet to LambdaRank to LambdaMART: An Overview</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><surname>Burges</surname></persName>
		</author>
		<idno>MSR-TR-2010-82</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Microsoft Research</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report Technical Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to rank: from pairwise approach to listwise approach</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Feng</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 24th International Conference on Machine Learning</title>
				<meeting>of the 24th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Yahoo! learning to rank challenge overview</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Learning to Rank Challenge</title>
				<meeting>of the Learning to Rank Challenge</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Greedy function approximation: a gradient boosting machine</title>
		<author>
			<persName><surname>Jerome H Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 32nd International Conference on Machine Learning (ICML)</title>
				<meeting>of the 32nd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cumulated gain-based evaluation of IR techniques</title>
		<author>
			<persName><forename type="first">Kalervo</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaana</forename><surname>Kekäläinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Training linear SVMs in linear time</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">LightGBM: A Highly Efficient Gradient Boosting Decision Tree</title>
		<author>
			<persName><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Direct maximization of rank-based metrics for information retrieval</title>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Donald A Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CIIR report</title>
		<imprint>
			<biblScope unit="volume">429</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">TF-Ranking: Scalable TensorFlow Library for Learning-to-Rank</title>
		<author>
			<persName><forename type="first">Rama</forename><surname>Kumar Pasumarthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Bruch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Pfeifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Golbandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.2597</idno>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note>Introducing LETOR 4.0 Datasets.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A general approximation framework for direct optimization of information retrieval measures</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="375" to="397" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SoftRank: Optimizing Non-smooth Rank Metrics</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Guiver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 1st International Conference on Web Search and Data Mining</title>
				<meeting>of the 1st International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="77" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The LambdaLoss Framework for Ranking Metric Optimization</title>
		<author>
			<persName><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Golbandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 27th ACM International Conference on Information and Knowledge Management</title>
				<meeting>of the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1313" to="1322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adapting boosting for information retrieval measures</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krysta</forename><forename type="middle">M</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Svore</surname></persName>
		</author>
		<author>
			<persName><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="254" to="270" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Listwise approach to learning to rank: theory and algorithm</title>
		<author>
			<persName><forename type="first">Fen</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wensheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 25th International Conference on Machine Learning</title>
				<meeting>of the 25th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1192" to="1199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">AdaRank: A Boosting Algorithm for Information Retrieval</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="391" to="398" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
