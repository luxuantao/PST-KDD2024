<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BOND: BERT-Assisted Open-Domain Named Entity Recognition with Distant Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-28">28 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chen</forename><surname>Liang</surname></persName>
							<email>cliang73@gatech.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yue</forename><surname>Yu</surname></persName>
							<email>yueyu@gatech.edu</email>
						</author>
						<author>
							<persName><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
							<email>jianghm@gatech.edu</email>
						</author>
						<author>
							<persName><forename type="first">Siawpeng</forename><surname>Er</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ruijia</forename><surname>Wang</surname></persName>
							<email>rwang@gatech.edu</email>
						</author>
						<author>
							<persName><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
							<email>tourzhao@gatech.edu</email>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
							<email>chaozhang@gatech.edu</email>
						</author>
						<title level="a" type="main">BOND: BERT-Assisted Open-Domain Named Entity Recognition with Distant Supervision</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-28">28 Jun 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2006.15509v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the open-domain named entity recognition (NER) problem under distant supervision. The distant supervision, though does not require large amounts of manual annotations, yields highly incomplete and noisy distant labels via external knowledge bases. To address this challenge, we propose a new computational framework -BOND, which leverages the power of pre-trained language models (e.g., BERT and RoBERTa) to improve the prediction performance of NER models. Specifically, we propose a two-stage training algorithm: In the first stage, we adapt the pre-trained language model to the NER tasks using the distant labels, which can significantly improve the recall and precision; In the second stage, we drop the distant labels, and propose a self-training approach to further improve the model performance. Thorough experiments on 5 benchmark datasets demonstrate the superiority of BOND over existing distantly supervised NER methods. The code and distantly labeled data have been released in https://github.com/cliang1453/BOND.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named Entity Recognition (NER) is the task of detecting mentions of real-world entities from text and classifying them into predefined types <ref type="bibr">(e.g., locations, persons, organizations)</ref>. It is a core task in knowledge extraction and is important to various downstream applications such as user interest modeling <ref type="bibr" target="#b12">(Karatay and Karagoz, 2015)</ref>, question answering <ref type="bibr" target="#b13">(Khalid et al., 2008)</ref> and dialogue systems <ref type="bibr" target="#b1">(Bowden et al., 2018)</ref>. Traditional approaches to NER mainly train statistical sequential models, such as Hidden Markov Model (HMM) <ref type="bibr" target="#b46">(Zhou and Su, 2002)</ref> and Conditional Random Field (CRF) <ref type="bibr" target="#b15">(Lafferty et al., 2001)</ref> based on hand-crafted features. To alleviate the burden of designing hand-crafted features, deep learning models <ref type="bibr" target="#b24">(Ma and Hovy, 2016;</ref><ref type="bibr" target="#b10">Huang et al., 2015)</ref> have been proposed for NER and shown strong performance. However, most deep learning methods rely on large amounts of labeled training data. As NER tasks require token-level labels, annotating a large number of documents can be expensive, time-consuming, and prone to human errors. In many real-life scenarios, the lack of labeled data has become the biggest bottleneck that prevents deep learning models from being adopted for NER tasks.</p><p>To tackle the label scarcity issue, one approach is to use distant supervision to generate labels automatically. In distant supervision, the labeling procedure is to match the tokens in the target corpus with concepts in knowledge bases (e.g. Wikipedia<ref type="foot" target="#foot_0">1</ref> and YAGO<ref type="foot" target="#foot_1">2</ref> ), which are usually easy and cheap to access. Nevertheless, the labels generated by the matching procedure suffer from two major challenges. The first challenge is incomplete annotation, which is caused by the limited coverage of existing knowledge bases. Take two common open-domain NER datasets as examples.</p><p>From Table <ref type="table">1</ref>, we find that the coverage of tokens on both datasets is very low (less than 60%).This issue renders many entities mentions unmatched and produces many false-positive labels, which can hurt subsequent NER model training significantly. The second challenge is noisy annotation. The annotation is often noisy due to the labeling ambiguity -the same entity mention can be mapped to multiple entity types in the knowledge bases. For instance, the entity mention 'Liverpool' can be mapped to both 'Liverpool City' (type: LOC) and 'Liverpool Football Club' (type: ORG) in the knowledge base. While existing methods adopt label induction methods based on type popularity, they will potentially lead to a matching bias toward popular types. Consequently, it can lead to many false-positive samples and hurt the performance of NER models. What's worse, there is often a trade-off between the label accuracy and coverage: generating the high-quality label requires setting strict matching rules which may not generalize well for all the tokens and thus reduce the coverage and introduce false-negative labels. On the other hand, increasing the coverage of annotation suffers from the increasing number of incorrect labels due to label ambiguity. From the above, it is still very challenging to generate high-quality labels with high coverage to the target corpus.</p><p>Several studies have attempted to address the above challenges in distantly-supervised NER. To address the label incompleteness issue, some works adopt the partial annotation CRFs to consider all possible labels for unlabeled tokens <ref type="bibr" target="#b44">(Yang et al., 2018;</ref><ref type="bibr" target="#b35">Shang et al., 2018)</ref>, but they still require a considerable amount of annotated tokens or external tools. To address the label noise issue, <ref type="bibr" target="#b27">Ni et al. Ni et al. (2017)</ref> use heuristic rules to filter out sentences with low matching quality. However, this filtering strategy improves the precision at the expense of lowering the recall. <ref type="bibr" target="#b2">Cao et al. Cao et al. (2019)</ref> attempt to induce labels for entity mentions based on their occurrence popularity in the concept taxonomy, which can suffer from labeling bias and produce mislabeled data. Moreover, most of the methods mainly focus on NER tasks in specific domains (e.g. biomedical, chemistry, etc.) where the ambiguity of the named entity is very low. When the matching ambiguity issue is more severe, such methods will be less effective especially under open-domain scenarios. Till now, training open-domain NER models with distant supervision remains a challenging problem.</p><p>We propose our model BOND, short for BERT-Assisted Open-Domain Named entity recognition with Distant Supervision, which learns accurate named entity taggers from distant supervision without any restriction on the domain or the content of the corpora. To address the challenges in learning from distant supervision, our approach leverages the power of pre-trained language models (e.g., ELMo <ref type="bibr" target="#b29">(Peters et al., 2018)</ref>, BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>, XLnet <ref type="bibr" target="#b45">(Yang et al., 2019)</ref>) which are particularly attractive to this task due to the following merits: First, they are very large neural networks trained with huge amounts of unlabeled data in a completely unsupervised manner, which can be cheaply obtained; Second, due to their massive sizes (usually having hundreds of millions or billions of parameters), they have strong expressive power to capture general semantics and syntactic information effectively. These language models have achieved state-of-the-art performance in many popular NLP benchmarks with appropriate fine-tuning <ref type="bibr" target="#b5">(Devlin et al., 2019;</ref><ref type="bibr" target="#b22">Liu et al., 2019b;</ref><ref type="bibr" target="#b45">Yang et al., 2019;</ref><ref type="bibr" target="#b17">Lan et al., 2020b;</ref><ref type="bibr" target="#b30">Raffel et al., 2019)</ref>, which demonstrates their strong ability in modeling the text data.</p><p>To fully harness the power of pre-trained language models for tackling the two challenges, we propose a two-stage training framework. In the first stage, we fine-tune the RoBERTa model <ref type="bibr" target="#b22">(Liu et al., 2019b)</ref> with distantly-matched labels to essentially transfer the semantic knowledge in RoBERTa, which will improve the quality of prediction induced from distant supervision. It is worth noting that we adopt early stopping to prevent the model from overfitting to the incomplete annotated labels<ref type="foot" target="#foot_2">3</ref> and significantly improve the recall. Then we use the RoBERTa model to predict a set of pseudo soft-labels for all data. In the second stage, we replace the distantly-matched labels with the pseudo soft-labels and design a teacher-student framework to further improve the recall. The student model is first initialized by the model learned in the first stage and trained using pseudo soft-labels. Then, we update the teacher model from the student model in the previous iteration to generate a new set of pseudo-labels for the next iteration to continue the training of the student model. This teacher-student framework enjoys the merit that it progressively improves the model confidence over data. In addition, we select samples based on the prediction confidence of the student model to further improve the quality of soft labels. In this way, we can better exploit both the knowledge base information and the language models and improve the model fitting.</p><p>Our proposed method is closely related to low-resource NER and semi-supervised learning. We discuss more details in Section 5. We summarize the key contributions of our work as follows:</p><p>• We demonstrate that the pre-trained language model can also provide additional semantic information during the training process and reduce the label noise for distantly-supervised named entity recognition. To the best of our knowledge, this is the first work that leverages the power of pre-trained language model for open-domain NER tasks with distant supervision.</p><p>• We design a two-stage framework to fully exploit the power of language models in our task. Specifically, we refine the distant label iteratively with the language model in the first stage and improve the model fitting under the teacher-student framework in the second stage, which is able to address the challenge of noisy and incomplete annotation.</p><p>• We conduct comprehensive experiments on 5 datasets for named entity recognition tasks with distant supervision. Our proposed method significantly outperforms state-of-the-art distantly supervised NER competitors in all 5 datasets (4 of which by significant margins).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>We briefly introduce the distantly-supervised NER problem and the pre-trained language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Distantly Supervised NER</head><p>NER is the process of locating and classifying named entities in text into predefined entity categories, such as person names, organizations, locations, etc. Formally, given a sentence with N tokens X = [x 1 , ..., x N ], an entity is a span of tokens s = [x i , ..., x j ] (0 ≤ i ≤ j ≤ N ) associated with an entity type. Based on the BIO schema <ref type="bibr" target="#b18">(Li et al., 2012)</ref>, NER is typically formulated as a sequence labeling task of assigning a sequence of labels Y = [y 1 , ..., y N ] to the sentence X. Specifically, the first token of an entity mention with type X is labeled as B-X; the other tokens inside that entity mention are labeled as I-X; and the non-entity tokens are labeled as O.</p><p>For (fully) supervised NER, we are given M sentences that are already annotated at token level, denoted as {(X m , Y m )} M m=1 . Let f (X; θ) denote an NER model, which can compute N probability simplexes for predicting the entity labels of any new sentence X, where θ is the parameter of the NER model. We train such a model by minimizing the following loss over {(X m , Y m )} M m=1 :</p><formula xml:id="formula_0">θ = argmin θ 1 M M m=1 (Y m , f (X m ; θ)),<label>(1)</label></formula><p>where (•, •) is the cross-entropy loss.</p><p>For distantly-supervised NER, we do not have access to well-annotated true labels, but only distant labels generated by matching unlabeled sentences with external gazetteers or knowledge bases (KBs). The matching can be achieved by string matching <ref type="bibr" target="#b8">(Giannakopoulos et al., 2017)</ref>, regular expressions <ref type="bibr" target="#b7">(Fries et al., 2017)</ref> or heuristic rules (e.g., POS tag constraints). Accordingly, we learn an NER model by minimizing Eq. (1) with {Y m } M m=1 replaced by their distantly labeled counterparts.</p><p>Challenges. The labels generated by distant supervision are often noisy and incomplete. This is particularly true for open-domain NER where there is no restriction on the domain or the content of the corpora. <ref type="bibr" target="#b7">Fries et al. Fries et al. (2017)</ref> and <ref type="bibr" target="#b8">Giannakopoulos et al. Giannakopoulos et al. (2017)</ref> have proposed distantly-supervised NER methods for specific domains (e.g., biomedical domain), where the adopted domain-specific gazetteers or KBs are often of high matching quality and yield high precision and high recall distant labels. For the open domain, however, the quality of the distant labels is much worse, as there is more ambiguity and limited coverage over entity types in open-domain KBs. Table <ref type="table">1</ref> illustrates the matching quality of distant labels on the open-domain and the biomedical-domain datasets. As can be seen, the distant labels for the open-domain datasets suffer from much lower precision and recall. This imposes great challenges to training accurate NER models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pre-trained Language Model</head><p>Pre-trained language models, such as BERT and its variants (e.g., RoBERTa <ref type="bibr" target="#b22">(Liu et al., 2019b)</ref>, ALBERT <ref type="bibr" target="#b17">(Lan et al., 2020b)</ref> and T5 <ref type="bibr" target="#b30">(Raffel et al., 2019)</ref>), have achieved state-of-the-art performance in many natural language understanding tasks <ref type="bibr" target="#b11">(Jiang et al., 2019)</ref>. These models are essentially massive neural networks based on bi-directional transformer architectures, and are trained using open-domain data in a completely unsupervised manner. The stacked self-attention modules of the Table <ref type="table">1</ref>: Existing Gazetteer Matching Performance on Open-Domain <ref type="bibr" target="#b34">(Sang and De Meulder, 2003;</ref><ref type="bibr" target="#b36">Strauss et al., 2016)</ref> and Biomedical Domain NER Datasets <ref type="bibr" target="#b35">(Shang et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric</head><p>Open  <ref type="bibr">et al., 2015)</ref> (800 million words) and English Wikipedia (2500 million words). More importantly, many pre-trained language models have been publicly available online. One does not need to train them from scratch. When applying pre-trained language models to downstream tasks, one only needs to slightly modify the model and adapt the model through efficient and scalable stochastic gradient-type algorithms.</p><p>3 Two-Stage Framework: BOND</p><p>We introduce our proposed two-stage framework-BOND. In the first stage of BOND, we adapt the BERT model to the distantly supervised NER task. In the second stage, we use a self-training approach to improve the model fitting to the training data. We summarize the BOND framework in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Stage I: BERT-Assisted Distantly Supervised Learning with Early Stopping</head><p>Before proceeding with our proposed method, we briefly introduce how we generate distant labels for open-domain NER tasks. Our label generation scheme contains two steps: We first identify potential entities by POS tagging and hand-crafted rules. We then query from Wikidata to identify the types of these entities using SPARQL (Vrandečić and Krötzsch, 2014) as illustrated in Figure <ref type="figure" target="#fig_4">2</ref>. We next collect gazetteers from multiple online resources to match more entities in the data <ref type="bibr" target="#b34">(Sang and De Meulder, 2003)</ref>. Please refer to the appendix for more technical details.</p><p>We then proceed with our proposed method. We use f (•; θ) to denote the NER model parameterized by θ, f n,c (•; •) to denote the probability of the n-th token belonging to the c-th class, and {(X m , D m )} M m=1 to denote the distantly labeled data, where</p><formula xml:id="formula_1">D m = [d m,1 , ..., d m,N ] and X m = [x m,1 , ..., x m,N ]. The NER model f (•; θ) is learned by minimizing the loss over {(X m , D m )} M m=1 : θ = argmin θ 1 M M m=1 (D m , f (X m ; θ)),<label>(2)</label></formula><p>where  The architecture of the NER model f (•, •) is a token-wise NER classifier on top of a pre-trained BERT, as shown in Figure <ref type="figure">3</ref>. The NER classifier takes in the token-wise output embeddings from the pre-trained BERT layers, and gives the prediction on the type for each token. The pre-trained BERT contains rich semantic and syntax knowledge, and yields high quality output embeddings. Using such embeddings as the initialization, we can efficiently adapt the pre-trained BERT to the target NER task using stochastic gradient-type algorithms, e.g., ADAM <ref type="bibr" target="#b14">(Kingma and Ba, 2014;</ref><ref type="bibr" target="#b21">Liu et al., 2020)</ref>. Following <ref type="bibr" target="#b30">Raffel et al. (2019)</ref>, our adaptation process updates the entire model including both the NER classification layer and the pre-trained BERT layers. </p><formula xml:id="formula_2">(D m , f (X m ; θ)) = 1 N N n=1 − log f n,d m,n (X m ; θ).</formula><formula xml:id="formula_3">{D m } M m=1 = Matching({X m , D m } M m=1 ; External KBs) // Model Adaptation for t = 1, 2, ..., T 1 do Sample a minibatch B t from {(X m , D m )} M m=1</formula><p>. Update the model using ADAM:</p><formula xml:id="formula_4">θ (t) = T (θ (t−1) , B t ).</formula><p>Output: The early stopped model: Figure <ref type="figure" target="#fig_3">4</ref> illustrates how the pre-trained BERT embeddings help the model adapt to distantly supervised NER tasks. We highlight that BERT is pre-trained through a masked language model (MLM) task, and is capable of predicting the missing words using the contextual information. Such a MLM task shares a lot of similarity with the NER task. Both of them are token-wise classification problems and heavily rely on the contextual information (see Figure <ref type="figure">3</ref>). This naturally enables the semantic knowledge of the pre-trained BERT to be transferred to the NER task. Therefore, the resulting model can better predict the entity types than those trained from scratch using only the distantly labeled data.</p><formula xml:id="formula_5">θ = θ (T 1 )</formula><p>Early Stopping. One important strategy we use in the adaptation process is early stopping. Due to the large model capacity as well as the limited and noisy supervision (distant labels), our NER model can overfit the noise in distant labels and forget the knowledge of the pre-trained BERT if without any intervention. Early stopping essentially serves as a strong regularization to prevent such overfitting and improves generalization ability to unseen data.</p><p>Remark 1. Stage I addresses both of the two major challenges in distantly supervised NER tasks: noisy annotation and incomplete annotation. As the semantic knowledge in the pre-trained BERT is transferred to the NER model, the noise is suppressed such that the prediction precision is improved. Moreover, early stopping prevents the model from overfitting the incomplete annotated labels and further improves the recall. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Stage II: Self-Training</head><p>We first describe a teacher-student framework of self-training to improve the model fitting, and then we propose to use high-confidence soft labels to further improve the self-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">The Teacher-student Framework</head><p>We use f (•; θ tea ) and f (•; θ stu ) to denote teacher and student models, respectively. Given the model learned in Stage I, f (•; θ), one option is to initialize the teacher model and the student model as:</p><formula xml:id="formula_6">θ (0) tea = θ (0) stu = θ,</formula><p>and another option is</p><formula xml:id="formula_7">θ (0) tea = θ and θ (0) stu = θ BERT , (<label>3</label></formula><formula xml:id="formula_8">)</formula><p>where θ BERT denotes the initial model with the pre-trained BERT layers used in Stage I. For simplicity, we refer the second option to "re-initialization". At the t-th iteration, the teacher model generates pseudo labels { Y</p><formula xml:id="formula_9">(t) m = [ y (t) m,1 , ..., y (t) m,N ]} M m=1 by y (t) m,n = argmax c f n,c (X m ; θ (t) tea ). (<label>4</label></formula><formula xml:id="formula_10">)</formula><p>Then the student model fits these pseudo-labels. Specifically, given the teacher model f (•; θ (t) tea ), the student model is learned by solving</p><formula xml:id="formula_11">θ (t) stu = argmin θ 1 M M m=1 ( Y (t) m , f (X m ; θ)).</formula><p>(5)</p><p>We then use ADAM to optimize Eq. ( <ref type="formula">5</ref>) with early stopping. At the end of t-th iteration, we update the teacher model and the student model by:</p><formula xml:id="formula_12">θ (t+1) tea = θ (t+1) stu = θ (t) stu .</formula><p>The algorithm is summarized in Algorithm 2. Remark 2. Note that we discard all pseudo-labels from the (t-1)-th iteration, and only train the student model using pseudo-labels generated by the teacher model at the t-th iteration. Combined with early stopping, such a self-training approach can improve the model fitting and reduce the noise of the pseudo-labels as illustrated in Figure <ref type="figure" target="#fig_5">5</ref>. With progressive refinement of the pseudo-labels, the student model can gradually exploit knowledge in the pseudo-labels and avoid overfitting.</p><p>Remark 3. Our teacher-student framework is quite general, and can be naturally combined with other training techniques, e.g., mean teacher <ref type="bibr" target="#b37">(Tarvainen and Valpola, 2017)</ref> and virtual adversarial training <ref type="bibr" target="#b26">(Miyato et al., 2018)</ref>. Please refer to Section 5 for more detailed discussions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Re-weighted High-Confidence Soft Labels</head><p>The hard pseudo-labels generated by Eq. ( <ref type="formula" target="#formula_9">4</ref>) only keeps the most confident class for each token. To avoid losing too much information of other classes, we propose to use soft labels with confidence re-weighting.</p><p>Recall that for the n-th token in the m-th sentence, the output probability simplex over C classes is denoted as [f n,1 (X m ; θ), ..., f n,C (X m ; θ)].</p><p>At the t-th iteration, the teacher model generates soft pseudo-labels {S </p><formula xml:id="formula_13">s (t) m,n = [s (t) m,n,c ] C c=1 =       f 2 n,c (X m ; θ (t) tea )/p c C c =1 f 2 n,c (X m ; θ (t) tea )/p c       C c=1<label>(6)</label></formula><p>where</p><formula xml:id="formula_14">p c = M m=1 N n=1 f n,c (X m ; θ (t)</formula><p>tea ) calculates the unnormalized frequency of the tokens belonging to the c-th class. As can be seen, such a squared re-weighting step in Eq. ( <ref type="formula" target="#formula_13">6</ref>) essentially favors the classes with higher confidence. The student model f (•; θ (t) stu ) is then optimized by minimizing</p><formula xml:id="formula_15">θ (t) stu = argmin θ 1 M M m=1 KL (S (t) m , f (X m ; θ)),</formula><p>where KL (•, •) denotes the KL-divergence-based loss:</p><formula xml:id="formula_16">KL (S (t) m , f (X m ; θ)) = 1 N N n=1 C c=1 −s (t) m,n,c log f n,c (X m ; θ). (<label>7</label></formula><formula xml:id="formula_17">)</formula><p>High-Confidence Selection. To further address the uncertainty in the data, we propose to select tokens based on the prediction confidence. Specifically, at the t-th iteration, we select a set of high confidence tokens from the m-th sentence by</p><formula xml:id="formula_18">H (t) m = {n : max c s (t) m,n,c &gt; },<label>(8)</label></formula><p>where ∈ (0, 1) is a tuning threshold. Accordingly, the student model f (•; θ (t) stu ) can be optimized by minimizing the loss only over the selected tokens:</p><formula xml:id="formula_19">θ (t) stu = argmin θ 1 M|H (t) m | M m=1 n∈H (t) m C c=1 −s (t) m,n,c log f n,c (X m ; θ).</formula><p>The high confidence selection essentially enforces the student model to better fit tokens with high confidence, and therefore is able to improve the model robustness against low-confidence tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct a series of experiments to demonstrate the superiority of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datasets</head><p>We consider the following NER benchmark datasets: (i) CoNLL03 (Tjong <ref type="bibr" target="#b38">Kim Sang and De Meulder, 2003)</ref> is a well-known open-domain NER dataset from the CoNLL 2003 Shared Task. It consists of 1393 English news articles and is annotated with four entity types: person, location, organization, and miscellaneous. (ii) Twitter <ref type="bibr" target="#b9">(Godin et al., 2015)</ref> is from the WNUT 2016 NER shared task. This is an open-domain NER dataset that consists of 2400 tweets (comprising 34k tokens) with 10 entity types. (iii) OntoNotes5.0 <ref type="bibr" target="#b40">(Weischedel et al., 2013</ref>) contains text documents from multiple domains, including broadcast conversation, P2.5 data and Web data. It consists of around 1.6 millions words and is annotated with 18 entity types. (iv) Wikigold <ref type="bibr" target="#b0">(Balasuriya et al., 2009)</ref> is a set of Wikipedia articles (40k tokens) randomly selected from a 2008 English dump and manually annotated with the four CoNLL03 entity types. (v) Webpage <ref type="bibr" target="#b32">(Ratinov and Roth, 2009</ref>) is an NER dataset that contains personal, academic, and computer science conference webpages. It consists of 20 webpages that cover 783 entities belonging to the four types the same as CoNLL03.</p><p>For distant labels generation, we match entity types in external KBs including Wikidata corpus and gazetteers collected from multiple online sources. The data sources and matching details are described in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Baselines</head><p>We compare our model with different groups of baseline methods.</p><p>• KB Matching. The first baseline performs string matching with external KBs using the mechanism described in the appendix.</p><p>• Fully-supervised Methods. We also include fully-supervised NER methods for comparison, including: (i) RoBERTa-base <ref type="bibr" target="#b22">(Liu et al., 2019b)</ref>-it adopts RoBERTa model with linear layers to perform token-level prediction; (ii) BiLSTM-CRF <ref type="bibr" target="#b24">(Ma and Hovy, 2016)</ref> adopts bi-directional LSTM with character-level CNN to produce token embeddings, which are fed into a CRF layer to predict token labels.</p><p>• Distantly-supervised Methods. The third group of baselines are recent deep learning models for distantly-supervised NER, including: (i) BiLSTM-CRF <ref type="bibr" target="#b24">(Ma and Hovy, 2016)</ref> is trained using the distant labels matched from KBs; (ii) AutoNER <ref type="bibr" target="#b35">(Shang et al., 2018)</ref> trains the model by assigning ambiguous tokens with all possible labels and then maximizing the overall likelihood using a fuzzy LSTM-CRF model; (iii) LRNT <ref type="bibr" target="#b2">(Cao et al., 2019)</ref> is the state-of-the-art model for low-resource named tagging, which applies partial-CRFs on high-quality data with non-entity sampling. When comparing with these distantly supervised methods, we use the same distant labels as the training data for fair comparison.</p><p>• Baselines with Different Settings. The following methods also conduct open-domain NER under distant supervision. We remark that they use different KBs and extra training data. Therefore, we only compare with the results reported in their papers. (i) KALM <ref type="bibr" target="#b20">(Liu et al., 2019a)</ref> augments a traditional language model with a KB and use entity type information to enhance the model. (ii) ConNET <ref type="bibr" target="#b16">(Lan et al., 2020a)</ref> leverages multiple crowd annotation and dynamically aggregates them by attention mechanism. It learn from imperfect annotations from multiple sources. <ref type="foot" target="#foot_3">4</ref>• For Ablation Study, we consider the following methods/tricks. (i) MT <ref type="bibr" target="#b37">(Tarvainen and Valpola, 2017)</ref> uses Mean Teacher method to average model weights and forms a target-generating teacher model. (ii) VAT <ref type="bibr" target="#b26">(Miyato et al., 2018)</ref> adopts virtual adversarial training to smooth the output distribution to make the model robust to noise. (iii) Hard Label generates pseudo-labels using Eq. ( <ref type="formula" target="#formula_9">4</ref>). (iv) Soft Label generates pseudo-labels using Eq. ( <ref type="formula" target="#formula_13">6</ref>). (v) Reinitialization initializes the student and teacher models using Eq. ( <ref type="formula" target="#formula_7">3</ref>). (vi) High-Confidence Selection selects tokens using Eq. (8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results</head><p>Our NER model use RoBERTa-base as the backbone. A linear classification layer is build up on the RoBERTa-base model. Please refer to the appendix for implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Main Results</head><p>Table <ref type="table" target="#tab_2">2</ref> presents the F 1 scores, precision and recall for all methods. Note that our implementations of the fully supervised NER methods attain very close to the state-of-the-art performance <ref type="bibr" target="#b5">(Devlin et al., 2019;</ref><ref type="bibr" target="#b19">Limsopatham and Collier, 2016)</ref>. Our results are summarized as follows:</p><p>• For all five datasets, our method consistently achieves the best performance under the distant supervision scenarios, in F 1 score, precision and recall. In particular, our method outperforms the strongest distantly supervised NER baselines by {11. <ref type="bibr">74, 21.91, 0.66, 14.35, 12</ref>.53} in terms of F 1 score. These results demonstrate the significant superiority of our proposed method.</p><p>• The standard adaptation of pre-trained language models have already demonstrated remarkable performance. The models obtained by the Stage I of our methods outperform the strongest distantly supervised NER baselines by <ref type="bibr">{5.87, 20.51, 0.42, 7.72, 4</ref>.01} in terms of F 1 score. The Stage II of our methods further improves the performance of the Stage I by {5.87, 1.4, 0.24, 6.63, 8.52}.</p><p>• On CoNLL03 dataset, compared with baselines which use different sources -KALM and ConNET, our model also outperforms them by significant margins. More detailed technical comparisons between our method and them are provided in Section 5. Note: † : KALM achieves better performance when using extra data. : ConNET studies NER under a crowd sourcing setting, where the best human annotator achieves F 1 score at 89.51.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Ablation Study</head><p>To gain insights of our two-stage framework, we investigate the effectiveness of several components of our method via ablation study. The table <ref type="table" target="#tab_4">3</ref> shows the results on both CoNLL03 and Wikigold datasets. Our results can be summarized as follows:</p><p>• For Stage I, Pre-trained Language Models significantly improve both precision and recall for both datasets. Specifically, when training the NER model from scratch, the F1 scores of the output model of Stage I drop from 75.61 to 36.66 on CoNLL03, and from 51.55 to 18.31 on Wikigold. This verifies that the rich semantic and contextual information in pre-trained RoBERTa has been successfully transferred to our NER model in Stage I. Moreover, the precision on Wikigold is also improved. This verifies that the soft labels preserve more information and yield better fitted models than those of the hard labels.</p><p>• For stage II, High-Confidence Selection improves the F 1 scores on both datasets. Specifically, compared with using soft labels, the F 1 scores and recall increase from 81.56/78.84 to 80.18/72.31 on CoNLL03, and from 58.64/59.74 to 60.07/68.58 on Wikigold. Besides, the precision on CoNLL03 is also improved. This verifies that the high-confidence labels help select data and yield more robust performance.</p><p>• For Stage II, Re-initialization improves both precision and recall, only when the hard labels are adopted. We believe that this is because the hard labels lose too much information about data uncertainty, re-initializing the RoBERTa layers restores semantic and contextual information, and can compensate such loss.</p><p>In contrast, when soft labels are adopted, Re-initialization deteriorates both precision and recall. We believe that this is because the soft label retains sufficient information (i.e., the knowledge transferred from RoBERTa and learned from the distant labels). As a result, re-initialization only leads to underfitting on the data.  teacher-student framework by adding an additional MT teacher or a VAT teacher. VAT marginally improves the F1 scores on both datasets. MT marginally improves the F1 scores on Wikigold, and deteriorates the F 1 scores on CoNLL03. We believe that this is because MT and VAT perform well with high quality labels, however, the labels in our NER tasks are not very precise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Parameter Study</head><p>We investigate the effects of the early stopping time of Stage I -T 1 , the early stopping time of Stage II-T 3 , and confidence threshold for selecting tokens using CoNLL03 data. The default values are T 1 = 900, T 3 = 1800, = 0.9. The learning curves are summarized in Figure <ref type="figure" target="#fig_7">6</ref>:</p><p>• Both T 1 and T 3 reflect trade-offs between precision and recall of the Stage I and Stage II, respectively. This verifies the importance of early stopping. The model performance is sensitive to T 1 , and less sensitive to T 3 .</p><p>• The recall increases along with . The precision shows a different behavior: it first decreases and then increases.</p><p>• We also consider a scenario, where T 3 is allowed to tune for each iteration of the Stage II. This requires more computational resource than the setting where T 3 remains the same for all iterations. This can further improve the model performance to 83.49, 84.09, 82.89 in terms of F 1 scores, precision and recall, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Case Study and Error Analysis</head><p>To demonstrate how BOND improves the recall, we compare the prediction performance of KB matching with the output models of Stage I and Stage II using Wikigold data. Figure <ref type="figure">8</ref> presents the bar plots of four entity types -"LOC", "PER", "ORG" and "MISC". As can be seen, the KB matching yields a large amount of "O" (non-entity) due to its limited coverage. As a result, the recall is very low 47.63%. In contrast, our model of the Stage I benefits from the transferred knowledge of pre-trained RoBERTa and is able to correct some wrongly matched O's to their corresponding entity types. Therefore, it enjoys a better recall 54.50%. Moreover, the self-training in the Stage II further improves the recall to 68.48%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work and Discussion</head><p>Our work is related to low-resource NER.  <ref type="bibr" target="#b33">(Rosenberg et al., 2005;</ref><ref type="bibr" target="#b37">Tarvainen and Valpola, 2017;</ref><ref type="bibr" target="#b26">Miyato et al., 2018;</ref><ref type="bibr" target="#b25">Meng et al., 2018;</ref><ref type="bibr" target="#b3">Clark et al., 2018)</ref>. Different from distant supervision, these semi-supervised learning methods usually has a partial set of labeled data. They rely on the labeled data to train an sufficiently accurate model. The unlabeled data are usually used for inducing certain regularization to further improve the generalization performance. The distant supervision, however, considers the setting with only noisy labels. Existing semi-supervised learning methods such as Mean Teacher and Virtual Adversarial Training can only marginally improve the performance, as shown in the ablation study in our experiments.</p><p>Other related works: <ref type="bibr" target="#b20">Liu et al. (2019a)</ref> propose a language model-based method -KALM for NER tasks. However, their approach has two drawbacks: (i) Since they design a language model designated for NER tasks, they need to first train the language models from scratch. However, this often requires a large amount of training corpus and enormous computational resources. In contrast, BOND uses general-purpose pre-trained language models, which are publicly available online. (ii) The training of their language model is not fully unsupervised and requires token-level annotations. To address this issue, they resort to distant supervision, which yields incomplete and noisy annotations. Therefore, their language model does not necessarily achieve the desired performance.</p><p>Larger Pre-trained Language Models: To further improve the performance of BOND, we can use larger pre-trained language models such as RoBERTa-large <ref type="bibr" target="#b22">(Liu et al., 2019b)</ref> (Three times as big as RoBERT-base in our experiments) and T5 <ref type="bibr" target="#b30">(Raffel et al., 2019)</ref> (Thirty times larger than RoBERTa-base). These larger models contain more general semantics and syntax information, and have the potentials to achieve even better performance for NER Tasks. Unfortunately, due to the limitation of our computational resources, we are unable to use them in our experiments.</p><p>baseline system proposed in <ref type="bibr" target="#b9">Godin et al. (2015)</ref> to generate the distant labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Baseline Settings</head><p>For the baselines, we implement LSTM-CNN-CRF with Pytorch 15 and use the pre-trained 100 dimension GloVe Embeddings <ref type="bibr" target="#b28">(Pennington et al., 2014)</ref> as the input vector. Then, we set the dimension of character-level embeddings to 30 and feed them into a 2D convolutional neural network (CNN) with kernel width as 3. Then, we tune the output dimension in range of <ref type="bibr">[25,</ref><ref type="bibr">50,</ref><ref type="bibr">75,</ref><ref type="bibr">100,</ref><ref type="bibr">150]</ref> and report the best performance. We train the model for 50 epochs with early stopping. We use SGD with momentum with m = 0.9 and set the learning rate as 2 × 10 −3 . We set the dropout rate to 0.5 for linear layers after LSTM. We tune weight decay in range of [10 −5 , 10 −6 , 10 −7 , 10 −8 ] and report the best performance.</p><p>For other baselines, we follow the officially released implementation from the authors: (1) Au-toNER: https://github.com/shangjingbo1226/AutoNER; (2) LRNT: https://github.com/zig-k win-hu/Low-Resource-Name-Tagging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Implementation Details of BOND</head><p>All implementation are based on the Huggingface Transformer codebase<ref type="foot" target="#foot_15">16</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Adapting RoBERTa to the NER task</head><p>We choose RoBERTa-base as the backbone model of our NER model. A linear classification layer is built upon the pre-trained RoBERTa-base as illustrated in Figure <ref type="figure" target="#fig_9">9</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The two-stage BOND framework. In Stage I, the pre-trained BERT is adapted to the distantly supervised NER task with early stopping. In Stage II, a student model and a teacher model are first initialized from the model learned in Stage I. Then the student model is trained using pseudo-labels generated by the teacher model. Meanwhile, the teacher model is iteratively updated by the early-stopped student.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :</head><label>1</label><figDesc>Stage I: BERT-Assisted Distantly Supervised Learning with Early Stopping Input: M unlabeled sentences, {X m } M m=1 ; External KBs including Wikidata and multi-source gazetteers; The NER model with pre-trained BERT layers f (•; θ (0) ); The early stopping time T 1 ; The updating formula of ADAM T . // Distant Label Generation (DLG)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Illustration of matching entities from Wikidata</figDesc><graphic url="image-5.png" coords="7,164.21,72.00,280.78,162.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of Stage I. Top) The pre-trained semantic knowledge is transferred to the NER task; Middle) Early stopping leverages the pre-trained knowledge and yields better prediction; Bottom) Without early stopping, the model overfits the noise. The token embeddings are evolving, as we update the pre-trained BERT layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 2 :</head><label>2</label><figDesc>Stage II: Self-Training Input: M training sentences, {X m } M m=1 ; The early stopped model obtained in Stage I, f (•; θ); The number of self-training iterations T 2 ; The early stopping time T 3 ; The updating formula of ADAM T . Initialize the teacher model and the student model: k = 1, 2, ..., T 3 do Sample a minibatch B k from {X m } M m=1 . Generate pseudo-labels { Y m } m∈B k by Eq. (4). Update the student model: θ (t,k) stu = T (θ (t,k−1) stu , {(X m , Y m )} m∈B k ). Update the teacher and student: θ The final student model: θ (T 2 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Illustration of self-training. The self-training can gradually reduce the noise of the pseudo-labels and improve model fitting.</figDesc><graphic url="image-6.png" coords="10,187.61,72.00,234.00,191.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Learning Curves of BOND, BOND (w/ reinit), BOND (w/ soft) and BOND (w/ soft + reinit)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Parameter Study using CoNLL03: F 1 , Precision, Recall on Testing Set (in %)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: The NER Model with Pre-trained RoBERTa</figDesc><graphic url="image-7.png" coords="22,187.61,451.85,234.00,179.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>capture deep contextual information, and their non-recurrent structures enable the training to scale to large amounts of open-domain data. For example, the popular BERT-base model contains 110 million parameters, and is trained using the BooksCorpus (Zhu</figDesc><table><row><cell></cell><cell cols="2">-Domain</cell><cell cols="2">Biomedical Domains</cell></row><row><cell></cell><cell cols="4">CoNLL03 Tweet BC5CDR NCBI-Disease</cell></row><row><cell>Entity Types</cell><cell>4</cell><cell>10</cell><cell>2</cell><cell>1</cell></row><row><cell>F-1</cell><cell>59.61</cell><cell>35.83</cell><cell>71.98</cell><cell>69.32</cell></row><row><cell>Precision</cell><cell>71.91</cell><cell>40.34</cell><cell>93.93</cell><cell>90.59</cell></row><row><cell>Recall</cell><cell>50.90</cell><cell>32.22</cell><cell>58.35</cell><cell>56.15</cell></row><row><cell>transformer architectures can</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>BERT-Assisted Distantly Supervised Learning with Early Stopping Training Instances Adilson Varela, commonly known as Cabral, …</head><label></label><figDesc></figDesc><table><row><cell>Unlabeled Training Data</cell><cell cols="3">Knowledge Bases Lynn Reaser Adilson Valera Jacksonville PER PER LOC Stage I: BERT Multi-source Gazetteers Barnett Banks Inc Cabral FC Basel ORG PER ORG BERT Classification Head Distant Labels Pseudo Classification Head Labels</cell><cell>Label Pseudo</cell><cell>BERT Classification Head</cell></row><row><cell></cell><cell></cell><cell></cell><cell>initialization Student Model</cell><cell>Teacher Model</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Iteratively Update</cell></row><row><cell></cell><cell>…</cell><cell>…</cell><cell>Adilson Varela, commonly known as Cabral, …</cell></row><row><cell></cell><cell cols="2">Generate Distantly Labeled Data</cell><cell>Training Instances</cell></row></table><note>IDSentence 1 "It appears that August is showing an economy again reversing course", said economist Lynn Reaser of Barnett Banks Inc. in Jacksonville. 2 Adilson Varela, commonly known as Cabral, is a footballer from Switzerland who plays as midfielder for FC Basel. B-PER I-PER O O O B-PER … Stage II: Self-training</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Main Results on Testing Set: F 1 Score (Precision/Recall) (in %)</figDesc><table><row><cell>Method</cell><cell>CoNLL03</cell><cell>Tweet</cell><cell>OntoNote5.0</cell><cell>Webpage</cell><cell>Wikigold</cell></row><row><cell>Entity Types</cell><cell>4</cell><cell>10</cell><cell>18</cell><cell>4</cell><cell>4</cell></row><row><cell cols="6">KB Matching 71.40(81.13/63.75) 35.83(40.34/32.22) 59.51(63.86/55.71) 52.45(62.59/45.14) 47.76(47.90/47.63)</cell></row><row><cell cols="2">Fully-Supervised (Our implementation)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RoBERTa</cell><cell cols="5">90.11(89.14/91.10) 52.19(51.76/52.63) 86.20(84.59/87.88) 72.39(66.29/79.73) 86.43(85.33/87.56)</cell></row><row><cell cols="6">BiLSTM-CRF 91.21(91.35/91.06) 52.18(60.01/46.16) 86.17(85.99/86.36) 52.34(50.07/54.76) 54.90(55.40/54.30)</cell></row><row><cell cols="2">Baseline (Our implementation)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">BiLSTM-CRF 59.50(75.50/49.10) 21.77(46.91/14.18) 66.41(68.44/64.50) 43.34(58.05/34.59) 42.92(47.55/39.11)</cell></row><row><cell>AutoNER</cell><cell cols="5">67.00(75.21/60.40) 26.10(43.26/18.69) 67.18(64.63/69.95) 51.39(48.82/54.23) 47.54(43.54/52.35)</cell></row><row><cell>LRNT</cell><cell cols="5">69.74(79.91/61.87) 23.84(46.94/15.98) 67.69(67.36/68.02) 47.74(46.70/48.83) 46.21(45.60/46.84)</cell></row><row><cell cols="2">Other Baseline (Reported Results)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>KALM  †</cell><cell>76.00( ---/ ---)</cell><cell>---</cell><cell>---</cell><cell>---</cell><cell>---</cell></row><row><cell>ConNET</cell><cell>75.57(84.11/68.61)</cell><cell>---</cell><cell>---</cell><cell>---</cell><cell>---</cell></row><row><cell cols="2">Our BOND Framework</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Stage I</cell><cell cols="5">75.61(83.76/68.90) 46.61(53.11/41.52) 68.11(66.71/69.56) 59.11(60.14/58.11) 51.55(49.17/54.50)</cell></row><row><cell>BOND</cell><cell cols="5">81.48(82.05/80.92) 48.01(53.16/43.76) 68.35(67.14/69.61) 65.74(67.37/64.19) 60.07(53.44/68.58)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>•</head><label></label><figDesc>For Stage I, Early stopping improves both precision and recall for both datasets. We increase the training iterations from 900 to 18000 on CoNLL03 and from 350 to 7000 on Wikigold, and the F1 scores of the output model of Stage I drop from 75.61 to 72.11 on CoNLL03, and from 51.55 to 49.68 on Wikigold. This verifies that Early Stopping eases the overfitting and improves the generalization ability of our NER model.• For Stage II, Soft labels improve the F 1 score and recall on both datasets. Specifically, the F 1 scores and recall increase from 77.28/71.98 to 80.18/78.84 on CoNLL03, and from 56.90/59.74 to 58.64/65.79 on Wikigold.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation Study: F 1 Score (Precision/Recall) (in %)Note † : We use BOND to denote our two-stage framework using hard pseudo-labels in this table for clarity.</figDesc><table><row><cell>Method</cell><cell>CoNLL03</cell><cell>Wikigold</cell></row><row><cell>Stage I</cell><cell></cell><cell></cell></row><row><cell>Stage I</cell><cell cols="2">75.61(83.76/68.90) 51.55(49.17/54.50)</cell></row><row><cell>Stage I w/o pre-train</cell><cell cols="2">36.66(37.49/35.75) 18.31(18.14/18.50)</cell></row><row><cell>Stage I w/o early stop</cell><cell cols="2">72.11(81.65/64.57) 49.68(48.67/50.74)</cell></row><row><cell>Stage I w/ MT</cell><cell cols="2">76.30(82.92/70.67) 46.68(49.82/43.91)</cell></row><row><cell>Stage I w/ VAT</cell><cell cols="2">76.38(82.58/71.04) 47.54(50.02/45.30)</cell></row><row><cell>Stage I + Stage II</cell><cell></cell><cell></cell></row><row><cell>BOND  †</cell><cell cols="2">77.28(83.42/71.98) 56.90(54.32/59.74)</cell></row><row><cell>BOND w/ soft</cell><cell cols="2">80.18(81.56/78.84) 58.64(58.29/65.79)</cell></row><row><cell cols="3">BOND w/ soft+high conf 81.48(82.05/80.92) 60.07(53.44/68.58)</cell></row><row><cell>BOND w/ reinit</cell><cell cols="2">78.17(85.05/72.31) 58.55(55.31/62.19)</cell></row><row><cell>BOND w/ soft+reinit</cell><cell cols="2">76.92(83.39/71.38) 54.09(50.72/57.94)</cell></row><row><cell>BOND w/ MT</cell><cell cols="2">77.16(82.79/72.25) 57.93(55.66/60.39)</cell></row><row><cell>BOND w/ VAT</cell><cell cols="2">77.64(85.62/70.69) 57.39(55.05/59.41)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>This line of research focuses on leveraging cross lingual information to improve the model performance. For examples,<ref type="bibr" target="#b4">Cotterell and Duh (2017)</ref>;<ref type="bibr" target="#b6">Feng et al. (2018)</ref> consider NER for a low resource target language. They propose to train an NER model with annotated language that are closely related to the target language.<ref type="bibr" target="#b43">Xie et al. (2018)</ref> propose to use the bilingual dictionaries to tackle this challenge. More recently,<ref type="bibr" target="#b31">Rahimi et al. (2019)</ref> propose a Bayesian graphical model approach to further improve the low resource NER performance. Our work is also relevant to semi-supervised learning, where the training data is only partially labeled. There have been many semi-supervised learning methods, including the popular Mean Teacher and Virtual Adversarial Training methods used in our experiments for comparison</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://www.wikipedia.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/ yago/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">Here the incomplete annotated labels refer to tokens wrongly labeled as type 'O'.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">For KALM and ConNET model, the KB and crowd annotation are not public available, and thus we are unable to reproduce the results.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4">Moreover, we also consider Multiple Re-initialization, and observe similar results.• Mean Teacher and Virtual Adversarial Training can be naturally integrated into our versatile</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5">https://github.com/dominictarr/random-name</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6">https://data.world/len/us-first-names-database</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_7">https://github.com/imsky/wordlists</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_8">https://www2.census.gov/topics/genealogy/2010surnames/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_9">https://ziegenfuss.bplaced.net/zfuss/surnames-all.php?tree=1</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_10">https://www.surnamedb.com/Surname</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_11">https://surnameslist.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_12">https://footballdatabase.com/ranking/world</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_13">https://www.ducksters.com/sports/list_of_mlb_teams.php</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_14">https://en.wikipedia.org/wiki/List_of_intergovernmental_organizations</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_15">https://github.com/huggingface/transformers</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Detailed Description of Distant Label Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 External Knowledge Bases</head><p>Wikidata is a collaborative and free knowledge base for the acquisition and maintenance of structured data. It contains over 100 million tokens extracted from the set of verified articles on Wikipedia. Wikidata knowledge imposes a high degree of structured organization. It provides a SPARQL query service for users to obtain entity relationships. Multi-sources Gazetteers. For each dataset, we build a gazetteer for each entity type. Take CoNLL03 as an example, we build a gazetteer for the type PER by collecting data from multiple online sources including Random Name 5 , US First Names Database 6 , Word Lists 7 , US Census Bureau 8 , German Surnames 9 , Surnames Database 10 and Surname List 11 . We build a gazetteer for the type ORG by collecting data from Soccer Team 12 , Baseball Team 13 and Intergovernmental Organization 14 . We will release all gazetteers and codes for matching distant labels after the paper is accepted for publication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Distant Labels Generation Details</head><p>We first find potential entities by POS tagging obtained from POS tagger, e.g., NLTK <ref type="bibr" target="#b23">(Loper and Bird, 2002)</ref>. We then match these potential entities by using Wikidata query service. Specifically, we use SPARQL to query the parent categories of an entity in the knowledge tree. We continue querying to the upper levels until a category corresponding to a type is found. For entities with ambiguity (e.g., those linked with multiple parent categories), we discard them during the matching process (i.e., we assign them with type O). The above procedure is summarized in Figure <ref type="figure">2</ref>.</p><p>We then build, for each entity type in each dataset, a multi-sources gazetteer by crawling online data sources. Following the previous exact string matching methods <ref type="bibr" target="#b34">(Sang and De Meulder, 2003;</ref><ref type="bibr" target="#b8">Giannakopoulos et al., 2017)</ref>, we match an entity with a type if the entity appears in the gazetteer for that type.</p><p>For the unmatched tokens, we further use a set of hand-crafted rules to match entities. We notice that among the true entities, there is usually a stamp word. We match a potential entity with a type if there exists a stamp word in this entity that has frequent occurrence in that type. For example, "Inc." frequently occurs in organization names, thus the appearance of "Inc." indicates that the entity labels of words in the "XXX Inc." should be B-ORG or I-ORG).</p><p>Note that for Twitter, we do not build our own multi-sources gazetteer. We directly use the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Pseudo-labels Generation Details</head><p>BERT uses WordPiece <ref type="bibr" target="#b41">(Wu et al., 2016)</ref> for tokenization of the input text. When the teacher model predicts a set of pseudo-labels for all training data in Stage II, it assign labels for padded tokens as well. We ignore those labels in training and loss computation step by label masking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Parameter Settings</head><p>There are several key parameters in our model: 1) For CoNLL03, we choose T 1 = 900 (about 1 epoch) and T 3 = 1756 (about 2 epochs). For Tweet, we choose T 1 = 900 and T 3 = 900. For OntoNotes5, we choose T 1 = 16500 and T 3 = 1000. For Webpage, we choose T 1 = 300 and T 3 = 200. For Wikigold, we choose T 1 = 350 and T 3 = 700. As for T 2 , we stop training when the number of total training epochs reaches 50 for all datasets. 2) We choose 10 −5 as the learning rate for CoNLL03, Webpage and Wikigold and 2 × 10 −5 for OntoNotes5, Twitter, all with learning rate linear decay of 10 −4 .</p><p>3) We use AdamW with β 1 =0.9 and β 2 =0.98 as optimizer for all datasets. 4) We set =0.9 for all datasets. 5) The training batch size is 16 for all datasets except OntoNotes5.0, which uses 32 as the training batch size. 6) For the NER token-wise classification head, we set dropout rate as 0.1 and use a linear classification layer with hidden size 768. For MT, we set ramp-up step as 300 for CoNLL03, 200 for Tweet, 200 for OntoNotes5.0, 300 for Webpage and 40 for Wikigold. We choose the moving average parameters as α 1 = 0.99 and α 2 = 0.995 for all datasets. For VAT, we set the perturbation size vat = 10 −4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Multiple Re-initialization</head><p>Multiple Re-initialization is implemented as follows: In Stage II, as the performance of the student model no longer improves, we re-initialize it from the pre-trained RoBERTa-base and start a new self-training iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Combine BOND w/ MT&amp;VAT</head><p>MT&amp;VAT can easily combined with BOND as follows: During training, we update the student model by minimize the sum of weighted MT (or VAT) loss and Eq. ( <ref type="formula">7</ref>). The weight of MT (or VAT) loss is selected in [10, 1, 10 −1 , 10 −2 , 10 −3 ] using development set.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Named entity recognition in wikipedia</title>
		<author>
			<persName><forename type="first">D</forename><surname>Balasuriya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ringland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
	<note>Workshop on The People&apos;s Web Meets NLP</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Slugnerds: A named entity recognition tool for open domain dialogue systems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oraby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Low-resource name tagging learned with weakly labeled data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence modeling with cross-view training</title>
		<author>
			<persName><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Low-resource named entity recognition with cross-lingual, character-level neural conditional random fields</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Duh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP. Asian Federation of Natural Language Processing</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving low resource named entity recognition using cross-lingual knowledge transfer</title>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Swellshark: A generative model for biomedical named entity recognition without labeled data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06360</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised aspect term extraction with b-LSTM &amp; CRF using automatically labelled datasets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Giannakopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hossmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baeriswyl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Godin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vandersmissen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>De Neve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van De Walle</surname></persName>
		</author>
		<title level="m">Multimedia lab@ acl wnut ner shared task: Named entity recognition for twitter microposts using distributed word representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>WNUT</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<title level="m">Bidirectional lstm-crf models for sequence tagging</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Smart: Robust and efficient finetuning for pre-trained natural language models through principled regularized optimization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03437</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">User interest modeling in twitter with named entity recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Karatay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Karagoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Making Sense of Microposts (# Microposts2015)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The impact of named entity normalization on information retrieval for question answering</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Khalid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECIR</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to contextually aggregate multi-source supervision for sequence labeling</title>
		<author>
			<persName><forename type="first">O</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint bilingual name tagging for parallel corpora</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Bidirectional lstm for named entity recognition in twitter messages</title>
		<author>
			<persName><forename type="first">N</forename><surname>Limsopatham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Collier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Knowledge-augmented language model and its application to unsupervised named-entity recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the variance of the adaptive learning rate and beyond</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<idno>arXiv preprint cs/0205028</idno>
		<title level="m">Nltk: the natural language toolkit</title>
				<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Weakly-supervised neural text classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>IEEE</publisher>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1979" to="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Weakly supervised cross-lingual named entity recognition via effective annotation and representation projection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<title level="m">Deep contextualized word representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>NAACL-HLT</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Multilingual ner transfer for low-resource languages</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00193</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>In CoNLL</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semi-supervised self-training of object detection models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Introduction to the conll-2003 shared task: Languageindependent named entity recognition</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De Meulder</surname></persName>
		</author>
		<idno>arXiv preprint cs/0306050</idno>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning named entity tagger using domain-specific dictionary</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Results of the wnut16 named entity recognition shared task</title>
		<author>
			<persName><forename type="first">B</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Toma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-C</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WNUT</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">Tjong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>De Meulder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>In CoNLL-2003</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Vrandeči Ć</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kr Ötzsch</surname></persName>
		</author>
		<title level="m">Wikidata: a free collaborative knowledgebase. Communications of the ACM</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="78" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Franchini</surname></persName>
		</author>
		<title level="m">Ontonotes release 5.0 ldc2013t19. Linguistic Data Consortium</title>
				<meeting><address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Neural cross-lingual named entity recognition with minimal resources</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D18-1034" />
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Distantly supervised ner with partial annotation learning and reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Named entity recognition using an hmm-based chunk tagger</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
