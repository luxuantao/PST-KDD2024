<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2016 LEARNING WITH A STRONG ADVERSARY</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-01-16">16 Jan 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ruitong</forename><surname>Huang</surname></persName>
							<email>ruitong@ualberta.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Alberta Edmonton</orgName>
								<address>
									<postCode>T6G 2E8</postCode>
									<region>AB</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Alberta Edmonton</orgName>
								<address>
									<postCode>T6G 2E8</postCode>
									<region>AB</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Alberta Edmonton</orgName>
								<address>
									<postCode>T6G 2E8</postCode>
									<region>AB</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Csaba</forename><surname>Szepesvári</surname></persName>
							<email>szepesva@ualberta.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Alberta Edmonton</orgName>
								<address>
									<postCode>T6G 2E8</postCode>
									<region>AB</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Under review as a conference paper at ICLR 2016 LEARNING WITH A STRONG ADVERSARY</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-01-16">16 Jan 2016</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1511.03034v6[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The robustness of neural networks to intended perturbations has recently attracted significant attention. In this paper, we propose a new method, learning with a strong adversary, that learns robust classifiers from supervised data by generating adversarial examples as an intermediate step. A new and simple way of finding adversarial examples is presented that is empirically stronger than existing approaches in terms of the accuracy reduction as a function of perturbation magnitude. Experimental results demonstrate that resulting learning method greatly improves the robustness of the classification models produced.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep Neural Network (DNN) models have recently demonstrated impressive learning results in many visual and speech classification problems <ref type="bibr" target="#b7">(Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b3">Hinton et al., 2012a)</ref>. One reason for this success is believed to be the expressive capacity of deep network architectures. Even though classifiers are typically evaluated by their misclassification rate, robustness is also a highly desirable property: intuitively, it is desirable for a classifier to be 'smooth' in the sense that a small perturbation of its input should not change its predictions significantly. An intriguing recent discovery is that DNN models do not typically possess such a robustness property <ref type="bibr" target="#b14">(Szegedy et al., 2013</ref>). An otherwise highly accurate DNN model can be fooled into misclassifying typical data points by introducing a human-indistinguishable perturbation of the original inputs. We call such a perturbed data set 'adversarial examples'. An even more curious fact is that the same set of such adversarial examples is also misclassified by a diverse set of classification models, such as KNN, Boosting Tree, even if they are trained with different architectures and different hyperparameters.</p><p>Since the appearance of <ref type="bibr" target="#b14">Szegedy et al. (2013)</ref>, increasing attention has been paid to the curious phenomenon of 'adversarial perturbation' in the deep learning community; see, for example <ref type="bibr" target="#b2">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b1">Fawzi et al., 2015;</ref><ref type="bibr" target="#b10">Miyato et al., 2015;</ref><ref type="bibr" target="#b12">Nøkland, 2015;</ref><ref type="bibr" target="#b15">Tabacof &amp; Valle, 2015)</ref>. <ref type="bibr" target="#b2">Goodfellow et al. (2014)</ref> suggest that one reason for the detrimental effect of adversarial examples lies in the implicit linearity of the classification models in high dimensional spaces. Additional exploration by <ref type="bibr" target="#b15">Tabacof &amp; Valle (2015)</ref> has demonstrated that, for image classification problems, adversarial images inhabit large "adversarial pockets" in the pixel space. Based on these observations, different ways of finding adversarial examples have been proposed, among which the most relevant to our study is that of <ref type="bibr" target="#b2">(Goodfellow et al., 2014)</ref>, where a linear approximation is used to obviate the need for any auxiliary optimization problem to be solved. In this paper, we further investigate the role of adversarial training on classifier robustness and propose a simple new approach to finding 'stronger' adversarial examples. Experimental results suggest that the proposed method is more effective than previous approaches in the sense that the resulting DNN classifiers obtain worse performance under the same magnitude of perturbation.</p><p>The main achievement of this paper is a training method that is able to produce robust classifiers with high classification accuracy in the face of stronger data perturbation. The approach we propose differs from previous approaches in a few ways. First, <ref type="bibr" target="#b2">Goodfellow et al. (2014)</ref> suggests using an augmented objective that combines the original training objective with an additional objective that is measured after after the training inputs have been perturbed. Alternatively, <ref type="bibr" target="#b12">(Nøkland, 2015)</ref> suggest, as a specialization of the method in <ref type="bibr" target="#b2">(Goodfellow et al., 2014)</ref>, to only use the objective defined on the perturbed data. However, there is no theoretical analysis to justify that classifiers learned in this way are indeed robust; both methods are proposed heuristically. In our proposed approach, we formulate the learning procedure as a min-max problem that forces the learned DNN model to be robust against adversarial examples, so that the learned classifier is inherently robust. In particular, we allow an adversary to apply perturbations to each data point in an attempt to maximize classification error, while the learning procedure attempts to minimize misclassification error against the adversary. We call this learning procedure 'learning with a strong adversary'. Such min-max formulation has been discussed specifically in <ref type="bibr" target="#b2">(Goodfellow et al., 2014)</ref>, but emphasis is on its regularization effect particularly for logistic regression. Our setting is more general and applicable to different loss functions and different types of perturbations, and is the origin of our learning procedure. <ref type="foot" target="#foot_0">1</ref> It turns out that an efficient method for finding such adversarial examples is required as an intermediate step to solve such a min-max problem, which is the first problem we address. Then we develop the full min-max training procedure that incorporates robustness to adversarially perturbed training data. The learning procedure that results turns out to have some similarities to the one proposed in <ref type="bibr" target="#b12">(Nøkland, 2015)</ref>. Another min-max formulation is proposed in <ref type="bibr" target="#b10">Miyato et al. (2015)</ref> but still with the interpretation of regularization. These approaches are based on significantly different understandings of this problem. Recently, a theoretical exploration of the robustness of classifiers <ref type="bibr" target="#b1">(Fawzi et al., 2015)</ref> suggests that, as expected, there is a trade-off between expressive power and robustness. This paper can be considered as an exploration into this same trade-off from an engineering perspective.</p><p>The remainder of the paper is organized as follows. First, we propose a new method for finding adversarial examples in Section 2. Section 3 is then devoted to developing the main method: a new procedure for learning with a stronger form of adversary. Finally, we provide an experimental evaluation of the proposed method on MNIST and CIFAR-10 in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">NOTATIONS</head><p>We denote the supervised training data by Z = {(x 1 , y 1 ), . . . , (x N , y N )}. Let K be the number of classes in the classification problem. The loss function used for training will be denoted by . Given a norm • , let • * denote its dual norm, such that u * = max v ≤1 u, v . Denote the network by N whose last layer is a softmax layer g(x) α = (α 1 , . . . , α K ) to be used for classification. So N (x) is the predicted label for the sample x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">FINDING ADVERSARIAL EXAMPLES</head><p>Consider an example (x, y) ∈ X × {1, 2, . . . , K} and assume that N (x) = y, where y is the true label for x. Our goal is to find a small perturbation r ∈ X so that N (X + r) = y. This problem was originally investigated by <ref type="bibr" target="#b14">Szegedy et al. (2013)</ref>, who propose the following perturbation procedure: given</p><formula xml:id="formula_0">x, solve min r r s.t. N (X + r) = N (X).</formula><p>The simple method we propose to find such a perturbation r is based on the linear approximation of g(x), ĝ(x + r) = g(x) + Hr, where H = ∂g ∂w | x is the Jacobian matrix. As an alternative, we consider the following question: for a fixed index j = y, what is the minimal r (j) satisfying N (x + r (j) ) = j? Replacing g by its linear approximation ĝ, one of the necessary conditions for such a perturbation r is:</p><formula xml:id="formula_1">H j r (j) − H y r (j) ≥ α y − α j ,</formula><p>where H j is the j-th row of H. Therefore, the norm of the optimal r * (j) is greater than the following objective value:</p><formula xml:id="formula_2">min r (j) r (j) s.t. H j r (j) − H y r (j) ≥ α y − α j .<label>(1)</label></formula><p>The optimal solution to this problem is provided in Proposition 1.</p><p>Proposition 1. It is straightforward that the optimal objective value is r (j) = αy−αj Hj −Hy * . In particular, the optimal r * (j) for common norms are:</p><formula xml:id="formula_3">1. If • is the L 2 norm, then r * (j) = αy−αj Hj −Hy 2 2 (H j − H y ); 2. If • is the L ∞ norm, then r * (j) = αy−αj Hj −Hy 1 sign(H j − H y ); 3. If • is the L 1 norm, then r * (j) = c Hj −Hy ∞ e k where k satisfies |(H j − H y ) k | = H j − H y ∞ . Here V k is the k-th element of V .</formula><p>However, such r * (j) is necessary but not sufficient to guarantee that argmax i ĝ(x + r (j) ) i = j. The following proposition shows that in order to have ĝ make a wrong prediction, it is enough to use the minimum among all r * (j) 's. Proposition 2. Let I = argmin i r * (i) . Then r * I is the solution of the following problem: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TOWARD ROBUST NEURAL NETWORKS</head><p>We enhance the robustness of a neural network model by preparing the network for the worst examples by training with the following objective:</p><formula xml:id="formula_4">min g i max r (i) ≤c (g(x i + r (i) ), y i );<label>(2)</label></formula><p>where g ranges over functions expressible by the network model N (i.e. ranging over all parameters in N ). In this formulation, the hyperparameter c that controls the magnitude of the perturbation needs to be tuned. Note that when (g(x i +r (i) ), y i ) = I (maxj (g(xi+r (i) )j ) =yi) , the objective function is the misclassification error under perturbations. Often, is a surrogate for the misclassification loss that is differentiable and smooth. Let</p><formula xml:id="formula_5">L i (g) = max r (i) 2≤c (g(x i + r (i) ), y i ).</formula><p>Thus the problem is to find g * = arg min g i L i (g).</p><p>Related Works:</p><p>1. When taking to be the logistic loss, g to be a linear function, and the norm for perturbation to be ∞ , then the inner max problem has analytical solution, and Equation (2) matches the learning objective in Section 5 of <ref type="bibr" target="#b2">(Goodfellow et al., 2014)</ref>. 2. Let be the negative log function and g be the probability predicted by the model. Viewing y i as a distribution that has weight 1 on y i and 0 on other classes, then the classical entropy loss is in fact D KL (y i p) where p = g(x i ). Using these notions, Equation ( <ref type="formula" target="#formula_4">2</ref>) can be rewritten as min f max r (i) D KL (y i p) where p = g(x i + r (i) ). Similarly, the objective function proposed in <ref type="bibr" target="#b2">(Goodfellow et al., 2014)</ref> can be generalized as αD KL (y i p) + (1 − α) max r (i) D KL (y i p). Moreover, the one proposed in <ref type="bibr" target="#b10">Miyato et al. (2015)</ref> can be interpreated as D KL (y i p) + D KL (p p).</p><p>Experiments shows that our approach is able to achieve the best robustness while maintain high classification accuracy.</p><p>To solve the problem (2) using SGD, one needs to compute the derivative of L i with respect to (the parameters that define) g. The following preliminary proposition suggests a way of computing this derivative.</p><p>Proposition 3. Given h : U × V → W differentiable almost everywhere, define L(v) = max u∈U h(u, v). Assume that L is uniformly Lipschitz-continuous as a function of v, then the following results holds almost everywhere:</p><formula xml:id="formula_6">∂L ∂v (v 0 ) = ∂h ∂v (u * , v 0 ),</formula><p>where u * = arg max u h(u, v 0 ).</p><p>Proof. Note that L is uniformly Lipschitz-continuous, therefore by Rademacher's theorem, L is differentiable almost everywhere. For v 0 where L is differentiable, the Fréchet subderivative of L is actually a singleton set of its derivative.</p><p>Consider the function</p><formula xml:id="formula_7">L(v) = h(u * , v). Since h is differentiable, ∂h ∂v (u * , v 0 ) is the derivative of L at point v 0 . Also L(v 0 ) = L(v 0 )</formula><p>. Thus, by Proposition 2 of <ref type="bibr" target="#b11">(Neu &amp; Szepesvári, 2012)</ref>, ∂h ∂v (u * , v 0 ) also belongs to the subderivative of L. Therefore,</p><formula xml:id="formula_8">∂L ∂v (v 0 ) = ∂h ∂v (u * , v 0 ).</formula><p>The differentiability of h in Proposition 3 usually holds. The uniformly Lipschitz-continuous of neural networks was also discussed in the paper of <ref type="bibr" target="#b14">Szegedy et al. (2013)</ref>. It still remains to compute u * in Proposition 3. In particular given (x i , y i ), we need to solve max</p><formula xml:id="formula_9">r (i) ≤c (g(x i + r (i) ), y i ).<label>(3)</label></formula><p>We postpone the solution for the above problem to the end of this section. Given that we can have an approximate solution for Equation (3), a simple SGD method to compute a local solution for Equation ( <ref type="formula" target="#formula_4">2</ref>) is then shown in Algorithm 2.</p><p>Algorithm 2 Learning with an Adversary input (x i , y i ) for 1 ≤ i ≤ N ; Initial g 0 ; output ĝ 1: for t = 1, 2, . . . , T do 2:</p><p>for (x i , y i ) in the current batch do Compute r * as the optimal perturbation to x, using the proposed methods in Section 3.1 5:</p><p>Create a pseudo-sample to be (</p><formula xml:id="formula_10">x i = x i + c r * r * 2 , y i ) 6:</formula><p>end for 7:</p><p>Update the network ĝ using forward-backward propagation on the pseudo-sample (x i , y i ) for 1 ≤ i ≤ N 8: end for 9: Return ĝ.</p><p>For complex prediction problems, deeper neural networks are usually proposed, which can be interpreted as consisting of two parts: the lower layers of the network can be interpreted as learning a representation for the input data, while the upper layers can be interpreted as learning a classification model on top of the learned representation. The number of layers that should be interpreted as providing representations versus classifications is not precise and varies between datasets; we treat this as a hyperparameter in our method. Given such an interpretation, denote the representation layers of the network as N rep and the classification layers of the network as N cla . We propose to perform the perturbation over the output of N rep rather than the raw data. Thus the problem of learning with an adversary can be formulated as follows:</p><formula xml:id="formula_11">min Nrep,N cla i max r (i) ≤c N cla N rep (x i ) + r (i) , y i .<label>(4)</label></formula><p>Similarly, Equation ( <ref type="formula" target="#formula_11">4</ref>) can be solved by the following SGD method given in Algorithm 3.</p><p>Algorithm 3 Learning with an Adversary in the Split Network Interpretation input (x i , y i ) for 1 ≤ i ≤ N ; Initial N cla and N rep ; output f 1: for t = 1, 2, . . . , T do 2:</p><p>for (x i , y i ) in the current batch do We propose two different perturbation methods based on two different principles. The first proposed method, similar to that of <ref type="bibr" target="#b2">(Goodfellow et al., 2014)</ref>, does not require the solution of an optimization problem. Experimental results show that this method, compared to the method proposed in <ref type="bibr" target="#b2">(Goodfellow et al., 2014)</ref>, is more effective in the sense that, under the same magnitude of perturbation, the accuracy reduction in the network is greater.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">LIKELIHOOD BASED LOSS</head><p>Assume the loss function (x, y) = h(α y ), where h is a non-negative decreasing function. A typical example of such a loss would be the logistic regression loss. In fact, most of the network models use a softmax layer as the last layer and a cross-entropy objective function. Recall that we would like to find</p><formula xml:id="formula_12">r * = arg max r (i) ≤c h g(x i + r (i) ) yi ,<label>(5)</label></formula><p>where x i could be the raw data or the output of N rep . Since h is decreasing, r * = arg min r (i) ≤c g(</p><formula xml:id="formula_13">x i + r (i) ) yi .</formula><p>This problem can still be difficult to solve in general. Similarly, using its linear approximation g(x i + r (i) ) yi , i.e. g( i) , such that H = ∂g ∂w | x is the Jacobian matrix, it is then easy to see that using g(x i + r (i) ) yi in Equation ( <ref type="formula" target="#formula_12">5</ref>), the solution for r * can be recovered as r * = {r : r ≤ c; H yi , r (i) = c H yi * }.</p><formula xml:id="formula_14">x i + r (i) ) yi ≈ g(x i + r (i) ) yi = g(x i ) yi + H yi , r<label>(</label></formula><p>The optimal solutions for r * given common norms are:</p><formula xml:id="formula_15">1. If • is the L 2 norm, then r * (j) = c Hy i Hy i 2 ; 2. If • is the L ∞ norm, then r * (j) = c sign(H yi ); 3. If • is the L 1 norm, then r * (j) = c e k , where k satisfies |H yi | = H yi ∞ .</formula><p>Note that the second item here is exactly the method suggested in <ref type="bibr" target="#b2">(Goodfellow et al., 2014)</ref>. 2 norm is used in <ref type="bibr" target="#b10">(Miyato et al., 2015)</ref> but with different objective function rather than negative log likelihood, as mentioned at the begining of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">MISCLASSIFICATION BASED LOSS</head><p>In the case that the loss function is a surrogate loss for the misclassification rate, it is reasonable to still use the misclassification rate as the loss function in Equation (3). In this case, the problem in Equation ( <ref type="formula" target="#formula_9">3</ref>) becomes finding a perturbation r : r ≤ c that forces the network N to misclassify x i . In practice, for N to achieve a good approximation, c needs to be chosen to have a small value, hence it might not be large enough to force a misclassification. One intuitive way to achieve this is to perturb by r in the direction proposed in Section 2, since such direction is arguably the most damaging direction for the perturbation. Therefore, in this case, we use</p><formula xml:id="formula_16">r * = c r * I / r * I ,</formula><p>where r * I is the output of Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL EVALUATION</head><p>To investigate the training method proposed above, in conjunction with the different approaches for determining perturbations, we consider the <ref type="bibr">MNIST (LeCun et al., 1998b)</ref> and CIFAR-10 data sets.</p><p>The MNIST data set contains 28x28 grey scale images of handwritten digits. We normalize the pixel values into the range [0, 1] by dividing by 256. The CIFAR-10 ( <ref type="bibr" target="#b6">Krizhevsky &amp; Hinton, 2009)</ref> dataset is a tiny nature image dataset. CIFAR-10 datasets contains 10 different classes images, each image is an RGB image in size of 32x32. Input images are subtracted by mean value 117, and randomly cropped to size 28x28. We also normalize the pixel value into the range (-1, 1) by dividing 256. This normalization is for evaluating perturbation magnitude by L 2 norm. For both datasets, we randomly choose 50,000 images for training and 10,000 for testing.</p><p>All experiment models are trained by using MXNet <ref type="bibr" target="#b0">(Chen et al., 2015)</ref> <ref type="foot" target="#foot_1">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">FINDING ADVERSARIAL EXAMPLES</head><p>We tested a variety of different perturbation methods on MNIST, including: 1. Perturbation based on α using the 2 norm constraint, as shown in Section 2 (Adv Alpha); 2. Perturbation based on using a loss function with the 2 norm constraint, as shown in Section 3.1 (Adv Loss); 3. Perturbation based on using a loss function with the ∞ norm constraint, as shown in Section 3.1 (Adv Loss Sign). In particular, a standard 'LeNet' model is trained on MNIST, with training and validation accuracy being 100% and 99.1% respectively. Based on the learned network, different validation sets are then generated by perturbing the original data with different perturbation methods. The magnitudes of the perturbations range from 0.0 to 4.0 in 2 norm. An example of such successful perturbation is show in Table <ref type="table">1</ref>. The classification accuracies on differently perturbed data sets are reported in Figure <ref type="figure" target="#fig_3">1</ref>.</p><p>The networks classification accuracy decreases with increasing magnitude of the perturbation. These results suggest that Adv Alpha is consistently, but slightly, more effective than Adv Loss, and these two method are significantly more effective than Adv Loss Sign.</p><p>Drawback of using α to find perturbations Note that the difference in perturbation effectiveness between using α and using the loss function is small. On the other hand, to compute the perturbation using α, one needs to compute ∂α ∂x , which is K times more expensive than the method using the loss function, which only needs to compute ∂ ∂x . (Recall that K is the number of classes.) Due to time limit, during the training procedure we only use the adversarial examples generated from the loss function for our experiments.</p><p>Table 1: An visual example of using different perturbations; The magnitude of all the perturbations are 1.5 in 2 norm. The true label of the image is 8. The first perturbed image is still predicted to be 8, while the other 2 perturbed images are predicted to be 3.</p><p>Adv Loss Sign Adv Loss Adv Alpha Original Image</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perturbed Image</head><p>Noise </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">LEARNING WITH AN ADVERSARY</head><p>We tested our overall learning approach on both MNIST and CIFAR-10. We measure the robustness of each classifier on various adversarial sets. An adversarial set of the same type for each learned classifier is generated based on the targeted classifier. We generated 3 types of adversarial data sets for the above 5 classifiers corresponding to Adv Alpha, Adv Loss, and Adv Loss Sign. In addition, we also evaluated the accuracy of these 5 classifiers on a fixed adversarial set, which is generated based on the 'Normal' network using Adv Loss. Finally, we also report the original validation accuracies of the different networks. We use 2 norm for our method to train the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments on MNIST:</head><p>We first test different training methods on a 2-hidden-layer neural network model. In particular, we considered: 1. Normal back-forward propagation training, 100 × 100 (Normal); 2. Normal back-forward propagation training with Dropout, 200 × 200 (Dropout <ref type="bibr" target="#b4">(Hinton et al., 2012b</ref>)); 3. The method in <ref type="bibr" target="#b2">(Goodfellow et al., 2014)</ref>, 500 × 500 (Goodfellow's method); 4. Learning with an adversary on raw data, 500 × 500 (LWA); 5. Learning with an adversary at the representation layer, 200 × 500 (LWA Rep). Here n × m denote that the network has n nodes for the first hidden layer, and m nodes for the second one. All of the results are tested under perturbations of with the 2 norm constrained to at most 1.5.</p><p>We sumarize the results in Table <ref type="table" target="#tab_0">2</ref>. Note that the normal method can not afford any perturbation on the validation set, showing that it is highly non-robust. By training with dropout, both the accuracy and robustness of the neural network are improved, but robustness remains weak; for the adversarial set generated by Adv Alpha in particular, the resulting classification accuracy is only 19.3%. Goodfellow's method improves the network's robustness greatly, compared to the previous meth- ods. However, the best accuracy and the most robustness are both achieved by LWA. In particular, on the adversarial sets generated by our methods (Adv Loss and Adv Alpha), the performance is improved from 84.4% to 86.7%, and from 83.6% to 86.2%. The result of LWA Rep is also reported for comparison. Overall, it achieves worse performance than Goodfellow's method <ref type="bibr" target="#b2">(Goodfellow et al., 2014)</ref> and LWA, but still much more robust than Dropout.</p><p>We also evaluated these learning methods on the LeNet model <ref type="bibr" target="#b8">(LeCun et al., 1998a)</ref>, which is more complex, including convolution layers. We use Dropout for Goodfellow's method and LWA. The resulting learning curves are reported in Figure <ref type="figure" target="#fig_4">2</ref>. It is interesting that we do not observe the trade-off between robustness and accuracy once again; this phenomenon also occurred with the 2-hidden-layers neural network. The final result is summarized in Table <ref type="table" target="#tab_1">3</ref>, which shows the   <ref type="bibr" target="#b5">(Ioffe &amp; Szegedy, 2015)</ref> to stabilize the learning of LWA Rep. We also test the performances of different methods with batch normalization (BN). The results are summarized in Table <ref type="table" target="#tab_2">4</ref>. Learning with a strong adversary again achieves better robustness, but we also observe a small decrease on their classification accuracies. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We investigate the curious phenomenon of 'adversarial perturbation' in a formal min-max problem setting. A generic algorithm is developed based on the proposed min-max formulation, which is more general and allows to replace previous heuristic algorithms with formally derived ones. We also propose a more efficient way in finding adversarial examples for a given network. The experimental results suggests that learning with a strong adversary is promising in the sense that compared to the benchmarks in the literature, it achieves significantly better robustness while maintain high normal accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A ANALYSIS OF THE REGULARIZATION EFFECT OF LEARNING WITH AN ADVERSARY ON LOGISTIC REGRESSION</head><p>Perhaps the most simple neural network is the binary logistic regression. We investigate the behavior of learning with an adversary on the logistic regression model in this section. Let (z) = − log(1 + exp(−z)). Thus logistic regression is to solve the following problem:</p><formula xml:id="formula_17">min w i max r (i) ≤c (y i w (x i + r (i) )).<label>(6)</label></formula><p>Proposition 4. The problem 6 is still a convex problem.</p><p>Proof. In fact, there is a closed form solution for the maximization subproblem in (6). Not that is strictly decreasing, thus, Equation ( <ref type="formula" target="#formula_17">6</ref>) is equivalent to</p><formula xml:id="formula_18">min w i (y i w x i − c w * ).</formula><p>Let h(w) denote the concave function of w, y i w x i − c w * . To prove (h(w)) is convex, consider</p><formula xml:id="formula_19">(h(w 1 )) + (h(w 2 )) ≥ 2 h(w 1 ) + h(w 2 ) 2 ≥ 2 h w 1 + w 2 2 ,</formula><p>where the first inequality is because of the convexity of , and the second inequality is because of the monotonicity of and the concavity of h.</p><p>Let R z (w) = max r (i) ≤c (y i w (x i + r (i) )) − (y i w x i ) ≥ 0. The behavior of R z is a datadependent regularization. The next proposition shows that different to common p regularization, it is NOT a convex function. The figure of this function clearly shows its non-convexity.</p><p>One of the common problem about logistic regression without regularization is that there is no valid solution given a linearly separable data set Z. The following proposition shows that this issue is relieved by learning with an adversary. Proposition 6. Assume that the linearly separable data set Z has a margin less than 2c, then logistic regression with adversary is guaranteed to have bounded solutions. Moreover, if • * is twice differentiable almost everywhere and i ∂Qi ∂w ∂Qi ∂w is positive definite where Q i = −y i x i w+ w * , then logistic regression with adversary has unique solution.</p><p>Proof. Since the margin of Z is less than 2c, after the data is perturbed, it is no longer linear separable. Therefore for any w, there exists some i such that −y i w x i + c w * . Note that −y i w x i + c w * is homogeneous in w. Thus, if the optimal solution w * has a infinity norm, then i log 1 + exp(−y i w * x i + c w * * ) = ∞.</p><p>However, assigning w = 0 have finite loss. Therefore, w * is bounded.</p><p>Moreover, since • * is twice differentiable, consider the Hession matrix</p><formula xml:id="formula_20">H = i e Qi (1 + e Qi ) 2 ∂Q i ∂w ∂Q i ∂w + e Qi (1 + e Qi ) 2 ∂ 2 Q i ∂ 2 w .</formula><p>Note that w * is convex, so is Q i . Thus ∂ 2 Qi ∂ 2 w is positive semi-definite. Since i ∂Qi ∂w ∂Qi ∂w is positive definite, and e Q i (1+e Q i ) 2 &gt; 0 for any i, H is positive definite. Therefore, the objective function is strictly convex with respect to w. Combining the boundedness of w and being strictly convex of leads to the unique of w * .</p><p>Remark 1. Unlike adding a regularization term to the objective function, learning with an adversary can only guarantee unique solutions, when the data set is linearly separable with small margin. In such sense, learning with an adversary is a weaker regularization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>X + r)) i = y. Putting these observations together, we achieve an algorithm for finding adversarial examples, as shown in Algorithm 1. Algorithm 1 Finding Adversarial Examples input (x, y); Network N ; output r 1: Compute H by performing forward-backward propagation from the input layer to the softmax layer g(x) 2: for j = 1, 2, . . . , K do 3: Compute r * (j) from Equation (1) 4: end for 5: Return r = r * (I) where I = argmin i r * (i) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>to compute the output of N rep , xi 4: Take xi as the input for N cla 5: Use forward-backward propagation to compute ∂α ∂ xi 6: Compute r * as the optimal perturbation to xi , using the proposed methods in Section 3.1 7: Create a pseudo-sample to be ( xi = xi + c r * r * , y i ) to compute the output of N cla on ( xi , y i ) for 1 ≤ i ≤ N 10: Use backward propagation to update both N cla 11: Use backward propagation to compute ∂Nrep ∂W | xi,yi 12: Update N rep by ∂ ∂ xi | ( xi,yi) ∂Nrep ∂W | xi,yi 13: end for 14: Return N cla and N rep 3.1 COMPUTING THE PERTURBATION</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Validation accuracies of different perturbation methods. x axis denotes the 2 norm of the perturbation.</figDesc><graphic url="image-8.png" coords="7,167.40,258.62,277.21,148.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Validation accuracies on MNIST for different perturbation methods. x axis denotes the 2 norm of the perturbation.great robustness of LWA. We don't observe the superiority of perturbing the representation layer to perturbing the raw data. In the learned networks, a small perturbation on the raw data incurs a much larger perturbation on the representation layer, which makes LWA Rep difficult to achieve both high accuracy and robustness. How to avoid such perturbation explosion in the representation network remains open for future investigation.</figDesc><graphic url="image-9.png" coords="8,167.40,369.61,277.18,138.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Proposition 5 .</head><label>5</label><figDesc>The induced regularization R z (w) is non-convex.Proof. An counter example is enough to prove that R z (w) is non-convex. Let c = 0.5 and z = (x, y) = (1, 1), then R z (w) : R → R is R z (w) = log (1 + exp(−0.5w)) − log (1 + exp(−w)) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Classification accuracies for 2-hidden-layers neural network on MNIST: the best performance on each adversarial sets are shown in bold. The magnitude of perturbations are 1.5 in 2 norm.</figDesc><table><row><cell>METHODS</cell><cell></cell><cell></cell><cell>Validation Sets</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">Validation Fixed Adv Loss Sign Adv Loss Adv Alpha</cell></row><row><cell>Normal</cell><cell>0.977</cell><cell>0.361</cell><cell>0.287</cell><cell>0.201</cell><cell>0.074</cell></row><row><cell>Dropout</cell><cell>0.982</cell><cell>0.446</cell><cell>0.440</cell><cell>0.321</cell><cell>0.193</cell></row><row><cell>Goodfellow's Method</cell><cell>0.991</cell><cell>0.972</cell><cell>0.939</cell><cell>0.844</cell><cell>0.836</cell></row><row><cell>LWA</cell><cell>0.990</cell><cell>0.974</cell><cell>0.944</cell><cell>0.867</cell><cell>0.862</cell></row><row><cell>LWA Rep</cell><cell>0.986</cell><cell>0.797</cell><cell>0.819</cell><cell>0.673</cell><cell>0.646</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Classification accuracies for LeNet trained using LWA on MNIST. The magnitude of perturbations are 1.5 in 2 norm. CIFAR-10 is a more difficult task compared to MNIST. Inspired by VGG-D Network<ref type="bibr" target="#b13">(Simonyan &amp; Zisserman, 2014)</ref>, we use a network formed by 6 convolution layers with 3 fully connected layers. Same to VGG-Network, we use ReLU as the activation function. For each convolution stage, we use three 3x3 convolution layers followed by a max-pooling layer with stride of 2. We use 128, 256 filters for each convolution stage correspondingly. For three fully connected layer, we use 2048, 2048, 10 hidden units. We also split this network in representation learner and classifier view: the last two fully connected layers with hidden units 2048 and 10 are classifier and other layers below formed a representation learner. We compare the following methods: 1.</figDesc><table><row><cell>METHODS</cell><cell></cell><cell></cell><cell>Validation Sets</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Validation Fixed</cell><cell cols="3">Adv Loss Sign Adv Loss Adv Alpha</cell></row><row><cell>Normal</cell><cell>0.9912</cell><cell>0.7456</cell><cell>0.8082</cell><cell>0.5194</cell><cell>0.5014</cell></row><row><cell>Dropout</cell><cell>0.9922</cell><cell>0.7217</cell><cell>0.7694</cell><cell>0.5322</cell><cell>0.4893</cell></row><row><cell>Goodfellow's method</cell><cell>0.9937</cell><cell>0.9744</cell><cell>0.9755</cell><cell>0.9066</cell><cell>0.9035</cell></row><row><cell>LWA</cell><cell>0.9934</cell><cell>0.9822</cell><cell>0.9859</cell><cell>0.9632</cell><cell>0.9627</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Classification accuracies on CIFAR-10: the best performance on each adversarial sets are shown in bold. The magnitude of perturbations are 0.5 in 2 norm.</figDesc><table><row><cell>METHODS</cell><cell></cell><cell></cell><cell>Validation Sets</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">Validation Fixed Adv Loss Sign Adv Loss Adv Alpha</cell></row><row><cell>Normal</cell><cell>0.850</cell><cell>0.769</cell><cell>0.510</cell><cell>0.420</cell><cell>0.410</cell></row><row><cell>Dropout</cell><cell>0.881</cell><cell>0.774</cell><cell>0.571</cell><cell>0.480</cell><cell>0.466</cell></row><row><cell>Goodfellow's method</cell><cell>0.856</cell><cell>0.818</cell><cell>0.794</cell><cell>0.745</cell><cell>0.734</cell></row><row><cell>LWA</cell><cell>0.864</cell><cell>0.831</cell><cell>0.803</cell><cell>0.760</cell><cell>0.750</cell></row><row><cell>Normal + BN</cell><cell>0.887</cell><cell>0.793</cell><cell>0.765</cell><cell>0.747</cell><cell>0.738</cell></row><row><cell>Dropout + BN</cell><cell>0.895</cell><cell>0.810</cell><cell>0.729</cell><cell>0.697</cell><cell>0.687</cell></row><row><cell>Goodfellow's method + BN</cell><cell>0.886</cell><cell>0.856</cell><cell>0.821</cell><cell>0.775</cell><cell>0.754</cell></row><row><cell>LWA + BN</cell><cell>0.890</cell><cell>0.863</cell><cell>0.823</cell><cell>0.785</cell><cell>0.759</cell></row><row><cell>LWA Rep + BN</cell><cell>0.832</cell><cell>0.746</cell><cell>0.636</cell><cell>0.604</cell><cell>0.574</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Analysis of the regularization effect of such min-max formulation is postponed in the appendix, since it is not closly related to the main content of the paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Reproduce code: https://github.com/Armstring/LearningwithaStrongAdversary</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Ian Goodfellow, Naiyan Wang, Yifan Wu for meaningful discussions. Also we thank Mu Li for granting us access of his GPU machine to run experiments. This work was supported by the Alberta Innovates Technology Futures and NSERC.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Yutian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Naiyan</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Minjie</surname></persName>
		</author>
		<author>
			<persName><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><surname>Tianjun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Analysis of classifiers&apos; robustness to adversarial perturbations</title>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02590</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Abdel-Rahman</surname></persName>
		</author>
		<author>
			<persName><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><surname>Navdeep</surname></persName>
		</author>
		<author>
			<persName><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012a</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><surname>Nitish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012">2012b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><surname>Léon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Distributional smoothing by virtual adversarial examples</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><surname>Shin-Ichi</surname></persName>
		</author>
		<author>
			<persName><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><surname>Masanori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Nakae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin</forename><surname>Ishii</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.00677</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Gergely</forename><surname>Neu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Csaba</forename><surname>Szepesvári</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.5264</idno>
		<title level="m">Apprenticeship learning using inverse reinforcement learning and gradient methods</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improving back-propagation by adding an adversarial gradient</title>
		<author>
			<persName><forename type="first">Arild</forename><surname>Nøkland</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.04189</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><surname>Wojciech</surname></persName>
		</author>
		<author>
			<persName><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><surname>Dumitru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Intriguing properties of neural networks</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Exploring the space of adversarial images</title>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Tabacof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduardo</forename><surname>Valle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.05328</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
