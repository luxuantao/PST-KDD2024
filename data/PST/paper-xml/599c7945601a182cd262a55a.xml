<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Distance Maps Based Action Recognition with Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chuankun</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Yonghong</forename><surname>Hou</surname></persName>
						</author>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Pichao</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">Joint Distance Maps Based Action Recognition with Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5BF75E428EC40CF8E07D74F3FC5A472C</idno>
					<idno type="DOI">10.1109/LSP.2017.2678539</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/LSP.2017.2678539, IEEE Signal Processing Letters This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/LSP.2017.2678539, IEEE Signal Processing Letters</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Action Recognition</term>
					<term>Convolutional Neural Networks</term>
					<term>Joint Distance Maps</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Motivated by the promising performance achieved by deep learning, an effective yet simple method is proposed to encode the spatio-temporal information of skeleton sequences into color texture images, referred to as Joint Distance Maps (JDMs), and Convolutional Neural Networks (ConvNets) are employed to exploit the discriminative features from the JDMs for human action and interaction recognition. The pair-wise distances between joints over a sequence of single or multiple person skeletons are encoded into color variations to capture temporal information. The efficacy of the proposed method has been verified by the state-of-the-art results on the large NTU RGB+D Dataset and small UTD-MHAD Dataset in both singleview and cross-view settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>R ECOGNITION of human actions from RGB-D (Red, Green, Blue and Depth) data has attracted increasing attention <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> in computer vision in recent years due to the recent advance of easy-to-use and low-cost depth sensors such as Kinect sensors. Compared to conventional RGB data, the depth modality has the advantages of being insensitive to illumination changes and reliable to estimate body silhouettes and skeletons <ref type="bibr" target="#b2">[3]</ref>. In the past a few years, many handcrafted skeleton features <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref> have been proposed for action recognition. Skeleton based features is often less computationally intensive compared to the features extracted from depth and RGB sequences. However, most existing skeleton based action recognition methods primarily focus on single subject scenarios and representing spatial information based on hand-designed features. The temporal information is often modelled using Dynamic Time Warpings (DTWs) or Hidden Markov Models (HMMs). Recently, several methods based on Recurrent Neural Networks (RNNs) <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref> have also been proposed for skeleton based action recognition. RNNs tend to overemphasize the temporal information especially when the training data are not sufficient, leading to overfitting. To date, it remains unclear how skeleton sequences of both single This work was partly supported by the National Natural Science Foundation of China (grant 61571325) and Key Projects in the Tianjin Science &amp; Technology Pillar Program (grant15ZCZD GX001900) (Corresponding author: Pichao <ref type="bibr">Wang)</ref>.</p><p>C. Li and Y. Hou are with the School of Electronic Information Engineering, Tianjin University, Tianjin, China. (email: chuankunli@tju.edu.cn; houroy@tju.edu.cn).</p><p>P. Wang and W. Li are with the Advanced Multimedia Research Lab, University of Wollongong, Wollongong, Australia. (e-mail: pw212@uowmail.edu.au; wanqing@uow.edu.au). and multiple subjects could be effectively represented and fed to deep neural networks for recognition. For example, one can conventionally consider a skeleton sequence as a set of individual frames with some form of temporal smoothness, or as a subspace of poses or pose features, or as the output of a neural network encoder. Which one among these and other possibilities would result in the most effective representation for action recognition is not well understood.</p><p>This paper presents an effective yet simple method to encode the pair-wise distances of skeleton joints of single or multiple subjects into texture images, referred to as Joint Distance Maps (JDM), as the input of ConvNets for action recognition. The proposed method follows a similar approach to that in the works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. In <ref type="bibr" target="#b19">[20]</ref>, the joint coordinates of a skeleton sequence are organized in a matrix where the three Cartesian components (x, y, z) of joints are seen respectively as the three channels (R,G,B) of a color image. Due to the small size of the resultant images, it is impossible to tune an existing ConvNets. Wang et al. <ref type="bibr" target="#b20">[21]</ref> proposed to encode joint trajectories into texture images and capture temporal information by mapping the trajectories into a HSV (Hue, Saturation, Value) space. However, this method is viewdependent. The method proposed in this paper overcomes the drawbacks of the previous works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Firstly, the size of JDMs is comparable to the image size used to train existing ConvNets and consequently JDMs can be used for fine-tuning a pre-trained model. Secondly, because the 3D space distances between joints are view-independent, the proposed JDMs are suitable for cross-view action recognition. The proposed method was evaluated on two popular benchmark datasets, NTU RGB+D Dataset <ref type="bibr" target="#b17">[18]</ref> which includes actions performed by single and two subjects and UTD-MHAD Dataset <ref type="bibr" target="#b21">[22]</ref>. State-of-the-art performance was achieved.</p><p>The rest of paper is organized as follows: Section II describes the proposed method. The experimental results on two popular public datasets are presented in Section III. Section IV concludes the paper with remarks and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. THE PROPOSED METHOD</head><p>The proposed method consists of three major components, as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, the construction of four JDMs as the input of four ConvNets, ConvNets training and the late score fusion. Unlike the previous methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref> in which 3D points are created and projected onto three orthogonal planes of the real-world coordinate system centered on the camera to encode a sequence of skeletons (i.e. locations of joints) into  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>× t matrix will be generated from the sequence. In order to generate fixed length t images, bilinear interpolation is applied to scale the number of columns of the distance matrix from t to t . The distance values in the matrix are then converted into colors to generate a m×(m-1) 2 ×t JDM image so as to encode the spatial-temporal information into colors and texture. Fig. <ref type="figure" target="#fig_2">3</ref> illustrates the process of mapping joint distances to colors. For single subject performing actions, the first person joints are copied to the second person. In the following, the process to generate the four JDMs from the pair-wise distances of joints is detailed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Mapping of Joint Distance ( from Skeleton Sequences to Images)</head><p>Let p j = (px, py, pz) be the coordinates of the j th joint in each frame, where j ∈ {1, ...., m}. For each subject, 12 joints (hip center, spine, shoulder center, head, elbow left, wrist left, hand left, elbow right, wrist right, hand right, ankle left and ankle right) that have relatively less noise are used in this paper. The m joints of all subjects in each frame can be represented as: p = {p 1 , p 2 , ..., p m } and the numbering of joints follows a fixed order to maintain the correspondence between frames. In this paper, the numbering ranges from subject to subject and within a subject it is from the trunk to right leg, passing through the left arm, right arm and left leg. An action or interaction instance G of t frames can then be expressed as G = {p 1 , p 2 , ..., p t }. The Euclidean distance D i jk between joints j and k at frame i is denoted as</p><formula xml:id="formula_0">D i jk = p i j -p i k 2 , j, k ∈ m; j = k (1)</formula><p>The joint distances of all frames in a skeleton sequence are arranged in their temporal order and expressed as follows:</p><formula xml:id="formula_1">H jk = {D 1 jk , D 2 jk , ..., D t jk }<label>(2)</label></formula><p>where each column represents the pair-wise joint distances of one frame. In such a way, the spatial configuration of joints is described implicitly through the pair-wise distances while the temporal information is represented explicitly. The number of columns of H kj is then scaled from t to t through bilinear interpolation, i.e.</p><formula xml:id="formula_2">BL jk = f (H jk ) = {D 1 jk , D 2 jk , ..., D t jk }<label>(3)</label></formula><p>where f (H jk ) is the interpolation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Encoding Joint Distance</head><p>Inspired by the works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> where the authors used color to encode the temporal information, Hue is adopted in this paper to encode the variations of joint distances. Specifically, the jet colormap ranging from blue to red is employed though other color schemes may be equally effective as well. Notice that subjects may be of different heights which lead to the joint distances vary significantly for subjects to subjects. Instead of normalizing the skeleton as in the work <ref type="bibr" target="#b6">[7]</ref>, joint distances ranging from zero to the maximum within a sample are mapped to the colormap. Such mapping serves an analog function to normalization.</p><formula xml:id="formula_3">H(j, k, i) = f loor( D i jk max(BL jk ) × (h max -h min )), i ∈ t<label>(4</label></formula><p>) where H(j, k, i) denotes the Hue that is assigned to the jk th joint distance in i th frame; h max and h min are the range of the Hue.</p><p>Four JDMs are generated using the above process, three are generated by calculating the pair-wise distances in the three orthogonal planes (xy,yz and xz) in the real-world coordinates centered at the camera and the forth JDM is generated in the 3D space (xyz). Fig. <ref type="figure" target="#fig_1">2</ref> shows sample JDMs calculated from sample actions of the NTU RGB+D dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network Training &amp; Class Score Fusion</head><p>The original AlexNet is a classic neural network with five convolutional layers and three full-connected layers. It performs very well in image classification. In this paper, the network is fine-tuned on a small dataset and trained from scratch on a large dataset. In fine-tuning, the pre-trained models on ILSVRC-2012 (Large Scale Visual Recognition Challenge 2012, a version of ImageNet) are used for initialization. As observed, The fine-tuning model performs better (i.e. with higher recognition accuracy) when the training data is not sufficient. However, training from scratch is expected to perform better than fine-tuning on large datasets, especially when the constructed images are very different in texture from the images of ImageNet.</p><p>Training: In the experiments, the same layer configuration as the one in <ref type="bibr" target="#b25">[26]</ref> was adopted for the four ConvNets. The network weights are learned using the mini-batch stochastic gradient descent with the momentum being set to 0.9 and weight decay being set to 0.0005. The batch size was set to 256. All JDMs are resized to 256 × 256. The learning rate was initially set to 0.001 for fine-tuning and 0.01 for the training from scratch and then it is decreased according to a fixed schedule. For NTU RGB+D dataset, the learning was stopped after 35000 iterations and the learning rate decreases every 15000 iterations. The training was stopped after 900 iterations and the learning rate decreases every 400 iterations on UTD-MHAD dataset. For all experiments, the dropout regularization ratio was set to 0.5 in order to reduce complex co-adaptations of neurons in nets.</p><p>Multiply-Score Fusion: Given a testing skeleton sequence (sample), four JDMs are generated and fed into four different trained ConvNets. Multiply-score fusion was used, that is, the score vectors outputted by the four ConvNets are multiplied in an element-wise way and the max score in the resultant vector is assigned as the probability of the test sequence being the recognized class. The index of this max score corresponds to the recognized class label and expressed as follows:</p><formula xml:id="formula_4">label = F in(max(v 1 • v 2 • v 3 • v 4 )) (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>where v is a score vector, • refers to element-wise multiplication and F in(•) is a function to find the index of the element having the maximum score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTAL RESULTS</head><p>The proposed method was evaluated on two public benchmark datasets: NTU RGB+D Dataset and UTD-MHAD Dataset. The NTU RGB+D Dataset has 60 actions, 11 of which are interactions between two subjects. Experiments were conducted to evaluate the effectiveness of individual JDMs in the proposed method, three types of score fusion methods and two training models. In all experiments, min = 0, h max = 255, t = 200, m = 24.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation of Individual of JDMs, Fusion Methods and training models</head><p>The results of individual JDMs, three fusion methods and two training models are listed in Table <ref type="table" target="#tab_0">I</ref>, where F and S represent the models of fine-tuning and training from scratch respectively. For example, F-Front (xy) stands for training the front channel JDM by fine-tuning.</p><p>From Table <ref type="table" target="#tab_0">I</ref>, it can be seen that the four JDMs are complimentary to each other, and the multiply-score fusion method improves the final accuracy substantially compared with average and max fusion methods. The fine-tuning performed better on the UTD-MHAD dataset as expected due to its relatively small size. Training the model from scratch works much better on the large NTU RGB+D dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. NTU RGB+D Dataset</head><p>To the best knowledge of authors, NTU RGB+D Dataset <ref type="bibr" target="#b17">[18]</ref> is currently the largest action recognition dataset. It was captured by multiple Kincet v2 cameras. The dataset has more than 56 thousand sequences and 4 million frames, containing 60 actions and interactions (11 in total) by 40 subjects aged between 10 and 35. In addition, it has front view, two side views and left, right 45 degree views of the actions. This dataset is challenging due to the large intra-class and viewpoint variations.</p><p>We follow the protocols used in <ref type="bibr" target="#b17">[18]</ref>, namely cross-subject evaluation and cross-view evaluation. In cross-subject evaluation, The IDs of training subjects in this evaluation are <ref type="bibr">: 1, 2, 4, 5, 8, 9, 13, 14, 15, 16, 17, 18, 19, 25, 27, 28, 31, 34, 35, 38</ref>; remaining subjects are reserved for testing. In crossview evaluation, testing set includes the samples of camera 1,  Method Cross subject Cross view Lie Group <ref type="bibr" target="#b11">[12]</ref> 50.1% 52.8% Dynamic Skeletons <ref type="bibr" target="#b12">[13]</ref> 60.2% 65.2% HBRNN <ref type="bibr" target="#b14">[15]</ref> 59.1% 64.0% Deep RNN <ref type="bibr" target="#b17">[18]</ref> 56.3% 64.1% Part-aware LSTM <ref type="bibr" target="#b17">[18]</ref> 62.9% 70.3% Deep LSTM <ref type="bibr" target="#b17">[18]</ref> 60.7% 67.3% ST-LSTM <ref type="bibr" target="#b18">[19]</ref> 65.2% 76.1% ST-LSTM+ Trust Gate <ref type="bibr" target="#b18">[19]</ref> 69.2% 77.7% JTM <ref type="bibr" target="#b20">[21]</ref> 73.4% 75.2% Proposed Method 76.2% 82.3%</p><p>From this Table we can see that our proposed method achieved the state-of-the-art results compared with both handcrafted features based methods and deep learning methods. The work <ref type="bibr" target="#b11">[12]</ref> focused only on single person action and could not model well multi-person interactions. The dynamic Skeletons method <ref type="bibr" target="#b12">[13]</ref> performed better than some RNN-based methods, implying the weakness of general RNNs <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18]</ref> that only mines the short-term dynamics and tends to overemphasize the temporal information even on large training data. LSTM and its variants <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> performed better due to their ability to utilize long-term context compared to RNNs but it is still weak in exploiting spatial information. The JTM method <ref type="bibr" target="#b20">[21]</ref> encoding the joint trajectories into texture images performed better than the ST-LSTM and Trust Gate methods <ref type="bibr" target="#b18">[19]</ref> in cross subject setting but lower in the cross view setting due to the fact that JTMs are sensitive to viewpoints. The proposed method demonstrates its robustness to view variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. UTD-MHAD Dataset</head><p>UTD-MHAD <ref type="bibr" target="#b21">[22]</ref> is a multimodal action dataset, captured one Microsoft Kinect camera and one wearable inertial sensor. This dataset contains 27 actions performed by 8 subjects (4 females and 4 males) with each subject performing each action 4 times. In the evaluations, a cross-subject protocol is adopted, that is, odd subjects were used for training and even subjects for testing. Table <ref type="table" target="#tab_1">III</ref> compared the performance of the proposed method with the state-of-the-art methods.</p><p>Please notice that both depth and inertial sensor data were used in <ref type="bibr" target="#b21">[22]</ref>. The confusion matrix is shown in <ref type="bibr">Fig 4.</ref> This dataset is very challenging, because of most actions based on arm motion. From the confusion matrix, we can see that the proposed method still needs improvement in distinguishing some actions of similar pairwise joint distances, such as "draw-circle-CW" and "draw-circle-CCW" which are performed in opposite directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE III: Experimental results (accuracies) on UTD-MHAD Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy(%) ElC-KSVD <ref type="bibr" target="#b26">[27]</ref> 76.19% Kinect &amp; Inertial <ref type="bibr" target="#b21">[22]</ref> 79.10% Cov3DJ <ref type="bibr" target="#b27">[28]</ref> 85.58% JTM <ref type="bibr" target="#b20">[21]</ref> 85.81% Proposed Method 88.10%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>This paper addresses the problem of skeleton based human action recognition with ConvNets. A simple yet effective method is proposed that encodes the pair-wise joint distances to color variations, where the spatial-temporal information is converted into texture patterns. This new representation well captures the spatio-temporal information in skeleton sequences, and is suitable for both single-view and cross-view action recognition. State-of-the-art results were obtained on two widely-used datasets and have verified the effectiveness of the proposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: The framework of the proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Sample JDMs generated by the proposed method on the NTU RGB+D dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: The process of mapping joint distances to colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: The confusion matrix of proposed method for UTD-MHAD.</figDesc><graphic coords="4,360.39,58.50,190.20,141.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Comparisons of the different schemes on the NTU RGB+D and UTD-MHAD dataset. samples of cameras 2 and 3 were used for training. TableIIlists the performance of the proposed method and those reported to date.</figDesc><table><row><cell></cell><cell></cell><cell>Dataset</cell><cell></cell></row><row><cell>Scheme</cell><cell cols="2">NTU RGB+D</cell><cell>UTD-MHAD</cell></row><row><cell></cell><cell cols="3">cross-subject cross-view cross-subject</cell></row><row><cell>F-Front(xy)</cell><cell>58.2%</cell><cell>60.2%</cell><cell>79.3%</cell></row><row><cell>F-Top(xz)</cell><cell>58.7%</cell><cell>57.3%</cell><cell>74.7%</cell></row><row><cell>F-Side(yz)</cell><cell>62.1%</cell><cell>60.4%</cell><cell>68.8%</cell></row><row><cell>F-All(xyz)</cell><cell>60.0%</cell><cell>67.9%</cell><cell>80.0%</cell></row><row><cell>F-Max-Score fusion</cell><cell>68.5%</cell><cell>73.8%</cell><cell>85.1%</cell></row><row><cell>F-Ave-Score fusion</cell><cell>69.3%</cell><cell>74.1%</cell><cell>86.2%</cell></row><row><cell>F-Mul-Score fusion</cell><cell>71.5%</cell><cell>77.1%</cell><cell>88.1%</cell></row><row><cell>S-Front(xy)</cell><cell>65.1%</cell><cell>67.4%</cell><cell>68.6%</cell></row><row><cell>S-Top(xz)</cell><cell>63.5%</cell><cell>63.3%</cell><cell>67.9%</cell></row><row><cell>S-Side(yz)</cell><cell>62.8%</cell><cell>63.5%</cell><cell>65.4%</cell></row><row><cell>S-All(xyz)</cell><cell>66.8%</cell><cell>74.9%</cell><cell>73.2%</cell></row><row><cell>S-Max-Score fusion</cell><cell>72.9%</cell><cell>79.1%</cell><cell>80.5%</cell></row><row><cell>S-Ave-Score fusion</cell><cell>74.4%</cell><cell>80.6%</cell><cell>80.9%</cell></row><row><cell>S-Mul-Score fusion</cell><cell>76.2%</cell><cell>82.3%</cell><cell>85.8%</cell></row></table><note><p>and</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Experimental results (accuracies) on NTU RGB+D Dataset.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Copyright c 2017 IEEE. Personal use of this material is permitted. However, permission to use this material for any other purposes must be obtained from the IEEE by sending a request to pubs-permissions@ieee.org.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>IEEE SIGNAL PROCESSING LETTERS, VOL. XX, NO. XX, XX 201Y</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Action recognition based on a bag of 3D points</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>CVPRW</publisher>
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scene flow to action map: A new representation for rgb-d based action recognition with convolutional neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1297" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Eigenjoints-based action recognition using naive-bayes-nearest-neighbor</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="14" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mining actionlet ensemble for action recognition with depth cameras</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1290" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">View invariant human action recognition using histograms of 3D joints</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The moving pose: An efficient 3D kinematics descriptor for low-latency action recognition and detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2752" to="2759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A new descriptor for multiple 3D motion trajectories recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>IEEE International Conference on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="4749" to="4754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Histogram of oriented displacements (HOD): Describing trajectories of human joints for action recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Torki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>El-Saban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Joint Conference on Artificial Intelligence(IJCAI)</title>
		<meeting>International Joint Conference on Artificial Intelligence(IJCAI)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1351" to="1357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bioinspired dynamic 3D discriminative skeletal features for human action recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ofli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kurillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajcsy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="471" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fusing spatiotemporal features and joints for 3D action recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="486" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3D skeletons as points in a lie group</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint angles similarities and HOG2 for action recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="465" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mining midlevel features for action recognition based on effective skeleton representation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Digital lmage Computing: Techniques and Applications</title>
		<meeting>IEEE International Conference on Digital lmage Computing: Techniques and Applications</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Differential recurrent neural networks for action recognition</title>
		<author>
			<persName><forename type="first">V</forename><surname>Veeriah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4041" to="4049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Co-occurrence feature learning for skeleton based action recognition using regularized deep LSTM networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conference on Artificial Intelligence</title>
		<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3697" to="3704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">NTU RGB+ D: A large scale dataset for 3D human activity analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatio-temporal LSTM with trust gates for 3D human action recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Skeleton based action recognition with convolutional neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asian Conference on Pattern Recognition</title>
		<meeting>Asian Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="579" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Action recognition based on joint trajectory maps using convolutional neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM on Multimedia Conference</title>
		<meeting>ACM on Multimedia Conference</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="102" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">UTD-MHAD: A multimodal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kehtarnavaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference onImage Processing</title>
		<meeting>IEEE International Conference onImage essing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convnets-based action recognition from depth maps through virtual cameras and pseudocoloring</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM on Multimedia Conference</title>
		<meeting>ACM on Multimedia Conference</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1119" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Action recognition from depth maps using deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Human-Machine Systems</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="498" to="509" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Skeleton optical spectra based action recognition using convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Discriminative key pose extraction using extended LC-KSVD for action recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Digital lmage Computing: Techniques and Applications</title>
		<meeting>IEEE International Conference on Digital lmage Computing: Techniques and Applications</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Human action recognition using a temporal hierarchy of covariance descriptors on 3D joint locations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Torki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>El-Saban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Joint Conference on Artificial Intelligence(IJCAI)</title>
		<meeting>International Joint Conference on Artificial Intelligence(IJCAI)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2466" to="2472" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
