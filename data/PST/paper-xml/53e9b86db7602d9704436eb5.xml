<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Extremal Inequality Motivated by Multiterminal Information Theoretic Problems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2006-11-08">November 8, 2006</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tie</forename><surname>Liu</surname></persName>
							<email>tieliu@ece.tamu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Pramod</forename><surname>Viswanath</surname></persName>
							<email>pramodv@uiuc.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">the Texas A&amp;M University</orgName>
								<address>
									<postCode>77843</postCode>
									<settlement>College Station</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Extremal Inequality Motivated by Multiterminal Information Theoretic Problems</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2006-11-08">November 8, 2006</date>
						</imprint>
					</monogr>
					<idno type="MD5">B89E7C78684B5723DB6F4F9E0F52734A</idno>
					<idno type="arXiv">arXiv:cs.IT/0604025v3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Differential entropy</term>
					<term>distributed source coding</term>
					<term>entropy-power inequality (EPI)</term>
					<term>Fisher information</term>
					<term>vector Gaussian broadcast channel</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We prove a new extremal inequality, motivated by the vector Gaussian broadcast channel and the distributed source coding with a single quadratic distortion constraint problems. As a corollary, this inequality yields a generalization of the classical entropypower inequality (EPI). As another corollary, this inequality sheds insight into maximizing the differential entropy of the sum of two dependent random variables.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Like many other important results in information theory, the classical entropy-power inequality (EPI) was discovered by Shannon <ref type="bibr" target="#b0">[1]</ref> (even though the first rigorous proof was given by Stam <ref type="bibr" target="#b1">[2]</ref> and was later simplified by Blachman <ref type="bibr" target="#b2">[3]</ref>). In <ref type="bibr">[1, p. 641</ref>], Shannon used the EPI to prove a lower bound on the capacity of additive noise channels. While this first application was on a point-to-point scenario, the real value of the EPI showed up much later in the multiterminal source/channel coding problems where the tension among users of different interests cannot be resolved by Fano's inequality alone. The most celebrated examples include Bergman's solution <ref type="bibr" target="#b3">[4]</ref> to the scalar Gaussian broadcast channel problem, Oohama's solution <ref type="bibr" target="#b4">[5]</ref> to the scalar quadratic Gaussian CEO problem, and Ozarow's solution <ref type="bibr" target="#b5">[6]</ref> to the scalar Gaussian two-description problem.</p><p>Denote the set of real numbers by R. Let X, Z be two independent random vectors with densities in R n . The classical EPI states that exp</p><formula xml:id="formula_0">2 n h(X + Z) ≥ exp 2 n h(X) + exp 2 n h(Z) .<label>(1)</label></formula><p>Here h(X) denotes the differential entropy of X, and the equality holds if and only if X, Z are Gaussian and with proportional covariance matrices.</p><p>Fix Z to be Gaussian with covariance matrix K Z . Assume that K Z is strictly positive definite. Consider the optimization problem max</p><formula xml:id="formula_1">p(x) {h(X) -µh(X + Z)} ,<label>(2)</label></formula><p>where µ ∈ R, and the maximization is over all random vector X independent of Z. The classical EPI can be used to show that for any µ &gt; 1, a Gaussian X with a covariance matrix proportional to K Z is an optimal solution of this optimization problem. This can be done as follows. By the classical EPI, h(X) -µh(X + Z) ≤ h(X) -µn 2 log exp 2 n h(X) + exp 2 n h(Z) .</p><p>For any fixed a ∈ R and µ &gt; 1, the function</p><formula xml:id="formula_3">f (t; a) = t - µn 2 log exp 2 n t + exp 2 n a ,<label>(4)</label></formula><p>is concave in t and has a global maxima at</p><formula xml:id="formula_4">t = a - n 2 log(µ -1). (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>Hence the right-hand side of (3) can be further bounded from above as</p><formula xml:id="formula_6">h(X) - µn 2 log exp 2 n h(X) + exp 2 n h(Z) ≤ f h(Z) - n 2 log(µ -1); h(Z) .<label>(6)</label></formula><p>The equality conditions of (3) and <ref type="bibr" target="#b5">(6)</ref> imply that a Gaussian X with covariance matrix (µ -1) -1 K Z is an optimal solution of the optimization problem <ref type="bibr" target="#b1">(2)</ref>.</p><p>Note that in solving the above optimization problem, the classical EPI not only forces the optimal solution to be Gaussian, but also imposes a certain covariance structure on the Gaussian optimal solution. Hence a natural question to ask is what happens if there is an extra covariance constraint such that the original Gaussian optimal solution is no longer admissible. In that case, the classical EPI can still be used; however, the equality condition may no longer be met by the new optimal Gaussian solution because it may no longer have the required proportionality. In particular, one would be interested in finding out whether under the extra covariance constraint, a Gaussian X is still an optimal solution to optimization problems such as <ref type="bibr" target="#b1">(2)</ref>.</p><p>One particular type of covariance constraint is the following matrix covariance constraint: Cov(X) S. <ref type="bibr" target="#b6">(7)</ref> Here Cov(X) denotes the covariance matrix of X, " " represents "less or equal to" in the positive semidefinite partial ordering of real symmetric matrices, and S is a positive semidefinite matrix. The reason for considering such a matrix covariance constraint is largely due to its generality: it subsumes many other covariance constraints including the important trace constraint.</p><p>The focus of this paper is the following slightly more general optimization problem:</p><formula xml:id="formula_7">max p(x) h(X + Z 1 ) -µh(X + Z 2 ) subject to Cov(X) S,<label>(8)</label></formula><p>where Z 1 , Z 2 are Gaussian vectors with strictly positive definite covariance matrix K Z 1 and K Z 2 , respectively, and the maximization is over all random vector X independent of Z 1 and Z 2 . As we shall see, such an optimization problem appears naturally when one is to evaluate certain genie-aided outer bounds on the capacity/rate region for the vector Gaussian broadcast channel and the distributed source coding with a single quadratic distortion constraint problems. Our main result is summarized in the following theorem.</p><p>Theorem 1 For any µ ≥ 1 and any positive semidefinite S, a Gaussian X is an optimal solution of the optimization problem <ref type="bibr" target="#b7">(8)</ref>.</p><p>The rest of the paper is organized as follows. In Section 2, we prove our main result. We give two proofs: a direct proof using the classical EPI, and a strengthened proof following the perturbation approach of Stam <ref type="bibr" target="#b1">[2]</ref> and Blachman <ref type="bibr" target="#b2">[3]</ref>. In Section 3, we discuss some ramifications of the main result. In Section 4, we apply our main result to the vector Gaussian broadcast channel and the distributed source coding with a single quadratic distortion constraint problems. For the former problem, our main result leads to an exact characterization of the capacity region. Finally, in Section 5, we conclude by summarizing our contribution in the context of the applications of information theoretic inequalities in resolving multiterminal transmission/compression problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proofs of the Main Result 2.1 A Direct Proof</head><p>In this first proof, we show that the classical EPI can be appropriately used to give a direct proof to Theorem 1. The fact that the classical EPI is relevant here is not surprising, considering that the objective function of the optimization problem <ref type="bibr" target="#b7">(8)</ref> involves the entropy of the sum of two independent random vectors. Nonetheless, based on our discussion in Section 1, a direct use of the classical EPI might be loose because the covariance matrix of the optimal Gaussian solution might not have the required proportionality.</p><p>Our approach to resolve this issue is inspired by the mathematical import of an interesting technique, called enhancement, introduced by Weingarten et al. <ref type="bibr" target="#b6">[7]</ref>. Our proof combines the idea of enhancement with the worst additive noise lemma <ref type="bibr" target="#b7">[8]</ref>, [9, Lemma II.2] stated as follows.</p><p>Lemma 2 (Worst additive noise lemma) Let Z be a Gaussian vector with covariance matrix K Z , and let K X be a positive semidefinite matrix. Consider the following optimization problem:</p><formula xml:id="formula_8">min p(x) I(Z; Z + X) subject to Cov(X) = K X ,<label>(9)</label></formula><p>where I(Z; Z + X) denotes the mutual information between Z and X + Z, and the maximization is over all random vector X independent of Z. A Gaussian X is an optimal solution of this optimization problem (no matter K X and K Z are proportional or not).</p><p>The details of the direct proof are in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">A Perturbation Proof</head><p>From the optimization theoretic point of view, the power of the classical EPI lies in its ability to find global optima in nonconvex optimization problems such as <ref type="bibr" target="#b1">(2)</ref>. Hence one can imagine that proof of the classical EPI cannot be accomplished by any local optimization techniques. Indeed, in their classical proof Stam <ref type="bibr" target="#b1">[2]</ref> and Blachman <ref type="bibr" target="#b2">[3]</ref> used a perturbation approach, which amounts to find a monotone path from any distributions of the participating random vectors (i.e., X and Z in (1)) to the optimal distributions (Gaussian distributions with proportional covariance matrices) for which the classical EPI holds with equality. The monotonicity guarantees that any distributions along the path satisfy the desired inequality, and hence the ones to begin with. A different perturbation was later used by Dembo et al. <ref type="bibr">[10, p. 1509</ref>]. The main idea, however, remains the same as that of Stam and Blachman's.</p><p>Proving monotonicity needs isoperimetric inequalities. In case of the classical EPI, it needs the classical Fisher information inequality (FII) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">Theorem 13]</ref>. Fisher information is an important quantity in statistical estimation theory. An interesting estimation theoretic proof using the data processing inequality for Fisher information was given by Zamir <ref type="bibr" target="#b10">[11]</ref>. (The classical FII can also be proved by using the standard data processing inequality for mutual information, invoking a connection between Fisher information and mutual information explicitly established by Guo et al. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr">Corollary 2]</ref>.) This connection between the EPI and the FII is usually thought of as the estimation view of the classical EPI.</p><p>We can use the perturbation idea to give a stronger proof to Theorem 1. We construct a monotone path using the "covariance-preserving" transformation, which was previously used by Dembo et al. <ref type="bibr">[10, p. 1509]</ref> in their perturbation proof of the classical EPI. To prove the monotonicity, we need the following results on Fisher information matrix.</p><p>Lemma 3 Denote by J(X) the Fisher information matrix of random vector X.</p><p>1. (Cramér-Rao inequality) For any random vector U (of which the Fisher information matrix is well defined) with a strictly positive definite covariance matrix,</p><formula xml:id="formula_9">J(U) Cov -1 (U). (<label>10</label></formula><formula xml:id="formula_10">)</formula><p>2. (Matrix FII) For any independent random vectors U, V and any square matrix A,</p><formula xml:id="formula_11">J(U + V) AJ(U)A t + (I -A)J(V)(I -A) t . (<label>11</label></formula><formula xml:id="formula_12">)</formula><p>Here I is the identity matrix.</p><p>For completeness, a proof of the above lemma using the properties of score function is provided in Appendix B. The details of the perturbation proof are in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Ramifications of the Main Result</head><p>In this section, we discuss two special cases of the optimization problem <ref type="bibr" target="#b7">(8)</ref> to demonstrate the breadth of our main result. We term these two scenarios as the degraded case and the extremely-skewed case. By considering the degraded case, we prove a generalization of the classical EPI. By considering the extremely-skewed case, we establish a connection between our result and the classical result of Cover and Zhang <ref type="bibr" target="#b12">[13]</ref> on the maximum differential entropy of the sum of two dependent random variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Degraded Case</head><p>In the degraded case, we have either</p><formula xml:id="formula_13">K Z 1 K Z 2 or K Z 1 K Z 2 . First consider the case K Z 1 K Z 2 .</formula><p>We have the following results.</p><p>Corollary 4 Let Z 1 , Z be two independent Gaussian vectors with covariance matrix K Z 1 and K Z , respectively. Assume that K Z 1 is strictly positive definite. Consider the following optimization problem:</p><formula xml:id="formula_14">max p(x) h(X + Z 1 ) -µh(X + Z 1 + Z) subject to Cov(X) S,<label>(12)</label></formula><p>where the maximization is over all random vector X independent of Z 1 and Z. For any µ ∈ R and any positive semidefinite S, a Gaussian X is an optimal solution of this optimization problem.</p><p>Proof. For µ ≥ 1, the corollary is a special case of Theorem 1 with Z 2 = Z 1 + Z. For µ ≤ 0, the corollary also holds because h(X + Z 1 ) and h(X + Z 1 + Z) are simultaneously maximized when X is Gaussian with covariance matrix S. This left us the only case where µ ∈ (0, 1), which we prove next.</p><p>The objective function of optimization problem <ref type="bibr" target="#b11">(12)</ref> can be written as</p><formula xml:id="formula_15">(1 -µ)h(X + Z 1 ) -µI(Z; X + Z 1 + Z).<label>(13)</label></formula><p>Here h(X + Z 1 ) is maximized when X is Gaussian with covariance matrix S. By the worst noise result of Lemma 2, I(Z; X + Z 1 + Z) is minimized when X is Gaussian. Further within the Gaussian class, the one with the full covariance matrix S minimizes I(Z; Z + X + Z 1 ). For µ ∈ (0, 1), both µ and 1 -µ are positive. We conclude that the objective function <ref type="bibr" target="#b12">(13)</ref> is maximized when X is Gaussian with covariance matrix S. This completes the proof.</p><p>Corollary 5 Let Z be a Gaussian vector with covariance matrix K Z . Assume that K Z is strictly positive definite. Consider the following optimization problem</p><formula xml:id="formula_16">max p(x) h(X) -µh(X + Z) subject to Cov(X) S,<label>(14)</label></formula><p>where the maximization is over all random vector X independent of Z. For any µ ∈ R and any positive semidefinite S, a Gaussian X is an optimal solution of this optimization problem.</p><p>Observe that the optimization problem ( <ref type="formula" target="#formula_16">14</ref>) is simply a constrained version of the optimization problem <ref type="bibr" target="#b1">(2)</ref>. Recall from Section 1 that the optimization problem (2) can be solved by the classical EPI. Conversely, it can be shown that the special case of the classical EPI with one of the participant random vectors (say, Z in (1)) fixed to be Gaussian can also be obtained from the fact that a Gaussian X is an optimal solution of the optimization problem (2). This can be done as follows. Choosing</p><formula xml:id="formula_17">µ = 1 + exp 2 n (h(Z) -h(X)) ,<label>(15)</label></formula><p>we have from (5</p><formula xml:id="formula_18">) that h(X * G ) = h(Z) - n 2 log(µ -1) = h(X).<label>(16)</label></formula><p>Since X * G is an optimal solution of the optimization problem (2) (recall that X * G has a special covariance structure of being proportional to K Z ), we have</p><formula xml:id="formula_19">h(X) -µh(X + Z) ≤ h(X * G ) -µh(X * G + Z).<label>(17)</label></formula><p>Substituting ( <ref type="formula" target="#formula_18">16</ref>) into (17), we have h(X + Z) ≥ h(X * G + Z) for any random vector X independent of Z and satisfying h(X) = h(X * G ). This is precisely the Costa-Cover form of the classical EPI [10, Theorem 6], so we have proved the converse statement.</p><p>In light of the above statements, Corollary 5 can be thought of a generalization of the classical EPI. For technical reasons, we were not able to prove Corollary 5 directly from Corollary 4 by letting K Z 1 vanish. Instead, we can resort to arguments (direct and perturbation ones) similar to those for Theorem 1 to prove Corollary 5. Observe that in the optimization problem <ref type="bibr" target="#b13">(14)</ref> the lower constraint K X 0 never bites, so no enhancement is needed in the perturbation proof. The details of the proof is omitted from the paper.</p><p>We now turn to the other degraded case where</p><formula xml:id="formula_20">K Z 1 K Z 2 . Consider the optimization problem max p(x) h(X + Z 2 + Z) -µh(X + Z 2 ) subject to Cov(X) S,<label>(18)</label></formula><p>where the maximization is over all random vector X independent of Z 2 and Z. For any µ ≥ 1, by Theorem 1 a Gaussian X is an optimal solution of this optimization problem. For µ ≤ 0, this is also true because h(X + Z 2 + Z) and h(X + Z 2 ) are simultaneously maximized when X is Gaussian with covariance matrix S. However, as we shall see next, this is generally not the case for µ ∈ (0, 1).</p><p>Consider the cases where</p><formula xml:id="formula_21">0 ≺ µ 1 -µ K Z -K Z 2 ≺ S.<label>(19)</label></formula><p>(Note that this can only happen when µ ∈ (0, 1) and also depends on the realizations of K Z , K Z 2 and S.) Under this assumption, we can verify that the covariance matrix K * X of X * G must satisfy:</p><formula xml:id="formula_22">K * X = µ 1 -µ K Z -K Z 2 . (<label>20</label></formula><formula xml:id="formula_23">)</formula><p>Let X be a non-Gaussian random vector satisfying:</p><formula xml:id="formula_24">1. h(X + Z 2 ) = h(X * G + Z 2 ); 2. Cov(X) S.</formula><p>Such an X exists because by the assumption, K * X is strictly between 0 and S. Since X is non-Gaussian, by the Costa-Cover form of the classical EPI, we have</p><formula xml:id="formula_25">h(X + Z 2 + Z) &gt; h(X * G + Z 2 + Z).<label>(21)</label></formula><p>We thus conclude that at least for the cases where the condition ( <ref type="formula" target="#formula_21">19</ref>) holds, the optimal Gaussian solution X * G cannot be an optimal solution of the optimization problem (18).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Extremely-Skewed Case</head><p>Suppose that Z 1 , Z 2 are in R 2 . Let</p><formula xml:id="formula_26">K Z 1 = V 1 Σ 1 V t 1 , K Z 2 = V 2 Σ 2 V t 2 ,<label>(22)</label></formula><p>where V 1 , V 2 are orthogonal matrices and</p><formula xml:id="formula_27">Σ 1 = Diag(λ 11 , λ 12 ), Σ 2 = Diag(λ 21 , λ 22 )<label>(23)</label></formula><p>are diagonal matrices. Consider the limiting situation where λ 12 , λ 21 → ∞, while λ 11 , λ 22 are kept fixed. Compared with the degraded case where K Z 1 dominates K Z 2 in every possible direction (or vice versa), this situation between K Z 1 and K Z 2 is extremely skewed. We have the following result.</p><p>Corollary 6 Let Z be a Gaussian random variable, and let v 1 , v 2 be two deterministic vectors in R 2 . Consider the optimization problem</p><formula xml:id="formula_28">max p(x) h(v t 1 X + Z) -µh(v t 2 X + Z) subject to Cov(X) S,<label>(24)</label></formula><p>where the maximization is over all random vector X (in R 2 ) independent of Z. For any µ ≥ 1 and any positive semidefinite S, a Gaussian X is an optimal solution of this optimization problem.</p><p>Proof. See Appendix D.</p><p>Next, we use Corollary 6 to solve an optimization problem that involves maximizing the differential entropy of the sum of two dependent random variables. To put it in perspective, let us first consider the following simple optimization problem:</p><formula xml:id="formula_29">max p(x 1 ,x 2 ) h(X 1 + X 2 ) subject to Var(X 1 ) ≤ a 1 , Var(X 2 ) ≤ a 2 ,<label>(25)</label></formula><p>where a 1 , a 2 ≥ 0 are real numbers, Var(X) denotes the variance of X, and the maximization is over all jointly distributed random variables (X 1 , X 2 ). The solution to this optimization problem is clear: h(X 1 + X 2 ) is maximized when X 1 , X 2 are jointly Gaussian with variance a 1 and a 2 , respectively, and are aligned, i.e., X 1 = a 1 /a 2 X 2 almost surely.</p><p>Replacing both variance constraints in the optimization problem (25) by the entropy constraints, we have the following optimization problem:</p><formula xml:id="formula_30">max p(x 1 ,x 2 ) h(X 1 + X 2 ) subject to h(X 1 ) ≤ a 1 , h(X 2 ) ≤ a 2 ,<label>(26)</label></formula><p>where a 1 , a 2 ∈ R, and the maximization is over all jointly distributed random variables (X 1 , X 2 ). Different from the optimization problem (25), a jointly Gaussian (X 1 , X 2 ) is not always an optimal solution of (26). This can seen as follows. Consider the case a 1 = a 2 . Let (X * 1G , X * 2G ) be the optimal Gaussian solution of the optimization problem (26). We have X * 1G = X * 2G almost surely, i.e., X * 1G and X * 2G are aligned and have the same marginal distribution. Consider all jointly distributed random variables (X 1 , X 2 ) for which X 1 , X 2 have the same marginal density function f which satisfies:</p><formula xml:id="formula_31">1. h(X 1 ) = h(X * 1G ); 2. f is not log-concave.</formula><p>The classical result of Cover and Zhang <ref type="bibr" target="#b12">[13]</ref> asserts that among all (X 1 , X 2 ) satisfying the above conditions, there is at least one that satisfies</p><formula xml:id="formula_32">h(X 1 + X 2 ) &gt; h(2X 1 ) = h(2X * 1G ) = h(X * 1G + X * 2G ). (<label>27</label></formula><formula xml:id="formula_33">)</formula><p>We thus conclude that a jointly Gaussian (X 1 , X 2 ) is not always an optimal solution of the optimization problem (26).</p><p>Between (25) and ( <ref type="formula" target="#formula_30">26</ref>) is the following optimization problem:</p><formula xml:id="formula_34">max p(x 1 ,x 2 ) h(X 1 + X 2 ) subject to Var(X 1 ) ≤ a 1 , h(X 2 ) ≤ a 2 ,<label>(28)</label></formula><p>where a 1 , a 2 are real numbers with a 1 ≥ 0, and the maximization is over all jointly distributed random variables (X 1 , X 2 ). The question whether a Gaussian (X 1 , X 2 ) is an optimal solution of this optimization problem remains, to our best knowledge, an open problem. The following result, however, can be proved using Corollary 6.</p><p>Corollary 7 Let Z be a Gaussian variable, and let a 1 , a 2 be real numbers with a 1 ≥ 0.</p><p>Consider the optimization problem</p><formula xml:id="formula_35">max p(x 1 ,x 2 ) h(X 1 + X 2 + Z) subject to Var(X 1 ) ≤ a 1 , h(X 2 + Z) ≤ a 2 , (<label>29</label></formula><formula xml:id="formula_36">)</formula><p>where the maximization is over all jointly distributed random variables (X 1 , X 2 ) independent of Z. A Gaussian (X 1 , X 2 ) is an optimal solution of this optimization problem for any a 1 ≥ 0 and any h(Z) ≤ a 2 ≤ a * 2 where</p><formula xml:id="formula_37">a * 2 = 1 2 log 2πe Var(Z) + 1 4 a 1 + 4Var(Z) - √ a 1 2 . (<label>30</label></formula><formula xml:id="formula_38">)</formula><p>Proof. See Appendix E.</p><p>4 Applications in Multiterminal Information Theory</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Vector Gaussian Broadcast Channel</head><p>We now use our main result to give an exact characterization of the capacity region of the vector Gaussian broadcast channel. The capacity region of the vector Gaussian broadcast channel was first characterized by Weigarten et al. <ref type="bibr" target="#b6">[7]</ref>.</p><p>Consider the following two-user discrete-time vector Gaussian broadcast channel:</p><formula xml:id="formula_39">Y k [m] = X[m] + Z k [m], k = 1, 2,<label>(31)</label></formula><p>where {X[m]} is the channel input subject to an average matrix power constraint</p><formula xml:id="formula_40">1 N N m=1 X[m]X t [m] S,<label>(32)</label></formula><p>and the noise {Z k [m]} is i.i.d. Gaussian with zero mean and strictly positive definite covariance matrix K Z k and is independent of {X[m]}. The covariance structure of the Gaussian noise models a scalar Gaussian broadcast channel with memory. Alternatively, it can also model the downlink of a cellular system with multiple antennas; this was the motivation of <ref type="bibr" target="#b6">[7]</ref>.</p><p>A vector Gaussian broadcast is in general a nondegraded broadcast channel. An exact characterization of the capacity region had been a long-standing open problem in multiterminal information theory, particularly when viewed in the context of a scalar Gaussian broadcast channel with memory. Prior to <ref type="bibr" target="#b6">[7]</ref>, only bounds were known. An outer bound, derived by Marton and Körner [14, <ref type="bibr">Theorem 5]</ref>, is given by</p><formula xml:id="formula_41">O = O 1 ∩ O 2 , where O 1 is the set of rate pairs (R 1 , R 2 ) satisfying R 1 ≤ I(X; Y 1 |U) (33) R 2 ≤ I(U; Y 2 ) (<label>34</label></formula><formula xml:id="formula_42">)</formula><p>for some p(y </p><formula xml:id="formula_43">R 1 ≤ I(V ; Y 1 ) (35) R 2 ≤ I(X; Y 2 |V )<label>(36)</label></formula><p>for some p(y 1 , y 2 , x, v) = p(y 1 , y 2 |x)p(x, v) such that p(y 1 , y 2 |x) is the channel transition matrix and p(x) satisfies the constraint E[XX t ] S.</p><p>Next, we derive a tight upper bound on the achievable weighted sum rate</p><formula xml:id="formula_44">µ 1 R 1 + µ 2 R 2 ,<label>(37)</label></formula><p>using the Marton-Körner outer bound as the starting point. Since a capacity region is always convex (per time-sharing argument), an exact characterization of all the achievable weighted sum rates for all nonnegative µ 1 , µ 2 provides an exact characterization of the entire capacity region. First consider the case µ 2 ≥ µ 1 ≥ 0. By the Marton-Körner outer bound, any achievable rate pair (R 1 , R 2 ) must satisfy:</p><formula xml:id="formula_45">µ 1 R 1 + µ 2 R 2 ≤ µ 1 • max {I(X; Y 1 |U) + µI(U; Y 2 )} (38) = µ 1 • max {-h(Z 1 ) + µh(X + Z 2 ) + [h(X + Z 1 |U) -µh(X + Z 2 |U)]} . (39)</formula><p>Here µ = µ 2 µ 1 ≥ 1, and the maximization is over all (U, X) independent of (Z 1 , Z 2 ) and satisfying the matrix constraint E[XX t ] S. Consider the terms h(Z 1 ), h(X + Z 2 ) and h(X + Z 1 |U) -µh(X + Z 2 |U) separately. We have</p><formula xml:id="formula_46">h(Z 1 ) = 1 2 log ((2πe) n |K Z 1 |)<label>(40)</label></formula><p>and</p><formula xml:id="formula_47">h(X + Z 2 ) ≤ 1 2 log ((2πe) n |S + K Z 2 |) .<label>(41)</label></formula><p>Further note that maximizing h(X + Z 1 |U) -µh(X + Z 2 |U) is simply a conditional version of the optimization problem <ref type="bibr" target="#b7">(8)</ref>. We have the following result, which is a conditional version of Theorem 1.</p><p>Theorem 8 Let Z 1 , Z 2 be two Gaussian vectors with strictly positive definite covariance matrices K Z 1 and K Z 2 , respectively. Let µ ≥ 1 be a real number, S be a positive semidefinite matrix, and U be a random variable independent of Z 1 and Z 2 . Consider the optimization problem</p><formula xml:id="formula_48">max p(x|u) h(X + Z 1 |U) -µh(X + Z 2 |U) subject to Cov(X|U) S,<label>(42)</label></formula><p>where the maximization is over all conditional distribution of X given U independent of Z 1 and Z 2 . A Gaussian p(x|u) with the same covariance matrix for each u is an optimal solution of this optimization problem.</p><p>The result of the above theorem has two parts. The part that says a Gaussian p(x|u) is an optimal solution follows directly from Theorem 1; the part that says the optimal Gaussian p(x|u) has the same covariance matrix for each u is equivalent to that the optimal value of the optimization problem ( <ref type="formula" target="#formula_7">8</ref>) is a concave function of S. Despite being a matrix problem, a direct proof of the concavity turns out to be difficult. Instead, Theorem 8 can be proved following the same footsteps as those for Theorem 1, except that we need to replace the classical EPI by a conditional version proved by Bergmans [4, Lemma II]. Let Z be a Gaussian vector. Bergmans' conditional EPI states that exp</p><formula xml:id="formula_49">2 n h(X + Z|U) ≥ exp 2 n h(X|U) + exp 2 n h(Z)<label>(43)</label></formula><p>for any (X, U) independent of Z. The equality holds if and only if conditional on U = u, X is Gaussian with a covariance matrix proportional to that of Z and has the same covariance matrix for each u. The details of the proof are omitted from the paper.</p><p>By Theorem 8, we have</p><formula xml:id="formula_50">h(X+Z 1 |U)-µh(X+Z 2 |U) ≤ max 0 K X S 1 2 log ((2πe) n |K X + K Z 1 |) - µ 2 log ((2πe) n |K X + K Z 2 |) .</formula><p>(44) Substituting (40), (41) and (44) into (39), we obtain</p><formula xml:id="formula_51">µ 1 R 1 + µ 2 R 2 ≤ max 0 K X S µ 1 2 log K X + K Z 1 K Z 1 + µ 2 2 log S + K Z 2 K X + K Z 2 . (<label>45</label></formula><formula xml:id="formula_52">)</formula><p>Note that the weighted sum rates give by (45) can be achieved by dirty-paper coding <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, so (45) is an exact characterization of all the achievable weighted sum rates for µ 2 ≥ µ 1 ≥ 0.</p><p>For µ 1 ≥ µ 2 ≥ 0, we have from the Marton-Körner bound that</p><formula xml:id="formula_53">µ 1 R 1 + µ 2 R 2 ≤ µ 2 • max p(X,V ) {µI(V ; Y 1 ) + I(X; Y 2 |V )} . (<label>46</label></formula><formula xml:id="formula_54">)</formula><p>Here µ = µ 1 µ 2 ≥ 1, and the maximization is over all (V, X) independent of (Z 1 , Z 2 ) and satisfying the matrix constraint E[XX t ] S. Relabeling V as U, the optimization problem becomes identical to that in (38). We thus conclude that</p><formula xml:id="formula_55">µ 1 R 1 + µ 2 R 2 ≤ max 0 K X S µ 1 2 log S + K Z 1 K X + K Z 1 + µ 2 2 log K X + K Z 2 K Z 2 . (<label>47</label></formula><formula xml:id="formula_56">)</formula><p>is an exact characterization of all the achievable weighted sum rates for µ 1 ≥ µ 2 ≥ 0. This settles the problem of characterizing the entire capacity region of the vector Gaussian broadcast channel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Distributed Source Coding with a Single Quadratic Distortion Constraint</head><p>Our result is also relevant in the following distributed source coding problem. Let The encoder is only allowed to perform separate encoding on the sources. The decoder, on the other hand, can reconstruct the sources based on both encoded messages. We wish to characterize the entire rate region for which the quadratic distortion for reconstructing</p><formula xml:id="formula_57">{Y 1 [m]} at the decoder 1 N N m=1 Y 1 [m] -Y 1 [m] Y 1 [m] -Y 1 [m] t D. (<label>48</label></formula><formula xml:id="formula_58">)</formula><p>(There is no distortion constraint on the source {Y 2 [m]}.) This is the so-called distributed source coding with a single quadratic distortion constraint problem.</p><formula xml:id="formula_59">Note that Y 1 [m], Y 2 [m]</formula><p>are jointly Gaussian, so without loss of generality we can write</p><formula xml:id="formula_60">Y 1 [m] = AY 2 [m] + Z[m],<label>(49)</label></formula><p>where </p><formula xml:id="formula_61">A</formula><formula xml:id="formula_62">R 1 ≥ I(Y 1 ; Y 1 |U) R 2 ≥ I(U; Y 2 ) (<label>50</label></formula><formula xml:id="formula_63">)</formula><p>for some p(u, y 1 , y 1 , y 2 ) = p( y 1 |u, y 1 )p(u|y 2 )p(y 1 , y 2 ), where p(y 1 , y 2 ) is the joint distribution of the sources and p( y 1 |u, y 1 ) satisfies the matrix constraint</p><formula xml:id="formula_64">E[(Y 1 -Y 1 )(Y 1 -Y 1 ) t ] D.</formula><p>The proof is deferred to Appendix F. Next, we derive a lower bound on all the achievable weighted sum rates µ 1 R 1 + µ 2 R 2 for all nonnegative µ 1 , µ 2 , using this outer bound as the starting point.</p><p>By the outer bound (50), all the achievable rate pairs (R 1 , R 2 ) must satisfy:</p><formula xml:id="formula_65">µ 1 R 1 + µ 2 R 2 ≥ µ 1 • min p(u,b y|y 1 ,y 2 ) µI(Y 1 ; Y 1 |U) + I(U; Y 2 ) (51) = µ 1 • min p(u,b y|y 1 ,y 2 ) h(Y 2 ) -µh(Y 1 | Y 1 , U) -[h(Y 2 |U) -µh(Y 2 + Z|U)] . (<label>52</label></formula><formula xml:id="formula_66">)</formula><p>Here µ = µ 2 µ 1 ≥ 0, and the minimization is over all p(u, </p><formula xml:id="formula_67">y|y 1 , y 2 ) such that U is independent of Z and E[(Y 1 -Y 1 )(Y 1 -Y 1 ) t ] D is satisfied. Consider the terms h(Y 2 ), h(Y 1 | Y 1 , U) and h(Y 2 |U) -µh(Y 2 + Z|U) separately. We have h(Y 2 ) = 1 2 log ((2πe) n |K Y 2 |) (53) and h(Y 1 | Y 1 , U) = h(Y 1 -Y 1 | Y 1 , U) ≤ h(Y 1 -Y 1 ) ≤ 1 2 log ((2πe) n |D|) .<label>(54)</label></formula><formula xml:id="formula_68">h(Y 2 |U) -µh(Y 2 + Z|U) ≤ max 0 K K Y 2 1 2 log ((2πe) n |K|) - µ 2 log ((2πe) n |K + K Y 1 -K Y 2 |) .</formula><p>(56) Substituting (53), (54) and (56) into (52), we have</p><formula xml:id="formula_69">µ 1 R 1 + µ 2 R 2 ≥ max 0 K K Y 2 µ 1 2 log K Y 2 K + µ 2 2 log K + K Y 1 -K Y 2 D .<label>(57)</label></formula><p>On the other hand, this weighted sum rate can be achieved by the following natural Gaussian separation scheme: This would have settled the rate region for the distributed source coding with a single quadratic constraint problem.</p><p>Unfortunately, there are indeed instances where the constraint Cov(Y 1 |U) D cannot be ignored; in such cases, the outer bound studied here will be strictly inside the inner bound achieved by the natural Gaussian separation scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Concluding Remarks</head><p>The classical EPI is an important inequality with interesting connections to statistical estimation theory. In information theory, it has been key to the proof of the converse coding theorem in several important scalar Gaussian multiterminal problems <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. In the vector situation, the equality condition of the classical EPI is stringent: the equality requires the participating random vectors not only be Gaussian but also have proportional covariance matrices. In several instances, this coupling between the Gaussianity and the proportionality is the main cause that prevents the classical EPI from being directly useful in extending the converse proof from the scalar case to the vector situation.</p><p>In this paper, we proved a new extremal inequality involving entropies of random vectors. In one special case, this inequality can be seen as a robust version of the classical EPI. By "robust", we refer to the fact that in the new extremal inequality, the optimality of a Gaussian distribution does not couple with a specific covariance structure, i.e. proportionality. We show that the new extremal inequality is useful in evaluating certain genie-aided outer bounds for the capacity/rate region for the vector Gaussian broadcast channel and the distributed source coding with a single quadratic constraint problems.</p><p>We offered two proofs to the new extremal inequality: one by appropriately using the classical EPI, and the other by the perturbation approach of Stam <ref type="bibr" target="#b1">[2]</ref> and Blachman <ref type="bibr" target="#b2">[3]</ref>. The perturbation approach gives more insights: it takes the problem (via the de Bruijn identity) to the Fisher information domain where the proportionality no longer seems a hurdle. Whereas the advantage of the perturbation approach is not crucial for the entropy inequalities discussed in this paper, it becomes crucial in some other situations <ref type="bibr" target="#b19">[20]</ref> where the enhancement technique of Weingarten et al. does not suffice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A A Direct Proof of Theorem 1</head><p>We now show that the classical EPI can be appropriately used to prove Theorem 1. We first give the outline of the proof.</p><p>Proof Outline. We first show that without loss of generality, we can assume that S is strictly positive definite. Next, we denote the optimization problem ( <ref type="formula" target="#formula_7">8</ref>) by P and the optimal value of P by (P ). To show that a Gaussian X is an optimal solution of P, it is sufficient to show that (P) = (P G ), where P G is the Gaussian version of P by restricting the solution space within Gaussian distributions:</p><formula xml:id="formula_70">max K X 1 2 log ((2πe) |K X + K Z 1 |) -µ 2 log ((2πe) n |K X + K Z 2 |) subject to 0 K X S. (<label>58</label></formula><formula xml:id="formula_71">)</formula><p>Since restricting the solution space can only decrease the optimal value of a maximization problem, we readily have (P ) ≥ (P G ). To prove the reverse inequality (P ) ≤ (P G ), we shall consider an auxiliary optimization problem P and its Gaussian version P G . In particular, we shall construct a P such that:</p><formula xml:id="formula_72">(P ) ≤ ( P ), ( P ) = ( P G ), ( P G ) = (P G ). (<label>59</label></formula><formula xml:id="formula_73">)</formula><p>We will then have (P ) ≤ (P G ) and hence (P ) = (P G ).</p><p>The proof is rather long, so we divide it into several steps.</p><p>Step 1: S 0, |S| = 0. We show that for any S 0 but |S| = 0, there is an equivalent optimization problem of type <ref type="bibr" target="#b7">(8)</ref> in which the the upper bound on X is strictly positive definite.</p><p>Suppose that the rank of S is r &lt; n, i.e., S is rank deficient. Let</p><formula xml:id="formula_74">S = Q S Σ S Q t S ,<label>(60)</label></formula><p>where Q S is an orthogonal matrix, and</p><formula xml:id="formula_75">Σ S = Diag(λ 1 , • • • , λ r , 0, • • • , 0) (61) is a diagonal matrix. For any X S, let X = X t a , X t b t = Q t S X where X a is of a length r. We have Cov(X) = Q t S Cov(X)Q S Q t S SQ S = Σ S ,<label>(62)</label></formula><p>which implies that Cov(X b ) = 0, i.e., X b is deterministic. Without loss of generality, let us assume that X b = 0. So an optimization over Cov(X) S is the same as an optimization over Cov(X a ) Diag(λ</p><formula xml:id="formula_76">1 , • • • , λ r ).</formula><p>Next, let</p><formula xml:id="formula_77">Q t S K Z i Q S = A i B t i B i C i (63)</formula><p>where A i , B i and C i are submatrices of size r × r, (n -r) × r, and (n -r) × (n -r), respectively, and let</p><formula xml:id="formula_78">D i = I -B t i C -1 i 0 I . (<label>64</label></formula><formula xml:id="formula_79">)</formula><p>We have</p><formula xml:id="formula_80">DQ t S X = I -B t i C -1 i 0 I X a 0 = X a 0 ,<label>(65)</label></formula><p>and</p><formula xml:id="formula_81">Cov(DQ t S Z i ) = I -B t i C -1 i 0 I A i B t i B i C i I 0 -C -1 i B i I = A i -B t i C -1 i B i 0 0 C i . (66) Hence if we let DQ t S Z i = (Z t i,a , Z t i,b</formula><p>) t where Z i,a is of a length r, then Z i,a and Z i,b are statistically independent. It follows that</p><formula xml:id="formula_82">h(X + Z i ) = h(DQ t S X + DQ t S Z i ) = h(X a + Z i,a , Z i,b ) = h(X a + Z i,a ) + h(Z i,b ). (67) So maximizing h(X + Z 1 ) -µh(X + Z 2 ) is equivalent to maximizing h(X a + Z 1,a ) -µh(X a + Z 2,a ) + h(Z 1,b ) -µh(Z 2,b ). Note that h(Z i,b</formula><p>), i = 1, 2, are constants. Hence to show that ( <ref type="formula" target="#formula_7">8</ref>) has a Gaussian optimal solution for a rank deficient S, it is sufficient to show that</p><formula xml:id="formula_83">max p(xa) h(X a + Z 1,a ) -h(X a + Z 1,a ) subject to Cov(X a ) Diag(λ 1 , • • • , λ r ), (<label>68</label></formula><formula xml:id="formula_84">)</formula><p>has a Gaussian optimal solution. Since Diag(λ 1 , • • • , λ r ) now has a full rank, we conclude that without loss of generality, we may assume that S in ( <ref type="formula" target="#formula_7">8</ref>) is strictly positive definite.</p><p>Step 2: Construction of P . Let X * G be an optimal Gaussian solution of P , and let K * X be the covariance matrix of X * G . Then K * X is an optimal solution to the optimization problem (58). Although this conic program is generally nonconvex, it was shown in [7, <ref type="bibr">Lemma 5]</ref> that for S ≻ 0, K * X must satisfy the following KKT-like conditions:</p><formula xml:id="formula_85">1 2 (K * X + K Z 1 ) -1 + M 1 = µ 2 (K * X + K Z 2 ) -1 + M 2 (69) M 1 K * X = 0 (70) M 2 (S -K * X ) = 0, (<label>71</label></formula><formula xml:id="formula_86">)</formula><p>where M 1 , M 2 0 are Lagrange multipliers corresponding to K X 0 and K X S, respectively. Let K e Z 1 , K e Z 2 be two real symmetric matrices satisfying 1 2</p><formula xml:id="formula_87">(K * X + K Z 1 ) -1 + M 1 = 1 2 (K * X + K e Z 1 ) -1 , (72) µ 2 (K * X + K Z 2 ) -1 + M 2 = µ 2 (K * X + K e Z 2 ) -1 .<label>(73)</label></formula><p>We have the following results on K e Z 1 and K e Z 2 proved in [7, <ref type="bibr">Lemma 11,</ref><ref type="bibr" target="#b11">12]</ref>.</p><formula xml:id="formula_88">Lemma 9 For K * X , K Z i , K e Z i , M i , i = 1, 2</formula><p>, related through (69) to (73), and µ ≥ 1, we have</p><formula xml:id="formula_89">0 K e Z 1 K Z 1 ,<label>(74)</label></formula><formula xml:id="formula_90">K e Z 1 K e Z 2 K Z 2 .<label>(75)</label></formula><p>The matrices K e Z 1 , K e Z 2 are positive semidefinite, so they can serve as covariance matrices. Let Z 1 , Z 2 be two Gaussian vectors with covariance matrix K e Z 1 and K e Z 2 , respectively. Let us define the auxiliary optimization problem P as:</p><formula xml:id="formula_91">max p(x) h(X + Z 1 ) -µh(X + Z 2 ) + F subject to Cov(X) S,<label>(76)</label></formula><p>where the constant</p><formula xml:id="formula_92">F := h(Z 1 ) -h( Z 1 ) + µ h(X (S) G + Z 2 ) -h(X (S) G + Z 2 ) ,<label>(77) X (S)</label></formula><p>G is a Gaussian vector with covariance matrix S and independent of Z 2 and Z 2 , and the maximization is over all random vector X independent of Z 1 and Z 2 .</p><p>In <ref type="bibr">[7, p. 3937</ref>], the authors call the process of replacing Z 1 and Z 2 with Z 1 and Z 2 , respectively, enhancement. Next, we show that the auxiliary optimization problem P defined in (76) satisfies the desired chain of relationships (59).</p><p>Step 3: Proof of (P ) ≤ ( P ). Note that P and P have the same solution space. So to show that (P ) ≤ ( P ), it is sufficient to show that for each admissible solution, the value of the objective function of P is less or equal to that of P .</p><p>The difference between the objective functions of P and P can be written as</p><formula xml:id="formula_93">h(X + Z 1 ) -h(Z 1 ) -h(X + Z 1 ) + h( Z 1 ) -µ h(X + Z 2 ) -h(X + Z 2 ) -h(X S + Z 2 ) + h(X S + Z 2 ) .<label>(78)</label></formula><p>By Lemma 9, K Z i K e Z i for i = 1, 2. So we can write Z i = Z i + Z i , where Z i is a Gaussian vector independent of Z i . We have</p><formula xml:id="formula_94">h(X + Z 1 ) -h(Z 1 ) -h(X + Z 1 ) + h( Z 1 ) = I(X; X + Z 1 ) -I(X; X + Z 1 ) (79) = I(X; X + Z 1 + Z 1 ) -I(X; X + Z 1 ) (80) ≤ 0,<label>(81)</label></formula><p>where the inequality is due to the Markov chain</p><formula xml:id="formula_95">X → X + Z 1 → X + Z 1 + Z 1 .<label>(82)</label></formula><p>Further, let X G be a Gaussian random vector with the same covariance matrix as that of X.</p><p>Assume that X G is independent of Z 2 and Z 2 . Note that both X G and X (S)</p><formula xml:id="formula_96">G are Gaussian and that Cov(X G ) = Cov(X) S = Cov(X (S) G ).<label>(83)</label></formula><p>So we can write</p><formula xml:id="formula_97">X (S) G = X G + X G</formula><p>, where X G is a Gaussian random vector independent of X G . We have</p><formula xml:id="formula_98">h(X + Z 2 ) -h(X + Z 2 ) -h(X (S) G + Z 2 ) + h(X (S) G + Z 2 ) = h(X + Z 2 + Z 2 ) -h(X + Z 2 ) -(h(X (S) G + Z 2 + Z 2 ) -h(X (S) G + Z 2 )) (84) = I( Z 2 ; X + Z 2 + Z 2 ) -I( Z 2 ; X (S) G + Z 2 + Z 2 ) (85) ≥ I( Z 2 ; X G + Z 2 + Z 2 ) -I( Z 2 ; X (S) G + Z 2 + Z 2 ) (86) = I( Z 2 ; X G + Z 2 + Z 2 ) -I( Z 2 ; X G + X G + Z 2 + Z 2 ) (87) ≥ 0,<label>(88)</label></formula><p>where inequality (86) follows from</p><formula xml:id="formula_99">I( Z 2 ; X + Z 2 + Z 2 ) ≥ I( Z 2 ; X G + Z 2 + Z 2 )<label>(89)</label></formula><p>which is due to the worst noise result of Lemma 2, and inequality (88) follows from the Markov chain</p><formula xml:id="formula_100">Z 2 → X G + Z 2 + Z 2 → X G + X G + Z 2 + Z 2 .<label>(90)</label></formula><p>Substituting ( <ref type="formula" target="#formula_94">81</ref>) and ( <ref type="formula" target="#formula_98">88</ref>) into (78), we conclude that the difference between the objective functions of P and P is nonpositive for any admissible X (i.e., Cov(X) S) and any µ &gt; 1.</p><p>Step 4: Proof of ( P ) = ( P G ). To show that ( P ) = ( P G ), it is sufficient to show that X * G , the optimal solution of P G , is also an optimal solution of P . We consider the cases µ = 1 and µ &gt; 1 separately.</p><p>First assume that µ &gt; 1. By Lemma 9, K e</p><formula xml:id="formula_101">Z 2 K e Z 1 .</formula><p>So we can write Z 2 = Z 1 + Z, where Z is Gaussian and independent of Z 1 . We have</p><formula xml:id="formula_102">h(X + Z 1 ) -µh(X + Z 2 ) = h(X + Z 1 ) -µh(X + Z 1 + Z) (91) ≤ h(X + Z 1 ) - µn 2 log exp 2 n h(X + Z 1 ) + exp 2 n h( Z) (92) ≤ f h( Z) - n 2 log(µ -1); h( Z) ,<label>(93)</label></formula><p>where (92) follows from the classical EPI, and the function f in (93) was defined in (4). Next, we verify that the upper bound on the right-hand side of (93) is achieved by X * G . Substituting (72) and (73) into the KKT-like condition (69), we obtain</p><formula xml:id="formula_103">(K * X + K e Z 1 ) -1 = µ(K * X + K e Z 2 ) -1 ,<label>(94)</label></formula><p>which gives</p><formula xml:id="formula_104">K * X + K e Z 1 = (µ -1) -1 K e Z .<label>(95)</label></formula><p>Hence, X * G + Z 1 and Z have proportional covariance matrices and inequality (92) holds with equality. Further by (95),</p><formula xml:id="formula_105">h(X * G + Z 1 ) = h( Z) - n 2 log(µ -1). (<label>96</label></formula><formula xml:id="formula_106">)</formula><p>A comparison of (96) and ( <ref type="formula" target="#formula_4">5</ref>) confirms that h(X * G + Z 1 ) achieves the global maxima of function f (t; h( Z)), i.e., inequality (93) becomes equality with X * G . We thus conclude that X *</p><p>G is an optimal solution of P for all µ &gt; 1.</p><p>For µ = 1, we have from (94) that K e Z 1 = K e Z 2 . So the objective function of P is constant, and X * G is trivially an optimal solution of P .</p><p>Step 5: Proof of ( P G ) = (P G ). Note that X * G is an optimal solution of both P G and P G . So to show that ( P G ) = (P G ), we only need to compare the objective functions of P G and P G evaluated at X * G . The following result, which is a minor generalization of <ref type="bibr" target="#b6">[7,</ref><ref type="bibr">Lemma 11,</ref><ref type="bibr" target="#b11">12]</ref>, shows that the objective functions of P G and P G take equal values at X * G .</p><p>Lemma 10 For K * X , K Z i , K e Z i , M i , i = 1, 2, defined through (69) to (73) and µ ≥ 1, we have</p><formula xml:id="formula_107">(K * X + K e Z 1 ) -1 K e Z 1 = (K * X + K Z 1 ) -1 K Z 1 ,<label>(97)</label></formula><formula xml:id="formula_108">(K * X + K e Z 2 ) -1 (S + K e Z 2 ) = (K * X + K Z 2 ) -1 (S + K Z 2 ).<label>(98)</label></formula><p>Combining Steps 1-5, we conclude that for any µ ≥ 1 and any positive semidefinite S, a Gaussian X is an optimal solution of (8). This completes the direct proof of Theorem 1.</p><p>A few comments on why we need the auxiliary optimization problem P are now in place. For the classical EPI to be tight, we need K * X + K Z 1 and K * X + K Z 2 to be proportional to each other. However, by the KKT-like condition (69), a guarantee of proportionality requires both multipliers M 1 and M 2 be zero. The purpose of enhancement is to absorb the (possibly) nonzero Lagrange multipliers M 1 , M 2 into the covariance matrices of Z 1 and Z 2 , creating a new optimization problem which can be solved directly by the classical EPI. The constant F is needed to make sure that (P G ) = ( P G ); the choice of F is motivated by the vector Gaussian broadcast channel problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proof of Lemma 3</head><p>We first give some preliminaries on Fisher information and score function. This material can be found, for example, in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr">Chapter 3.2]</ref>.</p><p>Definition 11 For a random vector U with a differentiable density function f U in R n , the Fisher information matrix J(•) is defined as</p><formula xml:id="formula_109">J(U) := E[ρ U (U)ρ t U (U)],<label>(99)</label></formula><p>where the vector-valued score function ρ U (•) is defined as</p><formula xml:id="formula_110">ρ U (u) := ∇ log f U (u) = ∂ ∂u 1 log f U (u), • • • , ∂ ∂u n log f U (u) t .<label>(100)</label></formula><p>The following results on score function are known.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 12</head><p>The following statements on score function are true.</p><p>1. (Gaussian Distribution) If U is a Gaussian vector with zero mean and positive definite covariance matrix K U , then</p><formula xml:id="formula_111">ρ U (u) = -K -1 U u.<label>(101)</label></formula><p>2. (Stein Identity) For any smooth scalar-valued function g well behaved at infinity, we have</p><formula xml:id="formula_112">E[g(U)ρ U (U)] = -E[∇g(U)].<label>(102)</label></formula><p>In particular, we have</p><formula xml:id="formula_113">E[ρ U (U)] = 0 and E[Uρ t U (U)] = -I, (<label>103</label></formula><formula xml:id="formula_114">)</formula><p>where I is the identity matrix.</p><p>3. (Behavior on Convolution) If U, V are two independent random vectors and</p><formula xml:id="formula_115">W = U + V, then ρ W (w) = E[ρ U (U)|W = w] = E[ρ V (V)|W = w].<label>(104)</label></formula><p>and similarly</p><formula xml:id="formula_116">E[ρ V (V)ρ t U (U)] = 0.<label>(117)</label></formula><p>Substituting ( <ref type="formula">113</ref>)-( <ref type="formula" target="#formula_116">117</ref>) into (112), we obtain</p><formula xml:id="formula_117">0 J(W) + AJ(U)A t + (I -A)J(V)(I -A) t -J(W)A t -AJ(W) -J(W)(I -A) t -(I -A)J(W) (118) = -J(W) + AJ(U)A t + (I -A)J(V)(I -A) t ,<label>(119)</label></formula><p>which gives</p><formula xml:id="formula_118">J(W) AJ(U)A t + (I -A)J(V)(I -A) t<label>(120)</label></formula><p>for any square matrix A. This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C A Perturbation Proof of Theorem 1</head><p>We first give the outline of the proof.</p><p>Proof Outline. Without loss of generality, let us assume that S ≻ 0. To show that a Gaussian X is an optimal solution of P , it is sufficient to show that (P ) = (P G ). We have (P ) ≥ (P G ) (for free); we only need to show that (P ) ≤ (P G ). For that purpose we shall consider the auxiliary optimization problem P :</p><formula xml:id="formula_119">max p(x) h(X + Z 1 ) -µh(X + Z 2 ) + h(Z 1 ) -h( Z 1 ) subject to Cov(X) S,<label>(121)</label></formula><p>where the maximization is over all random vector X independent of Z 1 and Z 2 . Compared with the auxiliary optimization problem P in the direct proof, this enhancement is only on Z 1 . Following the same footsteps as those in the direct proof, we can show that (P ) ≤ (P ) and (P G ) = (P G ). (In proving (P ) ≤ (P ), only the equations ( <ref type="formula">79</ref>)-(81) and the Markov chain (82) in Appendix A are needed.) All we need to show now is that (P ) = (P G ).</p><p>Proof of (P ) = (P G ). To show that (P ) = (P G ), we shall show that X * G is a global optimal solution of P . For that we shall prove the following strong result: for any admissible random vector X there is a monotone increasing path connecting X and X * G (see Figure <ref type="figure" target="#fig_1">1</ref>).</p><p>We consider the "covariance-preserving" transformation of Dembo et al. <ref type="bibr" target="#b9">[10]</ref>:</p><formula xml:id="formula_120">X λ = √ 1 -λX + √ λX * G , λ ∈ [0, 1]. (<label>122</label></formula><formula xml:id="formula_121">)</formula><p>Then {X λ } is a family of distributions indexed by λ ∈ [0, 1] and connecting X (when λ = 0) with X * G (when λ = 1). Let g(λ) be the objective function of P evaluated along the path {X λ }:</p><formula xml:id="formula_122">g(λ) := h(X λ + Z 1 ) -µh(X λ + Z 2 ) + h(Z 1 ) -h( Z 1 ).<label>(123)</label></formula><p>.</p><p>. .</p><formula xml:id="formula_123">X : Cov(X) S X * G X λ := √ 1 -λX + √ λX * G Figure 1: A monotone path connecting X and X * G .</formula><p>Next, we calculate the derivative of g over λ.</p><p>Note that Z 1 is Gaussian and that a Gaussian distribution is stable under convolution. We can write</p><formula xml:id="formula_124">Z 1 = √ 1 -λ Z 1,1 + √ λ Z 1,2 ,<label>(124)</label></formula><p>where Z 1,1 , Z 1,2 are independent and have the same distribution as that of Z 1 . We have</p><formula xml:id="formula_125">h(X λ + Z 1 ) = h( √ 1 -λX + √ λX * G + Z i ) (125) = h( √ 1 -λ(X + Z 1,1 ) + √ λ(X * G + Z 1,2 )) (126) = h(X + Z 1,1 + λ(1 -λ) -1 (X * G + Z 1,2 )) + (n/2) log(1 -λ). (<label>127</label></formula><formula xml:id="formula_126">)</formula><p>By the (vector) de Bruijn identity [10, Theorem 14],</p><formula xml:id="formula_127">2(1 -λ) d dλ h(X λ + Z 1 ) = (1 -λ) -1 Tr (K * X + K e Z 1 )J X + Z 1,1 + λ(1 -λ) -1 (X * G + Z 1,2 ) -n (128) = Tr (K * X + K e Z 1 )J √ 1 -λ(X + Z 1,1 ) + √ λ(X * G + Z 1,2 ) -n (129) = Tr (K * X + K e Z 1 )J X λ + Z 1 -n.<label>(130)</label></formula><p>Similarly, we have</p><formula xml:id="formula_128">2(1 -λ) d dλ h(X λ + Z 2 ) = Tr ((K * X + K Z 2 )J (X λ + Z 2 )) -n.<label>(131)</label></formula><p>Combining ( <ref type="formula" target="#formula_127">130</ref>) and (131), we have</p><formula xml:id="formula_129">2(1 -λ)g ′ (λ) = Tr (K * X + K e Z 1 )J(X λ + Z 1 ) -µ(K * X + K Z 2 )J(X λ + Z 2 ) + n(µ -1). (<label>132</label></formula><formula xml:id="formula_130">)</formula><p>Lemma 13 Let Z = (Z 1 , Z 2 ) t where Z 1 , Z 2 are two independent Gaussian variables with variance σ 2 1 and σ 2 2 , respectively. For any random vector X = (X 1 , X 2 ) t with finite variances and independent of Z, we have lim</p><formula xml:id="formula_131">σ 2 2 →∞ I(X; X + Z) = I(X 1 ; X 1 + Z 1 ). (<label>152</label></formula><formula xml:id="formula_132">)</formula><p>Proof. By the chain rule of mutual information,</p><formula xml:id="formula_133">I(X; X + Z) = I(X 1 ; X 1 + Z 1 ) + I(X 1 ; X 2 + Z 2 |X 1 + Z 1 ) + I(X 2 ; X + Z|X 1 ). (<label>153</label></formula><formula xml:id="formula_134">)</formula><p>Due to the Markov chains</p><formula xml:id="formula_135">X 1 + Z 1 → X 1 → X 2 + Z 2 and X 1 → X 2 → X 2 + Z 2 , we have I(X 1 ; X 2 + Z 2 |X 1 + Z 1 ) ≤ I(X 1 ; X 2 + Z 2 ) ≤ I(X 2 ; X 2 + Z 2 ).<label>(154)</label></formula><p>Furthermore, we have</p><formula xml:id="formula_136">I(X 2 ; X + Z|X 1 ) = I(X 2 ; X 2 + Z 2 |X 1 ) + I(X 2 ; X 1 + Z 1 |X 1 , X 2 + Z 2 ) (155) = I(X 2 ; X 2 + Z 2 |X 1 ) + I(X 2 ; Z 1 |X 1 , X 2 + Z 2 ) (156) = I(X 2 ; X 2 + Z 2 |X 1 ) (157) ≤ I(X 2 ; X 2 + Z 2 ),<label>(158)</label></formula><p>where (157) follows from the fact that Z 1 is independent of Z 2 and X so I(X 2 ; Z 1 |X 1 , X 2 + Z 2 ) = 0, and (158) is due to the Markov chain</p><formula xml:id="formula_137">X 1 → X 2 → X 2 + Z 2 . Note that lim σ 2 2 →∞ I(X 2 ; X 2 + Z 2 ) ≤ lim σ 2 2 →∞ 1 2 log 1 + Var(X 2 ) σ 2 2 = 0<label>(159)</label></formula><p>with finite Var(X 2 ). We thus have from ( <ref type="formula" target="#formula_135">154</ref>) and (158) that both I(X 1 ; X 2 + Z 2 |X 1 + Z 1 ) and I(X 2 ; X + Z|X 1 ) tend to zero in the limit as σ 2 2 → ∞. The desired result (152) follows by taking the limit σ 2 2 → ∞ on both sides of (153), which completes the proof.</p><formula xml:id="formula_138">Let Z i = (Z i,1 , Z i,2 ) t = V t i Z i . Then, Z i,1 and Z i,2 are independent. By Lemma 13, lim λ 12 →∞ I(X; X + Z 1 ) = lim λ 12 →∞ I(V t 1 X; V t 1 X + V t 1 Z 1 ) = I(v t 11 X; v t 11 X + Z 11 ) (160) lim λ 21 →∞ I(X; X + Z 2 ) = lim λ 21 →∞ I(V t 2 X; V t 2 X + V t 2 Z 1 ) = I(v t 22 X; v t 22 X + Z 22 ),<label>(161)</label></formula><p>which gives lim</p><formula xml:id="formula_139">λ 12 ,λ 21 →∞ I(X; X + Z 1 ) -µI(X + Z 2 ) = I(v t 11 X; v t 11 X + Z 11 ) -µI(v t 22 X; v t 22 X + Z 22 ). (162)</formula><p>Next, we consider the limit of the right-hand side of (151). For any semidefinite K X , we have lim</p><formula xml:id="formula_140">λ 12 ,λ 21 →∞ 1 2 log I + K -1 Z 1 K X - µ 2 log I + K -1 Z 2 K X = lim λ 12 ,λ 21 →∞ 1 2 log I + Σ -1 1 V t 1 K X V 1 - µ 2 log I + Σ -1 2 V t 2 K X V 2 (163) = 1 2 log 1 + λ -1 11 v t 11 K X v 11 - µ 2 log 1 + λ -1 22 v t 22 K X v 22<label>(164)</label></formula><p>due to the continuity of log |I + A| over the semidefinite A. Moreover, the convergence of (164) is uniform in K X , because the continuity of log |I+A| over A is uniform and V t i K X V i , i = 1, 2, are bounded for 0 K X S. we thus have lim </p><formula xml:id="formula_141">λ 12 ,λ 21 →∞ max 0 K X S 1 2 log I + K -1 Z 1 K X - µ 2 log I + K -1 Z 2 K X = max 0 K X S<label>1</label></formula><p>for any random vector X such that Cov(X) S and any µ ≥ 1. This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Proof of Corollary 7</head><p>Let v 1 = (1, 1) t and v 2 = (0, 1) t . Consider {X : Var(X 1 ) ≤ a 1 } = S {X : Cov(X) S} where the union is over all S such that (S) 11 = a 1 . By Corollary 6, a Gaussian (X 1 , X 2 is an optimal solution to the optimization problem max p(x 1 ,x 2 ) h(X 1 + X 2 + Z) -µh(X 2 + Z) subject to Var(X 1 ) ≤ a 1 ,</p><p>where µ ≥ 1, and the maximization is over all jointly distributed random variables (X 1 , X 2 ) independent of Z. Let (X * 1G , X * 2G ) be the Gaussian optimal solution of the (168). Then h(X 1 + X 2 + Z) -µh(X 2 + Z) ≤ h(X * 1G + X * 2G + Z) -µh(X * 2G + Z)</p><p>for any jointly distributed random variables (X 1 , X 2 ) such that Var(X 1 ) ≤ a 1 .</p><p>It is easy to verify that h(X * 2G + Z) is a continuous function of µ. When µ = ∞, h(X * 2G + Z) = h(Z); when µ = 1, h(X * 2G + Z) = a * 2 where a * 2 was defined in (30). By the intermediate value theorem, for any h(Z) ≤ a 2 ≤ a * 2 there is a µ for which h(X * 2G + Z) = a 2 . Hence for any jointly distributed random variables (X 1 , X 2 ) such that Var(X 1 ) ≤ a 1 and h(X 2 + Z) ≤ a 2 , we have by (169) that</p><formula xml:id="formula_145">h(X 1 + X 2 + Z) ≤ h(X * 1G + X * 2G + Z) + µ (h(X 2 + Z) -h(X * 2G + Z)) (170) ≤ h(X * 1G + X * 2G + Z).<label>(171)</label></formula><p>We conclude that a Gaussian solution is an optimal solution of (29) for any a 1 ≥ 0 and any h(Z) ≤ a 2 ≤ a * 2 . This completes the proof. Finally, let Q be a random variable uniformly distributed over {1, • • • , N} and independent of any other random variables/vectors. We have from (179) and (188) that</p><formula xml:id="formula_146">R 1 ≥ I(U[Q]; Y 2 [Q]|Q) = I(U[Q], Q; Y 2 [Q]) -I(Q; Y 2 [Q]) = I(U[Q], Q; Y 2 [Q]) = I(U; Y 2 ) (189) and that R 2 ≥ I( Y 1 [Q]; Y 1 [Q]|U[Q], Q) = I( Y 1 ; Y 1 |U)<label>(190)</label></formula><p>by defining</p><formula xml:id="formula_147">U := (Q, U[Q]), Y 1 := Y 1 [Q], Y 1 := Y 1 [Q], Y 2 := Y 2 [Q].<label>(191)</label></formula><p>For each m = 1,</p><formula xml:id="formula_148">• • • , N, Y 1 [m] = Y 2 [m] + Z[m] → Y 2 [m] → U[m] = (W 2 , Y m-1 1 )<label>(192)</label></formula><p>forms a Markov chain because Z[m] is independent of (W 2 , Y m-1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>). Therefore,</p><formula xml:id="formula_149">Y 1 → Y 2 → U<label>(193)</label></formula><p>also forms a Markov chain. This completes the proof.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>{Y 1 [m]}, {Y 2 [m]} be two i.i.d. vector Gaussian sources with strictly positive definite covariance matrix K Y 1 and K Y 2 , respectively. At each time m, Y 1 [m] and Y 2 [m] are jointly Gaussian.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 .</head><label>1</label><figDesc>Quantize {Y 1 [m]} and {Y 2 [m]} separately using Gaussian codebooks; 2. Use Slepian-Wolf coding [18] on the quantized version of {Y 1 [m]}, treating the quantized version of {Y 2 [m]} as decoder side information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>F 1 ) 2 ; 2 ) 2 ) 2 ) 2 , 1 ) 1 is a degraded version of Y m- 1 2for m = 1 , 1 ; 1 ) 1 , 1 ) 1 )I( Y 1</head><label>1222221111111111</label><figDesc>Proof of the Outer Bound (50) Let W 1 and W 2 be the encoded messages for {Y 1 [m]} and {Y 2 [m]}, respectively. Let Y mi := (Y i [1], • • • , Y i [m]) and U[m] := (W 2 , Y m-1 . We have NR 2 = H(W 2 ) (172) ≥ H(W 2 ) -H(W 2 |Y N 2 Y 2 [m]|Y m-1 -h(Y 2 [m]|W 2 , Y m-1 2 [m]) -h(Y 2 [m]|W 2 , Y m-1 2 [m]) -h(Y 2 [m]|W 2 , Y m-1 [m]; Y 2 [m]),(179)where (177) follows from the fact that Y m-1 • • • , N. Furthermore,NR 1 = H(W 1 ) (180) ≥ H(W 1 ) -H(W 1 |W 2 , Y 1 [m]|W 2 , Y m-1 Y 1 [m]; Y 1 [m]|W 2 , Y m-1 1 [m]; Y 1 [m]|W 2 , Y m-1 [m]; Y 1 [m]|U[m])(188)where (186) follows from the Markov chain Y 1 [m] → (W 1 , W 2 ) → Y 1 [m] for m = 1, • • • , N.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>2 [m] by relabeling AY 2 [m] as Y 2 [m].In this case, an outer bound can be obtained similarly to that for the discrete memoryless degraded broadcast channel<ref type="bibr" target="#b18">[19]</ref>:</figDesc><table /><note><p>is an invertible matrix and Z[m] is Gaussian and independent of Y 2 [m]. Since there is no distortion constraint on {Y 2 [m]}, we can always assume that Y 1 [m] is a degraded version of Y</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>2 log 1 + λ -1 11 v t 11 K X v 11 -</figDesc><table><row><cell></cell><cell></cell><cell cols="2">µ 2</cell><cell>log 1 + λ -1 22 v t 22 K X v 22 . (165)</cell></row><row><cell cols="2">Substituting (162) and (165) into (150), we obtain</cell><cell></cell><cell></cell></row><row><cell cols="3">I(v t 11 X; v t 11 X + Z 11 ) -µI(v t 22 X; v t 22 X + Z 22 ) ≤ max 0 K X S 1 2 log 1 + λ -1 11 v t 11 K X v 11 -</cell><cell>µ 2</cell><cell>log 1 + λ -1 22 v t 22 K X v 22</cell><cell>(166)</cell></row><row><cell>and hence</cell><cell></cell><cell></cell><cell></cell></row><row><cell>h(v t 11 X + Z 11 ) -µh(v t 22 X + Z 22 ) ≤ max 0 K X S 1 2 log 2πe v t 11 K X v 11 + λ 11 -</cell><cell>µ 2</cell><cell cols="3">log 2πe v t 22 K</cell></row></table><note><p>X v 22 + λ 22</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>The authors wish to thank both the reviewers and the Associate Editor for their careful review of the manuscript, which has helped to improve the technical quality of the paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We now use the above properties of score function to prove Lemma 3. We first prove the Cramér-Rao inequality. The Fisher information matrix J(U) has nothing to do with the mean of U, so without loss of generality we can assume that U has zero mean. We have</p><p>Here in (107) we use the facts that</p><p>by the definition of Fisher information matrix and that</p><p>by the Stein identity. We conclude that J(U) K -1 U for any random vector U with a strictly positive definite covariance matrix K U .</p><p>The matrix FII can be proved similarly:</p><p>By the definition of Fisher information matrix,</p><p>By the convolution behavior of score function,</p><p>and similarly</p><p>Finally, since U, V are independent and by the Stein identity with f = 1, we have</p><p>By the definition of K e Z 1 and the KKT-like condition (69), we have 1</p><p>By the facts that µ ≥ 1 and M 2 0, we obtain from (133) that</p><p>and hence that</p><p>We can now write Z 2 = Z 1 + Z, where Z is Gaussian and independent of Z 1 . Applying the matrix FII of Lemma 3 with</p><p>we have</p><p>where the last equality follows from the fact that Z is Gaussian so</p><p>Substituting (139) into (132) and using the fact that</p><p>where the equality follows from (133). Further by the Cramér-Rao inequality of Lemma 3,</p><p>Substitute ( <ref type="formula">146</ref>) into (142) and recall from the KKT-like condition (71) that (S-K * X )M 2 = 0. We have</p><p>We conclude that</p><p>i.e., {X λ } is a monotone increasing path connecting X and X * G . We have found a monotone increasing path for every admissible X, so X * G is an optimal solution of P . This completes the perturbation proof of Theorem 1.</p><p>A few comments on the difference between the direct proof and the perturbation proof of Theorem 1 are now in place. In the direct proof, we enhance both Z 1 and Z 2 to obtain the proportionality so that the classical EPI can be applied to solve the auxiliary optimization problem P . For the perturbation proof, however, we only need to enhance Z 1 . (If the lower constraint K X 0 does not bite, i.e., M 1 = 0, no enhancement is needed at all.) A direct perturbation is then used to show that X * G is an optimal solution of the auxiliary optimization problem P . Neither the classical EPI nor the worst noise result of Lemma 2 is needed in the perturbation proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Proof of Corollary 6</head><p>For any random vector X in R 2 such that Cov(X) ≤ S and any µ ≥ 1, we have from Theorem 1 that</p><p>(150) Adding a constant term µh(Z 2 ) -h(Z 1 ) to both sides of (150), we obtain I(X; X + Z 1 ) -µI(X; X + Z 2 ) ≤ max</p><p>) is an orthonormal matrix and Σ i = Diag(λ i1 , λ i2 ) is a diagonal matrix. Next, we consider taking the limits of both sides of (151) as λ 12 , λ 21 → ∞.</p><p>First consider the limit of the left-hand side of (151). We need the following simple lemma.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="623" to="656" />
			<date type="published" when="1948-10">Oct. 1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Some inequalities satisfied by the quantities of information of Fisher and Shannon</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Stam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Info. Ctrl</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="101" to="112" />
			<date type="published" when="1959-06">Jun. 1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The convolution inequality for entropy powers</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Blachman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Info. Theory</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="267" to="271" />
			<date type="published" when="1965-04">Apr. 1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A simple converse for broadcast channels with additive white Gaussian noise</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Bergman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Info. Theory</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="279" to="280" />
			<date type="published" when="1974-03">Mar. 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The rate-distortion function for the quadratic Gaussian CEO problem</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Oohama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Info. Theory</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="1057" to="1070" />
			<date type="published" when="1998-05">May 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On a source coding problem with two channels and three receivers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ozarow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1909" to="1921" />
			<date type="published" when="1980-12">Dec. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The capacity region of the Gaussian MIMO broadcast channel</title>
		<author>
			<persName><forename type="first">H</forename><surname>Weingarten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Steinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shamai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">)</forename><surname>Shitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Info. Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3936" to="3964" />
			<date type="published" when="2006-09">Sept. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the capacity of channels with additive non-Gaussian noise</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Info. Ctrl</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="39" />
			<date type="published" when="1978-04">Apr. 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The worst additive noise under a covariance constraint</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Diggavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Info. Theory</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3072" to="3081" />
			<date type="published" when="2001-11">Nov. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Information theoretic inequalities</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dembo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Info. Theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1501" to="1518" />
			<date type="published" when="1991-11">Nov. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A proof of the Fisher information inequality via a data processing argument</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Info. Theory</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1246" to="1250" />
			<date type="published" when="1998-05">May 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Additive non-Gaussian noise channels: mutual information and conditional mean estimation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shamai (shitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">)</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Verdú</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Info. Theory</title>
		<meeting>IEEE Int. Symp. Info. Theory<address><addrLine>Adelaide, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09">Sept. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the maximum entropy of the sum of two dependent random variables</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Info. Theory</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1244" to="1246" />
			<date type="published" when="1994-07">Jul. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A coding theorem for the discrete memoryless broadcast channel</title>
		<author>
			<persName><forename type="first">K</forename><surname>Marton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Info. Theory</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="306" to="311" />
			<date type="published" when="1979-05">May 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Johnson</surname></persName>
		</author>
		<title level="m">Information Theory and The Central Limit Theorem</title>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Imperial College Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the achievable throughput of a multi-antenna Gaussian broadcast channel</title>
		<author>
			<persName><forename type="first">G</forename><surname>Caire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shamai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Info. Theory</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1691" to="1706" />
			<date type="published" when="2003-07">Jul. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Writing on colored paper</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sutivong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Juian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE. Int. Symp. Inform. Theory</title>
		<meeting>IEEE. Int. Symp. Inform. Theory<address><addrLine>Washington DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-06">Jun. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Noiseless coding of correlated information sources</title>
		<author>
			<persName><forename type="first">D</forename><surname>Slepian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Info. Theory</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="471" to="480" />
			<date type="published" when="1973-07">Jul. 1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Capacity and coding for degraded broadcast channels</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gallager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Prob. Pered. Info</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="3" to="14" />
			<date type="published" when="1974-07">Jul.-Sept. 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The capacity region of the degraded multiple input multiple output broadcast compound channel</title>
		<author>
			<persName><forename type="first">H</forename><surname>Weingarten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Steinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shamai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">)</forename><surname>Shitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Viswanath</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in prepration</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
