<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Path Neural Networks: Expressive and Accurate Graph Neural Networks</title>
				<funder ref="#_2sX7Yrb">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder>
					<orgName type="full">?cole Universitaire de Recherche (EUR) Bertip</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Gaspard</forename><surname>Michel</surname></persName>
							<email>&lt;gmichel@deezer.com&gt;.</email>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">LIX</orgName>
								<orgName type="institution">?cole Polytechnique</orgName>
								<address>
									<settlement>Paris</settlement>
									<region>IP</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Deezer Research</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Giannis</forename><surname>Nikolentzos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">LIX</orgName>
								<orgName type="institution">?cole Polytechnique</orgName>
								<address>
									<settlement>Paris</settlement>
									<region>IP</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Deezer Research</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Johannes</forename><surname>Lutzeyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">LIX</orgName>
								<orgName type="institution">?cole Polytechnique</orgName>
								<address>
									<settlement>Paris</settlement>
									<region>IP</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Deezer Research</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">LIX</orgName>
								<orgName type="institution">?cole Polytechnique</orgName>
								<address>
									<settlement>Paris</settlement>
									<region>IP</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Deezer Research</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Path Neural Networks: Expressive and Accurate Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks (GNNs) have recently become the standard approach for learning with graph-structured data. Prior work has shed light into their potential, but also their limitations. Unfortunately, it was shown that standard GNNs are limited in their expressive power. These models are no more powerful than the 1-dimensional Weisfeiler-Leman (1-WL) algorithm in terms of distinguishing non-isomorphic graphs. In this paper, we propose Path Neural Networks (PathNNs), a model that updates node representations by aggregating paths emanating from nodes. We derive three different variants of the PathNN model that aggregate single shortest paths, all shortest paths and all simple paths of length up to K. We prove that two of these variants are strictly more powerful than the 1-WL algorithm, and we experimentally validate our theoretical results. We find that PathNNs can distinguish pairs of nonisomorphic graphs that are indistinguishable by 1-WL, while our most expressive PathNN variant can even distinguish between 3-WL indistinguishable graphs. The different PathNN variants are also evaluated on graph classification and graph regression datasets, where in most cases, they outperform the baseline methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graphs have emerged in recent years as a powerful tool for representing irregular data. Among many other applications, graphs have been used to model the relationships between individuals within a social network <ref type="bibr" target="#b21">(Easley &amp; Kleinberg, 2010)</ref> and the interactions between the atoms of a molecule <ref type="bibr" target="#b53">(Stokes et al., 2020)</ref>. The large availability of graph-structured data has motivated the development of machine learning algorithms that are designed for this kind of data. Notably, most of those algorithms belong either to the family of graph kernels <ref type="bibr" target="#b37">(Kriege et al., 2020;</ref><ref type="bibr" target="#b9">Borgwardt et al., 2020;</ref><ref type="bibr" target="#b49">Nikolentzos et al., 2021)</ref> or to that of graph neural networks (GNNs) <ref type="bibr" target="#b60">(Wu et al., 2020;</ref><ref type="bibr" target="#b70">Zhou et al., 2020)</ref>. Due to some limitations that are inherent to kernels (e. g., they do not scale to large datasets, they struggle with continuous features, etc.), GNNs have become the most common approach for dealing with graph learning problems.</p><p>GNNs have been studied extensively in the past years. So far, research in the field has mainly focused on message passing architectures <ref type="bibr" target="#b27">(Gilmer et al., 2017)</ref>. These models follow a recursive neighborhood aggregation scheme where each node aggregates the representations of its neighbors along with its own representation to compute new updated representations. It has been shown that there is a connection between the neighborhood aggregation scheme of these models and the relabeling procedure of the Weisfeiler-Leman (WL) algorithm, a well-known heuristic for testing graph isomorphism <ref type="bibr" target="#b61">(Xu et al., 2019;</ref><ref type="bibr" target="#b43">Morris et al., 2019)</ref>. More importantly, it was shown that the standard message passing architectures are at most as powerful as the WL algorithm in terms of distinguishing non-isomorphic graphs. This has led to the development of more complex models that focus on subgraphs <ref type="bibr">(Maron et al., 2019b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b15">Cotta et al., 2021)</ref>. Some of these models are inspired by higher-order variants of the WL algorithm <ref type="bibr" target="#b43">(Morris et al., 2019;</ref><ref type="bibr">2020b)</ref>.</p><p>Besides subgraphs, there are also other structures of graphs that can improve a model's expressive power. For instance, paths can distinguish connected from disconnected graphs, while the WL algorithm fails to distinguish this property. Unfortunately, finding all paths in a graph is NP-hard. However, some subsets of paths can be computed in polynomial time. For instance, computing shortest paths in a graph is a problem solvable in polynomial time <ref type="bibr" target="#b17">(Dijkstra, 1959)</ref>. It has been shown that models that use shortest path distances between nodes as features can provide more expressive power than the WL algorithm <ref type="bibr" target="#b38">(Li et al., 2020)</ref>. Furthermore, if we restrict the length of the paths to some small integer k, the parameterized complexity of computing all the bounded length paths is tractable.</p><p>In this paper, we propose Path Neural Networks (PathNNs), a GNN that generates node representations that are based on paths emanating from the nodes of graphs. By computing 1 arXiv:2306.05955v1 <ref type="bibr">[cs.</ref>LG] 9 Jun 2023 different subsets of paths, we derive different variants of the proposed model. We focus on subsets whose computation is possible in polynomial time, namely shortest paths and simple paths of bounded length. For each path length, the proposed model uses a recurrent layer to encode paths into vectors and then, the representations of all paths emanating from a node are aggregated to produce the node's new representation. We show that two of the three PathNN variants are strictly more powerful than the WL algorithm in terms of distinguishing non-isomorphic graphs. Our theoretical results are confirmed by experiments on synthetic datasets that measure the models' expressive power. Furthermore, we evaluate the PathNNs on real-world graph classification and regression datasets 1 . Our results demonstrate that the different PathNN variants achieve high levels of performance and outperform the baseline methods in most cases. Our main contributions are summarized as follows:</p><p>? We develop PathNN, a neural network that computes node representations by aggregating paths of increasing length. We derive three different variants of the model that focus on single shortest paths, all shortest paths and all simple paths, respectively.</p><p>? We prove that two of the variants are strictly more powerful than the WL algorithm and we empirically measure their expressive power.</p><p>? We evaluate the proposed model on several graph classification and regression datasets where it achieves performance comparable to state-of-the-art GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>GNNs and kernels that process paths. Shortest path distances between nodes have been incorporated as structural features into several GNN architectures. For instance, Graphormer encodes shortest path distances between two nodes as a bias term in the softmax attention <ref type="bibr" target="#b63">(Ying et al., 2021)</ref>, while other models annotate nodes with features that emerge from shortest paths (e. g., shortest path distances) <ref type="bibr" target="#b38">(Li et al., 2020;</ref><ref type="bibr" target="#b65">You et al., 2019)</ref>. On the other hand, PEGN uses shortest path distances to create persistence diagrams based on which messages between nodes are re-weighted <ref type="bibr" target="#b69">(Zhao et al., 2020)</ref>. Instead of aggregating the representations of each node's direct neighbors, some models such as SP-MPNN <ref type="bibr" target="#b1">(Abboud et al., 2022</ref>) also consider the nodes at shortest path distance exactly k from the node. A recently proposed GNN framework, so-called Geodesic GNN <ref type="bibr" target="#b36">(Kong et al., 2022)</ref>, generates representations for pairs of nodes by aggregating the representations of the nodes on a single shortest path between the two nodes and also the direct 1 Code available at https://github.com/gasmichel/ PathNNs_expressive neighbors of the two nodes that are on any of their shortest paths. Perhaps the work most related to our approach is the PathNet model <ref type="bibr" target="#b54">(Sun et al., 2022)</ref>, which also aggregates path information. However, there are major differences between PathNet and our PathNNs. PathNet samples paths instead of enumerating all of them, it is only evaluated on node classification datasets, and lastly, the authors do not provide an extensive study of the expressive power of the model. There exist also models that instead of paths simulate walks which are then either aggregated <ref type="bibr" target="#b33">(Jin et al., 2022)</ref> or processed by a convolution neural network <ref type="bibr" target="#b55">(Toenshoff et al., 2021)</ref>.</p><p>Our proposed PathNNs are also related to graph kernels which compare paths of two graphs to each other. Such kernels include the shortest path kernel <ref type="bibr">(Borgwardt &amp; Kriegel, 2005)</ref> and the GraphHopper kernel <ref type="bibr" target="#b24">(Feragen et al., 2013)</ref>. The former just compares shortest path distances to each other, while the latter is more similar to the proposed model since it also takes into account the attributes of the nodes that appear on a shortest path.</p><p>Expressive power of GNNs. Since we will study the expressive power of our PathNNs, our work is furthermore related to the extensive literature exploring the expressive power of GNNs. Several of those studies have investigated the connections between GNNs and the WL test of isomorphism <ref type="bibr" target="#b3">(Barcel? et al., 2020;</ref><ref type="bibr" target="#b26">Geerts &amp; Reutter, 2022)</ref>. For instance, standard GNNs were shown to be at most as powerful as the WL algorithm in terms of distinguishing non-isomorphic graphs <ref type="bibr" target="#b43">(Morris et al., 2019;</ref><ref type="bibr" target="#b61">Xu et al., 2019)</ref>. Other studies capitalized on high-order variants of the WL algorithm to derive new models that are more powerful than standard GNNs <ref type="bibr" target="#b43">(Morris et al., 2019;</ref><ref type="bibr">2020b)</ref>. Another line of research focused on k-order graph networks <ref type="bibr">(Maron et al., 2019b)</ref>. Importantly, k-order graph networks were found to be at least as powerful as the k-WL graph isomorphism test in terms of distinguishing non-isomorphic graphs, while a reduced 2-order network containing just a scaled identity operator, augmented with a single quadratic operation was shown to have 3-WL discrimination power <ref type="bibr">(Maron et al., 2019a)</ref>. Furthermore, it was shown that a GNN with a maximum tensor order 2 defined on the ring of equivariant functions can distinguish some pairs of non-isomorphic regular graphs with the same degree <ref type="bibr" target="#b13">(Chen et al., 2019)</ref>. Graph isomorphism testing and invariant function approximation, the two main perspectives for studying the expressive power of GNNs, have been shown to be equivalent to each other <ref type="bibr" target="#b13">(Chen et al., 2019)</ref>. Recently, a large body of work focused on making GNNs more powerful than WL, e. g., by encoding vertex identifiers <ref type="bibr" target="#b57">(Vignac et al., 2020)</ref>, taking into account all possible node permutations <ref type="bibr" target="#b46">(Murphy et al., 2019;</ref><ref type="bibr" target="#b16">Dasoulas et al., 2020)</ref>, using random features <ref type="bibr" target="#b51">(Sato et al., 2021;</ref><ref type="bibr" target="#b0">Abboud et al., 2021)</ref>, using node features <ref type="bibr" target="#b66">(You et al., 2021)</ref>, spectral information (Balcilar et al., 2021), simplicial and cellular complexes <ref type="bibr">(Bodnar et al., 2021b;</ref><ref type="bibr">a)</ref> and directional information <ref type="bibr" target="#b4">(Beaini et al., 2021)</ref>. Furthermore, several recent studies extract and process subgraphs to make GNNs more expressive <ref type="bibr" target="#b48">(Nikolentzos et al., 2020;</ref><ref type="bibr" target="#b5">Bevilacqua et al., 2022)</ref>. For example, the expressive power of GNNs can be increased by aggregating the representations of subgraphs (produced by standard GNNs) that arise from removing one or more vertices from a given graph <ref type="bibr" target="#b15">(Cotta et al., 2021;</ref><ref type="bibr" target="#b50">Papp et al., 2021)</ref>. Finally, it was recently shown that models that process each node's k-hop neighborhood and which aggregate nodes at shortest path distance exactly k from that node (such as the SP-MPNN model <ref type="bibr" target="#b1">(Abboud et al., 2022)</ref>) are more powerful than standard GNNs <ref type="bibr" target="#b23">(Feng et al., 2022)</ref>, while the expressive power of those models can be further improved by taking into account the edges that connect the nodes at shortest path distance exactly k from the considered node.</p><formula xml:id="formula_0">v 5 v 6 v 6 v 5 v 6 v 6 v 6 v 6 v 6 v 5 v 5 v 4 v 3 (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Path Neural Networks</head><p>In this section, we begin by introducing some key notation for graphs which will be used later, and then we describe Path Neural Networks (PathNNs), a message-passing GNN that learns representations of paths of increasing length which are aggregated to update node representations. We first introduce the theoretical framework of PathNNs and then detail their architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Notation</head><p>Let { {} } denote a multiset, i. e., a generalized concept of a set that allows multiple instances for its elements. Let G = (V, E) be an undirected graph consisting of a set of nodes V and a set of edges E ? V ? V . We denote by n the number of nodes in G and by m its number of edges. The set N (v) represents the neighbors of vertex v. When considering attributed graphs, each vertex v ? V is endowed with an initial node feature vector denoted by x v that can contain categorical or real-valued properties of v.</p><p>A path from source node v to target node u is denoted</p><formula xml:id="formula_1">by ? = [v 1 , v 2 , . . . , v k ] such that v 1 = v, v k = u and (v i , v i+1 ) ? E for i ? {1, . . . , k -1}.</formula><p>Note that paths only contain distinct vertices. We denote by ?(j) = v j the jth node encountered when hopping along the path, and by |?| = k -1 its length, defined by the number of edges it contains. In this work, we consider various collections of paths. The first collection is the set of shortest paths denoted as SP containing a single shortest path for all possible node pair combinations. Multiple shortest paths between a source node and a target node might exist. We thus also consider the collection of all shortest paths that we denote as SP + , containing all possible shortest paths between every node pair combinations. A simple path can be any path, not necessarily the shortest, from a source node to a target node. We denote by AP the collection of all simple paths between node pair combinations. Figure <ref type="figure" target="#fig_0">1</ref> provides an example of these three collections of paths. Note that, we have SP ? SP + ? AP. In practice, we only consider paths up to a fixed length k. We use the notation P k v = {? ? P : ?(1) = v, |?| = k} for P ? {SP, SP + , AP} to denote all paths of length k contained in P starting from node v. Similarly, P v denotes the set of all paths in P ? {SP, SP + , AP} starting from node v of any length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Theoretical Framework</head><p>In this section, we assume that for any graph G, we have the collections of all paths of maximum length K at our disposal for all nodes in G. We furthermore consider any collection of paths P ? {SP, SP + , AP} that is induced by a graph G. All proofs can be found in Appendix A. We begin by introducing the concept of WL-Trees and Path-Trees. Path-Trees are an intuitive path-based, rather than walk-based, analogue to WL-Trees. We will then show that Path-Trees are not able to distinguish graphs at a node-level for all collections of paths, which will motivate us to instead define a model operating on annotated sets of paths, which we will show to disambiguate graphs at least as well as WL-Trees.</p><p>WL-Trees of a given root node are constructed one level </p><formula xml:id="formula_2">v 3 v 2 v 4 v 6 v 1 v 6 v 1 v 5 v 1 v 3 v 4 v 2 v 2 v 6 v 3 v 4 v 2 v 4 v 5 v 3 v 3 v 4 v 2 v 4 v 5 v 3 v 1 (a) v 3 v 2 v 4 v 6 v 5 v 1 (b) v 3 v 2 v 4 v 6 v 5 v 1 v 6 (c) v 3 v 2 v 4 v 6 v 5 v 1 v 6 v 6 v 4 v 5 v 3 v 5 (d)</formula><formula xml:id="formula_3">P v for v ? V,</formula><p>is a tree such that the node set at level k of the tree is equal to the multisets of nodes that occur at position k in the paths in P k v , i.e., { {u : ?(k) = u for ? ? P k v } }. Nodes at level k and level k + 1 in the tree are connected if and only if they occur in adjacent positions k and k + 1 in a given path in the set of paths P v , i.e., (x, y) ? P v if ?(k) = x and ?(k + 1) = y for any ? ? P v such that each node at level k + 1 is connected only to a single node at level k. The height k Path-Tree P k v ? T k rooted at v corresponds to the Path-Tree of v pruned from all levels l &gt; k.</p><p>Different collections of paths lead to different Path-Trees, as shown in Figure <ref type="figure" target="#fig_1">2</ref>. To give the reader a more practical understanding of Path-Trees we now explain how a k Path-Tree can be constructed from any collection of paths P ? {SP, SP + , AP}. We iteratively build the Path-Tree for a given source node v one level k ? {1, . . . , K} at a time, where we make use of the subset P k v of P v ? P to construct level k of the Path-Tree. In the first iteration we add the root node v to the Path-Tree. Then, at subsequent iterations we add the multiset of nodes at position k in the paths in P k v to the Path-Tree. We then iteratively connect these added nodes to the Path-Tree via edges, where a given node u of the added nodes is connected to a single leaf w in the existing Path-Tree such that the ordered set of ancestors of w in the Path-Tree is identical to the ordered set of nodes preceding u in its path of length k from which the addition of u resulted.</p><p>Intuitively, for a given graph G and a given node v the SP-Tree rooted at v is a subgraph of the SP + -Tree which is itself a subgraph of the AP-Tree. Another interesting property of Path-Trees is provided in the following lemma.</p><p>Lemma 3.2. Let G = (V, E) and P ? {SP, SP + , AP} be any collection of paths in G.</p><formula xml:id="formula_4">Let P k v be the Path-Tree rooted at v of height k and W k v be the WL-Tree rooted at v of height k for v ? V . Then P k v is a subgraph of W k v .</formula><p>Lemma 3.2 can be easily proved by noting that a WL-Tree rooted at node v of height k is equivalent to an enumeration of all walks that start at v of length up to k. Since paths are a special case of walks, Path-Trees are subgraphs of WL-Trees, for all collections of paths.</p><p>Each type of Path-Trees contain a different type of structural information over the input graph. SP and SP + Trees contain the most compressed information over shortest paths.</p><p>In particular, they can only answer the question of what is the shortest path distance between a given node and any other node in G? The AP-Trees encode the largest amount of structural information over the input graph. Besides, they are not height-limited by the graph's diameter. However, they usually grow exponentially with the height k and the density of the input graph. We now state our main theorem on the relation between AP-Trees and WL-Trees.</p><p>Theorem 3.3. Let G = (V, E) and AP be the collection of all simple paths in G. Let {AP k v , AP k u } ? T k be the AP-Trees of height k rooted at v and u, respectively, and </p><formula xml:id="formula_5">W k v , W k u be the WL-Trees of height k rooted at v and u, respectively, for v, u ? V . If W k v is structurally different (i. e., not isomorphic) than W k u , then AP k v is structurally different than AP k u .</formula><formula xml:id="formula_6">W k v , W k u be the WL-Trees of height k rooted at v and u, respectively, for v, u ? V . If W k v is structurally different than W k u , then P k v is not necessarily structurally different from P k u .</formula><p>Hence, Path-Trees based on the collections of shortest paths do not necessarily allow us to disambiguate individual nodes structurally even when the WL test does so. We therefore propose to operate on annotated sets of paths instead of Path-Trees. We use SP, SP + and ?P to denote annotated paths in the single shortest path, all shortest path and all simple path collections, respectively. The annotations of nodes in P ? { SP, SP + , ?P} depend on the length of the considered path. For paths of length 1, i.e., P 1 ? P, all nodes have equal annotations. For paths of length 2, i.e., P 2 ? P, all nodes v in these paths are annotated with hashes of their respective annotated path sets of length 1, i.e., P 1 v . In general for paths of length k, i.e., P k ? P, all nodes v in these paths are annotated with hashes of their respective annotated path sets of length k -1, i.e., P k-1 v . Each annotation of paths of length k &gt; 2, is therefore a multiset of sequences of annotations. In Theorem 3.5 we demonstrate that any annotated path set P ? { SP, SP We want to remark that depending on the particular shortest path that is sampled in the single shortest path collection SP it is possible that even isomorphic nodes have different annotated sets of paths. For SP + and ?P isomorphic nodes always have corresponding identical annotated sets of paths. Therefore, models operating on SP + and ?P are strictly more powerful than the WL algorithm for graph isomorphism. While models operating on SP can only disambiguate graphs at least as well as the WL algorithm and are not strictly more powerful, since also isomorphic graphs could be mapped to different representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model Architecture</head><p>We adopt a message passing scheme that iteratively updates node representations using paths of increasing length. This message passing scheme allows us to refine sets of paths with additional structural information included in paths of length smaller than K.</p><p>Noting that paths are ordered sequences of nodes, PathNNs learn to embed paths into d-dimensional feature vectors, using a function operating on sequences f : R k?d ? R d , where k is the path length. PathNNs employ a messagepassing scheme that aggregates path embeddings to form updated node representations. Specifically, at each layer k, paths of length k are embedded by f and aggregated. Let ? be any path of length k. Then, PathNNs compute the embedding of ? by using the node representations of layer k -1</p><formula xml:id="formula_7">h (k) ? = f h (k-1) ?(k+1) , . . . , h (k-1) ?<label>(1)</label></formula><p>.</p><p>(1)</p><p>PathNNs start to encode paths of length 1 in the first layer (i. e., sequences of a node and its neighbors' representations), then iteratively encode paths of increasing length. Path representations form the message in the message passing architecture and are used to update node feature vectors in a similar way to traditional MPNNs</p><formula xml:id="formula_8">a (k) v = AGGREGATE (k) h (k) ? |? ? P k v , h (k) v = COMBINE (k) h (k-1) v , a (k) v .</formula><p>Paths of length up to K are given as input to PathNNs by stacking K layers. Figure <ref type="figure" target="#fig_3">3</ref> illustrates the functioning of PathNNs. Graph representations are obtained using a permutation-invariant readout function</p><formula xml:id="formula_9">h G = READOUT h (K) v |v ? V .</formula><p>Without loss of generality, we propose to use a simple instance of PathNNs in this paper. The function f operating  on sequences is modeled and learned by a Long-Short-Term-Memory (LSTM) cell <ref type="bibr" target="#b30">(Hochreiter &amp; Schmidhuber, 1997)</ref>.</p><formula xml:id="formula_10">v 1 v 2 v 3 v 4 v 1 v 3 v 1 v 4 v 2 v 4 v 3 v 1 v 3 v 4 v 4 v 1 v 4 v 2 v 4 v 3 f ( , ) = f ( , ) = f ( , ) = f ( , ) = f ( , ) = f ( , ) = f ( , ) = f ( , ) = Input Graph Paths Extraction Paths Representation Update Nodes Representation = h (1) v1 = COMBINE ( ,AGG ( , )) = h (1) v2 = COMBINE ( ,AGG ( )) = h (1) v3 = COMBINE ( ,AGG ( , )) = h (1) v4 = COMBINE ( ,AGG ( , , )) (a) First iteration of PathNN-SP. v 1 v 2 v 3 v 4 v1 v4 f ( , , ) = v2 v2 v4 f ( , , ) = v1 v2 v4 f ( , , ) = v3 v3 v4 f ( , , ) = v2 h (2) v1 = COMBINE ( , AGG h (2) v2 = h (2) v3 = h (2) v4 = ( )) v4 f ( ? ) = COMBINE ( , AGG ( , )) COMBINE ( , ) COMBINE ( , AGG<label>(</label></formula><p>LSTMs are a valid choice for the function f thanks to the universal approximation results for Recurrent Neural Network <ref type="bibr" target="#b29">(Hammer, 2000)</ref>: they can approximate any injective sequence function arbitrarily well in probability, which is a desirable and necessary property for f . Note that the last element in the sequence is the starting node representation.</p><p>The LSTM operates on reversed paths as we hypothesize that the most important information in the sequence is the starting node's current representation. We project node initial feature vectors to match the LSTM's hidden dimension with a 2-layer Multilayer Perceptron (MLP). Using paths embeddings, PathNNs then update node representations using the following rule</p><formula xml:id="formula_11">g (k) v = NORM (k) ? ? h (k-1) v + ??P k v h (k) ? ? ? ,<label>(2)</label></formula><formula xml:id="formula_12">h (k) v = ? g (k) v .<label>(3)</label></formula><p>Noting that the collection of paths P k v grows larger with graph density, the right hand side of Equation ( <ref type="formula" target="#formula_11">2</ref>) can be of very high magnitude, leading to numerical instabilities. A Batch Normalization (BN) layer <ref type="bibr" target="#b32">(Ioffe &amp; Szegedy, 2015)</ref> is thus applied in Equation ( <ref type="formula" target="#formula_11">2</ref>) to avoid such situations during training. After normalization, node representations are passed through a ? function, which can be either the identity function or a 2-layer MLP. Both functions are a valid choice as sum and 2-layer MLPs are injective functions over multisets <ref type="bibr" target="#b61">(Xu et al., 2019)</ref>. Equipped with the identity aggregation function, the number of parameters of PathNNs only slightly increases with path length (caused by BN parame-ters), resulting in a low number of trainable parameters even for higher path lengths. Replacing the identity function with an MLP allows finer updated node representations but leads to higher time and memory complexity. Finally, PathNNs use the sum over the multiset of final node representations as a readout function to produce a vector representation of the entire graph</p><formula xml:id="formula_13">h G = v?V h (K)</formula><p>v .</p><p>(4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Time and Space Complexity</head><p>To enumerate all shortest paths and all simple paths of length up to k from a source vertex to all other vertices, we use the depth-first search (DFS) algorithm. The time complexity of the algorithm is at most O(b k ), where b is the branching factor of the graph which is upper bounded by the maximum of the nodes' degrees. Thus, for all the nodes of the graph, the time complexity is O(nb k ). The space complexity is O(nbk) if duplicate nodes are not eliminated.</p><p>Real-world graphs are often small (e. g., molecules) and/or sparse (e. g., social networks), i.e., n is often small and/or b ? n. Thus, for bounded k, the time and space complexity for enumerating the paths is not prohibitive.</p><p>The running time of the model depends on the number of paths which is O(nb k ). Typically, the larger the value of k, the larger the number of paths and therefore, the complexity of our model increases as a function of k. For k = 1, the complexity of our model is comparable to that of standard GNNs which aggregate 2m ? nb representations. However, for larger values of k, the time complexity of our model is greater than that of standard GNNs which only aggregate 2m representations in all neighborhood aggregation layers. We report the empirical running times of our PathNNs on two real-world datasets in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Evaluation</head><p>We now evaluate the performance of our PathNNs in synthetic experiments specifically designed to exhibit the expressiveness of GNNs in Section 4.1 and on a range of real-world datasets in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Synthetic Datasets</head><p>Datasets. We use 3 publicly available datasets: (1) the Circular Skip Link (CSL) dataset <ref type="bibr" target="#b46">(Murphy et al., 2019)</ref>; (2) the EXP dataset <ref type="bibr" target="#b0">(Abboud et al., 2021)</ref>; (3) the SR dataset.</p><p>Experimental setup. For all experiments, the aggregation function ? of Equation ( <ref type="formula" target="#formula_12">3</ref>) is set to the identity function and the normalization layer of Equation ( <ref type="formula" target="#formula_11">2</ref>) is removed. We set initial node features to be vectors of ones, and process them using a 2-layer MLP. A 1-layer MLP is applied to the final graph representation to generate predictions.</p><p>The benchmarks CSL and EXP-Class require the classification of graphs into isomorphism classes. To evaluate the model's performance, we used 5-fold cross validation on CSL and 4-fold cross validation on EXP-Class. Graphs contained in the CSL dataset present a maximum diameter of 11. We thus set K to 11 for PathNN-SP and PathNN-SP + . K is set to 5 for PathNN-AP. As EXP graphs are disconnected, we empirically set K to 10 for PathNN-SP and PathNN-SP + and 5 for PathNN-AP since these values allow the models to achieve perfect performance in this task. For both experiments and all models, we train for 200 epochs using the Adam optimizer with learning rate 10 -3 . The hidden dimension size is set to 64. Batch sizes are set to 32 except for PathNN-AP where we set it to 8 for CSL and 16 for CEXP to be able to fit all paths in memory.</p><p>For PathNN-AP, we apply Euclidean normalization before feeding the represenentations to the LSTM. Euclidean normalization is used instead of BN because the latter results in training instabilities. All PathNN-AP models are trained with distance encoding.</p><p>For the SR and EXP-Iso datasets, we investigate whether the proposed models have the right inductive bias to distinguish these pairs of graphs. Similarly to <ref type="bibr">Bodnar et al. (2021a)</ref>, we consider two graphs isomorphic if the Euclidean distance between their representation is below a fixed threshold ?.</p><p>Graph representations are computed by an untrained model variant where the prediction layer is removed. We remove the normalization layer of Equation ( <ref type="formula" target="#formula_11">2</ref>) and instead apply Euclidean normalization to the LSTM's input. For EXP-Iso, we use the same K as in the experiments on EXP-Class.</p><p>For the SR datasets, we set K to 4 as retrieving all paths GIN <ref type="bibr" target="#b61">(Xu et al., 2019)</ref> 10.0 ? 0.0 50.0 ? 0.0 600 3WLGNN <ref type="bibr">(Maron et al., 2019a)</ref> 97.8 ? 10.9 100.0 ? 0.0 0 of lengths higher than 4 is computationally challenging on these densely connected graphs. Hidden dimension size is set to 16 and ? to 10 -5 . The experiment is repeated with 5 different seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PathNN</head><p>Results. The results are given in Table <ref type="table" target="#tab_2">1</ref> and in Figure <ref type="figure" target="#fig_4">4</ref>. We observe that the two most expressive variants of the proposed model, namely Path-SP + and Path-AP, can distinguish all 10 isomorphism classes present in the CSL dataset.</p><p>On the other hand, PathNN-SP fails to distinguish one of the 10 classes, but still significantly outperforms the GIN model. All three variants of the proposed model achieve perfect accuracy on EXP-Class and never fail on the EXP-Iso dataset. Those datasets contain graphs that are not distinguishable by 1-WL, and thus, not surprisingly, GIN maps the graphs of each pair to the same vector. Finally, we evaluate the most expressive variant of our model (i. e., PathNN-AP) on the SR dataset whose instances contain graphs not distinguishable by 3-WL. We can see in Figure <ref type="figure" target="#fig_4">4</ref> that in most cases, PathNN-AP can successfully distinguish more than 80% of the graphs contained in each instance of the dataset. SR(29, 14, 6, 7) seems to be particularly hard for PathNN-AP since its failure rate on this instance is high compared to its performance on other instances. Overall, our experiments confirm the high expressiveness of the proposed model in terms of distinguishing non-isomorphic graphs.  <ref type="bibr" target="#b68">(Zhang et al., 2018)</ref> 76.6 ? 4.5 72.9 ? 3.5 76.4 ? 1.7 38.9 ? 5.7 69.2 ? 3.0 45.6 ? 3.4 DiffPool <ref type="bibr" target="#b64">(Ying et al., 2018)</ref> 75.0 ? 4.3 73.7 ? 3.5 76.9 ? 1.9 59.5 ? 5.6 68.4 ? 3.3 45.6 ? 3.4 ECC <ref type="bibr" target="#b52">(Simonovsky &amp; Komodakis, 2017)</ref> 72.6 ? 4.1 72.3 ? 3.4 76.2 ? 1.4 29.5 ? 8.2 67.7 ? 2.8 43.5 ? 3.1 GIN <ref type="bibr" target="#b61">(Xu et al., 2019)</ref> 75.3 ? 2.9 73.3 ? 4.0 80.0 ? 1.4 59.6 ? 4.5 71.2 ? 3.9 48.5 ? 3.3 GraphSAGE <ref type="bibr" target="#b28">(Hamilton et al., 2017)</ref> 72.9 ? 2.0 73.0 ? 4.5 76.0 ? 1.8 58.2 ? 6.0 68.8 ? 4.5 47.6 ? 3.5 GAT <ref type="bibr" target="#b56">(Veli?kovi? et al., 2018)</ref> 73.9 ? 3.4 70.9 ? 2.7 77.3 ? 2.5 49.5 ? 8.9 69.2 ? 4.8 48.2 ? 4.9 SPN (K = 1) <ref type="bibr" target="#b1">(Abboud et al., 2022)</ref> 72.7 ? 2.6 71.0 ? 3.7 80.0 ? 1.5 67.5 ? 5.5 NA NA SPN (K = 5) <ref type="bibr" target="#b1">(Abboud et al., 2022)</ref> 77.4 ? 3.8 74.2 ? 2.7 78.6 ? 3.8 69.4 ? 6.2 NA NA PathNet (N = 10, K = 2) <ref type="bibr" target="#b54">(Sun et al., 2022)</ref> OOM 70.5 ? 3.9 64.1 ? 2.3 69.3 ? 5.4 70.4 ? 3.8 49.1 ? 3.6 Nested GNN <ref type="bibr" target="#b67">(Zhang &amp; Li, 2021)</ref> 77.8 ? 3.9 74.2 ? 3.7 NA 31.2 ? 6.7 NA NA PathNN-P (K = 1) 76.9 ? 3.7 75.2 ? 3.9 77. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Real-World Datasets</head><p>Datasets. We evaluate the proposed model on 6 datasets contained in the TUDataset collection <ref type="bibr">(Morris et al., 2020a)</ref>: DD, NCI1, PROTEINS, ENZYMES, IMDB-B and IMDB-M. We also evaluate the proposed model on ogbg-molhiv, a molecular property prediction dataset from the Open Graph Benchmark (OGB) <ref type="bibr" target="#b31">(Hu et al., 2020)</ref>. We conduct an experiment on the ZINC 12K dataset <ref type="bibr" target="#b19">(Dwivedi et al., 2020)</ref>. Finally, we experiment with Peptides-struct and Peptidesfunc <ref type="bibr" target="#b20">(Dwivedi et al., 2022)</ref>, two datasets that require longrange dependencies between nodes to be captured.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment setup.</head><p>In all experiments, we use 2-layer MLPs with BN to map initial node representations to the desired dimension, and 2-layer MLPs without BN for prediction. All PathNN-AP are trained with distance encoding.</p><p>Following <ref type="bibr" target="#b22">Errica et al. (2020)</ref>, we evaluate TUDatasets using a 10-fold cross validation using their provided data splits. We let ? be a ReLU non-linearity and choose hidden dimension size from {32, 64}. We apply dropout to the input of the LSTM layer and between the first and second layer of the final MLP. The dropout rate is chosen from {0, 0.5}. We run the experiment for various values of K ? {1, 2, 3} to analyze the effect of increased path length on performance. The batch size is set to 32 for all datasets and all values of K except for simple paths on DD with K = 2 where we had to decrease batch size to 16 because of memory constraints. The Adam optimizer is used with fixed learning rate 0.001. The diameter of the graphs contained in IMDB-B and IMDB-M is at most equal to 2. Thus, for those two datasets, we do not provide results for paths of length up to K = 3. Similarly to <ref type="bibr" target="#b22">Errica et al. (2020)</ref>, we use one-hot encodings of given node attributes for all datasets except IMDB-B and IMDB-M, where we instead use one-hot encodings of node degrees. We also include the 18 continuous node feature vectors available for ENZYMES. We fit each model for 500 epochs and stop training if validation accuracy does not increase for 250 epochs.</p><p>All other experiments are conducted using available data splits. We set ? to a 2-layer MLP with BN for all experiments and set K and the layer's hidden size to respect the 500K parameter budget for ZINC, Peptides-functional and Peptides-structural. Details about the hyperparameter configuration can be found in Appendix D. All of these datasets contain additional edge features. Similarly to the Distance aware LSTM cell described in Appendix C.1, we build an Edge LSTM cell that takes as input a sequence of node and edge representations. For AP, the LSTM cell encodes both edges in the path and distance from the central node. Further detail can be found in Appendix E.</p><p>Results. Table <ref type="table" target="#tab_3">2</ref> illustrates the classification accuracy achieved by the proposed PathNNs and the baselines on the six datasets from the TUDataset collection. We observe that our PathNNs outperform the baselines on 5 out of the 6 datasets. When K = 1, all three variants of our model are identical, since all path variants are identical when paths of length one are considered. We denote this variant by PathNN-P. On most datasets, PathNN-P provides the highest accuracy. This is not surprising for IMDB-B and IMDB-M since the graphs contained in these datasets are ego-networks of radius 2. On the other hand, it appears that on DD and NCI1, more global information needs to be captured. On these two datasets, models that consider paths of length up to K = 3 achieve the highest accuracy. On some datasets, the proposed model significantly outperforms the baselines. Notably, on the ENZYMES, NCI1 and ogbg-molhiv ? GCN <ref type="bibr" target="#b34">(Kipf &amp; Welling, 2017)</ref> 76.06 ? 0.97 GIN <ref type="bibr" target="#b61">(Xu et al., 2019)</ref> 75.58 ? 1.40 GCN+FLAG <ref type="bibr" target="#b35">(Kong et al., 2020)</ref> 76.83 ? 1.02 GIN+FLAG <ref type="bibr" target="#b35">(Kong et al., 2020)</ref> 76.54 ? 1.14 GSN <ref type="bibr" target="#b11">(Bouritsas et al., 2022)</ref> 77.99 ? 1.00 HIMP <ref type="bibr" target="#b25">(Fey et al., 2020)</ref> 78.80 ? 0.82 PNA <ref type="bibr" target="#b14">(Corso et al., 2020)</ref> 79.05 ? 1.32 DGN <ref type="bibr" target="#b4">(Beaini et al., 2021)</ref> 79.70 ? 0.97 Graphormer <ref type="bibr" target="#b63">(Ying et al., 2021)</ref> 80.51 ? 0.53 CIN <ref type="bibr">(Bodnar et al., 2021a)</ref> 80.94 ? 0.57 ESAN <ref type="bibr" target="#b5">(Bevilacqua et al., 2022)</ref> 78.00 ? 1.42 E-SPN <ref type="bibr" target="#b1">(Abboud et al., 2022)</ref> 77.10 ? 1.20 GRWNN <ref type="bibr" target="#b47">(Nikolentzos &amp; Vazirgiannis, 2023)</ref> 78.38 ? 0.99 AgentNet <ref type="bibr" target="#b41">(Martinkus et al., 2023)</ref> 78.33 ? 0.69 PathNN-SP (K = 2) 78.62 ? 1.30 PathNN-SP + (K = 2) 79.17 ? 1.09 PathNN-AP (K = 2) 78.84 ? 1.46</p><p>IMDB-M datasets, our PathNNs offer respective absolute improvements of 3.6%, 2.3% and 1.7% in accuracy over the best competitor. Overall, our results indicate that PathNNs achieve high levels of performance on the TUDatasets.</p><p>We next evaluate the proposed model on the two datasets that require long-range interaction reasoning to achieve strong performance. The results are shown in Table <ref type="table" target="#tab_5">3</ref>. We can see that the variants of our PathNNs outperform the baselines on both datasets. On Peptides-Functional, our model offers a respective absolute improvement of 8.86% in average precision over GCN, while on Peptides-Structural, it offers a respective absolute improvement of 8.80% in mean absolute error over GatedGCN. To summarize, our results suggest that PathNNs can better capture long-range interactions between nodes than the baseline models.</p><p>Table <ref type="table" target="#tab_6">4</ref> shows the ROC-AUC of the different methods on the ogbg-molhiv dataset. We observe that PathNN is ranked as the third best model on this dataset. Among the three variants of the proposed model, PathNN-SP + achieves the  <ref type="bibr" target="#b28">(Hamilton et al., 2017)</ref> 16 0.398 ? 0.002 MoNet <ref type="bibr" target="#b42">(Monti et al., 2017)</ref> 16 0.292 ? 0.006 GAT <ref type="bibr" target="#b56">(Veli?kovi? et al., 2018)</ref> 16 0.384 ? 0.007 GIN <ref type="bibr" target="#b61">(Xu et al., 2019)</ref> 5 0.387 ? 0.015 GatedGCN <ref type="bibr" target="#b12">(Bresson &amp; Laurent, 2017)</ref> 4 0.435 ? 0.011 GatedGCN-E <ref type="bibr" target="#b12">(Bresson &amp; Laurent, 2017)</ref> 4 0.282 ? 0.015 RingGNN-E <ref type="bibr" target="#b13">(Chen et al., 2019)</ref> 2 0.353 ? 0.019 3WLGNN <ref type="bibr">(Maron et al., 2019a)</ref> 3 0.407 ? 0.028 3WLGNN-E <ref type="bibr">(Maron et al., 2019a)</ref> 3 0.256 ? 0.054 GNNML3 <ref type="bibr" target="#b2">(Balcilar et al., 2021)</ref> NA 0.161 ? 0.006 Graphormer <ref type="bibr" target="#b63">(Ying et al., 2021)</ref> NA 0.122 ? 0.006 CIN <ref type="bibr">(Bodnar et al., 2021a)</ref> NA 0.079 ? 0.006 ESAN <ref type="bibr" target="#b5">(Bevilacqua et al., 2022)</ref> NA 0.102 ? 0.003 KP-GIN <ref type="bibr" target="#b23">(Feng et al., 2022)</ref> NA 0.093 ? 0.007 AgentNet <ref type="bibr">(Martinkus et al.,</ref>  Finally, we evaluate the proposed model in a graph regression task on ZINC12K in Table <ref type="table" target="#tab_7">5</ref>. The three PathNN variants outperform most of the baselines on this dataset, despite, some of the baseline models, such as 3WLGNN, ESAN and GNNML3, being very expressive models considered to be state-of-the-art for many graph learning problems. With regards to the three PathNN variants, PathNN-AP performs best, but all three models exhibit promising performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented the PathNN model that aggregates path representations to generate node representations. We proposed three different variants that focus on single shortest paths, all shortest paths and all simple paths of length up to K. We have shown some of our PathNNs to be strictly more powerful than the 1-WL algorithm. Experimental results confirm our theoretical results. The different PathNN variants were also evaluated on graph classification and graph regression tasks. In most cases, our PathNNs outperform the baselines.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Running Time</head><p>We measured the running time of the proposed model on the DD (which contains the largest graphs among the considered TUDatasets) and ZINC12K datasets. We use the same hyperparameter values for all models (e. g., batch size, etc.). We provide the running time of the proposed model and that of GIN (in seconds) in Table <ref type="table" target="#tab_9">6</ref>. These experiments were run over an NVIDIA Tesla T4 GPU with 16GB of memory. As expected, we observe that the running time of the proposed model is higher than that of GIN. For K = 1, the increase in the running time is not large, while no preprocessing takes place. For K &gt; 1, on DD, the extraction of the paths takes a significant amount of time (since the graphs contained in this dataset are large and dense), while on ZINC12K, extracting paths of length up to 4 is relatively inexpensive (since graphs are small and sparse). With regards to the increase in training and inference time, the most expensive model (PathNN-AP) requires approximately 8 times and 5 times the time of GIN on DD and ZINC, respectively. We conclude that, the experimental running times of the model for bounded values of k are manageable. On ZINC12K, we also measured the running time of a subgraph GNN model, DSS-GIN <ref type="bibr" target="#b5">(Bevilacqua et al., 2022)</ref>, and of KP-GIN <ref type="bibr" target="#b23">(Feng et al., 2022)</ref> which are more expressive than GIN. We can see that the preprocessing time of these two models is significantly greater than these of the proposed models. Furthermore, training and inference time of KP-GIN is slightly smaller than those of the proposed models, while DSS-GIN is less efficient than PathNN-SP and PathNN-SP + , but slightly more efficient than PathNN-AP.</p><p>As discussed above, the running time on DD is higher than that on ZINC12K since the graphs contained in DD are much larger than those contained in ZINC12K. We provide in Table <ref type="table" target="#tab_10">7</ref> the average number of paths of different type (i. e., SP, SP + and AP) and of different length for the two considered datasets. We observe that the average number of paths for DD is much larger than that of the other dataset. This justifies the difference in running time observed between the two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Distance-Aware AP-Trees</head><p>While AP-Trees are more expressive than the 1-WL algorithm, they still fail in some cases where two nodes have similar AP-Tree structures but play a different structural role. An example is provided in Figure <ref type="figure" target="#fig_6">8</ref> where nodes v and u have similar AP-Trees. However, at level 3 of the tree rooted at v, only nodes with a geodesic distance of 1 from v are considered <ref type="bibr">(Figure 8(b)</ref>, left), while nodes with distance 2 are considered at the same level of the tree rooted at u (Figure <ref type="figure" target="#fig_6">8</ref>(b), right). Using distance information in this situation allows us to disambiguate these two structurally similar trees, and therefore allows for more expressiveness.</p><p>We validate this theory empirically by noting that PathNN without the distance-aware LSTM cell can not distinguish any pair of graphs in SR datasets when considering simple paths of length up to 4. On the other hand, PathNN with similar hyperparameters using the distance-aware LSTM cell is able to distinguish most pairs of graphs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Distance Encoding</head><p>Shortest paths naturally encode the information of distance to the starting node. However, simple paths lack this distance information as distance and position in simple paths are not directly related. PathNNs directly take sequences of paths of increasing length as input and thus only have access to information over the sequence order. We found that including distance information when processing a node in the sequence is crucial for distinguishing harder examples of non-isomorphic graphs. An example of how this process helps disambiguating Path-Trees is provided in Appendix C. We straightforwardly include distance information by designing a distance-aware LSTM cell. We start by mapping distance to the start node to d-dimensional vectors using a learned embedding look-up table. Let d l ? R d be the representation of the distance value l ? {0, . . . , <ref type="bibr">K} and d(v, u)</ref> be the geodesic distance between nodes v and u. d l , l ? 0, . . . , K are vectors of weights, randomly initialized that will be learned during training to produce rich representation of distances to the central node. Distance representations are shared across nodes, such that two pairs of nodes have equal distance representations if they have equal geodesic distance l. We modify the usual LSTM cell to include distance information as follows l = d (?(1), ?(t)) ,</p><formula xml:id="formula_14">i t = ? W ii h ?(t) + W ih h t-1 + W id d l , f t = ? W f i h ?(t) + W f h h t-1 + W f d d l , g t = tanh W gi h ?(t) + W gh h t-1 + W gd d l , o t = ? W oi h ?(t) + W oh h t-1 + W od d l , c t = f t ? c t-1 + i t ? g t , h t = o t ? tanh(c t ),</formula><p>where h t is the LSTM hidden state at time t, c t is the cell state at time t, ? is the Hadamard product and i t , f t , g t , o t are the input, forget, cell and output gates respectively and ? is the sigmoid function. The distance-aware LSTM cell operates on sequences of tuples containing a node representation and the embedding of its distance to the starting node. The distance embedding learns to produce meaningful representations of distances that help distinguishing graphs. Since shortest paths directly include this distance information, the distance-aware LSTM cell is only used when PathNNs operate on AP.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. (a) A graph G with n = 6 vertices. (b) Shortest paths of length up to 2 starting from vertex v1, (c) all shortest paths of length up to 2 starting from vertex v1 and (d) all paths of length up to 3 starting from vertex v1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. (a) WL-Tree rooted at node v1 for the graph of Figure 1(a). (b) One of the two SP-Trees rooted at v1 of height 2. (c) SP + -Tree rooted at v1 of height 2 and (d) AP-Tree rooted at v1 of height 3. Note that we consider only SP and SP + -Trees of height up to 2because there is no shortest path of length higher than 2 that starts at v1 in the input graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>)) (b) Second iteration of PathNN-SP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Aggregation process of PathNN with P = SP for two iterations. Node colors corresponds to node feature vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Failure rate on different instances of the SR dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>AP 2 v and AP 2 u , respectively, augmented with distance encoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Example of two nodes with similar AP-Trees structures that can be distinguished with distance encoding. Node symbols in (b) corresponds to geodesic distance to the root node (squares for distance 1 and diamonds for distance 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>that non-isomorphic trees are embedded into different vectors, also equipped with an injective readout function is more expressive than the 1-WL algorithm. Note that only considering AP-Trees up to a fixed height K is sufficient to distinguish a broad class of graphs. Now that we have considered the case of all paths AP, we turn our attention to the case of the shortest paths in SP and SP + . In Proposition 3.4 we show that SP-Trees and SP + -Trees are unable to capture all differences in WL-Trees.Proposition 3.4. Let G = (V, E) and P be the collection of either single shortest paths SP or all shortest paths SP + in G. Let {P k v , P k u } ? T k be the P-Trees of height k rooted at v and u, respectively, and</figDesc><table><row><cell>Additionally, AP k v and AP k u can be v and W k different even if W k u are identical.</cell></row></table><note><p>Theorem 3.3 states that, if at iteration k 1-WL decides that two nodes have different colors, then their AP-Trees are structurally different. Additionally, it states that AP-Trees are also able to disambiguate nodes that the 1-WL algorithm would determine to be structurally similar. Hence, a model f : AP -? R d that embeds AP-Trees into d-dimensional vectors, such</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Test set classification accuracy (CSL, EXP-Class) and number of undistinguished pairs of graphs (EXP-Iso). Best results are highlighted in bold.</figDesc><table><row><cell>Model</cell><cell>CSL ?</cell><cell>EXP-Class ? EXP-Iso ?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Classification accuracy (? standard deviation) of our PathNNs and the baselines on the datasets from the TUDataset collection. Best performance is highlighted in bold. OOM means out-of-memory and NA means not available.</figDesc><table><row><cell>DD</cell><cell>PROTEINS</cell><cell>NCI1</cell><cell>ENZYMES</cell><cell>IMDB-B</cell><cell>IMDB-M</cell></row><row><cell>DGCNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Results on the Peptides-Functional and Peptides-Structural datasets (? standard deviation). Evaluation metrics are Average Precision and Mean Absolute Error, respectively. Best performance is highlighted in bold. Parameter budget is set to 500K parameters. Results are averaged over 4 random seeds.</figDesc><table><row><cell></cell><cell cols="4">K Peptides-Functional ? K Peptides-Structural ?</cell></row><row><cell cols="2">GCN (Kipf &amp; Welling, 2017) 5</cell><cell>59.30 ? 0.23</cell><cell>5</cell><cell>0.3496 ? 0.0013</cell></row><row><cell>GIN (Xu et al., 2019)</cell><cell>5</cell><cell>54.98 ? 0.79</cell><cell>5</cell><cell>0.3547 ? 0.0045</cell></row><row><cell>GatedGCN (Bresson &amp; Laurent, 2017)</cell><cell>5</cell><cell>58.64 ? 0.77</cell><cell>5</cell><cell>0.3420 ? 0.0013</cell></row><row><cell>PathNN-SP</cell><cell>8</cell><cell>68.16 ? 0.26</cell><cell>4</cell><cell>0.2545 ? 0.0032</cell></row><row><cell>PathNN-SP +</cell><cell>8</cell><cell>67.84 ? 0.52</cell><cell>4</cell><cell>0.2540 ? 0.0046</cell></row><row><cell>PathNN-AP</cell><cell>7</cell><cell>68.07 ? 0.72</cell><cell>4</cell><cell>0.2569 ? 0.0030</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>ROC-AUC score (? standard deviation) of the different methods on the ogbg-molhiv dataset. Results are averaged over 10 random seeds. Best performance is highlighted in bold.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Mean absolute error (? standard deviation) of the different methods on the ZINC12K datasets. Results are averaged over 10 random seeds. Best performance is highlighted in bold. Parameter budget is set to 500K parameters.</figDesc><table><row><cell>K</cell><cell>ZINC12K ?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>Running time of the different models on the DD and ZINC12K datasets. All reported times are in seconds.</figDesc><table><row><cell>DD</cell><cell>Preprocessing</cell><cell>Time per Epoch (Training)</cell><cell>Inference Time</cell></row><row><cell>GIN (L = 1)</cell><cell>-</cell><cell>0.36 (? 0.03)</cell><cell>0.03 (? 0.00)</cell></row><row><cell>GIN (L = 2)</cell><cell>-</cell><cell>0.47 (? 0.04)</cell><cell>0.03 (? 0.00)</cell></row><row><cell>PathNN-P (K = 1)</cell><cell>-</cell><cell>0.56 (? 0.02)</cell><cell>0.05 (? 0.00)</cell></row><row><cell>PathNN-SP (K = 2)</cell><cell>076.53 (? 1.88)</cell><cell>1.22 (? 0.02)</cell><cell>0.11 (? 0.01)</cell></row><row><cell>PathNN-SP + (K = 2)</cell><cell>139.17 (? 2.38)</cell><cell>1.57 (? 0.02)</cell><cell>0.13 (? 0.01)</cell></row><row><cell>PathNN-AP (K = 2)</cell><cell>098.98 (? 4.18)</cell><cell>3.33 (? 0.04)</cell><cell>0.23 (? 0.02)</cell></row><row><cell>ZINC12K</cell><cell>Preprocessing</cell><cell>Time per Epoch (Training)</cell><cell>Inference Time</cell></row><row><cell>GIN (L = 1)</cell><cell>-</cell><cell>0.90 (? 0.08)</cell><cell>0.13 (? 0.03)</cell></row><row><cell>GIN (L = 4)</cell><cell>-</cell><cell>1.97 (? 0.06)</cell><cell>0.18 (? 0.06)</cell></row><row><cell>DSS-GIN (K = 4)</cell><cell>112.27 (? 4.17)</cell><cell>4.25 (? 0.11)</cell><cell>0.25 (? 0.05)</cell></row><row><cell>KP-GIN (K = 4)</cell><cell>197.47 (? 0.57)</cell><cell>3.09 (? 0.05)</cell><cell>0.13 (? 0.03)</cell></row><row><cell>PathNN-P (K = 1)</cell><cell>-</cell><cell>1.65 (? 0.03)</cell><cell>0.23 (? 0.06)</cell></row><row><cell>PathNN-SP (K = 4)</cell><cell>021.28 (? 0.36)</cell><cell>3.56 (? 0.04)</cell><cell>0.26 (? 0.06)</cell></row><row><cell>PathNN-SP + (K = 4)</cell><cell>029.09 (? 0.43)</cell><cell>3.66 (? 0.06)</cell><cell>0.25 (? 0.05)</cell></row><row><cell>PathNN-AP (K = 4)</cell><cell>025.93 (? 0.89)</cell><cell>4.38 (? 0.06)</cell><cell>0.32 (? 0.05)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 .</head><label>7</label><figDesc>Average number of paths per graph on the DD and ZINC12K datasets. We do not report the number of paths of length higher than 2 on DD since we did not measure the proposed model's running time for such lengths of paths.</figDesc><table><row><cell></cell><cell></cell><cell>DD</cell><cell></cell><cell></cell><cell>ZINC12K</cell><cell></cell></row><row><cell></cell><cell>SP</cell><cell>SP +</cell><cell>AP</cell><cell>SP</cell><cell>SP +</cell><cell>AP</cell></row><row><cell>K = 2</cell><cell>2,466</cell><cell>3,583</cell><cell>6,581</cell><cell>68.8</cell><cell>68.9</cell><cell>69.2</cell></row><row><cell>K = 3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>68.1</cell><cell>79.0</cell><cell>88.0</cell></row><row><cell>K = 4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>61.7</cell><cell>70.5</cell><cell>110.3</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>G.N. is supported by <rs type="funder">ANR</rs> via the <rs type="projectName">AML-HELAS</rs> (<rs type="grantNumber">ANR-19-CHIA-0020</rs>) project and by the funding <rs type="funder">?cole Universitaire de Recherche (EUR) Bertip</rs>, plan France 2030.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_2sX7Yrb">
					<idno type="grant-number">ANR-19-CHIA-0020</idno>
					<orgName type="project" subtype="full">AML-HELAS</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A. Proofs of the Statements in Section 3.2 A.1. Proof of Theorem 3.3</p><p>Let W k v be the WL tree rooted at v of height k and AP k v be the AP-Tree rooted at v of height k. We have that W k v is an enumeration of walks of length k that starts at v, and that AP k v is an enumeration of simple paths of length k that starts at v. We start this proof by analyzing how nodes are distributed among levels of WL-Trees. Nodes in a WL-Tree usually appear at multiple levels of the tree. If a node is part of level l of the tree, it will always be part of levels {l + i : i is even}. Besides, the same node can occur multiple times at the same level. On the other hand, Lemma 3.2 states that AP k v is a subgraph of W k v . WL Trees thus contain elements that are not part of AP-Trees. We define such elements as 'redundant' where the terms comes from the fact that they have been processed on a higher level of the WL-Tree A 'redundant' node u at level l of the tree is a node that either belongs to the set of its ascendants, or a node that has a 'redundant' node in the set of its ascendants. Ascendants of u can be found by backtracking from u up to the root node. Similarly, we define as 'incremental' a node that is not 'redundant'. With this definition, it is easy to reconstruct paths by only visiting 'incremental' nodes when hopping from the root node to a leaf on W k v . Similarly, walks can be reconstructed by visiting at least one 'redundant' node from the top to the bottom of the tree.</p><p>Since we can recover all paths using this definition, then we can see AP k v as W k v truncated from all its 'redundant' nodes. Figure <ref type="figure">5</ref> provides an example based on the WL tree of the graph in Figure <ref type="figure">1</ref>(a), rooted at node v 1 . Now that we have decomposed WL trees, let's see how the WL algorithm decides that two trees are structurally different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Suppose that at iteration</head><p>Then there exists an 'incremental' node, v i , belonging to level k -1 of W k-1 v and an 'incremental' node, u i , belonging to level k -1 of W k u such that v i and u i share similar higher level structure, but have different degree,</p><p>The difference can only occur between "incremental" nodes v i and u i since 'redundant' nodes have been processed on higher level of the tree k ? ? {1, . . . , k -1} and did not succeed to disambiguate</p><p>Besides, this structural difference must exist and must happen at level k of the tree since</p><p>In other words, it suffices to compare the 'incremental' node distribution with respect to level k of the trees of N (v i ) and</p><p>Comparing all 'incremental' nodes included in W k v to the ones included in W k u is thus sufficient to distinguish WL trees, which can be done by comparing</p><p>We now show that there exist pairs of graphs such that</p><p>. We provide a simple example in Figure <ref type="figure">6</ref> where taking simple paths of length 3 is sufficient to distinguish G 1 from G 2 , which concludes this proof. In particular, WL fails to distinguish k-regular graphs because of this tendency to capture redundant element, polluting the tree's structure. Instead, only capturing simple paths allows to remove all redundancy and thus to distinguish harder example of graphs.</p><p>The WL tree rooted at v1 of the graph presented in Figure <ref type="figure">1</ref>(a), W 3 v 1 . Incremental nodes are marked in red, while redundant nodes are marked in green. v u</p><p>(c) P 2 v and P 2 u for P ? {SP, SP + }.</p><p>Figure <ref type="figure">7</ref>. WL-Trees can distinguish node v and u, while SP-Trees and SP + -Trees fail to do so, since they do not include the 'problematic edge' highlighted in red. However, SP-Trees or SP + -Trees rooted at any node different than u in the triangle graph will take the 'problematic edge' into account and hence guarantee disambiguation at the graph level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Proof of Proposition 3.4</head><p>To prove Proposition 3.4 it suffices to show a counterexample for which W k v and W k u are structurally different and P k v are identical P k u . Nodes v and u in Figure <ref type="figure">7</ref>(a) have the required property, which concludes this proof.</p><p>A.3. Proof of Theorem 3.5</p><p>We will show that if P k v = P k u holds for two nodes v, u ? V , then W k v = W k u also holds. We will prove this by induction on the depth k. We begin with the case where k = 1. Since P 1 u contains as many paths as node u has neighbours,</p><p>Note that for all types of paths and for any length k &gt; 0, the second node encountered when traversing the path is a neighbor of the source node, i. e., ?(2) ? N (v) for all ? ? P v . Then, it suffices to show that if</p><p>u , thus the two multisets have identical cardinalities. Furthermore, the two multisets contain the same elements since</p><p>that all the neighbors of the source node participate in at least one path since we extend terminating nodes with dummy nodes). Thus, the only way in which the two multisets can be unequal is if the</p><p>We will next show that this also cannot happen. If the multiplicities of WL-Trees in the two multisets are different from each other, we can always find two nodes w ? N (v) and z</p><p>We then consider nodes that occur as terminal nodes in paths where nodes w and z are involved, i. e., { {?(k</p><p>Furthermore, for k &gt; 2, the annotations of nodes in the final positions of the annotated paths encode their degree. Thus, we have that { {?(k</p><p>We have reached a contradiction and therefore, the multiplicities of WL-Trees of the two multisets are equal to each other. Thus, we have that</p><p>Therefore, by contraposition, we have that W t v ? = W t u =? P t v ? = P t u . To show the second component of our statement, that there are pairs of nodes v, u for which</p><p>, we simply need to consider one of the examples for which the WL test fails to distinguish two non-isomorphic graphs. Figure <ref type="figure">6</ref> contains such a pair of graphs, which concludes our proof. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Hyperparameter Configuration</head><p>Table <ref type="table">8</ref> displays the hyperparameter configuration for experiments conducted on ogbg-molhiv, Zinc12K, peptides-function and peptides-structural. For all of these experiments, we use 2-layer MLPs with BN as the ? function in Equation (3). Initial node and edge features are projected to the desired dimensional space using ogb Atom and Bound Encoders.</p><p>We found that using a mean readout function led to a performance boost on ogbg-molhiv and peptides-functional, as previously reported by traditional GNNs. We also experimented with a mean path aggregation function that replaces the sum over path representations in Equation ( <ref type="formula">2</ref>) with averaging over path representations. We found that using a mean aggregation function combined with a sum readout function led to better model performances on peptides-structural. Note that we remove the BN layer of Equation ( <ref type="formula">2</ref>) when using a mean path aggregation function.</p><p>We employ two kinds of early stopping strategy. The first method, patience, stops training when the validation score has not improved for r rounds, where r is a hyperparameter. The second strategy, lr, is used in addition to a learning rate scheduler that reduces the learning rate if the validation score has not improved for r rounds. The training is stopped when the learning rate reaches a minimum threshold value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Including Edge Features</head><p>Molecular datasets used in our experiments contain additional edge features. We create a modified LSTM cell that computes path representations using node and edge features, as well as distance embeddings when processing simple paths. Let e u,v be the edge representation of the edge (u, v). Similarly to the distance-aware LSTM cell presented in Section C.1, we define the edge LSTM cell as follow:</p><p>For t = 1, we set e ?(t-1),?(t) to a vector of zeros as the edge LSTM cell starts by processing a node representation, and no connection to other nodes exists yet. For t ? 2, the feature vector of the edge that connects the previous and current node in the path is processed together with the current node's representation. Note that we also use a version of this LSTM cell that also processes distance embeddings (as explained in Appendix C.1) when processing simple paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Datasets Details</head><p>The CSL dataset contains 4-regular graphs with edges connected to form a cycle and containing skip-links between nodes.</p><p>There are 150 graphs in total, each consisting of 41 nodes. The 150 graphs are equally divided into 10 isomorphism classes based on the skip-link length of the graph. EXP is a synthetic dataset that contains pairs of graphs that are not distinguishable by the 1-WL algorithm. However, these graphs are 2-WL distinguishable. Each graph encodes a propositional formula, and given a pair G, G ? of non-isomorphic graphs, the two graphs have different SAT outcomes, i. e., G encodes a satisfiable formula, while G ? encodes an unsatisfiable formula. We used the dataset for both graph isomorphism testing and as a binary classification task. SR is a dataset that contains strongly regular graphs. Those graphs are not distinguishable by the 3-WL algorithm. A set of strongly regular graphs with parameters (p 1 , p 2 , p 3 , p 4 ) means that each graph has p 1 nodes, the degree of each node is equal to p 2 , nodes that are connected by an edge share p 3 common neighbors and nodes that are not connected by an edge share p 4 common neighbors.</p><p>The ENZYMES dataset contains 600 protein tertiary structures represented as graphs obtained from the BRENDA enzyme database. Each enzyme is a member of one of the Enzyme Commission top level enzyme classes (EC classes) and the task is to correctly assign the enzymes to their classes <ref type="bibr">(Borgwardt et al., 2005)</ref>. The NCI1 dataset contains more than four thousand chemical compounds screened for activity against non-small cell lung cancer and ovarian cancer cell lines <ref type="bibr" target="#b58">(Wale et al., 2008)</ref>. PROTEINS contains proteins represented as graphs where vertices are secondary structure elements and there is an edge between two vertices if they are neighbors in the amino-acid sequence or in 3D space. The task is to classify proteins into enzymes and non-enzymes <ref type="bibr">(Borgwardt et al., 2005)</ref>. D&amp;D is a dataset that contains over a thousand protein structures. Each protein is a graph whose nodes correspond to amino acids and a pair of amino acids are connected by an edge if they are less than 6 ?ngstroms apart. The task is to predict if a protein is an enzyme or not <ref type="bibr" target="#b18">(Dobson &amp; Doig, 2003)</ref>.</p><p>The IMDB-B and IMDB-M datasets were created from IMDb, an online database of information related to movies and television programs. The graphs contained in the two datasets correspond to movie collaborations. The vertices of each graph represent actors/actresses and two vertices are connected by an edge if the corresponding actors/actresses appear in the same movie. Each graph is the ego-network of an actor/actress, and the task is to predict which genre an ego-network belongs to <ref type="bibr" target="#b62">(Yanardag &amp; Vishwanathan, 2015)</ref>.</p><p>The ogbg-molhiv dataset is a molecular property prediction dataset that is adopted from the MoleculeNet <ref type="bibr" target="#b59">(Wu et al., 2018)</ref>.</p><p>The dataset consists of 41, 127 molecules and corresponds to a binary classification dataset where the task is to predict whether a molecule inhibits HIV virus replication or not. The molecules in the training, validation and test sets are divided using a scaffold splitting procedure that splits the molecules based on their two-dimensional structural frameworks. The scaffold splitting attempts to separate structurally different molecules into different subsets.</p><p>ZINC is one of the most popular molecular datasets where the task is to predict the constrained solubility of molecules, an important chemical property for designing generative GNNs for molecules.</p><p>Peptides-func and Peptides-struct datasets are derived from 15, 535 peptides retrieved from SATPdb database that includes the sequence, molecular graph, function, and 3D structure of the peptides. Both datasets use the same set of graphs but differ in their prediction tasks. Peptides-func is a multi-label graph classification dataset where there exist 10 classes in total associated with the peptide function. Peptides-struct is a multi-label graph regression dataset where the goal is to predict aggregated 3D properties of the peptides at the graph level.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Surprising Power of Graph Neural Networks with Random Node Initialization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Abboud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">I</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lukasiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 30th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2112" to="2118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shortest Path Networks for Graph Property Prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Abboud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dimitrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">I</forename><surname>Ceylan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Learning on Graphs Conference</title>
		<meeting>the 1st Learning on Graphs Conference</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Breaking the Limits of Message Passing Graph Neural Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Balcilar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>H?roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gauzere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vasseur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Honeine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="599" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The logical expressiveness of graph neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Barcel?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Kostylev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Monet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Reutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Directional Graph Networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Passaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>L?tourneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="748" to="758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Equivariant Subgraph Aggregation Networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Balamurugan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weisfeiler and Lehman Go Cellular: CW Networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bodnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Otter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Montufar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2625" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Message Passing Simplicial Networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bodnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Otter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Montufar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lehman</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Topological</forename><surname>Go</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1026" to="1037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sch?nauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>Suppl. 1</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph Kernels: State-of-the-Art and Future Challenges</title>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ghisu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Llinares-L?pez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>O'bray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rieck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends? in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="531" to="712" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Shortest-Path Kernels on Graphs</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th IEEE International Conference on Data Mining</title>
		<meeting>the 5th IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bouritsas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="657" to="668" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Residual Gated Graph Con-vNets</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07553</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the equivalence between graph isomorphism testing and function approximation with GNNs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Principal Neighbourhood Aggregation for Graph Nets</title>
		<author>
			<persName><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="13260" to="13271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reconstruction for Powerful Graph Representations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1713" to="1726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Coloring Graph Neural Networks for Node Disambiguation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dasoulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Virmaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 29th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2126" to="2132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A note on two problems in connexion with graphs</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Dijkstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numerische Mathematik</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="269" to="271" />
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distinguishing Enzyme Structures from Non-enzymes Without Alignments</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Benchmarking Graph Neural Networks</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ramp??ek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parviz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.08164</idno>
		<title level="m">Long Range Graph Benchmark</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Easley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><surname>Networks</surname></persName>
		</author>
		<title level="m">crowds, and markets: Reasoning about a highly connected world</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Fair Comparison of Graph Neural Networks for Graph Classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Errica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Podda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bacciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Micheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">How Powerful are K-hop Message Passing Graph Neural Networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scalable kernels for graphs with continuous attributes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Feragen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kasenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Bruijne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="216" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-G</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Weichert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12179</idno>
		<title level="m">Hierarchical Inter-Message Passing for Learning on Molecular Graphs</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Expressiveness and Approximation Properties of Graph Neural Networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Geerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Reutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the approximation capability of recurrent neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Open Graph Benchmark: Datasets for Machine Learning on Graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="22118" to="22133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">RAW-GNN: RAndom Walk Aggregation based Graph Neural Network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Joint Conference on Artificial Intelligence</title>
		<meeting>the 31st International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2108" to="2114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Flag: Adversarial Data Augmentation for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09891</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Geodesic Graph Neural Network for Efficient Graph Representation Learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02636</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A survey on graph kernels</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Network Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Distance Encoding: Design Provably More Powerful Neural Networks for Graph Representation Learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4465" to="4478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Provably Powerful Graph Networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Invariant and Equivariant Graph Networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Agent-based Graph Neural Networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Martinkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Papp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wattenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model CNNs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 33rd AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">TUDataset: A collection of benchmark datasets for learning with graphs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graph Representation Learning and Beyond Workshop</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Weisfeiler and Leman go sparse: Towards scalable higher-order graph embeddings</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="21824" to="21840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Relational Pooling for Graph Representations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4663" to="4673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Geometric Random Walk Graph Neural Networks via Implicit Layers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 26th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2035" to="2053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dasoulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="195" to="205" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Graph Kernels: A Survey</title>
		<author>
			<persName><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Siglidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="943" to="1027" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Random Dropouts Increase the Expressiveness of Graph Neural Networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Papp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Martinkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Faber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wattenhofer</surname></persName>
		</author>
		<author>
			<persName><surname>Dropgnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="21997" to="22009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Random Features Strengthen Graph Neural Networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 SIAM International Conference on Data Mining</title>
		<meeting>the 2021 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="333" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A deep learning approach to antibiotic discovery</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Stokes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cubillos-Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Donghia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Macnair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Carfrae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bloom-Ackermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="688" to="702" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Structure-aware Path Aggregation Graph Neural Network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Beyond</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Homophily</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Joint Conference on Artificial Intelligence</title>
		<meeting>the 31st International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2233" to="2240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Toenshoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08786</idno>
		<title level="m">Graph Learning with 1D Convolutions on Random Walks</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Building powerful and equivariant graph neural networks with structural message-passing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vignac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="14143" to="14155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Comparison of descriptor spaces for chemical compound retrieval and classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="347" to="375" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Moleculenet: a benchmark for molecular machine learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A Comprehensive Survey on Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep Graph Kernels</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Do Transformers Really Perform Bad for Graph Representation?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="28877" to="28888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Hierarchical Graph Representation Learning with Differentiable Pooling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4805" to="4815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Position-aware Graph Neural Networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7134" to="7143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Identity-aware graph neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Gomes-Selman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 35th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10737" to="10745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Nested Graph Neural Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15734" to="15747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">An End-to-End Deep Learning Architecture for Graph Classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 32nd AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4438" to="4445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Persistence Enhanced Graph Neural Network</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Artificial Intelligence and Statistic</title>
		<meeting>the 23rd International Conference on Artificial Intelligence and Statistic</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2896" to="2906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Graph neural networks: A review of methods and applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Open</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="57" to="81" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
