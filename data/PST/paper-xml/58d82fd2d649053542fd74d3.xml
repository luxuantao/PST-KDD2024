<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Benchmark for Endoluminal Scene Segmentation of Colonoscopy Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-07-26">26 July 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">David</forename><surname>Vázquez</surname></persName>
							<email>dvazquez@cvc.uab.es</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Vision Center</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution">Universitat Autonoma de Barcelona</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Montreal Institute for Learning Algorithms</orgName>
								<orgName type="institution" key="instit2">Université de Montréal</orgName>
								<address>
									<settlement>Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jorge</forename><surname>Bernal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Vision Center</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution">Universitat Autonoma de Barcelona</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">F</forename><forename type="middle">Javier</forename><surname>Sánchez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Vision Center</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution">Universitat Autonoma de Barcelona</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gloria</forename><surname>Fernández-Esparrach</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Endoscopy Unit</orgName>
								<orgName type="department" key="dep2">Gastroenterology Service</orgName>
								<orgName type="institution" key="instit1">CIBERHED</orgName>
								<orgName type="institution" key="instit2">IDIBAPS, Hospital Clinic</orgName>
								<orgName type="institution" key="instit3">Universidad de Barcelona</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Antonio</forename><forename type="middle">M</forename><surname>López</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Vision Center</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution">Universitat Autonoma de Barcelona</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Montreal Institute for Learning Algorithms</orgName>
								<orgName type="institution" key="instit2">Université de Montréal</orgName>
								<address>
									<settlement>Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Montreal Institute for Learning Algorithms</orgName>
								<orgName type="institution" key="instit2">Université de Montréal</orgName>
								<address>
									<settlement>Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michal</forename><surname>Drozdzal</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">École Polytechnique de Montréal</orgName>
								<address>
									<settlement>Montréal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Imagia Inc</orgName>
								<address>
									<settlement>Montréal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Montreal Institute for Learning Algorithms</orgName>
								<orgName type="institution" key="instit2">Université de Montréal</orgName>
								<address>
									<settlement>Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Benchmark for Endoluminal Scene Segmentation of Colonoscopy Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-07-26">26 July 2017</date>
						</imprint>
					</monogr>
					<idno type="MD5">0871589447E79EBE7566C034B250C878</idno>
					<idno type="DOI">10.1155/2017/4037190</idno>
					<note type="submission">Received 24 February 2017; Accepted 22 May 2017;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Colorectal cancer (CRC) is the third cause of cancer death worldwide. Currently, the standard approach to reduce CRC-related mortality is to perform regular screening in search for polyps and colonoscopy is the screening tool of choice. The main limitations of this screening procedure are polyp miss rate and the inability to perform visual assessment of polyp malignancy. These drawbacks can be reduced by designing decision support systems (DSS) aiming to help clinicians in the different stages of the procedure by providing endoluminal scene segmentation. Thus, in this paper, we introduce an extended benchmark of colonoscopy image segmentation, with the hope of establishing a new strong benchmark for colonoscopy image analysis research. The proposed dataset consists of 4 relevant classes to inspect the endoluminal scene, targeting different clinical needs. Together with the dataset and taking advantage of advances in semantic segmentation literature, we provide new baselines by training standard fully convolutional networks (FCNs). We perform a comparative study to show that FCNs significantly outperform, without any further postprocessing, prior results in endoluminal scene segmentation, especially with respect to polyp segmentation and localization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Colorectal cancer (CRC) is the third cause of cancer death worldwide <ref type="bibr" target="#b0">[1]</ref>. CRC arises from adenomatous polyps (adenomas) which are initially benign; however, over time, some of them can become malignant. Currently, the standard approach to reduce CRC-related mortality is to perform regular screening in search for polyps and colonoscopy is the screening tool of choice. During the examination, clinicians visually inspect the intestinal wall (see Figure <ref type="figure" target="#fig_1">1</ref>(a) for an example of intestinal scene) in search of polyps. Once detected, they are resected and sent for histological analysis to determine their degree of malignancy and define the corresponding treatment the patient should undertake.</p><p>The main limitations of colonoscopy are its associated polyp miss rate (small/flat polyps or the ones hidden behind intestine folds can be missed <ref type="bibr" target="#b1">[2]</ref>) and the fact that polyp's malignancy degree is only known after histological analysis. These drawbacks can be reduced by developing new colonoscopy modalities to improve visualization (e.g., highdefinition imaging, narrow-band imaging (NBI) <ref type="bibr" target="#b2">[3]</ref>, and magnification endoscopes <ref type="bibr" target="#b3">[4]</ref>) and/or by developing decision support systems (DSS) aiming to help clinicians in the different stages of the procedure. A clinically useful DSS should be able to detect, segment, and assess the malignancy degree (e.g., by optical biopsy <ref type="bibr" target="#b4">[5]</ref>) of polyps during the colonoscopy procedure, following a similar pipeline to the one shown in Figure <ref type="figure" target="#fig_3">1(b)</ref>.</p><p>The development of DSS for colonoscopy has been an active research topic during the last decades. The majority of available works on optical colonoscopy are focused on polyp detection (e.g., see <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>), and only few works address the problems of endoluminal scene segmentation.</p><p>Endoluminal scene segmentation is of crucial relevance for clinical applications <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>. Polyp segmentation is important to define the area covered by a potential lesion that should be carefully inspected and possibly removed by clinicians. Moreover, having a system for accurate in vivo prediction of polyp histology might significantly improve clinical workflow. Lumen segmentation is relevant to help clinicians navigate through the colon during the procedure. Additionally, it can be used to establish quality metrics related to the degree of the colon wall that has been explored, since a weak exploration can lead to polyp overlooking. Finally, specular highlights have proven to be useful in reducing polyp detection false-positive ratio in the context of handcrafted methods <ref type="bibr" target="#b14">[15]</ref>.</p><p>In recent years, convolutional neural networks (CNNs) have become a de facto standard in computer vision, achieving state-of-the-art performance in tasks such as image classification, object detection, and semantic segmentation; and making traditional methods based on handcrafted features obsolete. Two major components in this groundbreaking progress were the availability of increased computational power (GPUs) and the introduction of large labeled datasets <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. Despite the additional difficulty of having limited amounts of labeled data, CNNs have successfully been applied to a variety of medical imaging tasks, by resorting to aggressive data augmentation techniques <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. More precisely, CNNs have excelled at semantic segmentation tasks in medical imaging, such as the EM ISBI 2012 dataset <ref type="bibr" target="#b19">[20]</ref>, BRATS <ref type="bibr" target="#b20">[21]</ref>, or MS lesions <ref type="bibr" target="#b21">[22]</ref>, where the top entries are built on CNNs <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>. Surprisingly, to the best of our knowledge, CNNs have not been applied to semantic segmentation of colonoscopy data. We associate this to the lack of large publicly available annotated databases, which are needed in order to train and validate such networks.</p><p>In this paper, we aim to overcome this limitation by introducing an extended benchmark of colonoscopy images created from the combination of the two largest public datasets of colonoscopy images <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26]</ref> and by incorporating additional annotations to segment lumen and specular highlights, with the hope of establishing a new strong benchmark for colonoscopy image analysis research. We provide new baselines on this dataset by training standard fully convolutional networks (FCNs) for semantic segmentation <ref type="bibr" target="#b26">[27]</ref> and significantly outperforming, without any further postprocessing, prior results in endoluminal scene segmentation.</p><p>Therefore, the contributions of this paper are twofold:</p><p>(1) Extended benchmark for colonoscopy image segmentation</p><p>(2) New state-of-the-art in colonoscopy image segmentation.  The rest of the paper is organized as follows. In Section 2, we present the new extended benchmark, including the introduction of datasets as well as the performance metrics. After that, in Section 3, we introduce the FCN architecture used as baseline for the new endoluminal scene segmentation benchmark. Then, in Section 4, we show qualitative and quantitative experimental results. Finally, Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Endoluminal Scene Segmentation Benchmark</head><p>In this section, we describe the endoluminal scene segmentation benchmark, including evaluation metrics.</p><p>2.1. Dataset. Inspired by already published benchmarks for polyp detection, proposed within a challenge held in conjunction with MICCAI 2015 (http://endovis.grand-challenge.org) <ref type="bibr" target="#b27">[28]</ref>, we introduce a benchmark for endoluminal scene object segmentation. We combine CVC-ColonDB and CVC-ClinicDB into a new dataset (CVC-EndoSceneStill) composed of 912 images obtained from 44 video sequences acquired from 36 patients.</p><p>(i) CVC-ColonDB contains 300 images with associated polyp masks obtained from 13 polyp video sequences acquired from 13 patients.</p><p>(ii) CVC-ClinicDB contains 612 images with associated polyp and background (here, mucosa and lumen) segmentation masks obtained from 31 polyp video sequences acquired from 23 patients.</p><p>We extend the old annotations to account for lumen, specular highlights with new hand-made pixel-wise annotations, and we define a void class for black borders present in each frame. In the new annotations, background only  3 Journal of Healthcare Engineering contains mucosa (intestinal wall). Please refer to Table <ref type="table" target="#tab_0">1</ref> for dataset details and to Figure <ref type="figure" target="#fig_2">2</ref> for a dataset sample.</p><formula xml:id="formula_0">) (a) (b) (c) (d)</formula><p>We split the resulting dataset into three sets: training, validation, and test containing 60%, 20%, and 20% images, respectively. We impose the constraint that one patient cannot be in different sets. As a result, the final training set contains 20 patients and 547 frames, the validation set contains 8 patients and 183 frames, and the test set contains 8 patients and 182 frames. The dataset is publicly available (http://www.cvc.uab.es/CVC-Colon/ index.php/databases/cvc-endoscenestill/).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Metrics.</head><p>We use Intersection over Union (IoU), also known as Jaccard index, and per pixel accuracy as segmentation metrics. These metrics are commonly used in medical image segmentation tasks <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>We compute the mean of per class IoU. Each per class IoU is computed over a validation/test set according to the following formula:</p><formula xml:id="formula_1">IoU PR class , GT class = PR class ∩ GT class PR class ∪ GT class , 1</formula><p>where PR represents the binary mask produced by the segmentation method, GT represents the ground truth mask, ∩ represents set intersection, and ∪ represents set union. We compute the mean global accuracy for each set as follows:</p><formula xml:id="formula_2">Acc PR, GT = #TP #pixels , 2</formula><p>where TP represents the number of true positives. Notably, this new benchmark might as well be used for the relevant task of polyp localization. In that case, we follow Pascal VOC challenge metrics <ref type="bibr" target="#b30">[31]</ref> and determine that a polyp is localized if it has a high overlap degree with its associated ground truth, namely,</p><formula xml:id="formula_3">IoU PR polyp , GT polyp &gt; 0 5, 3</formula><p>where the metric is computed for each polyp independently and averaged per set to give a final score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Baseline</head><p>CNNs are a standard architecture used for tasks, where a single prediction per input is expected (e.g., image classification). Such architectures capture hierarchical representations of the input data by stacking blocks of convolutional, nonlinearity, and pooling layers on top of each other. Convolutional layers extract local features. Nonlinearity layers allow deep networks to learn nonlinear mappings of the input data.</p><p>Pooling layers reduce the spatial resolution of the representation maps by aggregating local statistics. FCNs <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b26">27]</ref> were introduced in the computer vision and medical imaging communities in the context of semantic segmentation. FCNs naturally extend CNNs to tackle per pixel prediction problems, by adding upsampling layers to recover the spatial resolution of the input at the output layer. As a consequence, FCNs can process images of arbitrary size.</p><p>In order to compensate for the resolution loss induced by pooling layers, FCNs introduce skip connections between their downsampling and upsampling paths. Skip connections help the upsampling path recover fine-grained information from the downsampling layers.</p><p>We implemented FCN8 architecture from <ref type="bibr" target="#b26">[27]</ref> and trained the network by means of stochastic gradient descent with the rmsprop adaptive learning rate <ref type="bibr" target="#b31">[32]</ref>. The validation split is used to early stop the training; we monitor mean IoU for validation set and use patience of 50. We used a minibatch size of 10 images. The input image is normalized in the range 0-1. We randomly crop the training images to 224 × 224 pixels. As regularization, we use dropout <ref type="bibr" target="#b32">[33]</ref> of 0.5, as mentioned in the paper <ref type="bibr" target="#b26">[27]</ref>. We do not use any weight decay.</p><p>As described in Section 2.1, colonoscopy images have a black border that we consider as a void class. Void classes do not influence the computation of the loss nor the metrics of any set, since the pixels marked as void class are ignored. As the number of pixels per class is unbalanced, in some experiments, we apply the median frequency balancing of <ref type="bibr" target="#b33">[34]</ref>.</p><p>During training, we experiment with data augmentation techniques such as random cropping, rotations, zooming, and sharing and elastic transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section, we report semantic segmentation and polyp localization results on the new benchmark.  <ref type="table" target="#tab_1">2</ref> presents an analysis on the influence of different data augmentation techniques and their impact on the validation performance. We evaluate random zoom from 0.9 to 1.1, rotations from 0 to 180 degrees, shearing from 0 to 0.4, and warping with σ ranging from 0 to 10. Finally, we evaluate the combination of all the data augmentation techniques.</p><p>As shown in the table, polyps significantly benefit from all data augmentation methods, in particular, from warping. Note that warping applies small elastic deformation locally, accounting for many realistic variations in the polyp shape. Rotation and zoom also have a strong positive impact on the polyp segmentation performance. It goes without saying that such transformations are the least aggressive ones, since they do not alter the polyp appearance. Shearing is most likely the most aggressive transformation, since it changes the polyp appearance and might, in some cases, result in unrealistic deformations.</p><p>While for lumen it is difficult to draw any strong conclusions, it looks like zooming and warping slightly deteriorate the performance, whereas shearing and rotation slightly improve it. As for specular highlights, all the data augmentation techniques that we tested significantly boost the segmentation results. Finally, background (mucosa) shows only slight improvement when incorporating data augmentations. This is not surprising; given its predominance throughout the data, it could be even considered background.</p><p>Overall, combining all the discussed data augmentation techniques leads to better results in terms of mean IoU and mean global accuracy. More precisely, we increase the mean IoU by 4.51% and the global mean accuracy by 1.52%. <ref type="table" target="#tab_2">3</ref> presents endoluminal scene semantic segmentation results for different numbers of classes. As shown in the table, using more underrepresented classes such as lumen or specular highlights makes the optimization problem more difficult. As expected and contrary to handcrafted segmentation methods, when considering polyp segmentation, deep learning-based approaches do not suffer from specular highlights, showing the robustness of the learnt features towards saturation zones in colonoscopy images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Influence of the Number of Classes. Table</head><p>Best results for polyp segmentation are obtained in the 2-class scenario (polyp versus background). However, segmenting lumen is a relevant clinical problem as mentioned in Section 1. Results achieved in the 3-class scenario are very encouraging, with a IoU higher than 50% for both polyp and lumen classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Comparison to</head><p>State-of-the-Art. Finally, we evaluate the FCN model on the test set. We compare our results to the combination of previously published handcrafted methods: map-based method (1) for polyp segmentation and <ref type="bibr" target="#b11">[12]</ref> a watershed-based method (2) for lumen segmentation and <ref type="bibr" target="#b14">[15]</ref> (3) for specular highlights segmentation.</p><p>The segmentation results on the test set are reported in Table <ref type="table" target="#tab_3">4</ref> and show a clear improvement of FCN8 over previously published methods. The following improvements can be observed when comparing previously published methods to the 4-class FCN8 model trained with data augmentation: 15% in IoU for background (mucosa), 29% in IoU for polyps, 18% in IoU for lumen, 14% in mean IoU, and 14% in mean accuracy. FCN8 is still outperformed by traditional methods when it comes to specular highlight class. However, it is important to note that specular highlight class is used by handcrafted methods to reduce false-positive ratio of polyp detection, and from our analysis, it looks like the FCN model is able to segment well polyps even when ignoring this class. For example, the best mean IoU of 72.74% and mean   of the model, where polyps have been missed or undersegmented. In row 5, the small polyp is missed by our segmentation method while, in row 6, the polyp is undersegmented. All cases exhibit decent lumen segmentation and good background (mucosa) segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Polyp Localization.</head><p>Endoluminal scene segmentation can be seen as a proxy to proper polyp detection in a colonoscopy video. In order to understand how well suited FCNs are to localize polyps, we perform a last experiment. In this experiment, we compute the polyp localization rate as a function of IoU between the model prediction and the ground truth. We can compute this IoU per frame, since our dataset contains a maximum of one polyp per image. This analysis describes the ability of a given method to cope with polyp appearance variability and stability on polyp localization.</p><p>The localization results are presented in Figure <ref type="figure" target="#fig_6">4</ref> and show a significant improvement when comparing FCN8 variants to the previously published method <ref type="bibr" target="#b12">[13]</ref>. For example, when considering a correct polyp localization to have at least 50% IoU, we observe an increase of 40% in the polyp localization rate. As a general trend, we observe that architectures trained using a fewer number of classes achieve a higher IoU, though the polyp localization difference starts to be more visible when really high overlapping degrees are imposed. Finally, as one would expect, we observe that the architectures that show better results in polyp segmentation are the ones that show better results in polyp localization.</p><p>4.3. Towards Clinical Applicability. Sections 4.1.3 and 4.2 presented results of a comparative study between FCNs and previous state-of-the-art of endoluminal scene object segmentation in colonoscopy images. As mentioned in Section 1, we foresee several clinical applications, which can be built from the results of endoluminal scene segmentation. However, in order to be deployed in the exploration room, they must comply with real-time constraints apart from offering a good segmentation performance. In this case and considering videos recorded at 25 frames per second, a DSS should not take more than 40 ms to process an image in order not to delay the procedure.</p><p>Considering this, we have computed processing times for each of the approaches studied in this paper. Results are presented in Table <ref type="table" target="#tab_4">5</ref>.</p><p>As shown in the table, none of the presented approaches currently meet real-time constraints. Running the FCN8 inference on an NVIDIA Titan X GPU takes 88 ms per frame. Note that this could easily be addressed by taking advantage of recent research on model compression <ref type="bibr" target="#b34">[35]</ref> by applying fancier FCN architectures that encourage feature reuse <ref type="bibr" target="#b35">[36]</ref>. Alternatively, we could exploit the temporal component and build more sophisticated architectures that would take advantage of the similarities among consecutive frames.  Despite computational constraints, FCNs' superior performance could lead to more reliable and impactful computer-assisted clinical applications, since they offer both a better performance and computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we have introduced an extended benchmark for endoluminal scene semantic segmentation. The benchmark includes extended annotations of polyps, background (mucosa), lumen, and specular highlights. The dataset provides the standard training, validation, and test splits for machine learning practitioners and will be publicly available upon paper acceptance. Moreover, standard metrics for the comparison have been defined, with the hope to speed up the research in the endoluminal scene segmentation area.</p><p>Together with the dataset, we provided new baselines based on fully convolutional networks, which outperformed by a large margin previously published results, without any further postprocessing. We extended the proposed pipeline and used it as proxy to perform polyp detection. Due to the lack of nonpolyp frames in the dataset, we reformulated the task as polyp localization. Once again, we highlighted the superiority of deep learning-based models over traditional handcrafted approaches. As expected and contrary to handcrafted segmentation methods, when considering polyp segmentation, deep learning-based approaches do not suffer from specular highlights, showing the robustness of the learnt features towards saturation zones in colonoscopy images. Moreover, given that FCN not only excels in terms of performance but also allows for nearly real-time processing, it has a great potential to be included in future DSS for colonoscopy.</p><p>Knowing the potential of deep learning techniques, efforts in the medical imaging community should be devoted to gather larger labeled datasets as well as designing deep learning architectures that would be better suited to deal with colonoscopy data. This paper pretends to make a first step towards novel and more accurate DSS by making all code and data publicly available, paving the road for more researchers to contribute to the endoluminal scene segmentation domain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) Colonoscopy image and corresponding labeling: blue for lumen, red for background (mucosa wall), and green for polyp. (b) Proposed pipeline of a decision support system for colonoscopy.</figDesc><graphic coords="2,132.40,221.07,335.23,128.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of a colonoscopy image and its corresponding ground truth: (a) original image, (b) polyp mask, (c) specular highlights mask, and (d) lumen mask.</figDesc><graphic coords="3,124.95,330.63,170.08,127.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4. 1 .</head><label>1</label><figDesc>Endoluminal Scene Semantic Segmentation. In this section, we first analyze the influence of different data augmentation techniques. Second, we evaluate the effect of having different numbers of endoluminal classes on polyp segmentation results. Finally, we compare our results with previously published methods. 4.1.1. Influence of Data Augmentation. Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3</head><label>3</label><figDesc>shows qualitative results of the 4-class FCN8 model trained with data augmentation. From left to right, each row shows a colonoscopy frame, followed by the corresponding ground truth annotation and FCN8 prediction. Rows 1 to 4 show correct segmentation masks, with very clean polyp segmentation. Rows 5 and 6 show failure modes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples of predictions for 4-class FCN8 model. Each subfigure represents a single frame, a ground truth annotation, and a prediction image. We use the following color-coding in the annotations: red for background (mucosa), blue for lumen, yellow for polyp, and green for specularity. (a), (b), (c), (d) show correct polyp segmentation, whereas (e), (d) show incorrect polyp segmentation.</figDesc><graphic coords="6,158.26,535.80,283.51,81.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Localization rate of polyps as a function of IoU. The x-axis represents the degree of overlap between ground truth and model prediction. The y-axis represents the percentage of correctly localized polyps. Different color plots represent different models: FCN8 with 4 classes, FCN8 with 3 classes, and FCN8 with 2 classes and previously published method<ref type="bibr" target="#b12">[13]</ref> (referred to as state-of-the-art in the plot).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of prior database content. All frames show at least one polyp.</figDesc><table><row><cell>Database</cell><cell cols="3">Number of patients Number of seq. Number of frames</cell><cell>Resolution</cell><cell>Annotations</cell></row><row><cell>CVC-ColonDB</cell><cell>13</cell><cell>13</cell><cell>300</cell><cell>500 × 574</cell><cell>Polyp, lumen</cell></row><row><cell>CVC-ClinicDB</cell><cell>23</cell><cell>31</cell><cell>612</cell><cell>384 × 288</cell><cell>Polyp</cell></row><row><cell>CVC-EndoSceneStill</cell><cell>36</cell><cell>44</cell><cell>912</cell><cell>500 × 574 &amp; 384 × 288</cell><cell>Polyp, lumen, background, specularity, border (void</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>FCN8 endoluminal scene semantic segmentation results for different data augmentation techniques. The results are reported on validation set.</figDesc><table><row><cell>Data augmentation</cell><cell>IoU background</cell><cell>IoU polyp</cell><cell>IoU lumen</cell><cell>IoU spec.</cell><cell>IoU mean</cell><cell>Acc mean</cell></row><row><cell>None</cell><cell>88.93</cell><cell>44.45</cell><cell>54.02</cell><cell>25.54</cell><cell>57.88</cell><cell>92.48</cell></row><row><cell>Zoom</cell><cell>89.89</cell><cell>52.73</cell><cell>51.15</cell><cell>37.10</cell><cell>57.72</cell><cell>90.72</cell></row><row><cell>Warp</cell><cell>90.00</cell><cell>54.00</cell><cell>49.69</cell><cell>37.27</cell><cell>58.97</cell><cell>90.93</cell></row><row><cell>Shear</cell><cell>89.60</cell><cell>46.61</cell><cell>54.27</cell><cell>36.86</cell><cell>56.83</cell><cell>90.49</cell></row><row><cell>Rotation</cell><cell>90.52</cell><cell>52.83</cell><cell>56.39</cell><cell>35.81</cell><cell>58.89</cell><cell>91.38</cell></row><row><cell>Combination</cell><cell>92.62</cell><cell>54.82</cell><cell>55.08</cell><cell>35.75</cell><cell>59.57</cell><cell>93.02</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>FCN8 endoluminal scene semantic segmentation results for different numbers of classes. The results are reported on validation set. In all cases, we selected the model that provided best validation results (with or without class balancing).</figDesc><table><row><cell>Number of classes</cell><cell>IoU background</cell><cell>IoU polyp</cell><cell>IoU lumen</cell><cell>IoU spec.</cell><cell>IoU mean</cell><cell>Acc mean</cell></row><row><cell>4</cell><cell>92.07</cell><cell>39.37</cell><cell>59.55</cell><cell>40.52</cell><cell>57.88</cell><cell>92.48</cell></row><row><cell>3</cell><cell>92.19</cell><cell>50.70</cell><cell>56.48</cell><cell>-</cell><cell>66.46</cell><cell>92.82</cell></row><row><cell>2</cell><cell>96.63</cell><cell>56.07</cell><cell>-</cell><cell>-</cell><cell>76.35</cell><cell>96.77</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results on the test set: FCN8 with respect to previously published methods.</figDesc><table><row><cell cols="2">Data augmentation</cell><cell>IoU background</cell><cell>IoU polyp</cell><cell>IoU lumen</cell><cell>IoU spec.</cell><cell>IoU mean</cell><cell>Acc mean</cell></row><row><cell>FCN8 performance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4 classes</cell><cell>None</cell><cell>86.36</cell><cell>38.51</cell><cell>43.97</cell><cell>32.98</cell><cell>50.46</cell><cell>87.40</cell></row><row><cell>3 classes</cell><cell>None</cell><cell>84.66</cell><cell>47.55</cell><cell>36.93</cell><cell>-</cell><cell>56.38</cell><cell>86.08</cell></row><row><cell>2 classes</cell><cell>None</cell><cell>94.62</cell><cell>50.85</cell><cell>-</cell><cell>-</cell><cell>72.74</cell><cell>94.91</cell></row><row><cell>4 classes</cell><cell>Combination</cell><cell>88.81</cell><cell>51.60</cell><cell>41.21</cell><cell>38.87</cell><cell>55.13</cell><cell>89.69</cell></row><row><cell cols="2">State-of-the-art methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[12, 13, 15]</cell><cell>-</cell><cell>73.93</cell><cell>22.13</cell><cell>23.82</cell><cell>44.86</cell><cell>41.19</cell><cell>75.58</cell></row></table><note><p>accuracy of 94.91% are obtained by the 2-class model without additional data augmentation.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Summary of processing times achieved by the different methods studied in the paper. FCN results are the same for all four classes considered as segmentation of the four classes is done at the same time * .Clearly, handcrafted methods take much longer to process one image. Moreover, they need to apply different methods to segment each class of interest, making them less clinically useful. Note that this is not the case for FCN-like architectures.</figDesc><table><row><cell>Method</cell><cell>Polyp</cell><cell>Lumen</cell><cell>Specular highlights</cell><cell>Background</cell></row><row><cell>FCN</cell><cell cols="2">88 ms  *  88 ms  *</cell><cell>88 ms  *</cell><cell>88 ms  *</cell></row><row><cell cols="3">State-of-the-art 10000 ms 8000 ms</cell><cell>5000 ms</cell><cell>23000 ms</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Journal of Healthcare Engineering</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the developers of Theano <ref type="bibr" target="#b36">[37]</ref> and Keras <ref type="bibr" target="#b37">[38]</ref>. The authors acknowledge the support of the following agencies for research funding and computing support: Imagia Inc.; Spanish government through funded Project AC/DC TRA2014-57088-C2-1-R and iVEN-DIS (DPI2015-65286-R); SGR Projects 2014-SGR-1506, 2014-SGR-1470, and 2014-SGR-135; CERCA Programme/ Generalitat de Catalunya; and TECNIOspring-FP7-ACCI grant, FSEED, and NVIDIA Corporation for the generous support in the form of different GPU hardware units.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of Interest</head><p>The authors declare that they have no conflicts of interest. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robotics Journal of</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Society</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Colorectal cancer</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Factors inuencing the miss rate of polyps in a back-to-back colonoscopy study</title>
		<author>
			<persName><forename type="first">A</forename><surname>Leufkens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Oijen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Vleggaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Siersema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Endoscopy</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="470" to="475" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Narrow-band imaging in the diagnosis of colorectal mucosal lesions: a pilot study</title>
		<author>
			<persName><forename type="first">H</forename><surname>Machida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hamamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Endoscopy</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1094" to="1098" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Magnification endoscopy, high resolution endoscopy, and chromoscopy; towards a better optical diagnosis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bruno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gut</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Colonoscopic optical biopsy: bridging technological advances to clinical practice</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Backman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastroenterology</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">1863</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">WM-DOVA maps for accurate polyp highlighting in colonoscopy: validation vs. saliency maps from physicians</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fernández-Esparrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vilariño</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A comparison of blood vessel features and local binary patterns for colorectal polyp classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stehle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Behrens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Medical Imaging, article 72602Q</title>
		<imprint>
			<publisher>International Society for Optics and Photonics</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Colonoscopic polyp detection using convolutional neural networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sargent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Medical Imaging, article 978528, International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Colonic polyp classification with convolutional neural networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Uhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Häfner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE 29th International Symposium on Computer-Based Medical Systems (CBMS)</title>
		<meeting><address><addrLine>Dublin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="253" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Toward embedded detection of polyps in wce images for early diagnosis of colorectal cancer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Histace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Romain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Granado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Assisted Radiology and Surgery</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="283" to="293" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automated polyp detection in colonoscopy videos using shape and context information</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gurudu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="630" to="644" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discarding non informative regions for efficient colonoscopy image analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Sánchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Computer-Assisted and Robotic Endoscopy</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Polyp segmentation method in colonoscopy videos by means of MSA-DOVA energy maps calculation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Núñez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vilariño</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2014 Workshop on Clinical Image-Based Procedures</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="41" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Impact of keypoint detection on graph-based characterization of blood vessels in colonoscopy videos</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Núñez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vilariño</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Computer-Assisted and Robotic Endoscopy</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="22" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Impact of image preprocessing methods on polyp localization in colonoscopy frames</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vilarino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</title>
		<meeting><address><addrLine>Osaka</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="7350" to="7354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">ImageNet: a large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>CVPR09</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Microsoft coco: common objects in context</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<meeting><address><addrLine>Zurich</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The importance of skip connections in biomedical image segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chartrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kadoury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1608.04117" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">U-net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Crowdsourcing the creation of image segmentation algorithms for connectomics</title>
		<author>
			<persName><forename type="first">I</forename><surname>Arganda-Carreras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroanatomy</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">142</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The multimodal brain tumor image segmentation benchmark (BRATS)</title>
		<author>
			<persName><forename type="first">B</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1993" to="2024" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3d segmentation in the clinic: a grand challenge ii: Ms lesion segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Styner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Midas Journal</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep 3d convolutional encoder networks with shortcuts for multiscale feature integration applied to multiple sclerosis lesion segmentation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Y W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Traboulsee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1229" to="1239" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep contextual networks for neuronal structure segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 13th AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-02">February 2016</date>
			<biblScope unit="page" from="1167" to="1173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Brain tumor segmentation with deep neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Havaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1505.03540" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards automatic polyp detection with a polyp appearance model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vilarino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3166" to="3182" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Comparative validation of polyp detection methods in video colonoscopy: results from the miccai 2015 endoscopic vision challenge</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1231" to="1249" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Urinary bladder segmentation in ct urography using deep-learning convolutional neural network and level sets</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hadjiiski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Samala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Caoili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Cohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Physics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1882" to="1896" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A brain tumor segmentation framework based on outlier detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Prastawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bullitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="275" to="283" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">COURSERA: Neural Networks for Machine Learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>rmsprop adaptive learning</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1411.4734" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fitnets: hints for thin deep nets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1608.06993" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Theano: a python framework for fast computation of mathematical expressions</title>
		<author>
			<persName><forename type="first">Theano</forename><surname>Development</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Team</forename></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1605.02688" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Keras</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Journal of Healthcare Engineering</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
