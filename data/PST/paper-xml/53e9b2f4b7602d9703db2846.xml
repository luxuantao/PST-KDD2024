<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning interpretable fuzzy inference systems with FisPro</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011-04-06">6 April 2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Serge</forename><surname>Guillaume</surname></persName>
							<email>serge.guillaume@montpellier.cemagref.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR ITAP</orgName>
								<address>
									<addrLine>BP 5095</addrLine>
									<postCode>34196</postCode>
									<settlement>Cemagref, Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Brigitte</forename><surname>Charnomordic</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">INRA/SupAgro</orgName>
								<orgName type="institution" key="instit2">UMR MISTEA</orgName>
								<address>
									<postCode>34060</postCode>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning interpretable fuzzy inference systems with FisPro</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2011-04-06">6 April 2011</date>
						</imprint>
					</monogr>
					<idno type="MD5">903BE1DF6C200747978D49D9DFD0B51B</idno>
					<idno type="DOI">10.1016/j.ins.2011.03.025</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Fuzzy rule bases Interpretability Modeling Rule induction Fuzzy partitioning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fuzzy inference systems (FIS) are likely to play a significant part in system modeling, provided that they remain interpretable following learning from data. The aim of this paper is to set up some guidelines for interpretable FIS learning, based on practical experience with fuzzy modeling in various fields. An open source software system called FisPro has been specifically designed to provide generic tools for interpretable FIS design and learning. It can then be extended with the addition of new contributions. This work presents a global approach to design data-driven FIS that satisfy certain interpretability and accuracy criteria. It includes fuzzy partition generation, rule learning, input space reduction and rule base simplification. The FisPro implementation is discussed and illustrated through several detailed case studies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Fuzzy inference systems (FIS) have been shown to perform well in input-output mapping identification because they satisfy the universal approximation property and can be designed from expertise or data driven methods.</p><p>Furthermore, their inference engine implements approximate reasoning, which provides a good framework for representing and manipulating a wide body of linguistically expressed information, as pointed out in a recent survey <ref type="bibr" target="#b40">[41]</ref>. The implementation is done in the form of a set of potentially understandable IF-THEN rules.</p><p>Nevertheless, these advantages are not sufficient for FIS to be truly useful in modeling real world systems. Indeed, generalization ability and interpretability are fundamental prerequisites for a model to be used. By interpretability, we mean transparency and intelligibility.</p><p>Although no one can question the need for automatic learning from data, data-driven approaches still suffer from many drawbacks, as their performance strictly depends both on the volume and on the quality of the data.</p><p>Even if FIS have the potential ability to express the system behavior in a linguistic way, the use of a fuzzy formalism is not sufficient to ensure the transparency and intelligibility of a fuzzy rule base. As shown in <ref type="bibr" target="#b19">[20]</ref>, several conditions are required for interpretability, concerning the fuzzy partitioning, the consistency and number of rules of the rule base, and the presence of incomplete rules.</p><p>As the interpretability constraints may conflict with the objective of automatic learning methods to minimize numerical error, several works have proposed a tradeoff between interpretability and accuracy <ref type="bibr" target="#b9">[10]</ref>.</p><p>In this paper, our goal is to propose tools that can facilitate a modeling approach using supervised learning and data-driven FIS. The FisPro toolbox that we present is a step in that direction. It stands out among fuzzy software products because of the care taken to ensure the interpretability of the fuzzy systems that are automatically learnt from data, at all steps of the design.</p><p>The original contribution of the paper is that it provides guidelines for interpretable FIS learning based on practical experience with fuzzy modeling, acquired during several years of work in this field. The modeling was supported by the FisPro software.</p><p>The structure of the paper is as follows.</p><p>First, in Section 2, we investigate the learning steps required for interpretable FIS design: the objectives and evaluation criteria, fuzzy partitioning methods, and rule induction techniques. Special attention is paid to variable granularity and selection.</p><p>Then, in Section 3, we present the FisPro open source software features that are particularly useful for learning. Section 4 is dedicated to three case studies, each illustrating some particular points of our approach. Finally, we offer some concluding remarks and topics for further discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Learning steps for interpretable FIS</head><p>Since it was first introduced by Takagi and Sugeno <ref type="bibr" target="#b35">[36]</ref>, FIS design using data-driven approaches has been a favorite topic of fuzzy logic researchers. Data mining methods are available in large quantities, either originating from the field of statistics or inspired by artificial intelligence learning techniques, and they are easily transferable to FIS with a few changes to adapt them to the particular structure of FIS.</p><p>Following Sugeno's early work, significant efforts have been dedicated to the development of data-driven FIS design, focusing on numerical accuracy but weakening the key innovation and competitive advantage of fuzzy logic. An interpretability-oriented review of fuzzy inference system design from data can be found in <ref type="bibr" target="#b35">[36]</ref>. It analyzes the main methods of automatic rule generation and structure optimization from that perspective.</p><p>At the beginning of the 21st century, a new research trend appeared of a return to genuine principles of fuzzy logic. Renewed attention was paid to system interpretability while taking advantage of the specific learning capabilities of these methods, see <ref type="bibr" target="#b9">[10]</ref>.</p><p>Even so, not all researchers attribute the same meaning to the concepts of interpretability and transparency for FIS. A detailed study can be found in <ref type="bibr" target="#b32">[33]</ref>.</p><p>Our purpose here is not to provide an in-depth discussion of these concepts. Instead, we shall try to define some general learning principles and steps for an interpretable FIS learning approach.</p><p>Both the fuzzy partition and the rule base are involved in the design of FIS, and three main factors emerge as the primary determinants of interpretability.</p><p>First, all fuzzy partitions should obey some semantic integrity constraints, so that each fuzzy set is associated with a non ambiguous meaning, and in order to obtain a complete coverage of the variable domain ranges.</p><p>Second, the number of rules should be small and the rule base consistent. The curse of dimensionality is a real problem for FIS learning from data, as a full set of complete rules quickly leads to a combinatorial explosion when the number of variables and of fuzzy sets rises. This is annoying as some rules may be generated that cover a very small part of the input space, are matched only by a few examples, and lack generality.</p><p>The third condition is specific to complex systems with a large number of input variables: rules must not systematically include all input variables, but only the important ones in the context of the rule. These rules are often called incomplete rules.</p><p>FIS learning methods are essentially rule generation techniques, and rule generation can be decomposed into two main steps: rule induction and rule base optimization. Variable selection and rule simplification are two important stages of the optimization process.They are usually referred to as structure optimization. Apart from structure optimization, a FIS has many parameters that can also be optimized, such as membership function (MF) parameters and rule conclusions. This is called parameter optimization. A thorough study has been done by various authors <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b17">18]</ref>, and the respective advantages and drawbacks of these methods are well known.</p><p>Many methods perform fuzzy partitioning, rule induction and optimization in the same procedure. Nevertheless, this approach has serious drawbacks.</p><p>It makes it difficult to reuse the fuzzy partitioning for other learning procedures. However, a fuzzy partition can be valuable by itself, as it represents a mapping between numerical values and symbolic concepts in the input space. Furthermore, the use of the same fuzzy partitions in several rule bases makes it easier to compare these bases.</p><p>For similar reasons, it is desirable for the optimization procedure to be distinct from the induction procedure: new data sets can be used for the optimization as they become available, and different criteria can be used depending on the context of use.</p><p>All of this leads us to propose a FIS learning approach decomposed into several steps: fuzzy input (plus fuzzy output if defined) partitioning, rule induction and simplification. Optimization will not be discussed in this work, although FisPro offers parameter optimization procedures based on evolutionist algorithms, which can be applied to any FIS. The FisPro implementation pays attention to preserving semantics, as do some other works. For instance, in <ref type="bibr" target="#b14">[15]</ref>, an index is proposed to maintain the semantic interpretability in a multi-objective evolutionary optimization algorithm. For the reasons given above, we concentrate our presentation on the other points.</p><p>The following sections present each learning step in detail, and the last one focuses on the choice of variable granularity and selection. As a preliminary to presenting the method, we give some learning objectives and evaluation criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Objectives and evaluation criteria</head><p>Machine learning techniques cannot produce results better than what they find in the training data. The task of a supervised learner is to predict a value for any valid input object after having seen a number of training examples (i.e., pairs of inputs and target outputs). To achieve this, the learner has to generalize from the presented data to unseen situations in a reasonable way. FIS partition and rule learning have the same objectives and constraints.</p><p>A reasonable level accuracy must be achieved while avoiding overfitting. Overfitting occurs when a model describes random errors or noise instead of the underlying relationship. Overfitting generally happens when a model is excessively complex, such as having too many degrees of freedom, in relation to the amount of available data.</p><p>A model that has been overfit will generally have poor predictive performance on a data set that is significantly different from the learning set, as it can exaggerate minor fluctuations in the data.</p><p>To limit this risk, some parameters will be made defined for the learning methods; in particular, the learning will often be guided by a minimum rule matching degree. Test sets will be used whenever possible.</p><p>The coverage index and the performance indices, which are different for regression and classification cases, will be used as evaluation indices to assess the prediction capabilities of a FIS for a given dataset.</p><p>Denote by (x i , y i ) the ith row of the data set, where x i is a multidimensional input vector and y i the corresponding output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Coverage index</head><p>Data rows are labeled active or inactive for a given rule base. A row is active if its maximum matching degree over all of the rules is greater than a user defined threshold, and otherwise it is inactive.</p><p>Following this definition, a coverage index value is calculated by applying the formula CI ¼ A N where A is the number of active rows, and N is the file size. The coverage index value is a quality index that is complementary to the classical accuracy index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Performance indices</head><p>In the following, the error index only considers the number of active items defined previously, denoted A.</p><p>For regression cases, the performance index available in FisPro is based on the square root of the mean squared error:</p><formula xml:id="formula_0">RMSE ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 1 A X A i¼1 ð b y i À y i Þ 2 v u u t ;<label>ð1Þ</label></formula><p>where b y i is the inferred value. It allows to compute another error index that is often found in the literature, called the Mean Absolute Error (MAE):</p><formula xml:id="formula_1">MAE ¼ 1 A X A i¼1 j b y i À y i j:<label>ð2Þ</label></formula><p>For classification cases, the error index used in FisPro is the sum of classification errors (MC):</p><formula xml:id="formula_2">MC ¼ X A i¼1 ðerr i jerr i ¼ 1if b C i -C i ; 0 otherwiseÞ;<label>ð3Þ</label></formula><p>where C i is the observed class and b C i the inferred one. Another classification index, not yet available in FisPro, that is differentiable so that it can be used in gradient-based approaches, is the squared classification error:</p><formula xml:id="formula_3">SCE ¼ 1 A 1 K X A i¼1 X K k¼1 a k i À l k i À Á 2 :<label>ð4Þ</label></formula><p>K is the number of classes, and a k i is the activation of the kth class over the ith item, with l k i ¼ 1 if the correct class for the ith item is k, and 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Linguistic variable and fuzzy partitioning</head><p>Variable partitioning is the first step of FIS design. The necessary conditions for fuzzy partitions to be interpretable and to implement the linguistic variable concepts have been studied by several authors <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28]</ref>. The main points are distinguishability, a justifiable number of fuzzy sets, normalization, sufficient overlapping and coverage: each data point, x, should significantly belong (l(x) &gt; , is called the coverage level) to at least one fuzzy set.</p><p>Even if other membership function shapes are available and if the fuzzy partitions can be freely adjusted, the FisPro automatic procedures only generate strong input fuzzy partitions (see Fig. <ref type="figure" target="#fig_0">1</ref> for an example, with semi-trapezoidal shapes at the edges and either triangular or trapezoidal MFs elsewhere).</p><p>The overlap between two neighboring MFs is:</p><p>8x;</p><formula xml:id="formula_4">X c f ¼1</formula><p>l f ðxÞ ¼ 1 and 8f ; 9x such as l f ðxÞ ¼ 1;</p><p>where c is the number of fuzzy sets in the partition and l f (x) is the membership degree of x in the fth fuzzy set.</p><p>Using strong fuzzy partitions ensures semantic integrity, and compared to partitions made up of unbounded membership functions, such as the widely used Gaussian partitions, they also lead to more robust systems because the number of simultaneously fired rules is limited <ref type="bibr" target="#b11">[12]</ref>.</p><p>What should be the number of linguistic terms in the partition? The correct answer is the one required for reasoning with rules. However, in an automatic learning procedure, this number is unknown, so the question becomes: what is the most suitable number of terms according to the data?</p><p>The FisPro approach is to generate a collection of fuzzy partitions of various sizes from two to a user-defined maximum value, e.g., seven, allowing the decision to be made later, based upon indices or an objective function. Three methods are available. The first one generates regular grids without considering the data distribution. Another option is to use the kmeans algorithm with different numbers of groups. The last proposed method is called hfp, which stands for Hierarchical Fuzzy Partitioning. It is described in <ref type="bibr" target="#b21">[22]</ref>. According to the generation method, the partitions are either independent of each other (k-means), or the k À 1 term partition is derived from the k term partition by fuzzy set merging (hfp).</p><p>Several indices have been defined to characterize fuzzy partitions. The following indices are implemented in the GUAJE open source software <ref type="bibr" target="#b0">[1]</ref> described in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. GUAJE is an upgraded version of KBCT (Knowledge Base Configuration Tool).</p><p>Let N be the data set size and l i (k) the membership degree of the ith item in group k; we then have:</p><formula xml:id="formula_5">PC ¼ 1 N X N k¼1 X c i¼1 l 2 i ðkÞ; PE ¼ À 1 N X N k¼1 X c i¼1 ½l i ðkÞlog a ðl i<label>ðkÞÞ</label></formula><formula xml:id="formula_6">( ) ; CI ¼ 1 N X N k¼1 max i l i ðkÞ À 2 cðc À 1Þ X cÀ1 i¼1 X c j¼iþ1 1 N X N k¼1 minðl i ðkÞ; l j ðkÞÞ:</formula><p>The partition coefficient (PC) and the partition entropy (PE) were proposed by Bezdek <ref type="bibr" target="#b5">[6]</ref> in 1981; the Chen index (CI) is more recent <ref type="bibr" target="#b10">[11]</ref>.</p><p>The three indices described above can be applied to any partition, independently of the derivation method. According to these criteria a good partition should minimize the entropy and maximize the partition coefficient and the Chen index.</p><p>As an illustration, consider data distributions for two variables of the auto-mpg and wine data sets, which are described at the beginning of Section 4. The three indices are computed for partitions of 2-7 terms generated from these data by the three available methods. Two 15 class histograms are plotted in Fig. <ref type="figure" target="#fig_1">2</ref>, with the x-axis representing the input values and the y-axis the number of data items. The histogram on the left corresponds to the fifth variable of the auto-mpg data set, and the one on the right to the tenth variable of the wine data set. These two distributions are dissimilar in shape: a quasi Gaussian distribution for the fifth variable, and a multimodal skewed one for the tenth variable.</p><p>The index behavior is also quite different between these two variables. Tables <ref type="table" target="#tab_0">1</ref> and<ref type="table" target="#tab_1">2</ref> show the best partition size for the corresponding variable according to each of the three indices. The index value is given in parentheses following the partition size.</p><p>For the auto-v5 variable, the best partitions according to PC and PE are the k-means ones, while the CI index cannot differentiate between k-means and regular grids. For wine-v10, PC and PE agree that hfp is the best, and CI yields the same values for k-means and hfp. However the hfp partition size is smaller.</p><p>Recall that the indices cannot be compared with each other, the only valid comparison is between the behavior of a given index for several partitions.</p><p>This simple example shows that none of the proposed methods can be considered as uniformly better than the others in all cases. Moreover, even if the indices are useful for characterizing the fuzzy partitions, they are likely to differ in their results. One way to use them is to select one index and to follow its evolution throughout an optimization process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Rule induction</head><p>There are many FIS rule learning methods, and our purpose in the present paper is not to provide a review of them all, but to recall the principles of a few techniques that yield interpretable FIS, which are implemented in FisPro. FIS learning involves the fitting of many parameters, so we find it reasonable to consider only single output systems in the present study. Whatever their complexity and their origin (fuzzy clustering, statistical methods, machine learning or ad hoc data-driven ones specifically designed for fast fuzzy rule learning), rule learning methods may be classified into two broad categories: region based methods and prototype based ones. Indeed, once the input fuzzy partitions have been defined, the naive method of generating the complete set of rules corresponding to all fuzzy set combinations must be definitively rejected because it causes a very fast combinatorial explosion. The learning question then reduces to selecting the relevant rule premises and to assigning them appropriate conclusions.</p><p>In the case of the region based methods, the rule premises are chosen by splitting the input domains into regions and selecting the relevant regions by applying a given criterion to the data set.</p><p>In the case of prototype based methods, the rule premises are initialized from the data by analyzing each row and constructing the corresponding rule. The rule is kept if it satisfies a given criterion, and it is then assigned an appropriate conclusion.</p><p>We now present some methods that are available in FisPro. We discuss their advantages and drawbacks, and the parameters available for reaching our objectives: generalization, robustness, accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">Notations</head><p>A fuzzy rule is defined as:</p><formula xml:id="formula_7">IF x 1 is A 1 r and x 2 is A 2 r . . . and x p is A p r THEN y is C r :</formula><p>The rth rule matching degree for x is calculated as:   m r ðxÞ ¼</p><formula xml:id="formula_8">p j¼1 l A j r ðx j Þ;<label>ð5Þ</label></formula><p>where ^is the conjunction operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">Region based rule learning methods</head><p>Two methods are discussed in this section, first a fast but efficient technique and then a more elaborate one.</p><p>The Fast Prototyping Algorithm (FPA) <ref type="bibr" target="#b16">[17]</ref> consists of generating the rules that, out of all possible combinations of antecedents, satisfy the following criterion: the rule matching degree w is higher than a given threshold l t for more than a given number of data rows. Two strategies are available to compute the conclusion for a particular rule: the first only retains the data rows that most activate the rule, the second one all data rows that match it above l t . The subset so defined is called D r .</p><p>The conclusion C r of the rth rule is computed from the selected data in the subset D r , which are used as prototypes, whence the method name.</p><p>-for classification cases, the conclusion is assigned the majority class, -for regression cases, it is equal to:</p><formula xml:id="formula_9">C r ¼ P i2Dr m r ðx i Þ Ã y i P i2Dr m r ðx i Þ :<label>ð6Þ</label></formula><p>FPA gives a quick summary of the data set in the form of fuzzy rules that can be further tuned through optimization procedures on a test set. It is efficient for large data sets.</p><p>Fuzzy Decision Trees (FDT), which are an extension of classical decision trees <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b30">31]</ref>, constitute a popular elaborate application of region based methods. The FDT proposed in FisPro are based on the algorithm presented in <ref type="bibr" target="#b39">[40]</ref>. The FisPro implementation relies on a predefined fuzzy partition of the input variables, which is left untouched by the tree growing algorithm.</p><p>Starting from a root node including all data set items, the FDT uses a recursive procedure to split each node into M j child nodes, where M j is the number of fuzzy sets in the jth input variable partition selected for the split. For each node, the algorithm selects the variable that maximizes the gain according to a discriminant criterion:</p><formula xml:id="formula_10">G j n ¼ I n À X M j m¼1 w m I n;m ;<label>ð7Þ</label></formula><p>where I n,l is the criterion for the node, created by the split onto the jth variable into M j MFs, and corresponding to the mth MF.</p><p>w m is the relative weight of the mth MF, l m ðx j i Þ the membership of x j i in the mth MF.</p><p>Let l n i ¼ l n ðx i Þ denote the membership of the ith example in the nth node, the child of the (n À 1)th node. It is recursively defined as:</p><formula xml:id="formula_11">l 0 i ¼ 1; l n i ¼ l nÀ1 i ^lm ðx j i Þ:</formula><p>( An interesting feature of FDT is their ability to adapt to different kind of outputs: continuous numerical outputs or discretized outputs that represent classes, by using a suitable splitting criterion.</p><p>-If the output is a class, the absolute gain criterion is based on the definition of the fuzzy entropy for the nth node:</p><formula xml:id="formula_12">I n ¼ À X K k¼1 p k logðp k Þ;<label>ð8Þ</label></formula><p>where k is an output class, K is the total number of classes and p k is the fuzzy standardized kth class ratio at node n. It is calculated as follows.</p><p>Let l k (y i ) denote the membership of the output value y i in the MF associated to the output class k.</p><p>Then p k is given by Eq. ( <ref type="formula" target="#formula_13">9</ref>):</p><formula xml:id="formula_13">p k ¼ P A i¼1 l k ðy i Þ ^ln i P K c¼1 P A i¼1 l c ðy i Þ ^ln i ;<label>ð9Þ</label></formula><p>where A is the number of active rows defined in Section 2.1.1.</p><p>A relative entropy gain criterion is also available to avoid the bias against variables with an unequal distribution of examples (not classes) between the various MFs.</p><p>-If the output is continuous, the criterion is based on the output variance of all examples attracted by the node:</p><formula xml:id="formula_14">I n ¼ X N i¼1 l n i ðy i À y i Þ 2 :<label>ð10Þ</label></formula><p>FDT have several assets. They provide a flexible, compact and interpretable representation. They also determine a sorting order for influential variables, with the most discriminating variables appearing near the root. Therefore, they are useful for selecting the more relevant variables prior to applying another learning technique.</p><p>Currently available parameters include the tree depth, the minimum membership l t for a data row to be considered in the process, and the accuracy loss allowed for pruning a branch. A test set can be used for pruning, which may be different from the set used for tree learning. A tree equivalent FIS is obtained by creating a fuzzy rule for each path leading to a terminal node (leaf). The assigned conclusion is based on the majority class or average value, computed over all examples attracted by the leaf. The tree equivalent FIS rule base is composed of incomplete rules, where only the influential variables appear in each rule premise. Their main drawbacks are their sensitivity to small variations in the data set and the fast growth of the tree size, especially for continuous outputs. Therefore a good practice is to prune the tree by transforming a node into a leaf node, according to an accuracy criterion computed on a test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3.">Prototype based rule learning methods</head><p>In contrast to region based learning methods, where it is easier to guarantee interpretability because of the use of predefined regions, this is not so easy for prototype based learning methods. Many works found in the literature use the Gaussian unbounded MF or assign different MF centers for each data row.</p><p>In FisPro, we constrain the prototype based learning techniques to use a predefined interpretable partitioning to overcome these difficulties. We now introduce two such techniques: the first one was especially designed for fuzzy systems and the second is inherited from statistical methods.</p><p>Wang and Mendel <ref type="bibr" target="#b38">[39]</ref> originally proposed the following procedure, called WM:</p><p>1. Each variable of the input space is automatically divided into a user defined number of triangular membership fuzzy sets. 2. One fuzzy rule is generated for each data row, in the form given in Section 2.3.1.</p><p>The fuzzy sets A j i are those that maximize the matching degree of x j i for each input variable j from the ith row. The fuzzy set C i is the one that maximizes the observed output matching degree. 3. A degree m r is assigned to each rule according to Eq. ( <ref type="formula" target="#formula_8">5</ref>). In case two rules have identical premises, only the one with the higher degree is kept. 4. The output is computed by centroid defuzzification.</p><p>In the FisPro implementation, to comply with our interpretability requirements, the first step is removed and the fuzzy rules are generated for the preexisting fuzzy partitions. Therefore the generated rules are not centered on the data set examples, and WM is no longer a purely prototype based method, as the fuzzy partitions delineate fuzzy regions. However, if we compare it with FPA, the main difference stems from the way the rule conclusions are initialized. With FPA, they are calculated using a subset of examples, whereas WM only considers a single item. The WM procedure allows the rule base to be adaptive: new rules compete with existing ones. One drawback is the rough management of conflicts between rule conclusions, which are resolved by selecting the more representative one in the data set.</p><p>Fuzzy Orthogonal Least squares (OLS) is an advanced example of a prototype based learning method. The technique is inspired by linear regression model fitting. Wang and Mendel <ref type="bibr" target="#b37">[38]</ref> introduced the use of Fuzzy Basis Functions to map the input variables into a new linear space. In their original implementation, OLS are even more prototype oriented than WM, with each data row being used for straightforward rule initialization using a Gaussian MF centered on the corresponding data values. Fig. <ref type="figure" target="#fig_2">3</ref> illustrates a flowchart describing the two pass method used in the original OLS and the modifications introduced in the FisPro implementation to increase interpretability. All details can be found in <ref type="bibr" target="#b11">[12]</ref>.</p><p>The algorithm allows us to use a test set for the second pass that is different from the learning set used for the first pass. Two stopping criteria are available: the amount of explained variance and the number of rules. Indeed a great advantage of fuzzy OLS rule learning is its ranking of the rules in the rule base in decreasing order of explained output variance. A side effect is the easy detection of outliers, by checking whether the rules that come out first are matched by only a few data items. The sensitivity to outliers could be a drawback, if it is not carefully monitored.</p><p>Table <ref type="table" target="#tab_2">3</ref> gives a summary of the various features and parameters for the four methods presented above. The method category and rule nature, complete or incomplete, is specified. The minimum matching degree l is relevant to all methods, except WM, and refers to the minimum rule matching degree for a data item to be significant. It increases the method robustness. The parameter #ro., which is the minimum number of data rows that must match the rule to keep it in the rule base, appears only in FPA. It could easily be introduced into WM, for the removal of rules that are weakly represented within the data set, or into FDT.</p><p>Other specific parameters mentioned previously for each method are indicated in the third parameter related column.</p><p>When the observed output is a continuous value, we have a regression case, denoted R. When the objective is to predict a class label of the observed output, we have a classification case, denoted C.</p><p>The FIS output type and the aptitude to model regression/classification cases are indicated in the last two columns. For all methods except OLS, which is designed for crisp outputs only, the FIS output type is either crisp, corresponding to Sugeno FIS, or fuzzy for Mamdani FIS. Because of the way rule conclusions are assigned, WM is well suited to classification cases. Although classical decision trees are often used for classification purposes, fuzzy ones are of great interest also for modeling regression cases because of their interpolation capabilities. FDT were also found to be good rankers, as shown in <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Variable granularity and selection</head><p>Independently of the rule induction and partitioning methods, the selection of variables and the choice of the appropriate granularity are recurrent questions in FIS design, and more generally in Machine Learning. The goal of this section is not to offer a general and thorough view, which can be found in <ref type="bibr" target="#b22">[23]</ref>, but to highlight original approaches implemented in FisPro to dealing with variable granularity and rule base simplification.</p><p>Recall that FDT includes a selection variable process, as mentioned in Section 2.3.2 and it yields incomplete rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1.">Refinement</head><p>The refinement procedure aims to determine the suitable number of terms for a given variable by exploiting the hierarchies of fuzzy partitions presented in Section 2.2.  The key idea is to introduce as many variables, described by a sufficient number of fuzzy sets, as are necessary to get a good rule base. A good FIS represents a reasonable trade-off between complexity, determined by the number of rules, and accuracy, measured by the performance index, denoted Perf in the algorithms. Algorithm 1. Refinement procedure 1 Initialization: iter = 1, "j n j = 1 2 CALL FIS Generation (Algorithm 2) 3 while iter 6 iter max do 4</p><p>Store system as base system 5</p><p>for The refinement procedure is responsible for the selection of the variables or fuzzy sets to be introduced in the FIS. Let p be the number of input variables, FP n j j the fuzzy partition of the variable j of size n j , where the fuzzy set centers are the coordinates given by the hierarchy, FP n j j ¼ fMF k=n j j ; k ¼ 1; . . . ; n j g, where MF k=n j j refers to the kth membership function of the n jterm fuzzy partition for the jth variable.</p><p>The initial FIS is the simplest one possible with a single rule (Algorithm 1, lines 1-2). The search loop (lines 5-14) builds up temporary fuzzy inference systems, each of which corresponds to adding to the initial FIS one fuzzy set in a given dimension. The dimension to retain is selected in lines 18-19. Following this selection, the FIS to be kept is built up. It will serve as a base to iterate the sequence (lines 3-23).</p><p>Thus, the result of the procedure is not a single FIS but a series FIS 1 , FIS 2 , . . . of increased complexity. When necessary, the procedure calls a FIS generation algorithm, denoted as Algorithm 2, which is now detailed. The rules are generated by combining the fuzzy sets of the FP n j j partitions for j = 1,. . . , p, as described by Algorithm 2. The algorithm then removes the less influential rules and evaluates the rule conclusions. The rule conclusion initialization, line 7, depends on both the rule induction method -either WM or FPA can be used -and on the system output type, regression or classification.</p><p>As noted above, the outcome of the procedure is not a single fuzzy inference system, but K FIS of increasing complexity. The best one is selected based on the performance and the coverage indices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2.">Simplification</head><p>Most of the induction methods, fuzzy decision trees being a noticeable exception, yield bases consisting of rules described by the same set of input variables. We call such bases complete rule bases.</p><p>In these complete rule bases, it is somewhat difficult to give a meaning to the rules. All variables are showing up equally in all rules. If we wish to interpret the rules as interaction rules, it is important to think of a means to privilege the strongest interactions. A good way to do so is to try out a simplification procedure leading to an incomplete rule base, where some variables (one or more) appear in some rules only.</p><p>The elimination of variables in order to obtain incomplete rules could be undertaken at different levels. Many existing methods remove variables from the whole rule base, on the faith of overall indicators which could be misleading. Other techniques remove variables from one rule at a time, not considering any relationship that could exist between the rules. We favor an intermediate selection level, which is an attempt to make up for these difficulties. This intermediate level is chosen as the level of a group of rules with a common context. Our main axis in the simplification procedure focuses on the merging of some rules into a more generic incomplete rule.</p><p>A group of rules is a set of rules whose premises only differ by a single fuzzy set label, corresponding to the same variable v.</p><p>The procedure consists of examining each group of rules to see if it can be replaced by a generic incomplete rule, formed by removing the variable v in each of the premises of the rules that constitute the group. These rules are all identical within the group, so they can be replaced by a single new rule.</p><p>In the illustration of Fig. <ref type="figure" target="#fig_3">4</ref>, the group consists of three rules that only differ by the label of V 2 . The procedure merges them into the R g rule defined only by the variables V 1 and V 3 .</p><p>The merging is guided by the performance index (as defined in Section 2.1.2 for regression and classification cases), but also by a careful examination of the heterogeneity of the output. For regression cases, it is computed as the ratio of the output variance of the N r items matching the rule standardized by the similar computation for the whole data set, according to Eq. ( <ref type="formula" target="#formula_15">11</ref>)</p><formula xml:id="formula_15">H r ¼ r r r :<label>ð11Þ</label></formula><p>The calculations of r r and r are detailed in Eq. ( <ref type="formula" target="#formula_16">12</ref>)</p><formula xml:id="formula_16">r ¼ 1 N X N i¼1 ðy i À y i Þ 2 ; r r ¼ P N r i¼1 w r ðx i Þðy i À y i Þ 2 P N r i¼1 w r ðx i Þ :<label>ð12Þ</label></formula><p>For classification cases the index is computed as a normalized entropy as shown in Eq. ( <ref type="formula" target="#formula_17">13</ref>), where p k is the ratio of examples with class k among those matching the rule</p><formula xml:id="formula_17">H r ¼ À P K k¼1 p k logðp k Þ log K ; p k ¼ P N r i¼1 w r ðx i Þd k ðxÞ P N r i¼1 w r ðx i Þ ;<label>ð13Þ</label></formula><formula xml:id="formula_18">where d k is the characteristic function of class k; d k (x) = 1 if x belongs to class k, 0 otherwise.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Why bother with this index?</head><p>To gain interpretability, we can tolerate a loss of performance. However, we must be cautious about the consequences of the widening of the space potentially covered by the new rule. If the output heterogeneity associated to the new rule is too high, then the replacement should not be made. Nevertheless, some heterogeneity is unavoidable and even desirable. It keeps the rules from being too specific. The simplification procedure is an iterative one. If the rule belongs to at least one group that is to be replaced by a generic rule, the replacement will be made and the original rule will be removed from the base. Indeed, the generic rule covers a wider multidimensional space than the original one.</p><p>Redundancy is likely to occur in the generated rule base, as there is no redundancy control during the generation phase. To overcome this drawback, rules of the base issued from the simplification procedure are tentatively removed one after the other.</p><p>The simplification procedure can be applied to any rule base, whatever the induction method. These three approaches, fuzzy decision trees, refinement and simplification are illustrated with case studies in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Concluding remarks</head><p>Some general points are worth noting when using FIS learning methods. Some regions of the input domains may not be handled by the FIS. For instance, this could happen for regions that are hardly (or not at all) represented in the learning data set. This is the price to pay to improve generalization.</p><p>Regarding the output domain, as the fuzzy inference engine classically interpolates between rules, some output values may be inferred even if they do not correspond to a rule conclusion. This possibility is interesting for modeling continuous phenomena, it but must be avoided in non-ordered classification cases by choosing a suitable defuzzification operator, as illustrated in Section 4.2.</p><p>Learning methods implemented in FisPro yield coherent non-contradictory rules. However, the simplification of the rule base may lead to systems where a rule is included within another. In that case, both rules are considered during the inference process, which does not include any checking mechanism for such a phenomenon. FisPro actually proposes another inference mechanism (see <ref type="bibr" target="#b26">[27]</ref> for details), adapted to fuzzy implicative rules, where Fig. <ref type="figure">5</ref>. Flowchart for learning with FisPro -n is the number of data rows; p is the number of input variables.</p><p>partially redundant rules are handled in a more proper way. The KBCT <ref type="bibr" target="#b1">[2]</ref> software also has an option to eliminate redundant rules from a fuzzy rule base.</p><p>A flowchart is displayed in Fig. <ref type="figure">5</ref> to provide some guidelines for the main choices during FIS generation. The main steps are summarized from top to bottom. If expert knowledge is available, input partitioning can be done by hand; otherwise, the input partitions can be designed entirely from the data, using k-means or hfp.</p><p>The output design depends on the nature of output data: categorical data (classes) or numerical continuous data.</p><p>In the first case, the output nature is crisp, whereas in the second case it can be crisp for Sugeno FIS, or fuzzy for Mamdani FIS. The choice of the appropriate defuzzification operator is described in Fig. <ref type="figure">5</ref>.</p><p>In FisPro, the rule aggregation operator is selected at the output level, with a choice between max and sum (for conjunctive rule systems).</p><p>There is no universal best choice of the rule learning method. However, if the number p of input variables is large, it is recommended to do first a variable selection procedure, for which Fuzzy Decision Trees are particularly suitable. Once p is sufficiently small, the appropriate rule method may be WM for classification problems. For continuous outputs, when the number n of examples is not too high, Fuzzy Decision Trees can be the best choice, as they have a good interpolating capability. When n is large, a statistically inspired method like OLS will do very well to extract the most significant rules and pinpoint the outliers, while FPA will run faster and give an image of the data prototypes.</p><p>Independently of the way the FIS was designed, the rule base can be simplified, as explained in Section 2.4.2. FisPro also includes an optimization module, which allows us to optimize any FIS component with interpretability constraints. The simplification and optimization procedures can be performed using a validation data set.</p><p>Let us briefly consider the complexity of the rule learning methods. If we neglect the number of MFs per variable, the time complexity only depends on the number of rows n in the data set, and the number p of input variables. FPA and WM have a complexity of O(np), FDT has a complexity of O(np!), while OLS has a complexity of O(n 3 ).</p><p>Obviously, FisPro current limits will first be reached for the rule induction using the OLS algorithm, which may also be limited by the available memory because it requires the storage of a square size n matrix. On an Intel Pentium 4 CPU 3.40 GHz with 1 Gigabyte memory, an OLS rule learning procedure takes about 30 s for a data set with 54,000 rows.</p><p>Parallel computing could be used to circumvent the current limits of FisPro. It is already available to speed up inference calculations with an OpenMp compliant implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">FisPro</head><p>Fuzzy software was first developed for the needs of fuzzy control, the popularity of which has been asserted in the 1990s. Industrial as well as academic software became available, and the targeted audience was control engineers, who used fuzzy software as an alternative in the domain of control system design. When elaborate learning methods became more mature and when fuzzy logic expanded to other fields of interest, more general fuzzy software appeared to provide these methods.</p><p>A special session on software for soft computing was organized at the 2007 FuzzIEEE conference <ref type="bibr" target="#b24">[25]</ref>. A comprehensive review of fuzzy software, an interesting discussion of useful features, and a call for building a fuzzy tool kit that supports the take-up of fuzzy systems in business applications can be found in <ref type="bibr" target="#b28">[29]</ref>, which appeared in the proceedings. During the same conference, some advanced software projects were presented, such as FrlDA <ref type="bibr" target="#b6">[7]</ref>, a free intelligent data analysis toolbox, or Xfuzzy <ref type="bibr" target="#b4">[5]</ref>. Xfuzzy is a development environment that integrates a set of tools to help the user through the several stages involved in the process of designing fuzzy logic-based inference systems.</p><p>The FisPro toolbox is an open source toolkit for interpretable FIS design using expert knowledge and data. It stands out among fuzzy software products, because of the interpretability of the fuzzy systems automatically learnt from data, guaranteed in each step of the FIS design, according to the principles detailed in Section 2: variable partitioning, rule induction, optimization. We now present the main features that are useful for FIS learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Sample generation</head><p>Sample files may be generated by random sampling from a data file. There are two possibilities: to generate learning and test pairs or blocks.</p><p>In the first case, each pair includes a sample file and its complement, with a given relative sample file size. In the second case, the procedure splits the data file into K blocks for K-fold cross validation procedures.</p><p>Sampling can be done so as to respect the class proportions in a data file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Fuzzy partitioning and FIS with no rules</head><p>Visualization tools are available to examine partitions and data jointly. An example is given in Fig. <ref type="figure" target="#fig_4">6</ref>. The X axis range is set to the input variable range, while the Y axis ranges from 0 to 1 for the fuzzy partition (bottom part) and from 0 to the maximum number of elements in a class (top part). This visualization is useful to examine the concept significance in relation to actual data.</p><p>FIS with no rules can be generated by all partitioning methods: k-means, regular grids, or hfp hierarchy, and it can be reused in any further rule learning procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">FIS learning</head><p>When a data file is open in FisPro, it becomes available as a learning set. Variable histograms, two and three dimensional plots are proposed to examine the data distribution and the correlations between variables. Data rows can be individually activated or deactivated from the plots or from the data array viewing window.</p><p>Likewise, input variables may be activated or deactivated, and all modifications are automatically passed onto the learning set that will be used for learning.</p><p>Rule learning methods are assigned their parameters through a user-friendly interface, with the possibility to use test files when they are relevant to the method.</p><p>Links between rules and data can be evaluated using a utility function that creates several files. One contains the number of items that activate each rule beyond the l threshold parameter and the cumulative weight of the rule over the data file.</p><p>Another one indicates the rules matched by each data row, and the third one contains a square matrix, whose size is equal to the number of rules, and for which the i, j cell gives the corresponding linkage level, calculated as:</p><formula xml:id="formula_19">L i;j ¼ N i;j N i ;</formula><p>where N i is the size of the subset E i of the items that activate the ith rule, and N i,j is the size of E i \ E j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Viewing results</head><p>After learning, the FIS performance can be calculated on a whole data set, and a summary of coverage and accuracy results is displayed. For regression cases, several plots are available: histograms, X-Y plots with or without a regression line, error plots, and response surfaces. For small data files, an exploratory analysis may be done by moving the mouse over a data point and displaying its row number and the activated rule numbers (beyond a given l).</p><p>For a classification system, the result is presented as a confusion matrix (an example is given in Fig. <ref type="figure" target="#fig_6">9</ref>, Section 4.2). Specific graphical representations are available for FDT, in table or graph form, with relevant information including the number of examples attracted by a leaf, the fuzzy cardinality per leaf, and the entropy or deviance per leaf.</p><p>Examples of specific plots are given in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Case studies</head><p>In this section, the ability of FisPro to design accurate and interpretable FIS is illustrated in three case studies. The two first case studies are well known data sets from the UCI repository <ref type="bibr" target="#b13">[14]</ref>, one is a regression problem, and the other one deals with classification. The data sets are the following: auto-mpg (392 samples): From the StatLib library maintained at Carnegie Mellon University, this case concerns the prediction of city-cycle fuel consumption in miles per gallon from four continuous and three multi-valued discrete variables. wine (178 samples):</p><p>The data set contains samples that are grown in the same region of Italy but derived from three different cultivars. The numbers of instances in each class are: 59, 71 and 48. Each pattern consists of 13 continuous features resulting from chemical analysis.</p><p>The last data set used is related to pesticide losses during spraying.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Regression case</head><p>The first step for FIS design is to build the fuzzy partitions. This is done using the Generate FIS without rules option. The number of terms is set to three for all of the input variables, and the k-means algorithm is used. The min is used for premise combination, the output is crisp, and the defuzzification operator is the Sugeno weighted average, given in Eq. ( <ref type="formula">14</ref>). Let m be the number of distinct rule conclusion values and C r the conclusion of the rth rule. A given x input vector matches the rth rule to a degree denoted w r (x). The cumulative weights are calculated for all distinct values j, and the inferred output is the j value that corresponds to the maximum weight.</p><formula xml:id="formula_20">W j ¼ P r m r ðxÞjC r ¼ j; ŷ ¼ arg maxðW j Þjj ¼ 1; . . . ; m: 8 &lt; :<label>ð15Þ</label></formula><p>WM is used for rule induction. As the input space is relatively large, the number of cells is 3 13 and there are few items in a given cell (subspace). This yields 133 rules. As the number of rules is comparable with the training set size, the rule base cannot claim to be general.</p><p>FPA is an alternative for rule induction. The FisPro default parameters, at least 3 samples that match a rule to a degree higher than 0.3, lead to a huge rule base with 2506 rules. The number of rules could be reduced using the control parameter, but this would give a poor image of the data. Despite the high number of rules, the inference results are not really good: 50 misclassified items with FPA and 15 with WM.</p><p>In such a case, it is better to reduce the input space before rule induction. The refinement procedure is then applied, using WM as the rule induction method.</p><p>Table <ref type="table">4</ref> summarizes the main results. The first variable to be introduced is V12. At this step, there are only two rules in the rule base, and 79 items are misclassified.  As new variables are automatically introduced or refined, the number of error cases decreases. The coverage index, as defined in Section 2.1.1, remains equal to 1 for all of these configurations.</p><p>The final choice results from a trade-off between accuracy and model complexity. The 12 rule FIS, presented in Fig. <ref type="figure">8</ref>, appears to be a good compromise, with a misclassification rate equal to 12/178, i.e. 6.7%. Figs. <ref type="figure">8</ref> and<ref type="figure" target="#fig_6">9</ref> show the selected rule base and the corresponding confusion matrix. Reference results are available in the literature <ref type="bibr" target="#b33">[34]</ref>. We do not present a detailed study here, but, as in the regression case introduced in Section 4.1, cross validation procedures can be applied to sampled data sets to test the robustness of the algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Pesticide loss modeling</head><p>These data are part of an experiment whose goal is to study the influence of micro-meteorological factors on pesticide loss to the air during vine spraying. A thorough description of both the problem statement and the results can be found in <ref type="bibr" target="#b15">[16]</ref>.</p><p>The spraying is achieved using air assisted devices to aid the transport of the droplets toward the target. The modeling objective is to propose a relationship between the proportion of product lost in the atmosphere, given as a percentage of the total volume (%), and some micro-meteorological variables. The following variables are considered: W: Wind speed (m/s). T: Air temperature (°C). DT: Wet bulb temperature depression (°C). z/L: Atmosphere stability parameter.</p><p>The data are difficult to measure, and the sample size is small: 32 experiments. We proceed as follows: first we use the refinement procedure described in Section 2.2 to select the best number of fuzzy sets per input variable, except for the Wind speed, which is a three-term expert designed partition, shown in Fig. <ref type="figure" target="#fig_7">10</ref>. It is in agreement with the Beaufort scale, leading to highly interpretable rules, as each linguistic label corresponds to a Beaufort degree.</p><p>The refinement procedure selects four fuzzy sets for the second and fourth variable, and two for the third one.</p><p>Then, a fuzzy regression decision tree is generated with a minimum l equal to 0.3. It is pruned to allow different temporary accuracy losses: 5% and 30%. The results are shown in Table <ref type="table" target="#tab_5">5</ref>, including the accuracy and the average number of variables per rule, and the six leaf pruned decision tree is displayed in Fig. <ref type="figure" target="#fig_0">11</ref>, with each leaf labeled by the number of attracted data rows and their mean value.</p><p>The tree is very easy to interpret: two rules include only one variable, two others include two variables, and the last two rules include three variables. This kind of system can be used to recommend suitable spraying periods: avoid windy times, but even if the wind velocity is moderate, prefer times when air temperature is high to minimize losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This work has endeavored to present a generic approach to the design of interpretable data-driven FIS for system modeling and its implementation in an open source software called FisPro. <ref type="foot" target="#foot_1">1</ref>The approach addresses several FIS design related topics, including fuzzy partitioning, rule learning, variable selection and rule base simplification.</p><p>It has been illustrated on data sets of varying natures: regression and classification benchmarks, and a short case study in the field of environmental modeling.</p><p>Several real world modeling problems in various fields -food science, image analysis and agriculture -were treated in collaboration with the authors of FisPro. The reader can find more details in the following references <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>The main features of FisPro related to FIS learning were described, as well the available learning methods and the dynamical interface and exploratory analysis tools that allow interactive design.</p><p>In addition to the user-friendly FisPro interface, the learning programs are also available as C++ self contained source code, which allows them to be called from a line command and easily included in batch scripts for cross validation procedures. This functionality, associated with automatic sampling procedures, allows the system to run intensively computational procedures and facilitates the testing of the algorithm's robustness and efficiency.</p><p>Modular and open source, the FisPro software project welcomes contributions from artificial intelligence scientists or engineers. It has already been used for modeling projects in application fields different from the ones cited above and by researchers from different communities, as shown by the recent references appearing in the literature <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b7">8]</ref>. More references are available on the FisPro Web Site. Hopefully the present paper will serve as a reference for users and will help them accomplish their modeling tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of a strong fuzzy partition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Data distribution for v5 from auto-mpg (left) and v10 from wine (right) data sets: v5 is the car weight and v10 is the wine color intensity.</figDesc><graphic coords="5,59.53,54.71,417.40,154.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Flowchart for the modified OLS algorithm implemented in FisPro.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. Group rule merging.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Examples of fuzzy partition and data distribution -auto data set, fourth variable (horsepower).</figDesc><graphic coords="13,133.23,54.71,269.41,333.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Auto-mpg: Inference on test sample#3 with FIS built from training sample #3.</figDesc><graphic coords="14,134.70,395.43,277.97,277.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Wine: The confusion matrix.</figDesc><graphic coords="16,117.69,274.45,311.87,74.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Wind speed fuzzy input partition for pesticide loss data.</figDesc><graphic coords="16,137.54,387.67,269.41,285.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="15,53.86,498.22,431.42,174.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Optimal partition size and index value for auto-v5 according to the indices.</figDesc><table><row><cell>auto-v5</cell><cell>PC</cell><cell>PE</cell><cell>CI</cell></row><row><cell>km</cell><cell>2 (0.80)</cell><cell>2 (0.30)</cell><cell>5 (0.75)</cell></row><row><cell>hfp</cell><cell>7 (0.63)</cell><cell>7 (0.54)</cell><cell>7 (0.70)</cell></row><row><cell>reg</cell><cell>3 (0.69)</cell><cell>3 (0.47)</cell><cell>5 (0.75)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Optimal partition size and index value for wine-v10 according to the indices.</figDesc><table><row><cell>wine-v10</cell><cell>PC</cell><cell>PE</cell><cell>CI</cell></row><row><cell>km</cell><cell>2 (0.82)</cell><cell>2 (0.26)</cell><cell>4 (0.80)</cell></row><row><cell>hfp</cell><cell>2 (0.86)</cell><cell>2 (0.21)</cell><cell>2 (0.80)</cell></row><row><cell>reg</cell><cell>7 (0.67)</cell><cell>7 (0.49)</cell><cell>7 (0.75)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Main features and parameters of some rule learning methods -R denotes regression, and C denotes classification.</figDesc><table><row><cell>Method</cell><cell>Category</cell><cell>Rules</cell><cell>Parameters</cell><cell></cell><cell></cell><cell>Test set</cell><cell>Output</cell><cell>R/C</cell></row><row><cell></cell><cell></cell><cell></cell><cell>l</cell><cell>#ro.</cell><cell>Specific</cell><cell></cell><cell></cell></row><row><cell>FPA</cell><cell>Region</cell><cell>Complete</cell><cell>x</cell><cell>x</cell><cell>Strategy</cell><cell></cell><cell>Crisp/fuzzy</cell><cell>R/C</cell></row><row><cell>FDT</cell><cell>Region</cell><cell>Incomplete</cell><cell>x</cell><cell></cell><cell>Gain</cell><cell>x</cell><cell>Crisp/fuzzy</cell><cell>R/C</cell></row><row><cell>WM</cell><cell>Prototype</cell><cell>Complete</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Crisp/fuzzy</cell><cell>C/R</cell></row><row><cell>OLS</cell><cell>Prototype</cell><cell>Complete</cell><cell>x</cell><cell></cell><cell>Expl. var.</cell><cell>x</cell><cell>Crisp</cell><cell>R</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>#Rules</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>(x k ) is the matching degree of example k with rule r. The condition stated in line 4, where CV t is a given threshold, ensures that the rule is significantly fired by the training set examples.</figDesc><table><row><cell cols="2">Algorithm 2. FIS generation</cell></row><row><cell cols="2">input: fn j jj ¼ 1; . . . ; pg; FP 1 Generate the Q p j¼1 n j rule premises n j j 8j ¼ 1; . . . ; p</cell></row><row><cell cols="2">2 for all Rule r 2 FIS do 3 CV r ¼ P n k¼1 w r ðx k Þ</cell></row><row><cell>4</cell><cell>if CV r &lt; CV t then</cell></row><row><cell>5</cell><cell>remove rule r</cell></row><row><cell>6</cell><cell>else</cell></row><row><cell>7</cell><cell>initialize rule conclusion</cell></row><row><cell>8</cell><cell>end</cell></row><row><cell cols="2">9 end</cell></row><row><cell cols="2">10 Compute Perf</cell></row><row><cell>w r</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>Tree equivalent FIS features for pesticide loss -initial and pruned trees.</figDesc><table><row><cell>#Rules</cell><cell>Tree</cell><cell>Avg. #var. per rule</cell><cell>RMSE</cell></row><row><cell>29</cell><cell>Initial tree</cell><cell>3.4</cell><cell>0.021</cell></row><row><cell>14</cell><cell>First pruned tree</cell><cell>2.4</cell><cell>0.016</cell></row><row><cell>6</cell><cell>Final pruned tree</cell><cell>2.0</cell><cell>0.019</cell></row></table><note><p><p><p>Fig.</p>11</p>. Fuzzy decision tree for pesticide loss data.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>S. Guillaume, B. Charnomordic / Information Sciences 181 (2011) 4409-4427</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>Freely available at http://www.inra.fr/Internet/Departements/MIA/M/fispro.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>m r (x) is defined in Eq. ( <ref type="formula">5</ref>), and R is the number of rules.</p><p>The rules are then induced with the OLS method using a ten-fold cross validation. Ten pairs of training (75%) and test (25%) sets are randomly chosen. For each pair, the rule induction is based on the training set, while the performance evaluation is based on the corresponding test one.</p><p>The number of induced rules is chosen to be equal to 19 in order to ensure a sufficient accuracy. It is measured by the Mean Absolute Error (MAE) defined in Eq. <ref type="bibr" target="#b1">(2)</ref>.</p><p>The average results over the ten test sets yield a MAE of 2.23 with a coverage index of 97 %. The standard deviations of the MAE and coverage index are equal to 0.18 and 1.9, respectively, which show a satisfactory robustness of the learning procedure.</p><p>To illustrate the system's behavior, we use the FIS generated by the third pair, for which the results are close to the average ones. Fig. <ref type="figure">7</ref> shows a screenshot from FisPro. Three examples do not fire any rules, and are set to a user defined default output value of 5, in order to easily identify them. Mouse-over events are enabled to get row number information (44, 54 and 70).</p><p>As said previously, OLS generates complete rules. We do not expect all of the variables to be necessary in all of the rules. The simplification procedure is applied to the FIS induced from sample #3, with the same training data.</p><p>The new rule base is simpler: it has 12 rules instead of 19, and 5 of them are incomplete rules. Nevertheless, the system is still accurate: the new RMSE, measured over the whole data set is 0.0101 instead of 0.0098, i.e. a 3% loss. Readers can refer to <ref type="bibr" target="#b11">[12]</ref> for a compared study of performance results on these data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Classification case</head><p>First a FIS without rules is generated with three fuzzy sets by variable, using a k-means procedure. The output is crisp with a classification flag enabled, meaning that the output is a class discrete value. To avoid rule interpolation the defuzzification operator chosen is a special one called max crisp, defined below. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Guaje: generating understandable and accurate fuzzy models in a java environment</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Alonso</surname></persName>
		</author>
		<ptr target="&lt;http://www.softcomputing.es/guaje&gt;" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Kbct: a knowledge management tool for fuzzy inference systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Magdalena</surname></persName>
		</author>
		<ptr target="&lt;http://www.mat.upm.es/projects/advocate/kbct.htm&gt;" />
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Guaje -a java environment for generating understandable and accurate models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Magdalena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spanish Conference for Fuzzy Logic and Technology</title>
		<imprint>
			<publisher>Spain</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="399" to="404" />
		</imprint>
		<respStmt>
			<orgName>Universidad de Huelva</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hilk: a new methodology for designing highly interpretable linguistic knowledge bases using the fuzzy logic formalism</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Magdalena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guillaume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="761" to="794" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Using xfuzzy environment for the whole design of fuzzy systems</title>
		<author>
			<persName><forename type="first">I</forename><surname>Baturone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Moreno-Velo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Snchez-Solano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Barrios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Jimnez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gersnoviez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Bezdek</surname></persName>
		</author>
		<title level="m">Pattern Recognition with Fuzzy Objective Functions Algorithms</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Plenum Press</publisher>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Gonzáles-Rodrı ´guez, Frida -a free intelligent data analysis toolbox</title>
		<author>
			<persName><forename type="first">C</forename><surname>Borgelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simulation of trust in client-wealth management adviser relationships</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bossomaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Standish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Simulation and Process Modelling</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="40" to="49" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Classification and Regression Trees</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<pubPlace>Belmont, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Wadsworth International Group</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Casillas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Cordón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Magdalena</surname></persName>
		</author>
		<title level="m">Interpretability improvements to find the balance interpretability-accuracy in fuzzy modeling: an overview</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="3" to="22" />
		</imprint>
	</monogr>
	<note>Interpretability Issues in Fuzzy Modeling</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Establishing interpretable fuzzy models from numerical data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th World Congress on Intelligent Control and Automation</title>
		<meeting>the 4th World Congress on Intelligent Control and Automation</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1857" to="1861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Building an interpretable fuzzy rule base from data using orthogonal least squares-application to a depollution problem</title>
		<author>
			<persName><forename type="first">S</forename><surname>Destercke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Charnomordic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fuzzy Sets and Systems</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page" from="2078" to="2094" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Integrating spot-5 time series, crop growth modeling and expert knowledge for monitoring agricultural practices -the case of sugarcane harvest on reunion island</title>
		<author>
			<persName><forename type="first">M</forename><surname>El Hajj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bégué</surname></persName>
		</author>
		<author>
			<persName><surname>Guillaume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing of Environment</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2052" to="2061" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Asuncion</surname></persName>
		</author>
		<title level="m">UCI machine learning repository</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Integration of an index to preserve the semantic interpretability in the multiobjective evolutionary rule selection and tuning of linguistic fuzzy systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Gacto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alcalá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="515" to="531" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Influence of micrometeorological factors on pesticide loss to the air during vine spraying: data analysis with statistical and fuzzy inference models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sinfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Brunet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Palagos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biosystems Engineering</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="184" to="197" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Quelques aspects analytiques des systèmes d&apos;inférence floue</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Glorennec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal Européen des Systèmes automatisés</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="231" to="254" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Algorithmes d&apos;apprentissage pour systèmes d&apos;inférence floue</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Glorennec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Editions Hermès</publisher>
			<pubPlace>Paris</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simulation and optimization software for alcoholic fermentation in winemaking conditions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Goelzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Charnomordic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Colombié</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Fromion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sablayrolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Food Control</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="635" to="642" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Designing fuzzy inference systems from data: an interpretability-oriented review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guillaume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="426" to="443" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fuzzy Inference Systems to Model Sensory Evaluation, Intelligent Sensory Evaluation -Methodologies and Applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Charnomordic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="197" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fuzzy models to deal with sensory data in food industry</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Charnomordic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Donghua University</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="43" to="48" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An introduction to variable and feature selection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1157" to="1182" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Why fuzzy decision trees are good rankers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hüllermeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vanderlooy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1233" to="1244" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">FUZZ-IEEE</title>
		<author>
			<persName><surname>Ieee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Fuzzy Systems</title>
		<meeting><address><addrLine>Imperial College, London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-07">2007. 2007. July 2007</date>
			<biblScope unit="page" from="23" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S R</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mizutani</surname></persName>
		</author>
		<title level="m">Neuro-Fuzzy and Soft Computing</title>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Practical inference with systems of gradual implicative rules</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Charnomordic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guillaume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="78" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Interpretability constraints for fuzzy information granulation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mencar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Fanelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="4585" to="4618" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Gnu fuzzy</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Nauck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1019" to="1024" />
		</imprint>
	</monogr>
	<note>in: [25</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semantic constraints for membership functions optimization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>De Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man and Cybernetics, Part A</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="128" to="138" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Induction of decision trees</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="81" to="106" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Modeling of interactions among sustainability components of an agro-ecosystem using local knowledge through cognitive mapping and fuzzy inference system</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rajaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1734" to="1744" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Riid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rüstern</surname></persName>
		</author>
		<title level="m">Transparent Fuzzy Systems in Modelling and Control, Interpretability Issues in Fuzzy Modeling, Studies in Fuzziness and Soft Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="452" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning fuzzy classification rules from labeled data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Roubos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Setnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Abonyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="page" from="77" to="93" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Ruspini</surname></persName>
		</author>
		<title level="m">Recent Developments in Fuzzy Clustering</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Pergamon Press</publisher>
			<date type="published" when="1982">1982</date>
			<biblScope unit="page" from="133" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fuzzy identification of systems and its applications to modeling and control</title>
		<author>
			<persName><forename type="first">T</forename><surname>Takagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugeno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on System Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="116" to="132" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Development and validation of fuzzy logic inference to determine optimum rates of n for corn on the basis of field and crop features</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bouroubi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Panneton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vigneault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bélec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Precision Agriculture</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="621" to="635" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fuzzy basis functions, universal approximation, and orthogonal least squares learning</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="807" to="814" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generating fuzzy rules by learning from examples</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1414" to="1427" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fuzzy-id3: a class of methods for automatic knowledge acquisition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Fuzzy Logic and Neural Networks</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="265" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A framework for reasoning with soft information</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Yager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="page" from="1390" to="1406" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
