<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Socially-Aware Self-Supervised Tri-Training for Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-06-07">7 Jun 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
							<email>h.yin1@uq.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Min</forename><surname>Gao</surname></persName>
							<email>gaomin@cqu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Xia</surname></persName>
							<email>x.xia@uq.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
							<email>xiangliang.zhang@kaust.edu.sa</email>
						</author>
						<author>
							<persName><forename type="first">Nguyen</forename><surname>Quoc</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Viet</forename><surname>Hung</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The University of Queensland Brisbane</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">The University of Queensland Brisbane</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Chongqing University Chongqing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">The University of Queensland Brisbane</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">KAUST Thuwal</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Griffith University Gold Coast</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Socially-Aware Self-Supervised Tri-Training for Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-06-07">7 Jun 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2106.03569v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Self-Supervised Learning</term>
					<term>Tri-Training</term>
					<term>Recommender Systems</term>
					<term>Contrastive Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised learning (SSL), which can automatically generate ground-truth samples from raw data, holds vast potential to improve recommender systems. Most existing SSL-based methods perturb the raw data graph with uniform node/edge dropout to generate new data views and then conduct the self-discrimination based contrastive learning over different views to learn generalizable representations. Under this scheme, only a bijective mapping is built between nodes in two different views, which means that the self-supervision signals from other nodes are being neglected. Due to the widely observed homophily in recommender systems, we argue that the supervisory signals from other nodes are also highly likely to benefit the representation learning for recommendation. To capture these signals, a general socially-aware SSL framework that integrates tri-training is proposed in this paper. Technically, our framework first augments the user data views with the user social information. And then under the regime of tri-training for multi-view encoding, the framework builds three graph encoders (one for recommendation) upon the augmented views and iteratively improves each encoder with self-supervision signals from other users, generated by the other two encoders. Since the tri-training operates on the augmented views of the same data sources for self-supervision signals, we name it self-supervised tri-training. Extensive experiments on multiple real-world datasets consistently validate the effectiveness of the self-supervised tritraining framework for improving recommendation. The code is released at https://github.com/Coder-Yu/QRec.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Self-supervised learning (SSL) <ref type="bibr" target="#b16">[17]</ref>, emerging as a novel learning paradigm that does not require human-annotated labels, recently has received considerable attention in a wide range of fields <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b44">45]</ref>. As the basic idea of SSL is to learn with the automatically generated supervisory signals from the raw data, which is an antidote to the problem of data sparsity in recommender systems, SSL holds vast potential to improve recommendation quality. The recent progress in self-supervised graph representation learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b39">40]</ref> has identified an effective training scheme for graph-based tasks. That is, performing stochastic augmentation by perturbing the raw graph with uniform node/edge dropout or random feature shuffling/masking to create supplementary views and then maximizing the agreement between the representations of the same node but learned from different views, which is known as graph contrastive learning <ref type="bibr" target="#b39">[40]</ref>. Inspired by its effectiveness, a few studies <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b45">46]</ref> then follow this training scheme and are devoted to transplanting it to recommendation.</p><p>With these research effort, the field of self-supervised recommendation recently has demonstrated some promising results showing that mining supervisory signals from stochastic augmentations is desirable <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b45">46]</ref>. However, in contrast to other graph-based tasks, recommendation is distinct because there is widely observed homophily across users and items <ref type="bibr" target="#b19">[20]</ref>. Most existing SSL-based methods conduct the self-discrimination based contrastive learning over the augmented views to learn generalizable representations against the variance in the raw data. Under this scheme, a bijective mapping is built between nodes in two different views, and a given node can just exploit information from itself in another view. Meanwhile, the other nodes are regarded as the negatives that are pushed apart from the given node in the latent space. Obviously, a number of nodes are false negatives which are similar to the given node due to the homophily, and can actually benefit representation learning in the scenario of recommendation if they are recognized as the positives. Conversely, roughly classifying them into the negatives could lead to a performance drop.</p><p>To tackle this issue, a socially-aware SSL framework which combines the tri-training <ref type="bibr" target="#b46">[47]</ref> (multi-view co-training) with SSL is proposed in this paper. For supplementary views that can capture the homophily among users, we resort to social relations which can be another data source that implicitly reflects users' preferences <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref>. Owing to the prevalence of social platforms in the past decade, social relations are now readily accessible in many recommender systems. We exploit the triadic structures in the useruser and user-item interactions to augment two supplementary data views, and socially explain them as profiling users' interests in expanding social circles and sharing desired items to friends, respectively. Given the use-item view which contains users' historical purchases, we have three views that characterize users' preferences from different perspectives and also provide us with a scenario to fuse tri-training and SSL.</p><p>Tri-training <ref type="bibr" target="#b46">[47]</ref> is a popular semi-supervised learning algorithm which exploits unlabeled data using three classifiers. In this work, we employ it to mine self-supervision signals from other users in recommender systems with the multi-view encoding. Technically, we first build three asymmetric graph encoders over the three views, of which two are only for learning user representations and giving pseudo-labels, and another one working on the useritem view also undertakes the task of generating recommendations. Then we dynamically perturb the social network and user-item interaction graph to create an unlabeled example set. Following the regime of tri-training, during each epoch, the encoders over the other two views predict the most probable semantically positive examples in the unlabeled example set for each user in the current view. Then the framework refines the user representations by maximizing the agreement between representations of labeled users in the current view and the example set through the proposed neighbor-discrimination based contrastive learning. As all the encoders iteratively improve in this process, the generated pseudolabels also become more informative, which in turn recursively benefit the encoders again. The recommendation encoder over the user-item view thus becomes stronger in contrast to those only enhanced by the self-discrimination SSL scheme. So far, there is no work that integrates both tri-training and SSL for graph-based recommendation. Since the tri-training operates on the complementary views of the same data sources to learn self-supervision signals, we name it self-supervised tri-training.</p><p>The major contributions of this paper are summarized as follows:</p><p>â€¢ We propose a general socially-aware self-supervised tri-training framework for recommendation. By unifying the recommendation task and the SSL task under this framework, the recommendation performance can achieve significant gains. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Graph Neural Recommendation Models</head><p>Recently, graph neural networks (GNNs) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b33">34]</ref> have gained considerable attention in the field of recommender systems for their effectiveness in solving graph-related recommendation tasks. Particularly, GCN <ref type="bibr" target="#b14">[15]</ref>, as the prevalent formulation of GNNs which is a first-order approximation of spectral graph convolutions, has driven a multitude of graph neural recommendation models like GCMC <ref type="bibr" target="#b1">[2]</ref>, NGCF <ref type="bibr" target="#b27">[28]</ref>, and LightGCN <ref type="bibr" target="#b10">[11]</ref>. The basic idea of these GCN-based models is to exploit the high-order neighbors in the user-item graph by aggregating the embeddings of neighbors to refine the target node's embeddings <ref type="bibr" target="#b32">[33]</ref>. In addition to these general models, GNNs also empower other recommendation methods working on specific graphs such as SR-GNN <ref type="bibr" target="#b31">[32]</ref> and DHCN <ref type="bibr" target="#b34">[35]</ref> over the session-based graph, and DiffNet <ref type="bibr" target="#b30">[31]</ref> and MHCN <ref type="bibr" target="#b43">[44]</ref> over the social network. It is worth mentioning that GNNs are often used for social computing as the information spreading in social networks can be well captured by the message passing in GNNs <ref type="bibr" target="#b30">[31]</ref>. That is the reason why we resort to social networks for self-supervisory signals generated by graph neural encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Self-Supervised Learning in RS</head><p>Self-supervised learning <ref type="bibr" target="#b16">[17]</ref> (SSL) is an emerging paradigm to learn with the automatically generated ground-truth samples from the raw data. It was firstly used in visual representation learning and language modeling <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b44">45]</ref> for model pretraining. The recent progress in SSL seeks to harness this flexible learning paradigm for graph representation learning <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. SSL models over graphs mainly mine self-supervision signals by exploiting the graph structure. The dominant regime of this line of research is graph contrastive learning which contrasts multiple views of the same graph where the incongruent views are built by conducting stochastic augmentations on the raw graph <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b39">40]</ref>. The common types of stochastic augmentations include but are not limited to uniform node/edge dropout, random feature/attribute shuffling, and subgraph sampling using random walk.</p><p>Inspired by the success of graph contrastive learning, there have been some recent works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b45">46]</ref> which transplant the same idea to the scenario of recommendation. Zhou et al. <ref type="bibr" target="#b45">[46]</ref> devise auxiliary self-supervised objectives by randomly masking attributes of items and skipping items and subsequences of a given sequence for pretraining sequential recommendation model. Yao et al. <ref type="bibr" target="#b36">[37]</ref> propose a two-tower DNN architecture with uniform feature masking and dropout for self-supervised item recommendation. Ma et al. <ref type="bibr" target="#b18">[19]</ref> mine extra signals for supervision by looking at the longer-term future and reconstruct the future sequence for self-supervision, which adopts feature masking in essence. Wu et al. <ref type="bibr" target="#b28">[29]</ref> summarize all the stochastic augmentations on graphs and unify them into a general self-supervised graph learning framework for recommendation. Besides, there are also some studies <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b43">44]</ref> refining user representations with mutual information maximization among a set of certain members (e.g. ad hoc groups) for self-supervised recommendation. However, these methods are used for specific situations and cannot be easily generalized to other scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED FRAMEWORK</head><p>In this section, we present our SElf-suPervised Tri-training framework, called SEPT, with the goal of mining self-supervision signals from other users by the multi-view encoding. The overview of SEPT is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>.  ğ‘¹ âˆˆ R ğ‘šÃ—ğ‘› is the binary matrix with entries only 0 and 1 that represent user-item interactions in G ğ‘Ÿ . For each entry (ğ‘¢, ğ‘–) in ğ‘¹, if user ğ‘¢ has consumed/clicked item ğ‘–, ğ‘Ÿ ğ‘¢ğ‘– = 1, otherwise ğ‘Ÿ ğ‘¢ğ‘– = 0. As for the social relations, we use ğ‘º âˆˆ R ğ‘šÃ—ğ‘š to denote the social adjacency matrix which is binary and symmetric because we work on undirected social networks with bidirectional relations. We use ğ‘· âˆˆ R ğ‘šÃ—ğ‘‘ and ğ‘¸ âˆˆ R ğ‘›Ã—ğ‘‘ to denote the learned final user and item embeddings for recommendation, respectively. To facilitate the reading, in this paper, matrices appear in bold capital letters and vectors appear in bold lower letters. <ref type="bibr" target="#b46">[47]</ref> is a popular semi-supervised learning algorithm which develops from the co-training paradigm <ref type="bibr" target="#b2">[3]</ref> and tackles the problem of determining how to label the unlabeled examples to improve the classifiers. In contrast to the standard co-training algorithm which ideally requires two sufficient, redundant and conditionally independent views of the data samples to build two different classifiers, tri-training is easily applied by lifting the restrictions on training sets. It does not assume sufficient redundancy among the data attributes, and initializes three diverse classifiers upon three different data views generated via bootstrap sampling <ref type="bibr" target="#b5">[6]</ref>. Then, in the labeling process of tri-training, for any classifier, an unlabeled example can be labeled for it as long as the other two classifiers agree on the labeling of this example. The generated pseudo-label is then used as the ground-truth to train the corresponding classifier in the next round of labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Tri-Training. Tri-training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Augmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">View Augmentation.</head><p>As has been discussed, there is widely observed homophily in recommender systems. Namely, users and items have many similar counterparts. To capture the homophily for self-supervision, we exploit the user social relations for data augmentation as the social network is often known as a reflection of homophily <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b38">39]</ref> (i.e., users who have similar preferences are more likely to become connected in the social network and vice versa). Since many service providers such as Yelp<ref type="foot" target="#foot_0">1</ref> encourage users to interact with others on their platforms, it provides their recommender systems with opportunities to leverage abundant social relations. However, as social relations are inherently noisy <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b42">43]</ref>,</p><p>for accurate supplementary supervisory information, SEPT only utilizes the reliable social relations by exploiting the ubiquitous triadic closure <ref type="bibr" target="#b12">[13]</ref> among users. In a socially-ware recommender system, by aligning the user-item interaction graph G ğ‘Ÿ and the social network G ğ‘  , we can readily get two types of triangles: three users socially connected with each other (e.g. ğ‘¢ 1 , ğ‘¢ 2 and ğ‘¢ 4 in Fig. <ref type="figure" target="#fig_0">1</ref>) and two socially connected users with the same purchased item (e.g. ğ‘¢ 1 , ğ‘¢ 2 and ğ‘– 1 in Fig. <ref type="figure" target="#fig_0">1</ref>). The former is socially explained as profiling users' interests in expanding social circles, and the latter is characterizing users' interests in sharing desired items with their friends. It is straightforward to regard the triangles as strengthened ties because if two persons in real life have mutual friends or common interests, they are more likely to have a close relationship.</p><p>Following our previous work <ref type="bibr" target="#b43">[44]</ref>, the mentioned two types of triangles can be efficiently extracted in the form of matrix multiplication. Let ğ‘¨ ğ‘“ âˆˆ R ğ‘šÃ—ğ‘š and ğ‘¨ ğ‘  âˆˆ R ğ‘šÃ—ğ‘š denote the adjacency matrices of the users involved in these two types of triangular relations. They can be calculated by:</p><formula xml:id="formula_0">ğ‘¨ ğ‘“ = (ğ‘ºğ‘º) âŠ™ ğ‘º, ğ‘¨ ğ‘  = (ğ‘¹ğ‘¹ âŠ¤ ) âŠ™ ğ‘º.<label>(1)</label></formula><p>The multiplication ğ‘ºğ‘º (ğ‘¹ğ‘¹ âŠ¤ ) accumulates the paths connecting two user via shared friends (items), and the Hadamard product âŠ™ğ‘º makes these paths into triangles. Since both ğ‘º and ğ‘¹ are sparse matrices, the calculation is not time-consuming. The operation âŠ™ğ‘º ensures that the relations in ğ‘¨ ğ‘“ and ğ‘¨ ğ‘  are subsets of the relations in ğ‘º. As ğ‘¨ ğ‘“ and ğ‘¨ ğ‘  are not binary matrices, Eq. ( <ref type="formula" target="#formula_0">1</ref>) can be seen a special case of bootstrap sampling on ğ‘º with the complementary information from ğ‘¹. Given ğ‘¨ ğ‘“ and ğ‘¨ ğ‘  as the augmentation of ğ‘º and ğ‘¹, we have three views that characterize users' preferences from different perspectives and also provide us with a scenario to fuse tri-training and SSL. To facilitate the understanding, we name the view over the user-item interaction graph preference view, the view over the triangular social relations friend view, and another one sharing view, which are represented by ğ‘¹, ğ‘¨ ğ‘“ , and ğ‘¨ ğ‘  , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Unlabeled Example Set.</head><p>To conduct tri-training, an unlabeled example set is required. We follow existing works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b39">40]</ref> to perturb the raw graph with edge dropout at a certain probability ğœŒ to create a corrupted graph from where the learned user presentations are used as the unlabeled examples. This process can be formulated as:</p><formula xml:id="formula_1">G = (N ğ‘Ÿ âˆª N ğ‘  , ğ’ âŠ™ (E ğ‘Ÿ âˆª E ğ‘  )),<label>(2)</label></formula><p>where N ğ‘Ÿ and N ğ‘  are nodes, E ğ‘Ÿ and E ğ‘  are edges in G ğ‘Ÿ and G ğ‘  , and ğ’ âˆˆ {0, 1} | E ğ‘Ÿ âˆªE ğ‘  | is the masking vector to drop edges. Herein we perturb both G ğ‘Ÿ and G ğ‘  instead of G ğ‘Ÿ only, because the social information is included in the aforementioned two augmented views. For integrated self-supervision signals, perturbing the joint graph is necessary.</p><p>3.3 SEPT: Self-Supervised Tri-Training</p><formula xml:id="formula_2">3.3.1 Architecture.</formula><p>With the augmented views and the unlabeled example set, we follow the setting of tri-training to build three encoders. Architecturally, the proposed self-supervised training framework can be model-agnostic so as to boost a multitude of graph neural recommendation models. But for a concrete framework which can be easily followed, we adopt LightGCN <ref type="bibr" target="#b10">[11]</ref> as the basic structure of the encoders due to its simplicity. The general form of encoders is defined as follows:</p><formula xml:id="formula_3">ğ’ = ğ» (ğ‘¬, V),<label>(3)</label></formula><p>where ğ» is the encoder, ğ’ âˆˆ R ğ‘šÃ—ğ‘‘ or R (ğ‘š+ğ‘›)Ã—ğ‘‘ denotes the final representation of nodes, ğ‘¬ of the same size denotes the initial node embeddings which are the bottom shared by the three encoders, and V âˆˆ {ğ‘¹, ğ‘¨ ğ‘  , ğ‘¨ ğ‘“ } is any of the three views. It should be noted that, unlike the vanilla tri-training, SEPT is asymmetric. The two encoders ğ» ğ‘“ and ğ» ğ‘  that work on the friend view and sharing view are only in charge of learning user representations through graph convolution and giving pseudo-labels, while the encoder ğ» ğ‘Ÿ working on the preference view also undertakes the task of generating recommendations and thus learns both user and item representations (shown in Fig. <ref type="figure" target="#fig_0">1</ref>). Let ğ» ğ‘Ÿ be the dominant encoder (recommendation model), and ğ» ğ‘“ and ğ» ğ‘  be the auxiliary encoders.</p><p>Theoretically, given a concrete ğ» ğ‘Ÿ like LightGCN <ref type="bibr" target="#b10">[11]</ref>, there should be the optimal structures of ğ» ğ‘“ and ğ» ğ‘  . However, exploring the optimal structures of the auxiliary encoders is out of the scope of this paper. For simplicity, we assign the same structure to ğ» ğ‘“ and ğ» ğ‘  . Besides, to learn representations of the unlabeled examples from the perturbed graph G, another encoder is required, but it is only for graph convolution. All the encoders share the bottom embeddings ğ‘¬ and are built over different views with the LightGCN structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Constructing</head><p>Self-Supervision Signals. By performing graph convolution over the three views, the encoders learn three groups of user representations. As each view reflects a different aspect of the user preference, it is natural to seek supervisory information from the other two views to improve the encoder of the current view. Given a user, we predict its semantically positive examples in the unlabeled example set using the user representations from the other two views. Taking user ğ‘¢ in the preference view as an instance, the labeling is formulated as:</p><formula xml:id="formula_4">ğ’™ ğ‘  ğ‘¢ = Zğ’› ğ‘  ğ‘¢ , ğ’™ ğ‘“ ğ‘¢ = Zğ’› ğ‘“ ğ‘¢ , ğ’š ğ‘  ğ‘¢+ = ğ‘†ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥ (ğœ™ (ğ’™ ğ‘  ğ‘¢ )), ğ’š ğ‘“ ğ‘¢+ = ğ‘†ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥ (ğœ™ (ğ’™ ğ‘“ ğ‘¢ )),<label>(4)</label></formula><p>where ğœ™ is the cosine operation, ğ’› ğ‘  ğ‘¢ and ğ’› ğ‘“ ğ‘¢ are the representations of user ğ‘¢ learned by ğ» ğ‘  and ğ» ğ‘“ , respectively, Z is the representations of users in the unlabeled example set obtained through graph convolution, and ğ’š ğ‘  ğ‘¢+ and ğ’š ğ‘“ ğ‘¢+ denote the predicted probability of each user being the semantically positive example of user ğ‘¢ in the corresponding views.</p><p>Under the scheme of tri-training, to avoid noisy examples, only if both ğ» ğ‘  and ğ» ğ‘“ agree on the labeling of a user being the positive sample, and then the user can be labeled for ğ» ğ‘Ÿ . We obey this rule and add up the predicted probabilities from the two views and obtain:</p><formula xml:id="formula_5">ğ’š ğ‘Ÿ ğ‘¢+ = 1 2 (ğ’š ğ‘  ğ‘¢+ + ğ’š ğ‘“ ğ‘¢+ ).<label>(5)</label></formula><p>With the probabilities, we can select ğ¾ positive samples with the highest confidence. This process can be formulated as:</p><formula xml:id="formula_6">P ğ‘Ÿ ğ‘¢+ = { Zğ‘˜ | ğ‘˜ âˆˆ Top-ğ¾ (ğ’š ğ‘Ÿ ğ‘¢+ ), Z âˆ¼ G}.<label>(6)</label></formula><p>In each iteration, G is reconstructed with the random edge dropout for varying user representations. SEPT dynamically generates positive pseudo-labels over this data augmentation for each user in every view. Then these labels are used as the supervisory signals to refine the shared bottom representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Contrastive Learning.</head><p>Having the generated pseudo-labels, we develop the neighbor-discrimination contrastive learning method to fulfill self-supervision in SEPT. Given a certain user, we encourage the consistency between his node representation and the labeled user representations from P ğ‘¢+ , and minimizing the agreement between his representation and the unlabeled user representations. The idea of the neighbordiscrimination is that, given a certain user in the current view, the positive pseudo-labels semantically represent his neighbors or potential neighbors in the other two views, then we should also bring these positive pairs together in the current view due to the homophily across different views. And this can be achieved through the neighbor-discrimination contrastive learning. Formally, we follow the previous studies <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29]</ref> to adopt InfoNCE <ref type="bibr" target="#b11">[12]</ref>, which is effective in mutual information estimation, as our learning objective to maximize the agreement between positive pairs and minimize that of negative pairs:</p><formula xml:id="formula_7">L ğ‘ ğ‘ ğ‘™ = âˆ’E âˆ‘ï¸ ğ‘£ âˆˆ {ğ‘Ÿ,ğ‘ ,ğ‘“ } log ğ‘ âˆˆ P ğ‘£ ğ‘¢+ ğœ“ (ğ‘§ ğ‘£ ğ‘¢ , zğ‘ ) ğ‘ âˆˆ P ğ‘£ ğ‘¢+ ğœ“ (ğ‘§ ğ‘£ ğ‘¢ , zğ‘ ) + ğ‘— âˆˆğ‘ˆ /P ğ‘£ ğ‘¢+ ğœ“ (ğ‘§ ğ‘£ ğ‘¢ , zğ‘— ) (7) where ğœ“ (ğ‘§ ğ‘£ ğ‘¢ , zğ‘ ) = exp ğœ™ (ğ‘§ ğ‘£ ğ‘¢ â€¢ zğ‘ )/ğœ , ğœ™ (â€¢) : R ğ‘‘ Ã— R ğ‘‘ â†¦ âˆ’â†’ R</formula><p>is the discriminator function that takes two vectors as the input and then scores the agreement between them, and ğœ is the temperature to amplify the effect of discrimination (ğœ = 0.1 is the best in our implementation). We simply implement the discriminator by applying the cosine operation. Compared with the selfdiscrimination, the neighbor-discrimination leverages the supervisory signals from the other users. When only one positive example is used and if the user itself in Z has the highest confidence in ğ’š ğ‘¢+ , the neighbor-discrimination degenerates to the self-discrimination. So, the self-discrimination can be seen as a special case of the neighbor-discrimination. But when a sufficient number of positive examples are used, these two methods could also be simultaneously adopted because the user itself in Z is often highly likely to be in the Top-K similar examples P ğ‘¢+ . With the training proceeding, the encoders iteratively improve to generate evolving pseudo-labels, which in turn recursively benefit the encoders again.</p><p>Compared with the vanilla tri-training, it is worth noting that in SEPT, we do not add the pseudo-labels into the adjacency matrices for subsequent graph convolution during training. Instead, we adopt a soft and flexible way to guide the user representations via mutual information maximization, which is distinct from the vanilla tri-training that adds the pseudo-labels to the training set for nextround training. The benefits of this modeling are two-fold. Firstly, adding pseudo-labels leads to reconstruction of the adjacency matrices after each iteration, which is time-consuming; secondly, the pseudo-labels generated at the early stage might not be informative; repeatedly using them would mislead the framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Optimization.</head><p>The learning of SEPT consists of two tasks: recommendation and the neighbor-discrimination based contrastive learning. Let L ğ‘Ÿ be the BPR pairwise loss function <ref type="bibr" target="#b23">[24]</ref> which is defined as:</p><formula xml:id="formula_8">L ğ‘Ÿ = âˆ‘ï¸ ğ‘– âˆˆI (ğ‘¢),ğ‘—âˆ‰I (ğ‘¢) âˆ’ log ğœ ( rğ‘¢ğ‘– âˆ’ rğ‘¢ ğ‘— ) + ğœ†âˆ¥ğ‘¬ âˆ¥ 2 2 ,<label>(8)</label></formula><p>where I (ğ‘¢) is the item set that user ğ‘¢ has interacted with, rğ‘¢ğ‘– = ğ‘· âŠ¤ ğ‘¢ ğ‘¸ ğ‘– , ğ‘· and ğ‘¸ are obtained by splitting ğ’ ğ‘Ÿ , and ğœ† is the coefficient controlling the ğ¿ 2 regularization. The training of SEPT proceeds in two stages: initialization and joint learning. To start with, we warm up the framework with the recommendation task by optimizing L ğ‘Ÿ . Once trained with L ğ‘Ÿ , the shared bottom ğ‘¬ has gained far stronger representations than randomly initialized embeddings. The selfsupervised tri-training then proceeds as described in Eq. ( <ref type="formula" target="#formula_4">4</ref>) - <ref type="bibr" target="#b6">(7)</ref>, acting as an auxiliary task which is unified into a joint learning objective to enhance the performance of the recommendation task. The overall objective of the joint learning is defined as:</p><formula xml:id="formula_9">L = L ğ‘Ÿ + ğ›½L ğ‘ ğ‘ ğ‘™ ,<label>(9)</label></formula><p>where ğ›½ is a hyper-parameter used to control the magnitude of the self-supervised tri-training. The overall process of SEPT is presented in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussions</head><p>3.4.1 Connection with Social Regularization. Social recommendation <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref> integrates social relations into recommender systems to address the data sparsity issue. A common idea of social recommendation is to regularize user representations by minimizing the euclidean distance between socially connected users, which is termed social regularization <ref type="bibr" target="#b17">[18]</ref>. Although the proposed SEPT also leverages socially-aware supervisory signals to refine user representations, it is distinct from the social regularization. The differences are also two-fold. Firstly, the social regularization is a static process which is always performed on the socially connected users, whereas the neighbor-discrimination is dynamic and iteratively improves the supervisory signals imposed on uncertain users; secondly, negative social relations (dislike) cannot be readily retrieved in social recommendation, and hence the social regularization can only keep socially connected users close. But SEPT can also pushes users who are not semantically positive in the three views apart. Prec@10 Rec@10 NDCG@10 Prec@10 Rec@10 NDCG@10 Prec@10 Rec@10 NDCG@10 previous research <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref> to leave out ratings less than 4 in the dataset of Douban-Book which consists of explicit ratings with a 1-5 rating scale, and assign 1 to the rest. The statistics of the datasets is shown in Table <ref type="table" target="#tab_2">1</ref>. For precise assessment, 5-fold cross-validation is conducted in all the experiments and the average results are presented.</p><p>Baselines. Three recent graph neural recommendation models are compared with SEPT to test the effectiveness of the self-supervised tri-training for recommendation:</p><p>â€¢ LightGCN <ref type="bibr" target="#b10">[11]</ref> is a GCN-based general recommendation model that leverages the user-item proximity to learn node representations and generate recommendations, which is reported as the state-of-the-art. â€¢ DiffNet++ <ref type="bibr" target="#b29">[30]</ref> is a recent GCN-based social recommendation method that models the recursive dynamic social diffusion in both the user and item spaces. â€¢ MHCN <ref type="bibr" target="#b43">[44]</ref> is a latest hypergraph convolutional network-based social recommendation method that models the complex correlations among users with hyperedges to improve recommendation performance.</p><p>LightGCN <ref type="bibr" target="#b10">[11]</ref> is the basic encoder in SEPT. Investigating the performance of LightGCN and SEPT is essential. Since LightGCN is a widely acknowledged SOTA baseline reported in many recent papers <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b43">44]</ref>, we do not compare SEPT with other weak baselines such as NGCF <ref type="bibr" target="#b27">[28]</ref>, GCMC <ref type="bibr" target="#b1">[2]</ref>, and BPR <ref type="bibr" target="#b23">[24]</ref>. Two strong social recommendation models are also compared to SEPT to verify that the self-supervised tri-training, rather than the use of social relations, is the main driving force of the performance improvements. Metrics. To evaluate all the methods, we first perform item ranking on all the candidate items. Then two relevancy-based metrics Precision@10 and Recall@10 and one ranking-based metric NDCG@10 are calculated on the truncated recommendation lists, and the values are presented in percentage.</p><p>Settings. For a fair comparison, we refer to the best parameter settings reported in the original papers of the baselines and then fine tune all the hyperparameters of the baselines to ensure the best performance of them. As for the general settings of all the methods, we empirically set the dimension of latent factors (embeddings) to 50, the regularization parameter ğœ† to 0.001, and the batch size to 2000. In section 4.4, we investigate the parameter sensitivity of SEPT, and the best parameters are used in section 4.2 and 4.3. We use Adam to optimize all these models with an initial learning rate 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Overall Performance Comparison</head><p>In this part, we validate if SEPT can improve recommendation. The performance comparisons are shown in Table <ref type="table" target="#tab_3">2 and 3</ref>. We conduct experiments with different layer numbers in Table <ref type="table" target="#tab_3">2</ref>. In Table <ref type="table">3</ref>, a two-layer setting is adopted for all the methods because they all reach their best performance on the used datasets under this setting. The performance improvement (drop) marked by â†‘ (â†“) is calculated by using the performance difference to divide the subtrahend. According to the results, we can draw the following observations and conclusions:</p><p>â€¢ Under all the different layer settings, SEPT can significantly boost LightGCN. Particularly, on the sparser datasets: Douban-Book and Yelp, the improvements get higher. The maximum improvement can even reach 11%. This can be an evidence that demonstrates the effectiveness of self-supervised learning. Besides, although both LightGCN and SEPT suffer the over-smoothed problem when the layer number is 3, SEPT can still outperform Light-GCN. We think the possible reason is that contrastive learning can, to some degree, alleviate the over-smooth problem because the dynamically generated unlabeled examples provide sufficient data variance.</p><p>In addition to the comparison with LightGCN, we also compare SEPT with social recommendation models to validate if the selfsupervised tri-training rather than social relations primarily promote the recommendation performance. Since MHCN is also built upon LightGCN, comparing these two models can be more informative. Besides, ğ‘† 2 -MHCN, which is the self-supervised variant of MHCN is also compared. The improvements (drops) are calculated by comparing the results of SEPT and ğ‘† 2 -MHCN. According to the results in Table <ref type="table">3</ref>, we make the following observations and conclusions:</p><p>â€¢ Although integrating social relations into graph neural models are helpful (comparing MHCN with LightGCN), learning under the scheme of SEPT can achieve more performance gains (comparing SEPT with MHCN). DiffNet++ is uncompetitive compared with the other three methods. Its failure can be attributed to its Table <ref type="table">3</ref>: Performance comparison with social recommendation models on three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Last.fm Douban-Book Yelp Prec@10 Rec@10 NDCG@10 Prec@10 Rec@10 NDCG@10 Prec@10 Rec@10 NDCG@10 DiffNet++ Pr ec @ 10 R ec @ 10 N D C G @ 10  redundant and useless parameters and operations <ref type="bibr" target="#b10">[11]</ref>. On both LastFM and Douban-Book, SEPT outperforms ğ‘† 2 -MHCN. On Yelp, ğ‘† 2 -MHCN exhibits better performance than SEPT does. The superiority of SEPT and ğ‘† 2 -MHCN demonstrates that self-supervised learning holds vast capability for improving recommendation. In addition, SEPT does not need to learn other parameters except the bottom embeddings, whereas there are a number of other parameters that ğ‘† 2 -MHCN needs to learn. Meanwhile, SEPT runs much faster than ğ‘† 2 -MHCN does in our experiments, which makes it more competitive even that it is beaten by ğ‘† 2 -MHCN on Yelp by a small margin.</p><p>4.3 Self-Discrimination v.s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neighbor-Discrimination</head><p>In SEPT, the generated positive examples can include both the user itself and other users in the unlabeled example set. It is not clear which part contributes more to the recommendation performance.</p><p>In this part, we investigate the self-discrimination and the neighbordiscrimination without the user itself being the positive example.</p><p>For convenience, we use SEPT-SD to denote the self-discrimination, and SEPT-ND to denote the latter. It also should be mentioned that, for SEPT-ND only, a small ğ›½ = 0.001 can lead to the best performance on all the datasets. A two-layer setting is used in this case.</p><p>According to Fig. <ref type="figure" target="#fig_2">2</ref>, we can observe that both SEPT-SD and SEPT-ND exhibit better performances than LightGCN does, which proves that both the supervisory signals from the user itself and other users can benefit a self-supervised recommendation model. Our claim about the self-supervision signals from other users is validated.  Besides, the importance of the self-discrimination and the neighbordiscrimination varies from dataset to dataset. On LastFM, they almost contribute equally. On Douban-Book, self-discrimination shows much more importance. On Yelp, neighbor-discrimination is more effective. This phenomenon can be explained by Fig. <ref type="figure">5</ref>.</p><p>With the increase of the used positive examples, we see that the performance of SEPT almost remains stable on LastFM and Yelp but gradually declines on Douban-Book. We guess that there is widely observed homophily in LastFM and Yelp, so a large number of users share similar preferences, which can be the high-quality positive examples in these two datasets. However, users in Douban-Book may have more diverse interests, which results in the quality drop when the number of used positive examples increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">View Study</head><p>In SEPT, we build two augmented views to conduct tri-training for mining supervisory signals. In this part, we ablate the framework to investigate the contribution of each view. A two-layer setting is used in this case. In Fig. <ref type="figure" target="#fig_4">3</ref>, 'Friend' or 'Sharing' means that the corresponding view is detached. When only two views are used, SEPT degenerates to the self-supervised co-training. 'Preference-Only' means that only the preference view is used. In this case, SEPT further degenerates to the self-training. From Fig. <ref type="figure" target="#fig_4">3</ref>, we can observe that on both LastFM and Yelp, all the views contribute, whereas on Douban-Book, the self-supervised co-training setting achieves the best performance. Moreover, when only the preference view is used, SEPT shows lower performance but it is still better than that of LightGCN. With the decrease of used number of views, the performance of SEPT slightly declines on LastFM, and an obvious performance drop is observed on Yelp. On Douban-Book, the performance firstly gets a slight rise and then declines obviously when there is only one view. The results demonstrate that, under the semi-supervised setting, even a single view can generate desirable self-supervised signals, which is encouraging since social relations or other side information are not always accessible in some situations. Besides, increasing the used number of views may bring more performance gains, but it is not absolutely right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Parameter Sensitivity Analysis</head><p>There are three important hyper-parameters used in SEPT: ğ›½ for controlling the magnitude of self-supervised tri-training, ğ¾ -the number of used positive examples and ğœŒ -the edge dropout rate of G. We choose some representative values for them to investigate the parameter sensitivity of SEPT. The results are presented in Fig. <ref type="figure">4 -6</ref>. When investigating the influence of ğ›½, we fix ğ¾ = 10 and ğœŒ = 0.3. For the influence of ğ¾ in Fig. <ref type="figure">5</ref>, we fix ğ›½ = 0.005 on LastFM and Yelp, ğ›½ = 0.02 on Douban-Book, and ğœŒ = 0.3. Finally, for the effect of ğœŒ in Fig. <ref type="figure">6</ref>, the setting of ğ›½ is as the same as the last case, and ğ¾ = 10. A two-layer setting is used in this case. As can be observed from Fig. <ref type="figure">4</ref>, SEPT is sensitive to ğ›½. On different datasets, we need to choose different values of ğ›½ for the best performance. Generally, a small value of ğ›½ can lead to a desirable performance, and a large value of ğ›½ results in a huge performance drop. Figure <ref type="figure">5</ref> has been interpreted in Section 4.3. According to Fig. <ref type="figure">6</ref>, we observe that SEPT is not sensitive to the edge dropout rate. Even a large value of ğœŒ (e.g., 0.8) can create informative selfsupervision signals, which is a good property for the possible wide use of SEPT. When the perturbed graph is highly sparse, it cannot provide useful information for self-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORK</head><p>The self-supervised graph contrastive learning, which is widely used in the field of graph representation learning, recently has been transplanted to recommendation for improving the recommendation performance. However, most SSL-based methods only exploit self-supervision signals through the self-discrimination, and SSL cannot fully exert itself in the scenario of recommendation to leverage the widely observed homophily. To address this issue, in this paper, we propose a socially-aware self-supervised tri-training framework named SEPT to improve recommendation by discovering self-supervision signals from two complementary views of the raw data. Under the self-supervised tri-training scheme, the neighbor-discrimination based contrastive learning method is developed to refine user representations with pseudo-labels from the neighbors. Extensive experiments demonstrate the effectiveness of SEPT, and a thorough ablation study is conducted to verify the rationale of the self-supervised tri-training.</p><p>In this paper, only the self-supervision signals from users are exploited. However, items can also analogously provide informative pseudo-labels for self-supervision. This can be implemented by leveraging the multimodality of items. We leave it as our future work. We also believe that the idea of self-supervised multi-view co-training can be generalized to more scenarios beyond recommendation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the proposed self-supervised tri-training framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparisons between self-discrimination and neighbor-discrimination.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Contributions of each component in SEPT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :Figure 5 :Figure 6 :</head><label>456</label><figDesc>Figure 4: Sensitivity analysis of ğ›½.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>SEPT: Self-Supervised Tri-Training Recommendation User-Item Graph Primary Task Contrastive Learning Auxiliary Task</head><label></label><figDesc>In this paper, we use two graphs as the data sources including the user-item interaction graph G ğ‘Ÿ and the user social network G ğ‘  . U = {ğ‘¢ 1 , ğ‘¢ 2 , ..., ğ‘¢ ğ‘š } (|U| = ğ‘š) denotes the user nodes across both G ğ‘Ÿ and G ğ‘  , and I = {ğ‘– 1 , ğ‘– 2 , ..., ğ‘– ğ‘› } (|I| = ğ‘›) denotes the item nodes in G ğ‘Ÿ . As we focus on item recommendation,</figDesc><table><row><cell>i 1 3.1.1 Notations. Preference View i 2 i 3 i 4 Encoder Sharing View Friend View Social Network u 1 u 2 u 3 u 4 u 5 u 7 u 6 u 1 u 2 u 4 u 3 u 5 u 6 u 7 u 1 u 2 u 4 u 3 u 5 u 6 User (Recommendation Model) embeddings u 7 Encoder Labeling User embeddings Data u 1 u 2 u 4 u 3 embeddings u 5 Augmentation u 7 Encoder Unlabeled Item embeddings (Pseudo-labels) Supervisory Signals u 6 User example set</cell><cell>Joint Learning</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Algorithm 1: The running process of SEPT Input: Bidirectional social relations S, User feedback R 1 , and randomly initialized node embeddings ğ‘¬ ; Output: Recommendation lists 2 Pretraining with L ğ‘Ÿ in Eq. (8); Dataset Statistics</figDesc><table><row><cell cols="2">3 View augmentation with Eq. (1);</cell></row><row><cell cols="2">4 for each iteraction do</cell></row><row><cell>5</cell><cell>Construct G and obtain the unlabeled example set</cell></row><row><cell></cell><cell>through graph convolution;</cell></row><row><cell>6</cell><cell>for each batch do</cell></row><row><cell>7</cell><cell>Randomly select ğ‘ users from Z to be labeled;</cell></row><row><cell>8</cell><cell>for each user ğ‘¢ do</cell></row><row><cell>9</cell><cell>Predict the probabilities of the ğ‘ users being the</cell></row><row><cell></cell><cell>semantically positive examples in different</cell></row><row><cell></cell><cell>views with Eq. (4) -(5);</cell></row><row><cell>10</cell><cell>Obtain Top-K positive examples with Eq. (6);</cell></row><row><cell>11</cell><cell>end</cell></row><row><cell>12</cell><cell>Jointly optimize the overall objective in Eq. (9);</cell></row></table><note>13 end 14 end 3.4.2 Complexity. Architecturally, SEPT can be model-agnostic, and its complexity mainly depends on the structure of the used encoders. In this paper, we present a LightGCN-based architecture. Given O (|ğ‘¹|ğ‘‘) as the time complexity of the recommendation encoder for graph convolution, the total complexity for the graph convolution is less than 3O (|ğ‘¹|ğ‘‘) because ğ‘¨ ğ‘“ and ğ‘¨ ğ‘  are usually sparser than ğ‘¹. The prime cost of the labeling process comes from the Top-K operation in Eq. (6), which usually requires O (ğ‘š log(ğ¾)) by using the max heap. To reduce the cost and speed up training, in each batch for training, only ğ‘ (ğ‘ â‰ª ğ‘š, e.g. 1000) users in a batch are randomly selected and being the unlabeled example set of the pseudo-labels, and this sampling method can also prevent overfitting. The complexity of the neighbor-discrimination based contrastive learning is O (ğ‘ğ‘‘).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance improvements brought by SEPT on the three datasets.</figDesc><table><row><cell>Method</cell><cell>Last.fm</cell><cell>Douban-Book</cell><cell>Yelp</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">http://www.yelp.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">http://files.grouplens.org/datasets/hetrec2011/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://github.com/librahu/HIN-Datasets-for-Recommendation-and-Network-Embedding</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><ref type="bibr" target="#b3">4</ref> https://github.com/Coder-Yu/QRec</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This work was supported by ARC Discovery Project (Grant No. DP190101985) and ARC Training Centre for Information Resilience (Grant No. IC200100022).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15535" to="15545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Graph convolutional matrix completion</title>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02263</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh annual conference on Computational learning theory</title>
				<meeting>the eleventh annual conference on Computational learning theory</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Social boosted recommendation with folded bipartite network embedding</title>
		<author>
			<persName><forename type="first">Hongxu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">An introduction to the bootstrap</title>
		<author>
			<persName><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02216</idno>
		<title level="m">Inductive representation learning on large graphs</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Self-supervised co-training for video representation learning</title>
		<author>
			<persName><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09709</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Kaveh</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hosein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khasahmadi</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05582</idno>
		<title level="m">Contrastive Multi-View Representation Learning on Graphs</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong-Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval</title>
				<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mining triadic closure patterns in social networks</title>
		<author>
			<persName><forename type="first">Hong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on World wide web</title>
				<meeting>the 23rd international conference on World wide web</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="499" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haochen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10141</idno>
		<title level="m">Self-supervised learning on graphs: Deep insights and new direction</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Self-supervised learning: Generative or contrastive</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanjin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08218</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recommender systems with social regularization</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>Michael R Lyu</surname></persName>
		</author>
		<author>
			<persName><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth ACM international conference on Web search and data mining</title>
				<meeting>the fourth ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Disentangled Self-Supervision in Sequential Recommenders</title>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="483" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Birds of a feather: Homophily in social networks</title>
		<author>
			<persName><forename type="first">Lynn</forename><surname>Miller Mcpherson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Smithlovin</surname></persName>
		</author>
		<author>
			<persName><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Sociology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="415" to="444" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph Representation Learning via Graphical Mutual Information Maximization</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minnan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="259" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">BPR: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence</title>
				<meeting>the twenty-fifth conference on uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanhong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hari</forename><surname>Sundaram</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03736</idno>
		<title level="m">GroupIM: A Mutual Information Maximization Framework for Neural Group Recommendation</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Multi-Stage Self-Supervised Learning for Graph Convolutional Networks on Graphs with Few Labels</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.11038</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep Graph Infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>LiÃ²</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural graph collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd international ACM SIGIR conference on Research and development in Information Retrieval</title>
				<meeting>the 42nd international ACM SIGIR conference on Research and development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Self-supervised Graph Learning for Recommendation</title>
		<author>
			<persName><forename type="first">Jiancan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10783</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Le</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peijie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.00844</idno>
		<title level="m">DiffNet++: A Neural Influence and Interest Diffusion Network for Social Recommendation</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Le</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peijie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<idno>CoRR abs/1904.10322</idno>
		<title level="m">A Neural Influence Diffusion Model for Social Recommendation</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Session-based recommendation with graph neural networks</title>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyuan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="346" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Shiwen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.02260</idno>
		<title level="m">Graph Neural Networks in Recommender Systems: A Survey</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Self-Supervised Hypergraph Convolutional Networks for Sessionbased Recommendation</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.06852</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Self-Supervised Reinforcement Learning for Recommender Systems</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Arapakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joemon M</forename><surname>Jose</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05779</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Self-supervised Learning for Deep Models in Recommendations</title>
		<author>
			<persName><forename type="first">Tiansheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">Zhiyuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Tjoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Ettinger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.12865</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dynamic user modeling in social media systems</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Hongzhi Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1" to="44" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adapting to User Interest Drift for POI Recommendation</title>
		<author>
			<persName><forename type="first">Xiaofang</forename><surname>Hongzhi Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><surname>Viet</surname></persName>
		</author>
		<author>
			<persName><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2566" to="2581" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph Contrastive Learning with Augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adaptive Implicit Friends Identification over Heterogeneous Network for Social Recommendation</title>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
				<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="357" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Generating reliable friends via adversarial training to improve social recommendation</title>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongming</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinyong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Data Mining (ICDM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="768" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Enhance Social Recommendation with Adversarial Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02340</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinyong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06448</idno>
		<title level="m">Nguyen Quoc Viet Hung, and Xiangliang Zhang. 2021. Self-Supervised Multi-Channel Hypergraph Convolutional Network for Social Recommendation</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">S4l: Self-supervised semi-supervised learning</title>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1476" to="1485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.07873</idno>
		<title level="m">SË†3-Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Tri-training: Exploiting unlabeled data using three classifiers</title>
		<author>
			<persName><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1529" to="1541" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
