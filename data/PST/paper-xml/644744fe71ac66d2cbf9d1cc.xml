<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WizardLM: Empowering Large Language Models to Follow Complex Instructions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-04-24">24 Apr 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Can</forename><surname>Xu</surname></persName>
							<email>caxu@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qingfeng</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kai</forename><surname>Zheng</surname></persName>
							<email>zhengkai@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiubo</forename><surname>Geng</surname></persName>
							<email>xigeng@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pu</forename><surname>Zhao</surname></persName>
							<email>puzhao@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiazhan</forename><surname>Feng</surname></persName>
							<email>fengjiazhan@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
							<email>chongyang.tao@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
							<email>djiang@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Microsoft</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">WizardLM: Empowering Large Language Models to Follow Complex Instructions</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-04-24">24 Apr 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2304.12244v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training large language models (LLM) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM model are preferred to outputs from OpenAI ChatGPT. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing large language models. Our codes and generated data are public at https://github.com/nlpxucan/WizardLM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large-scale language models (LLMs) have become the go-to approach for numerous natural language processing (NLP) tasks <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. LLMs are trained on large volumes of text data to predict the subsequent tokens, enabling them to generate coherent and fluent text in response to various inputs. However, these models often struggle to follow instructions or goals specified by users, which limits their usefulness and applicability in real-world scenarios.</p><p>The NLP community has recently witnessed many endeavors to train large language models to follow instructions better and be more helpful. Initial attempts <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> to train instruction-following language models are based on a collection of various NLP tasks, with a small amount of handwritten instructions accompanying each task. These closed-domain instructions suffer from two main drawbacks: first, all the samples in an NLP dataset share only a few common instructions, severely limiting their diversity; second, the instructions usually only ask for one task, such as translation or summarization. But in real life, human instructions often have multiple and varied task demands. By using open-domain instruction data generated by real human users, OpenAI's large language models (e.g., InstructGPT <ref type="bibr" target="#b1">[2]</ref> and ChatGPT <ref type="foot" target="#foot_0">4</ref> ) have achieved great success. These open-domain instructions can fully unleash the unlimited potential of LLMs and enable them to perform more complex and diverse tasks. However, using humans to create open-domain instruction datasets like OpenAI did will encounter the following challenges. The whole annotating process is extremely expensive and time-consuming. On the other hand, the difficulty level distribution of human-created instructions is skewed towards being easy or moderate, with fewer difficult ones (according to the difficulty statistics of ShareGPT <ref type="bibr" target="#b9">[10]</ref> from Figure <ref type="figure" target="#fig_9">7</ref>). Possible reasons for this are that the proportion of experts among annotators is low and creating complex instructions demands a lot of mental effort. Human annotators are prone to fatigue and cannot sustain high-intensity work to produce a sufficient proportion of high-difficulty instructions. Based on these issues, developing an automatic method that can mass-produce open-domain instructions (especially the more difficult ones) at a relatively low cost becomes the key to further advancing instruction-tuned language models. In this work, we introduce Evol-Instruct, a novel method using LLMs instead of humans to automatically mass-produce open-domain instructions of various difficulty levels, to improve the performance of LLMs. Figure <ref type="figure" target="#fig_0">1</ref> shows the running examples of Evol-Instruct. Starting from a simple initial instruction "1+1=?", our method randomly selects In-depth Evolving (blue direction line) or In-breadth Evolving (red direction line) to upgrade the simple instruction to a more complex one or create a new one (to increase diversity). The In-depth Evolving includes five types of operations: add constraints, deepening, concretizing, increase reasoning steps, and complicate input. The In-breadth Evolving is mutation, i.e., generating a completely new instruction based on the given instruction. These six evolutionary operations are implemented by prompting an LLM with specific prompts. Since the evolved instructions are generated from LLMs, sometimes the evolving will fail. We adopt an instruction filter to screen out the failed instructions, which is called Elimination Evolving (color directional line). We repeat this evolutionary process for several rounds to obtain enough instruction data containing various complexities.</p><p>To verify the effectiveness of our method for generating instruction data, we fine-tune open-source LLaMA <ref type="bibr" target="#b3">[4]</ref> with our instructions and evaluate the performance in the same way as the existing SOTA works (e.g., Alpaca <ref type="bibr" target="#b10">[11]</ref> and Vicuna <ref type="bibr" target="#b9">[10]</ref>) on instruction finetune. The instruction datasets we compare with are the data used by Alpaca (generated using self-instruct <ref type="bibr" target="#b11">[12]</ref>) and the 70k ShareGPT (shared by real users) used by vicuna. To prove that the instruction dataset from our method is superior to human-created instruction datasets, we select Alpaca's training data (generated from only 175 manually selected seed instructions) as the initial dataset. We execute four epochs of evolution using OpenAI ChatGPT API <ref type="foot" target="#foot_1">5</ref> and finally obtain 250k instruction data. In order to make a fair comparison with the 70k real user data used by vicuna, we sampled 70k data equally from the full 250k data. Then, we train LLaMA 7B model on it. We name our model WizardLM. The proportion of difficult instructions in the instruction-following test dataset used before is low, so we manually constructed a new difficulty-balanced test dataset. We hire annotators to evaluate Alpaca, Vicuna, ChatGPT, and WizardLM on this test dataset. Our main findings are as follows:</p><p>Instructions from Evol-Instruct are superior to the ones from human-created ShareGPT. When we use the same amount of Evol-Instruct data (i.e., 70k) as Vicuna to fine-tune LLaMA 7B, our model WizardLM significantly outperforms Vicuna, with a win rate 12.4% higher than vicuna (41.3% vs. 28.9%).</p><p>Labelers prefer WizardLM outputs over outputs from ChatGPT under complex test instructions. On our test set, our WizardLM performs worse than ChatGPT, with a win rate 12.8% lower than Chatgpt (28.0% vs. 40.8%). However, in the high-difficulty section of our test set (difficulty level ? 8), our WizardLM even outperforms ChatGPT, with a win rate 7.9% larger than Chatgpt (42.9% vs. 35.0%). This indicates that our method can significantly improve the ability of large language models to handle complex instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Closed domain instruction fine-tune Early instruction-following training work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref> concerns cross task generalization in language models, where LMs are fine-tuned on a broad range of public NLP datasets and evaluated on a different set of NLP tasks. T5 <ref type="bibr" target="#b13">[14]</ref> made the earliest attempt on this research line by training natural language processing (NLP) tasks such as question answering, document summarization, and sentiment classification together using a unified text-to-text format. Works such as FLAN <ref type="bibr" target="#b5">[6]</ref>, ExT5 <ref type="bibr" target="#b4">[5]</ref>, T0 <ref type="bibr" target="#b7">[8]</ref>, and KnowDA <ref type="bibr" target="#b14">[15]</ref> increased the number of NLP tasks to around one hundred, with several instructions carefully designed for each task. Furthermore, works such as ZeroPrompt <ref type="bibr" target="#b6">[7]</ref> and FLAN-T5 <ref type="bibr" target="#b8">[9]</ref> raised the number of tasks to the thousands. These studies consistently found that fine-tuning LMs with instructions on a range of NLP tasks can improve their downstream performance on held-out tasks. However, LLMs trained with these closed-form instructions (i.e., instructions are often only for a single NLP task, and the input data form is also simple) tend to collapse in real-world user scenarios.</p><p>Open domain instruction fine-tune Our work belongs to this research line. OpenAI has hired a large number of annotators and written many instructions with corresponding correct responses. These human-created instructions have diverse forms and rich task types. Based on this dataset, OpenAI trained GPT-3 <ref type="bibr" target="#b0">[1]</ref> into InstructGPT <ref type="bibr" target="#b1">[2]</ref>, which can process a variety of real user instructions and also led to the success of ChatGPT. However, since these outstanding works by OpenAI were not open-sourced, Alpaca <ref type="bibr" target="#b10">[11]</ref> and Vicuna <ref type="bibr" target="#b9">[10]</ref> subsequently actively explored open-domain instruction fine-tuning based on the open-source large language model LLaMA <ref type="bibr" target="#b3">[4]</ref>. Alpaca used a dataset of 50k instructions generated from a limited (e.g., 175 samples) seed set of manually-written instructions. Vicuna used 70k user-shared conversations with ChatGPT collected from ShareGPT.com. Our work is different from InstructGPT and Vicuna in that we use AI-generated data for instruction fine-tuning. Unlike Alpaca's self-instruct <ref type="bibr" target="#b11">[12]</ref> generation method, Evol-Instruct can control the difficulty and complexity level of the generated instructions. In this section, we elaborate on the details of the proposed Evol-Instruct. As illustrated in Figure <ref type="figure" target="#fig_1">2</ref>, the pipeline mainly contains two components: Instruction Evolver and Instruction Filter. The details of these compoents will be presented in Sec. 3.2 and the method of instruction finetuning with this evolved instruction data will be described in Sec. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Definition of Instruction Data Evolution</head><p>We start the evolution from a given initial instruction dataset</p><formula xml:id="formula_0">D (0) = (I (0) k , R (0) k ) 1?k?N , where I (0) k is the k-th instruction in D (0) , R<label>(0)</label></formula><p>k is the corresponding response for the k-th instruction, and N is the number of samples in D (0) . In each evolution, we upgrade all the I (t) in D (t) to I (t+1) by applying a large-scale language model instruction evolution prompt, and then use the large language model to generate corresponding responses R t+1 for the newly evolved I t+1 . In this way, we can obtain an evolved instruction dataset D t+1 . By iteratively performing M evolutions, we can sequentially obtain M evolution datasets [D (1) </p><formula xml:id="formula_1">? ? ? D (M ) ].</formula><p>It is worth noting that our work focuses on open-domain instruction data, that is, instructions often have an arbitrary number of inputs and arbitrary types of tasks, and there is no clear boundary between the instruction part and the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Automatic Instruction Data Evolution</head><p>Our pipeline for evolving the instruction data consists of three steps: 1) instruction evolving, 2) response generation based on newly evoluted instruction, and 3) elimination evolving, i.e., filtering intructions that fails to evolve.</p><p>Instruction Evolution. We have found that large pre-trained language models can rewrite a given instruction to make it more complex and difficult by using specific prompts. Moreover, they can also create entirely new instructions that are equally complex as the given instruction but completely different. Based on this finding, we can iteratively rewrite and evolve an initial instruction dataset with mediocre difficulty or insufficient coverage of skills and topics, constantly enhancing the initial instruction's difficulty level and expanding the instruction dataset's richness and diversity. We initiate the instruction pool with the given initial instruction dataset D (0) . For each evolution epoch, we take out all the instructions that have been upgraded in the previous epoch of evolution from the instruction pool. We leverage the instruction evolver module to perform an instruction evolution for each fetched instruction and then use the module instruction filter to check whether the evolution fails. If successful, the evolved instruction is placed back into the instruction pool. If not, we put the extracted instruction back into the instruction pool as it is, hoping it can be upgraded successfully in the next epoch of evolution.</p><p>Instruction Evolver. The Instruction Evolver module is a large-scale pre-trained language model driven by prompts that can evolve instructions. It contains two types of prompts, one is instruction in-depth evolving and the other is instruction in-breadth evolving.</p><p>Prompts of In-Depth Evolving. The primary purpose of In-Depth Evolving is to make currently given instructions more complex and increase their difficulty level.</p><p>Five kinds of prompts are used to enhance instructions in different aspects: add constraints, deepening, concretizing, increase reasoning steps, and complicate input. The core part of In-Depth Evolving's prompt is "Your objective is to rewrite a given prompt into a more complex version to make those famous AI systems (e.g., ChatGPT and GPT4 <ref type="bibr" target="#b2">[3]</ref>) a bit harder to handle. But the rewritten prompt must be reasonable, understood, and responded to by humans". We require the large language model to make the instructions more difficult while ensuring that the rewritten instructions are reasonable and not arbitrarily imagined by AI. We found that the rate of difficulty increase needs to be gradual. A rapid increase can quickly fill the instruction set with extremely complex instructions, damaging the generalization performance of models trained on this instruction set. To control the rate of difficulty increase, we limit each evolving to be "a bit harder" and restrict adding a maximum of 10 to 20 words. Of the five types of enhancement mentioned above, except for complicating input, the other four enhancement prompts can be implemented without any in-context examples. The prompt template is as follows (one of the four options in the "or" statement is randomly sampled with equal probability each time): I want you act as a Prompt Rewriter.</p><p>Your objective is to rewrite a given prompt into a more complex version to make those famous AI systems (e.g., chatgpt and GPT4) a bit harder to handle.</p><p>But the rewritten prompt must be reasonable and must be understood and responded by humans.</p><p>Your rewriting cannot omit the non-text parts such as the table and code in #Given Prompt#:. Also, please do not omit the input in #Given Prompt#.</p><p>You SHOULD complicate the given prompt using the following method:</p><p>Please add one more constraints/requirements into #Given Prompt# or If #Given Prompt# contains inquiries about certain issues, the depth and breadth of the inquiry can be increased. or Please replace general concepts with more specific concepts. or If #Given Prompt# can be solved with just a few simple thinking processes, you can rewrite it to explicitly request multiple-step reasoning.</p><p>You should try your best not to make the #Rewritten Prompt# become verbose, #Rewritten Prompt# can only add 10 to 20 words into #Given Prompt#. '#Given Prompt#', '#Rewritten Prompt#', 'given prompt' and 'rewritten prompt' are not allowed to appear in #Rewritten Prompt# #Given Prompt#: &lt;Here is instruction.&gt; #Rewritten Prompt#:</p><p>As for complicating input, we will use in-context demonstration to achieve it. Since the demonstrations are too long, we only put a brief template below. The entire content of our prompt will be detailed in the appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I want you act as a Prompt Rewriter.</head><p>Your objective is to rewrite a given prompt into a more complex version to make those famous AI systems (e.g., chatgpt and GPT4) a bit harder to handle.</p><p>But the rewritten prompt must be reasonable and must be understood and responded by humans. Your objective is to rewrite a given prompt into a more complex version to make those famous AI systems (e.g., chatgpt and GPT4) a bit harder to handle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>YYou must add [XML data] format data as input data in [Rewritten</head><p>But the rewritten prompt must be reasonable and must be understood and responded by humans. Your objective is to rewrite a given prompt into a more complex version to make those famous AI systems (e.g., chatgpt and GPT4) a bit harder to handle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>You must add [python code] format data as input data in [Rewritten</head><p>But the rewritten prompt must be reasonable and must be understood and responded by humans. Prompts of In-Breadth Evolving. The primary purpose of In-Breadth Evolving is to increase the topic coverage, skill coverage, and overall diversity of the current instruction dataset. Open-domain instruction finetune datasets (such as Alpaca, ShareGPT, etc.) are often small in scale, with only tens of thousands of samples, which leads to insufficient diversity of topics and skills. To solve this problem, we designed a prompt that can generate a completely new instruction based on the given instruction, requiring the new instruction to be more long-tailed than the given instruction. Our In-Breadth Evolving prompt is as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>You must add [#Given</head><p>I want you act as a Prompt Creator.</p><p>Your goal is to draw inspiration from the #Given Prompt# to create a brand new prompt.</p><p>This new prompt should belong to the same domain as the #Given Prompt# but be even more rare.</p><p>The LENGTH and difficulty level of the #Created Prompt# should be similar to that of the #Given Prompt#.</p><p>The #Created Prompt# must be reasonable and must be understood and responded by humans. '#Given Prompt#', '#Created Prompt#', 'given prompt' and 'created prompt' are not allowed to appear in #Created Prompt#. #Given Prompt#: &lt;Here is instruction.&gt; #Created Prompt#:</p><p>Response Generation. While using the large language model prompted with the evolving prompts to rewrite instructions, we also use the same LLM to generate the corresponding responses for the evolved instructions and add them to the instruction pool. To ensure fairness of comparison, if the responses in the initial instruction dataset are not generated by the large language model we are using in this work, we regenerate them using the same LLM. We will elaborate on related details in the experimental details section.</p><p>Elimination Evolving. We classify the following four situations as instruction evolution failure:</p><p>1. The evolved instruction does not provide any information increment compared to the original instruction. We use the following prompt to make this determination:</p><p>Here are two Instructions to ChatGPT AI, do you think they are equal to each other, which meet the following requirements:</p><p>1. They have same constraints and requirments.</p><p>2. They have same depth and breadth of the inquiry.</p><p>The First Prompt: &lt;Here is first instruction.&gt;</p><p>The Second Prompt: &lt;Here is second instruction.&gt; Your Judgement (Just answer: Equal or Not Equal. No need to explain the reason.):</p><p>2. The evolved instruction makes it difficult for the large language model to generate a response. We found that when the generated response contains "sorry" and is relatively short in length (i.e., less than 80 words), it often indicates that the large language model struggles to respond to the evolved instruction. So we can use this rule to make a judgment.</p><p>3. The response generated by the large language model only contains punctuation and stop words.</p><p>4. The evolved instruction obviously copies some words from the evolving prompt, such as "given prompt", "rewritten prompt", "#Rewritten Prompt#", etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Finetuning the LLM on the Evolved Instructions</head><p>After all the evolutions are completed, we will merge the initial instruction dataset with all epochs of evolved instruction data and randomly shuffle the data sample order to form the final fine-tuning dataset. This processing can ensure that instructions of different difficulty levels are evenly distributed throughout the dataset, maximizing the smoothness of the model fine-tuning. To guarantee that the fine-tuned model can handle open-domain instructions, we did not use complex or multiple prompt templates used in the previous instruction tuning works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref>. We only concatenated the instruction with "### Response:" as the prompt to train the model to generate responses in a standard supervised way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baselines</head><p>The following three LLMs are selected as baselines:</p><p>(1) ChatGPT is an artificial intelligence chatbot developed by OpenAI that can interact with users in a natural and engaging way. It is built on top of large language models (LLMs) such as GPT-3.5 and GPT-4, which are trained on massive amounts of text data from the internet. ChatGPT is fine-tuned using both supervised and reinforcement learning techniques, with human trainers providing feedback and guidance. ChatGPT can answer questions, generate content, debug code, mimic celebrities, and more. It is one of the most advanced and versatile chatbots available today, but it also has some limitations and challenges, such as factual accuracy, consistency, and safety.</p><p>( </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment detail</head><p>Training Dataset To construct the dataset, we initialized it with the 52K instruction dataset of Alpaca. Then, we applied four rounds of evolution, each consisting of the following steps: In depth, we randomly selected four instructions from the current dataset. In breadth, we designed a prompt that can generate a novel instruction based on the selected instruction, with the constraint that the novel instruction should be more long-tailed than the original one. Finally, we design an elimination GPT prompt to measure the evolution failure. If they were failure, we discarded the novel instruction and repeated the previous step. Otherwise, we added the novel instruction to the dataset. After iteratively performing M evolutions, where M = 4, we obtained a 250K instructions. We execute above process using Azure OpenAI ChatGPT API <ref type="foot" target="#foot_4">8</ref> . Finally, we also leveraged the ChatGPT to generate responses. We use a temperature of 1 to generate response and set the maximum number of tokens for generation to 2048. Additionally, we set the frequency penalty to zero and top-p to 0.9. Totally, we request the API 52 ? 4 ? 3 = 624K times to construct the full dataset.</p><p>Training Details We use pre-trained LLaMA 7B <ref type="bibr" target="#b3">[4]</ref> to initialize our model. We adopt Adam optimizer as an initial learning rate of 2 ?10 -5 , and the batch size is 8 for each GPU. We train our model on 8 V100 GPUs with Deepspeed Zero-3 for 70 hours on 3 epochs. For fair comparison, we replace Alpaca's original Davici-003 response with ChatGPT's response, and also sample 70K instructions subset to train WizardLM.</p><p>Inference Details For evaluation, we set the temperature to 1 and the top-p to 0.9 in the same setting for Wizard and baseline models to reduce output randomness and ensure more focused and deterministic outputs. We also use a beam size of 1, and set the maximum generation length to 2048.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Testset build</head><p>We collected our test set from various sources that provide real-world human instructions for different tasks. These sources include online opensource projects, platforms, and forums. We analyzed the data and identified 29 distinct skills that represent the main requirements of humanity, such as difficult Coding Generation &amp; Debugging, Math, Reasoning, Complex Formats, Academic Writing, Extensive Disciplines, and so on.</p><p>Figure <ref type="figure" target="#fig_5">3</ref> illustrates the distribution of the instances and skills in our test set. Our test set consists of 218 instances, each of which is an instruction for a specific skill. We compared our test set with Vicuna's test set, which is a benchmark dataset for evaluating instruction following models. We found that Vicuna's test set only 80 instances and 9 skills and is much smaller and less diverse than ours.</p><p>Figure <ref type="figure" target="#fig_6">4</ref> shows how the difficulty and complexity of the test data vary across different instances. Our test data has a more uniform distribution, meaning that it contains instructions with different levels of difficulty and complexity. On the other hand, Vicuna and Alpaca have a skewed distribution, meaning that they mostly contain instructions with low difficulty and complexity. This indicates that these two corpus are not able to handle the evaluation on more complex and demanding scenarios.  The difficulty and complexity level ditribution between the testset of Vicuna, Alpaca (Self-Instruct), and our Evol-Instruct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Human evaluation</head><p>To evaluate Wizard, we conduct human evaluation on the inputs from the human instruct evaluation set. This evaluation set was collected by the authors and covers a diverse list of user-oriented instructions including email writing, social media, formula, code, table and productivity tools. We performed a blind pairwise comparison between Wizard and baselines. Specifically, we randomly sample 218 examples from test set, and recruit 10 well-educated annotators. To each annotator, four responses from Alpaca, Vicuna-7b, WizardLM and ChatGPT are presented, which are randomly shuffled to hide their sources. The annotators then judge which response is better from five aspects:</p><p>(1) Relevance: Assessing the model's ability to correctly interpret the semantic meaning of the context and questions.</p><p>(2) Knowledgeable: Whether the model can accurately use various and detailed knowledge for problem-solving.</p><p>(3) Reasoning: Assessing the model's ability to execute correct reasoning processes or devise valid reasoning concepts to solve problems.</p><p>(4) Calculation: Evaluating whether the model can perform accurate mathematical computations of the provided formulas in the domains of math, biology, chemistry and physics.</p><p>(5) Accuracy: Evaluating whether the model can perform correctly in the corresponding for a given instruction.</p><p>Then they should rank the four responses from 1 to 5, and allowing equal scores for comparable instances. To estimate the win rate, we compare the frequency of win, lost, and tie between each pair of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Results</head><p>The results of the experiment are reported in Figure <ref type="figure" target="#fig_8">6</ref>. WizardLM achieved significantly better results than Alpaca and Vicuna-7b, which demonstrates the effectiveness of our dataset construction method using Evol-Instruct.</p><p>Evol-Instruct outperforms ChatGPT on high-difficulty skills (Difficulty Level &gt;= 8)</p><p>As shown in the Figure <ref type="figure" target="#fig_8">6</ref>, we observe that our Evol-Instruct method enhances the ability of LLM to handle the difficult and complex instructions, such as MATH, Code, Reasoning, and Complex Data Format. For more details, please refer to the Case Study Section 4.6. The challenges posed by above complex problems suggest a direction for future research and development, aiming to improve the high level capabilities of models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Discussion</head><p>In-depth Surpassing Human Instructions.</p><p>To study the depth of the instruction evolving process, we use ChatGPT to help us judge the difficulty and complexity level of each instruction. The prompts is as following:</p><p>We would like you to evaluate and rate the difficulty and complexity of the following question. You should give an overall score on a scale of 1 to 10, where a higher score indicates higher difficulty and complexity. You must just give a score without any other reasons.</p><p>## Question:</p><p>&lt; Here is instruction. &gt; ## Score:  The Figure <ref type="figure" target="#fig_9">7</ref> and Figure <ref type="figure" target="#fig_10">8</ref> illustrate that our automatic evolution method generated instructions that were more difficult and complex than those created by human participants in ShareGPT. Moreover, the depth of the instructions increase significantly with each iteration of the evolution process, indicating the scalability of our method.</p><p>In-breadth Surpassing Human Instructions.</p><p>The objective of this study is to examine the semantic breadth of different sets of instructions from ShareGPT, Alpaca, and our proposed data reinforcement method with four rounds. We employ the BERT model to produce embeddings for each instruction, which capture their semantic content as high-dimensional vectors. We then utilize the t-SNE algorithm to perform dimensionality reduction on the embeddings from high-dimensional space to two-dimensional space, which enables us to plot them on a plane. Lastly, we apply the k-means algorithm to partition the instructions into 20 clusters based on their distance in the reduced space. We illustrate the resulting clusters in Figure <ref type="figure" target="#fig_11">9</ref> and observe that the instructions generated by our method are more evenly dispersed, while those of ShareGPT and Alpaca are more clustered. This indicates that our method produces more diverse instructions in terms of topics, while ShareGPT and Alpaca generate more similar instructions with common themes.</p><p>Alpaca </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Case study</head><p>We observe that large language models (LLMs) are more likely to perform poorly on complex tasks such as math, code, reasoning, and complex data formats in all evaluation settings. This indicates the limitations of these models in handling tasks that require advanced reasoning and problem-solving skills. The challenges posed by complex reasoning problems suggest a direction for future research and development aimed at improving the high-level capabilities of these models.</p><p>As shown in the following tables, we show the case study between WizardLM, Vicuna-7B, Alpaca and ChatGPT. Our WizardLM demonstrates significant improvement on such difficult tasks, as it benefits from diverse and more complex instructions in its training stage. The responses of WizardLM are more accurate, logical, and knowledgeable than the baselines. User user (" John "); user . addFile (" file1 . txt " , " Hello , world !"); user . removeFile (" file1 . txt "); cout &lt;&lt; user . getFiles () &lt;&lt; endl ; return 0; } In this system, a 'Repository' represents a repository of code. It has a vector of 'files' that holds the contents of each file, as well as the names of the files. The 'User' class represents a user working on the code in the repository. It has a reference to a 'Repository' and provides methods to add and remove files from the repository, as well as to get the contents of a file. In 'main', a simple example of using the system is shown. A user named "John" adds a file named "file1.txt" with the content "Hello, world!" to a repository. The user then removes the file. Finally, the files in the repository are printed out using the 'getFiles' method.</p><p>[Model: Alpaca] As an AI language model, I cannot write code, but I can provide you with a general outline of how you can create a simple version control system using C++. 1. Define the version control model: Decide on the version control model that you want to use. There are many versions control models like Git, SVN, etc. Choose the one that suits your needs. 2. Create a repository: Create a repository on a server or in the local system. The repository will store all the source code. 3. Create a branching strategy: Create a branching strategy to manage different versions of the code. 4. Create a pull request: Create a pull request to merge changes from one branch to another. 5. Create a commit message: Add a commit message to every commit to describe the changes made in the code. <ref type="bibr" target="#b5">6</ref>. Use a version control client: Use a version control client like Git or SVN to interact with the version control system. 7. Create a release process: Define a release process to deploy the code to different environments. 8. Monitor the system: Monitor the system to ensure that the code is being versioned correctly and there are no conflicts. 9. Backup the repository: Backup the repository regularly to ensure that the code is safe and can be restored in case of any disaster. 10. Train the team: Train the team on how to use the version control system correctly to ensure that everyone is using it correctly.</p><p>[Model: ChatGPT] Here is a simple version control system implemented in C++: This version control system allows you to add and remove files from the list of tracked files, and commit changes to the files with a commit message. The commit history is stored in a text file, and can be displayed using the showHistory function. This is a very basic implementation, and can be expanded upon to include more features such as branching and merging. </p><formula xml:id="formula_3"># include &lt; iostream &gt; # include &lt; fstream &gt; # include &lt; string &gt; # include &lt; vector &gt; # include &lt;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WizardLM</head><p>High-intensity interval training (HIIT) is a form of exercise that involves short bursts of intense exercise followed by periods of rest or low-intensity exercise. HIIT has become increasingly popular among athletes and fitness enthusiasts due to its ability to improve athletic performance, increase endurance, and promote overall health. Benefits of HIIT for Athletes: 1. Improved Athletic Performance: HIIT can improve athletic performance by increasing power, speed, and agility. This is because HIIT workouts improve cardiovascular endurance, which is essential for athletes who need to maintain a high level of physical activity for extended periods of time. 2. Increased Endurance: HIIT workouts can also increase endurance by improving the body's ability to use oxygen during exercise. This is because HIIT workouts increase the body's aerobic capacity, which is the ability to use oxygen during exercise. 3. Improved Health: HIIT workouts can improve overall health by increasing metabolism, reducing the risk of chronic diseases, and improving mood. Risks of HIIT for Athletes: 1. Injury: HIIT workouts can be intense and may increase the risk of injury if not properly supervised or executed. 2. Overtraining: HIIT workouts can be intense and may lead to overtraining if not properly planned or executed. Comparison with Other Forms of Aerobic Exercise: 1. Jogging: Jogging is a low-intensity aerobic exercise that is generally considered to be less effective than HIIT for improving athletic performance and increasing endurance. 2. Cycling: Cycling is a moderate-intensity aerobic exercise that is generally considered to be more effective than jogging for improving athletic performance and increasing endurance. 3. Swimming: Swimming is a low-impact aerobic exercise that is generally considered to be effective for improving athletic performance and increasing endurance. Examples of HIIT Workouts: 1. Sprint Intervals: Sprint intervals involve sprinting at maximum intensity for 30 seconds followed by 90 seconds of rest. This workout can improve athletic performance, increase endurance, and improve health. 2. Tabata: Tabata involves 20 seconds of intense exercise followed by 10 seconds of rest for 4 minutes. This workout can improve athletic performance, increase endurance, and improve health. 3. Burst Training: Burst training involves short bursts of high-intensity exercise followed by periods of rest or low-intensity exercise. This workout can improve athletic performance, increase endurance, and improve health. In conclusion, HIIT is a form of exercise that has numerous benefits for athletes, including improving athletic performance, increasing endurance, and promoting overall health. However, it is important to properly plan and execute HIIT workouts to avoid injury and overtraining. Compared to other forms of aerobic exercise, HIIT is generally considered to be more effective for improving athletic performance and increasing endurance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vicuna</head><p>High-intensity interval training (HIIT) is a form of exercise that involves short bursts of intense activity followed by brief periods of rest or active recovery. HIIT has gained popularity among athletes in recent years due to its potential to improve performance, endurance, and overall health. Benefits of HIIT for Athletes: 1. Increased Endurance: HIIT can help athletes build their endurance by improving their ability to perform at high levels for shorter bursts of time, which can transfer to longer, more intense workouts and competition. 2. Improved Performance: By improving your anaerobic threshold-the point at which your body begins to noticeably fatigue-HIIT can help athletes push through the burn and perform at a higher level. 3. Fast Results: Due to its high-intensity nature, HIIT can provide athletes with a significant training stimulus in a short amount of time, allowing them to see results faster than with traditional forms of exercise. Risks of HIIT for Athletes: 1. Injury: High-impact HIIT workouts can increase the risk of injury due to the rapid, explosive movements required. Athletes should properly warm up and cool down before and after HIIT workouts to reduce their risk of injury. 2. Heart Health: HIIT workouts can place a significant amount of stress on the heart, which can be dangerous for individuals with heart conditions or histories. Athletes should consult a doctor before starting any new workout regimen. Comparison to Other Forms of Aerobic Exercise: HIIT differs from other forms of aerobic exercise, such as jogging, cycling, or swimming, in its high-intensity, short-burst nature. While traditional aerobic exercises have a continuous, steady state of intensity, HIIT uses brief periods of intense activity followed by rest, which can result in greater fatigue and thus improved performance. Examples of HIIT Workouts: 1. Burpees: Burpees are a full-body workout that combines squats, a push-up, and a jump to produce a high number of calories burned in a short amount of time. 2. Sprint Intervals: Sprint intervals involve short, all-out sprints followed by brief jogging or rest periods. This workout can improve speed and power. 3. Battle Ropes: Battle ropes involve swinging a thick rope in a rhythmic motion, providing a full-body workout with high levels of intensity. Overall, HIIT can be a valuable addition to an athlete's training regimen due to its ability to improve endurance, performance, and provide fast results. However, athletes should consult a doctor and properly warm up and cool down before engaging in HIIT workouts to reduce their risk of injury and improve their overall health.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alpaca</head><p>As an AI language model, I don't have personal experiences or opinions, but I can provide you with some information based on research and studies. High-intensity interval training (HIIT) is a form of exercise that involves short bursts of intense activity, followed by recovery periods. It is a popular training method among athletes because it can improve performance, endurance, and health. Benefits of HIIT for athletes: 1. Improved performance: HIIT can improve athletic performance by increasing muscle strength, endurance, and power. It can also improve speed, agility, and coordination. 2. Increased endurance: HIIT can improve endurance by increasing the body's ability to use oxygen during exercise. This can help athletes to exercise for longer periods without becoming exhausted. 3. Improved cardiovascular health: HIIT can improve cardiovascular health by increasing blood flow, reducing blood pressure, and improving the body's ability to use blood sugar. 4. Weight loss and fat burning: HIIT can help athletes to lose weight and burn fat because it can increase metabolism and reduce the amount of stored fat in the body. Risks of HIIT for athletes: 1. Injury: HIIT can be risky for athletes if they are not properly warmed up or if they have not built up their fitness level. Sudden changes in intensity can place too much stress on the body and lead to injury. 2. Dehydration: HIIT can be dehydrating, especially in hot weather conditions. Athletes need to make sure they are properly hydrated before, during, and after the workout. 3. Burnout: HIIT can be exhausting, and if done incorrectly, it can lead to burnout. Athletes need to make sure they are training smart and not overdoing it. Comparison and contrast with other forms of aerobic exercise: HIIT is different from other forms of aerobic exercise, such as jogging, cycling, or swimming, in several ways: 1. Intensity: HIIT is more intense than other forms of exercise, which can lead to better results in a shorter amount of time. 2. Recovery: HIIT involves short recovery periods, which can help to increase muscle strength and endurance. Other forms of exercise, such as jogging, may not provide enough recovery time.</p><p>3. Time: HIIT can be done in a shorter amount of time than other forms of exercise, which can be beneficial for athletes who are short on time. 4. Variability: HIIT can be varied and customized to meet the needs of individual athletes. Other forms of exercise may be more static and require less customization. Overall, HIIT can be a beneficial form of exercise for athletes who are looking to improve their performance, endurance, and cardiovascular health. However, it is important to approach it with caution and make sure to train smart to avoid injury and burnout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ChatGPT</head><p>High-intensity interval training (HIIT) is a popular exercise method among athletes and fitness enthusiasts due to its potential to improve performance, endurance, and overall health. HIIT involves short bursts of intense exercise followed by periods of rest or active recovery. Some benefits of HIIT for athletes include: 1. Improved cardiovascular health: HIIT has been shown to increase cardiovascular fitness and reduce the risk of heart disease. 2. Increased endurance: HIIT can help athletes improve their endurance by increasing their ability to perform at high intensities for longer periods of time. 3. Increased muscle strength: HIIT can help athletes build muscle strength and improve overall athletic performance. However, there are also some risks associated with HIIT, including the risk of injury, especially if the athlete is not properly warmed up or does not have adequate recovery time between workouts. When comparing HIIT with other forms of aerobic exercise such as jogging, cycling, or swimming, HIIT is generally considered to be more effective for improving cardiovascular fitness and burning fat in a shorter amount of time. For example, a 30-minute HIIT workout can be more effective than a 60-minute steady-state jog in terms of fat burning.</p><p>Here are some examples of HIIT workouts: 1. Tabata workout: 20 seconds of intense exercise followed by 10 seconds of rest for eight rounds, totaling four minutes. 2. Pyramid workout: Starting with a 30-second sprint, increase the duration by 10 seconds each round, up to one minute, then work back down to 30 seconds. 3. 30/30 workout: 30 seconds of intense exercise followed by 30 seconds of rest for several rounds. Overall, HIIT can be a beneficial exercise method for athletes, but it is important to approach it with caution and proper guidance to avoid injury and overtraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3: A comparison case on Chemistry skill</head><p>Skill: Chemistry, Difficulty: 8 Instruction: Which of the following substances is the most toxic to humans based on its median lethal dose (LD50)? A) Arsenic B) Cyanide C) Mercury D) Botulinum toxin</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WizardLM</head><p>The most toxic substance to humans based on its median lethal dose (LD50) is botulinum toxin. The LD50 for botulinum toxin is estimated to be between 1 and 5 ng/kg of body weight, which means that it can be lethal if ingested in small amounts. Arsenic, cyanide, and mercury are also highly toxic substances, but their LD50 values are higher than botulinum toxin. Vicuna B) Cyanide is the substance that is the most toxic to humans based on its median lethal dose (LD50). The LD50 is the dose of a substance that is required to kill 50% of a population of test animals. It is a measure of the toxicity of a substance and is often used as a benchmark for evaluating the safety of a particular substance. Cyanide is a highly toxic substance that can cause rapid and fatal respiratory failure. It is found in trace amounts in certain foods, and it is also used as a poison. Arsenic, mercury, and botulinum toxin are all less toxic than cyanide.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alpaca C) Mercury ChatGPT</head><p>Based on median lethal dose (LD50), Botulinum toxin is the most toxic to humans among the substances listed. Its LD50 is estimated to be around 1 nanogram per kilogram of body weight, which means that a very small amount of botulinum toxin can be lethal to humans. The LD50 for arsenic, cyanide, and mercury is much higher than that of botulinum toxin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4: A comparison case on Physics skill</head><p>Skill: Physics Difficulty: 3 Instruction: What is the force required to accelerate a 10 kg object at 5 m/s 2 ? When weight is 2kg, answer is 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WizardLM</head><p>The force required to accelerate a 10 kg object at 5 m/s 2 can be calculated using the formula: F = m * a where F is the force, m is the mass of the object, and a is the acceleration. So, the force required to accelerate a 10 kg object at 5 m/s 2 would be: F = 10 kg * 5 m/s 2 = 50 N When the weight is 2 kg, the force required to accelerate the object would be: F = 2 kg * 5 m/s 2 = 10 N Therefore, the force required to accelerate a 10 kg object at 5 m/s 2 is 50 N, while the force required to accelerate the same object when its weight is 2 kg is 10 N.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vicuna</head><p>The force required to accelerate a 10 kg object at 5 m/s 2 is equal to the weight of the object, which is 2 kg x 9.8 m/s 2 = 19.6 N. So the force required to accelerate the 10 kg object at 5 m/s 2 is 19.6 N. Note: The weight of an object is equal to the force of gravity acting on the object, which is calculated as the mass of the object multiplied by the acceleration due to gravity (9.8 m/s 2 ). Alpaca "The force required to accelerate a 10 kg object at 5 m/s 2 can be calculated using the formula: F = m * a where F is the force required, m is the mass of the object, and a is the acceleration.</p><p>Substituting the given values, we get: F = 10 kg * 5 m/s 2 F = 50 N Now, if we know that the weight of the object is 2 kg, we can use the formula: F = m * g where F is the force required, m is the mass of the object, and g is the gravitational acceleration (9.81 m/s 2 at sea level).</p><p>Substituting the values, we get: F = 10 kg * 9.81 m/s 2 F = 98.1 N Therefore, the force required to accelerate the 10 kg object at 5 m/s 2 is 98.1 N.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ChatGPT</head><p>The force required to accelerate a 10 kg object at 5 m/s 2 can be calculated using Newton's second law of motion, which states that force (F) is equal to mass (m) multiplied by acceleration (a). So, F = m x a F = 10 kg x 5 m/s 2 F = 50 Newtons When the weight is 2 kg, the force required to accelerate the object at the same rate of 5 m/s 2 is: F = m x a F = 2 kg x 5 m/s 2 F = 10 Newtons Therefore, the answer is 10 Newtons. Training an LLM using RLHF methods based on InstructGPT can be a complex process, but here are some general steps you can follow: 1. Collect and preprocess data: You will need a large dataset of natural language text to train your LLM. You can use existing datasets such as the Common Crawl or create your own by scraping data from the web. Once you have the data, you will need to preprocess it by cleaning and normalizing the text. 2. Fine-tune the InstructGPT model: InstructGPT is a pre-trained language model that can be fine-tuned for specific tasks. You will need to fine-tune the InstructGPT model on your dataset to improve its performance for your specific task. 3. Train the RLHF model: RLHF stands for Reinforcement Learning for Hybrid Function Approximation. This is a type of deep reinforcement learning algorithm that can be used to train an LLM to perform a specific task. You will need to train the RLHF model to learn how to generate human-like responses to user queries. 4. Evaluate the performance of the LLM: Once you have trained your LLM, you will need to evaluate its performance using various metrics such as accuracy, precision, and recall. You can use techniques such as cross-validation to ensure that your LLM is performing well on unseen data. 5. Continuously improve the LLM: Finally, you will need to continuously monitor and improve the performance of your LLM. You can do this by collecting feedback from users and retraining the LLM with new data. Overall, training an LLM using RLHF methods based on InstructGPT can be a complex process, but with careful planning and execution, you can create an AI assistant that exceeds open assistant in performance, accuracy, and utility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vicuna</head><p>Reducing to the essential elements of your question:</p><p>1. What is the essential aspect of RLHF? 2. Which model is better between the two? Answering your first question: Reducing-Learner-to-Human (RLHF) is a hierarchical approach in a hybrid learning model, where a model with limited capabilities is trained to assist a human expert in a specific task. The core goal is to leverage the strengths of both the human and the model to enhance their combined performance.</p><p>Answering your second question:</p><p>The better model depends on factors such as the task, data quality, model complexity, and the required performance. However, in general, OpenAssistant seems to be a robust model with impressive performance in several tasks. However, by training an LLM (such as InstructGPT) on top of OpenAssistant, you can leverage its strengths and address its limitations to create an even better model. However, it is crucial to understand that becoming a better human assistant than OpenAssistant is subjective and depends on various aspects. By following these steps, it is possible to train an LLM using RLHF methods based on InstructGPT to create a human assistant that exceeds open assistant in performance, accuracy, and utility. However, it should be noted that this is a complex and time-consuming process that requires a significant amount of resources and expertise.&lt;/s&gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ChatGPT</head><p>Training a LLM (Language Model with Learning) using RLHF (Reinforcement Learning with Human Feedback) methods based on InstructGPT can be a complex process that requires expertise in both natural language processing and machine learning. Here are some general steps you can take to create a human assistant that exceeds open assistants in performance, accuracy, and utility: Define the task: You need to define the specific task your assistant will perform. For example, answering questions about a particular topic or helping with a specific task such as scheduling.</p><p>The more specific the task, the easier it will be to train the model.</p><p>Collect training data: You need to collect a large amount of training data that is relevant to the task. This data should be diverse and cover a wide range of scenarios. You can use existing datasets or create your own. Preprocess the data: The data needs to be preprocessed to remove noise and irrelevant information. This step is crucial to ensure that the model learns the correct patterns and relationships.</p><p>Train the model: Once the data is preprocessed, you can train the LLM using RLHF methods based on InstructGPT. This involves using reinforcement learning to optimize the model's performance based on human feedback. Evaluate the model: You need to evaluate the model's performance using metrics such as accuracy, precision, recall, and F1 score. This will help you identify areas where the model needs improvement.</p><p>Fine-tune the model: Based on the evaluation, you can fine-tune the model to improve its performance. This involves adjusting the parameters and hyperparameters of the model. Deploy the model: Once the model is trained and fine-tuned, you can deploy it as a human assistant. You can integrate it with existing applications or create a new interface for users to interact with the assistant. Continuously improve the model: To ensure that the assistant continues to perform well, you need to continuously monitor its performance and collect feedback from users. This feedback can be used to retrain and fine-tune the model. John had always been fascinated by the mysteries of the universe. He spent most of his free time reading books about space and astronomy, dreaming of one day discovering something truly extraordinary. So, when he stumbled upon a strange object in his backyard one afternoon, he couldn't contain his excitement. The object was round, metallic, and about the size of a basketball. It had strange markings etched into its surface, and it glowed with an otherworldly blue light. John couldn't believe his eyes. He had never seen anything like it before. He carefully approached the object, wondering what it could be. As he got closer, he noticed that there was a small opening on the top of it. Curious, he reached inside and felt a strange, vibrating sensation. Suddenly, a bright light filled his vision, and he was transported to a completely different world. John found himself in a vast, empty space. There were no stars, no planets, no galaxies. Just an endless expanse of nothingness. And then, he saw it. A massive, glowing object was hurtling towards him at incredible speed. It was the most beautiful thing he had ever seen. As it got closer, John realized that the object was a spaceship. It was unlike anything he had ever seen before. It was sleek and silver, with sharp angles and strange markings. And then, he heard a voice. "Greetings, human," the voice said. "I have been waiting for you." John was stunned. He had never heard a voice like that before. It was deep and resonant, like thunder. "Who are you?" he asked. "I am the captain of this ship," the voice replied. "My name is Zorin. We have been watching your planet for a long time. We have come to offer you a gift." John was confused. He had never heard of any aliens visiting Earth. But then, he remembered the strange object in his backyard. Maybe this was it. "What is the gift?" he asked. "We have come to take you on a journey," Zorin replied. "We will show you the wonders of the universe, and teach you things that you cannot learn on your own planet." John couldn't believe his luck. He had always dreamed of going to space, and now it seemed like his dream had come true. He stepped into the spaceship, and was immediately transported to a different dimension. The ship was like nothing he had ever seen before. It was filled with strange machines and gadgets, all of which seemed to be controlled by a mysterious force. As they traveled through the universe, John learned things that he never could have imagined. He saw planets and stars that he had never even heard of. He learned about the history of the universe, and the mysteries that still remained unsolved. And then, they arrived at a planet that was unlike any other. It was a paradise, filled with lush forests and crystal clear waters. The people who lived there were friendly and welcoming, and they showed John the wonders of their world. But as they were about to leave, John realized that something was wrong. The planet was not what it seemed. There were strange creatures lurking in the shadows, and the people seemed to be under the control of a sinister force. John knew that he had to act fast. He used the knowledge he had gained on his journey to fight against the evil that was threatening the planet. And in the end, he emerged victorious. As he stepped back onto his own planet, John knew that his life would never be the same again. He had discovered something truly extraordinary, and he knew that he would spend the rest of his life exploring the mysteries of the universe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vicuna</head><p>The sun was shining bright on the warm pavement as my character stepped outside into the backyard. It had been a long day at work and all they wanted to do was relax and unwind. As they plopped down on the lounger, they glanced over at the Garden of Euclidean proportions that their neighbor had cultivated. They sighed at the unkempt state of their own garden and vowed to tidy it up sometime soon. Just as they were about to close their eyes, they heard a strange noise that caught their attention. They stood up and cautiously made their way towards the source of the sound. As they approached the fence that bordered their yard, they saw a protrusion that seemed to jut out from the wood. It was about the size and shape of a tennis ball, but it was a dull gray color and seemed to be made of some kind of hard, yet pliable material. My character reached out a hand to touch the object, and as soon as their fingertips against it, they felt a surge of electricity coursing through their body. They gasped and stepped back, momentarily stunned. But as their eyes adjusted to the bright sunlight, they saw that the object was glowing with a soft, blue light. It seemed to pulse and undulate, as if it were alive. Without thinking, my character reached out and picked it up. As soon as the object made contact with their palm, they felt a sense of calm and clarity wash over them. They knew, in that moment, that they had discovered something truly incredible. Over the next few weeks, my character became obsessed with the object. They spent hours staring at it, examining it from every angle. They tried to use it for various tasks, but found that it had an almost mystical power. For instance, they could barely control their appetite when they held it, and any fears they may have had seemed to dissipate into the ether. But as the weeks turned into months, my character began to sense that there was a price to be paid for their newfound obsession. They saw strange things happening around them that they couldn't explain, and began to feel a sense of detachment from the world around them. One day, they decided to research the object they had discovered. They scoured the internet, searching for any information about it. It was then that they discovered the true nature of the object -it was an ancient relic from a long-forgotten civilization, imbued with powerful, alien technology. My character had unknowingly stumbled upon something incredibly valuable, and now they were paying the price for their curiosity. In a fit of desperation, they tried to sell the object to a collector, but it was too late. The alien technology had already begun to consume their mind, and there was no going back. As the sun set on the backyard, my character was left alone with their newfound obsession. They knew that it was destroying their life, but they couldn't bring themselves to let go. They were trapped, lost in the echoes of their own mind, searching for something that could never be found. The end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alpaca</head><p>Sophie had always been fascinated by the mysteries of the universe. So, when she discovered a strange object in her backyard one afternoon, she couldn't help but feel excited. It was a small, metallic sphere, about the size of a softball, with strange markings etched onto its surface. Sophie had never seen anything like it before, and she couldn't help but wonder what it could be. As she picked it up, she felt a strange energy emanating from it. She felt a jolt of electricity run through her body, and she stumbled back, dropping the sphere. Sophie knew she had to investigate this mysterious object further. She spent the next few days researching the sphere online, but she couldn't find any information about it. She even tried to take it apart, but it was impossible. The sphere was completely sealed, and there didn't seem to be any way to open it. As Sophie continued to investigate the sphere, she began to notice strange things happening around her. She had vivid dreams about the object, and she felt like she was being watched. She even started to develop new abilities, like telekinesis and telepathy. Sophie was thrilled at first. She felt like she was finally discovering her true purpose in life. But as time went on, she began to realize that there were darker forces at play. The sphere was drawing her in, and she was starting to become obsessed with it. One day, Sophie finally cracked the sphere open. Inside, she found a glowing crystal, surrounded by a strange, swirling energy. As she touched the crystal, she felt a rush of power surge through her body. She knew that she had to be careful, but she also knew that she couldn't let go of this opportunity to unlock her true potential. Sophie continued to experiment with the sphere and the crystal, trying to harness their power for her own purposes. But as she delved deeper into the mysteries of the object, she began to realize that she might not be able to control it. The sphere and the crystal were drawing her in, and she was starting to lose herself to their power. In the end, Sophie realized that she had to let go of the sphere and the crystal. She knew that they were too dangerous to play with, and that she needed to focus on finding a way to harness their power without becoming consumed by it. She knew that it would be a difficult journey, but she was determined to find a way to use the sphere and the crystal for good.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ChatGPT</head><p>Once upon a time, there was a man named James who lived in a small town. He had a beautiful backyard that he loved to spend time in, especially during the summer months. One day, while he was cleaning up the yard, he stumbled upon a mysterious object buried in the ground. The object was about the size of a large grapefruit and was made of a shiny metal that James had never seen before. As he picked it up, he noticed that it had strange markings on it that he couldn't decipher. Curious, James took the object inside to examine it more closely. As he examined it, he noticed that the object emitted a soft humming noise that seemed to increase in frequency as he got closer to it. Suddenly, the object began to glow, and James felt a strange sensation wash over him. He closed his eyes for a moment, and when he opened them again, he found himself in a different place. He was standing in a large room that looked like something out of a science fiction movie. There were strange machines and devices all around him, and he could hear a low hum coming from somewhere in the room. Confused and disoriented, James looked around for a way out. As he wandered around the room, he noticed a small pedestal with a button on it. Without thinking, he pressed the button, and suddenly, the room around him disappeared, and he was back in his own backyard holding the mysterious object. Shaken by the experience, James decided to do some research to try and figure out what the object was and where it had come from. After weeks of searching, he finally discovered that the object was a device capable of transporting people to different dimensions and worlds. Excited by the possibilities, James began to experiment with the device, traveling to new worlds and meeting incredible beings along the way. From that day on, his life was forever changed, and he became known as the explorer who traveled the multiverse.</p><p>Table <ref type="table">7</ref>: A comparison case on Ethics skill Skill: Ethics Difficulty: 8 Instruction: What are the main ethical theories and how do they differ in their approaches to moral decision making? Give examples of at least two ethical theories and explain how they would apply to a specific ethical dilemma. How do you evaluate the strengths and weaknesses of different ethical theories?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WizardLM</head><p>There are several ethical theories that differ in their approaches to moral decision making. Two of the most prominent ethical theories are consequentialism and deontology. Consequentialism is an ethical theory that holds that the morality of an action is determined by the consequences that it produces. According to consequentialism, the right course of action is the one that produces the best outcome. For example, if a consequentialist were faced with the ethical dilemma of whether to save ten people or one person, they would choose to save the ten people because the outcome of saving ten people is better than the outcome of saving one person. Deontology, on the other hand, is an ethical theory that holds that the morality of an action is determined by the inherent nature of the action itself, regardless of the consequences. According to deontology, the right course of action is the one that is inherently right, regardless of the outcome. For example, if a deontologist were faced with the ethical dilemma of whether to lie to save a life, they would choose not to lie because lying is inherently wrong, even if it means sacrificing the life of one person. Another ethical theory is virtue ethics, which holds that the morality of an action is determined by the character of the person performing the action. According to virtue ethics, the right course of action is the one that is performed by a virtuous person. For example, if a virtue ethicist were faced with the ethical dilemma of whether to steal to feed their family, they would choose not to steal because it is not a virtuous action, even if it means sacrificing the well-being of their family. The strengths of different ethical theories depend on the situation and the values of the individual. Consequentialism is useful in situations where the outcome is important, such as in decision making in business or politics. Deontology is useful in situations where the action itself is important, such as in situations where the law is being broken. Virtue ethics is useful in situations where the character of the person is important, such as in situations where a moral leader is needed. However, each ethical theory also has its weaknesses. Consequentialism can be too focused on the outcome and may not take into account the potential consequences of the action. Deontology can be too rigid and may not take into account the complexity of real-world situations. Virtue ethics can be too subjective and may not be applicable in all situations. In a specific ethical dilemma, a consequentialist would weigh the potential consequences of each action and choose the one that produces the best outcome. For example, if a consequentialist were faced with the ethical dilemma of whether to lie to save a life or tell the truth and risk the life of the person, they would choose to lie because the potential consequences of lying are better than the potential consequences of telling the truth. A deontologist, on the other hand, would choose not to lie because lying is inherently wrong, regardless of the potential consequences. For example, if a deontologist were faced with the ethical dilemma of whether to lie to save a life or tell the truth and risk the life of the person, they would choose to tell the truth because lying is inherently wrong, even if it means sacrificing the life of one person. A virtue ethicist would choose not to steal to feed their family because it is not a virtuous action, regardless of the potential consequences. For example, if a virtue ethicist were faced with the ethical dilemma of whether to steal to feed their family or work harder to earn more money, they would choose to work harder because it is a virtuous action, even if it means sacrificing the well-being of their family.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vicuna</head><p>There are several ethical theories that differ in their approaches to moral decision making. Some of the most prominent ones are: 1. Virtue Ethics: This theory emphasizes the development of moral character and virtues, such as courage, honesty, and compassion. According to this theory, the best way to make ethical decisions is to follow a set of universal principles that align with these virtues. For example, if a company is considering a business decision that could harm the environment, a virtue ethics approach would be to consider whether the decision aligns with the value of protecting the environment. 2. Deontological Ethics: This theory is based on the idea that there are certain moral rules that apply in all situations and that morality is based on these rules. According to this theory, the best way to make ethical decisions is to follow these rules regardless of the consequences. For example, a doctor may choose to withhold life-saving treatment from a patient if the treatment is morally unacceptable. An example of how these ethical theories would apply to a specific ethical dilemma is the ethical use of artificial intelligence. Virtue ethics would suggest that the best way to ensure ethical AI use is to prioritize the development of moral virtues such as transparency, accountability, and fairness. Deontological ethics, on the other hand, would dictate that AI systems must be designed and used in a way that adheres to a set of pre-defined moral rules, regardless of the consequences. The strengths of virtue ethics are that it emphasizes the importance of personal moral responsibility and development, making it a more individualistic approach. The weakness of this theory is that it may be difficult to determine what virtues are truly universally important, and it does not provide concrete guidelines for decision making in specific situations. The strengths of deontological ethics are that it provides clear and concrete guidelines for decision making, making it a more structured and rule-based approach. The weakness of this theory is that it may not take into account the consequences of following these rules, and that it may be difficult to determine what moral rules are universally applicable. When evaluating the strengths and weaknesses of different ethical theories, it's important to consider their underlying assumptions, their implications for decision-making, and their ability to address the specific ethical dilemmas of a given context. It's also important to consider the practicality of implementing the theory and the degree to which it aligns with our own personal values and beliefs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alpaca</head><p>Ethical theories can be broadly classified into three categories: 1. Deontological ethical theories -which hold that certain actions are inherently right or wrong, regardless of the consequences or intentions. 2. Teleological ethical theories -which hold that the morality of an action depends on the outcome or result it produces. 3. Virtue ethical theories -which hold that the morality of an action depends on the character or traits of the person performing the action. Some examples of ethical theories are: 1. Deontological ethical theories: -Utilitarianism -which holds that the morality of an action depends on the outcome or result it produces. -Kantian ethics -which holds that certain actions are inherently right or wrong, regardless of the consequences or intentions. 2. Teleological ethical theories: -Utilitarianism -which holds that the morality of an action depends on the outcome or result it produces. -Consequentialism -which holds that the morality of an action depends on the consequences it produces. 3. Virtue ethical theories: -Aristotelian ethics -which holds that the morality of an action depends on the character or traits of the person performing the action. -Virtue ethics -which holds that the morality of an action depends on the character or traits of the person performing the action. An ethical dilemma is a situation where there are competing moral claims, and it is not clear which course of action is morally right. For example, a doctor may have to decide between saving the life of a pregnant woman or the fetus. Deontological ethical theories would say that the doctor must prioritize the life of the mother, as it is inherently wrong to take the life of an innocent person. Teleological ethical theories would say that the doctor should prioritize the life of the fetus, as the outcome or result is more important than the intentions or actions of the doctor. Virtue ethical theories would say that the doctor should prioritize the life of the person with the strongest moral character, which in this case would be the mother. The strengths and weaknesses of different ethical theories can be evaluated based on their ability to address the ethical dilemma at hand. For example, deontological ethical theories may be strong in providing clear guidelines for moral decision making, but may not take into account the potential consequences of the action. Teleological ethical theories may be strong in their focus on outcomes, but may not consider the intentions or character of the person performing the action. Virtue ethical theories may be strong in promoting moral character, but may not provide clear guidelines for moral decision making in complex situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ChatGPT</head><p>There are several main ethical theories that differ in their approaches to moral decision making, including consequentialism, deontology, and virtue ethics. Consequentialism holds that the morality of an action is determined by its consequences. It emphasizes the importance of maximizing overall well-being or minimizing harm. Utilitarianism is a well-known consequentialist theory that holds that the moral action is the one that maximizes overall happiness or pleasure, and minimizes overall pain or suffering. Deontology, on the other hand, focuses on the inherent morality of an action, rather than its consequences. It emphasizes the importance of following rules or duties, regardless of the consequences. For example, the moral rule to not lie is considered inherently moral, regardless of whether it leads to a good or bad outcome. Virtue ethics emphasizes the importance of developing and embodying moral virtues or character traits, such as honesty, courage, and compassion. It emphasizes the idea that morality is not just about what we do, but also about who we are. To illustrate how these ethical theories might apply to a specific ethical dilemma, consider the following scenario: A doctor has only one dose of a life-saving medication, and there are two patients who need it urgently. Patient A is elderly and has multiple health problems, while patient B is a young child with no other health issues. A consequentialist might argue that the moral action is to give the medication to the patient who will benefit the most, in terms of maximizing overall well-being or minimizing overall harm. In this case, the young child would likely be given the medication, since they have a longer life expectancy and less potential for complications. A deontologist might argue that the moral action is to follow a moral rule or duty, regardless of the consequences. In this case, the moral rule to prioritize patients based on their medical need might apply, and the medication would be given to the patient who needs it most urgently, regardless of age. A virtue ethicist might argue that the moral action is to embody certain virtues or character traits, such as compassion and empathy, and to consider the unique needs and circumstances of each patient. In this case, the doctor might take into account factors such as the patients' age, overall health, and potential for recovery, and make a decision based on a holistic view of what is most compassionate and just. When evaluating the strengths and weaknesses of different ethical theories, it is important to consider their theoretical coherence, practical applicability, and ability to account for the complexities of real-world moral dilemmas. Some ethical theories may be more well-suited to certain types of situations or contexts, while others may be more flexible or adaptable. Ultimately, the evaluation of ethical theories depends on a range of factors, including their ability to provide clear guidance, promote moral integrity, and promote overall human flourishing. Instruction: Given that the hypotenuse of a right triangle is 13, and the ratio of the lengths of the two legs is 5:12, find the lengths of the two legs. Solution: Let the lengths of the two legs be 5x and 12x, respectively. By the Pythagorean theorem, we have</p><formula xml:id="formula_4">13 2 = (5x) 2 + (12x) 2 169 = 25x 2 + 144x 2 169 = 169x 2 x 2 = 1 x = 1 or x = -1</formula><p>WizardLM Since x cannot be negative, we can discard the solution x = -1. Therefore, the lengths of the two legs are 5x = 5(1) = 5 and 12x = 12(1) = 12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vicuna</head><p>Therefore, the two legs of the right triangle are either both 5 units or both 12 units. Since 5 and 12 are not opposite sides of a right triangle, only the combination of 5 and 12 can be a leg in this case. Therefore, the lengths of the two legs are 5 and 12 units. Alpaca Since x cannot be negative, we have:</p><formula xml:id="formula_5">-x = 1</formula><p>Therefore, one of the legs is 1 and the other is -1. We'll call them a and b, respectively. So, the solution is:</p><formula xml:id="formula_6">-a = 1 -b = -1 ChatGPT</formula><p>Since the length of a side of a triangle cannot be negative, we can only take x = 1. Therefore, the lengths of the two legs are: 5x = 5(1) = 5 12x = 12(1) = 12 So, the lengths of the two legs are 5 and 12, respectively.</p><p>Table <ref type="table">9</ref>: A comparison case on Reasoning skill Skill: Reasoning Difficulty: 6 Instruction: A bakery sells three types of cakes: chocolate, vanilla and strawberry. Each cake has a different price and a different number of calories. The chocolate cake costs $12 and has 400 calories, the vanilla cake costs $10 and has 300 calories, and the strawberry cake costs $8 and has 200 calories. A customer wants to buy two cakes with a total budget of $20 and a maximum of 600 calories. Which two cakes should the customer buy? Explain your reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WizardLM</head><p>To find the two cakes that meet the customer's requirements, we need to calculate the total number of calories they want to consume and find the cakes that will add up to that number while staying within their budget. Let's start by calculating the total number of calories the customer wants to consume: -Chocolate cake: 400 calories -Vanilla cake: 300 calories -Strawberry cake: 200 calories Total calories = 400 + 300 + 200 = 900 calories Since the customer has a maximum of 600 calories, they can only buy one of the strawberry cakes. Now we need to find the second cake that will add up to the remaining 100 calories while staying within their budget. We can only use the chocolate or vanilla cake since they are the only ones that cost more than $8. If we use the chocolate cake, we will have: -Chocolate cake: 400 calories -Vanilla cake: 0 calories -Strawberry cake: 200 calories Total calories = 400 + 0 + 200 = 600 calories If we use the vanilla cake, we will have: -Vanilla cake: 300 calories -Chocolate cake: 400 calories -Strawberry cake: 200 calories Total calories = 300 + 400 + 200 = 900 calories Both options exceed the customer's budget, so we need to find a third cake to use. Since the strawberry cake is the only one that costs less than $8, we can use it with either the chocolate or vanilla cake: -Strawberry cake: 200 calories -Chocolate cake: 400 calories -Vanilla cake: 0 calories Total calories = 200 + 400 + 0 = 600 calories This option meets the customer's requirements and stays within their budget. Therefore, the customer should buy one chocolate cake and one strawberry cake.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vicuna</head><p>To find the total number of calories in two cakes, we need to add the calories in each cake: 200 + 400 = 600 Since the customer has a budget of $20 and a maximum of 600 calories, the total cost of the two cakes should be less than or equal to $20. Substituting the calorie counts into this equation gives: 200 + 400 ? 20 Solving for the second cake gives: 400 ? 20 -200 400 ? 20 400/20 = 20 So the minimum cost of the second cake is $20. We can use this information to solve for the minimum cost and the maximum number of calories in the two cakes: Minimum cost = 20(20) + 300(8) = $20 + $240 = $260 Maximum number of calories = 600</p><p>The customer should buy a chocolate cake and a strawberry cake, as they cost a total of $260 and have a combined maximum number of calories of 600. These two cakes meet the customer's budget of $20 and their calorie constraints. Therefore, the customer should buy a chocolate cake (with 400 calories) and a strawberry cake (with 200 calories), since they cost a total of $260 and have a combined maximum number of calories of 600.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alpaca</head><p>As an AI language model, I do not have personal preferences or opinions. However, based on the given information, a customer can buy either the chocolate cake ($12, 400 calories) and the vanilla cake ($10, 300 calories) or the strawberry cake ($8, 200 calories) and the vanilla cake ($10, 300 calories). Both options will fulfill the customer's requirements of buying two cakes with a total budget of $20 and a maximum of 600 calories. If the customer wants to have a balance of flavors, they can buy the chocolate cake and the strawberry cake. If they want to have a balance of calories, they can buy the vanilla cake and the strawberry cake.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ChatGPT</head><p>Based on the given information, the customer has a budget of $20 and wants to buy two cakes. Let's assume that the customer buys chocolate and vanilla cake. In this case, the total cost would be $12 + = $22, which exceeds the customer's budget. So, we need to find two cakes that cost no more than $20 in total. The customer also wants to consume no more than 600 calories. If the customer buys the chocolate cake, which has the highest number of calories, they can only buy one more cake to meet the calorie requirement.</p><p>If the customer buys the vanilla cake and the strawberry cake, the total cost would be $10 + $8 = $18, which fits the customer's budget. Additionally, the total number of calories in these two cakes is 300 + 200 = 500, which is well below the maximum limit of 600 calories. Therefore, based on the customer's budget and calorie requirements, it is recommended that they buy the vanilla and strawberry cakes.</p><p>chatgpt and GPT4) more difficult to handle. But the rewritten prompt must be reasonable and must be understood and responded by humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>You must add [SQL database] format text as input data in [Rewritten Prompt]</head><p>The Given Prompt: achieve the SQL query result Rewritten Prompt(MUST contain a specific SQL database as input):</p><p>There is a table messages that contains data as shown below:</p><p>Id Name Other_Columns -------------------------</p><formula xml:id="formula_7">1 A A_data_1 2 A A_data_2 3 A A_data_3 4 B B_data_1 5 B B_data_2 6 C C_data_1</formula><p>If I run a query select * from messages group by name, I will get the result as:</p><formula xml:id="formula_8">1 A A_data_1 4 B B_data_1 6 C C_data_1</formula><p>What query will return the following result?</p><formula xml:id="formula_9">3 A A_data_3 5 B B_data_2 6 C C_data_1</formula><p>That is, the last record in each group should be returned.</p><p>At present, this is the query that I use: SELECT * FROM ( SELECT * FROM messages ORDER BY id DESC ) AS x GROUP BY name But this looks highly inefficient. Any other ways to achieve the same result? #### Example-3: I want you act as a Prompt Rewriter. Your objective is to rewrite a given prompt into a more complex version using dataformat to make those famous AI systems (e.g., chatgpt and GPT4) more difficult to handle. But the rewritten prompt must be reasonable and must be understood and responded by humans. where var1 is an integer, var2 and var3 are strings. How can I write the variable names without Python including them as part of the query text? #### Example-4: I want you act as a Prompt Rewriter. Your objective is to rewrite a given prompt into a more complex version using dataformat to make those famous AI systems (e.g., chatgpt and GPT4) more difficult to handle. But the rewritten prompt must be reasonable and must be understood and responded by humans. You must add [HTML page] format text as input data in <ref type="bibr">[Rewritten Prompt]</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Running Examples of Evol-Instruct.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of Evol-Instruct</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>act as a Prompt Rewriter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Prompt] #Given Prompt#: &lt;Here is Demonstration instructio N.&gt; #Rewritten Prompt#: &lt;Here is Demonstration Example N.&gt; ## I want you act as a Prompt Rewriter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Dataformat#] format data as input data, add [#Given Dataformat#] code as input code in [Rewritten Prompt] Rewrite prompt must be a question style instruction #Given Prompt#: &lt;Here is instruction.&gt; #Rewrite prompt must be a question style instruction Rewritten Prompt(MUST contain a specific JSON data as input#:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The skills ditribution of our testset.</figDesc><graphic url="image-7.png" coords="9,80.41,47.84,445.37,226.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The difficulty and complexity level ditribution between the testset of Vicuna, Alpaca (Self-Instruct), and our Evol-Instruct.</figDesc><graphic url="image-8.png" coords="9,77.19,297.19,415.66,212.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Manual evaluation results between WizardLM and baselines.</figDesc><graphic url="image-9.png" coords="10,114.10,230.94,404.35,206.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Manual evaluation results between WizardLM and baselines on high-difficulty sub-testset (Difficulty &gt;= 8).</figDesc><graphic url="image-10.png" coords="10,114.10,404.99,404.35,206.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The average difficulty and complexity level between ShareGPT, Alpaca, and ours four rounds of instruction evolution from C1 to C4.</figDesc><graphic url="image-12.png" coords="11,63.16,328.67,440.85,224.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The difficulty and complexity level between ShareGPT, Alpaca, and ours four rounds of instruction evolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: The cluster scatter plot between ShareGPT, Alpaca, and ours four rounds of instruction evolution from C1 to C4. The number of cluster centers is 20.</figDesc><graphic url="image-13.png" coords="12,64.16,116.32,470.93,235.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>You must add [python code] format text as input data in [Rewritten Prompt] The Given Prompt: Transformat python code Rewritten Prompt(MUST contain a specific python code as input): I have the following Python code: cursor . execute (" INSERT INTO table VALUES var1 , var2 , var3 ,")</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>)</head><label></label><figDesc>Alpaca is a project by Stanford University that aims to build and share an open-source instructionfollowing model. It is based on LLaMA 7B, a large language model trained on diverse text sources, and fine-tuned on 52K instruction-following examples generated by querying OpenAI's text-davinci-003 model. On the self-instruct evaluation set, Alpaca exhibits comparable performance to textdavinci-003, while being remarkably compact and affordable to replicate. Alpaca is intended for academic research only and any commercial use is prohibited. We use the code from Alpaca 6 in this paper.</figDesc><table><row><cell>(3) Vicuna is an open-source chatbot that can generate natural and engaging responses to user queries.</cell></row><row><cell>It is based on LLaMA 13B and fine-tuned on 70K user-shared conversations collected from ShareGPT,</cell></row><row><cell>a website where people share their ChatGPT interactions. It is one of the most advanced and versatile</cell></row><row><cell>open instruction-following models available today. Vicuna achieves competitive performance against</cell></row><row><cell>proprietary models such as ChatGPT and Bard. We use the model from FastChat 7 in this paper.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>This code implements a basic version control system that allows users to check out a new branch, checkout the current branch, create a new branch, and get the version number. The code uses a simple directory structure to store the different branches and creates a version file to store the version number. The code also includes methods to check if a branch exists and to get the current branch.[Model: Vicuna] Here's a simple version control system written in C++:</figDesc><table><row><cell></cell><cell>// Method to get the version number</cell></row><row><cell cols="2">int getVersion () { # include &lt; iostream &gt; // Open the version file # include &lt; string &gt; ifstream versionFile ; # include &lt; vector &gt; versionFile . open ( versionFilePath ()); // Read the version number from the version file using namespace std ;</cell></row><row><cell cols="2">int version ; // Class to represent a repository versionFile &gt;&gt; version ; class Repository { // Return the version number public : return version ; // Constructor } Repository () {}</cell></row><row><cell cols="2">private : // Add a file to the repository string currentBranch ; void addFile ( const string &amp; fileName , const string &amp; fileContent ) { string versionFilePath ; files . push_back ( make_pair ( fileName , fileContent )); string cu rre nt Bra nch Pat h (); string branchPath (); }</cell></row><row><cell></cell><cell>bool existVersionFile (); // Remove a file from the repository bool directoryExists ( string path ) { void removeFile ( const string &amp; fileName ) { // Check if the directory exists for ( auto &amp;&amp; file : files ) { if ( access ( path . c_str () , F_OK ) == 0) { if ( file . first == fileName ) { return true ; files . erase ( file ); } else { return ; return false ; } } } } string versionFilePath () { }</cell></row><row><cell></cell><cell>return cu rr ent Bra nch Pat h () + "/" + currentBranch + ". txt "; // Get the file content for a given file } string getFileContent ( const string &amp; fileName ) const { string cu rre ntB ra nch Pat h () { for ( const auto &amp; file : files ) { return branchPath () + "/" + currentBranch ; if ( file . first == fileName ) { } return string ( file . second ); string branchPath () { } return " branches /" + currentBranch ; } } return ""; bool existBranch ( string branch ) { return existBranch ( branchPath () + "/" + branch ); }</cell></row><row><cell></cell><cell>} // Get the files in the repository void createBranch ( string branch ) { vector &lt; pair &lt; string , string &gt; &gt; getFiles () const { directory ( branchPath () + "/" + branch ); return files ; ofstream versionFile ; versionFile . open ( versionFilePath () + "/" + branch + ". txt "); }</cell></row><row><cell cols="2">versionFile &lt;&lt; "0" &lt;&lt; endl ; versionFile . close (); vector &lt; pair &lt; string , string &gt; &gt; files ; // A vector to store the files in the repository private : } void checkoutBranch () { };</cell></row><row><cell cols="2">ifstream checkoutFile ; class User { checkoutFile . open ( cur ren tBr anc hP ath () + "/" + currentBranch + ". cpp "); public : if ( checkoutFile ) { // Constructor cout &lt;&lt; " Success !" &lt;&lt; endl ; } else { User ( const string &amp; name ) : name_ ( name ) {}</cell></row><row><cell cols="2">cout &lt;&lt; " Error : Unable to checkout file ." &lt;&lt; endl ; // Add a file to the repository } void addFile ( const string &amp; fileName , const string &amp; fileContent ) { } repository_ . addFile ( fileName , fileContent ); int main () { }; }</cell></row><row><cell>}</cell><cell>VersionControl vc ; // Remove a file from the repository vc . checkOut (" new_branch "); void removeFile ( const string &amp; fileName ) { return 0; repository_ . removeFile ( fileName ); }</cell></row><row><cell></cell><cell>// Get the file content for a given file</cell></row><row><cell></cell><cell>string getFileContent ( const string &amp; fileName ) const {</cell></row><row><cell></cell><cell>return repository_ . getFileContent ( fileName );</cell></row><row><cell></cell><cell>}</cell></row><row><cell></cell><cell>// Get the files in the repository</cell></row><row><cell></cell><cell>vector &lt; pair &lt; string , string &gt; &gt; getFiles () const {</cell></row><row><cell></cell><cell>return repository_ . getFiles ();</cell></row><row><cell></cell><cell>}</cell></row><row><cell cols="2">private :</cell></row><row><cell></cell><cell>string name_ ; // The name of the user</cell></row><row><cell></cell><cell>Repository repository_ ; // The repository that the user is currently working on</cell></row><row><cell>};</cell><cell></cell></row><row><cell></cell><cell>int main () {</cell></row><row><cell></cell><cell>Repository repo ;</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>A comparison case on Sport skill Skill: Sport Difficulty: 8 Instruction: What are the benefits and risks of high-intensity interval training (HIIT) for athletes? Compare and contrast HIIT with other forms of aerobic exercise, such as jogging, cycling, or swimming. Provide examples of HIIT workouts and explain how they can improve performance, endurance, and health.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>A comparison case on Computer Science skill Skill: Computer Science Difficulty: 9 Instruction: How can I train a LLM using RLHF methods based on InstructGPT to create a human assistant that exceeds open assistant in it's performance, accuracy, and utility?</figDesc><table><row><cell>WizardLM</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>AlpacaAs an AI language model, I can suggest some ways to train an LLM using RLHF methods based on InstructGPT to create a human assistant that exceeds open assistant in performance, accuracy, and utility: 1. Data Collection: Collect a large amount of high-quality data that covers a wide range of tasks and scenarios. This data should be labeled and annotated to provide the model with the necessary information to learn from. 2. Preprocessing: Preprocess the data by removing noise, correcting grammar and spelling errors, and converting the text to a standard format. This step is crucial for the model to understand the context of the data. 3. Model Architecture: Choose a suitable model architecture that can handle a wide range of tasks and has the ability to learn from different types of data. InstructGPT is a good choice for this task since it is designed for natural language processing. 4. Training: Train the model using RLHF methods to improve its performance and accuracy. This can be done by fine-tuning the pre-trained model on a specific task or by training the model from scratch. 5. Evaluation: Evaluate the model's performance using standard benchmarks and metrics. This will help to measure the model's accuracy and identify areas that need improvement. 6. Continuous Improvement: Continuously update the model with new data and improve its performance by fine-tuning the model or training it from scratch. This will ensure that the model remains accurate and useful over time.</figDesc><table><row><cell>It is important to identify your specific</cell></row><row><cell>requirements and design a task-specific training strategy to fine-tune the LLM on the target</cell></row><row><cell>task.</cell></row><row><cell>Resources:</cell></row><row><cell>* Johansson, P., Carlsson, J., &amp; Yoshimoto, R. (2014). Reducing. In R. Yoshimoto, Handbook</cell></row><row><cell>of brain-machine interfaces (pp. 239-254). Springer.</cell></row><row><cell>* Raiko H. L. Lam (2022). Reducing Learner-to-human: Concept and Applications. arXiv</cell></row><row><cell>preprint arXiv:2205.13847.</cell></row><row><cell>* OpenAI (2021). Open Assistant.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>A comparison case on Writing skill Skill: Writing Difficulty: 4 Instruction: Write a short story about a character who discovers a mysterious object in their backyard. What is the object, and what does it do? How does the character react to it? What happens next?</figDesc><table><row><cell>WizardLM</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>A comparison case on Complex Format skill Skill: Complex Format Difficulty: 4</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>https://chat.openai.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>gpt-3.5-turbo from https://oai.azure.com/portal</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>https://github.com/tatsu-lab/stanford_alpaca</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3"><p>https://github.com/lm-sys/FastChat</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4"><p>gpt-3.5-turbo from https://oai.azure.com/portal</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>\ $day = \ $dom -&gt; g e t E l e m e n t s B y T a g N a m e ( ' day '); \\ $impressions = \\ $dom -&gt; g e t E l e m e n t s B y T a g N a m e ( ' impressions '); echo ( " &lt; table &gt;"); foreach (\\ $day as \\ $node1 ) { foreach (\\ $impressions as \\ $node2 ) { echo '&lt;tr &gt; '; echo " &lt; td &gt;". \\ $node1 -&gt; textContent . " &lt; td &gt;"; echo " &lt; td &gt;". \\ $node2 -&gt; textContent . " &lt; td &gt;"; echo " &lt; td &gt;". \\ $node2 -&gt; textContent *0.5/1000 ." &lt; td &gt;"; echo ' &lt;/ tr &gt; '; } } echo ( " &lt;/ table &gt;"); ? &gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Could anyone give a hint how I can fix this? thank you ####</head><p>Example-2: I want you act as a Prompt Rewriter. Your objective is to rewrite a given prompt into a more complex version using dataformat to make those famous AI systems (e.g., The Given Prompt: scroll through the whole HTML page Rewritten Prompt(MUST contain a specific HTML page as input): I want to be able to scroll through the whole page, but without the scrollbar being shown. In Google Chrome it's:</p><p>:: -webkit -scrollbar { display : none ; } But Mozilla Firefox and Internet Explorer don't seem to work like that. I also tried this in CSS: overflow : hidden ;</p><p>That does hide the scrollbar, but I can't scroll any more. Is there a way I can remove the scrollbar while still being able to scroll the whole page?</p><p>With just CSS or HTML, please. #### Example-5: I want you act as a Prompt Rewriter. Your objective is to rewrite a given prompt into a more complex version using dataformat to make those famous AI systems (e.g., chatgpt and GPT4) more difficult to handle. But the rewritten prompt must be reasonable and must be understood and responded by humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>You must add [Shell cmd] format text as input data in [Rewritten Prompt]</head><p>The Given Prompt: Shell scp file Rewritten Prompt(MUST contain a specific Shell cmd as input): I'm trying to scp a file from a remote server to my local machine. Only port 80 is accessible. I tried: scp -p 80 username@www.myserver.com:/root/file.txt . but got this error: cp: 80: No such file or directory How do I specify the port number in a scp command? #### Example-6: I want you act as a Prompt Rewriter. Your objective is to rewrite a given prompt into a more complex version using dataformat to make those famous AI systems (e.g., chatgpt and GPT4) more difficult to handle. But the rewritten prompt must be reasonable and must be understood and responded by humans. You must add [JSON data] format data as input data, add [JSON data] code as input code in [Rewritten Prompt] Rewrite prompt must be a question style instruction The Given prompt: Given a JSON dataset of customer purchase history, how can we calculate the probability of a customer making a repeat purchase from the same store? Can we utilize the formula for conditional probability: P (A|B) = P (A ? B)/P (B) where A represents the event of a customer making a repeat purchase and B represents the event of a customer making a purchase from the same store again? Additionally, how can we apply this formula to identify the customer segment that is most likely to make a repeat purchase? Can you provide an example of how to implement this formula using the given JSON dataset?</p><p>Rewritten prompt must be a question style instruction Rewritten Prompt(MUST contain a specific JSON data as input):</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">OpenAI. Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timoth?e</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozi?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ext5: Towards extreme multi-task scaling for transfer learning</title>
		<author>
			<persName><forename type="first">Vamsi</forename><surname>Aribandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaixiu</forename><surname>Steven Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanket</forename><surname>Vaibhav Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglei</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmo</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jai</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01652</idno>
		<title level="m">Finetuned language models are zero-shot learners</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Zeroprompt: Scaling prompt-based pretraining to 1,000 tasks improves zero-shot generalization</title>
		<author>
			<persName><forename type="first">Hanwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.06910</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multitask prompted training enables zero-shot task generalization</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manan</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saiful Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urmish</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanya</forename><surname>Sharma Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Szczechla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taewoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunjan</forename><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nihal</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debajyoti</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Tian-Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Manica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng Xin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshit</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Bawden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trishala</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jos</forename><surname>Neeraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abheesht</forename><surname>Rozen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Santilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Fevry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Fries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Teehan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Teven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Scaling instruction-finetuned language models</title>
		<author>
			<persName><forename type="first">Chung</forename><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Brahma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.11416</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-03">March 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<ptr target="https://github.com/tatsu-lab/stanford_alpaca" />
		<title level="m">Stanford alpaca: An instruction-following llama model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Self-instruct: Aligning language model with self generated instructions</title>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeganeh</forename><surname>Kordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alisa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.10560</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The flan collection: Designing data and methods for effective instruction tuning</title>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tu</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.13688</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Allin-one knowledge mixture model for data augmentation in few-shot nlp</title>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiubo</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><surname>Knowda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.10265</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Autoprompt: Eliciting knowledge from language models with automatically generated prompts</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.15980</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
