<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recurrent Convolutional Neural Networks for Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Siwei</forename><surname>Lai</surname></persName>
							<email>swlai@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liheng</forename><surname>Xu</surname></persName>
							<email>lhxu@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
							<email>kliu@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
							<email>jzhao@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recurrent Convolutional Neural Networks for Text Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text classification is a foundational task in many NLP applications. Traditional text classifiers often rely on many human-designed features, such as dictionaries, knowledge bases and special tree kernels. In contrast to traditional methods, we introduce a recurrent convolutional neural network for text classification without human-designed features. In our model, we apply a recurrent structure to capture contextual information as far as possible when learning word representations, which may introduce considerably less noise compared to traditional window-based neural networks. We also employ a max-pooling layer that automatically judges which words play key roles in text classification to capture the key components in texts. We conduct experiments on four commonly used datasets. The experimental results show that the proposed method outperforms the state-of-the-art methods on several datasets, particularly on document-level datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Text classification is an essential component in many applications, such as web searching, information filtering, and sentiment analysis <ref type="bibr" target="#b0">(Aggarwal and Zhai 2012)</ref>. Therefore, it has attracted considerable attention from many researchers.</p><p>A key problem in text classification is feature representation, which is commonly based on the bag-of-words (BoW) model, where unigrams, bigrams, n-grams or some exquisitely designed patterns are typically extracted as features. Furthermore, several feature selection methods, such as frequency, MI <ref type="bibr" target="#b6">(Cover and Thomas 2012)</ref>, pLSA <ref type="bibr" target="#b5">(Cai and Hofmann 2003)</ref>, LDA <ref type="bibr" target="#b9">(Hingmire et al. 2013)</ref>, are applied to select more discriminative features. Nevertheless, traditional feature representation methods often ignore the contextual information or word order in texts and remain unsatisfactory for capturing the semantics of the words. For example, in the sentence "A sunset stroll along the South Bank affords an array of stunning vantage points.", when we analyze the word "Bank" (unigram), we may not know whether it means a financial institution or the land beside a river. In addition, the phrase "South Bank" (bigram), particularly considering the two uppercase letters, may mislead people who are not particularly knowledgeable about London to take it as a financial institution. After we obtain the greater context "stroll along the South Bank" (5-gram), we can easily distinguish the meaning. Although high-order ngrams and more complex features (such as tree kernels <ref type="bibr" target="#b22">(Post and Bergsma 2013)</ref>) are designed to capture more contextual information and word orders, they still have the data sparsity problem, which heavily affects the classification accuracy.</p><p>Recently, the rapid development of pre-trained word embedding and deep neural networks has brought new inspiration to various NLP tasks. Word embedding is a distributed representation of words and greatly alleviates the data sparsity problem <ref type="bibr" target="#b2">(Bengio et al. 2003)</ref>. <ref type="bibr" target="#b16">Mikolov, Yih, and Zweig (2013)</ref> shows that pre-trained word embeddings can capture meaningful syntactic and semantic regularities. With the help of word embedding, some composition-based methods are proposed to capture the semantic representation of texts. <ref type="bibr" target="#b23">Socher et al. (2011a;</ref><ref type="bibr" target="#b24">2011b;</ref><ref type="bibr">2013)</ref> proposed the Recursive Neural Network (RecursiveNN) that has been proven to be efficient in terms of constructing sentence representations. However, the RecursiveNN captures the semantics of a sentence via a tree structure. Its performance heavily depends on the performance of the textual tree construction. Moreover, constructing such a textual tree exhibits a time complexity of at least O(n 2 ), where n is the length of the text. This would be too time-consuming when the model meets a long sentence or a document. Furthermore, the relationship between two sentences can hardly be represented by a tree structure. Therefore, RecursiveNN is unsuitable for modeling long sentences or documents.</p><p>Another model, which only exhibits a time complexity O(n), is the Recurrent Neural Network (RecurrentNN). This model analyzes a text word by word and stores the semantics of all the previous text in a fixed-sized hidden layer <ref type="bibr" target="#b7">(Elman 1990</ref>). The advantage of RecurrentNN is the ability to better capture the contextual information. This could be beneficial to capture semantics of long texts. However, the RecurrentNN is a biased model, where later words are more dominant than earlier words. Thus, it could reduce the effectiveness when it is used to capture the semantics of a whole document, because key components could appear anywhere in a document rather than at the end.</p><p>To tackle the bias problem, the Convolutional Neural Network (CNN), an unbiased model is introduced to NLP tasks, which can fairly determine discriminative phrases in a text with a max-pooling layer. Thus, the CNN may better capture the semantic of texts compared to recursive or recurrent neural networks. The time complexity of the CNN is also O(n). However, previous studies on CNNs tends to use simple convolutional kernels such as a fixed window <ref type="bibr" target="#b5">(Collobert et al. 2011;</ref><ref type="bibr" target="#b12">Kalchbrenner and Blunsom 2013)</ref>. When using such kernels, it is difficult to determine the window size: small window sizes may result in the loss of some critical information, whereas large windows result in an enormous parameter space (which could be difficult to train). Therefore, it raises a question: can we learn more contextual information than conventional window-based neural networks and represent the semantic of texts more precisely for text classification.</p><p>To address the limitation of the above models, we propose a Recurrent Convolutional Neural Network (RCNN) and apply it to the task of text classification. First, we apply a bi-directional recurrent structure, which may introduce considerably less noise compared to a traditional windowbased neural network, to capture the contextual information to the greatest extent possible when learning word representations. Moreover, the model can reserve a larger range of the word ordering when learning representations of texts. Second, we employ a max-pooling layer that automatically judges which features play key roles in text classification, to capture the key component in the texts. By combining the recurrent structure and max-pooling layer, our model utilizes the advantage of both recurrent neural models and convolutional neural models. Furthermore, our model exhibits a time complexity of O(n), which is linearly correlated with the length of the text length.</p><p>We compare our model with previous state-of-the-art approaches using four different types of tasks in English and Chinese. The classification taxonomy contains topic classification, sentiment classification and writing style classification. The experiments demonstrate that our model outperforms previous state-of-the-art approaches in three of the four commonly used datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work Text Classification</head><p>Traditional text classification works mainly focus on three topics: feature engineering, feature selection and using different types of machine learning algorithms. For feature engineering, the most widely used feature is the bag-of-words feature. In addition, some more complex features have been designed, such as part-of-speech tags, noun phrases (Lewis 1992) and tree kernels <ref type="bibr" target="#b22">(Post and Bergsma 2013)</ref>. Feature selection aims at deleting noisy features and improving the classification performance. The most common feature selection method is removing the stop words (e.g., "the"). Advanced approaches use information gain, mutual information <ref type="bibr" target="#b6">(Cover and Thomas 2012)</ref>, or L1 regularization <ref type="bibr" target="#b20">(Ng 2004)</ref> to select useful features. Machine learning algorithms often use classifiers such as logistic regression (LR), naive Bayes (NB), and support vector machine (SVM). However, these methods have the data sparsity problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep neural networks</head><p>Recently, deep neural networks <ref type="bibr" target="#b10">(Hinton and Salakhutdinov 2006)</ref> and representation learning <ref type="bibr" target="#b3">(Bengio, Courville, and Vincent 2013)</ref> have led to new ideas for solving the data sparsity problem, and many neural models for learning word representations have been proposed <ref type="bibr" target="#b2">(Bengio et al. 2003;</ref><ref type="bibr" target="#b18">Mnih and Hinton 2007;</ref><ref type="bibr" target="#b17">Mikolov 2012;</ref><ref type="bibr" target="#b5">Collobert et al. 2011;</ref><ref type="bibr" target="#b11">Huang et al. 2012;</ref><ref type="bibr">Mikolov et al. 2013)</ref>. The neural representation of a word is called word embedding and is a realvalued vector. The word embedding enables us to measure word relatedness by simply using the distance between two embedding vectors.</p><p>With the pre-trained word embeddings, neural networks demonstrate their great performance in many NLP tasks. <ref type="bibr" target="#b24">Socher et al. (2011b)</ref> use semi-supervised recursive autoencoders to predict the sentiment of a sentence. <ref type="bibr" target="#b23">Socher et al. (2011a)</ref> proposed a method for paraphrase detection also with recurrent neural network. <ref type="bibr" target="#b25">Socher et al. (2013)</ref> introduced recursive neural tensor network to analyse sentiment of phrases and sentences. <ref type="bibr" target="#b17">Mikolov (2012)</ref> uses recurrent neural network to build language models. <ref type="bibr" target="#b12">Kalchbrenner and Blunsom (2013)</ref> proposed a novel recurrent network for dialogue act classification. Collobert et al. ( <ref type="formula">2011</ref>) introduce convolutional neural network for semantic role labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>We propose a deep neural model to capture the semantics of the text. Figure <ref type="figure" target="#fig_0">1</ref> shows the network structure of our model. The input of the network is a document D, which is a sequence of words w 1 , w 2 . . . w n . The output of the network contains class elements. We use p(k|D, θ) to denote the probability of the document being class k, where θ is the parameters in the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word Representation Learning</head><p>We combine a word and its context to present a word. The contexts help us to obtain a more precise word meaning. In our model, we use a recurrent structure, which is a bidirectional recurrent neural network, to capture the contexts.</p><p>We define c l (w i ) as the left context of word w i and c r (w i ) as the right context of word w i . Both c l (w i ) and c r (w i ) are dense vectors with |c| real value elements. The left-side context c l (w i ) of word w i is calculated using Equation (1), where e(w i−1 ) is the word embedding of word w i−1 , which is a dense vector with |e| real value elements. c l (w i−1 ) is the left-side context of the previous word w i−1 . The left-side context for the first word in any document uses the same shared parameters c l (w 1 ). W (l) is a matrix that transforms the hidden layer (context) into the next hidden layer. W (sl) is a matrix that is used to combine the semantic of the current word with the next word's left context. f is a non-linear activation function. The right-side context c r (w i ) is calculated in a similar manner, as shown in Equation ( <ref type="formula" target="#formula_0">2</ref>). The right-side contexts of the last word in a document share the parameters c r (w n ).</p><formula xml:id="formula_0">c l (w i ) = f (W (l) c l (w i−1 ) + W (sl) e(w i−1 )) (1) c r (w i ) = f (W (r) c r (w i+1 ) + W (sr) e(w i+1 ))<label>(2)</label></formula><p>... ...</p><p>x 3</p><p>x 4</p><p>x 5</p><p>x 6</p><p>x 7 c l (w 3 ) As shown in Equations ( <ref type="formula">1</ref>) and (2), the context vector captures the semantics of all left-and right-side contexts. For example, in Figure <ref type="figure" target="#fig_0">1, c l (w 7</ref> ) encodes the semantics of the left-side context "stroll along the South" along with all previous texts in the sentence, and c r (w 7 ) encodes the semantics of the right-side context "affords an . . . ". Then, we define the representation of word w i in Equation (3), which is the concatenation of the left-side context vector c l (w i ), the word embedding e(w i ) and the right-side context vector c r (w i ). In this manner, using this contextual information, our model may be better able to disambiguate the meaning of the word w i compared to conventional neural models that only use a fixed window (i.e., they only use partial information about texts).</p><formula xml:id="formula_1">c l (w 4 ) c l (w 5 ) c l (w 6 ) c l (w 7 ) c r (w 3 ) c r (w 4 ) c r (w 5 ) c r (w 6 ) c r (w 7 ) y (2) 3 y (2) 4 y<label>(</label></formula><formula xml:id="formula_2">x i = [c l (w i ); e(w i ); c r (w i )]</formula><p>(3) The recurrent structure can obtain all c l in a forward scan of the text and c r in a backward scan of the text. The time complexity is O(n). After we obtain the representation x i of the word w i , we apply a linear transformation together with the tanh activation function to x i and send the result to the next layer.</p><formula xml:id="formula_3">y (2) i = tanh W (2) x i + b (2) (4) y (2) i</formula><p>is a latent semantic vector, in which each semantic factor will be analyzed to determine the most useful factor for representing the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Representation Learning</head><p>The convolutional neural network in our model is designed to represent the text. From the perspective of convolutional neural networks, the recurrent structure we previously mentioned is the convolutional layer.</p><p>When all of the representations of words are calculated, we apply a max-pooling layer.</p><formula xml:id="formula_4">y (3) = n max i=1 y (2) i (5)</formula><p>The max function is an element-wise function. The k-th element of y (3) is the maximum in the k-th elements of y</p><p>(2) i . The pooling layer converts texts with various lengths into a fixed-length vector. With the pooling layer, we can capture the information throughout the entire text. There are other types of pooling layers, such as average pooling layers <ref type="bibr" target="#b5">(Collobert et al. 2011</ref>). We do not use average pooling here because only a few words and their combination are useful for capturing the meaning of the document. The max-pooling layer attempts to find the most important latent semantic factors in the document. The pooling layer utilizes the output of the recurrent structure as the input. The time complexity of the pooling layer is O(n). The overall model is a cascade of the recurrent structure and a max-pooling layer, therefore, the time complexity of our model is still O(n).</p><p>The last part of our model is an output layer. Similar to traditional neural networks, it is defined as</p><formula xml:id="formula_5">y (4) = W (4) y (3) + b (4)<label>(6)</label></formula><p>Finally, the softmax function is applied to y (4) . It can convert the output numbers into probabilities.</p><formula xml:id="formula_6">p i = exp y (4) i n k=1 exp y (4) k (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training</head><p>Training Network parameters We define all of the parameters to be trained as θ.</p><formula xml:id="formula_7">θ = {E, b (2) , b (4) , c l (w 1 ), c r (w n ), W (2) , W (4) , W (l) , W (r) , W (sl) , W (sr) } (8)</formula><p>Specifically, the parameters are word embeddings (sl) , W (sr) ∈ R |e|×|c| , where |V | is the number of words in the vocabulary, H is the hidden layer size, and O is the number of document types.</p><formula xml:id="formula_8">E ∈ R |e|×|V | , the bias vectors b (2) ∈ R H , b (4) ∈ R O , the initial contexts c l (w 1 ), c r (w n ) ∈ R |c| and the transformation ma- trixes W (2) ∈ R H×(|e|+2|c|) , W (4) ∈ R O×H , W (l) , W (r) ∈ R |c|×|c| , W</formula><p>The training target of the network is used to maximize the log-likelihood with respect to θ:</p><formula xml:id="formula_9">θ → D∈D log p(class D |D, θ) (9)</formula><p>where D is the training document set and class D is the correct class of document D.</p><p>We use stochastic gradient descent <ref type="bibr" target="#b4">(Bottou 1991)</ref> to optimize the training target. In each step, we randomly select an example (D, class D ) and make a gradient step.</p><formula xml:id="formula_10">θ ← θ + α ∂ log p(class D |D, θ) ∂θ (<label>10</label></formula><formula xml:id="formula_11">)</formula><p>where α is the learning rate.</p><p>We use one trick that is widely used when training neural networks with stochastic gradient descent in the training phase. We initialize all of the parameters in the neural network from a uniform distribution. The magnitude of the maximum or minimum equals the square root of the "fanin" <ref type="bibr" target="#b21">(Plaut and Hinton 1987</ref>). The number is the network node of the previous layer in our model. The learning rate for that layer is divided by "fan-in".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training Word Embedding</head><p>Word embedding is a distributed representation of a word. Distributed representation is suitable for the input of neural networks. Traditional representations, such as one-hot representation, will lead to the curse of dimensionality <ref type="bibr" target="#b2">(Bengio et al. 2003)</ref>. Recent research <ref type="bibr" target="#b10">(Hinton and Salakhutdinov 2006;</ref><ref type="bibr" target="#b8">Erhan et al. 2010)</ref> shows that neural networks can converge to a better local minima with a suitable unsupervised pre-training procedure.</p><p>In this work, we use the Skip-gram model to pre-train the word embedding. this model is the state-of-the-art in many NLP tasks <ref type="bibr" target="#b1">(Baroni, Dinu, and Kruszewski 2014)</ref>. The Skipgram model trains the embeddings of words w 1 , w 2 . . . w T by maximizing the average log probability where |V | is the vocabulary of the unlabeled text. e (w i ) is another embedding for w i . We use the embedding e because some speed-up approaches (e.g., hierarchical softmax <ref type="bibr" target="#b19">(Morin and Bengio 2005)</ref>) will be used here, and e is not calculated in practice.</p><formula xml:id="formula_12">1 T T t=1 −c≤j≤c,j =0 log p(w t+j |w t )<label>(11</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Datasets</head><p>To demonstrate the effectiveness of the proposed method, we perform the experiments using the following four datasets: 20Newsgroups, Fudan Set, ACL Anthology Network, and Sentiment Treebank. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Settings</head><p>We preprocess the dataset as follows. For English documents, we use the Stanford Tokenizer<ref type="foot" target="#foot_4">5</ref> to obtain the tokens.</p><p>For Chinese documents, we use ICTCLAS<ref type="foot" target="#foot_5">6</ref> to segment the words. We do not remove any stop words or symbols in the texts. All four datasets have been previously separated into training and testing sets. The ACL and SST datasets have a pre-defined training, development and testing separation.</p><p>For the other two datasets, we split 10% of the training set into a development set and keep the remaining 90% as the real training set. The evaluation metric of the 20Newsgroups is the Macro-F1 measure followed by the state-of-the-art work. The other three datasets use accuracy as the metric.</p><p>The hyper-parameter settings of the neural networks may depend on the dataset being used. We choose one set of commonly used hyper-parameters following previous studies <ref type="bibr" target="#b5">(Collobert et al. 2011;</ref><ref type="bibr" target="#b26">Turian, Ratinov, and Bengio 2010)</ref>. Moreover, we set the learning rate of the stochastic gradient descent α as 0.01, the hidden layer size as H = 100, the vector size of the word embedding as |e| = 50 and the size of the context vector as |c| = 50. We train word em- beddings using the default parameter in word2vec 7 with the Skip-gram algorithm. We use Wikipedia dumps in both English and Chinese to train the word embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Methods</head><p>We compare our method with widely used text classification methods and the state-of-the-art approaches for each dataset.</p><p>Bag of Words/Bigrams + LR/SVM <ref type="bibr" target="#b27">Wang and Manning (2012)</ref> proposed several strong baselines for text classification. These baselines mainly use machine learning algorithms with unigram and bigrams as features. We use logistic regression (LR) and SVM 8 , respectively. The weight of each feature is the term frequency.</p><p>Average Embedding + LR This baseline uses the weighted average of the word embeddings and subsequently applies a softmax layer. The weight for each word is its tfidf value. <ref type="bibr" target="#b11">Huang et al. (2012)</ref> also used this strategy as the global context in their task. <ref type="bibr" target="#b13">Klementiev, Titov, and Bhattarai (2012)</ref> used this in crosslingual document classification.</p><p>LDA LDA-based approaches achieve good performance in terms of capturing the semantics of texts in several classification tasks. We select two methods as the methods for comparison: ClassifyLDA-EM <ref type="bibr" target="#b9">(Hingmire et al. 2013</ref>) and Labeled-LDA (Li, Sun, and Zhang 2008).</p><p>Tree Kernels <ref type="bibr" target="#b22">Post and Bergsma (2013)</ref> used various tree kernels as features. It is the state-of-the-art work in the ACL native language classification task. We list two major methods for comparison: the context-free grammar (CFG) produced by the Berkeley parser <ref type="bibr" target="#b20">(Petrov et al. 2006)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head><p>The experimental results are shown in Table <ref type="table" target="#tab_1">2</ref>.  <ref type="formula">2013</ref>) is approximately 3-5 hours. Training the RCNN on the SST dataset only takes several minutes using a single-thread machine. • In all datasets expect ACL and SST dataset, RCNN outperforms the state-of-the-art methods. In the ACL dataset, RCNN has a competitive result with the best baseline. We reduce the error rate by 33% for the 20News dataset and by 19% for the Fudan set with the best baselines. The results prove the effectiveness of the proposed method. • We compare our RCNN to well-designed feature sets in the ACL dataset. The experimental results show that the RCNN outperforms the CFG feature set and obtains a result that is competitive with the C&amp;J feature set. We believe that the RCNN can capture long-distance patterns, which are also introduced by tree kernels. Despite the competitive results, the RCNN does not require handcrafted feature sets, which means that it might be useful in low-resource languages. • We also compare the RCNN to the CNN and find that the RCNN outperforms the CNN in all cases. We believe that the reason is the recurrent structure in the RCNN captures contextual information better than window-based structure in CNNs. This results demonstrate the effectiveness of the proposed method. To illustrate this point more clearly, we propose a detailed analysis in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contextual Information</head><p>In this subsection, we investigate the ability of the recurrent structure in our model for capturing contextual information in further detail. The difference between CNNs and RCNNs is that they use different structure for capturing contextual information. CNNs use a fixed window of words as contextual information, whereas RCNNs use the recurrent structure to capture a wide range of contextual information. The performance of a CNN is influenced by the window size. A small window may result in a loss of some long-distance patterns, whereas large windows will lead to data sparsity. Furthermore, a large number of parameters are more difficult to train. We consider all odd window sizes from 1 to 19 to train and test the CNN model. For example, when the window size is one, the CNN only uses the word embedding [e(w i )] to represent the word. When the window size is three, the CNN uses [e(w i−1 ); e(w i ); e(w i+1 )] to represent word w i . The test scores for these various window sizes are shown in Figure 2. Because of space limitations, we only show the classification results for the 20Newsgroups dataset. In this figure, we can observe that the RCNN outperforms the CNN for all window sizes. It illustrate that the RCNN could capture contextual information with a recurrent structure that does not rely on the window size. The RCNN outperforms windowbased CNNs because the recurrent structure can preserve longer contextual information and introduces less noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learned Keywords</head><p>To investigate how our model constructs the representations of texts, we list the most impor-  <ref type="table" target="#tab_4">3</ref>. The most important words are the information most frequently selected in the max-pooling layer. Because the word representation in our model is a word together with its context, the context may contain the entire text. We only present the center word and its neighboring trigram. For comparison, we also list the most positive/negative trigram phrases extracted by the RNTN <ref type="bibr" target="#b25">(Socher et al. 2013)</ref>.</p><p>In contrast to the most positive and most negative phrases in RNTN, our model does not rely on a syntactic parser, therefore, the presented n-grams are not typically "phrases". The results demonstrate that the most important words for positive sentiment are words such as "worth", "sweetest", and "wonderful", and those for negative sentiment are words such as "awfully", "bad", and "boring".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We introduced recurrent convolutional neural networks to text classification. Our model captures contextual information with the recurrent structure and constructs the representation of text using a convolutional neural network. The experiment demonstrates that our model outperforms CNN and RecursiveNN using four different text classification datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: The structure of the recurrent convolutional neural network. This figure is a partial example of the sentence "A sunset stroll along the South Bank affords an array of stunning vantage points", and the subscript denotes the position of the corresponding word in the original sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>) p(w b |w a ) = exp e (w b ) T e(w a ) |V | k=1 exp e (w k ) T e(w a ) (12)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Table1provides detailed information about each dataset. A summary of the datasets, including the number of classes, the number of train/dev/test set entries, the average text length and the language of the dataset.20Newsgroups 1 This dataset contains messages from twenty newsgroups. We use the bydate version and select four major categories (comp, politics, rec, and religion) followed by<ref type="bibr" target="#b9">Hingmire et al. (2013)</ref>.ACL Anthology Network 3 This dataset contains scientific documents published by the ACL and by related organizations. It is annotated by<ref type="bibr" target="#b22">Post and Bergsma (2013)</ref> with the five most common native languages of the authors: English, Japanese, German, Chinese, and French.</figDesc><table><row><cell cols="2">Dataset C</cell><cell>Train/Dev/Test</cell><cell cols="2">Len Lang</cell></row><row><cell cols="2">20News 4</cell><cell>7520/836/5563</cell><cell>429</cell><cell>EN</cell></row><row><cell>Fudan</cell><cell>20</cell><cell>8823/981/9832</cell><cell>2981</cell><cell>CH</cell></row><row><cell>ACL</cell><cell cols="2">5 146257/28565/28157</cell><cell>25</cell><cell>EN</cell></row><row><cell>SST</cell><cell>5</cell><cell>8544/1101/2210</cell><cell>19</cell><cell>EN</cell></row><row><cell cols="5">Fudan set 2 The Fudan University document classification</cell></row><row><cell cols="5">set is a Chinese document classification set that consists of</cell></row><row><cell cols="4">20 classes, including art, education, and energy.</cell><cell></cell></row></table><note>Stanford Sentiment Treebank 4 The dataset contains movie reviews parsed and labeled by<ref type="bibr" target="#b25">Socher et al. (2013)</ref>. The labels are Very Negative, Negative, Neutral, Positive, and Very Positive.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Test set results for the datasets. The top, middle, and bottom parts are the baselines, the state-of-the-art results and the results of our model, respectively. The state-of-the-art results are reported by the corresponding essays.</figDesc><table><row><cell>Model</cell><cell cols="3">20News Fudan ACL</cell><cell>SST</cell></row><row><cell>BoW + LR</cell><cell>92.81</cell><cell>92.08</cell><cell cols="2">46.67 40.86</cell></row><row><cell>Bigram + LR</cell><cell>93.12</cell><cell>92.97</cell><cell cols="2">47.00 36.24</cell></row><row><cell>BoW + SVM</cell><cell>92.43</cell><cell>93.02</cell><cell cols="2">45.24 40.70</cell></row><row><cell>Bigram + SVM</cell><cell>92.32</cell><cell>93.03</cell><cell cols="2">46.14 36.61</cell></row><row><cell>Average Embedding</cell><cell>89.39</cell><cell>86.89</cell><cell cols="2">41.32 32.70</cell></row><row><cell>ClassifyLDA-EM (Hingmire et al. 2013)</cell><cell>93.60</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Labeled-LDA (Li, Sun, and Zhang 2008)</cell><cell>-</cell><cell>90.80</cell><cell>-</cell><cell>-</cell></row><row><cell>CFG (Post and Bergsma 2013)</cell><cell>-</cell><cell>-</cell><cell>39.20</cell><cell>-</cell></row><row><cell>C&amp;J (Post and Bergsma 2013)</cell><cell>-</cell><cell>-</cell><cell>49.20</cell><cell>-</cell></row><row><cell>RecursiveNN (Socher et al. 2011b)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>43.20</cell></row><row><cell>RNTN (Socher et al. 2013)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>45.70</cell></row><row><cell>Paragraph-Vector (Le and Mikolov 2014)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>48.70</cell></row><row><cell>CNN</cell><cell>94.79</cell><cell>94.04</cell><cell cols="2">47.47 46.35</cell></row><row><cell>RCNN</cell><cell>96.49</cell><cell>95.20</cell><cell cols="2">49.19 47.21</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>) time to construct the representations of sentences, our model exhibits a lower time complexity of O(n). In practice, the training time of the RNTN as reported inSocher et al. (</figDesc><table><row><cell>approaches, which require O(n 2</cell></row><row><cell>• When we compare neural network approaches (Recur-</cell></row><row><cell>siveNN, CNN, and RCNN) to the widely used traditional</cell></row><row><cell>methods (e.g., BoW+LR), the experimental results show</cell></row><row><cell>that the neural network approaches outperform the tradi-</cell></row><row><cell>tional methods for all four datasets. It proves that neu-</cell></row><row><cell>ral network based approach can effective compose the se-</cell></row><row><cell>mantic representation of texts. Neural networks can cap-</cell></row><row><cell>ture more contextual information of features compared</cell></row><row><cell>with traditional methods based on BoW model, and may</cell></row><row><cell>suffer from the data sparsity problem less.</cell></row><row><cell>• When comparing CNNs and RCNNs to RecursiveNNs us-</cell></row><row><cell>ing the SST dataset, we can see that the convolution-based</cell></row><row><cell>approaches achieve better results. This illustrates that the</cell></row><row><cell>convolution-based framework is more suitable for con-</cell></row><row><cell>structing the semantic representation of texts compared</cell></row><row><cell>with previous neural networks. We believe the main rea-</cell></row><row><cell>son is that CNN can select more discriminative features</cell></row><row><cell>through the max-pooling layer and capture contextual in-</cell></row><row><cell>formation through convolutional layer. By contrast, Re-</cell></row><row><cell>cursiveNN can only capture contextual information us-</cell></row><row><cell>ing semantic composition under the constructed textual</cell></row><row><cell>tree, which heavily depends on the performance of tree</cell></row><row><cell>construction. Moreover, compared to the recursive-based</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of positive and negative features extracted by the RCNN and the RNTN tant words in the test set in Table</figDesc><table><row><cell></cell><cell>97</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>96.49</cell></row><row><cell></cell><cell>96</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Macro-F1</cell><cell>94 95</cell><cell cols="2">94.28</cell><cell cols="2">94.69</cell><cell cols="2">94.75</cell><cell cols="2">94.76</cell><cell>94.79</cell><cell>94.76</cell><cell>94.50</cell><cell>94.45</cell><cell>94.42</cell></row><row><cell></cell><cell>93</cell><cell>93.26 0 2</cell><cell cols="2">4</cell><cell cols="2">6</cell><cell cols="2">8</cell><cell cols="3">93.6 10 12 14 16 18 20</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Context window size</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">CNN</cell><cell></cell><cell></cell><cell cols="5">ClassifyLDA-EM</cell><cell>RCNN</cell></row><row><cell cols="12">Figure 2: Macro-F1 curve for how context window size in-</cell></row><row><cell cols="12">fluences the performance of the 20Newsgroups classifica-</cell></row><row><cell>tion</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>RCNN P well worth the; a wonderful movie; even stinging at; and invigorating film; and ingenious entertainment; and enjoy .; 's sweetest movie N A dreadful live-action; Extremely boring .; is n't a; 's painful .; Extremely dumb .; an awfully derivative; 's weaker than; incredibly dull .; very bad sign; RNTN P an amazing performance; most visually stunning; wonderful all-ages triumph; a wonderful movie N for worst movie; A lousy movie; a complete failure; most painfully marginal; very bad sign</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">qwone.com/˜jason/20Newsgroups/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">www.datatang.com/data/44139 and 43543</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">old-site.clsp.jhu.edu/˜sbergsma/Stylo/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">nlp.stanford.edu/sentiment/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">nlp.stanford.edu/software/tokenizer.shtml</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">ictclas.nlpir.org</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the anonymous reviewers for the constructive comments. This work was sponsored by the National Basic Research Program of China (No. 2014CB340503) and the National Natural Science Foundation of China (No. 61202329, 61272332).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey of text classification algorithms</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mining text data</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="163" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A Neural Probabilistic Language Model</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stochastic gradient learning in neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neuro-Nımes</title>
				<meeting>Neuro-Nımes</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">91</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Coarse-to-fine n-best parsing and maxent discriminative reranking</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL, 173-180</title>
				<imprint>
			<date type="published" when="2003">2003. 2005. 2011</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
		</imprint>
	</monogr>
	<note>SIGIR. Natural language processing (almost) from scratch. JMLR</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Elements of information theory</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Why does unsupervised pretraining help deep learning?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Document classification by topic labeling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hingmire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chougule</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Palshikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chakraborti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="877" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for discourse compositionality</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on CVSC</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="119" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inducing crosslingual distributed representations of words</title>
		<author>
			<persName><forename type="first">A</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bhattarai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coling</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1459" to="1474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An evaluation of phrasal and clustered representations on a text categorization task</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR, 37-50</title>
				<editor>
			<persName><surname>Icml</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1992">2014. 1992. 2008</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="620" to="627" />
		</imprint>
	</monogr>
	<note>Text classification based on labeled-lda model</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-T</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Statistical language models based on neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>Brno University of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Dissertation</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="246" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feature selection, l1 vs. l2 regularization, and rotational invariance</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coling-ACL</title>
				<imprint>
			<date type="published" when="2004">2004. 2006</date>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning sets of filters using back-propagation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Plaut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="61" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Explicit and implicit syntactic features for text classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bergsma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="866" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2011">2011a</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2011">2011b</date>
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="90" to="94" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
