<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients</title>
				<funder ref="#_efmXkcR">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Juntang</forename><surname>Zhuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tommy</forename><surname>Tang</surname></persName>
							<email>tommymt2@illinois.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yifan</forename><surname>Ding</surname></persName>
							<email>yf.ding@knights.ucf.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sekhar</forename><surname>Tatikonda</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nicha</forename><surname>Dvornek</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xenophon</forename><surname>Papademetris</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
							<email>james.duncan@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most popular optimizers for deep learning can be broadly categorized as adaptive methods (e.g. Adam) and accelerated schemes (e.g. stochastic gradient descent (SGD) with momentum). For many models such as convolutional neural networks (CNNs), adaptive methods typically converge faster but generalize worse compared to SGD; for complex settings such as generative adversarial networks (GANs), adaptive methods are typically the default because of their stability. We propose AdaBelief to simultaneously achieve three goals: fast convergence as in adaptive methods, good generalization as in SGD, and training stability. The intuition for AdaBelief is to adapt the stepsize according to the "belief" in the current gradient direction. Viewing the exponential moving average (EMA) of the noisy gradient as the prediction of the gradient at the next time step, if the observed gradient greatly deviates from the prediction, we distrust the current observation and take a small step; if the observed gradient is close to the prediction, we trust it and take a large step. We validate AdaBelief in extensive experiments, showing that it outperforms other methods with fast convergence and high accuracy on image classification and language modeling. Specifically, on ImageNet, AdaBelief achieves comparable accuracy to SGD. Furthermore, in the training of a GAN on Cifar10, AdaBelief demonstrates high stability and improves the quality of generated samples compared to a well-tuned Adam optimizer. Code is available at https://github.com/juntang-zhuang/Adabelief-Optimizer</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modern neural networks are typically trained with first-order gradient methods, which can be broadly categorized into two branches: the accelerated stochastic gradient descent (SGD) family <ref type="bibr" target="#b0">[1]</ref>, such as Nesterov accelerated gradient (NAG) <ref type="bibr" target="#b1">[2]</ref>, SGD with momentum <ref type="bibr" target="#b2">[3]</ref> and heavy-ball method (HB) <ref type="bibr" target="#b3">[4]</ref>; and the adaptive learning rate methods, such as Adagrad <ref type="bibr" target="#b4">[5]</ref>, AdaDelta <ref type="bibr" target="#b5">[6]</ref>, RMSProp <ref type="bibr" target="#b6">[7]</ref> and Adam <ref type="bibr" target="#b7">[8]</ref>. SGD methods use a global learning rate for all parameters, while adaptive methods compute an individual learning rate for each parameter.</p><p>Compared to the SGD family, adaptive methods typically converge fast in the early training phases, but have poor generalization performance <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Recent progress tries to combine the benefits of both, such as switching from Adam to SGD either with a hard schedule as in SWATS <ref type="bibr" target="#b10">[11]</ref>, or with a smooth transition as in AdaBound <ref type="bibr" target="#b11">[12]</ref>. Other modifications of Adam are also proposed: AMSGrad <ref type="bibr" target="#b12">[13]</ref> fixes the error in convergence analysis of Adam, Yogi <ref type="bibr" target="#b13">[14]</ref> considers the effect of minibatch size, MSVAG <ref type="bibr" target="#b14">[15]</ref> dissects Adam as sign update and magnitude scaling, RAdam <ref type="bibr" target="#b15">[16]</ref> rectifies the variance of learning rate, Fromage <ref type="bibr" target="#b16">[17]</ref> controls the distance in the function space, and AdamW <ref type="bibr" target="#b17">[18]</ref> decouples weight decay from gradient descent. Although these modifications achieve better accuracy compared to Adam, their generalization performance is typically worse than SGD on large-scale datasets such as ImageNet <ref type="bibr" target="#b18">[19]</ref>; furthermore, compared with Adam, many optimizers are empirically unstable when training generative adversarial networks (GAN) <ref type="bibr" target="#b19">[20]</ref>.</p><p>To solve the problems above, we propose "AdaBelief", which can be easily modified from Adam. Denote the observed gradient at step t as g t and its exponential moving average (EMA) as m t . Denote the EMA of g 2 t and (g t -m t ) 2 as v t and s t , respectively. m t is divided by ? v t in Adam, while it is divided by ? s t in AdaBelief. Intuitively, 1 ? st is the "belief" in the observation: viewing m t as the prediction of the gradient, if g t deviates much from m t , we have weak belief in g t , and take a small step; if g t is close to the prediction m t , we have a strong belief in g t , and take a large step. We validate the performance of AdaBelief with extensive experiments. Our contributions can be summarized as:</p><p>? We propose AdaBelief, which can be easily modified from Adam without extra parameters.</p><p>AdaBelief has three properties: <ref type="bibr" target="#b0">(1)</ref> fast convergence as in adaptive gradient methods, (2) good generalization as in the SGD family, and (3) training stability in complex settings such as GAN. ? We theoretically analyze the convergence property of AdaBelief in both convex optimization and non-convex stochastic optimization. ? We validate the performance of AdaBelief with extensive experiments: AdaBelief achieves fast convergence as Adam and good generalization as SGD in image classification tasks on CIFAR and ImageNet; AdaBelief outperforms other methods in language modeling; in the training of a W-GAN <ref type="bibr" target="#b20">[21]</ref>, compared to a well-tuned Adam optimizer, AdaBelief significantly improves the quality of generated images, while several recent adaptive optimizers fail the training.</p><p>2 Methods</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Details of AdaBelief Optimizer</head><p>Notations By the convention in <ref type="bibr" target="#b7">[8]</ref>, we use the following notations:</p><p>? f (?) ? R, ? ? R d : f is the loss function to minimize, ? is the parameter in R d ? F ,M (y) = argmin x?F ||M 1/2 (x -y)||: projection of y onto a convex feasible set F ? g t : the gradient at step t ? m t : exponential moving average (EMA) of g t ? v t , s t : v t is the EMA of g 2 t , s t is the EMA of (g t -m t ) 2 ? ?, : ? is the learning rate, default is 10 -3 ; is a small number, typically set as 10 -8</p><p>? ? 1 , ? 2 : smoothing parameters, typical values are ? 1 = 0.9, ? 2 = 0.999 ? ? 1t , ? 2t are the momentum for m t and v t respectively at step t, and typically set as constant (e.g.</p><formula xml:id="formula_0">? 1t = ? 1 , ? 2t = ? 2 , ?t ? {1, 2, ...T } Algorithm 1: Adam Optimizer Initialize ? 0 , m 0 ? 0 , v 0 ? 0, t ? 0 While ? t not converged t ? t + 1 g t ? ? ? f t (? t-1 ) m t ? ? 1 m t-1 + (1 -? 1 )g t v t ? ? 2 v t-1 + (1 -? 2 )g 2 t Bias Correction m t ? mt 1-? t 1 , v t ? vt 1-? t 2 Update ? t ? F , ? vt ? t-1 -? mt ? vt+</formula><p>Algorithm 2: AdaBelief Optimizer Initialize ? 0 , m 0 ? 0 , s 0 ? 0, t ? 0 While ? t not converged t ? t + 1</p><formula xml:id="formula_1">g t ? ? ? f t (? t-1 ) m t ? ? 1 m t-1 + (1 -? 1 )g t s t ? ? 2 s t-1 + (1 -? 2 )(g t -m t ) 2 Bias Correction m t ? mt 1-? t 1 , s t ? st+ 1-? t 2 Update ? t ? F , ? st ? t-1 -? mt ? st+</formula><p>Comparison with Adam Adam and AdaBelief are summarized in Algo. Intuitively, viewing m t as the prediction of g t , AdaBelief takes a large step when observation g t is close to prediction m t , and a small step when the observation greatly deviates from the prediction. . represents bias-corrected value. Note that an extra is added to s t during bias-correction, in order to Table <ref type="table">1</ref>: Comparison of optimizers in various cases in Fig. <ref type="figure" target="#fig_8">1</ref>. "S" and "L" represent "small" and "large" stepsize, respectively. |?? t | ideal is the stepsize of an ideal optimizer. Note that only AdaBelief matches the behaviour of an ideal optimizer in all three cases.</p><p>Case 1 Case 2 Case 3</p><formula xml:id="formula_2">|g t |, v t S L L |g t -g t-1 |, s t S L S |?? t | ideal L S L |?? t | SGD Adam AdaBelief SGD Adam AdaBelief SGD Adam AdaBelief S L L L S S L S L</formula><p>better match the assumption that s t is bouded below (the lower bound is at leat ). For simplicity, we omit the bias correction step in theoretical analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Intuitive explanation for benefits of AdaBelief</head><p>Figure <ref type="figure" target="#fig_8">1</ref>: An ideal optimizer considers curvature of the loss function, instead of taking a large (small) step where the gradient is large (small) <ref type="bibr" target="#b21">[22]</ref>.</p><p>AdaBelief uses curvature information Update formulas for SGD, Adam and AdaBelief are:</p><formula xml:id="formula_3">?? SGD t = -?m t , ?? Adam t = -?m t / ? v t , ?? AdaBelief t = -?m t / ? s t<label>(1)</label></formula><p>Note that we name ? as the "learning rate" and |?? i t | as the "stepsize" for the ith parameter. With a 1D example in Fig. <ref type="figure" target="#fig_8">1</ref>, we demonstrate that AdaBelief uses the curvature of loss functions to improve training as summarized in Table <ref type="table">1</ref>, with a detailed description below:</p><p>(1) In region 1 in Fig. <ref type="figure" target="#fig_8">1</ref>, the loss function is flat, hence the gradient is close to 0. In this case, an ideal optimizer should take a large stepsize. The stepsize of SGD is proportional to the EMA of the gradient, hence is small in this case; while both Adam and Ad-aBelief take a large stepsize, because the denominator ( ? v t and ? s t ) is a small value.</p><p>(2) In region 2 , the algorithm oscillates in a "steep and narrow" valley, hence both |g t | and |g t -g t-1 | is large. An ideal optimizer should decrease its stepsize, while SGD takes a large step (proportional to m t ). Adam and AdaBelief take a small step because the denominator ( ? s t and ? v t ) is large.</p><p>(3) In region 3 , we demonstrate AdaBelief's advantage over Adam in the "large gradient, small curvature" case. In this case, |g t | and v t are large, but |g t -g t-1 | and s t are small; this could happen because of a small learning rate ?. In this case, an ideal optimizer should increase its stepsize. SGD uses a large stepsize (? ?|g t |); in Adam, the denominator ? v t is large, hence the stepsize is small; in AdaBelief, denominator ? s t is small, hence the stepsize is large as in an ideal optimizer.</p><p>To sum up, AdaBelief scales the update direction by the change in gradient, which is related to the Hessian. Therefore, AdaBelief considers curvature information and performs better than Adam.</p><p>AdaBelief considers the sign of gradient in denominator We show the advantages of AdaBelief with a 2D example in this section, which gives us more intuition for high dimensional cases. In Fig. <ref type="figure" target="#fig_10">2</ref>, we consider the loss function: f (x, y) = |x| + |y|. Note that in this simple problem, the gradient in each axis can only take {1, -1}. Suppose the start point is near the x-axis, e.g. y 0 ? 0, x 0 0. Optimizers will oscillate in the y direction, and keep increasing in the x direction. Suppose the algorithm runs for a long time (t is large), so the bias of EMA (? t 1 Eg t ) is small:</p><formula xml:id="formula_4">m t = EM A(g 0 , g 1 , ...g t ) ? E(g t ), m t,x ? Eg t,x = 1, m t,y ? Eg t,y = 0<label>(2)</label></formula><formula xml:id="formula_5">v t = EM A(g 2 0 , g 2 1 , ...g 2 t ) ? E(g 2 t ), v t,x ? Eg 2 t,x = 1, v t,y ? Eg 2 t,y = 1.<label>(3)</label></formula><p>Step 1 2 3 4 5</p><formula xml:id="formula_6">g x 1 1 1 1 1 g y -1 1 -1 1 -1 Adam v x 1 1 1 1 1 v y 1 1 1 1 1 AdaBelief s x 0 0 0 0 0 s y 1 1 1 1 1</formula><p>Figure <ref type="figure" target="#fig_10">2</ref>: Left: Consider f (x, y) = |x| + |y|. Blue vectors represent the gradient, and the cross represents the optimal point. The optimizer oscillates in the y direction, and keeps moving forward in the x direction. Right: Optimization process for the example on the left. Note that denominator ? v t,x = ? v t,y for Adam, hence the same stepsize in x and y direction; while ? s t,x &lt; ? s t,y , hence</p><p>AdaBelief takes a large step in the x direction, and a small step in the y direction.</p><p>In practice, the bias correction step will further reduce the error between the EMA and its expectation if g t is a stationary process <ref type="bibr" target="#b7">[8]</ref>. Note that:</p><formula xml:id="formula_7">s t = EM A (g 0 -m 0 ) 2 , ...(g t -m t ) 2 ? E (g t -Eg t ) 2 = Varg t , s t,x ? 0, s t,y ? 1 (4)</formula><p>An example of the analysis above is summarized in Fig. <ref type="figure" target="#fig_10">2</ref>. From Eq. 3 and Eq. 4, note that in Adam, v x = v y ; this is because the update of v t only uses the amplitude of g t and ignores its sign, hence the stepsize for the x and y direction is the same 1/ ? v t,x = 1/ ? v t,y . AdaBelief considers both the magnitude and sign of g t , and 1/ ? s t,x 1/ ? s t,y , hence takes a large step in the x direction and a small step in the y direction, which matches the behaviour of an ideal optimizer.</p><p>Update direction in Adam is close to "sign descent" in low-variance case In this section, we demonstrate that when the gradient has low variance, the update direction in Adam is close to "sign descent", hence deviates from the gradient. This is also mentioned in <ref type="bibr" target="#b14">[15]</ref>.</p><p>Under the following assumptions: ( </p><formula xml:id="formula_8">= -? mt ? vt+ ? -? Egt ? (Egt) 2 +Vargt+ ? -? Egt ||Egt|| = -? sign(Eg t )<label>(5)</label></formula><p>In this case, Adam behaves like a "sign descent"; in 2D cases the update is ?45 ? to the axis, hence deviates from the true gradient direction. The "sign update" effect might cause the generalization gap between adaptive methods and SGD (e.g. on ImageNet) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b8">9]</ref>. For AdaBelief, when the variance of g t is the same for all coordinates, the update direction matches the gradient direction; when the variance is not uniform, AdaBelief takes a small (large) step when the variance is large (small).</p><p>Numerical experiments In this section, we validate intuitions in Sec. 2.2. Examples are shown in Fig. <ref type="figure" target="#fig_1">3</ref>, and we refer readers to more video examples<ref type="foot" target="#foot_1">1</ref> for better visualization. In all examples, compared with SGD with momentum and Adam, AdaBelief reaches the optimal point at the fastest speed. Learning rate is ? = 10 -3 for all optimizers. For all examples except Fig. <ref type="figure" target="#fig_11">3(d)</ref>, we set the parameters of AdaBelief to be the same as the default in Adam <ref type="bibr" target="#b7">[8]</ref>, ? 1 = 0.9, ? 2 = 0.999, = 10 -8 , and set momentum as 0.9 for SGD. For Fig. <ref type="figure" target="#fig_11">3(d)</ref>, to match the assumption in Sec. 2.2, we set   Above cases occur frequently in deep learning Although the above cases are simple, they give hints to local behavior of optimizers in deep learning, and we expect them to occur frequently in deep learning. Hence, we expect AdaBelief to outperform Adam in general cases. Other works in the literature <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12]</ref> claim advantages over Adam, but are typically substantiated with carefullyconstructed examples. Note that most deep networks use ReLU activation <ref type="bibr" target="#b25">[26]</ref>, which behaves like an absolute value function as in Fig. <ref type="figure" target="#fig_11">3(a)</ref>. Considering the interaction between neurons, most networks behave like case Fig. <ref type="figure" target="#fig_11">3(b)</ref>, and typically are ill-conditioned (the weight of some parameters are far larger than others) as in the figure. Considering a smooth loss function such as cross entropy or a smooth activation, this case is similar to Fig. <ref type="figure" target="#fig_11">3(c</ref>). The case with Fig. <ref type="figure" target="#fig_11">3(d</ref></p><formula xml:id="formula_9">? 1 = ? 2 = 0.</formula><formula xml:id="formula_10">) requires |m t | ? |Eg t |</formula><p>Varg t , and this typically occurs at the late stages of training, where the learning rate ? is decayed to a small value, and the network reaches a stable region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Convergence analysis in convex and non-convex optimization</head><p>Similar to <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27]</ref>, for simplicity, we omit the de-biasing step (analysis applicable to de-biased version). Proof for convergence in convex and non-convex cases is in the appendix.</p><p>Optimization problem For deterministic problems, the problem to be optimized is min ??F f (?); for online optimization, the problem is min ??F T t=1 f t (?), where f t can be interpreted as loss of the model with the chosen parameters in the t-th step.</p><p>Theorem 2.1. (Convergence in convex optimization) Let {? t } and {s t } be the sequence obtained by AdaBelief, let 0</p><formula xml:id="formula_11">? ? 2 &lt; 1, ? t = ? ? t , ? 11 = ? 1 , 0 ? ? 1t ? ? 1 &lt; 1, s t ? s t+1 , ?t ? [T ]. Let ? ? F, where F ? R d is a convex feasible set with bounded diameter D ? . Assume f (?) is a convex function and ||g t || ? ? G ? /2 (hence ||g t -m t || ? ? G ? ) and s t,i ? c &gt; 0, ?t ? [T ], ? ? F.</formula><p>Denote the optimal point as ? * . For ? t generated with AdaBelief, we have the following bound on the regret:</p><formula xml:id="formula_12">T t=1 [f t (? t ) -f t (? * )] ? D 2 ? ? T 2?(1 -? 1 ) d i=1 s 1/2 T,i + (1 + ? 1 )? ? 1 + log T 2 ? c(1 -? 1 ) 3 d i=1 g 2 1:T,i 2 + D 2 ? 2(1 -? 1 ) T t=1 d i=1 ? 1t s 1/2 t,i ? t Corollary 2.1.1. Suppose ? 1,t = ? 1 ? t , 0 &lt; ? &lt; 1 in Theorem (2.1</formula><p>), then we have:</p><formula xml:id="formula_13">T t=1 [f t (? t ) -f t (? * )] ? D 2 ? ? T 2?(1-?1) d i=1 s 1/2 T,i + (1+?1)? ? 1+log T 2 ? c(1-?1) 3 d i=1 g 2 1:T,i 2 + D 2 ? ?1G? 2(1-?1)(1-?) 2 ?</formula><p>For the convex case, Theorem 2.1 implies the regret of AdaBelief is upper bounded by O( ? T ). Conditions for Corollary 2.1.1 can be relaxed to ? 1,t = ? 1 /t as in <ref type="bibr" target="#b12">[13]</ref>, which still generates O( ? T ) regret. Similar to Theorem 4.1 in <ref type="bibr" target="#b7">[8]</ref> and corollary 1 in <ref type="bibr" target="#b12">[13]</ref>, where the term</p><formula xml:id="formula_14">d i=1 v 1/2 T,i exists, we have d i=1 s 1/2 T,i . Without further assumption, d i=1 s 1/2 T,i &lt; dG ? since ||g t -m t || ? &lt; G ?</formula><p>as assumed in Theorem 2.1, and dG ? is constant. The literature <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b4">5]</ref> exerts a stronger assumption that</p><formula xml:id="formula_15">d i=1 ? T v 1/2 T,i dG ? ? T .</formula><p>Our assumption could be similar or weaker, because</p><formula xml:id="formula_16">Es t = Varg t ? Eg 2 t = Ev t , then we get better regret than O( ? T ). Theorem 2.2.</formula><p>(Convergence for non-convex stochastic optimization) Under the assumptions:</p><formula xml:id="formula_17">? f is differentiable; ||?f (x) -?f (y)|| ? L||x -y||, ?x, y; f is also lower bounded.</formula><p>? The noisy gradient is unbiased, and has independent noise, i.e. g t = ?f (? t ) + ? t , E? t = 0, ? t ?? j , ?t, j ? N, t = j. ? At step t, the algorithm can access a bounded noisy gradient, and the true gradient is also bounded.</p><p>i</p><formula xml:id="formula_18">.e. ||?f (? t )|| ? H, ||g t || ? H, ?t &gt; 1. Assume min j?[d] (s 1 ) j ? c &gt; 0, noise in gradient has bounded variance, Var(g t ) = ? 2 t ? ? 2</formula><p>, ?t ? N, then the proposed algorithm satisfies:</p><formula xml:id="formula_19">min t?[T ] E ?f (? t ) 2 ? H ? T ? C1? 2 (H 2 +? 2 )(1+log T ) c + C 2 d? ? c + C 3 d? 2 c + C 4</formula><p>as in <ref type="bibr" target="#b26">[27]</ref>, C 1 , C 2 , C 3 are constants independent of d and T , and C 4 is a constant independent of T . Corollary 2.2.1. If c &gt; C 1 H and assumptions for Theorem 2.2 are satisfied, we have:</p><formula xml:id="formula_20">1 T T t=1 E ? 2 t ?f (? t ) 2 ? 1 T 1 1 H - C 1 c C1? 2 ? 2 c 1 + log T + C 2 d? ? c + C 3 d? 2 c + C 4</formula><p>Theorem 2.2 implies the convergence rate for AdaBelief in the non-convex case is O(log T / ? T ), which is similar to Adam-type optimizers <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27]</ref>. Note that regret bounds are derived in the worst possible case, while empirically AdaBelief outperforms Adam mainly because the cases in Sec. 2.2 occur more frequently. It is possible that the above bounds are loose; we will try to derive a tighter bound in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We performed extensive comparisons with other optimizers, including SGD <ref type="bibr" target="#b2">[3]</ref>, AdaBound <ref type="bibr" target="#b11">[12]</ref>, Yogi <ref type="bibr" target="#b13">[14]</ref>, Adam <ref type="bibr" target="#b7">[8]</ref>, MSVAG <ref type="bibr" target="#b14">[15]</ref>, RAdam <ref type="bibr" target="#b15">[16]</ref>, Fromage <ref type="bibr" target="#b16">[17]</ref> and AdamW <ref type="bibr" target="#b17">[18]</ref>. The experiments include: (a) image classification on Cifar dataset <ref type="bibr" target="#b27">[28]</ref> with VGG <ref type="bibr" target="#b28">[29]</ref>, ResNet <ref type="bibr" target="#b29">[30]</ref> and DenseNet <ref type="bibr" target="#b30">[31]</ref>, and image recognition with ResNet on ImageNet <ref type="bibr" target="#b31">[32]</ref>; (b) language modeling with LSTM <ref type="bibr" target="#b32">[33]</ref> on Penn TreeBank dataset <ref type="bibr" target="#b33">[34]</ref>; (c) wasserstein-GAN (WGAN) <ref type="bibr" target="#b20">[21]</ref> on Cifar10 dataset. We emphasize (c) because prior work focuses on convergence and accuracy, yet neglects training stability.</p><p>Hyperparameter tuning We performed a careful hyperparameter tuning in experiments. On image classification and language modeling we use the following:</p><p>? AdaBelief: We use the default parameters of Adam: ? 1 = 0.9, ? 2 = 0.999, = 10 -8 , ? = 10 -3 .</p><p>? SGD, Fromage: We set the momentum as 0.9, which is the default for many networks such as ResNet <ref type="bibr" target="#b29">[30]</ref> and DenseNet <ref type="bibr" target="#b30">[31]</ref>. We search learning rate among {10.0, 1.0, 0.1, 0.01, 0.001}. ? Adam, Yogi, RAdam, MSVAG, AdaBound: We search for optimal ? 1 among {0.5, 0.6, 0.7, 0.8, 0.9},   search for ? as in SGD, and set other parameters as their own default values in the literature.</p><p>? AdamW: We use the same parameter searching scheme as Adam. For other optimizers, we set the weight decay as 5 ? 10 -4 ; for AdamW, since the optimal weight decay is typically larger <ref type="bibr" target="#b17">[18]</ref>, we search weight decay among {10 -4 , 5 ? 10 -4 , 10 -3 , 10 -2 }.</p><p>For the training of a GAN, we set ? 1 = 0.5, = 10 -12 for AdaBelief in a small GAN with vanilla CNN generator, and use = 10 -16 for a larger spectral normalization GAN (SN-GAN) with a ResNet generator; for other methods, we search for ? 1 among {0.5, 0.6, 0.7, 0.8, 0.9}, and search for among {10 -3 , 10 -5 , 10 -8 , 10 -10 , 10 -12 }. We set learning rate as 2 ? 10 -4 for all methods. Note that the recommended parameters for Adam <ref type="bibr" target="#b35">[36]</ref> and for RMSProp <ref type="bibr" target="#b36">[37]</ref> are within the search range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNNs on image classification</head><p>We experiment with VGG11, ResNet34 and DenseNet121 on Cifar10 and Cifar100 dataset. We use the official implementation of AdaBound, hence achieved an exact replication of <ref type="bibr" target="#b11">[12]</ref>. For each optimizer, we search for the optimal hyperparameters, and report the mean and standard deviation of test-set accuracy (under optimal hyperparameters) for 3 runs with random initialization. As Fig. <ref type="figure" target="#fig_3">4</ref> shows, AdaBelief achieves fast convergence as in adaptive methods such as Adam while achieving better accuracy than SGD and other methods.</p><p>We then train a ResNet18 on ImageNet, and report the accuracy on the validation set in Table <ref type="table" target="#tab_3">2</ref>. Due to the heavy computational burden, we could not perform an extensive hyperparameter search; instead, we report the result of AdaBelief with the default parameters of Adam (? 1 = 0.9, ? 2 = 0.999, = 10 -8 ) and decoupled weight decay as in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref>; for other optimizers, we report the best result in the literature. AdaBelief outperforms other adaptive methods and achieves comparable accuracy to SGD (70.08 v.s. 70.23), which closes the generalization gap between adaptive methods and SGD. Experiments validate the fast convergence and good generalization performance of AdaBelief.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSTM on language modeling</head><p>We experiment with LSTM on the Penn TreeBank dataset <ref type="bibr" target="#b33">[34]</ref>, and report the perplexity (lower is better) on the test set in Fig. <ref type="figure" target="#fig_4">5</ref>. We report the mean and standard deviation across 3 runs. For both 2-layer and 3-layer LSTM models, AdaBelief achieves the lowest  Generative adversarial networks Stability of optimizers is important in practice such as training of GANs, yet recently proposed optimizers often lack experimental validations. The training of a GAN alternates between generator and discriminator in a mini-max game, and is typically unstable <ref type="bibr" target="#b19">[20]</ref>; SGD often generates mode collapse, and adaptive methods such as Adam and RMSProp are recommended in practice <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref>. Therefore, training of GANs is a good test for the stability.</p><p>We experiment with one of the most widely used models, the Wasserstein-GAN (WGAN) <ref type="bibr" target="#b20">[21]</ref> and the improved version with gradient penalty (WGAN-GP) <ref type="bibr" target="#b36">[37]</ref> using a small model with vanilla CNN generator. Using each optimizer, we train the model for 100 epochs, generate 64,000 fake images from noise, and compute the Frechet Inception Distance (FID) <ref type="bibr" target="#b39">[40]</ref> between the fake images and real dataset (60,000 real images). FID score captures both the quality and diversity of generated images and is widely used to assess generative models (lower FID is better). For each optimizer, under its optimal hyperparameter settings, we perform 5 runs of experiments, and report the results in Fig. <ref type="figure" target="#fig_5">6</ref> and Fig. <ref type="figure" target="#fig_6">7</ref>. AdaBelief significantly outperforms other optimizers, and achieves the lowest FID score.</p><p>Besides the small model above, we also experiment with a large model using a ResNet generator and spectral normalization in the discriminator (SN-GAN). Results are summarized in Table . 3.</p><p>Compared with a vanilla GAN, all FID scores are lower because the SN-GAN is more advanced. Compared with other optimizers, AdaBelief achieves the lowest FID with both large and small GANs.</p><p>Remarks Recent research on optimizers tries to combine the fast convergence of adaptive methods with high accuracy of SGD. AdaBound <ref type="bibr" target="#b11">[12]</ref> achieves this goal on Cifar, yet its performance on ImageNet is still inferior to SGD <ref type="bibr" target="#b34">[35]</ref>. Padam <ref type="bibr" target="#b34">[35]</ref> closes this generalization gap on ImageNet; writing the update as ? t+1 = ? t -?m t /v p t , SGD sets p = 0, Adam sets p = 0.5, and Padam searches p between 0 and 0.5 (outside this region Padam diverges <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b40">41]</ref>). Intuitively, compared to Adam, by using a smaller p, Padam sacrifices the adaptivity for better generalization as in SGD; however, without good adaptivity, Padam loses training stability. As in Table <ref type="table" target="#tab_5">4</ref>, compared with Padam, AdaBelief achieves a much lower FID score in the training of GAN, meanwhile achieving slightly higher accuracy on ImageNet classification. Furthermore, AdaBelief has the same number of parameters as Adam, while Padam has one more parameter hence is harder to tune.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related works</head><p>This work considers the update step in first-order methods. Other directions include Lookahead <ref type="bibr" target="#b41">[42]</ref> which updates "fast" and "slow" weights separately, and is a wrapper that can combine with other optimizers; variance reduction methods <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref> which reduce the variance in gradient; and LARS <ref type="bibr" target="#b45">[46]</ref> which uses a layer-wise learning rate scaling. AdaBelief can be combined with these methods.</p><p>Other variants of Adam have also been proposed (e.g. NosAdam <ref type="bibr" target="#b46">[47]</ref>, Sadam <ref type="bibr" target="#b47">[48]</ref> and Adax <ref type="bibr" target="#b48">[49]</ref>).</p><p>Besides first-order methods, second-order methods (e.g. Newton's method <ref type="bibr" target="#b49">[50]</ref>, Quasi-Newton method and Gauss-Newton method <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b50">51]</ref>, L-BFGS <ref type="bibr" target="#b52">[53]</ref>, Natural-Gradient <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55]</ref>, Conjugate-Gradient <ref type="bibr" target="#b55">[56]</ref>) are widely used in conventional optimization. Hessian-free optimization (HFO) <ref type="bibr" target="#b56">[57]</ref> uses second-order methods to train neural networks. Second-order methods typically use curvature information and are invariant to scaling <ref type="bibr" target="#b57">[58]</ref> but have heavy computational burden, and hence are not widely used in deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose the AdaBelief optimizer, which adaptively scales the stepsize by the difference between predicted gradient and observed gradient. To our knowledge, AdaBelief is the first optimizer to achieve three goals simultaneously: fast convergence as in adaptive methods, good generalization as in SGD, and training stability in complex settings such as GANs. Furthermore, Adabelief has the same parameters as Adam, hence is easy to tune. We validate the benefits of AdaBelief with intuitive examples, theoretical convergence analysis in both convex and non-convex cases, and extensive experiments on real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>Optimization is at the core of modern machine learning, and numerous efforts have been put into it.</p><p>To our knowledge, AdaBelief is the first optimizer to achieve fast speed, good generalization and training stability. Adabelief can be used for the training of all models that can numerically estimate parameter gradients, hence can boost the development and application of deep learning models.</p><p>This work mainly focuses on the theory part, and the social impact is mainly determined by each application rather than by optimizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Detailed Algorithm of AdaBelief</head><p>Notations By the convention in <ref type="bibr" target="#b7">[8]</ref>, we use the following notations:</p><p>? f (?) ? R, ? ? R d : f is the loss function to minimize, ? is the parameter in R d ? g t : the gradient and step t ? ?, : ? is the learning rate, default is 10 -3 ; is a small number, typically set as 10 -8</p><p>? ? 1 , ? 2 : smoothing parameters, typical values are ? 1 = 0.9, ? 2 = 0.999</p><formula xml:id="formula_21">? m t : exponential moving average (EMA) of g t ? v t , s t : v t is the EMA of g 2 t , s t is the EMA of (g t -m t ) 2 ? F ,M (y) = argmin x?F ||M 1/2 (x -y)|| Algorithm 1: AdaBelief Initialize ? 0 m 0 ? 0 , s 0 ? 0, t ? 0 While ? t not converged t ? t + 1 g t ? ? ? f t (? t-1 ) m t ? ? 1 m t-1 + (1 -? 1 )g t s t ? ? 2 s t-1 + (1 -? 2 )(g t -m t ) 2 If AM SGrad s t ? max(s t , s t-1 ) Bias Correction m t ? m t /(1 -? t 1 ), s t ? (s t + )/(1 -? t 2 ) Update ? t ? F , ? st ? t-1 -m t ? ? st+ B.</formula><p>Convergence analysis in convex online learning case (Theorem 2.1 in main paper)</p><p>For the ease of notation, we absorb into s t . Equivalently, s t ? c &gt; 0, ?t ? [T ]. For simplicity, we omit the debiasing step in theoretical analysis as in <ref type="bibr" target="#b12">[13]</ref>. Our analysis can be applied to the de-biased version as well.</p><p>Lemma .1.</p><p>[?] For any Q ? S d + and convex feasible set</p><formula xml:id="formula_22">F ? R d , suppose u 1 = min x?F Q 1/2 (x- z 1 ) and u 2 = min x?F Q 1/2 (x -z 2 ) , then we have Q 1/2 (u 1 -u 2 ) ? Q 1/2 (z 1 -z 2 ) .</formula><p>Theorem .2. Let {? t } and {s t } be the sequence obtained by the proposed algorithm, let</p><formula xml:id="formula_23">0 ? ? 2 &lt; 1, ? t = ? ? t , ? 11 = ? 1 , 0 ? ? 1t ? ? 1 &lt; 1, s t-1 ? s t , ?t ? [T ]. Let ? ? F, where F ? R d is a convex feasible set with bounded diameter D ? . Assume f (?) is a convex function and ||g t || ? ? G ? /2 (hence ||g t -m t || ? ? G ? ) and s t,i ? c &gt; 0, ?t ? [T ], ? ? F.</formula><p>Denote the optimal point as ? * . For ? t generated with Algorithm 1, we have the following bound on the regret:</p><formula xml:id="formula_24">T t=1 f t (? t ) -f t (? * ) ? D 2 ? ? T 2?(1 -? 1 ) d i=1 s 1/2 T,i + (1 + ? 1 )? ? 1 + log T 2 ? c(1 -? 1 ) 3 d i=1 g 2 1:T,i 2 + D 2 ? 2(1 -? 1 ) T t=1 d i=1 ? 1t s 1/2 t,i ? t Proof: ? t+1 = F , ? st (? t -? t s -1/2 t m t ) = min ??F s 1/4 t [? -(? t -? t s -1/2 t m t )]</formula><p>Note that F , ? st (? * ) = ? * since ? * ? F. Use ? * i and ? t,i to denote the ith dimension of ? * and ? t respectively. From lemma (.1), using u 1 = ? t+1 and u 2 = ? * , we have:</p><formula xml:id="formula_25">s 1/4 t (? t+1 -? * ) 2 ? s 1/4 t (? t -? t s -1/2 t m t -? * ) 2 = s 1/4 t (? t -? * ) 2 + ? 2 t s -1/4 t m t 2 -2? t m t , ? t -? * = s 1/4 t (? t -? * ) 2 + ? 2 t s -1/4 t m t 2 -2? t ? 1t m t-1 + (1 -? 1t )g t , ? t -? *</formula><p>(1) Note that ? 1 ? [0, 1) and ? 2 ? [0, 1), rearranging inequality (1), we have:</p><formula xml:id="formula_26">g t , ? t -? * ? 1 2? t (1 -? 1t ) s 1/4 t (? t -? * ) 2 -s 1/4 t (? t+1 -? * ) 2 + ? t 2(1 -? 1t ) s -1/4 t m t 2 - ? 1t 1 -? 1t m t-1 , ? t -? * ? 1 2? t (1 -? 1t ) s 1/4 t (? t -? * ) 2 -s 1/4 t (? t+1 -? * ) 2 + ? t 2(1 -? 1t ) s -1/4 t m t 2 + ? 1t 2(1 -? 1t ) ? t s -1/4 t m t-1 2 + ? 1t 2? t (1 -? 1t ) s 1/4 t (? t -? * ) 2</formula><p>Cauchy-Schwartz and Young's inequality:</p><formula xml:id="formula_27">ab ? a 2 2 + b 2 2 , ? &gt; 0<label>(2)</label></formula><p>By convexity of f , we have:</p><formula xml:id="formula_28">T t=1 f t (? t ) -f t (? * ) ? T t=1 g t , ? t -? * ? T t=1 1 2? t (1 -? 1t ) s 1/4 t (? t -? * ) 2 -s 1/4 t (? t+1 -? * ) 2 + 1 2(1 -? 1t ) ? t s -1/4 t m t 2 + ? 1t 2(1 -? 1t ) ? t s -1/4 t m t-1 2 + ? 1t 2? t (1 -? 1t ) s 1/4 t (? t -? * ) 2 By f ormula (2) ? 1 2(1 -? 1 ) s 1/4 1 (? 1 -? * ) 2 ? 1 + 1 2(1 -? 1 ) T t=2 s 1/4 t (? t -? * ) 2 ? t - s 1/4 t-1 (? t -? * ) 2 ? t-1 + T t=1 1 2(1 -? 1 ) ? t s -1/4 t m t 2 + T t=2 ? 1 2(1 -? 1 ) ? t-1 s -1/4 t-1 m t-1 2 + T t=1 ? 1t 2? t (1 -? 1t ) s 1/4 t (? t -? * ) 2 0 ? s t-1 ? s t , 0 ? ? t ? ? t-1 , 0 ? ? 1t ? ? 1 &lt; 1 ? 1 2(1 -? 1 ) s 1/4 1 (? 1 -? * ) 2 ? 1 + 1 2(1 -? 1 ) T t=2 ? t -? * 2 s 1/2 t ? t - s 1/2 t-1 ? t-1 + 1 + ? 1 2(1 -? 1 ) T t=1 ? t s -1/4 t m t 2 + T t=1 ? 1t 2? t (1 -? 1t ) s 1/4 t (? t -? * ) 2 ? 1 2(1 -? 1 ) s 1/4 1 (? 1 -? * ) 2 ? 1 + 1 2(1 -? 1 ) T t=2 ? t -? * 2 s 1/2 t ? t - s 1/2 t-1 ? t-1 + 1 + ? 1 2(1 -? 1 ) T t=1 ? t s -1/4 t m t 2 + 1 2(1 -? 1 ) T t=1 ? 1t ? t s 1/4 t (? t -? * ) 2 since 0 ? ? 1t ? ? 1 &lt; 1 (3) Now bound T t=1 ? t ||s -1/4 t m t || 2 in Formula (3), assuming 0 &lt; c ? s t , ?t ? [T ]. T t=1 ? t s -1/4 t m t 2 = T -1 t=1 ? t s -1/4 t m t 2 + ? T s -1/4 T m T 2 ? T -1 t=1 ? t s -1/4 t m t 2 + ? T ? c m T 2 = T -1 t=1 ? t s -1/4 t m t 2 + ? ? cT d i=1 T j=1 (1 -? 1,j )g j,i T -j k=1 ? 1,T -k+1 2 since m T = T j=1 (1 -? 1,j )g j,i T -j k=1 ? 1,T -k+1 ? T -1 t=1 ? t s -1/4 t m t 2 + ? ? cT d i=1 T j=1 g j,i T -j k=1 ? 1 2 (since 0 &lt; ? 1,j ? ? 1 &lt; 1) = T -1 t=1 ? t s -1/4 t m t 2 + ? ? cT d i=1 T j=1 ? T -j 1 g j,i 2 ? T -1 t=1 ? t s -1/4 t m t 2 + ? ? cT d i=1 T j=1 ? T -j 1 T j=1 ? T -j 1 g 2 j,i Cauchy -Schwartz, u, v 2 ? u 2 v 2 , u j = ? T -j 1 , v j = ? T -j 1 g j,i = T -1 t=1 ? t s -1/4 t m t 2 + ? ? cT d i=1 1 -? T 1 1 -? 1 T j=1 ? T -j 1 g 2 j,i ? T -1 t=1 ? t s -1/4 t m t 2 + ? ? c(1 -? 1 ) d i=1 T j=1 ? T -j 1 g 2 j,i 1 ? T since 1 -? T 1 &lt; 1 ? ? ? c(1 -? 1 ) d i=1 T t=1 t j=1 ? t-j 1 g 2 j,i 1 ? t</formula><p>Recursively bound each term in the sum</p><formula xml:id="formula_29">T t=1 * = ? ? c(1 -? 1 ) d i=1 T t=1 g 2 t,i T j=t ? j-t 1 ? j ? ? ? c(1 -? 1 ) d i=1 T t=1 g 2 t,i T j=t ? j-t 1 ? t ? ? ? c(1 -? 1 ) 2 d i=1 T t=1 g 2 t,i 1 ? t since T j=t ? j-t 1 = T -t j=0 ? j 1 = 1 -? T -t+1 1 1 -? 1 ? 1 1 -? 1 ? ? ? c(1 -? 1 ) 2 d i=1 g 2 1:T,i 2 T t=1 1 t Cauchy -Schwartz, u, v ? u v , u t = g 2 t,i , v t = 1 ? t ? ? ? 1 + log T ? c(1 -? 1 ) 2 d i=1 g 2 1:T,i 2 since T t=1 1 t ? 1 + log T<label>(4)</label></formula><p>Apply formula ( <ref type="formula" target="#formula_29">4</ref>) to (3), we have:</p><formula xml:id="formula_30">T t=1 f t (? t ) -f t (? * ) ? 1 2(1 -? 1 ) s 1/4 1 (? 1 -? * ) 2 ? 1 + 1 2(1 -? 1 ) T t=2 ? t -? * 2 s 1/2 t ? t - s 1/2 t-1 ? t-1 + 1 + ? 1 2(1 -? 1 ) T t=1 ? t s -1/4 t m t 2 + 1 2(1 -? 1 ) T t=1 ? 1t ? t s 1/4 t (? t -? * ) 2 ? 1 2(1 -? 1 ) s 1/4 1 (? 1 -? * ) 2 ? 1 + 1 2(1 -? 1 ) T t=2 ? t -? * 2 s 1/2 t ? t - s 1/2 t-1 ? t-1 + (1 + ? 1 )? ? 1 + log T 2 ? c(1 -? 1 ) 3 d i=1 g 2 1:T,i 2 + 1 2(1 -? 1 ) T t=1 ? 1t ? t s 1/4 t (? t -? * ) 2 By formula (4) ? 1 2(1 -? 1 ) d i=1 s 1/2 1,i D 2 ? ? 1 + 1 2(1 -? 1 ) T t=2 d i=1 D 2 ? s 1/2 t,i ? t - s 1/2 t-1,i ? t-1 + (1 + ? 1 )? ? 1 + log T 2 ? c(1 -? 1 ) 3 d i=1 g 2 1:T,i 2 + D 2 ? 2(1 -? 1 ) T t=1 d i=1 ? 1t s 1/2 t,i ? t since x ? F, with bounded diameter D ? ,<label>and</label></formula><formula xml:id="formula_31">s 1/2 t,i ? t ? s 1/2 t-1,i ? t-1 by assumption. ? D 2 ? ? T 2?(1 -? 1 ) d i=1 s 1/2 T,i + (1 + ? 1 )? ? 1 + log T 2 ? c(1 -? 1 ) 3 d i=1 g 2 1:T,i 2 + D 2 ? 2(1 -? 1 ) T t=1 d i=1 ? 1t s 1/2 t,i ? t</formula><p>? t ? ? t+1 and perform telescope sum (5)</p><formula xml:id="formula_32">Corollary .2.1. Suppose ? 1,t = ? 1 ? t , 0 &lt; ? &lt; 1 in Theorem (.2</formula><p>), then we have:</p><formula xml:id="formula_33">T t=1 f t (? t ) -f t (? * ) ? D 2 ? ? T 2?(1 -? 1 ) d i=1 s 1/2 T,i + (1 + ? 1 )? ? 1 + log T 2 ? c(1 -? 1 ) 3 d i=1 g 2 1:T,i 2 + D 2 ? ? 1 G ? 2(1 -? 1 )(1 -?) 2 ?<label>(6)</label></formula><p>Proof: By sum of arithmetico-geometric series, we have:</p><formula xml:id="formula_34">T t=1 ? t-1 ? t ? T t=1 ? t-1 t ? 1 (1 -?) 2<label>(7)</label></formula><p>Plugging ( <ref type="formula" target="#formula_34">7</ref>) into (5), we can derive the results above.</p><p>C. Convergence analysis for non-convex stochastic optimization (Theorem 2.2 in main paper)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assumptions</head><p>? A1, f is differentiable and has L -Lipschitz gradient, ||?f (x) -?f (y)|| ? L||x -y||, ?x, y. f is also lower bounded. ? A2, at time t, the algorithm can access a bounded noisy gradient, the true gradient is also bounded. i.e. ||?f (? t )|| ? H, ||g t || ? H, ?t &gt; 1. ? A3, The noisy gradient is unbiased, and has independent noise. i.e. g t = ?f (? t )+? t , E? t = 0, ? t ?? j , ?j, t ? N, t = j Theorem .3. <ref type="bibr" target="#b26">[27]</ref> Suppose assumptions A1-A3 are satisfied, ? 1,t is chosen such that 0 ? ? 1,t+1 ?</p><formula xml:id="formula_35">? 1,t &lt; 1, 0 &lt; ? 2 &lt; 1, ?t &gt; 0. For some constant G, ? t mt ? st ? G, ?t. Then Adam-type algorithms yield E T t=1 ? t ?f (? t ), ?f (? t )/ ? s t ? E C 1 T t=1 ? t g t / ? s t 2 + C 2 T t=1 ? t ? s t - ? t-1 ? s t-1 1 + C 3 T t=1 ? t ? s t - ? t-1 ? s t-1 2 + C 4 (8)</formula><p>where C 1 , C 2 , C 3 are constants independent of d and T , C 4 is a constant independent of T , the expectation is taken w.r.t all randomness corresponding to {g t }. Furthermore, let ? t := min j?[d] min {gi} t i=1 ? i /( ? s i ) j denote the minimum possible value of effective stepsize at time t over all possible coordinate and past gradients {g i } t i=1 . The convergence rate of Adam-type algorithm is given by</p><formula xml:id="formula_36">min t?[T ] E ?f (? t ) 2 = O s 1 (T ) s 2 (T )<label>(9)</label></formula><p>where s 1 (T ) is defined through the upper bound of RHS of (8), and</p><formula xml:id="formula_37">T t=1 ? t = ?(s 2 (T ))</formula><p>Proof: We provide the proof from <ref type="bibr" target="#b26">[27]</ref> in next section for completeness.</p><p>Theorem .4. Assume min j?[d] (s 1 ) j ? c &gt; 0, noise in gradient has bounded variance, Var(g t ) = ? 2 t ? ? 2 , ?t ? N, then the AdaBelief algorithm satisfies:</p><formula xml:id="formula_38">min t?[T ] E ?f (? t ) 2 ? H ? T ? C 1 ? 2 (H 2 + ? 2 )(1 + log T ) c + C 2 d? ? c + C 3 d? 2 c + C 4 = 1 ? T (Q 1 + Q 2 log T )</formula><p>where</p><formula xml:id="formula_39">Q 1 = H ? C 1 ? 2 (H 2 + ? 2 ) c + C 2 d? ? c + C 3 d? 2 c + C 4 Q 2 = HC 1 ?(H 2 + ? 2 ) c</formula><p>Proof: We first derive an upper bound of the RHS of formula ( <ref type="formula">8</ref>), then derive a lower bound of the LHS of ( <ref type="formula">8</ref>).</p><formula xml:id="formula_40">E T t=1 ? t g t / ? s t 2 ? 1 c E T t=1 d i=1 (? t,i g t,i ) 2 since 0 &lt; c ? s t , ?t ? [T ] = 1 c d i=1 T t=1 ? 2 t E(g t,i ) 2 = 1 c T t=1 ? 2 t E ?f (? t ) 2 + ? t 2<label>(10)</label></formula><formula xml:id="formula_41">E T t=1 ? t ? s t - ? t-1 ? s t-1 1 = E d i=1 T t=1 ? t-1 ? s t-1,i - ? t ? s t , i since ? t ? ? t-1 , s t,i ? s t-1,i = E d i=1 ? 1 ? s 1,i - ? T ? s T,i ? E d i=1 ? 1 ? s 1,i ? d? ? c since 0 &lt; c ? s t , 0 ? ? t ? ? 1 = ?, ?t<label>(11)</label></formula><formula xml:id="formula_42">E T t=1 ? t ? s t - ? t-1 ? s t-1 2 = E T t=1 d i=1 ? t ? s t - ? t-1 ? s t-1 2 i ? E T t=1 d i=1 ? t ? s t - ? t-1 ? s t-1 i ? ? c Since ? t ? s t - ? t-1 ? s t-1 = ? t-1 ? s t-1 - ? t ? s t ? ? t-1 ? s t-1 ? ? ? c ? d? 2 c By (11)<label>(12)</label></formula><p>Next we derive the lower bound of LHS of ( <ref type="formula">8</ref>).</p><formula xml:id="formula_43">E T t=1 ? t ?f (? t ), ?f (? t ) ? s t ? 1 H E T t=1 ? t ?f (? t ) 2 ? ? ? T H min t?[T ] E ?f (? t ) 2<label>(13)</label></formula><p>Combining ( <ref type="formula" target="#formula_40">10</ref>), ( <ref type="formula" target="#formula_41">11</ref>), ( <ref type="formula" target="#formula_42">12</ref>) and ( <ref type="formula" target="#formula_43">13</ref>) to (8), we have:</p><formula xml:id="formula_44">? ? T H min t?[T ] E ?f (? t ) 2 ? E T t=1 ? t ?f (? t ), ?f (? t ) ? s t ? E C 1 T t=1 ? t g t / ? s t 2 + C 2 T t=1 ? t ? s t - ? t-1 ? s t-1 1 + C 3 T t=1 ? t ? s t - ? t-1 ? s t-1 2 + C 4 ? C 1 c T t=1 E ? 2 t ?f (? t ) 2 + ? 2 t ? t 2 + C 2 d? ? c + C 3 d? 2 c + C 4<label>(14)</label></formula><formula xml:id="formula_45">? C 1 c T t=1 E ? 2 t (H 2 + ? 2 ) + C 2 d? ? c + C 3 d? 2 c + C 4 ? C 1 ? 2 (H 2 + ? 2 )(1 + log T ) c + C 2 d? ? c + C 3 d? 2 c + C 4 (<label>15</label></formula><formula xml:id="formula_46">)</formula><formula xml:id="formula_47">since ? t = ? ? t , T t=1 1 t ? 1 + log T</formula><p>Re-arranging above inequality, we have</p><formula xml:id="formula_48">min t?[T ] E ?f (? t ) 2 ? H ? T ? C 1 ? 2 (H 2 + ? 2 )(1 + log T ) c + C 2 d? ? c + C 3 d? 2 c + C 4 = 1 ? T (Q 1 + Q 2 log T )<label>(16)</label></formula><p>where</p><formula xml:id="formula_49">Q 1 = H ? C 1 ? 2 (H 2 + ? 2 ) c + C 2 d? ? c + C 3 d? 2 c + C 4<label>(17)</label></formula><formula xml:id="formula_50">Q 2 = HC 1 ?(H 2 + ? 2 ) c<label>(18)</label></formula><p>Corollary .4.1. If c &gt; C 1 H and assumptions for Theorem .3 are satisfied, we have:</p><formula xml:id="formula_51">1 T T t=1 E ? 2 t ?f (? t ) 2 ? 1 T 1 1 H -C1 c C 1 ? 2 ? 2 c 1 + log T + C 2 d? ? c + C 3 d? 2 c + C 4<label>(19)</label></formula><p>Proof: From ( <ref type="formula" target="#formula_43">13</ref>) and ( <ref type="formula" target="#formula_44">14</ref>), we have</p><formula xml:id="formula_52">1 H E T t=1 ? t ?f (? t ) 2 ? E T t=1 ? t ?f (? t ), ?f (? t ) ? s t ? C 1 c T t=1 E ? 2 t ?f (? t ) 2 + ? 2 t ? t 2 + C 2 d? ? c + C 3 d? 2 c + C 4<label>(20)</label></formula><p>By re-arranging, we have</p><formula xml:id="formula_53">1 H - C 1 c T t=1 E ? 2 t ?f (? t ) 2 ? C 1 c T t=1 E ? 2 t ? t 2 + C 2 d? ? c + C 3 d? 2 c + C 4 ? C 1 ? 2 ? 2 c 1 + log T + C 2 d? ? c + C 3 d? 2 c + C 4<label>(21)</label></formula><p>By assumption, 1 H -C1 c &gt; 0, then we have</p><formula xml:id="formula_54">T t=1 E ? 2 t ?f (? t ) 2 ? 1 1 H -C1 c C 1 ? 2 ? 2 c 1 + log T + C 2 d? ? c + C 3 d? 2 c + C 4<label>(22)</label></formula><p>D. Proof of Theorem .3</p><p>Lemma .5. <ref type="bibr" target="#b26">[27]</ref> Let ? 0 ? 1 in the Algorithm, consider the sequence</p><formula xml:id="formula_55">z t = ? t + ? 1,t 1 -? 1,t (? t -? t-1 ), ?t ? 2</formula><p>The following holds true:</p><formula xml:id="formula_56">z t+1 -z t = - ? 1,t+1 1 -? 1,t+1 - ? 1,t 1 -? 1,t ? t m t ? s t - ? 1,t 1 -? 1,t ? t ? s t - ? t-1 ? s t-1 m t-1 - ? t g t ? s t , ?t &gt; 1 (<label>23</label></formula><formula xml:id="formula_57">)</formula><formula xml:id="formula_58">and z 2 -z 1 = - ? 1,2 1 -? 1,2 - ? 1,1 1 -? 1,1 ? 1 m 1 ? v 1 - ? 1 g 1 ? v 1<label>(24)</label></formula><p>Lemma .6. <ref type="bibr" target="#b26">[27]</ref> Suppose that the conditions in Theorem (.3) hold, then</p><formula xml:id="formula_59">E f (z t+1 -f (z t )) ? 6 i=1 T i<label>(25)</label></formula><p>where</p><formula xml:id="formula_60">T 1 = -E t i=1 ?f (z i ), ? 1,i 1 -? 1,i ? i ? v i - ? i-1 ? v i-1 m i-1<label>(26)</label></formula><formula xml:id="formula_61">T 2 = -E t i=1 ? i ?f (z i ), g i ? v i<label>(27)</label></formula><formula xml:id="formula_62">T 3 = -E t i=1 ?f (z i ), ? 1,i+1 1 -? 1,i+1 - ? i 1 -? i ? i m i ? v i<label>(28)</label></formula><formula xml:id="formula_63">T 4 = E t i=1 3L<label>2</label></formula><formula xml:id="formula_64">? 1,i+1 1 -? 1,i+1 - ? 1,i 1 -? 1,i ? i m i ? v i 2 (29) T 5 = E t i=1 3L 2 ? 1,i 1 -? 1,i ? i ? v i - ? i-1 ? v i-1 m i-1 2<label>(30)</label></formula><formula xml:id="formula_65">T 6 = E t i=1 3L<label>2</label></formula><formula xml:id="formula_66">? i g i ? v i 2<label>(31)</label></formula><p>Lemma .7. <ref type="bibr" target="#b26">[27]</ref> Suppose that the condition in Theorem .3 hold, T 1 in (26) can be bounded as:</p><formula xml:id="formula_67">T 1 = -E t i=1 ?f (z i ), ? 1,i 1 -? 1,i ? i ? v i - ? i-1 ? v i-1 m i-1 ? H 2 ? 1 1 -? 1 E t i=2 d j=1 ? i ? v i - ? i-1 ? v i-1 j<label>(32)</label></formula><p>Lemma .8. <ref type="bibr" target="#b26">[27]</ref> Suppose the conditions in Theorem .3 are satisfied, then T 3 in (28) can be bounded as</p><formula xml:id="formula_68">T 3 = -E t i=1 ?f (z i ), ? 1,i+1 1 -? 1,i+1 - ? i 1 -? i ? i m i ? v i ? ? 1 1 -? 1 - ? 1,t+1 1 -? 1,t+1 (H 2 + G 2 )<label>(33)</label></formula><p>Lemma .9. <ref type="bibr" target="#b26">[27]</ref> Suppose assumptions in Theorem .3 are satisfied, then T 4 in (29) can be bounded as:</p><formula xml:id="formula_69">T 4 = E t i=1 3L<label>2</label></formula><formula xml:id="formula_70">? 1,i+1 1 -? 1,i+1 - ? 1,i 1 -? 1,i ? i m i ? v i 2 ? 3L<label>2</label></formula><formula xml:id="formula_71">? 1 1 -? 1 - ? 1,t+1 1 -? 1,t+1 2 G 2<label>(34)</label></formula><p>Lemma .10. <ref type="bibr" target="#b26">[27]</ref> Suppose the assumptions in Theorem .3 are satisfied, then T 5 in (30) can be bounded as:</p><formula xml:id="formula_72">T 5 = E t i=1 3L<label>2</label></formula><formula xml:id="formula_73">? 1,i 1 -? 1,i ? i ? v i - ? i-1 ? v i-1 m i-1 2 ? 3L<label>2</label></formula><formula xml:id="formula_74">? 1 1 -? 1 2 H 2 E t i=2 d j=1 ? i ? v i - ? i-1 ? v i-1 2 j<label>(35)</label></formula><p>Lemma .11. <ref type="bibr" target="#b26">[27]</ref> Suppose the assumptions in Theorem 8 are satisfied, then T 2 in (27) are bounded as:</p><formula xml:id="formula_75">T 2 = -E t i=1 ? i ?f (z i ), g i ? v i ? E t i=2 1<label>2</label></formula><formula xml:id="formula_76">? i g i ? v i 2 + L 2 ? 1 1 -? 1 2 1 1 -? 1 2 E d j=1 t-1 i=2 ? i g i ? v i 2 j + L 2 H 2 ? 1 1 -? 1 4 1 1 -? 1 2 E d j=1 t-1 i=2 ? i ? v i - ? i-1 ? v i-1 2 j + 2H 2 E d j=1 t i=2 ? i ? v i - ? i-1 ? v i-1 j + 2H 2 E d j=1 ? 1 ? v 1 j -E t i=1 ? i ?f (x i ), ?f (x i )/ ? v i<label>(36)</label></formula><p>Proof of Theorem .3</p><p>We provide the proof from <ref type="bibr" target="#b26">[27]</ref> for completeness. We combine Lemma .5, .6, .7, .8, .9, .10 and .11 to bound the objective.</p><formula xml:id="formula_77">E f (z t+1 ) -f (z t ) ? 6 i=1 T i ? H 2 ? 1 1 -? 1 E t i=2 d j=1 ? i ? v i - ? i-1 ? v i-1 j + ? 1 1 -? 1 - ? 1,t+1 1 -? 1,t+1 (H 2 + G 2 ) + 3L 2 ? 1 1 -? 1 - ? 1,t 1 -? 1,t 2 G 2 + 3L 2 ? 1 1 -? 1 2 H 2 E t i=2 d j=1 ? i ? v i - ? i-1 ? v i-1 2 j + E t i=2 1<label>2</label></formula><formula xml:id="formula_78">? i g i ? v i 2 + L 2 ? 1 1 -? 1 2 1 1 -? 1 2 E d j=1 t-1 i=2 ? i g i ? v i 2 j + L 2 H 2 ? 1 1 -? 1 4 1 1 -? 1 2 E d j=1 t-1 i=2 ? i ? v i - ? i-1 ? v i-1 2 j + 2H 2 E d j=1 t i=2 ? i ? v i - ? i-1 ? v i-1 j + 2H 2 E d j=1 ? 1 ? v 1 j -E t i=1 ? i ?f (x i ), ?f (x i )/ ? v i ? E C 1 T t=1 ? t g t / ? s t 2 + C 2 T t=1 ? t ? s t - ? t-1 ? s t-1 1 + C 3 T t=1 ? t ? s t - ? t-1 ? s t-1 2 + C 4<label>(37)</label></formula><p>The constants are defined below:</p><formula xml:id="formula_79">C 1 3 2 L + 1 2 + L 2 ? 1 1 -? 1 1 1 -? 1 2 (38) C 2 H 2 ? 1 1 -? 1 + 2H 2<label>(39)</label></formula><formula xml:id="formula_80">C 3 1 + L 2 1 1 -? 1 2 ? 1 1 -? 1 H 2 ? 1 1 -? 1 2<label>(40)</label></formula><formula xml:id="formula_81">C 4 ? 1 1 -? 1 (H 2 + G 2 ) + ? 1 1 -? 1 2 G 2 + 2H 2 E ||? 1 / ? v 1 || 1 + E[f (z 1 ) -f (z * )]<label>(41)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Bayesian interpretation of AdaBelief</head><p>We analyze AdaBelief from a Bayesian perspective. Theorem .12. Assume the gradient follows a Gaussian prior with uniform diagonal covariance, g ? N (0, ? 2 I); assume the observed gradient follows a Gaussian distribution, g ? N (g, C), where C is some covariance matrix. Then the posterior is:</p><formula xml:id="formula_82">g g, C ? N (I + C ? 2 ) -1 g, ( I ? 2 + C -1 ) -1</formula><p>We skip the proof, which is a direct application of the Bayes rule in the Gaussian distribution case as in [?]. If g is averaged across a batch of size n, we can replace C with C n . According to Theorem .12, the gradient descent direction with maximum expected gain is:</p><formula xml:id="formula_83">E g g, C = (I + C ? 2 ) -1 g = ? 2 (? 2 I + C) -1 g ? (? 2 I + C) -1 g<label>(42)</label></formula><p>Denote = ? 2 , then adaptive optimizers update in the direction ( I + C) -1 g; considering the noise in g t , in practice most optimizers replace g t with its EMA m t , hence the update direction is From a practical perspective, can be interpreted as a numerical term to avoid division by 0; from the Bayesian perspective, represents our prior on g t , with a larger indicating a larger ? 2 . Note that as the network evolves with training, the distribution of the gradient is distorted (an example with Adam is shown in Fig. <ref type="figure" target="#fig_10">2</ref> of <ref type="bibr" target="#b15">[16]</ref>), hence the Gaussian prior might not match the true distribution. To solve the mismatch between prior and the true distribution, it might be reasonable to use a weak prior during late stages of training (e.g., let ? 2 grow at late training phases, and when ? 2 ? ? reduces to a uniform prior). We only provide a Bayesian perspective here, and leave the detailed discussion to future works.  </p><formula xml:id="formula_84">( I + C) -1 m t .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Experimental Details 1. Image classification with CNNs on Cifar</head><p>We performed experiments based on the official implementation<ref type="foot" target="#foot_2">2</ref> of AdaBound <ref type="bibr" target="#b11">[12]</ref>, and exactly replicated the results of AdaBound as reported in <ref type="bibr" target="#b11">[12]</ref>. We then experimented with different optimizers under the same setting: for all experiments, the model is trained for 200 epochs with a batch size of 128, and the learning rate is multiplied by 0.1 at epoch 150. We performed extensive hyperparameter search as described in the main paper. In the main paper we only report test accuracy; here we report both training and test accuracy in Fig. <ref type="figure" target="#fig_8">1</ref> and Fig. <ref type="figure" target="#fig_10">2</ref>. AdaBelief not only achieves the highest test accuracy, but also a smaller gap between training and test accuracy compared with other optimizers such as Yogi.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Image Classification on ImageNet</head><p>We experimented with a ResNet18 on ImageNet classication task. For SGD, we use the same learning rate schedule as <ref type="bibr" target="#b29">[30]</ref>, with an initial learning rate of 0.1, and multiplied by 0.1 at epoch 30 and 60; for AdaBelief, we use an initial learning rate of 0.001, and decayed it at epoch 70 and 80. Weight decay is set as 10 -4 for both cases. To match the settings in [?] and <ref type="bibr" target="#b15">[16]</ref>, we use decoupled weight decay.</p><p>As shown in Fig. <ref type="figure" target="#fig_1">3</ref>, AdaBelief achieves an accuracy very close to SGD, closing the generalization gap between adaptive methods and SGD. Meanwhile, when trained with a large learning rate (0.1 for SGD, 0.001 for AdaBelief), AdaBelief achieves faster convergence than SGD in the initial phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Robustness to hyperparameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness to</head><p>We test the performances of AdaBelief and Adam with different values of varying from 10 -4 to 10 -9 in a log-scale grid. We perform experiments with a ResNet34 on Cifar10 dataset, and summarize the results in Fig. <ref type="figure" target="#fig_3">4</ref>. Compared with Adam, AdaBelief is slightly more sensitive to the choice of , and achieves the highest accuracy at the default valiue = 10 -8 ; AdaBelief achieves accuracy higher than 94% for all values, consistently outperforming Adam which achieves an accuracy around 93%.   Robustness to learning rate We test the performance of AdaBelief with different learning rates. We experiment with a VGG11 network on Cifar10, and display the results in Fig. <ref type="figure" target="#fig_4">5</ref>. For a large range of learning rates from 5 ? 10 -4 to 3 ? 10 -<ref type="foot" target="#foot_3">3</ref> , compared with Adam, AdaBelief generates higher test accuracy curve, and is more robust to the change of learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments with LSTM on language modeling</head><p>We experiment with LSTM models on Penn-TreeBank dataset, and report the results in Fig. <ref type="figure" target="#fig_5">6</ref>. Our experiments are based on this implementation 3 . Results [? ? ?] are measured across 3 runs with independent initialization. For completeness, we plot both the training and test curves.</p><p>We use the default parameters ? = 0.001, ? 1 = 0.9, ? 2 = 0.999, = 10 -8 for 2-layer and 3-layer models; for 1-layer model we set = 10 -12 and set other parameters as default. For simple models (1-layer LSTM), AdaBelief's perplexity is very close to other optimizers; on complicated models, AdaBelief achieves a significantly lower perplexity on the test set.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) loss function is f (x, y) = |x| + |y| (b) f (x, y) = |x + y| + |x -y| / 10 (c) f (x, y) = (x + y) 2 + (x -y) 2 /10 (d) f (x, y) = |x|/10 + |y| ?1 = ?2 = 0.3 (e) Trajectory for Beale function in 2D. (f) Trajectory for Beale function in 3D. (g) Trajectory for Rosenbrock function in 2D. (h) Trajectory for Rosenbrock function in 3D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Trajectories of SGD, Adam and AdaBelief. AdaBelief reaches optimal point (marked as orange cross in 2D plots) the fastest in all cases. We refer readers to video examples.</figDesc><graphic url="image-8.png" coords="5,205.53,192.74,95.04,92.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) VGG11 on Cifar10 (b) ResNet34 on Cifar10 (c) DenseNet121 on Cifar10 (d) VGG11 on Cifar100 (e) ResNet34 on Cifar100 (f) DenseNet121 on Cifar100</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Test accuracy ([? ? ?]) on Cifar. Code modified from official implementation of AdaBound.</figDesc><graphic url="image-14.png" coords="7,108.00,195.34,130.68,106.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Left to right: perplexity ([? ? ?]) on Penn Treebank for 1,2,3-layer LSTM. Lower is better.</figDesc><graphic url="image-18.png" coords="8,241.17,72.00,126.73,102.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: FID score of WGAN and WGAN-GP using a vanilla CNN generator on Cifar10. Lower is better. For each model, successful and failed optimizers are shown in the left and right respectively, with different ranges in y value.</figDesc><graphic url="image-21.png" coords="8,229.29,204.03,69.30,79.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Left to right: real images, samples from WGAN, WGAN-GP (both trained by AdaBelief).</figDesc><graphic url="image-24.png" coords="9,113.43,72.00,126.72,126.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Training (top row) and test (bottom row) accuracy of CNNs on Cifar10 dataset. We report confidence interval [? ? ?] of 3 independent runs.</figDesc><graphic url="image-30.png" coords="24,108.00,191.22,130.68,106.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Training (top row) and test (bottom row) accuracy of CNNs on Cifar10 dataset. We report confidence interval [? ? ?] of 3 independent runs.</figDesc><graphic url="image-40.png" coords="25,308.24,361.26,158.39,123.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Training and test accuracy (top-1) of ResNet18 on ImageNet.</figDesc><graphic url="image-39.png" coords="25,145.37,361.30,160.38,123.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Training (top row) and test (bottom row) accuracy of ResNet34 on Cifar10, trained with AdaBelief (left column) and Adam (right column) using different values of . Note that AdaBelief achieves an accuracy above 94% for all values, while Adam's accuracy is consistently below 94%.</figDesc><graphic url="image-43.png" coords="26,143.58,215.46,161.17,132.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Training (top row) and test (bottom row) accuracy of VGG on Cifar10, trained with AdaBelief (left column) and Adam (right column) using different values of learning rate.</figDesc><graphic url="image-47.png" coords="26,138.44,546.78,166.32,137.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Fake samples from WGAN trained with different optimizers.</figDesc><graphic url="image-61.png" coords="28,108.00,461.36,130.68,130.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Fake samples from WGAN-GP trained with different optimizers.</figDesc><graphic url="image-70.png" coords="29,108.00,461.36,130.68,130.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1) assume g t is drawn from a stationary distribution, hence after bias correction, Ev t = (Eg t ) 2 + Varg t . (2) low-noise assumption, assume (Eg t ) 2</figDesc><table><row><cell>Varg t , hence Ev t ? Eg t / (Eg t ) 2 = sign(Eg t ). (3) low-bias assumption, assume ? t 1 (? 1 to the power of t) is small, hence m t as an estimator of Eg t has a small bias ? t we have Eg t / ? 1 Eg t . Then</cell></row><row><cell>?? Adam t</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>3 for both Adam and AdaBelief, and set momentum as 0.3 for SGD. For an inseparable L 2 loss, AdaBelief outperforms other methods under the same setting. (d) We set ? 1 = ? 2 = 0.3 for Adam and AdaBelief, and set momentum as 0.3 in SGD. This corresponds to settings of Eq. 5. For the loss f (x, y) = |x|/10 + |y|, g t is a constant for a large region, hence ||Eg t || Varg t . As mentioned in [8], Em t = (1 -? t )Eg t , hence a</figDesc><table><row><cell>(a) Consider the loss function f (x, y) = |x| + |y| and a starting point near the x axis. This</cell></row><row><cell>setting corresponds to Fig. 2. Under the same setting, AdaBelief takes a large step in the x</cell></row><row><cell>direction, and a small step in the y direction, validating our analysis. More examples such</cell></row><row><cell>as f (x, y) = |x|/10 + |y| are in the supplementary videos.</cell></row><row><cell>(b) For an inseparable L 1 loss, AdaBelief outperforms other methods under the same setting.</cell></row><row><cell>(c)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>AdaBelief</cell><cell>SGD</cell><cell>AdaBound</cell><cell>Yogi</cell><cell>Adam</cell><cell>MSVAG RAdam AdamW</cell></row><row><cell>70.08</cell><cell>70.23</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>Top-1 accuracy of ResNet18 on ImageNet. ? is reported in [35], ? is reported in [16] ? 68.13 ? 68.23 ? 63.79 ? (66.54 ? ) 65.99 67.62 ? 67.93 ?</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>FID (lower is better) of a SN-GAN with ResNet generator on Cifar10. 52?0.16 12.70?0.12 13.13?0.12 13.05?0.<ref type="bibr" target="#b18">19</ref> 42.75?0.15 14.25?0.15 49.70?0.41 48.35?5.44 55.65?2.15</figDesc><table><row><cell>AdaBelief</cell><cell>RAdam</cell><cell>RMSProp</cell><cell>Adam</cell><cell>Fromage</cell><cell>Yogi</cell><cell>SGD</cell><cell>MSVAG</cell><cell>AdaBound</cell></row><row><cell>12.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison of AdaBelief and Padam. Higher Acc (lower FID) is better. ? is from<ref type="bibr" target="#b34">[35]</ref>.</figDesc><table><row><cell></cell><cell>AdaBelief</cell><cell>p=1/2 (Adam)</cell><cell>p=2/5</cell><cell>p=1/4</cell><cell>Padam p=1/5</cell><cell>p=1/8</cell><cell>p=1/16</cell><cell>p = 0 (SGD)</cell></row><row><cell>ImageNet Acc</cell><cell>70.08</cell><cell>63.79 ?</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>70.07 ?</cell><cell>-</cell><cell>70.23  ?</cell></row><row><cell>FID (WGAN)</cell><cell>83.0? 4.1</cell><cell>96.6?4.5</cell><cell cols="5">97.5?2.8 426.4?49.6 401.5?33.2 328.1?37.2 362.6?43.9</cell><cell>469.3?7.9</cell></row><row><cell cols="2">FID (WGAN-GP) 61.8? 7.7</cell><cell>73.5?8.7</cell><cell cols="6">87.1?6.0 155.1?23.8 167.3?27.6 203.6?18.9 228.5?25.8 244.3?27.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>In practice, adaptive methods such as Adam and AdaGrad replace ( I + C) -1/2 ( I + C) -1/2 m t with ?I( I + C) -1/2 m t for numerical stability, where ? is some predefined learning rate. Both Adam and AdaBelief take this form; their difference is in the estimate of C: Adam uses an uncentered approximation C Adam ? EMA diag(g t g t ), while AdaBelief uses a centered approximation C AdaBelief ? EMA diag[(g t -Eg t )(g t -Eg t ) ]. Note that the definition of C is the covariance hence it is centered. Note that for the ith parameter, E(g i</figDesc><table><row><cell>when Var g i t</cell><cell>t ) 2 = (Eg i t ) 2 + Var(g i t ), so Adam , and AdaBelief behaves closer to the ideal AdaBelief &lt; C i t ||, we have C i ||Eg i</cell></row><row><cell cols="2">and takes a larger step than Adam because C is in the denominator.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>https://www.youtube.com/playlist?list=PL7KkG3n9bER6YmMLrKJ5wocjlvP7aWoOu</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>https://github.com/Luolc/AdaBound</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>https://github.com/salesforce/awd-lstm-lm</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>https://github.com/pytorch/examples</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5"><p>https://github.com/eriklindernoren/PyTorch-GAN</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6"><p>https://github.com/mseitzer/pytorch-fid</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_7"><p>https://github.com/POSTECH-CVLab/PyTorch-StudioGAN</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments and Disclosure of Funding</head><p>This research is supported by <rs type="funder">NIH</rs> grant <rs type="grantNumber">R01NS035193</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_efmXkcR">
					<idno type="grant-number">R01NS035193</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A stochastic approximation method</title>
		<author>
			<persName><forename type="first">Herbert</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sutton</forename><surname>Monro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The annals of mathematical statistics</title>
		<imprint>
			<date type="published" when="1951">1951</date>
			<biblScope unit="page" from="400" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A method of solving a convex programming problem with convergence rate o(1/k 2)</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">in Sov. Math. Dokl</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Some methods of speeding up the convergence of iteration methods</title>
		<author>
			<persName><forename type="first">T</forename><surname>Boris</surname></persName>
		</author>
		<author>
			<persName><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">USSR Computational Mathematics and Mathematical Physics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07">Jul. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Adadelta: an adaptive learning rate method</title>
		<author>
			<persName><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The marginal value of adaptive gradient methods in machine learning</title>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Ashia C Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nati</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4148" to="4158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Gradient descent maximizes the margin of homogeneous neural networks</title>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05890</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improving generalization performance by switching from adam to sgd</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keskar</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.07628</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adaptive gradient methods with dynamic bound of learning rate</title>
		<author>
			<persName><forename type="first">Liangchen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhao</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09843</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">On the convergence of adam and beyond</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satyen</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09237</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive methods for nonconvex optimization</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sashank</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satyen</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9793" to="9803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Dissecting adam: The sign, magnitude and variance of stochastic gradients</title>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Balles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Hennig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07774</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On the variance of the adaptive learning rate and beyond</title>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03265</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">On the distance between two neural networks and the stability of learning</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03432</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Wasserstein gan</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Lecture notes: Some notes on gradient descent</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Toussaint</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">signsgd: Compressed optimisation for non-convex problems</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamyar</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04434</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On minimizing a convex function subject to linear inequalities</title>
		<author>
			<persName><forename type="first">Evelyn Ml</forename><surname>Beale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="173" to="184" />
			<date type="published" when="1955">1955</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An automatic method for finding the greatest or least value of a function</title>
		<author>
			<persName><forename type="first">Hoho</forename><surname>Rosenbrock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="175" to="184" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">On the convergence of a class of adam-type algorithms for non-convex optimization</title>
		<author>
			<persName><forename type="first">Xiangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoyu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyi</forename><surname>Hong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.02941</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Ieee</publisher>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Long short-term memory neural network for traffic speed prediction using remote microwave sensor data</title>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhimin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunpeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part C: Emerging Technologies</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="187" to="197" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Closing the generalization gap of adaptive gradient methods in training deep neural networks</title>
		<author>
			<persName><forename type="first">Jinghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.06763</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Nips 2016 tutorial: Generative adversarial networks</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00160</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">On the convergence of adaptive gradient methods for nonconvex optimization</title>
		<author>
			<persName><forename type="first">Dongruo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05671</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Lookahead optimizer: k steps forward, 1 step back</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9593" to="9604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Stochastic variance reduction for nonconvex optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suvrit</forename><surname>Hefny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnab?s</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>P?czos</surname></persName>
		</author>
		<author>
			<persName><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="314" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Accelerating stochastic gradient descent using predictive variance reduction</title>
		<author>
			<persName><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Quasi-hyperbolic momentum and adam for deep learning</title>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.06801</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Scaling sgd batch size to 32k for imagenet training</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Nostalgic adam: Weighting more of the past gradients when designing the adaptive learning rate</title>
		<author>
			<persName><forename type="first">Haiwen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07557</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Sadam: A variant of adam for strongly convex functions</title>
		<author>
			<persName><forename type="first">Guanghui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02957</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Adax: Adaptive gradient descent with exponential long term memory</title>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinjiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.09740</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">P</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lieven</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex optimization</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Quasi-likelihood functions, generalized linear models, and the gauss-newton method</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><surname>Wedderburn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="439" to="447" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fast curvature matrix-vector products for second-order gradient descent</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nicol</surname></persName>
		</author>
		<author>
			<persName><surname>Schraudolph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1723" to="1738" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Updating quasi-newton matrices with limited storage</title>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of computation</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">151</biblScope>
			<biblScope unit="page" from="773" to="782" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Natural gradient works efficiently in learning</title>
		<author>
			<persName><forename type="first">Shun-Ichi</forename><surname>Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="276" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Revisiting natural gradient for deep networks</title>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3584</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Methods of conjugate gradients for solving linear systems</title>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Magnus R Hestenes</surname></persName>
		</author>
		<author>
			<persName><surname>Stiefel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of research of the National Bureau of Standards</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="409" to="436" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep learning via hessian-free optimization</title>
		<author>
			<persName><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="735" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">(a) 1-layer LSTM (b) 2-layer LSTM (c) 3-layer LSTM (d) 1-layer LSTM (e) 2-layer LSTM (f) 3-layer LSTM Figure 6: Training (top row) and test (bottom row) perplexity on Penn-TreeBank dataset, lower is better</title>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Battiti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Table 1: Structure of GAN Generator Discriminator ConvTranspose</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="141" to="166" />
		</imprint>
	</monogr>
	<note>First-and second-order methods for learning: between steepest descent and newton&apos;s method. inchannel = 100, outchannel = 512, kernel = 4?4, stride = 1</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Bn-Relu</forename><surname>Leakyrelu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName><surname>Convtranspose</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>inchannel = 512, outchannel = 256, kernel = 4?4, stride = 2. inchannel=64, outchannel=128, kernel = 4?4, stride=2</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Bn-Leakyrelu</forename><surname>Bn-Relu</surname></persName>
		</author>
		<author>
			<persName><surname>Convtranspose</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>inchannel = 256, outchannel = 128, kernel = 4?4, stride = 2. inchannel=128, outchannel=256, kernel = 4?4, stride=2</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Bn-Leakyrelu</forename><surname>Bn-Relu</surname></persName>
		</author>
		<author>
			<persName><surname>Convtranspose</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>inchannel = 128, outchannel = 64, kernel = 4?4, stride = 2. inchannel=256, outchannel=512, kernel = 4?4, stride=2</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Bn-Leakyrelu</forename><surname>Bn-Relu</surname></persName>
		</author>
		<author>
			<persName><surname>Convtranspose</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>inchannel = 64, outchannel = 3, kernel = 4?4, stride = 2]) Linear(-1, 1</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">The code is based on several public github repositories 4 , 5 . We summarize network structure in Table 1. For WGAN, the weight of discriminator is clipped within [-0.01, 0.01]; for WGAN-GP, the weight for gradient-penalty is set as 10.0, as recommended by the original implementation. For each optimizer, we perform 5 independent runs. We train the model for 100 epochs, generate 64,000 fake samples (60,000 real images in Cifar10), and measure the Frechet Inception Distance (FID) [40] between generated samples and real samples</title>
		<imprint/>
	</monogr>
	<note>Our implementation on FID heavily relies on an open-source implementation 6 . We report the FID scores in the main paper, and demonstrate fake samples in Fig. 7 and Fig. 8 for WGAN and WGAN-GP respectively. For this experiment, we set = 10 -16 and use the rectification technique as in RAdam. Other hyperparamters and training schemes are the same as in the repository</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
