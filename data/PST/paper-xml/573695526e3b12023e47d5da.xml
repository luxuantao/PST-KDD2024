<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Standard Plane Localization in Fetal Ultrasound via Domain Transferred Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Chen</forename><surname>Hao</surname></persName>
							<email>ie.haochen@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Dong</forename><surname>Ni</surname></persName>
							<email>nidong@szu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jing</forename><surname>Qin</surname></persName>
							<email>jqin@szu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Shengli</forename><surname>Li</surname></persName>
							<email>lishengli63@126.com</email>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tianfu</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Ann</forename><surname>Heng</surname></persName>
							<email>pheng@cse.cuhk.edu.hk</email>
						</author>
						<author>
							<persName><forename type="first">H</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Medicine</orgName>
								<orgName type="laboratory" key="lab1">are with National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="laboratory" key="lab2">Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Ultrasound</orgName>
								<orgName type="department" key="dep2">Affiliated Shenzhen Maternal</orgName>
								<orgName type="institution">Child Healthcare Hospital of Nanfang Medical University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Department of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Kong and Center for Human Computer Interaction</orgName>
								<orgName type="department" key="dep3">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution" key="instit1">The Chinese University of Hong</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Standard Plane Localization in Fetal Ultrasound via Domain Transferred Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">151CA0DCC35401A1FA57A282AB0BF862</idno>
					<idno type="DOI">10.1109/JBHI.2015.2425041</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/JBHI.2015.2425041, IEEE Journal of Biomedical and Health Informatics This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/JBHI.2015.2425041, IEEE Journal of Biomedical and Health Informatics</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Ultrasound</term>
					<term>standard plane</term>
					<term>deep learning</term>
					<term>domain transfer</term>
					<term>knowledge transfer</term>
					<term>convolutional neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic localization of the standard plane containing complicated anatomical structures in ultrasound (US) videos remains a challenging problem. In this paper, we present a learning based approach to locate the fetal abdominal standard plane (FASP) in US videos by constructing a domain transferred deep convolutional neural network (CNN). Compared with previous works based on low-level features, our approach is able to represent the complicated appearance of the FASP and hence achieve better classification performance. More importantly, in order to reduce the overfitting problem caused by the small amount of training samples, we propose a transfer learning strategy, which transfers the knowledge in the low layers of a base CNN trained from a large database of natural images to our task-specific CNN. Extensive experiments demonstrate that our approach outperforms the state-of-the-art method for the FASP localization as well as the CNN only trained on the limited US training samples. The proposed approach can be easily extended to other similar medical image computing problems, which often suffer from the insufficient training samples when exploiting the deep CNN to represent high-level features.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>U LTRASOUND (US) is a routine screening tool offered to all pregnant women because of its safety, relatively low cost and real-time manner. The main goal of a fetal US scan is to confirm fetal viability, establish gestational age accurately and look for malformation that could influence prenatal management. Recent study showed that the sensitivity for prenatal detection of malformations by US ranges from 27.5% to 96% in different medical institutes <ref type="bibr" target="#b0">[1]</ref>. This wide variation indicates that US-based pregnant diagnosis is operator-dependent and requires a significant period of training before reaching competency. Among the pipeline of US diagnosis, acquisition of the standard plane is the prerequisite step and crucial for Fig. <ref type="figure">1</ref>: Illustration of the FASP localization from 2-D US images (the frame with red rectangle is a FASP).</p><p>the subsequent biometric measurements and diagnosis <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. In this regard, the reported wide sensitivity variation may be due, at least in part, to the quality of the standard plane obtained. In clinical practice, acquisition of the standard plane by clinicians often requires a thorough knowledge of human anatomy and substantial experience. Therefore, it is very challenging for novices and even difficult and time consuming for clinical experts. Thus, the development of automatic methods for locating standard planes from 2-D US images would assist the novices as well as improve the efficiency of experts.</p><p>In this study, we focus on automatically locating the fetal abdominal standard plane (FASP) from US videos (a preliminary version of this work has been reported in <ref type="bibr" target="#b4">[5]</ref> and significant improvements have been made to the original paper). Clinically, to locate the FASP, a radiologist attempts to find the concurrent presence of three key anatomical structures (KASs): the stomach bubble (SB), the umbilical vein (UV) and the spine (SP) in one frame when moving the US probe across the patient body. The procedure is illustrated in Fig. <ref type="figure">1</ref>. Based on the acquired FASP, the clinician can measure abdominal circumference (AC), which is the most important measurement for estimating fetal weight. The accuracy of AC measurement is heavily dependent on both the quality of the FASP and the manual measurement on the FASP by clinicians. Recently, commercial tools have been developed for the automatic AC measurement on several US scanners including Siemens Acuson S2000, GE LOGIQ S8, Mindray DC8, etc. However, little attention has been paid to the prerequisite step, that is, FASP acquisition.</p><p>Over the past few years, some methods have been proposed for locating standard planes from 2-D US images. Zhang Fig. <ref type="figure">2</ref>: Fetal abdominal anatomy (marked with brown rectangle), typical true FASPs (marked with green rectangles) and false FASPs (marked with red rectangles) with similar anatomical structures, e.g., GB and IC. et al. <ref type="bibr" target="#b5">[6]</ref> proposed to select the standard plane of early gestational sac (SPGS) from the US video by utilizing cascade AdaBoost classifiers trained on Haar features. However, the proposed system may not perform well in the detection of the FASP by only using Haar feature trained classifiers, since the FASP contains more complicated anatomical structures and has higher intra-class variations than the SPGS, which cannot be well captured by Haar features. Bahbibi et al. <ref type="bibr" target="#b6">[7]</ref> proposed to detect KASs from the manually cropped US image of abdomen regions by combining local Haar features and a global multi-scale feature symmetry measure. This method is semi-automatic, and was primarily designed for detecting KASs rather than the FASP. Kwitt et al. <ref type="bibr" target="#b7">[8]</ref> explored a kernel dynamic textures (KDTs) model to locate target structures by augmenting two configurations: raw intensity values and Bag of Words (BoW) representation of 3D-HOG. This method was only evaluated on the phantom data. As actual patient data are usually much more complex than phantom data, further investigation is needed to evaluate the efficacy of this method. Ni and Yang et al. <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> presented a hierarchically supervised learning framework to locate FASP from US videos via the radial component model and vessel probability map (RVD). This method achieved acceptable detection accuracy by incorporating geometric constrains of KASs. However, more effective feature representation is still required for capturing the complicated appearance of FASPs and further boosting the performance.</p><p>The major issues need to be well addressed in the problem of locating the FASP from US videos can be summarized in threefold. First, as illustrated in the first and second rows of Fig. <ref type="figure">2</ref>, the FASP often has high intra-class variations caused by the image artifacts (e.g., speckle noise and acoustic shadows), deformations of soft tissues, as well as difference in gestational age, fetal posture and scanning orientation. Second, as shown in the third row of Fig. <ref type="figure">2</ref>, the FASP and non-FASP often have low inter-class difference since large numbers of regions, e.g., acoustic shadows, abdominal aorta (AO), gall bladder(GB), intestinal canal(IC) and inferior vena cava (IVC), often share similar echogenicity appearance to the KASs. The low inter-class difference sometimes makes it very difficult to locate the FASP from US videos, even for experienced obstetric experts. Considering these challenges of high intra-class variation and low inter-class difference in our task, simple low-level features such as Haar, local phase and HOG used in previous studies <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> may not be able to represent the complicated appearance of the FASP and thus lower the classification performance. Third, as collecting large amount of data from patients and labeling them are very time-consuming, overfitting often occurs when employing the supervised learning based methods on small amount of training dataset. It is often one of the main challenges in medical computing community.</p><p>Recently, the advance of deep convolutional neural networks (CNN) have achieved a great success on a variety of applications including image classification <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, object detection <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> and image segmentation <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. One of the main advantages of CNN is its powerful feature representation capability, which makes it a promising tool for automatic FASP localization. However, the performance of CNN classifiers heavily depends on the size of the training dataset, as small amount of training samples may probably result in overfitting in the fully-supervised deep architecture of CNN. Inspired by some recent studies on transfer learning <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, we propose a novel domain transferred CNN to address the three challenging issues in the problem of FASP localization from US videos. The main contributions of this work are threefold: 1) To the best of our knowledge, this is the first work to adopt deep learning methods for the automatic localization of standard planes from US videos. Deep learning with expressive power for feature representation can enlarge the inter-class distance and reduce the intra-class distance efficiently, hence suitable for the task. 2) A transfer learning strategy that constructs an efficient task-specific CNN with a limited specific dataset using the knowledge of a base CNN pre-trained on large crossdomain datasets is introduced to enhance the classification performance, and we believe this transfer strategy and its variants can bridge the gap between deep CNN and medical applications caused by the limitation of the training dataset, and expand its application in medical computing community. 3) A quantitative analysis is given to discuss the feasibility of sharing statistical strength between natural and medical domains. This analysis may provide empirical evidence for the knowledge transfer between different domains. The remainder of this paper is organized as follows. Section II describes the proposed method and its background in detail. Experimental results are given in Section III. Section IV discusses the advantages and limitations of the proposed method, as well as future research directions. Conclusions are drawn in Section V. II. METHODS Fig. <ref type="figure">3</ref> illustrates the pipeline of our proposed method. At the training stage, we first train a multi-layered base CNN from ImageNet detection data, which is an object-level annotated dataset containing a large number of natural images. Then we construct the domain transferred CNN for FASP localization in US videos by: (1) implanting the convolutional layers of the base CNN into the transferred CNN as the initial settings, and (2) jointly fine tuning the parameters of the convolutional layers and training the fully connected layers for the transferred CNN based on the US training samples. At the test stage, the trained classifier is utilized to generate a probability map for each frame in the US video and identify an US image as either a FASP or non-FASP.</p><p>In this section, we first experimentally demonstrate the feasibility of transferring knowledge from natural image domain to medical image domain through dictionary learning. Then we provide the implementation details of the domain transferred CNN for the FASP localization in US videos after a brief introduction of CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Ultrasound Representation with Natural Image Bases</head><p>Domain transfer (knowledge transfer or transfer learning) often refers to the use of knowledge learnt from one source domain to efficiently develop a more accurate hypothesis for a new target task only consisting of small numbers of training examples in another domain <ref type="bibr" target="#b19">[20]</ref>. Research in domain transfer began in the early 1980s <ref type="bibr" target="#b20">[21]</ref> and has become an active topic in computer vision field with the development of deep learning methods <ref type="bibr" target="#b21">[22]</ref>. Recently, Gupta et al. <ref type="bibr" target="#b22">[23]</ref>, for the first time, made use of cross-domain features to represent MRI data and achieved competitive performance for Alzheimer disease classification. However, with large differences between two domains, why and how the knowledge can be transferred from natural image domain to medical image domain to boost the classification performance for medical applications remains an open problem. In this study, we assume that although high level features of US and natural images are distinctively different, they do share similar statistical pattern in low level features, e.g., oriented edges, corners and junctions. In this regard, the low level representations learnt from natural images can be transferred to the medical image domain.</p><p>To validate this assumption, we calculate the residual errors of reconstructing target US images with the dictionaries learnt from the natural, US and corrupted images with random Gaussian noise, respectively. The corresponding residual errors are defined as E n , E u and E g , respectively. If the E n and E u are small enough and approximately equal, while the E g is much larger than E n and E u , we infer that dictionary bases learnt from natural image domain could reconstruct US data well, and hence the knowledge learnt from natural images is promising to be transferred to medical image domain.</p><p>In computer vision community, dictionary can learn a set of elementary bases or atoms from training data. These dictionary bases can be linearly combined to well approximate a given signal <ref type="bibr" target="#b23">[24]</ref>. In this study, we first randomly extracted three sets of patches (size 8 × 8) from natural images <ref type="bibr" target="#b24">[25]</ref>, US images and Gaussian noise images, respectively. Each set is composed of one million patches, where each patch is normalized with zero mean and unit variance. Then N 1 (750,000) patches of each set are used as training data to learn the dictionary bases D n , D u , D g for natural, US and Gaussian noise images respectively. The left N 2 (250,000) patches are used for evaluating the reconstruction error. The dictionary bases are calculated according to <ref type="bibr" target="#b25">[26]</ref> and defined as:</p><formula xml:id="formula_0">arg min α∈R q ,D∈C N1 i=1 ||x i -Dα i || 2 2 + λψ(α i )<label>(1)</label></formula><p>C {D ∈ R p×q , s.t. ∀ j = 1, ..., q, d T j d j ≤ 1}, where x i ∈ R p is the ith sample patch of training set and can be represented by a sparse linear combination over a set of basis vectors from an over-complete dictionary D with its corresponding sparse coefficient α i ; ψ(α i ) is a sparsityinducing regularizer, e.g., L 1 norm ψ(α i ) = ||α i || 1 , and λ is the regularization parameter; C is the convex set of matrixes, which is bounded on the column of dictionary d j ∈ R q to prevent D from having arbitrarily large values.</p><p>After the D n , D u , D g are calculated, the reconstruction residual error E is computed by:</p><formula xml:id="formula_1">E = 1 2N 2 N2 i=1 ||y i -Dα i || 2 2<label>(2)</label></formula><p>where y i is the ith sample patch of testing set. It is worth noting that α i in ( <ref type="formula" target="#formula_1">2</ref>) is recalculated by minimizing (1) on testing data y i with the calculated D from training data. Fig. <ref type="figure">4</ref> shows the learnt dictionary bases from natural and US images. It is observed that the statistical patterns in low level features extracted from natural and US images are quite similar. The errors E n , E u and E g of reconstructing US patches using dictionary bases learnt from natural, US and Gaussian noise images are 0.114, 0.111 and 0.263, respectively. The E n and E u are approximately equal and much smaller than E g . In addition, we also performed the paired-sample t-test to illustrate the significance of differences (the null hypothesis set at the 5% significance level). The E n and E u shows no significant difference (p = 0.249), whereas the pairs of E n and E g , E u and E g both show significant difference (p = 3.3 × 10 -4 and p = 4.8 × 10 -3 , respectively). These results demonstrate that although high level features of US and natural images are distinctively different, they do share similar statistical pattern in low level features. Hence, the knowledge transferred from natural image domain to medical image domain has the potential to enhance the learning performance with limited medical data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Convolutional Neural Network</head><p>Convolutional neural networks (CNN) are biologicallyinspired variants of multi-layered perceptrons, which exploit spatial correlation by extracting features generated from localized convolutional kernels <ref type="bibr" target="#b26">[27]</ref>. Their ability to learn complex, high-dimensional and nonlinear mappings from large The basic structure of CNN includes several pairs of convolutional and sub-sampling layers, followed by a shallow multilayered perceptron for classification <ref type="bibr" target="#b27">[28]</ref>. The convolutional layers (C layers) take local receptive fields in the previous layer as input and extract features while preserving spatial correlation. Suppose that h l j is the jth feature map in the lth layer and h l-1 m (m = 1, ..., M ) is the mth feature map in the (l -1)th layer, the feature map h l j can be computed by:</p><formula xml:id="formula_2">h l j = σ( M m=1 W l jm * h l-1 m + b l )<label>(3)</label></formula><p>where W l jm is the convolutional kernel connected to the mth feature map in the previous layer; b l is the bias in the lth layer; and σ(•) is the non-linear activation function. We employ rectified linear unit (ReLU) σ(x) = max(x, 0) as the activation function in our implementation because it can create sparse representation with hard non-linearity and achieve better performance compared with the traditional sigmoid function <ref type="bibr" target="#b28">[29]</ref>.</p><p>Sub-sampling layers are used to reduce the resolution of feature maps by pooling over local neighborhood. In our study, max-pooling layers (M layers) are used to perform non-max suppression and down-sample the resolution of the feature maps generated by the C layers. The multi-layered perceptron with fully connected layers (F layers) follows after several altering convolutional and max-pooling layers. The final layer of the multi-layered perceptron outputs the posterior probability for each class with softmax function.</p><p>In order to train the CNN classifier, the parameters of CNN, including the convolution kernel W, the bias b and the weights in the multi-layered perceptron, should be automatically learnt from the training samples I i (i = 1, ..., N ). In practice, the parameters are calculated by minimizing a loss function L(I i , y i ; θ) via mini-batch gradient descent (SGD) with momentum <ref type="bibr" target="#b29">[30]</ref>, where θ represents the set of parameters that should be trained. The loss function is defined as:</p><formula xml:id="formula_3">L(I i , t i ; θ) = - N i=1 K k=1 (1{t i = k} log p(c k = 1|I i , θ)) (4) p(c k = 1|I i , θ) = e o k K j=1 e oj , k = 1, ..., K<label>(5)</label></formula><p>where p(c k = 1|I i , θ) is the posterior probability of predicting the training sample I i as the kth class c k among total K classes; 1{•} is the indicator function and t i is the class label of training sample I i , 1{t i = k} evaluates to 1 when I i belongs to the kth class, otherwise 0; o k is the output of neural network before softmax given the training sample I i . Readers can refer to <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b26">[27]</ref> for more details of the implementation of CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Domain Transferred CNN for FASP Localization</head><p>Although the CNN has the advantage of learning powerful feature representations, with limited training data in many medical applications, the fully-supervised deep architectures may overfit the training data and hence degrade the learning performance. To the end, the small amount of training samples in many medical applications prohibits its use in medical domain. Recently, a number of studies <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> have demonstrated that transfer learning is a powerful tool to reduce overfitting by first training a base network on a base dataset and task, and then transferring the learnt features of the base network to a new target network to be trained on a target dataset and task. Yosinski et al. <ref type="bibr" target="#b17">[18]</ref> further pointed out that the fundamental principle for transfer learning is that features learnt from lower layers (such as C layers) of CNN are general so that they can be applicable to datasets across different image domains, while the features computed from higher layers (such as F layers) are more specific to the particular dataset. In this regard, the knowledge acquired from the lower layers has potential to be used in other tasks.</p><p>Inspired by these studies, we attempt to investigate if the knowledge acquired from CNN trained by a dataset of natural images can be transferred to a medical application where the training dataset is limited and directly employing CNN on the dataset will probably result in overfitting to some extent. In this study, we aim to locate the standard planes in fetal US videos by employing such kind of transferred CNN. In previous subsection A, we have experimentally demonstrated that low level cues learnt from natural images can be transferred and reused in the domain of fetal US. In this section, a transfer learning strategy is proposed to construct a transferred CNN that takes the advantage of the base CNN, which was trained from a large dataset of natural images. The transferred CNN can effectively identify the FASPs from US videos. Since we are interested in the object-level features, we first train the base CNN on the 2014 ImageNet detection dataset <ref type="bibr" target="#b24">[25]</ref>, which contains 478,807 and 20,121 objects in the training and validation dataset, respectively. The base CNN is denoted as N-CNN. Our N-CNN's architecture shares the same spirit with the widely used BVLC CaffeNet Model <ref type="bibr" target="#b30">[31]</ref>. The N-CNN includes five C layers and several M layers between C layers, followed by three F layers, as shown in Table <ref type="table" target="#tab_0">I</ref>. Noting that we converted the natural images into gray images since US images are single-channel images. Then we construct the domain transferred CNN for FASP detection, namly T-CNN. In order to transfer the knowledge from the base CNN, we implant the trained C and M layers of the base CNN to the same positions of T-CNN. Then we jointly fine tune these layers and train following three new F layers (called nF layers to distinguish them from F layers in the base CNN) on US training samples. These three nF layers consist of 1024, 256, 2 neurons, respectively. It is worth noting that during the training of T-CNN, parameters of transferred layers in T-CNN are initialized by the pre-trained N-CNN, while parameters of three nF layers in T-CNN are randomly initialized with Gaussian distribution. Meanwhile, the strategies of data augmentation and dropout <ref type="bibr" target="#b31">[32]</ref> are implemented in the training process for regularization in order to improve the generalization ability.</p><p>Once the T-CNN is trained, the probability map p for each image in the US video is calculated with a sliding window method. This approach involves scanning the image with a rectangular window after excluding non-US regions, and applying a T-CNN classifier to the sub-image defined by the window. In order to improve the robustness, each sub-image is further augmented by cropping its center and corners as well as its mirrored versions, resulting in 10 inputs (size 227×227) for the T-CNN. The score of the sliding window at its center is obtained by averaging the scores of these 10 inputs. After the probability map p is obtained, we further smooth it to eliminate noise with a bilateral filter. Then the final score of the image is the highest value of the smoothed probability map. Finally, the US image in the video with the highest detection score is identified as the FASP when the score is above a threshold T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset and System Implementation</head><p>The study protocol was reviewed and approved by the ethics committee of our institution, and informed consent was obtained from all subjects.</p><p>Training Dataset. To train the T-CNN classifier, we used 11942 expert annotated fetal abdominal US images from 300 videos for generating the training samples. First, 1991 positive samples were generated by cropping the image regions that contained the anatomical objects from FASPs, while 3160 negative samples were extracted randomly from the non-FASPs and the background of FASPs. Some of these had an overlap of 20% to 40% with a positive sample. Note that all training samples were further rotated, translated and mirrored to augment the training database.</p><p>Testing Dataset. A conventional US sweep was performed by three graduate students with a half-year of training in an obstetric US department to obtain 219 videos with a total of 8718 US images on 219 pregnant women (the fetal gestational age ranged from 18 to 40 weeks) in the supine position for evaluation of the T-CNN classifier. Such study protocol considers variations of operators in clinical practice and makes the FASP detection more challenging than the protocol reported in <ref type="bibr" target="#b3">[4]</ref>. Each sweep lasted approximately 2-5s and each video contained 17 -48 frames. During image acquisition, the US imaging parameters were adjusted by the students to obtain a desired imaging quality and were not required to conform to a pre-defined criterion. Then a radiologist with more than 5 years of experience in obstetric US carefully checked to see if one or more FASPs was in each video. Of the 219 videos, 199 videos contained at least one FASP, the remaining 20 videos contained no FASP. All the images and videos used in our experiments were acquired using a Siemens Acuson Sequoia 512 US scanner from Shenzhen Maternal and Child Healthcare Hospital.</p><p>System Implementation. The Caffe package <ref type="bibr" target="#b30">[31]</ref> was used to implement the CNN architecture. Our system was implemented with the mixed programming technology of MATLAB and C++. The running time for locating the FASP from one video depended primarily on the number of frames in the video. It generally took 1 min for detecting the FASP from a video containing 40 frames using a workstation with a 2.50 GHz Intel(R) Xeon(R) E5-2609 CPU and a NVIDIA GeForce GTX TITAN GPU.</p><p>In order to evaluate our system, we first qualitatively evaluated the performance of our T-CNN classifier by visualizing the high-level features learnt from T-CNN and showing typical FASPs detected from different US videos. Then, we evaluated our system quantitatively on US images and videos separately. The threshold T used to identify an US image as FASP or non-FASP was set as 0.68 by testing the algorithm on a set of samples from the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Qualitative Performance Evaluation</head><p>Data visualization is an effective technique to directly show the discriminant capability of feature representations, hence can indicate the performance of the classifier. We employed the Barnes-Hut-SNE method <ref type="bibr" target="#b32">[33]</ref> to reduce dimensions of raw training datasets, raw testing datasets, and features of intermediate layers of T-CNN extracted from the training and testing datasets. Fig. <ref type="figure">5</ref> shows these four data sets, where red and blue points represent FASP and non-FASP, respectively. The mixed distribution of training and testing data in the original domain illustrates the high intra-class variation of FASP and the low inter-class variation between FASP and non-FASP, which makes the FASP detection very challenging, as discussed in section I. In comparison, the features of nF 7 layer extracted from training and test data can be used to classify FASP and non-FASP very well. proposed T-CNN method, the state-of-the-art RVD method <ref type="bibr" target="#b3">[4]</ref> and the R-CNN method which directly trained the CNN on the training US dataset without exploiting the proposed transfer learning strategy (here R denotes randomly initializing the parameters of CNN at the beginning of training). In the second experiment, we investigated the performance of these three methods for detecting the FASP from US videos, which was consistent with the clinical practice. We computed the following evaluation measurements <ref type="bibr" target="#b33">[34]</ref>: recall (R), precision (P ), F β score (β = 1, i.e., F1 score in our experiment) and accuracy (A), as shown in ( <ref type="formula" target="#formula_4">6</ref>)- <ref type="bibr" target="#b8">(9)</ref>.</p><formula xml:id="formula_4">R = N tp N tp + N f n<label>(6)</label></formula><formula xml:id="formula_5">P = N tp N tp + N f p<label>(7)</label></formula><formula xml:id="formula_6">F β = (1 + β 2 ) • RP R + β 2 P<label>(8)</label></formula><formula xml:id="formula_7">A = N tp + N tn N tp + N tn + N f p + N f n<label>(9)</label></formula><p>where N tp , N tn , N f p and N f n are the number of true positives (TP), true negatives (TN), false positives (FP) and false negatives (FN), respectively. 1) Evaluation on US Images: In this experiment, the US image is classified as a FASP when its score is larger than the threshold T . As shown in Table <ref type="table" target="#tab_1">II</ref>, the accuracy, precision, recall and F1 score of the proposed T-CNN on testing images were 0.896, 0.714, 0.710 and 0.712, respectively, which significantly outperformed the state-of-the-art method <ref type="bibr" target="#b3">[4]</ref>. In addition, the results shown in Table <ref type="table" target="#tab_1">II</ref> further demonstrates that the T-CNN with knowledge transfer outperformed the R-CNN, which indicates that the knowledge transferred from natural image domain can obviously improve the results of our specific medical task with limited dataset for training. The Precision-Recall (PR) and Receiver Operating Characteristic (ROC) curves evaluated on US images were also shown in Fig. <ref type="figure" target="#fig_2">7</ref>. The areas under the ROC curve (AUC) obtained by the T-CNN, R-CNN, and RVD methods were 0.93, 0.9 and 0.8, respectively. Our proposed T-CNN method achieved the best performance and the result of the R-CNN method was better than the RVD method, which further illustrates the efficacy of both the deep CNN algorithm and the proposed knowledge transfer strategy .</p><p>2) Evaluation on US Videos: For the evaluation of the FASP localization in US videos, we used the same rules of <ref type="bibr" target="#b3">[4]</ref> to define the true positive and true negative. Each video was treated as one testing sample. A true positive was obtained when the correct FASP was detected from a video containing at least one FASP. And a true negative was obtained when no FASP was detected from a video containing no FASP. As shown in Table <ref type="table" target="#tab_2">III</ref>, the accuracy, precision, recall and F1 score of the T-CNN on testing videos were 0.904, 0.908, 0.995 and 0.950, respectively. Our proposed T-CNN method outperformed the other two methods significantly. The R-CNN performed better than the state-of-the-art RVD method. These results demonstrate the efficacy of the proposed domain  transferred deep learning method and the system equipped with this method is promising for clinical practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DISCUSSIONS</head><p>In this paper, a novel domain transferred CNN model for the automatic localization of the FASP from US videos is presented. Specifically, we first train a base CNN on the 2014 ImageNet detection dataset <ref type="bibr" target="#b24">[25]</ref>. Then the knowledge embedded in the convolutional layers of the base CNN is transferred to our task-specific CNN by implanting these trained convolutional layers into the proposed T-CNN model. The T-CNN model is generated by jointly fine tuning the transferred layers and training the fully connected layers. The proposed knowledge transferred deep CNN can reduce the overfitting of the classifier with limited size of training samples while taking the advantage of CNN for powerful feature representation. The experimental results presented in section III show that the proposed T-CNN method achieved better classification results than the state-of-the-art method <ref type="bibr" target="#b3">[4]</ref>. The high accuracy of locating the FASP from US videos demonstrates that our system has great potential for clinical applications.</p><p>Generally speaking, the data size of medical domain for learning-based approaches is much smaller than that of natural domain, since collecting data from patients need the approvement of the ethics committee and is very time-consuming, especially for some rare diseases. Therefore, it is a main challenge faced in the medical computing community that overfitting induced by limited training dataset may affect the performance of the learning system and make it difficult for clinical applications. Nowadays, data-driven medicine enabling the discovery of new treatment options with analysis on massive amounts of data has become the next frontier for modern medicine <ref type="bibr" target="#b34">[35]</ref>, while deep-learning models are breakthroughs over traditional methods in addressing longstanding computing problems and has been applied in a variety of applications. Our proposed domain transferred method will benefit the medical computing community in addressing the challenge of limited size of dataset and boost the application of the CNN in medical domain.</p><p>In this study, all US images and videos were acquired by three graduate students with a half-year of training in an obstetric US department. Thus the operator variation is taken into account for the system evaluation. It possibly lowered the performance of the RVD method compared with the results reported in <ref type="bibr" target="#b3">[4]</ref>. Our proposed T-CNN method has the superior performance on this challenging dataset. In addition, the RVD method is only designed for the specific task, i.e., the FASP localization. Although we applied the proposed method for the same task of <ref type="bibr" target="#b3">[4]</ref> in this work, our method is a general learning framework and can be easily adapted to the automatic localization of other standard planes in US videos. It can also be easily extended to other classification problems suffering from the small size of the training dataset in medical domain.</p><p>There are several limitations for this study. First, it takes approximately 1 min for our system to locate the FASP from one US video. It is much faster than the system developed in <ref type="bibr" target="#b3">[4]</ref>. However, our system is still not in real time and cannot be directly used in a clinical examination. In our system, the most time consuming step is the sliding window operation on the US image with the original size. We plan to further increase the speed of our system by first generating the sub-sampled feature maps of the US image from the T-CNN model and then performing the sliding window classifier on the feature maps. Second, although the high detection accuracy of our system makes it applicable for clinical practice, additional work is needed to further improve the system performance. Fig. <ref type="figure" target="#fig_3">8</ref> illustrates two examples of false FASPs detected from two videos by our method. This incorrect detection is possibly caused by the acoustic shadow and the GB being misidentified as the SB and UV, respectively, due to very similar appearance. Previous studies suggested that one possible way to improve the performance of CNN is to stochastically corrupt the training samples in the learning process <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Similarly, we may randomly corrupt the training data with acoustic shadows in the training process to improve the robustness of CNN. On the other hand, we observe that clinicians often select the FASPs with contextual clues in the US videos. Accordingly, the false positive findings identified by the computerized method can be possibly eliminated with the incorporation of the temporal context from consecutive frames into the mathematical detection model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>We propose an automatic approach to locate the fetal abdominal standard plane (FASP) from US videos. In contrast with existing approaches based on low-level features, our approach exploits the deep convolutional neural network (CNN) to represent the complex anatomical structures appearing in the FASP. We further propose a transfer learning strategy to reduce the overfitting problem resulted from inadequate training samples by leveraging the knowledge of a base CNN trained on a large database of natural images. We implant the knowledge into the transferred CNN model as initial settings, which can improve the performance of the transferred CNN on localization of FASP. Both qualitative and quantitative experiments demonstrate the efficacy of the proposed approach. We believe this approach is also appropriate for other medical image computing problems, where the sophisticated anatomical structures cannot be well captured by low-level features and the insufficiency of training samples makes it difficult to train a robust deep CNN model to represent the high-level features. The transfer learning strategy is promising to expand the applications of deep CNN in medical domain. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :Fig. 4 :</head><label>34</label><figDesc>Fig.3: The pipeline of our proposed method. The numbers in the upper and lower subfigures denote the network configuration; C, M and F denote the convolutional layer, the max-pooling layer and the fully connected layer, respectively.</figDesc><graphic coords="4,344.63,203.38,85.11,78.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 6 Fig. 5 :Fig. 6 :</head><label>656</label><figDesc>Fig. 5: Barnes-Hut-SNE visualizations of four data sets. Red and blue points represent FASP and non-FASP, respectively. (a) Raw training data, (b) Raw testing data, (c) Extracted features of nF 7 layer from training dataset, (d) Extracted features of nF 7 layer from testing dataset.</figDesc><graphic coords="6,312.31,347.58,121.89,92.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: (a) Precision-recall plane, (b) ROC Curve.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Two examples of false FASPs detected from videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>ACKNOWLEDGMENT</head><label></label><figDesc>The work described in this paper was supported in part by the National Natural Science Foundation of China (Nos. 81270707 and 61233012), in part by the Shenzhen Key Basic Research Project (Nos. JCYJ20130329105033277 and JCYJ20140509172609164), in part by the Shenzhen-Hong Kong Innovation Circle Funding Program (Nos. JSE201109150013A and SGLH20131010151755080), in part by the Hong Kong Innovation and Technology Fund (Nos. GHP/003/11SZ and GHP/002/13SZ), and in part by a grant from the Research Grants Council of Hong Kong (No. CUHK412510).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>The architecture of N-CNN</figDesc><table><row><cell cols="4">Layer Feature maps Kernel size Stride</cell></row><row><cell>Input</cell><cell>227x227x1</cell><cell>-</cell><cell>-</cell></row><row><cell>C1</cell><cell>55x55x96</cell><cell>11</cell><cell>4</cell></row><row><cell>M1</cell><cell>27x27x96</cell><cell>3</cell><cell>2</cell></row><row><cell>C2</cell><cell>27x27x256</cell><cell>5</cell><cell>-</cell></row><row><cell>M2</cell><cell>13x13x256</cell><cell>3</cell><cell>2</cell></row><row><cell>C3</cell><cell>13x13x384</cell><cell>3</cell><cell>-</cell></row><row><cell>C4</cell><cell>13x13x384</cell><cell>3</cell><cell>-</cell></row><row><cell>C5</cell><cell>13x13x256</cell><cell>3</cell><cell>-</cell></row><row><cell>M5</cell><cell>6x6x256</cell><cell>3</cell><cell>2</cell></row><row><cell>F6</cell><cell>4096</cell><cell>-</cell><cell>-</cell></row><row><cell>F7</cell><cell>4096</cell><cell>-</cell><cell>-</cell></row><row><cell>F8</cell><cell>200</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Results of FASP localization on US images</figDesc><table><row><cell>Method</cell><cell cols="3">Accuracy Precision Recall</cell><cell>F1</cell></row><row><cell>T-CNN</cell><cell>0.896</cell><cell>0.714</cell><cell>0.710</cell><cell>0.712</cell></row><row><cell>R-CNN</cell><cell>0.857</cell><cell>0.594</cell><cell>0.681</cell><cell>0.635</cell></row><row><cell>RVD [4]</cell><cell>0.833</cell><cell>0.532</cell><cell>0.693</cell><cell>0.602</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Results of FASP localization on US videos</figDesc><table><row><cell>Method</cell><cell cols="3">Accuracy Precision Recall</cell><cell>F1</cell></row><row><cell>T-CNN</cell><cell>0.904</cell><cell>0.908</cell><cell>0.995</cell><cell>0.950</cell></row><row><cell>R-CNN</cell><cell>0.822</cell><cell>0.826</cell><cell>0.994</cell><cell>0.902</cell></row><row><cell>RVD [4]</cell><cell>0.762</cell><cell>0.823</cell><cell>0.913</cell><cell>0.865</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 11, NO. 4, DECEMBER 2012</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A score-based method for quality control of fetal images at routine second-trimester ultrasound examination</title>
		<author>
			<persName><forename type="first">L</forename><surname>Salomon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Winer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Prenatal diagnosis</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="822" to="827" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Selective search and sequential detection for standard plane localization in ultrasound</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Abdominal Imaging. Computation and Clinical Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="203" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Standard plane localization in ultrasound by radial component</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biomedical Imaging (ISBI), 2014 IEEE 11th International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1180" to="1183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Standard plane localization in ultrasound by radial component model and selective search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ultrasound in medicine &amp; biology</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fetal abdominal standard plane localization through representation learning with knowledge transfer</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning in Medical Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="125" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Intelligent scanning: Automated standard plane selection and biometric measurement of early gestational sac in routine ultrasound examination</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical physics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="5015" to="5027" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Integration of local and global features for anatomical object detection in ultrasound</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rahmatullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Papageorghiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention-MICCAI 2012</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="402" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Localizing target structures in ultrasound video-a phantom study</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kwitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Razzaque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aylward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="712" to="722" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mitosis detection in breast cancer histology images with deep neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Cires ¸an</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention-MICCAI 2013</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="411" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">1-hkust: Object detection in ilsvrc 2014</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.6155</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep neural networks segment neuronal membranes in electron microscopy images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2843" to="2851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for multi-modality isointense infant brain image segmentation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="214" to="224" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.1531</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1403.6382</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Icml2011 unsupervised and transfer learning workshop</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lemaire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dror</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The need for biases in learning generalizations</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Laboratory for Computer Science Research, Rutgers Univ.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning of representations for unsupervised and transfer learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Unsupervised and Transfer Learning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="17" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Natural image bases to represent neuroimaging data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ayhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML-13)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML-13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="987" to="994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dictionaries for sparse representation modeling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Bruckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1045" to="1057" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Online learning for matrix factorization and sparse coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="19" to="60" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Max-pooling convolutional neural networks for vision-based hand gesture recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ducatelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Di Caro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal and Image Processing Applications (ICSIPA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="342" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 14th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML-13)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML-13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Barnes-hut-sne</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A probabilistic interpretation of precision, recall and f-score, with implication for evaluation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Goutte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gaussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="345" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The coming age of data-driven medicine: translational bioinformatics&apos; next frontier</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Some improvements on deep convolutional neural network based image classification</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5402</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hao Chen (S&apos;14) received the B.S. degree in Information Engineering from Beihang University (BUAA) in 2009. He is currently a PhD student in the Department of Computer Science and Engineering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>The Chinese University of Hong Kong (CUHK</orgName>
		</respStmt>
	</monogr>
	<note>Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. deep learning, object detection and segmentation</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
