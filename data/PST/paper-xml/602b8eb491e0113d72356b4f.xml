<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Unified Lottery Ticket Hypothesis for Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuxi</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">AWS Deep Learning</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Unified Lottery Ticket Hypothesis for Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>GNN Sparsity (%) GNN Sparsity (%)</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With graphs rapidly growing in size and deeper graph neural networks (GNNs) emerging, the training and inference of GNNs become increasingly expensive. Existing network weight pruning algorithms cannot address the main space and computational bottleneck in GNNs, caused by the size and connectivity of the graph. To this end, this paper first presents a unified GNN sparsification (UGS) framework that simultaneously prunes the graph adjacency matrix and the model weights, for effectively accelerating GNN inference on large-scale graphs. Leveraging this new tool, we further generalize the recently popular lottery ticket hypothesis to GNNs for the first time, by defining a graph lottery ticket (GLT) as a pair of core sub-dataset and sparse sub-network, which can be jointly identified from the original GNN and the full dense graph by iteratively applying UGS. Like its counterpart in convolutional neural networks, GLT can be trained in isolation to match the performance of training with the full model and graph, and can be drawn from both randomly initialized and self-supervised pre-trained GNNs. Our proposal has been experimentally verified across various GNN architectures and diverse tasks, on both small-scale graph datasets (Cora, Citeseer and PubMed), and large-scale datasets from the challenging Open Graph Benchmark (OGB). Specifically, for node classification, our found GLTs achieve the same accuracies with 20% ∼ 98% MACs saving on small graphs and 25% ∼ 85% MACs saving on large ones. For link prediction, GLTs lead to 48% ∼ 97% and 70% MACs saving on small and large graph datasets, respectively, without compromising predictive performance. Codes available at https://github. com/VITA-Group/Unified-LTH-GNN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure <ref type="figure">1</ref>. Summary of our achieved performance (y-axis) at different graph and GNN sparsity levels (x-axis) on Cora and Citeceer node classification. The size of markers represent the inference MACs (= 1 2 FLOPs) of each sparse GCN on the corresponding sparsified graphs. Black circles (•) indicate the baseline, i.e., unpruned dense GNNs on the full graph. Blue circles (•) are random pruning results. Orange circles (•) represent the performance of a previous graph sparsification approach, i.e., ADMM <ref type="bibr" target="#b38">(Li et al., 2020b)</ref>. Red stars (#) are established by our method (UGS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graph Neural Networks (GNNs) <ref type="bibr" target="#b71">(Zhou et al., 2018;</ref><ref type="bibr" target="#b34">Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b14">Chen et al., 2019;</ref><ref type="bibr" target="#b53">Veličković et al., 2017)</ref> have established state-of-the-art results on various graphbased learning tasks, such as node or link classification <ref type="bibr" target="#b34">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b53">Veličković et al., 2017;</ref><ref type="bibr" target="#b47">Qu et al., 2019;</ref><ref type="bibr" target="#b55">Verma et al., 2019;</ref><ref type="bibr" target="#b32">Karimi et al., 2019;</ref><ref type="bibr" target="#b66">You et al., 2020d;</ref><ref type="bibr" target="#b23">c)</ref>, link prediction <ref type="bibr" target="#b68">(Zhang &amp; Chen, 2018)</ref>, and graph classification <ref type="bibr" target="#b62">(Ying et al., 2018;</ref><ref type="bibr" target="#b59">Xu et al., 2018;</ref><ref type="bibr" target="#b64">You et al., 2020b)</ref>. GNNs' superior performance results from the structureaware exploitation of graphs. To update the feature of each node, GNNs first aggregate features from neighbor connected nodes, and then transform the aggregated embeddings via (hierarchical) feed-forward propagation.</p><p>However, the training and inference of GNNs suffer from the notorious inefficiency, and pose hurdle to GNNs from being scaled up to real-world large-scale graph applications. This hurdle arises from both algorithm and hardware levels. On the algorithm level, GNN models can be thought of a composition of traditional graphs equipped with deep neural network (DNN) algorithms on vertex features. The execution of GNN inference falls into three distinct cat-arXiv:2102.06790v1 <ref type="bibr">[cs.</ref>LG] 12 Feb 2021 egories with unique computational characteristics: graph traversal, DNN computation, and aggregation. Especially, GNNs broadly follow a recursive neighborhood aggregation (or message passing) scheme, where each node aggregates feature vectors of its multi-hop neighbors to compute its new feature vector. The aggregation phase costs massive computation when the graphs are large and with dense/complicated neighbor connections <ref type="bibr" target="#b59">(Xu et al., 2018)</ref>. On the hardware level, GNN's computational structure depends on the often sparse and irregular structure of the graph adjacency matrices. This results in many random memory accesses and limited data reuse, but also requires relatively little computation. As a result, GNNs have much higher inference latency than other neural networks, limiting them to applications where inference can be pre-computed offline <ref type="bibr" target="#b25">(Geng et al., 2020;</ref><ref type="bibr" target="#b61">Yan et al., 2020)</ref>.</p><p>This paper aims at aggressively trimming down the explosive GNN complexity, from the algorithm level. There are two streams of works: simplifying the graph, or simplifying the model. For the first stream, many have explored various sampling-based strategies <ref type="bibr" target="#b30">(Hübler et al., 2008;</ref><ref type="bibr" target="#b8">Chakeri et al., 2016;</ref><ref type="bibr" target="#b7">Calandriello et al., 2018;</ref><ref type="bibr" target="#b1">Adhikari et al., 2017;</ref><ref type="bibr" target="#b35">Leskovec &amp; Faloutsos, 2006;</ref><ref type="bibr" target="#b56">Voudigari et al., 2016;</ref><ref type="bibr" target="#b19">Eden et al., 2018;</ref><ref type="bibr" target="#b69">Zhao, 2015;</ref><ref type="bibr" target="#b9">Chen et al., 2018a)</ref>, often combined with mini-batch training algorithms for locally aggregating and updating features. <ref type="bibr" target="#b70">Zheng et al. (2020)</ref> investigated graph sparsification, i.e., pruning input graph edges, and learned an extra DNN surrogate. <ref type="bibr" target="#b38">Li et al. (2020b)</ref> also addressed graph sparsification by formulating an optimization objective, solved by alternating direction method of multipliers (ADMM) <ref type="bibr" target="#b4">(Bertsekas &amp; Rheinboldt, 1982)</ref>.</p><p>The second stream of efforts were traditionally scarce, since the DNN parts of most GNNs are (comparably) lightly parameterized, despite the recent emergence of increasingly deep GNNs <ref type="bibr" target="#b36">(Li et al., 2019)</ref>. Although model compression is well studied for other types of DNNs <ref type="bibr" target="#b15">(Cheng et al., 2017)</ref>, it has not been discussed much for GNNs. One latest work <ref type="bibr" target="#b52">(Tailor et al., 2021)</ref> explored the viability of training quantized GNNs, enabling the usage of low precision integer arithmetic during inference. Other forms of well-versed DNN compression techniques, such as model pruning <ref type="bibr" target="#b28">(Han et al., 2016)</ref>, have not been exploited for GNNs up to our best knowledge. More importantly, no prior discussion was placed on jointly simplifying the input graphs and the models for GNN inference. In view of such, this paper asks: to what extent could we co-simplify the input graph and the model, for ultra-efficient GNN inference?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Summary of Our Contributions</head><p>This paper makes multi-fold contributions to answer the above questions. Unlike pruning convolutional DNNs which are heavily overparameterized, directly pruning the much less parameterized GNN model would have only limited room to gain. Our first technical innovation is to for the first time present an end-to-end optimization framework called unified GNN sparsification (UGS) that simultaneously prunes the graph adjacency matrix and the model weights. UGS makes no assumption to any GNN architecture or graph structure, and can be flexibly applied across various graph-based learning scenarios at scale.</p><p>Considering UGS as the generalized pruning for GNNs, our second technical innovation is to generalize the popular lottery ticket hypothesis (LTH) to GNNs for the first time. LTH <ref type="bibr" target="#b21">(Frankle &amp; Carbin, 2018)</ref> demonstrates that one can identify highly sparse and independently trainable subnetworks from dense models, by iterative pruning. It was initially observed in convolutional DNNs, and later broadly found in natural language processing (NLP) <ref type="bibr" target="#b12">(Chen et al., 2020b)</ref>, generative models <ref type="bibr" target="#b31">(Kalibhat et al., 2020)</ref>, reinforcement learning <ref type="bibr" target="#b67">(Yu et al., 2020)</ref> and lifelong learning <ref type="bibr" target="#b12">(Chen et al., 2020b)</ref>. To meaningfully generalize LTH to GNNs, we define a graph lottery ticket (GLT) as a pair of core sub-dataset and sparse sub-network which can be jointly identified from the full graph and the original GNN model, by iteratively applying UGS. Like its counterpart in convolutional DNNs, a GLT could be trained from its initialization to match the performance of training with the full model and graph, and its inference cost is drastically smaller.</p><p>Our proposal has been experimentally verified, across various GNN architectures and diverse tasks, on both smallscale graph datasets (Cora, Citeseer and PubMed), and largescale datasets from the challenging Open Graph Benchmark (OGB). Our main observations are outlined below:</p><p>• UGS is widely applicable to simplifying a GNN during training and reducing its inference MACs (multiply-accumulate operations). Moreover, by iteratively applying UGS, GLTs can be broadly located from for both shallow and deep GNN models, on both smalland large-scale graph datasets, with substantially reduced inference costs and unimpaired generalization.</p><p>• For node classification, our found GLTs achieve 20% ∼ 98% MACs saving, with up to 5% ∼ 58.19% sparsity on graphs and 20% ∼ 97.75% sparsity on GNN models, at little to no performance degradation. For example in Figure <ref type="figure">1</ref>, on Cora and Citeseer node classification, our GLTs (#) achieve comparable or sometimes even slightly better performance than the baselines of full models and graphs (•), with only 41.16% and 5.57% MACs, respectively.</p><p>• For link prediction, GLTs lead to 48% ∼ 97% and 70% MACs saving, coming from up to 22.62% ∼ 55.99% sparsity on graphs and and 67.23% ∼ 97.19% sparsity on GNN models, again without performance loss.</p><p>• Our proposed framework can scale up to deep GNN models (up to 28 layers) on large graphs (e.g., Ogbn-ArXiv and Ogbn-Proteins), without bells and whistles.</p><p>• Besides from random initializations, GLTs can also be drawn from the initialization via self-supervised pre-training -an intriguing phenomenon recently just reported for NLP models <ref type="bibr" target="#b12">(Chen et al., 2020b)</ref>. Using a latest GNN pre-training algorithm <ref type="bibr" target="#b64">(You et al., 2020b)</ref> for initialization, GLTs can be found to achieve robust performance with even sparser graphs and GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Graph Neural Networks. There are mainly three categories of GNNs <ref type="bibr" target="#b18">(Dwivedi et al., 2020)</ref>: i) extending original convolutional neural networks to the graph regime <ref type="bibr" target="#b50">(Scarselli et al., 2008;</ref><ref type="bibr" target="#b5">Bruna et al., 2013;</ref><ref type="bibr" target="#b34">Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b27">Hamilton et al., 2017)</ref>; ii) introducing anisotropic operations on graphs such as gating and attention <ref type="bibr" target="#b3">(Battaglia et al., 2016;</ref><ref type="bibr" target="#b42">Monti et al., 2017;</ref><ref type="bibr" target="#b54">Veličković et al., 2018)</ref>, and iii) improving upon limitations of existing models <ref type="bibr" target="#b60">(Xu et al., 2019;</ref><ref type="bibr" target="#b43">Morris et al., 2019;</ref><ref type="bibr" target="#b14">Chen et al., 2019;</ref><ref type="bibr" target="#b44">Murphy et al., 2019)</ref>. Among this huge family, Graph Convolutional Networks (GCNs) are widely adopted, which can be categorized as spectral domain based methods <ref type="bibr" target="#b17">(Defferrard et al., 2016;</ref><ref type="bibr" target="#b34">Kipf &amp; Welling, 2016)</ref> and spatial domain bases methods <ref type="bibr" target="#b51">(Simonovsky &amp; Komodakis, 2017;</ref><ref type="bibr" target="#b27">Hamilton et al., 2017)</ref>.</p><p>The computational cost and memory usage of GNNs will expeditiously increase with the graph size. The aim of graph sampling or sparsification is to extract a small subgraph from the original large one, which can remain effective for learning tasks <ref type="bibr" target="#b70">(Zheng et al., 2020;</ref><ref type="bibr" target="#b27">Hamilton et al., 2017)</ref> while reducing the cost. Previous works on sampling focus on preserving certain pre-defined graph metrics <ref type="bibr" target="#b30">(Hübler et al., 2008)</ref>, graph spectrum <ref type="bibr" target="#b8">(Chakeri et al., 2016;</ref><ref type="bibr" target="#b1">Adhikari et al., 2017)</ref>, or node distribution <ref type="bibr" target="#b35">(Leskovec &amp; Faloutsos, 2006;</ref><ref type="bibr" target="#b56">Voudigari et al., 2016;</ref><ref type="bibr" target="#b19">Eden et al., 2018)</ref>. FastGCN <ref type="bibr" target="#b9">(Chen et al., 2018a)</ref>   <ref type="bibr" target="#b20">(Evci et al., 2019;</ref><ref type="bibr" target="#b49">Savarese et al., 2020;</ref><ref type="bibr" target="#b39">Liu et al., 2019;</ref><ref type="bibr" target="#b63">You et al., 2020a;</ref><ref type="bibr" target="#b24">Gale et al., 2019;</ref><ref type="bibr" target="#b67">Yu et al., 2020;</ref><ref type="bibr" target="#b31">Kalibhat et al., 2020;</ref><ref type="bibr" target="#b13">Chen et al., 2021;</ref><ref type="bibr">2020b)</ref>. However, GNN is NOT "yet another" field that can be easily cracked by LTH. That is again due to GNNs having much smaller models, while all the aforementioned LTH works focus on simplifying their redundant models. To our best knowledge, this work is not only the first to generalize LTH to GNNs, but also the first to extend LTH from simplifying models to a new data-model co-simplification prospect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Notations and Formulations</head><p>Let G = {V, E} represent an undirected graph with |V| nodes and |E| edges. For V = {v 1 , ..., v |V| }, let X ∈ R |V|×F denote the node feature matrix of the whole graph, where</p><formula xml:id="formula_0">x i = X[i, :] is the F-dimensional attribute vector of node v i ∈ V.</formula><p>As for E = {e 1 , ..., e |E| }, e n = (v i , v j ) ∈ E means that there exists a connection between node v i and v j . An adjacency matrix A ∈ R |V|×|V| is defined to describe the overall graph topology, where</p><formula xml:id="formula_1">A[i, j] = 1 if (v i , v j ) ∈ E else A[i, j] = 0.</formula><p>For example, the two-layer GNN <ref type="bibr" target="#b34">(Kipf &amp; Welling, 2016</ref>) can be defined as follows:</p><formula xml:id="formula_2">Z = S( Âσ( ÂXΘ (0) )Θ (1) ),<label>(1)</label></formula><p>where Z is the prediction of GNN f (G, Θ). The graph G can be alternatively denoted as {A, X}, Θ = (Θ (0) , Θ (1) ) is the weights of the two-layer GNN, S(•) represents the softmax function, σ(•) denotes the activation function (e.g., ReLU), Â = D− 1 2 (A + I) D 1 2 is normalized by the degree matrix D of A + I. Considering the transductive semisupervised classification task, the objective function L is:</p><formula xml:id="formula_3">L(G, Θ) = − 1 |V label | vi∈V label y i log(z i ), (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>where L is the cross-entropy loss over all labeled samples V label ⊂ V, and y i is the annotated label vector of node v i for its corresponding prediction z i = Z[i, :].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Unified GNN Sparsification</head><p>We present out end-to-end framework, Unified GNN Sparsification (UGS), to simultaneously reduce edges in G and the parameters in GNNs. Specifically, we introduce two differentiable masks m g and m θ for indicating the insignificant connections and weights in the graph and GNNs, respectively. The shapes of m g and m θ are identical to those the adjacency matrix A and the weights Θ, respectively. Given A, Θ, m g and m θ are co-optimized from end to end, under the following objective:</p><formula xml:id="formula_5">L UGS := L({m g A, X}, m θ Θ) +γ 1 m g 1 + γ 2 m θ 1 ,<label>(3)</label></formula><p>where is the element-wise product, γ 1 and γ 2 are the hyparameters to control 1 sparsity regularizers of m g and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GNNs</head><p>Sparse Graph Sparse GNNs</p><p>Iterative Unified Graph Sparsification</p><p>Figure <ref type="figure">2</ref>. An illustration of unified GNN sparsification (UGS). Dash/solid lines denote the removed/remaining edges and weights in the graph and GNNs, respectively. Note that graphs and GNNs are co-optimized to find optimal solutions for the unified sparsification.</p><p>Algorithm 1 Unified GNN Sparsification (UGS)</p><formula xml:id="formula_6">Input: Graph G = {A, X}, GNN f (G, Θ 0 ), GNN's ini- tialization Θ 0 , initial masks m 0 g = A, m 0 θ = 1 ∈ R Θ0 0 ,</formula><p>Step size η, λ g , and λ θ . Output: Sparsified masks m g and m θ 1:</p><formula xml:id="formula_7">for iteration i = 0, 1, 2, ..., N − 1 do 2: Forward f (•, m i θ Θ i ) with G = {m i g A, X} to compute the loss L UGS in Equation 3. 3: Backpropagate to update Θ i+1 ← Θ i −η∇ Θi L UGS . 4: Update m i+1 g ← m i g − λ g ∇ m i g L UGS . 5: Update m i+1 θ ← m i θ − λ θ ∇ m i θ L UGS . 6: end for 7: Set p g = 5% of the lowest magnitude values in m N</formula><p>g to 0 and others to 1, then obtain m g . 8: Set p θ = 20% of the lowest magnitude values in m N θ to 0 and others to 1, then obtain m θ . m θ respectively. After the training is done, we set the lowest-magnitude elements in m g and m θ to zero, w.r.t. pre-defined ratios p g and p θ . Then, the two sparse masks are applied to prune A and Θ, leading to the final sparse graph and model. Alg. 1 outlines the procedure of UGS, and it can be considered as the generalized pruning for GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Graph Lottery Tickets</head><p>Graph lottery tickets (GLT). Given a GNN f (•, Θ) and a graph G = {A, X}, the associated subnetworks of GNN and sub-graph can be defined as f (•, m θ Θ) and G s = {m g A, X}, where m g and m θ are binary masks defined in Section 3.2. If a subnetwork f (•, m θ Θ) trained on a sparse graph G s has performance matching or surpassing the original GNN trained on the full graph G in terms of achieved standard testing accuracy, then we define f ({m g A, X}, m θ Θ 0 ) as a unified graph lottery tickets (GLTs), where Θ 0 is the original initialization for GNNs which the found lottery ticket subnetwork is usually trained from.</p><p>Unlike previous LTH literature <ref type="bibr" target="#b21">(Frankle &amp; Carbin, 2018)</ref>, our identified GLT will consist of three elements: i) a sparse graph G s = {m g A, X}; ii) the sparse mask m θ for the model weight; and iii) the model weight's initialization Θ 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Iterative UGS to find Graph Lottery Tickets</head><formula xml:id="formula_8">Input: Graph G = {A, X}, GNN f (G, Θ 0 ), GNN's</formula><p>initialization Θ 0 , pre-defined sparsity levels s g for graphs and s θ for GNNs, Initial masks</p><formula xml:id="formula_9">m g = A, m θ = 1 ∈ R Θ0 0 . Output: GLT f ({m g A, X}, m θ Θ 0 ) 1: while 1 − mg 0 A 0 &lt; s g and 1 − m θ 0 Θ 0 &lt; s θ do 2: Sparsify GNN f (•, m θ Θ 0 ) with G = {m g A, X} using UGS, as presented in Algorithm 1. 3:</formula><p>Update m g and m θ accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Rewinding GNN's weights to Θ 0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Rewinding masks, m g = m g A 6: end while Finding GLT. Classical LTH leverages iterative magnitude-based pruning (IMP) to identify lottery tickets. In a similar fashion, we apply our UGS algorithm to prune both the model and the graph during training, as outlined Algorithm 2, obtaining the graph mask m g and model weight mask m θ of GLT. Then, the GNN weights are rewound to the original initialization Θ. We repeat the above two steps iteratively, until reaching the desired sparsity s g and s θ for the graph and GNN, respectively.</p><p>Complexity analysis of GLTs. The inference time com-</p><formula xml:id="formula_10">plexity of GLTs is O(L × m g A 0 × F + L × m θ 0 × |V| × F 2 )</formula><p>, where L is the number of layers, m g A 0 is the number of remaining edges in the sparse graph, F is the dimension of node features, |V| is the number of nodes. The memory complexity is O(L×|V|×F+L× m θ 0 ×F 2 ). In our implementation, pruned edges will be removed from E, and would not participate in the next round's computation. Baseline GLT 0 1 0 0 2 0 0 3 0 0 4 0 0 5 0 0 6 0 0 7 0 0 8 0 0 9 0 0 1 0 0 0 1 1 0 0 1 2 0 0 1 3 0 0 1 4 0 0 1 5 0 0 1 6 0 0 1 7 0 0 1 8 0 0 1 9 0 0 2 0 0 0  <ref type="formula">2007M</ref>) GLT (826M) 0 1 0 0 2 0 0 3 0 0 4 0 0 5 0 0 6 0 0 7 0 0 8 0 0 9 0 0 1 0 0 0 1 1 0 0 1 2 0 0 1 3 0 0 1 4 0 0 1 5 0 0 1 6 0 0 1 7 0 0 1 8 0 0 1 9 0 0 2 0 0 0 Inference MACs (M) 50 55 60 65 70 75 80 UGS (Ours) Random Pruning Baseline (2015M) GLT (1615M) 0 1 0 0 0 2 0 0 0 3 0 0 0 4 0 0 0 5 0 0 0 6 0 0 0 7 0 0 0 8 0 0 0 9 0 0 0 1 0 0 0 0 1 1 0 0 0 1 2 0 0 0 1 3 0 0 0 1 4 0 0 0 1 5 0 0 0   <ref type="formula">6323M</ref>) GLT (141M) 0 2 0 0 0 4 0 0 0 6 0 0 0 8 0 0 0 1 0 0 0 0 1 2 0 0 0 1 4 0 0 0 1 6 0 0 0 1 8 0 0 0 2 0 0 0 0 2 2 0 0 0 2 4 0 0 0 2 6 0 0 0 2 8 0 0 0 3 0 0 0 0 3 2 0 0 0 3 4 0 0 0 3 6 0 0 0 3 8 0 0 0 4 0 0 0 0 4 2 0 0 0 4 4 0 0 0 4 6 0 0 0 4 8 0 0 0 5 0 0 0 0 Baseline GLT 0 2 5 0 5 0 0 7 5 0 1 0 0 0 1 2 5 0 1 5 0 0 1 7 5 0 2 0 0 0 2 2 5 0 2 5 0 0 2 7 5 0 3 0 0 0 3 2 5 0 3 5 0 0 3 7 5 0 4 0 0 0 4 2 5 0 4 5 0 0 4 7 5 0 5 0 0 0 Inference MACs (M) 40 50 60 70 80 Accuracy (PubMed) UGS (Ours) Random Pruning ADMM Baseline (5168M) GLT (152M) 0 2 5 0 5 0 0 7 5 0 1 0 0 0 1 2 5 0 1 5 0 0 1 7 5 0 2 0 0 0 2 2 5 0 2 5 0 0 2 7 5 0 3 0 0 0 3 2 5 0 3 5 0 0 3 7 5 0 4 0 0 0 4 2 5 0 4 5 0 0 4 7 5 0 5 0 0 0 Inference MACs (M) 70 72 74 76 78 80 UGS (Ours) Random Pruning Baseline (5188M) GLT (345M) 0 2 0 0 0 4 0 0 0 6 0 0 0 8 0 0 0 1 0 0 0 0 1 2 0 0 0 1 4 0 0 0 1 6 0 0 0 1 8 0 0 0 2 0 0 0 0 2 2 0 0 0 2 4 0 0 0 2 6 0 0 0 2 8 0 0 0 3 0 0 0 0 3 2 0 0 0 3 4 0 0 0 3 6 0 0 0 3 8 0 0 0 4 0 0 0 0  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, extensive experiments are reported to validate the effectiveness of UGS and the existence of GLTs across diverse graphs and GNN models. Our subjects in-clude small-and medium-scale graphs with two-layer Graph Convolutional Network (GCN) <ref type="bibr" target="#b34">(Kipf &amp; Welling, 2016)</ref>, Graph Isomorphism Network (GIN) <ref type="bibr" target="#b59">(Xu et al., 2018)</ref>, and Graph Attention Network (GAT) <ref type="bibr" target="#b53">(Veličković et al., 2017)</ref> 0 1 0 0 2 0 0 3 0 0 4 0 0 5 0 0 6 0 0 7 0 0 8 0 0 9 0 0 1 0 0 0 <ref type="table" target="#tab_10">1 1 0 0  1 2 0 0  1 3 0 0  1 4 0 0  1 5 0 0  1 6 0 0  1 7 0 0  1 8 0 0  1 9 0 0  2 0 0</ref>   <ref type="table" target="#tab_10">2 0 0  3 0 0  4 0 0  5 0 0  6 0 0  7 0 0  8 0 0  9 0 0  1 0 0 0  1 1 0 0  1 2 0 0  1 3 0 0  1 4 0 0  1 5 0 0  1 6 0 0  1 7 0 0  1 8 0 0  1 9 0 0  2 0 0</ref>   <ref type="table" target="#tab_10">2 0 0 0  3 0 0 0  4 0 0 0  5 0 0 0  6 0 0 0  7 0 0 0  8 0 0 0  9 0 0 0  1 0 0 0 0  1 1 0 0 0  1 2 0 0 0  1 3 0 0 0  1 4 0 0 0  1 5 0 0</ref>   <ref type="formula">6323M</ref>) GLT (1680M) 0 2 0 0 0 4 0 0 0 6 0 0 0 8 0 0 0 1 0 0 0 0 1 2 0 0 0 1 4 0 0 0 1 6 0 0 0 1 8 0 0 0 2 0 0 0 0 2 2 0 0 0 2 4 0 0 0 2 6 0 0 0 2 8 0 0 0 3 0 0 0 0 3 2 0 0 0 3 4 0 0 0 3 6 0 0 0 3 8 0 0 0 4 0 0 0 0 4 2 0 0 0 4 4 0 0 0 4 6 0 0 0 4 8 0 0 0 5 0 0 0 0 Inference MACs (M) 91 92 93 94 95 96 UGS (Ours) Random Pruning Baseline (50293M) GLT (4343M) 0 2 5 0 5 0 0 7 5 0 1 0 0 0 1 2 5 0 1 5 0 0 1 7 5 0 2 0 0 0 2 2 5 0 2 5 0 0 2 7 5 0 3 0 0 0 3 2 5 0 3 5 0 0 3 7 5 0 4 0 0 0 4 2 5 0 4 5 0 0 4 7 5 0 5 0 0 0  <ref type="formula">5168M</ref>) GLT (599M) 0 2 5 0 5 0 0 7 5 0 1 0 0 0 1 2 5 0 1 5 0 0 1 7 5 0 2 0 0 0 2 2 5 0 2 5 0 0 2 7 5 0 3 0 0 0 3 2 5 0 3 5 0 0 3 7 5 0 4 0 0 0 4 2 5 0 4 5 0 0 4 7 5 0 5 0 0 0 Inference MACs (M) 80 82 84 86 88 90 UGS (Ours) Random Pruning Baseline (5188M) GLT (2697M) 0 2 0 0 0 4 0 0 0 6 0 0 0 8 0 0 0 1 0 0 0 0 1 2 0 0 0 1 4 0 0 0 1 6 0 0 0 1 8 0 0 0 2 0 0 0 0 2 2 0 0 0 2 4 0 0 0 2 6 0 0 0 2 8 0 0 0 3 0 0 0 0 3 2 0 0 0 3 4 0 0 0 3 6 0 0 0 3 8 0 0 0 4 0 0 0 0  in Section 4.2; as well as large-scale graphs with 28-layer deep ResGCNs <ref type="bibr" target="#b37">(Li et al., 2020a)</ref> in Section 4.2. Besides, in Section 4.3, we investigate GLTs under the self-supervised pre-training <ref type="bibr" target="#b64">(You et al., 2020b)</ref>. Ablation studies and visualizations are provided in Section 4.4 and 4.5.</p><p>Datasets We use popular semi-supervised graph datasets: Cora, Citeseer and PubMed <ref type="bibr" target="#b34">(Kipf &amp; Welling, 2016)</ref>, for both node classification and link prediction tasks. For experiments on large-scale graphs, we use the Open Graph Benchmark (OGB) <ref type="bibr" target="#b29">(Hu et al., 2020)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The Existence of Graph Lottery Ticket</head><p>We first examine whether unified graph lottery tickets exist and can be located by UGS. Results of GCN/GIN/GAT on Cora/Citesser/PubMed for node classification and link prediction are collected in Figures <ref type="figure" target="#fig_5">3 and 4</ref>, respectively.</p><p>Note that each point in the figures denotes the achieved performance with respect to a certain graph sparsity, GNN sparsity, and inference MACs. However, due to the limited space, we only include one or two of these three sparsity indicators in the main text, and the rest can be found in Appendix A2. We list the following Observations.</p><p>Obs.1. GLTs broadly exist with substantial MACs saving. Graph lottery tickets at a range of graph sparsity from 5% to 58.19% without performance deterioration, can be identified across GCN, GIN and GAT on Cora, Citeseer, and PubMed datasets for both node classification and link prediction tasks. Such GLTs significantly reduce 59% ∼ 97%, 20% ∼ 98%, 91% ∼ 97% inference MACs for GCN, GIN and GAT across all datasets.</p><p>Obs.2. UGS is flexible and consistently shows superior performance. UGS consistently surpasses random pruning by substantial performance margins across all datasets and GNNs, which validates the effectiveness of our proposal. The previous state-of-the-art method, i.e., ADMM <ref type="bibr" target="#b38">(Li et al., 2020b)</ref>, achieves a competitive performance to UGS at moderate graph sparsity levels, and performs 3 ∼ 4% worse than UGS when graphs are heavily pruned.</p><p>Note that the ADMM approach by <ref type="bibr" target="#b38">Li et al. (2020b)</ref> is only applicable when two conditions are met: i) graphs are stored via adjacency matrices-however, that is not practical for large graphs <ref type="bibr" target="#b29">(Hu et al., 2020)</ref>; ii) aggregating features with respect to adjacency matrices-however, recent designs of GNNs (e.g., GIN and GAT) commonly use the much more computation efficient approach of synchronous/asynchronous message passing <ref type="bibr" target="#b26">(Gilmer et al., 2017;</ref><ref type="bibr" target="#b6">Busch et al., 2020)</ref>. On the contrary, our proposed UGS is flexible enough and free of these limitations.</p><p>Obs.3. GNN-specific and Graph-specific analyses: GAT is more amenable to sparsified graphs; Cora is more sensitive to pruning. As demonstrated in Figures <ref type="figure" target="#fig_5">3 and 4</ref>, compared to GCN and GIN, GLTs in GAT can be found at higher sparsity levels; meanwhile randomly pruned graphs and GAT can still reach satisfied performance and maintain higher accuracies on severely sparsified graphs. One possible explanation is that attention-based aggregation is capable of re-identifying important connections in pruned graphs which makes GAT be more amenable to sparsification. Compared the sparsity of located GLTs (i.e., the position of red stars (#)) across three graph datasets, we find that Cora is the most sensitive graph to pruning and PubMed is more robust to be sparsified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Scale Up Graph Lottery Tickets</head><p>To scale up graph lottery tickets, we further conduct experiments on 28-layer deep ResGCNs <ref type="bibr" target="#b37">(Li et al., 2020a)</ref> on largescale datasets that have more than millions of connections, like Ogbn-ArXiv and Ogbn-Proteins for node classification, Ogbl-Collab for link prediction in Table <ref type="table" target="#tab_1">1</ref>. We summarize our observations and derive insights below. Obs.4. UGS is scaleable and GLT exists in deep GCNs on large-scale datasets. Figure <ref type="figure" target="#fig_6">5</ref> demonstrates that UGS can be scaled up to deep GCNs on large-scale graphs. Found GLTs obtain matched performance with 85%, 25%, 70% MACs saving on Ogbn-ArXiv, Ogbn-Proteins, and Ogbl-Collab, respectively.</p><p>Obs.5. Denser graphs (e.g., Ogbn-Proteins) are more resilient to sparsification. As shown in Figure <ref type="figure" target="#fig_6">5</ref>, comparing the node classification results on Ogbn-ArXiv (Ave. degree: 13.77) and Ogbn-Proteins (Ave. degree: 597.00), Ogbn-Proteins has a negligible performance gap between UGS and random pruning, even on heavily pruned graphs. Since nodes with high degrees in denser graphs have less chance to be totally isolated during pruning, it may contribute to more robustness to sparsification. Similar observations can be drawn from the comparison between PubMed and other two small graphs in Figure <ref type="figure" target="#fig_5">3 and 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Graph Lottery Ticket from Pre-training</head><p>0 1 0 0 2 0 0 3 0 0 4 0 0 5 0 0 6 0 0 7 0 0 8 0 0 9 0 0 1 0 0 0 1 1 0 0 1 2 0 0 1 3 0 0 1 4 0 0 1 5 0 0 1 6 0 0 1 7 0 0 1 8 0 0 1 9 0 0 2 0 0 0 Node Classification UGS GraphCL+UGS 0 1 0 0 2 0 0 3 0 0 4 0 0 5 0 0 6 0 0 7 0 0 8 0 0 9 0 0 1 0 0 0 1 1 0 0 1 2 0 0 1 3 0 0 1 4 0 0 1 5 0 0 1 6 0 0 1 7 0 0 1 8 0 0 1 9 0 0 2 0 0 0 Inference MACs (M) High-quality lottery tickets can be drawn from selfsupervised pre-trained models, as recently found in both NLP and computer vision fields <ref type="bibr" target="#b12">(Chen et al., 2020b;</ref><ref type="bibr">a)</ref>. In the GLT case, we also assess the impact of replacing random initialization with self-supervised graph pre-training, i.e., GraphCL <ref type="bibr" target="#b64">(You et al., 2020b)</ref>, on transductive semisupervised node classification and link prediction.</p><p>From Figure <ref type="figure" target="#fig_7">6</ref> and A12, we gain a few interesting observations. First, UGS with the GraphCL pre-trained initialization consistently presents superior performance at moderate sparsity levels (≤ 40% graph sparsity ≤ 85% MACs saving). While the two settings perform similar at extreme sparsity, it indicates that for excessively pruned graphs, the initialization is no longer the performance bottleneck; Second, GraphCL benefits GLT on multiple downstream tasks including node classification and link prediction; Third, especially on the transductive semi-supervised setup, GLTs with appropriate sparsity levels can even enlarge the performance gain from pre-training, for example, see GLT on Citeseer with 22.62% graph sparsity and 2068M inference MACs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>Pruning ratio p g and p θ . We extensively investigate the pruning ratios p g , p θ in UGS for graph and GNN sparsification. As shown in Figure <ref type="figure" target="#fig_8">7</ref>, with a fixed p θ = 20%, only the setting of p g = 5% can identify the GLT, and it performs close to p g = 10% at higher sparsity levels (e.g., ≥ 25%). Aggressively pruning the graph's connections in each round of iterative UGS, e.g., p g = 20%, leads to substantially degraded accuracies, especially for large sparsities. On the other hand, with a fixed p g = 20%, all there settings of p θ = 10%, 20%, 40% show similar performance, and even higher pruning ratios produce slight better results. It again verifies that the key bottleneck in pruning GNNs mainly lies in the sparsification of graphs. In summary, we adopt p g = 5% and p g = 20% (follow previous LTH works <ref type="bibr" target="#b21">(Frankle &amp; Carbin, 2018</ref>)) for all the experiments.  Random graph lottery tickets. Randomly re-initializing located sparse models, i.e., random lottery tickets, usually serves as a necessary baseline for validating the effectiveness of rewinding processes <ref type="bibr" target="#b21">(Frankle &amp; Carbin, 2018)</ref>. In Table <ref type="table" target="#tab_10">2</ref>, we compare GLT to Random GLT, the latter by randomly re-initializing GNN's weights and learnable masks, and GLT shows aapparently superior performance, consistent with previous observations <ref type="bibr" target="#b21">(Frankle &amp; Carbin, 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Visualization and Analysis</head><p>In this section, we visualize the sparsified graphs in GLTs from UGS in Figure <ref type="figure">8</ref>, and further measure the graph properties<ref type="foot" target="#foot_0">1</ref> shown in Table <ref type="table">A5</ref>, including clustering coefficient <ref type="bibr" target="#b40">(Luce &amp; Perry, 1949)</ref>, as well as node and edge betweenness centrality <ref type="bibr" target="#b23">(Freeman, 1977)</ref>. Specifically, clustering coefficient measures the proportion of edges between the nodes within a given node's neighborhood; node and edge betweenness centrality show the degree of central a vertex or an edge is in the graph <ref type="bibr" target="#b45">(Narayanan, 2005)</ref>. Reported numbers in Table <ref type="table">A5</ref> are averaged over all the nodes. Both Figure <ref type="figure">8</ref> and Table <ref type="table">A5</ref> show that sparse graphs obtained from UGS seem to maintain more "critical" vertices which used to have much denser connections. It may provide possible insights on what GLTs prefer to preserve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Discussion</head><p>In this paper, we first propose unified GNN sparsification to generalize the notion or pruning in GNNs. We further establish the LTH for GNNs, by leveraging UGS and considering a novel joint data-model lottery ticekt. The new unified LTH for GNNs generalizes across various GNN architectures, learning tasks, datasets, and even initialization ways. In general, we find GLT to tremendously trim down the inference MACs, without sacrificing task performance.</p><p>It remains open how much we could translate GLT's high sparsity into practical acceleration and energy-saving benefits. Most DNN accelerators are optimized for dense and regular computation, making edge-based operations hard to implement efficiently. To our best knowledge, the hardware acceleration research on GNNs just starts to gain interests <ref type="bibr" target="#b2">(Auten et al., 2020;</ref><ref type="bibr" target="#b0">Abadal et al., 2020;</ref><ref type="bibr" target="#b25">Geng et al., 2020;</ref><ref type="bibr" target="#b58">Wang et al., 2020;</ref><ref type="bibr" target="#b33">Kiningham et al., 2020)</ref>. We expect GLT to be implemented using sparse-dense matrix multiplication (SpMM) operations from highly optimized sparse matrix libraries, such as Intel MKL <ref type="bibr" target="#b57">(Wang et al., 2014)</ref> or cuSPARSE <ref type="bibr" target="#b46">(Naumov et al., 2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1. More Implementation Details</head><p>Datasets Download Links. As for small-scale datasets, we take the commonly used semi-supervised node classification graphs: Cora, Citeseer and pubMed. For larger-scale datasets, we use three Open Graph Benchmark (OGB) <ref type="bibr" target="#b29">(Hu et al., 2020)</ref> datasets: Ogbn-ArXiv, Ogbn-Proteins and Ogbl-Collab. All the download links of adopted graph datasets are included in Table <ref type="table" target="#tab_12">A3</ref>. More Details about GNNs. As for small-and mediumscale datasets Cora, Citeseer and PubMed, we choose the two-layer GCN/GIN/GAT networks with 512 hidden units to conduct all our experiments. As for large-scale datasets Ogbn-ArXiv, Ogbn-Proteins and Ogbl-Collab, we use the ResGCN <ref type="bibr" target="#b37">(Li et al., 2020a)</ref> with 28 GCN layers to conduct all our experiments. As for Ogbn-Proteins dataset, we also use the edge encoder module, which is a linear transform function in each GCN layer to encode the edge features. And we found if we also prune the weight of this module together with other weight, it will seriously hurt the performance, so we do not prune them in all of our experiments.</p><p>Training Details and Hyper-parameter Configuration.</p><p>We conduct numerous experiments with different hyperparameter, such as iterations, learning rate, γ 1 , γ 2 , and we choose the best hyper-parameter configuration to report the final results. All the training details and hyper-parameters are summarzed in Table <ref type="table">A4</ref>. As for Ogbn-Proteins dataset, due to its too large scale, we use the commonly used random sample method <ref type="bibr" target="#b37">(Li et al., 2020a)</ref> to train the whole graph. Specifically, we random sample ten subgraphs from the whole graph and we only feed one subgraph to the GCN at each iteration. For each subgraph, we train 10 iterations to ensure better convergence. And we train 100 epochs for the whole graph (100 iterations for each subgraph).</p><p>Evaluation Details We report the test accuracy/ROC-AUC/Hits@50 according to the best validation results during the training process to avoid overfitting. All training and evaluation are conducted for one run. As for the previous state-of-the-art method ADMM <ref type="bibr" target="#b38">(Li et al., 2020b)</ref>, we use the same training and evaluation setting as the original paper description, and we can reproduce the similar results compared with original paper.</p><p>Computing Infrastructures We use the NVIDIA Tesla V100 (32GB GPU) to conduct all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2. More Experiment Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.1. Node Classification on Small-and Medium-scale Graphs with shallow GNNs</head><p>As shown in Figure <ref type="figure" target="#fig_9">A9</ref>, we also provide extensive results over GNN sparsity of GCN/GIN/GAT on three small datasets, Cora/Citeseer/PubMed. We observe that UGS finds the graph wining tickets at a range of GNNs sparsity from 20% ∼ 90% without performance deterioration, which significantly reduces MACs and the storage memory during both training and inference processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.2. Link Prediction on Small-and Medium-scale Graphs with shallow GNNs</head><p>More results of link prediction with GCN/GIN/GAT on Cora/Citeseer/PubMed datasets are shown in Figure <ref type="figure" target="#fig_10">A10</ref>.</p><p>We observe the similar phenomenon as the node classification task: using our proposed UGS can find the graph wining tickets at a range of graph sparsity from 5% ∼ 50% and GNN sparsity from 20% ∼ 90% without performance deterioration, which greatly reduce the computational cost and storage space during both training and inference processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.3. Large-scale Graphs with Deep ResGCNs</head><p>More results of larger-scale graphs with deep ResGCNs are shown in Figure <ref type="figure">A11</ref>. Results show that our proposed UGS can found GLTs, which can reach the non-trivial sparsity levels of graph 30% ∼ 50% and weight 20% ∼ 80% without performance deterioration. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.4. Graph Lottery Ticket with Pre-training</head><p>More results of node classification and link prediction on Cora and Citeseer dataset of GraphCL <ref type="bibr" target="#b64">(You et al., 2020b)</ref> pre-training are shown in Figure <ref type="figure">A12</ref>. Results demonstrate that when using self-supervised pre-training, UGS can identify graph lottery tickets with higher qualities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.5. More Analyses of Sparsified Graphs</head><p>As shown in Table <ref type="table">A5</ref>, graph measurements are reported, including clustering coefficient, node and egde betweeness centrality. Results indicate that UGS seems to produce sparse graphs with more "critical" vertices which used to have more connections.</p><p>0 . 0 0 5 . 0 0 9 . 7 5 1 4 . 2 6 1 8 . 5 5 2 2 . 6 2 2 6 . 4 9 3 0 . 1 7 3 3 . 6 6 3 6 . 9 8 4 0 . 1 3 4 3 . 1 2 4 5 . 9 6 4 8 . 6 7 5 1 . 2 3 5 3 . 6 7 5 5 . 9 9 5 8 . 0 . 0 0 5 . 0 0 9 . 7 5 1 4 . 2 6 1 8 . 5 5 2 2 . 6 2 2 6 . 4 9 3 0 . 1 7 3 3 . 6 6 3 6 . 9 8 4 0 . 1 3 4 3 . 1 2 4 5 . 9 6 4 8 . 6 7 5 1 . 2 3 5 3 . 6 7 5 5 . 9 9 5 8 .  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Node classification performance over achieved graph sparsity levels or inference MACs of GCN, GIN, and GAT on Cora, Citeseer, and PubMed datasets, respectively. Red stars (#) indicate the located GLTs, which reach comparable performance with high sparsity and low inference MACs. Dash lines represent the baseline performance of full GNNs on full graphs. More results over GNN sparsity are provided in Appendix A2.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Link prediction performance over inference MACs of GCN, GIN, and GAT on Cora, Citeseer, and PubMed datasets, respectively. Red stars (#) indicate the located GLTs, which reach comparable performance with the least inference MACs. Dash lines represent the baseline performance of unpruned GNNs on full graphs. More results over graph sparsity and GNN sparsity are referred to Appendix A2.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Node classification and link prediction performance of 28-layer deep ResGCNs on large-scale graph datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Drawing graph lottery tickets from randomly initialized and self-supervised pre-trained (GraphCL (You et al., 2020b)) GCNs on node classification (low label rate: only 5.0% and 3.6% nodes in Cora and Citesser are labeled) and link prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Figure7. Ablation studies of pruning ratios for the graph and GNN sparsification by UGS, i.e., pg, p θ . Each curve records the achieved performance of 20 rounds iterative UGS. GCN on Cora is adopted here. The embedded sub-graph is a zoom-in of the red box region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure A9 .</head><label>A9</label><figDesc>Figure A9. Node classification performance over achieved GNNs sparsity of GCN, GIN, and GAT on Cora, Citeseer, and PubMed datasets, respectively. Red stars (#) indicate the located GLTs, which reach comparable performance with extreme GNN sparsity. Dash lines represent the baseline performance of unpruned GNNs on full graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure A10 .</head><label>A10</label><figDesc>Figure A10. Link prediction performance over achieved GNNs sparsity of GCN, GIN, and GAT on Cora, Citeseer, and PubMed datasets, respectively. Red stars (#) indicate the located GLTs, which reach comparable performance with the extreme graph sparsity and GNN sparsity. Dash lines represent the baseline performance of unpruned GNNs on full graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Graph datasets statistics.</figDesc><table><row><cell>Dataset</cell><cell>Task Type</cell><cell>Nodes</cell><cell>Edges</cell><cell cols="3">Ave. Degree Features Classes</cell><cell>Metric</cell></row><row><cell>Cora</cell><cell>Node Classification Link Prediction</cell><cell>2,708</cell><cell>5,429</cell><cell>3.88</cell><cell>1,433</cell><cell>7</cell><cell>Accuracy ROC-AUC</cell></row><row><cell>Citeseer</cell><cell>Node Classification Link Prediction</cell><cell>3,327</cell><cell>4,732</cell><cell>2.84</cell><cell>3,703</cell><cell>6</cell><cell>Accuracy ROC-AUC</cell></row><row><cell>PubMed</cell><cell cols="2">Node Classification 19,717 Link Prediction</cell><cell>44,338</cell><cell>4.50</cell><cell>500</cell><cell>3</cell><cell>Accuracy ROC-AUC</cell></row><row><cell>Ogbn-ArXiv</cell><cell cols="3">Node Classification 169,343 1,166,243</cell><cell>13.77</cell><cell>128</cell><cell>40</cell><cell>Accuracy</cell></row><row><cell cols="4">Ogbn-Proteins Node Classification 132,534 39,561,252</cell><cell>597.00</cell><cell>8</cell><cell>2</cell><cell>ROC-AUC</cell></row><row><cell>Ogbl-Collab</cell><cell>Link Prediction</cell><cell cols="2">235,868 1,285,465</cell><cell>10.90</cell><cell>128</cell><cell>2</cell><cell>Hits@50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>, such as Ogbn-ArXiv, Ogbn-Proteins, and Ogbl-Collab. More datasets statistics are summarized in Table1. Other details such as the datasets' trainval-test splits are included in Appendix A1.</figDesc><table><row><cell>Training and Inference Details Our evaluation metrics</cell></row><row><cell>are shown in Table 1, following Kipf &amp; Welling (2016); Hu</cell></row><row><cell>et al. (2020); Mavromatis &amp; Karypis (2020). More detailed</cell></row><row><cell>configurations such as learning rate, training iterations, and</cell></row><row><cell>hyperparameters in UGS, are referred to Appendix A1.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 2 .</head><label>2</label><figDesc>Performance comparisons of Random GLT versus GLT from GCN on Cora, at several sparsity levels.</figDesc><table><row><cell>Settings</cell><cell cols="4">(Graph Sparsity, GNN Sparsity)=(s g %, s θ %)</cell></row><row><cell></cell><cell cols="4">(18.55,59.04) (22.62,67.23) (36.98,86.58) (55.99,97.19)</cell></row><row><cell>Random GLT</cell><cell>79.70</cell><cell>78.50</cell><cell>75.70</cell><cell>63.70</cell></row><row><cell>GLT</cell><cell>80.80</cell><cell>80.30</cell><cell>79.30</cell><cell>75.30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Visualization of sub-graphs (Original/Sparsified) from Cora, Citeseer, and PubMed. Original sub-graphs in the first and third columns are randomly sampled from full graphs. The corresponding unified sparsified sub-graphs of GLTs at 48.67% sparsity, are provided in the second and forth columns.</figDesc><table><row><cell>Original</cell><cell>Sparsified</cell><cell>Original</cell><cell>Sparsified</cell></row><row><cell>Cora</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Citeseer</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PubMed</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Figure 8.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table A3 .</head><label>A3</label><figDesc>Download links of graph datasets.</figDesc><table><row><cell>Dataset</cell><cell>Download links and introduction websites</cell></row><row><cell>Cora</cell><cell>https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz</cell></row><row><cell>Citeseer</cell><cell>https://linqs-data.soe.ucsc.edu/public/lbc/citeseer.tgz</cell></row><row><cell>PubMed</cell><cell>https://linqs-data.soe.ucsc.edu/public/Pubmed-Diabetes.tgz</cell></row><row><cell>Ogbn-ArXiv</cell><cell>https://ogb.stanford.edu/docs/nodeprop/#ogbn-arxiv</cell></row><row><cell>Ogbn-Proteins</cell><cell>https://ogb.stanford.edu/docs/nodeprop/#ogbn-proteins</cell></row><row><cell>Ogbl-Collab</cell><cell>https://ogb.stanford.edu/docs/linkprop/#ogbl-collab</cell></row><row><cell cols="2">Train-val-test Splitting of Datasets. As for node classi-</cell></row><row><cell cols="2">fication of small-and medium-scale datasets, we use 140</cell></row><row><cell cols="2">(Cora), 120 (Citeseer) and 60 (PubMed) labeled data for</cell></row><row><cell cols="2">training, 500 nodes for validation and 1000 nodes for test-</cell></row><row><cell cols="2">ing. As for link prediction task of small-and medium-scale</cell></row><row><cell cols="2">datasets Cora, Citeseer and PubMed, we random sample</cell></row><row><cell cols="2">10% edges as our testing set, 5% for validation, and the</cell></row><row><cell cols="2">rest 85% edges are training set. The training/validation/test</cell></row><row><cell cols="2">splits for Ogbn-ArXiv, Ogbn-Proteins and Ogbl-Collab are</cell></row><row><cell cols="2">given by the benchmark (Hu et al., 2020). Specifically,</cell></row><row><cell cols="2">as for Ogbn-ArXiv, we train on the papers published until</cell></row><row><cell cols="2">2017, validation on those published in 2018 and test on</cell></row><row><cell cols="2">those published since 2019. As for Ogbn-Proteins, we split</cell></row><row><cell cols="2">the proteins nodes into training/validation/test sets accord-</cell></row><row><cell cols="2">ing to the species which the proteins come from. As for</cell></row><row><cell cols="2">Ogbl-Collab, we use the collaborations until 2017 as train-</cell></row><row><cell cols="2">ing edges, those in 2018 as validation edges, and those in</cell></row><row><cell cols="2">2019 as test edges.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">NetworkX ( https://networkx.org) is used for analyses.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Node Classification UGS GraphCL+UGS 0 .0 0 5 .0 0 9 .7 5 1 4 .2 6 1 8 .5 5 2 2 .6 2 2 6 .4 9 3 0 .1 7 3 3 .6 6 3 6 .9 8 4 0 .1 3 4 3 .1 2 4 5 .9 6 5 1 .2 3 5 5 .9 9 6 0 . Link Predication UGS GraphCL+UGS 0 .0 0 5 .0 0 9 .7 5 1 4 .2 6 1 8 .5 5 2 2 .6 2 2 6 .4 9 3 0 .1 7 3 3 .6 6 3 6 .9 8 4 0 .1 3 4 3 .1 2 4 5 .9 6 5 1 .2 3 5 5 .9 9 6 0 . </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Abadal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guirado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>López-Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Alarcón</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00130</idno>
		<title level="m">Computing graph neural networks: A survey from algorithms to accelerators</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Propagation-based temporal network summarization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Adhikari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Amiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bharadwaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Prakash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="729" to="742" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hardware acceleration of graph neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Auten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tomei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 57th ACM/IEEE Design Automation Conference (DAC)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
				<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Constrained optimization and lagrange multiplier methods</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Rheinboldt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Busch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Seidl</surname></persName>
		</author>
		<author>
			<persName><surname>Pushnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02228</idno>
		<title level="m">Efficient and adaptive neural message passing</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improved large-scale graph learning through ridge spectral sparsification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Calandriello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lazaric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Koutis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="688" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spectral sparsification in spectral clustering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chakeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Farhidzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 23rd international conference on pattern recognition (icpr)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2301" to="2306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><surname>Fastgcn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stochastic training of graph convolutional networks with variance reduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018b</date>
			<biblScope unit="page" from="942" to="950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The lottery tickets hypothesis for supervised and self-supervised pre-training in computer vision models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.06908</idno>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The lottery ticket hypothesis for pretrained bert networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.12223</idno>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long live the lottery: The existence of winning tickets in lifelong learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=LXMSvPmsm0g" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the equivalence between graph isomorphism testing and function approximation with gnns</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A survey of model compression and acceleration for deep neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09282</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
				<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Benchmarking graph neural networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Provable and practical approximations for the degree distribution using sublinear graph samples</title>
		<author>
			<persName><forename type="first">T</forename><surname>Eden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Seshadhri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference</title>
				<meeting>the 2018 World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="449" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">U</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<idno>arXiv, abs/1906.10732</idno>
		<title level="m">The difficulty of training sparse neural networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<idno>arXiv, abs/1912.05671</idno>
		<title level="m">Linear mode connectivity and the lottery ticket hypothesis</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A set of measures of centrality based on betweenness</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sociometry</title>
		<imprint>
			<biblScope unit="page" from="35" to="41" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hooker</surname></persName>
		</author>
		<idno>arXiv, abs/1902.09574</idno>
		<title level="m">The state of sparsity in deep neural networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Awb-gcn: A graph convolutional network accelerator with runtime workload rebalancing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tumeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="922" to="936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Metropolis algorithms for representative subgraph sampling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hübler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 Eighth IEEE International Conference on Data Mining</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="283" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Winning lottery tickets in deep generative models</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Kalibhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feizi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Explainable deep relational networks for predicting compound-protein affinities and contacts</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12553</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Kiningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Levis</surname></persName>
		</author>
		<author>
			<persName><surname>Grip</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13828</idno>
		<title level="m">A graph neural network acceleratorarchitecture</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sampling from large graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="631" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deepergcn: All you need to train deeper gcns</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sgcn: A graph sparsifier based on graph convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fardad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zafarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020b</date>
			<biblScope unit="page" from="275" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rethinking the value of network pruning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A method of matrix analysis of group structure</title>
		<author>
			<persName><forename type="first">R</forename><surname>Luce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Perry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="95" to="116" />
			<date type="published" when="1949">1949</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Graph infoclust: Leveraging cluster-level node information for unsupervised graph representation learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mavromatis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06946</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Relational pooling for graph representations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4663" to="4673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">The betweenness centrality of biological networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<pubPlace>Virginia Tech</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cusparse library</title>
		<author>
			<persName><forename type="first">M</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kapasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GPU Technology Conference</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><surname>Gmnn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06214</idno>
		<title level="m">Graph markov neural networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Comparing rewinding and fine-tuning in neural network pruning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Renda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Winning the lottery with continuous sparsification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="pre" to="proceedings" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dynamic edgeconditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Degree-quant: Quantization-aware training for graph neural networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Tailor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fernandez-Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=NSBrFgJAHg" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Graphmix: Regularized training of graph neural networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11715</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Rank degree: An efficient algorithm for graph sampling</title>
		<author>
			<persName><forename type="first">E</forename><surname>Voudigari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Salamanos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Yannakoudakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="120" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Intel math kernel library</title>
		<author>
			<persName><forename type="first">E</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High-Performance Computing on the Intel® Xeon Phi™</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="167" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Gnn-pim: A processing-in-memory architecture for graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Advanced Computer Architecture</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="73" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks?</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryGs6iA5Km" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Hygcn: A gcn accelerator with hybrid architecture</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="15" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Drawing early-bird tickets: Toward more efficient training of deep networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">When does self-supervision help graph convolutional networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v119/you20a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Iii</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</editor>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07">Jul 2020c</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">L2-gcn: Layerwise and learned efficient training of graph convolutional networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020d</date>
			<biblScope unit="page" from="2127" to="2135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Playing the lottery with rewards and multiple languages: lottery tickets in rl and nlp</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Graph motif based sparsification for graph clustering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Gsparsify</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</title>
				<meeting>the 24th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="373" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Robust graph representation learning via neural sparsification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11458" to="11468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph neural networks: A review of methods and applications</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
