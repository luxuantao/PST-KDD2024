<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ASK ME ANYTHING: A SIMPLE STRATEGY FOR PROMPTING LANGUAGE MODELS</title>
				<funder>
					<orgName type="full">Analog Devices</orgName>
				</funder>
				<funder>
					<orgName type="full">Stanford Center for Research on Foundation Models</orgName>
				</funder>
				<funder>
					<orgName type="full">Okawa Foundation, American Family Insurance</orgName>
				</funder>
				<funder>
					<orgName type="full">Moore Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">Department of Defense (DoD)</orgName>
				</funder>
				<funder ref="#_akqBb43">
					<orgName type="full">ONR</orgName>
				</funder>
				<funder ref="#_Jmatv6X">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder>
					<orgName type="full">NXP</orgName>
				</funder>
				<funder>
					<orgName type="full">Stanford HAI</orgName>
				</funder>
				<funder>
					<orgName type="full">Qualcomm</orgName>
				</funder>
				<funder>
					<orgName type="full">Ericsson</orgName>
				</funder>
				<funder>
					<orgName type="full">Xilinx</orgName>
				</funder>
				<funder>
					<orgName type="full">National Science Foundation Graduate Research Fellowship Program</orgName>
				</funder>
				<funder>
					<orgName type="full">Intelligence Community Postdoctoral Fellowship</orgName>
				</funder>
				<funder>
					<orgName type="full">Accenture</orgName>
				</funder>
				<funder>
					<orgName type="full">Fannie and John Hertz Foundation</orgName>
				</funder>
				<funder ref="#_kW2bHQe">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
				<funder>
					<orgName type="full">National Defense Science and Engineering Graduate Fellowship</orgName>
					<orgName type="abbreviated">NDSEG</orgName>
				</funder>
				<funder ref="#_yFzH7XU">
					<orgName type="full">Texas Instruments</orgName>
				</funder>
				<funder ref="#_dAn3tnK">
					<orgName type="full">DARPA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-05">5 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Simran</forename><surname>Arora</surname></persName>
							<email>simran@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Avanika</forename><surname>Narayan</surname></persName>
							<email>avanika@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mayee</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Laurel</forename><surname>Orr</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Neel</forename><surname>Guha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kush</forename><surname>Bhatia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ines</forename><surname>Chami</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Numbers Station</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Frederic</forename><surname>Sala</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>R?</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ASK ME ANYTHING: A SIMPLE STRATEGY FOR PROMPTING LANGUAGE MODELS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-05">5 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2210.02441v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt that demonstrates how to perform the task and no additional training. Prompting is a brittle process wherein small modifications to the prompt can cause large variations in the model predictions, and therefore significant effort is dedicated towards designing a painstakingly perfect prompt for a task. To mitigate the high degree of effort involved in prompting, we instead ask whether collecting multiple effective, yet imperfect, prompts and aggregating them can lead to a high quality prompting strategy. Our observations motivate our proposed prompting method, ASK ME ANYTHING PROMPTING (AMA). We first develop an understanding of the effective prompt formats, finding question-answering (QA) prompts, which encourage open-ended generation ("Who went to the park?") tend to outperform those that restrict the model outputs ("John went to the park. Output True or False"). Our approach recursively uses the LLM to transform task inputs to the effective QA format. We apply these prompts to collect several noisy votes for the input's true label. We find that these prompts can have very different accuracies and complex dependencies and thus propose to use weak supervision, a procedure for combining the noisy predictions, to produce the final predictions. We evaluate AMA across open-source model families (Neo, BLOOM, OPT, and T0) and sizes (125M-175B parameters), demonstrating an average performance lift of 10.2% over the few-shot baseline. This simple strategy enables the open-source GPT-Neo-6B model to match and exceed the performance of few-shot GPT3-175B on 15 of 20 popular benchmarks. Averaged across these tasks, the GPT-Neo-6B model outperforms few-shot GPT3-175B. We release our code for reproducing the results here: https://github.com/HazyResearch/ama_prompting.</p><p>Recent work has evaluated LLM prompting performance on a broad set of tasks and finds the process to be brittlesmall changes to the prompt result in large performance variations <ref type="bibr" target="#b2">[Zhao et al., 2021</ref><ref type="bibr" target="#b2">, Holtzman et al., 2021]</ref>. The performance further varies depending on the chosen LLM family <ref type="bibr" target="#b3">[Ouyang et al., 2022</ref>, Sanh et al., 2022, inter alia.]   and model size [Wei et al., 2022a<ref type="bibr" target="#b6">, Lampinen et al., 2022]</ref>. To improve reliability, significant effort is dedicated towards designing a painstakingly perfect prompt. For instance, <ref type="bibr" target="#b7">Mishra et al. [2021]</ref> and <ref type="bibr" target="#b8">Wu et al. [2022]</ref> recommend that users manually explore large search-spaces of strategies to tune their prompts on a task-by-task basis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large language models (LLMs) are bringing us closer to the goal of task-agnostic machine learning <ref type="bibr" target="#b0">[Brown et al., 2020</ref><ref type="bibr" target="#b1">, Bommasani et al., 2021]</ref>. Rather than training models for new tasks, LLMs are being applied to new tasks out-of-the box. In this paradigm, termed in-context learning, LLMs are instead controlled via natural language task specifications, or prompts. A prompt is defined by a template, which contains placeholders for the description and demonstrations of the inputs and outputs for the task.</p><p>Figure <ref type="figure">1</ref>: AMA first recursively uses the LLM to reformat tasks and prompts to effective formats, and second aggregates the predictions across prompts using weak-supervision. The reformatting is performed using prompt-chains, which consist of functional (fixed, reusable) prompts that operate over the varied task inputs. Here, given the input example, the prompt-chain includes a question()-prompt through which the LLM converts the input claim to a question, and an answer() prompt, through which the LLM answers the question it generated. Different prompt-chains (i.e., differing in the in-context question and answer demonstrations) lead to different predictions for the input's true label.</p><p>This work instead considers aggregating the predictions of multiple effective, yet imperfect, prompts to improve prompting performance over a broad set of models and tasks. Given a task input, each prompt produces a vote for the input's true label, and these votes are aggregated to produce a final prediction. In pursuit of high quality prompting via aggregation, we face the following challenges:</p><p>1. Effective prompts: High quality prompts are a precursor to improvements from aggregation. We take the original prompts which yield near-random performance in <ref type="bibr" target="#b0">Brown et al. [2020]</ref> for two SuperGLUE tasks (CB, RTE). Generating multiple prompts in the same format and taking majority vote prediction across prompts has a minor effect (+4% for CB) and can even hurt performance versus the average prompt performance (-2% for RTE). Many proposals for improved prompts focus on a single task type and evaluate on a single model-family and/or size <ref type="bibr">[Wei et al., 2022a</ref><ref type="bibr" target="#b9">, Jung et al., 2022]</ref>. We need a structure for prompting that works across tasks and models.</p><p>2. Scalable collection: After identifying effective prompt formats, we need to obtain multiple prompts in these formats -these prompts will be used to collect votes for an input's true label. The original format of a task varies widely and prior works manually rewrite input examples to new formats in a task-specific manner <ref type="bibr" target="#b7">[Mishra et al., 2021</ref><ref type="bibr">, Wu et al., 2022]</ref>, which is challenging to scale. We need a scalable strategy for reformatting task inputs.</p><p>3. Prompt aggregation: Using the prompts above (for CB and RTE), we see 9.5% average variation in accuracy and that the Jaccard index over errors is 69% higher than if prompt errors were i.i.d. Majority vote (MV) is the primary unsupervised aggregation strategy in prior prompting work <ref type="bibr">[Jiang et al., 2020, Schick and</ref><ref type="bibr" target="#b11">Sch?tze, 2021]</ref>, but it does not account for either property, making it unreliable. We need a strategy that accounts for the varying accuracies and dependencies.</p><p>In this work, we propose ASK ME ANYTHING PROMPTING (AMA), a simple approach that surprisingly enables open-source LLMs with 30x fewer parameters to exceed the few-shot performance of GPT3-175B. In AMA:</p><p>1. We identify properties of prompts that improve effectiveness across tasks, model types, and model sizes. We study standard prompt-formats categorized by prior work <ref type="bibr" target="#b0">[Brown et al., 2020]</ref> and find prompts that encourage open-ended answers ("Where did John go?") to be more effective than prompts that restrict the model output to particular tokens (e.g. "John went to the park. Output True or False?"). For instance, converting three SuperGLUE tasks (CB, RTE, WSC) from the original restrictive formats in <ref type="bibr" target="#b0">[Brown et al., 2020]</ref> to open-ended formats provides a 72% performance improvement (Section 3.2). Given a task input, we find that a simple structure of (1) forming questions based on the input and (2) prompting the LLM to answer the questions applies quite generally and improves performance across diverse benchmark tasks.</p><p>2. We propose a strategy for scalably reformatting task inputs to the effective formats found in (1). We propose to transform task inputs to the effective open-ended question-answering format by recursively using the LLM itself in a fixed two step pipeline. We first use question()-prompts, which contain task-agnostic examples of how to transform statements to various (e.g., yes-no, cloze) questions and second use answer()-prompts that demonstrate ways of answering questions (e.g., concise or lengthy answers). Applying prompt-chains-answer(question(x))-gives a final prediction for the input x.<ref type="foot" target="#foot_0">2</ref> Chains are (1) reused across inputs and (2) different pairs of functional prompts can be combined to create variety. We apply the varying functional prompt-chains to an input to collect multiple votes for the input's true label. 3. We propose the use of weak supervision (WS) to reliably aggregate predictions. We find that the errors produced by the predictions of different chains can be highly varying and correlated. While majority vote (MV) may do well on certain sets of prompts, it performs poorly in the above cases. AMA accounts for these cases by identifying dependencies among prompts and using WS, a procedure for modeling and combining noisy predictions without any labeled data <ref type="bibr" target="#b12">[Ratner et al., 2017</ref><ref type="bibr" target="#b13">, Varma et al., 2019]</ref>. We apply WS to prompting broadly for the first time in this work, showing it improves the reliability of prompting with off-the-shelf LLMs and no further training. We find that AMA can achieve up to 8.7 points of lift over MV and that on 9 tasks, it recovers dependencies among prompts to boost performance by up to 9.6 points.</p><p>We apply our proposed prompt-aggregation strategy, AMA, to 20 popular language benchmarks and 14 open-source LLMs from 4 model families <ref type="bibr">(Neo Black et al. [2021]</ref>, BLOOM, <ref type="bibr">OPT Zhang et al. [2022]</ref>, and T0 <ref type="bibr" target="#b4">Sanh et al. [2022]</ref>) spanning 3 orders-of-magnitude (125M-175B parameters). Our proof-of-concept provides an improvement over the few-shot (k = 3) baseline by an average of 10.2% ? 6.1% absolute (21.4% ? 11.2% relative) lift across models. We find the largest gains are on tasks where the knowledge required to complete the task is found in the provided context and comparatively less on closed-book tasks (e.g., factual recall). Most excitingly, ASK ME ANYTHING PROMPTING enables an open-source LLM, which is furthermore 30x parameters smaller, to match or exceed the challenging GPT3-175B few-shot baseline results in <ref type="bibr" target="#b0">Brown et al. [2020]</ref> on 15 of 20 benchmarks. We hope AMA and future work help address painpoints in LLM applications <ref type="bibr">[Narayan et al., 2022, Arora and</ref><ref type="bibr">R?, 2022]</ref> by improving the ability to proceed with less-than-perfect prompts and encouraging the use of small, private, and open-source LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Several existing works study how to improve the zero-to-few-shot task-transfer abilities of LLMs.</p><p>Training based strategies Prior works have improved prompting performance by training larger models over more or curated data, and for longer <ref type="bibr" target="#b18">[Kaplan et al., 2020</ref><ref type="bibr">, Chowdhery et al., 2022]</ref> -or by explicitly fine-tuning LMs over prompts <ref type="bibr">[Wang et al., 2022a</ref><ref type="bibr">, Wei et al., 2022b</ref><ref type="bibr">, Sanh et al., 2022</ref><ref type="bibr" target="#b3">, Ouyang et al., 2022]</ref>. We complementarily aim to improve the prompting performance of off-the-shelf language models with no additional fine-tuning.</p><p>Prompt-engineering Prompt-engineering is the process of designing natural language specifications of a task, which are used to condition the LLM at inference time. Prior work finds that the prompt format changes the model behavior and proposes particular formats. Some formats are designed-for or evaluated-on a narrow task type, model type, or model size <ref type="bibr">[Wei et al., 2022a</ref><ref type="bibr" target="#b9">, Jung et al., 2022]</ref>. Others require users to manually rewrite task-inputs to the prescribed formats on a example-by-example basis in a task-specific manner <ref type="bibr" target="#b7">[Mishra et al., 2021</ref><ref type="bibr" target="#b22">, Patel et al., 2022</ref><ref type="bibr">, Wu et al., 2022]</ref>. Our recursive use of the LLM is similar to <ref type="bibr" target="#b9">Jung et al. [2022]</ref>, which focuses on commonsense reasoning. We draw inspiration from and share similar ideas with these lines of work.</p><p>Complementary work investigates how to simplify complex tasks (e.g., multi-hop reasoning), to achieve better performance in the prompting paradigm. <ref type="bibr" target="#b23">Creswell et al. [2022]</ref>, <ref type="bibr" target="#b8">Wu et al. [2022]</ref> explicitly decompose the complex tasks into steps, which are each handled in a separate inference-pass. However, these methods draw a distinction between the complex tasks which can be naturally decomposed into multiple steps and "single-step" language tasks. These prior works do not support the single-step tasks such as classification, NLU, QA, which are the focus of our work.</p><p>Prompt sensitivity Prior works note the sensitivity of prompting under slight modifications and propose strategies to improve the performance of single prompts <ref type="bibr" target="#b2">[Zhao et al., 2021</ref><ref type="bibr" target="#b24">, Liu et al., 2021]</ref>. Complementing this, we aggregate the results of multiple prompts. A few prior works ensemble hand-curated prompt-patterns in a task-specific manner and for particular model types <ref type="bibr" target="#b10">[Jiang et al., 2020</ref><ref type="bibr" target="#b11">, Schick and Sch?tze, 2021</ref><ref type="bibr">, Wang et al., 2022b]</ref>, though these require training. AMA uses a label-free aggregation strategy and even weaker sources of supervision, as we use task-agnostic prompt templates.</p><p>Weak supervision (WS) WS is a powerful framework that learns the accuracies and correlations of multiple noisy sources and aggregates them to produce weak labels for training data <ref type="bibr" target="#b12">[Ratner et al., 2017</ref><ref type="bibr">, 2016</ref><ref type="bibr">, 2018</ref><ref type="bibr" target="#b13">, Varma et al., 2019</ref><ref type="bibr" target="#b28">, Fu et al., 2020]</ref>. WS has been applied to prompting in the context of distilling a LLM by aggregating the outputs of hand-curated prompts into a labeled dataset and training a smaller model on it <ref type="bibr" target="#b29">[Smith et al., 2022]</ref>. In contrast, we aim to use aggregation to improve out-of-the-box LLM performance reliably.  <ref type="figure">2</ref>: Relative lift with model scale using results and prompt-styles reported in <ref type="bibr" target="#b0">Brown et al. [2020]</ref> (Left). Ablating the promptstyle using the GPT-Neo-6B model. We include calibration results <ref type="bibr" target="#b2">Zhao et al. [2021]</ref> and the "-" indicates the method cannot be applied to the task (Right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ASK ME ANYTHING PROMPTING</head><p>We propose ASK ME ANYTHING PROMPTING (AMA), a prompting approach that uses multiple imperfect promptsrather than one painstakingly crafted perfect prompt-and reliably aggregates their outputs. We describe and motivate AMA's prompt format (Section 3.2), how AMA scalably produces collections of prompts (Section 3.3), and AMA's aggregation method (Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>We consider supervised tasks, (X , Y), where x ? X is the example and y ? Y is the output. We have an unlabeled dataset D = {x i } n i=1 for which we wish to predict each y i . We apply LLMs to this task by using a prompt-a natural language prefix that demonstrates how to complete a task. A prompt consists of a prompt template, with placeholders for (1) zero or more in-context task demonstrations and (2) for the inference example x as shown in Figure <ref type="figure" target="#fig_0">3</ref>. Given a prompt p, we use p : X ? Y to refer the output of the prompted LLM which produces a prediction ? = p(x). Specifically, the LLM runs inference on p with x substituted for the placeholder in the template.</p><p>We denote a collection of m prompts as P = [p 1 , p 2 , ..., p m ]. Given input D, we (1) apply a collection P to each x ? D and (2) aggregate their predictions, denoted as P(x) = [p 1 (x), . . . , p m (x)], using an aggregator function ? : Y m ? Y to produce outputs ? on each x. We can thus express the procedure via two key components we aim to understand, the prompts P and aggregator ?.</p><p>Running examples For the motivating observations in the rest of this section, we use three SuperGLUE <ref type="bibr" target="#b30">[Wang et al., 2019]</ref> tasks-CommitmentBank (CB), Recognizing Textual Entailement (RTE), and Winograd Schema Challenge (WSC)-and the DBPedia and AGNews classification tasks <ref type="bibr" target="#b31">[Zhang et al., 2015]</ref>. We evaluate over the GPT-Neo-6B model. CB and RTE require determining the vailidity of a statement is given some context (as in Figure <ref type="figure">1</ref>), WSC requires outputting the subject corresponding to a given pronoun, and DBPedia and AGNews contain 14 and 4 classes respectively. We use as a running example: determine if the statement "John went to the park" is valid, given the context "John invited Mark to watch Jurassic Park with his family at the theater".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simple baseline</head><p>To provide some intuition on the challenges around effectively designing the two levers, P and aggregator ?, we start with a na?ve baseline with off-the-shelf prompts and the unsupervised majority vote prompt aggregation strategy used in prior work <ref type="bibr">[Jiang et al., 2020, Schick and</ref><ref type="bibr" target="#b11">Sch?tze, 2021]</ref>. We take the prompts proposed in <ref type="bibr" target="#b0">[Brown et al., 2020]</ref> for GPT-3 and produce P with five prompts for each task by using different sets of in-context examples. Comparing majority vote (MV), the unsupervised aggregation strategy used in prior work, to the average performance of the prompts, MV gives 39.3% (+2.2%) for CB and 54.5% (-2%) for RTE. The delta from aggregating is minor and in the worst case, harmful. Ideally, we would expect that aggregation should lead to improvement by reducing noise, but we find that performance here is only comparable to the single prompt baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Effective Prompt Formats</head><p>First, we explore what makes an effective prompt format towards improving the quality of P(x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Standard prompt formats</head><p>We ground our analysis in three standard categories of prompts used in prior work including <ref type="bibr" target="#b0">Brown et al. [2020]</ref>, <ref type="bibr">Sanh et al. [2022, inter alia.]</ref>: (1) questions that restrict the model output particular tokens ("John invited Mark to come watch Jurassic Park. Output True or False?"); (2) cloze-questions which ask the model to fill in the remaining text ("John invited Mark to come watch Jurassic _" and using the LLM to fill-the-blank, "Park"); and (3) traditional (yes-no, Wh) free-form questions ("Where did John invite Mark?"). We compare these three prompting formats and make the following observations: 1. Open-ended prompts appear to outperform restrictive-prompts. We first group the results in <ref type="bibr" target="#b0">Brown et al. [2020]</ref> based on the format used for the task, along the above categorizations (see Figure <ref type="figure">2</ref>). When scaling from GPT3-6.7B to GPT3-175B, we find that the relative gain is far lower on open-ended (cloze and traditional QA) formats vs. restricted formats.</p><p>Next, CB, RTE, and WSC are originally formatted with restrictive-prompts in <ref type="bibr" target="#b0">Brown et al. [2020]</ref>, and we form copies of the tasks in the open-ended question (cloze and free-form QA) formats. This improves the performance of the small model on average from 41.7% to 71.5% (+72%) . Intuitively, the task of answering open-ended questions is aligned with the next-token prediction language modeling objective. We observe that more precise questions give larger lifts. For WSC the restrictive prompt form is: "The pronoun 'his' refers to "Mark" in the context. True or False?", given the context "Mark went to the park with his dog.". Reformatting to "What does 'his' refer to?" and evaluating whether the answer is "Mark" provides 38% lift (69.2% accuracy). Yet, further extracting the portion of the context that mentions the pronoun ("his dog"), reformatting ("Whose dog?") and prompting with precise questions gives 49.4% lift (74.7%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>The use of open-ended questions over restrictive-prompts can increase the difficulty of mapping open-ended answers to valid output classes. For tasks with output spaces that are likely observed during pretraining (yes-no questions, sentiment classification), we see that the LLM naturally generates valid ? ? Y. For tasks with specialized output classes (i.e. multi-class classification), we need to map the answer to the open-ended question (e.g., "What is the document about?") to a valid output class. For example, given 'Personality and Mental Health ... is a quarterly peer-reviewed academic journal published by ...", we observe that the LLM typically outputs semantically correct summaries of the document topic, e.g. "journal". We find that inserting a step for the LLM to map the open-ended output "journal" to a valid category via the prompt "A 'journal' maps to category: written work" enables a 33.3% and 11.1% lift over the few-shot baseline on DBPedia (14-way classification) and AGNews (4-way) respectively.</p><p>AMA's prompt format Motivated by the above two observations, we proceed in AMA with a two-step prompting pipeline: (1) generating questions based on the input and (2) prompting the LLM to answer the generated questions. These prompts are effective, and to further improve performance we next turn to generating and aggregating over multiple prompt-outputs for each input. For intuition, different questions (with our running example: "Who went to the park?", "Did John go the park?", "Where did John go?") emphasize different aspects of the input and can provide complementary information towards reasoning about the answer. Manually generating multiple prompts per input is challenging, and so we study how to do this at scale in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Creating Prompt Collections at Scale</head><p>Our goal is to produce a collection of prompts, P, that can be applied to tasks at scale. To produce prompts in the effective open-ended question-answering format, our insight is to recursively apply the LLM itself using a chain of functional prompts, referred to as a prompt()-chain. We describe these prompts as functional because they apply a task-agnostic operation to all inputs in the tasks, without any example-level customization. We describe the two functional prompts used in AMA below. We use Figure <ref type="figure">1</ref> as a running example to explain each type. (a) question(): x ? q generates a question q (such as "Did John go to the park?") from an input x ("John went to the park."). question() prompts simply contain demonstrations of how a statement can be transformed to an open-ended question. (b) answer(): q ? a applies the question generated by (a) to the context of x to produce intermediate answers a (such as "No" or "theater"). The answer() prompts contain demonstrations of how to answer a question (optionally) given some input context.</p><p>To create P for aggregation, AMA constructs different prompt()-chains where each unique prompt()-chain is a different view of the task and can emphasize different aspects of x. Inspired by <ref type="bibr" target="#b4">Sanh et al. [2022]</ref> and <ref type="bibr" target="#b24">Liu et al. [2021]</ref>, we also vary chains through two key levers-the in-context demonstrations and the style of prompt questions-as shown in Figure <ref type="figure" target="#fig_0">3</ref>. To vary the style of open-ended prompt questions, we construct question() and answer() prompts that produce and answer either Yes/No, Wh, multiple-choice, or cloze-questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Prompt Aggregation</head><p>To aggregate the prompt predictions P(x) into outputs ? reliably, we apply tools from weak supervision, a powerful approach for learning high-quality models from weaker sources of signal without labeled data <ref type="bibr" target="#b12">[Ratner et al., 2017]</ref>. We first describe properties of P(x) that illustrate when the simple baseline of majority vote may perform poorly. We then describe our aggregator ? WS , which explicitly identifies and then accounts for these properties. We observe the following properties on P:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline observations</head><p>1. Varied overall accuracies: While prompts in P E may seem more similar than those in P T , the gap between the best and worst p i ? P is large in both cases -12.1% for P E and 9.6% for P T . 2. Varied class-conditional accuracies <ref type="bibr" target="#b2">[Zhao et al., 2021]</ref>: Beyond overall prompt accuracy, the average variance of class-conditional prompt accuracies is 9.7% across the tasks and baselines. 3. Highly-correlated outputs: Prompt predictions have dependencies among each other. The Jaccard index over error sets averaged across tasks is 42.2 for P E and 39.9 for P T . For reference, two prompts that produce i.i.d. errors and have 60% accuracy each would have a score of 0.25.</p><p>The three observations present challenges in aggregating predictions via simple approaches like MV. MV tends to do better than using one prompt, but it weights all prompts equally and treats them independently. Such an aggregation method may be sufficient over certain collections of prompts but is not reliable across general P that may exhibit the three properties we have observed.</p><p>AMA's aggregation strategy Given the varying accuracies and dependencies among prompt()-chains, in AMA we draw on recent work in weak supervision <ref type="bibr" target="#b12">[Ratner et al., 2017]</ref>, which is designed to account for these accuracy and dependency properties without relying on labels. We learn a probabilistic graphical model on Pr G,? (y, P(x)) using D and define the aggregator as ? WS (x) = arg max y?Y Pr G,? (y| P(x)), where G is a dependency graph among y and all of the prompts, and ? are the accuracy and correlation parameters of the distribution. Since we lack labeled data y, we cannot estimate G or ? directly from D. We apply work from WS-in particular, structure learning <ref type="bibr" target="#b13">[Varma et al., 2019]</ref> and latent variable estimation <ref type="bibr" target="#b27">[Ratner et al., 2018]</ref>-to learn ? and ?, which are then used to construct ? WS and predictions ?AMA = ? WS (P(x)) on D (see Appendix C for explicit algorithm statements).</p><p>Excitingly, our aggregator is general enough to identify and model a broad range of complex dependencies and varied accuracies that may appear among sets of prompts, without any training. WS hence improves the reliability of aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Information Flow in AMA</head><p>Before evaluating end-to-end quality, we look at a simple information theoretic metric to understand the contributions of the individual components -P and ? -in the prompting procedure.</p><p>Information flow metric Specifically, we examine the conditional entropy, H(y|?), which measures the amount of uncertainty remaining in the true label y given a prediction ?. Intuitively, H(y|?) will be low when ? encodes information relevant to y. In our setting, ? = ?(P(x)) is dependent on the two components of the prompting procedure, the prompts P and aggregator ?. The following simple decomposition of H(y|?) enables studying the contribution of each component:  the information in P(x) to predict ?. An aggregator ? that more accurately matches the true Pr(y| P(x)) reduces the information loss in the compression step.</p><formula xml:id="formula_0">H(y|?) = H(y| P(x))</formula><p>Evaluation We use (1) to evaluate our proposed solution AMA both empirically and theoretically. First considering H(y| P(x)), in Figure <ref type="figure" target="#fig_1">4</ref> (Left) we observe AMA outperforms k-shot baselines with expected scaling in terms of both individual prompt()-chain quality (as shown by AMA No Agg) and their quantity.</p><p>Next we consider the gap term H(y|?) -H(y| P(x)). It enables us to understand why MV is insufficient: it compresses information from P(x) according to a specific construction of Pr(y, P(x)), for which p i (x) ? p j (x)|y for all i, j ? [m], and Pr(p i (x) = c|y = c) for c ? Y is a single better-than-random constant across i and c. When the true distribution is vastly different-as is common-this misspecification results in a large gap between the optimal H(y| P(x)) and H(y|? MV ) in Figure <ref type="figure" target="#fig_1">4</ref> (Right). Weak supervision can improve ? over the standard MV baseline to reduce the information loss H(y|? AMA ) -H(y| P(x)).</p><p>In addition to empirical measurements, we can provide a theoretical characterization for the information flow. In Appendix D, we express H(y|? AMA ) in terms of the individual prompt accuracies under the standard weak supervision model (i.e., Ising model on y and P(x) <ref type="bibr" target="#b27">[Ratner et al., 2018]</ref>).</p><p>There has been recent interest in how LLMs improve primarily along the three axes of parameter scale, training data, and compute <ref type="bibr" target="#b18">[Kaplan et al., 2020</ref><ref type="bibr" target="#b32">, Hoffmann et al., 2022</ref><ref type="bibr">, Wei et al., 2022c]</ref>. In Figure <ref type="figure" target="#fig_1">4</ref>, as we increase the number of prompts to be aggregated, the conditional entropy reduces. Prompt aggregation may be another useful axis for understanding LLM scaling performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We evaluate ASK ME ANYTHING PROMPTING on 20 popular language benchmarks used in <ref type="bibr" target="#b0">Brown et al. [2020]</ref>, <ref type="bibr" target="#b4">Sanh et al. [2022]</ref>. We report results across 14 unique LLMs including 4 model families (Neo <ref type="bibr" target="#b14">[Black et al., 2021]</ref>,</p><p>OPT <ref type="bibr" target="#b15">[Zhang et al., 2022]</ref>, BLOOM, and T0 <ref type="bibr" target="#b4">[Sanh et al., 2022]</ref>) spanning 3 orders-of-magnitude in size (125M-175B).</p><p>We aim to validate whether AMA provides consistent lift across diverse tasks (Section 5.1), works across model families (Section 5.2), and reliably aggregates the predictions across prompts (Section 5.3). Table <ref type="table">1</ref>: AMA results for the GPT-Neo-6B parameter model <ref type="bibr" target="#b14">[Black et al., 2021]</ref> compared to the few-shot GPT3-175B. The GPT-175B numbers are as reported in <ref type="bibr" target="#b0">Brown et al. [2020]</ref>, <ref type="bibr" target="#b2">Zhao et al. [2021]</ref>, where the numbers of in-context examples is in parentheses. Note that prompts can abstain from predicting, which can lead to lower average numbers for QA, including on COPA and StoryCloze. For the question-answering tasks and ReCoRD, we report the majority vote aggregation, as using WS is complex with the open-ended output space. The same results for the BLOOM 7.1B parameter model are in Appendix 3.</p><p>Experimental details We use a diverse set of tasks: SuperGLUE <ref type="bibr" target="#b30">[Wang et al., 2019]</ref>, <ref type="bibr">NLI [Mostafazadeh et al., 2017</ref><ref type="bibr" target="#b34">, Nie et al., 2020]</ref>, classification <ref type="bibr" target="#b31">[Zhang et al., 2015</ref><ref type="bibr" target="#b35">, Socher et al., 2013</ref><ref type="bibr" target="#b36">, He and McAuley, 2016]</ref>, and QA tasks <ref type="bibr" target="#b37">[Kasai et al., 2022</ref><ref type="bibr">, Kwiatkowski et al., 2019</ref><ref type="bibr" target="#b39">, Berant et al., 2013</ref><ref type="bibr" target="#b40">, Dua et al., 2019]</ref>. For all tasks, we compare to published results of the OpenAI few-shot-prompted GPT3-175B parameter model using the numbers reported in <ref type="bibr" target="#b0">Brown et al. [2020]</ref> and, for classification tasks, <ref type="bibr" target="#b2">Zhao et al. [2021]</ref>. <ref type="bibr" target="#b0">Brown et al. [2020]</ref> uses k ? [32.</p><p>.70] depending on the task and <ref type="bibr" target="#b2">Zhao et al. [2021]</ref> uses k ? [1..8], providing a challenging baseline for comparison.</p><p>For AMA we use 3 -6 prompt()-chains to generate predictions per input. We model the correlations between prompt-predictions per task, without using any labeled training data, to obtain the final prediction per example via weak supervision (WS). We report both the average performance over the prompt()-chains (QA) and with AMA's WS aggregation (QA + WS). We report QA + WS across 5 random seeds for the model. Model details and prompt()chains are in the Appendix.<ref type="foot" target="#foot_1">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Main Results</head><p>We report benchmark results in Table <ref type="table">1</ref> comparing the open-source GPT-Neo-6B and few-shot (k ? [32..70]) GPT3-175B. We find that the open-source 6B parameter model exceeds the average few-shot performance of the GPT3-175B model on 15 of 20 benchmarks. Over the 20 tasks, AMA gives an average improvement of 41% over the 6B parameter model's few-shot (k = 3) performance to achieve this.</p><p>We find that AMA provides the most lift on tasks where all requisite knowledge is included in the task input (e.g., reading comprehension) and that largely rely on model's natural language understanding (NLU) abilities. The lift is lower on tasks that rely on the LLMs memorized knowledge (e.g. commonsense, closed-book). AMA can help close the gap on knowledge-intensive tasks. The closed-book WebQ task includes simple questions, where the answers are likely seen during pretraining. We find that using an open-ended prompt that asks the LM to generate relevant context, and then prompting the model to answer the original question using the generated context is effective. However, there are limitations as seen on NQ. Figure <ref type="figure">5</ref>: Evaluation across model sizes for diagnostics and benchmarks. We report the absolute lift from AMA over few-shot (k = 3) performance, averaged over 7 tasks with 95% confidence intervals (Left). Diagnostic plots are ordered by the amount of lift models of the size-category see on 7 the benchmarks (Right).</p><p>We similarly see limitations when tasks cannot rely on the latent knowledge. We observe a small performance gap between model sizes on RealTimeQA, which includes questions that have temporally changing answers that are less likely to be memorized. Similarly, for tasks requiring domain knowledge, e.g. the "Amazon Instant Video" class in the Amazon task, all model-sizes achieve near-0 performance. In such cases, information retrieval may help close the gap.</p><p>The flexible LLM interface permits asking and answering questions over diverse knowledge sources such as databases or a search engine <ref type="bibr" target="#b41">[Nakano et al., 2021]</ref>. We provide an extended error analysis Table <ref type="table">1</ref> results in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation across Models</head><p>Benchmark results We evaluate the lift from AMA over out-of-the-box few-shot performance across different sizes of four open-source LMs (Neo, OPT, BLOOM, and T0) across 7 tasks (4 NLU, 2 NLI, 1 classification). In this analysis, we want to understand the effectiveness of AMA's prompt()-chains reformattings across models and report the average prompt performance over the 3-6 prompt()-chains used per task. Neo, OPT, and BLOOM are GPT models, while T0 is obtained by explicitly fine-tuning a T5 LM <ref type="bibr" target="#b42">[Raffel et al., 2019]</ref> on prompt-input-output tuples.</p><p>Excitingly, the AMA prompt()-chains apply quite generally. We see a 10.2% ? 6.1% absolute (21.4% ? 11.2% relative) lift on average across models and tasks (see Figure <ref type="figure">5a (a)</ref>). We observe the absolute lift increases with model size and levels out, however we note that there are few-models per size grouping. The average absolute (relative) lift by model family (across tasks and sizes) is 11.0% (24.4%) for Neo, 11.0% (23.4%) for BLOOM, and 11.9% (22.7%) for OPT, and 2.9% (8.3%) for T0. We hypothesize the lower lift on T0 arises because the model was fine-tuned on zero-shot prompts, which may compromise its in-context learning abilities.</p><p>Diagnostics for understanding AMA lift To further understand why models see different degrees lift, we create a set of diagnostic tasks that correspond to the steps in prompt()-chains. The diagnostics measure four basic operations -question generation, answer generation, answer selection, and extraction. For each operation, we create 1-3 tasks with 50 manually-labeled samples per task. See Appendix E for task details.</p><p>We measure the average performance across each operation across different sizes of models in the four families (Neo, OPT, BLOOM, and T0). We group models and sizes into four buckets of T0 (3B parameters) and GPT models (&lt; 1B, 1B, and 6 -7B parameters). Figure <ref type="figure">5b</ref> shows results where the buckets are ordered by their average AMA lift across the 7 tasks from Section 5.2, meaning T0 (3B) sees the least lift while 6 -7B GPT models realize the most lift. We find that overall, models with higher performance across the four operations see more lift with AMA. T0 performs poorly on the generative tasks, indicating the importance of text and question generation for AMA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation against other aggregation methods</head><p>We compare our WS aggregation approach with the standard unsupervised approach, majority vote (MV), on prompt()-chains. We find that AMA can achieve up to 8.7 points of lift over MV, and does not do worse than MV on 16 out of 20 tasks. On the remaining 4 tasks, we perform worse than MV by at most 1.0 points. We also examine the effect of modeling dependencies in WS. We find that on 9 tasks, our approach recovers dependencies in the data (rather than assuming conditionally independent P(x)), which improves performance by up to 9.6 points and an average of 2.2 points. We provide more details and evaluation against labeled data baselines in RTE respectively, and find that aggregating with MV yields an average lift of 3.7 accuracy points and aggregating with WS gives an average lift of 6.1 accuracy points (see Table <ref type="table" target="#tab_5">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we introduce ASK ME ANYTHING PROMPTING which (1) scalably obtains multiple prompts given a task input and (2) combines the intermediate answers to these prompts using weak supervision to give the final prediction. The steps in AMA stem from our observations on the effectiveness of open-ended questions over restrictive prompts, and the ability to model the varying accuracies and dependencies across a collection of prompts using weaksupervision. Overall, AMA provides lift across four language model families and across model sizes ranging from 125M-175B parameters. Most excitingly, we find that AMA enables a 30x smaller LM to exceed the average performance of few-shot GPT3-175B averaged across 20 popular language benchmarks. Several LM applications involve private data or require operating over large amounts of data -for these applications, using APIs to access closedsource models or hosting large models locally is challenging. We hope the strategies in AMA and subsequent work help enable such applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Reproducibility Statement</head><p>We release prompts and code for reproducing all benchmark results for few-shot and AMA prompting, and our diagnostic evaluation splits here: https://github.com/HazyResearch/ama_prompting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Ethics Statement</head><p>We intend for AMA to aid practitioners in their exploration and use of LLMs-especially smaller, open-source LLMs. However, we recognize that AMA could be used to perform harmful or unethical tasks. AMA is a proof-of-concept; it has error-modes and we recognize the inherent risks to using LLMs. Detailed discussions of these risks are in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experiment Details</head><p>We use A100 NVidia GPUs to run all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Models</head><p>We evaluate over 4 model families: T0, BLOOM, Neo, OPT, and GPT3. In our evaluations, we use the following model family variants: Neo (GPT-Neo-125M, GPT-Neo-1.3B, GPT-Neo-6B, GPT-Neo-20B), BLOOM (BLOOM-560M, BLOOM-1.7B, BLOOM-7.1B, BLOOM-176B), OPT(OPT-125M, OPT-1.3B, OPT-6.7B, OPT-13B, OPT-175B), T0 (T0-3B), and GPT-3 (davinci). We download T0, BLOOM, OPT, and Neo models from the HuggingFace Model Hub <ref type="bibr" target="#b45">[HuggingFace, 2021]</ref>. All inference calls to the OpenAI Davinci model were made using the OpenAI API davinci endpoint <ref type="bibr" target="#b46">[OpenAI, 2021]</ref>, the original GPT-3 175B parameter model used in <ref type="bibr" target="#b0">[Brown et al., 2020]</ref>. We access these models by passing our input prompts to the endpoint for a per-sample fee.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Metrics</head><p>For RealTimeQA, the reported GPT-3 performance in <ref type="bibr" target="#b37">Kasai et al. [2022]</ref> is reported over the text-davinci-002 API endpoint. Given that all our GPT-3 evaluations are over davinci, we re-evaluate the GPT-3 performance on RealTimeQA using the davinci endpoint and the few-shot prompt from RealTimeQA<ref type="foot" target="#foot_2">4</ref> .</p><p>We follow the metrics used in <ref type="bibr" target="#b0">Brown et al. [2020]</ref>. All tasks are scored using matching accuracy except for DROP/Re-alTimeQA that use text f1, WebQ/NQ that use span overlap accuracy, and MultiRC that uses f1a accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Weak Supervision</head><p>For each task, we use an unlabeled dataset constructed from the test set as well as 1000 samples from the training set (ignoring the labels). We run the structure learning part of the weak supervision algorithm (for ?) with the default parameters from <ref type="bibr" target="#b13">Varma et al. [2019]</ref>. If the recovered sparse matrix has all entries greater than 1, we pass in an empty edgeset to the next step of learning ? (e.g., data is too noisy to learn structure from); otherwise, we pass in the edge with the highest value in the sparse matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Weak Supervision Aggregation Ablations</head><p>Table 4 compares AMA's aggregation method against several other baselines for aggregating prompt()-chains, including majority vote. We compare against weighted majority vote (WMV), where we use labeled data to weight according to each prompt's accuracy by constructing ? WMV (P(x)) = m i=1 exp(-?? i )1 {p i (x) = y}. ? i is the error of p i on a labeled training set of 1000 examples, and ? is a temperature hyperparameter, for which we perform a sweep over <ref type="bibr">[0.25, 0.5, 1, 2, 4, 8, 16, 32</ref>] using a 20% validation split. We also compare against a simple strategy of using the prompt that performs the best on the labeled set of data (Pick Best). Finally, AMA (no deps) is our method when we pass in an empty edgeset to the algorithm in <ref type="bibr" target="#b27">Ratner et al. [2018]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Weak Supervision Algorithm</head><p>We briefly explain the weak supervision algorithm used for constructing ? WS . Weak supervision models learn the latent variable graphical model on the distribution Pr(y, P(x)) using the dataset D, and aggregate votes using the learned distribution by setting ?(x) = arg max y Pr(y|P(x)). Our key insight in our aggregation approach is to  parametrize Pr(y, P(x)) so that we can capture variations in accuracy as well as dependencies if they exist. The overall procedure of our aggregation is in Algorithm 1. Formally, we model Pr(y, P(x)) as a probabilistic graphical model with dependency graph G = (V, E), where V = {y, P(x)}. If p i (x) and p j (x) are not conditionally independent given y and the other prompt()-chains, then (p i (x), p j (x)) ? E. E also contains edges (p i (x), y)</p><formula xml:id="formula_1">for each i ? [m].</formula><p>The algorithm uses P(x) and D to first learn the dependency structure ? among prompts using the approach from <ref type="bibr" target="#b13">Varma et al. [2019]</ref>. The key insight from that work is that the inverse covariance matrix ? -1 over y and Procedure 1: AMA Aggregation Method 1: Input: Dataset D = {x i } n i=1 , collection of prompt()-chains P. Output: Predictions {? i } n i=1 . 2: Prompt the LLM with P to produce m predictions P(x) per input x ? D, constructing dataset D P ? R n?m . 3: Learn ? = (V, ?) via structure learning on D P (Algorithm 1 in <ref type="bibr" target="#b13">Varma et al. [2019]</ref>). 4: Learn Pr ?, ? (y, P(x)) using D P and ? (Algorithm 1 in <ref type="bibr" target="#b27">Ratner et al. [2018]</ref>). 5: Construct aggregator ? WS (P(x)) = arg max y?Y Pr ?, ? (y|P(x)). 6: Returns: ?AMA = ? WS (x) for all x ? D. P(x) is graph-structured, meaning that ? -1 ij = 0 iff p i (x) and p j (x) are conditionally independent given y. The graph structure means that the inverse covariance over just P(x) decomposes into sparse and low-rank matrices, which can hence be estimated together using RobustPCA <ref type="bibr" target="#b47">[Cand?s et al., 2011]</ref>, and the sparse matrix can be used to recover the graph. Next, the algorithm uses the recovered ? along with P(x) and D to learn the accuracies of the prompts with the approach from <ref type="bibr" target="#b27">Ratner et al. [2018]</ref>. The key insight from that work is to use the sparsity of ? -1 to construct a system of equations set equal to 0 that recover the latent accuracy parameters. Once the parameters of the distribution are learned, we can compute Pr ?, ? (y|P(x)) and aggregate our predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Information-Flow Theoretical Result</head><p>In equation 1, we decompose H(y|?) into H(y|P(x)) and H(y|?) -H(y|P(x)). For AMA, suppose that the weak supervision algorithm exactly recovers Pr(y, P(x)). That is, ?AMA is drawn from Pr(?|P(x)). Then, the second term H(y|?) -H(y|P(x)) can be thought of as an irreducible error corresponding to how much information about y is lost in converting P(x) into an i.i.d. y randomly drawn from Pr(?|P(x)). Since y is more likely to change values when this distribution has high entropy, the second term is correlated with our first term H(y|P(x)), the amount of randomness in Pr(y|P(x)). We thus focus on obtaining an expression for H(y|P(x)) in terms of individual prompt accuracies.</p><p>We assume that Y = {-1, 1}. We model Pr(y, P(x)) as a probabilistic graphical model with dependency graph G = (V, E), where V = {y, P(x)}. The density of Pr(y, P(x)) follows the following Ising model commonly used in weak supervision <ref type="bibr" target="#b12">[Ratner et al., 2017</ref><ref type="bibr" target="#b28">, Fu et al., 2020]</ref>:</p><formula xml:id="formula_2">Pr G,? (y, P(x)) = 1 Z exp ? y y + m i=1 ? i p i (x)y + (i,j)?E ? ij p i (x)p j (x) , (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where Z is the partition function for normalization and {? y , ? i ? i ? [m], ? ij ? (i, j) ? E}. Each ? i can be viewed as the strength of the correlation between y and p i (x), while each ? ij can be viewed as the strength of the dependence between p i (x) and p j (x). We assume that ? y = 0, which corresponds to Pr(y = 1) = 1 2 . We present our expression for H(y|P(x)). Define ? = [? 1 , . . . , ? m ] to be the vector of canonical parameters corresponding to the strength of correlation between y and each p i (x). Define ? = E [p i (x)], which can be written as 2 Pr(p i (x) = y) -1, a notion of accuracy scaled to [-1, 1].</p><p>Note that the above form of the distribution is in terms of canonical parameters ?. This distribution can also be parametrized in terms of the mean parameters corresponding to ?, which are</p><formula xml:id="formula_4">E [y] , E [p i (x)y] for i ? [m], and E [p i (x)p j (x)] for (p i (x), p j (x)) ? E.</formula><p>Theorem 1. Assume Pr(y, P(x)) follows equation 2 above. Then, the conditional entropy H(y|P(x)) can be expressed as</p><formula xml:id="formula_5">H(y|P(x)) = H(y) -? ? -E P(x) log cosh ? P(x)<label>(3)</label></formula><p>The quantity being subtracted from H(y) corresponds to the reduction in entropy of y given that we observe P(x). Within this expression, there are two terms. First, ? ? is correlated with how much signal each p i (x) contains about y. Note that this quantity is symmetric-if p i (x) is negatively correlated with y, it still provides information since both ? i and E [p i (x)y] will be negative. The second term, E P(x) log cosh ? P(x) , is for normalization (otherwise, the first term can grow arbitrarily large with ?). Note that this quantity is independent of ? ij , the interactions between prompts.</p><p>Proof. We can write H(y|P(x)) as H(y, P(x)) -H(P(x)), and H(y, P(x)) as H(P(x)|y) + H(y). Therefore, H(y|P(x)) = H(y) -H(P(x)) -H(P(x)|y) . We focus on simplifying H(P(x)) -H(P(x)|y):</p><formula xml:id="formula_6">H(P(x)) -H(P(x)|y) = - P(x)?{-1,1} m</formula><p>Pr(P(x)) log Pr(P(x)) + P(x)?{-1,1} m ,y</p><p>Pr(y, P(x)) log Pr(P(x)|y) (4)</p><formula xml:id="formula_7">= - P(x)?{-1,1} m ,y</formula><p>Pr(P(x), y) log Pr(P(x)) -log Pr(P(x)|y) = -</p><formula xml:id="formula_8">P(x)?{-1,1} m</formula><p>Pr(P(x), y = -1) log Pr(P(x)) -log Pr(P(x)|y = -1)</p><p>+ Pr(P(x), y = 1) log Pr(P(x)) -log Pr(P(x)|y = 1) .</p><p>We now write Pr(P(x)), Pr(P(x)|y = -1) and Pr(P(x)|y = 1) according to our Ising model in equation 2. Let A P(x) = m i=1 ? i p i (x), and let B P(x) = (i,j)?E ? ij p i (x)p j (x), so that Pr(y, P(x)) = 1 Z exp(A P(x) y + B P(x) ): Pr(P(x)) = Pr(P(x), y = -1) + Pr(P(x), y = 1)</p><formula xml:id="formula_9">= 1 Z exp(A P(x) + B P(x) ) + 1 Z exp(-A P(x) + B P(x) )) = 1 Z exp(B P(x) ) exp(A P(x) ) + exp(-A P(x) ) Pr(P(x)|y = -1) = 2 Pr(P(x), y = -1) = 2 Z exp(-A P(x) + B P(x) ))</formula><p>Pr(P(x)|y = 1) = 2 Pr(P(x), y = 1) = 2 Z exp(A P(x) + B P(x) ))</p><p>Therefore, we have that log Pr(P(x)) -log Pr(P(x)|y = -1) = -log Z + B P(x) + log exp(A P(x) ) + exp(-A P(x) )</p><p>-log 2 + log Z + A P(x) -B P(x) = -log 2 + A P(x) + log exp(A P(x) ) + exp(-A P(x) )</p><p>log Pr(P(x)) -log Pr(P(x)|y = 1) = -log Z + B P(x) + log exp(A P(x) ) + exp(-A P(x) )</p><p>-log 2 + log Z -A P(x) -B P(x) = -log 2 -A P(x) + log exp(A P(x) ) + exp(-A P(x) )</p><p>Plugging this back into equation 4, we have</p><formula xml:id="formula_10">P(x)?{-1,1} m ,y</formula><p>Pr(P(x), y)A P(x) y -Pr(P(x)) log exp(A P(x) ) + exp(-A P(x) ) -log 2</p><formula xml:id="formula_11">= P(x)?{-1,1} m ,y</formula><p>Pr(P(x), y)A P(x) y -Pr(P(x)) log cosh A P(x)</p><formula xml:id="formula_12">=E A P(x) y -E log cosh A P(x) .</formula><p>Substituting in our definitions of ? and ? give us our desired expression for H(y|P(x)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E AMA Diagnostics</head><p>We present a suite of 8 diagnostic tasks, which can be categorized into four task types: question generation, answer generation, answer selection and extraction. We provided details about the tasks and scoring below.</p><p>Question Generation: We measure the ability of the model to transform a statement to a question. We construct 3 question generation tasks which evaluate the models ability to transform a statement to a yes/no question (see Question Generation (Yes/No)), transform a statement to a whquestion (see Question Generation (wh-)) and finally, transform a statement about a placeholder entity to a question about the placeholder (see Question Generation (@placeholder)).</p><p>All question generation tasks are scored using the ROUGE score <ref type="bibr" target="#b48">[Lin, 2004]</ref>.</p><p>Question Generation (Yes/No)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Rewrite the statement as a yes / no question .</p><p>Statement : The father and son went camping to California . Question :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>Did the father and son go camping ?</p><p>Question Generation (wh-)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Convert statement to a question .</p><p>Statement : Aristide kills Prime Minister Robert Malval Question :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>Who killed Prime Minister Robert Malval ?</p><p>Question Generation (@placeholder)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Rewrite the statement as a question about the [ at ] placeholder .</p><p>Statement : Most of the light comes from the [ at ] placeholder Question :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>Where does most of the light come from ?</p><p>Answer Selection: We construct 2 answer selection tasks which measure the model's ability to generate an answer that is faithful to a set of provided answer choices. Concretely, we measure the models ability to select object categories from a fixed set of options specified in the context (see Answer Selection (category)). Further, we measure the model's ability to complete a sentence when provided with a context and set of sentence completion candidates (see Answer Selection (completion)). In both tasks, an answer is marked as correct if the generated response is one of the candidates provided in the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer Selection (category)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Select the correct category .</p><p>"  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Error Analysis</head><p>We bucket the common error modes of AMA into three categories: knowledge, instruction-following, and longcontext.</p><p>Knowledge errors. We find that AMA yields the most gains when the knowledge required to complete the task is explicitly provided in the context (e.g., reading comprehension, extractive QA), which is in line with the trends in Figure <ref type="figure" target="#fig_4">6</ref>. We find that AMA provides comparatively less lift on tasks where the model needs to (1) recall encoded factual knowledge or (2) apply common-sense / real-world knowledge to a given context. We provide concrete examples from the NaturalQuestions dataset (see Knowledge (Factual) below) in which the GPT-Neo-6B model wrongly answers the question due to a lack of latent factual knowledge. We additionally provide case-examples from the BoolQ dataset where the model's limited real-world knowledge limits its ability to correctly answer the questions where the model's failure to recognize that food that is smoked is cooked, leads it to incorrectly answer the question (see Knowledge (Commonsense) below).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge (Factual)</head><p>Input Question : what ' s the dog ' s name on tom and jerry Answer :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction</head><p>The dog ' s name is " Fido "</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>Spike Knowledge (Commonsense)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Passage : A Philadelphia roll is a makizushi ( also classified as a kawarizushi ) type of sushi generally made with smoked salmon , cream cheese , and cucumber . It can also include other ingredients , such as other types of fish , avocado , scallions , and sesame seed . Question : is the salmon cooked in a philadelphia roll Answer :</p><p>Prediction false Ground Truth true Instruction-following errors. We find that on tasks with more restrictive output spaces (e.g., multi-way classification tasks), a common failure mode is to generate an answer that is not in the desired output space of the AMA prompt, despite being explicitly prompted to do so. In Listing 3 and 4, we provide sample instances from the DBPedia classification task where GPT-Neo-6B does not correctly map a descriptive adjective (e.g., automobile or singer) to a valid class specified in the prompt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instruction Following (1)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Pick one category for the following text . was an original member of the American singing girl group the Chiffons .</p><p>Summary : This passage is about a singer .</p><p>The summary " Summary " fits " Category ":</p><p>Prediction singer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth artist</head><p>Long-context errors. We find that the AMA question() functions struggle to generate accurate statement-question transformations when the input is long or contains complex sentence structures (e.g. compound sentences). We provide sample instances from the SuperGLUE record task where GPT-Neo-6B fails to transform a sentence with a placeholder subject to a question about the placeholder subject (see Long-context (question()) below). Additionally, we find that the AMA answer() functions struggle to extract the correct span in long contexts (greater than 6 sentences). We show a sample instance from the DROP QA task where GPT-Neo-6B fails to extract the correct span from the long provided context (see Long-context (answer()) below).</p><p>Long-context (question())</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Rewrite the statement as a question about the [ at ] placeholder .</p><p>Statement : Most of the light comes from the [ at ] placeholder Question : Where does most of the light come from ?</p><p>Statement : The [ at ] placeholder was not hard Question : What was not hard ?</p><p>Statement : [ at ] placeholder went to the mall with her mom to buy a backpack Question : Who went to the mall with her mom to buy a backpack ?</p><p>Statement : Rossello warned the public on Sunday that the island could feel [ at ] placeholder ' s wrath around noon Wednesday . Question :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction</head><p>Who warned the public on Sunday that the island could feel [ at ] placeholder ' s wrath around noon Wednesday ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>Who ' s wrath could be felt around noon on Wednesday ?</p><p>Long-context (answer()) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Patriots</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dolphins</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Datasets and Prompts</head><p>We evaluate over 20 datasets which fall into 4 categories: SuperGLUE (BoolQ <ref type="bibr" target="#b50">[Clark et al., 2019]</ref>, CB <ref type="bibr" target="#b51">[De Marneffe et al., 2019]</ref>, COPA <ref type="bibr" target="#b52">[Roemmele et al., 2011]</ref>, MultiRC <ref type="bibr" target="#b53">[Khashabi et al., 2018]</ref>, ReCoRD <ref type="bibr">[Zhang et al., 2018]</ref>, RTE <ref type="bibr" target="#b30">[Wang et al., 2019]</ref>, <ref type="bibr">WiC [Pilehvar and Camacho-Collados, 2018]</ref>, <ref type="bibr">WSC [Levesque et al., 2012]</ref>), NLI (ANLI R1, ANLI R2, ANLI R3 <ref type="bibr" target="#b34">[Nie et al., 2020]</ref>, <ref type="bibr">StoryCloze [Mostafazadeh et al., 2017]</ref>), Classification (DBPedia <ref type="bibr" target="#b31">[Zhang et al., 2015]</ref>, AGNews <ref type="bibr" target="#b31">[Zhang et al., 2015]</ref>, SST2 <ref type="bibr" target="#b35">[Socher et al., 2013]</ref>, Amazon <ref type="bibr" target="#b36">[He and McAuley, 2016]</ref>), and Question-Answering (RealTimeQA <ref type="bibr" target="#b37">[Kasai et al., 2022]</ref>, DROP <ref type="bibr" target="#b40">[Dua et al., 2019]</ref>, NaturalQuestion <ref type="bibr" target="#b38">[Kwiatkowski et al., 2019]</ref>, WebQuestions <ref type="bibr" target="#b39">[Berant et al., 2013]</ref>). We provide dataset details along with few shot and AMA prompts for the dataset below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 AGNews</head><p>Description: News article classification dataset with 4 topics. <ref type="bibr" target="#b31">Zhang et al. [2015]</ref> Train Size: 120000, Test Size: 76000</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AGNews Few Shot</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Pick the correct category for the passage .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Categories :</head><p>-World News -Sports -Business -Technology and Science Passage : Wedding cad comes clean over invite sale ( Reuters ) . Reuters -A wedding guest who sparked a bidding frenzy when he offered for sale a pair of invitations to a wedding he did not want to attend has admitted that the bride was a former girl friend . Category : World News Passage : Tennis : Serena Williams Reaches Finals of China Open . Top seed Serena Williams of the United States has powered her way into the finals of the China Open tennis tournament in Beijing with a straight sets (6 -2 , 6 -3) victory over fourth -seeded Vera Zvonareva of Russia . Category : Sports Passage : Abramovich faces rich list challenge . Lakshmi Mittal , the Indian -born steel magnate , yesterday staked a claim to overtake Roman Abramovich as Britain ' s richest man with a 10 bn deal to create the world ' s largest steelmaker . Category : Business Passage : The Race is On : Second Private Team Sets Launch Date for Human Spaceflight ( SPACE . com ) .</p><p>SPACE . com -TORONTO , Canada --A second team of rocketeers competing for the #36;10 million Ansari X Prize , a contest for privately funded suborbital space flight , has officially announced the first launch date for its manned rocket . Category : ) is an American actor , comedian , writer , and director . He played Kim Jong -Un in the 2014 film " The Interview " , Minnesota governor Danny Chung in " Veep " , and beginning in 2015 he portrayed Eddie Huang ' s father , American restaurateur Louis Huang , in ABC ' s television show " Fresh Off the Boat ". Question : Randall Park is dead True , False , or Neither ? False Fragaria x vescana is a hybrid strawberry cultivar that was created in an effort to combine the best traits of the garden strawberry (" Fragaria " x " ananassa ") , which has large berries and vigorous plants , with the woodland strawberry (" Fragaria vesca ") , which has an exquisite flavour , but small berries . Question : Extensive testing went on to produce this berry True , False , or Neither ? Neither Daniel Zolnikov ( born January 29 , 1987) is a Republican member of the Montana Legislature . He was elected to House District 47 which represents Billings , Montana After redistricting , he now represents House District 45. He has made a name for himself pursuing pro -privacy legislation . Question : There is no information indicating whether Daniel Zolnikov is a good legislator or not .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold Output</head><p>True , False , or Neither ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold Output</head><p>neither ANLI R1 AMA prompt()-chain Example</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>question()</head><p>Rewrite the statement as a yes / no question .</p><p>Statement : most of the light comes from the sun Question : Does most of the light come from the sun ?</p><p>Statement : the test was not hard Question : Was the test not hard ?</p><p>Statement : it is a good idea to buy your parents gifts Question : Is it a good idea to buy your parents gifts ?</p><p>Statement : the balloon popped Question : Did the balloon pop ?</p><p>Statement : The father and son went camping to California . Question : Did the father and son go camping ?</p><p>Statement : There is no information indicating whether Daniel Zolnikov is a good legislator or not . Question :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Output</head><p>Is there information indicating whether Daniel Zolnikov is a good legislator ? answer() Answer the question . If there is no evidence in the context , return " Unknown ".</p><p>Context : According to Biraben , the plague was present somewhere in Italy and affected 1 ,200 people . Question : Based on the context , Did the plague affect people in Europe ? Answer : yes , people in Italy , Europe Context : Policies aiming at controlling unemployment and in particular at reducing its inequality -associated effects support economic growth . Question : Based on the context , Is confidence a factor in increasing self -esteem ? Answer : unknown Context : The term " matter " is used throughout physics in a bewildering variety of contexts : for example , one refers to " condensed matter physics " , " elementary matter " , " partonic " matter , " dark " matter , " anti " -matter , " strange " matter , and " nuclear " matter . Question : Based on the context , Is anti -matter made of electrons ? Answer : Unknown Context : Tonic water --Tonic water ( or Indian tonic water ) is a carbonated soft drink in which quinine is dissolved . Originally used as a prophylactic against malaria , tonic water usually now has a significantly lower quinine content and is consumed for its distinctive bitter flavor . It is often used in mixed drinks , particularly in gin and tonic . Question : does tonic water still have quinine in it ? Answer : yes Context : Northern bobwhite --The northern bobwhite , Virginia quail or ( in its home range ) bobwhite quail ( Colinus virginianus ) is a ground -dwelling bird native to the United States , Mexico , and the Caribbean . It is a member of the group of species known as New World quails ( Odontopho ridae ) . They were initially placed with the Old World quails in the pheasant family ( Phasianidae ) , but are not particularly closely related . The name '' bobwhite ' ' derives from its charac terist ic whistling call . Despite its secretive nature , the northern bobwhite is one of the most familiar quails in eastern North America because it is frequently the only quail in its range . Habitat degradation has likely contributed to the northern bobwhite population in eastern North America declining by roughly 85% from 1966 -2014. This population decline is apparently range -wide and continuing . Question : is a quail the same as a bobwhite ? Answer : yes Context : United States Department of Homeland Security --In fiscal year 2017 , it was allocated a net discretionary budget of $40 .6 billion . With more than 240 ,000 employees , DHS is the third largest Cabinet department , after the Departments of Defense and Veterans Affairs . Homeland security policy is coordinated at the White House by the Homeland Security Council . Other agencies with significant homeland security r e s p o n s i b i l i t i e s include the Departments of Health and Human Services , Justice , and Energy Question : is department of homeland security part of dod ? Answer : no Context : Harry Potter and the Escape from Gringotts --Harry Potter and the Escape from Gringotts is an indoor steel roller coaster at Universal Studios Florida , a theme park located within the Universal Orlando Resort . Similar to dark rides , the roller coaster utilizes special effects in a controlled -lighting environment and also employs motion -based 3 -D projection of both animation and live -action sequences to enhance the experience . The ride , which is themed to the Gringotts Wizarding Bank , became the flagship attraction for the expanded Wizarding World of Harry Potter when it opened on July 8 , 2014. Question : is harry potter and the escape from gringotts a roller coaster ride ? Answer : It is part of their religion , a religion I do not scoff at as it holds many elements which match our own even though it lacks the truth of ours . At one of their great festivals they have the ritual of driving out the devils from their bodies . First the drummers come on -I may say that no women are allowed to take part in this ritual and the ladies here will perhaps agree with me that they are fortunate in that omission . Question : no women are allowed to take part in this ritual True , False , or Neither ? True Modify the arachnids , said the researchers . Change their bodies and conditions , and you could get fibres like glass , still monofilament , but with logarithmic progressions of possibilities of strength and flexibility , and the ability to resonate light -particles or sound -waves undistorted , scarcely weakened over thousands of miles . Who said the arachnids had to be totally organic ? Question : arachnids had to be totally organic . True , False , or Neither ? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold Output</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Output</head><p>Did arachnids have to be totally organic ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>answer()</head><p>Pick the correct ending for the example .</p><p>Question : ( because ' she took medicine ' , because ' she got expelled ') My roommate was feeling better because ? Answer : ' she took medicine ' Question : ( because ' he does not practice ' , because ' he is fast ') Matt is not good at soccer because ? Answer : ' he does not practice ' Question : ( because ' she was smart ' , because ' she never did her homework ') The girl went to college and graduated with honors because ? Answer : ' she was smart ' Question : ( and so ' her family avoided her . ' , and so ' photographers followed her . ') The woman became famous and so ? Answer :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Choices</head><p>-woman became famous so -photographers followed her victims . In a conversation with the British ambassador in Bucharest , King Carol I mentioned a figure of " several thousand ". According to figures given by Austrian diplomats , between 3 ,000 -5 ,000 peasants were killed , while the French Embassy mentioned a death toll ranging between 10 ,000 -20 ,000. Historians put the figures between 3 ,000 -18 ,000 , the most common being 11 ,000 victims . Question : Which organizations said the death toll to be beyond 10 ,000? Context : Garita Palmera , El Salvador ( CNN ) --She talks to the pictures as if they could make her voice travel thousands of miles and reach her son ' s ears . " Oh , my son ," Julia Alvarenga , 59 , says in a tender voice at her home in this coastal town . And then she says , "I ' m going to see him again ." The past two weeks have been an emotional roller coaster for the Salvadoran woman . First , she learned her son had been missing for 13 months . Then she was told he had turned up half a world away . And now she ' s getting news he might be back home soon .. It ' s been an emotional time for the parents of castaway Jose Salvador Alvarenga . His mother , Julia , said her son didn ' t keep up , and they didn ' t even know he was missing . " I would pray to God , and I won ' t lie to you , I was crying ," she says . For the excited residents of his town in El Salvador , Alvarenga is a hero Answer : Even though their son has yet to return home , he ' s already a celebrity in Garita Palmera and neighboring towns .</p><p>Context : ( CNN ) --Members of a well -known hacking group --according to a statement and Twitter messages --took credit Sunday for an online attack targeting San Francisco ' s embattled transit system . Anonymous --in a news release attributed to the group , and backed up by related Twitter pages --said it would take down the website of the Bay Area Rapid Transit System , known as BART , between noon and 6 p . m . PT Sunday . This is in response to the system ' s decision to cut off cellphone signals at " select " subway stations in response to a planned protest last week . " By ( cutting cell service ) , you have not only threatened your citizens ' safety , you have also performed an act of censorship ," a seemingly computer -generated voice --speaking over dramatic music and images --said in a video posted online Sunday afternoon .</p><p>" By doing this , you have angered Anonymous .". NEW : A video urges protesters Monday to wear red shirts and record the event . Statements attributed to Anonymous promised an online attack Sunday on BART . MyBART . gov appears Sunday to have been hacked . The system said it was prepared for hacks , as well as a planned protest Monday Answer : " We ' re doing what we can to defend against any attack on the BART website ," the system said .. Context : Tracy Morgan hasn ' t appeared on stage since the devastating New Jersey crash that nearly ended his life last summer , but all that will change this fall when he returns to host Saturday Night Live . NBC announced on Twitter Monday that Morgan , an SNL alum with seven seasons as a cast member under his belt , will headline the third episode of Season 41 airing October 17. For Morgan , 46 , it will be a second time hosting the long -running variety show , the first since the June 2014 pileup on the New Jersey Turnpike that killed his friend and mentor James ' Jimmy Mack ' McNair .. Morgan , 46 , will host third episode of season 41 of SNL airing October 17. He tweeted to his fans : ' Stoked to be going home ...# SNL '. For the SNL alum who had spent seven years as cast member , it will be a second time hosting the show . Morgan has been sidelined by severe head trauma suffered in deadly June 2014 crash on New Jersey Turnpike that killed his friend . First episode of new SNL season will be hosted by Miley Cyrus , followed by Amy Schumer Answer : ' On October 10 , acclaimed comedian and star of the summer box office hit Trainwreck Amy Schumer will make her SNL debut , followed by Context : Tracy Morgan hasn ' t appeared on stage since the devastating New Jersey crash that nearly ended his life last summer , but all that will change this fall when he returns to host Saturday Night Live . NBC announced on Twitter Monday that Morgan , an SNL alum with seven seasons as a cast member under his belt , will headline the third episode of Season 41 airing October 17. For Morgan , 46 , it will be a second time hosting the long -running variety show , the first since the June 2014 pileup on the New Jersey Turnpike that killed his friend and mentor James ' Jimmy Mack ' McNair .. Morgan , 46 , will host third episode of season 41 of SNL airing October 17. He tweeted to his fans : ' Stoked to be going home ...# SNL '. For the SNL alum who had spent seven years as cast member , it will be a second time hosting the show . Morgan has been sidelined by severe head trauma suffered in deadly June 2014 crash on New Jersey Turnpike that killed his friend . First episode of new SNL season will be hosted by Miley Cyrus , followed by Amy Schumer . ' On October 10 , acclaimed comedian and star of the summer box office hit Trainwreck Amy Schumer will make her SNL debut , followed by Frontier Airlines and Spirit Airlines . Question : Which airline announced a deal this week to buy Spirit Airlines ? Answer : " JetBlue " Article 1: Oak Fire : California ' s fast -moving wildfire burns 14 ,000 acres and ... ( CNN ) A wildfire raging for a third day Sunday in central California ' s Mariposa County outside Yosemite National Park has burned more than 14 , 000 acres and forced thousands to evacuate from rural communities . Article 2: California Oak Fire : Rapidly -growing fire engulfs homes near ... For more on the fires , " United Shades of America with W . Kamau Bell " heads to California to discover how communities are learning to coexist with the frequent destruction . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Choices</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example prompt with the in-context demonstrations and placeholder (Left) with two different prompt variations (Right) created by changing demonstrations and question style.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The top plots are for Neo models of sizes ? {125M, 1.3B, 6B, 20B} and the bottom plots are for BLOOM models of sizes ? {560M, 1.7B, 7.1B, 175B}. The left plots show the conditional entropy metric H(y|?) as a function of model size. Lines represent different prompts p with k = {0, 2, 4, 8} in-context examples and AMA prompt-chains without aggregation. The right plots show the conditional entropy as we aggregate predictions over an increasing number of AMA prompt-chains, with both the majority vote (MV) and weak supervision (WS) aggregation stragegies for the GPT-Neo-6B and BLOOM 7.1B models. All plots are over RTE and each k-shot point is the average of 4 seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Average diagnostic score vs. model size (parameters).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>, Venezuela ( CNN ) --It ' s been more than 180 years since Venezuelans saw Simon Bolivar ' s face . But the revolutionary leader ' s thick sideburns , bushy eyebrows and steely gaze popped out from behind picture frames Tuesday in new 3 -D images unveiled by President Hugo Chavez . Researchers used several software programs to reconstruct the face of the man who liberated Bolivia , Colombia , Ecuador , Panama , Peru and Venezuela from the Spanish crown . Scans of Bolivar ' s skeletal remains , which investigators exhumed two years ago , factored into their calculations . So did historical paintings , photos of restored uniforms Bolivar wore and images of middle -aged Venezuelans , officials said .Extract the sentence containing " Simon Bolivar ":OutputCaracas , Venezuela ( CNN ) --It ' s been more than 180 years since Venezuelans saw Simon Bolivar ' s face .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Relative performance gain observed when scaling from GPT3-6.7B to GPT3-175B. Results are directly from Brown et al. [2020] and are categorized by type of knowledge required for the task.</figDesc><graphic url="image-2.png" coords="21,212.40,72.00,187.20,124.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>"</head><label></label><figDesc>High Speed -The Monteverdi High Speed was a grand tourer automobile built by Monteverdi in Basel Switzerland from 1967 to 1970. Contemporary rivals included the British Jensen Interceptor ( which was also powered by a Chrysler V8 ) . This car was designed by the Italian design house Frua and was actually built by Fissore of Italy from 1969. They redesigned the car in 1972 and again in 1975. The convertible version of the High Speed 375 was known as the Palm Beach .Summary : This passage is about a automobile .The summary " Summary " fits " Category ": : Patricia Bennett -Patricia Bennett( born 7 April 1947 in The Bronx New York )   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>InputContext:</head><label></label><figDesc>Looking to avoid back -to -back divisional losses , the Patriots traveled to Miami to face the 6 -4 Dolphins at Dolphin Stadium . After Carpenter ' s kickoff was returned from the 29 -yard line by Matthew Slater , the Patriots began their first possession at their own 40 -yard line . Cassel ' s first two passes were both completed for first downs , putting the Patriots in Dolphins territory and eventually their red zone . However , a holding penalty on Neal pushed the Patriots back 10 yards , forcing a 30 -yard Gostkowski field goal four plays later that gave the Patriots a 3 -0 lead . Following a Dolphins three -and -out , the Patriots ' second drive ended when a Cassel pass to Moss was bobbled by both Moss and cornerback Jason Allen to keep the ball in the air until Renaldo Hill intercepted it ; a 17 -yard return gave the Dolphins the ball at the Patriots ' 42 -yard line . On the next play , a 29 -yard David Martin reception moved the Dolphins into the Patriots ' red zone , where the Dolphins used their " Wildcat " formation on the next two plays .... Question : Which team scored first ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Adversarially mined natural language inference dataset from Wikipedia. Nie et al. [2020] Train Size: 16946, Test Size: 1000 ANLI R1 Few Shot Input Robert L . Hass ( born March 1 , 1941) is an American poet . He served as Poet Laureate of the United States from 1995 to 1997. He won the 2007 National Book Award and shared the 2008 Pulitzer Prize for the collection " Time and Materials : Poems 1997 -2005." In 2014 he was awarded the Wallace Stevens Award from the Academy of American Poets . Question : Robert L . Hass was Poet Laureate of the United States for two years . True , False , or Neither ? True Randall Park ( born March 23 , 1974</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Context : Daniel Zolnikov ( born January 29 , 1987) is a Republican member of the Montana Legislature . He was elected to House District 47 which represents Billings , Montana After redistricting , he now represents House District 45. He has made a name for himself pursuing pro -privacy legislation . Question : Based on the context , Is there information indicating whether Daniel Zolnikov is a good legislator Adversarially mined natural language inference dataset from Wikipedia. Nie et al. [2020] Train Size: 45460, Test Size: 1000 ANLI R2 Few Shot Input Answer the question using the context .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>False</head><label></label><figDesc>CB AMA prompt()-chain Example question() Rewrite the statement as a yes / no question . Statement : most of the light comes from the sun Question : Does most of the light come from the sun ? Statement : the test was not Question : Was the test hard ? Statement : it is a good idea to buy your parents gifts Question : Is it a good idea to buy your parents gifts ? Statement : the balloon popped Question : Did the balloon pop ? Statement : The father and son went camping to California . Question : Did the father and son go camping ? Statement : arachnids had to be totally organic Question :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Answer : Newspapers patronized by Constantin Mille Passage : Still searching for their first win , the Bengals flew to Texas Stadium for a Week 5 inter c on f er en c e duel with the Dallas Cowboys . In the first quarter , Cincinnati trailed early as Cowboys kicker Nick Folk got a 30 -yard field goal , along with RB Felix Jones getting a 33yard TD run . In the second quarter , Dallas increased its lead as QB Tony Romo completed a 4yard TD pass to TE Jason Witten . The Bengals would end the half with kicker Shayne Graham getting a 41 -yard and a 31 -yard field goal . In the third quarter , Cincinnati tried to rally as QB Carson Palmer completed an 18 -yard TD pass to WR T . J . Hou shmand zadeh . In the fourth quarter , the Bengals got closer as Graham got a 40 -yard field goal , yet the Cowboys answered with Romo completing a 57 -yard TD pass to WR Terrell Owens . Cincinnati tried to come back as Palmer completed a 10 -yard TD pass to H oushma ndzade h ( with a failed 2 -point conversion ) , but Dallas pulled away with Romo completing a 15 -yard TD pass to WR Patrick Crayton . Question : Which team scored the final TD of the game ? Answer : University of Pennsylvania has been named America ' s top party school by Playboy in the first time the Ivy League institution has made the list . Believe it or not the magazine gave the top spot to the college by declaring that ' UPenn puts other Ivies to shame with its union of brains , brewskies and bros . ' In the magazine ' s ninth annual ranking of universities around the country , the University of Wisconsin -Madison scored the runner up slot . The University of Pennsylvania ( pictured ) has been named America ' s top party school by Playboy in the first time the Ivy League institution has made the list . The University of Wisconsin -Madison scored the runner up slot . Last year ' s winner West Virginia University slipped down to third place . It is the magazine ' s ninth annual ranking of universities around the country Answer : Playboy writes : ' Boasting a notorious underground frat scene that school officials have deemed a nuisance , these renegades pony up thousands of dollars ' worth of liquor for their parties -and competition among the houses means a balls -out war of debauchery .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Article 3: 5 things to know for July 25: Wildfires , Ukraine , Monkeypox , Volcano ... If your day doesn ' t start until you ' re up to speed on the latest headlines , then let us introduce you to your new favorite morning fix . Article 4: Wildfires in US : 2 firefighting helicopter pilots die in Idaho ... Multiple wildfires raged across the U . S . Saturday , causing deaths , destruction and thousands of forced evacuations . Article 5: Boulder wildfires : Hundreds of homes burn evacuations ordered BOULDER , Colo . -A ferocious wind -driven wildfire on Thursday destroyed hundreds of homes and businesses near Denver , forcing tens of thousands to flee and blanketing the area in smoke . Question : A raging wildfire this week forced thousands of people to evacuate communities near which national park ? Answer : " Yosemite National Park " Article 1: 5 things to know for June 13: Gun laws , January 6 , Covid , White ... If your day doesn ' t start until you ' re up to speed on the latest headlines , then let us introduce you to your new favorite morning fix . Sign up here for the '5 Things ' newsletter . ( CNN ) Just imagine what a relief it would be if you could use the same charging cable for all of your devices -your phone , laptop , earbuds , camera , tablet , portable speaker , etc . Well , in a huge step to reduce cable clutter and waste , European regulators say that Apple and other smartphone makers Article 2: 5 things to know for March 11: Ukraine , Pandemic , MLB , North ... If your day doesn ' t start until you ' re up to speed on the latest headlines , then let us introduce you to your new favorite morning fix .Sign up here for the '5 Things ' newsletter . ( CNN ) America , the " land of the free ," is getting quite costly . Prices for gas , food and housing --which are all necessary expenses --are spiking across the country . Gas prices have risen 38% over the past year , and rising prices in pandemic -related sectors , such as travel and dining , are also expected as Article 3: Wi -Charge / consists of a transmitter and a receiver . Transmitter connects to a standard power outlet and converts electricity into infrared laser beam . Receivers use a miniature photo -voltaic cell to convert transmitted light into electrical power . Receivers can be embedded into a device or connected into an existing charging port . The transmitter automatically identifies chargeable receivers and start charging . Several devices can charge at the same time . According to Wi -Charge it can deliver several watts of power to a device at several meters away . The core technology is based on a " distributed laser resonator " which is formed by the Article 4: Mobile broadband / added in 2005. CDPD , CDMA2000 EV -DO , and MBWA are no longer being actively developed . In 2011 , 90% of the world ' s population lived in areas with 2 G coverage , while 45% lived in areas with 2 G and 3 G coverage , and 5% lived in areas with 4 G coverage . By 2017 more than 90% of the world ' s population is expected to have 2 G coverage , 85% is expected to have 3 G coverage , and 50% will have 4 G coverage . A barrier to mobile broadband use is the coverage provided by the mobile service networks . This may mean no mobile network or that Article 5: Mobile edge computing / Combining elements of information technology and t el e c o m m u n i c a t i o n s networking , MEC also allows cellular operators to open their radio access network ( RAN ) to authorized third -parties , such as application developers and content providers . Technical standards for MEC are being developed by the European T e l e c o m m u n i c a t i o ns Standards Institute , which has produced a technical white paper about the concept . MEC</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Standard-QA Format 83.3 69.2 62.0</figDesc><table><row><cell>Prompt Format</cell><cell cols="3">CB WSC RTE</cell></row><row><cell>Restricted 8-shot</cell><cell cols="3">22.0 50.0 53.0</cell></row><row><cell>+ Calibration</cell><cell>58.9</cell><cell>-</cell><cell>59.2</cell></row><row><cell>Cloze-QA Format</cell><cell cols="3">48.2 56.0 62.5</cell></row><row><cell>Figure</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>We understand how to aggregate P(x) by presenting a set of observations on CB, RTE, and WSC. For each, we compare two baselines for constructing P: (1) P T : varying the prompt template with no overlap in the in-context examples, and (2) P E : varying the in-context examples for a fixed prompt template, all with | P | = 5.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>Task</cell><cell cols="3">CB WIC WSC RTE</cell></row><row><cell>T0 (3B) Mean</cell><cell>45.4 50.7</cell><cell>65.1</cell><cell>64.6</cell></row><row><cell cols="2">T0 (3B) 10 Formats MV 60.7 50.5</cell><cell>68.3</cell><cell>60.6</cell></row><row><cell>T0 (3B) AMA MV</cell><cell>50.0 49.5</cell><cell>64.4</cell><cell>49.5</cell></row><row><cell cols="2">T0 (3B) 10 Formats WS 60.7 50.8</cell><cell>69.2</cell><cell>69.7</cell></row><row><cell>T0 (3B) AMA WS</cell><cell>50.0 51.4</cell><cell>66.4</cell><cell>59.2</cell></row></table><note><p><p><p><p><p>(Appendix B.1)</p>.</p>Next, we evaluate T0 on zero-shot prompts from the public PromptSource</p>[Bach et al., 2022]</p>, which are better aligned with how this model has been trained. Specifically, we take 10 unique PromptSource prompts for CB, WIC, WSC and</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Performance of T0 as reported in<ref type="bibr" target="#b4">Sanh et al. [2022]</ref> compared to majority vote (MV) and weak supervision (WS) over 10 different prompt formats in Prompt-Source. When using the Prompt-Source prompts, the average lift across tasks is 3.6 points for MV and 6.1 points for WS.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. ReCoRD: Bridging   the gap between human and machine commonsense reading comprehension. arXiv preprint 1810.12885, 2018. Mohammad Taher Pilehvar and Jose Camacho-Collados. Wic: the word-in-context dataset for evaluating contextsensitive meaning representations. arXiv preprint arXiv:1808.09121, 2018. Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thirteenth international conference on the principles of knowledge representation and reasoning, 2012.</figDesc><table /><note><p><p><p><p><ref type="bibr" target="#b1">Bommasani et al. [2021]</ref></p>,</p><ref type="bibr" target="#b44">Weidinger et al. [2021]</ref></p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>AMA results for the BLOOM-7.1B parameter model compared to the few-shot GPT3-175B. The GPT-175B numbers are as reported in<ref type="bibr" target="#b0">Brown et al. [2020]</ref>, where the numbers of shots is in parentheses, and the classification task baselines are from from<ref type="bibr" target="#b2">Zhao et al. [2021]</ref>.</figDesc><table><row><cell></cell><cell cols="2"># Prompts Avg</cell><cell>MV</cell><cell cols="2">WMV Pick Best</cell><cell cols="2">AMA (no dep) AMA (WS)</cell></row><row><cell>No labels:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Natural Language Understanding</cell><cell></cell><cell></cell></row><row><cell>WSC</cell><cell>3</cell><cell>74.7</cell><cell>77.8</cell><cell>77.8</cell><cell>75.0</cell><cell>77.8?0.0</cell><cell>77.8?0.0</cell></row><row><cell>WiC</cell><cell>5</cell><cell>59.0</cell><cell>61.3</cell><cell>60.9</cell><cell>60.0</cell><cell>60.8?0.0</cell><cell>61.3?0.2</cell></row><row><cell>RTE</cell><cell>5</cell><cell>61.4</cell><cell>66.0</cell><cell>71.4</cell><cell>62.0</cell><cell>65.1?0.5</cell><cell>74.7?0.0</cell></row><row><cell>CB</cell><cell>3</cell><cell>83.3</cell><cell>82.1</cell><cell>82.1</cell><cell>83.9</cell><cell>82.1?0.0</cell><cell>83.9?0.0</cell></row><row><cell>MultiRC</cell><cell>3</cell><cell>58.8</cell><cell>63.8</cell><cell>63.4</cell><cell>63.4</cell><cell>63.7?0.0</cell><cell>63.8?0.0</cell></row><row><cell>BoolQ</cell><cell>5</cell><cell>64.9</cell><cell>65.9</cell><cell>67.2</cell><cell>68.3</cell><cell>65.9?0.0</cell><cell>67.2?0.0</cell></row><row><cell>COPA</cell><cell>4</cell><cell>58.3</cell><cell>85.0</cell><cell>82.0</cell><cell>82.0</cell><cell>84.0?0.0</cell><cell>84.0?0.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Natural Language Inference</cell><cell></cell><cell></cell></row><row><cell>ANLI R1</cell><cell>5</cell><cell>34.6</cell><cell>37.6</cell><cell>36.1</cell><cell>36.8</cell><cell>37.4?1.0</cell><cell>37.8?0.2</cell></row><row><cell>ANLI R2</cell><cell>5</cell><cell>35.4</cell><cell>36.3</cell><cell>36.0</cell><cell>36.0</cell><cell>38.7?0.4</cell><cell>37.9?0.2</cell></row><row><cell>ANLI R3</cell><cell>5</cell><cell>37.0</cell><cell>39.0</cell><cell>38.4</cell><cell>38.4</cell><cell>39.6?0.9</cell><cell>40.9?0.5</cell></row><row><cell>StoryCloze</cell><cell>6</cell><cell>76.3</cell><cell>87.9</cell><cell>81.8</cell><cell>81.8</cell><cell>82.2?0.0</cell><cell>87.8?0.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Classification</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DBPedia</cell><cell>3</cell><cell>81.4</cell><cell>84.1</cell><cell>83.9</cell><cell>82.2</cell><cell>83.9?0.0</cell><cell>83.9?0.0</cell></row><row><cell>SST2</cell><cell>3</cell><cell>94.5</cell><cell>95.7</cell><cell>95.7</cell><cell>95.2</cell><cell>95.7?0.0</cell><cell>95.7?0.0</cell></row><row><cell>Amazon</cell><cell>3</cell><cell>67.0</cell><cell>68.6</cell><cell>68.6</cell><cell>67.3</cell><cell>68.6?0.0</cell><cell>68.6?0.0</cell></row><row><cell>AGNews</cell><cell>3</cell><cell>83.7</cell><cell>86.5</cell><cell>84.2</cell><cell>83.8</cell><cell>86.4?0.0</cell><cell>86.4?0.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>AMA Aggregation method ablation for the GPT-Neo-6B parameter model, as well as the number of prompt()-chains used for each task. For ReCoRD, and QA tasks (DROP, WebQs, RealTimeQA, NQ), we use 3 prompts each and use majority vote as our aggregation strategy reported in the (QA + WS) columns of Table1 and Table 3.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>We construct 1 answer generation task which measures the model's ability to generate candidate sentence completions given a context and portion of a statement (see Answer Generation). Here, a generated answer is marked as correct if the model generates 2 candidate answers. We construct 2 extraction tasks which evaluate the ability of the model to extract spans from a given context. The first, and easier task, tests the model's ability to extract an attribute value from a wikibio (see Extraction (Span)). The second, more difficult task, tests the model's ability to extract the sentence from the context that mentions a specified entity (see Extraction (Sentence)). For both tasks, we use the Text-F1 score introduced in SQuAD [<ref type="bibr" target="#b49">Rajpurkar et al., 2018]</ref>.</figDesc><table><row><cell>Categories ": -company -educational institution -artist -athlete -office holder Extraction: Extraction (Span)</cell></row><row><cell>-mean of tr anspor tation -building Input</cell></row><row><cell>-natural place Wiki Bio : -village name : robert king -animal article_title : robert king ( p h ot oj o ur na l is t ) -plant birth_place : memphis , tn , usa -album occupation : war correspondent ph o to j ou rn a li st filmmaker creative director art director birthname : -film robert whitfield king -written work birth_date : may 25 th</cell></row><row><cell>Example : A " journal " fits " Category ": Question : What is the birthname ?</cell></row><row><cell>Answer :</cell></row><row><cell>Output</cell></row><row><cell>written work</cell></row><row><cell>Answer Selection (sentence)</cell></row><row><cell>Input</cell></row><row><cell>Select one choice from the passage .</cell></row><row><cell>Select One Choice :</cell></row><row><cell>1. consumer electronics</cell></row><row><cell>2. Play Stations</cell></row><row><cell>3. cameras</cell></row><row><cell>Passage : Microsoft Corporation produces computer software , consumer electronics , and personal</cell></row><row><cell>computers . It is headquartered at the Microsoft Redmond campus located in Redmond , Washington</cell></row><row><cell>, United States .</cell></row><row><cell>Output</cell></row><row><cell>consumer electronics</cell></row><row><cell>Answer Generation: Answer Generation</cell></row><row><cell>Input</cell></row><row><cell>Output a list of unique alternatives for each example .</cell></row><row><cell>Example : Barrack Obama believes the :</cell></row><row><cell>List alternatives :</cell></row><row><cell>-best novel is Harry Potter</cell></row><row><cell>Output</cell></row><row><cell>-worst book is Harry Potter</cell></row><row><cell>-United States is great</cell></row></table><note><p>The passage " Passage " states : Microsoft Corporation sells : " Choice ":.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Now see I . A : I ' m intrigued by it , but I ' m not sure I want to go see it yet . B : Yeah , I don ' t think I want to see that either . Question : she wants to see that True , False , or Neither ? False A : Yeah . The radio doesn ' t really have much news sometimes . The stations I listen to are just mainly music . B : Yeah , I think you pretty much have to listen to all news station to get any news at all . A : Yeah . Do you think that TV is , uh , pretty accurate . Question : TV is pretty accurate True , False , or Neither ? Neither</figDesc><table><row><cell>B :</cell></row><row><cell>Model Output</cell></row><row><cell>true</cell></row><row><cell>G.7 CB</cell></row><row><cell>Description: Three-class textual entailement task. Wang et al. [2019]</cell></row><row><cell>Train Size: 250, Test Size: 56</cell></row><row><cell>CB Few Shot</cell></row><row><cell>Input</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Ontology classification dataset with 14 classes. Zhang et al. [2015] Train Size: 560000, Test Size: 70000 Garrison Cadet College Kohat -Garrison Cadet College Kohat is Situated in Kohat . Foundation stone was laid by the then Prime Minister of Islamic Republic of Pakistan Late Mohtarama Benazir Bhutto in 1992. Lieutenant General Arif Bangash Lieutenant General K . K Afridi Major General Shirendil Niazi and Colonel Idreesm ( founder Principal ) Dr . Category : Educational Institution Passage : River Ingrebourne -The River Ingrebourne is a tributary of the River Thames 27 miles (43.3 km ) in length . It is considered a strategic waterway in London forming part of the Blue Ribbon Network . It flows through the London Borough of Havering roughly from north to south joining the Thames at Rainham . Category : Natural Place Passage : USS Patrol No . 4 ( SP -8) -USS Patrol No . 4 ( SP -8) often rendered as USS Patrol #4 was an armed motorboat that served in the United States Navy as a patrol vessel from 1917 to 1919. Patrol No . 4 was built as a private motorboat of the same name in 1915 by Britt Brothers at Lynn Massachusetts . She was one of five motorboats built to the same design for private owners by Britt Brothers as part of the civilian Preparedness Movement program with an understanding that they would enter U . S . Category : Mean Of Transp ortati on Passage : TY KU -TY KU is an American alcoholic beverage company that specializes in sake and other spirits . The privately -held company was founded in 2004 and is headquartered in New York City New York . While based in New York TY KU ' s beverages are made in Japan through a joint venture with two sake breweries . Since 2011 TY KU ' s growth has extended its products into all 50 states . Category :Passage : As of the 2010 United States Census , there were 16 ,589 people , 6 ,548 households , and 4 ,643 families residing in the county . The population density was . There were 7 ,849 housing units at an average density of . The racial makeup of the county was 96.8% white , 0.7% black or African American , 0.6% American Indian , 0.2% Asian , 0.2% from other races , and 1.5% from two or more races . Those of Hispanic or Latino origin made up 0.6% of the population . In terms of ancestry , 23.4% were Germans , 22.3% were Americans , 13.6% were Irish people , and 11.0% were English people . The exact number of peasant deaths is unknown , and even the course of events are not clear , because the government , to hide the size of the massacre , ordered the destruction of all documents relating to the uprising . Historian Markus Bauer mentions a greatly underesti mated official figure of 419 deaths , while an unofficial figure , circulated by the press and widely accepted , of about 10 ,000 peasants killed , has never been proven to be true . The same figure of 419 deaths was mentioned by Ion I . C . Bratianu in the Romanian Parliament . The data available to the Prime Minister Dimitrie Sturdza indicated 421 deaths between 28 March and 5 April 1907. Likewise , about 112 were injured and 1 ,751 detained . Newspapers patronized by Constantin Mille , Adevarul and Dimineata , gave a figure of 12 ,000 -13 ,000</figDesc><table><row><cell>Pick the correct category for the passage .</cell></row><row><cell>Categories :</cell></row><row><cell>-company</cell></row><row><cell>-educational institution</cell></row><row><cell>-artist</cell></row><row><cell>-athlete</cell></row><row><cell>-office holder</cell></row><row><cell>-mean of tr anspor tation Question : How many percent of people were not Asian ?</cell></row><row><cell>-building Answer : unknown</cell></row><row><cell>-natural place</cell></row><row><cell>-village Passage : The health sector comprises 17 specialized hospitals and centers , 4 regional diagnostic</cell></row><row><cell>-animal and treatment centers , 9 district and 21 aimag general hospitals , 323 soum hospitals , 18</cell></row><row><cell>-plant feldsher posts , 233 family group practices , 536 private hospitals , and 57 drug supply</cell></row><row><cell>-album companies / pharmacies . In 2002 , the total number of health workers was 33 ,273 , of whom 6823</cell></row><row><cell>-film were doctors , 788 pharmacists , 7802 nurses , and 14 ,091 mid -level personnel . At present , there</cell></row><row><cell>-written work are 27.7 physicians and 75.7 hospital beds per 10 ,000 inhabitants .</cell></row><row><cell>Question : What profession had more health workers , doctors or nurses ?</cell></row><row><cell>Gold Output photographers followed her G.9 DBPedia Description: DBPedia Few Shot Input Answer : nurses Passage : Gold Output Passage :</cell></row><row><cell>company</cell></row><row><cell>DBPedia AMA prompt()-chain Example</cell></row><row><cell>question()</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>: Barack Hussein Obama is an American politician who served as the 44 th president of the United States from 2009 to 2017. A member of the Democratic Party , he was the first African -American president of the United States . Obama previously served as a U . S . senator from Illinois from 2005 to 2008 and as an Illinois state senator from 1997 to 2004. Obama was senator of the state of Illinois prior to becoming a US president . Islam Gadhafi , 38 , has never lived a day in which his father Moammar didn ' t rule Libya --as its undisputed leader inside the country and an enigmatic , controversial voice for the world . And yet , as the Libyan government faced a stiff popular uprising , it was Moammar Gadhafi ' s second eldest son --and not the Leader of the Revolution himself --who was first to talk to the nation about the unrest and detail a plan to address it . The speech , made early Monday on Libyan state television , does not mean that Saif Gadhafi has usurped power from his father : Senior U . S . officials said there ' s no indication the elder Gadhafi is losing his grip . Saif al -Islam Gadhafi , 38 , gives Libya ' s first public speech acknowledging unrest . There ' s been no public indication why he , and not his father Moammar , talked . Even while some may see the son as more open to change , there ' s little question that his loyalty remains first with Moammar and that his father has given little indication publicly that he ' s ready to let go and calls the shots . Context : The Beatles were an English rock band , formed in Liverpool in 1960 , that comprised John Lennon , Paul McCartney , George Harrison and Ringo Starr . They are regarded as the most influential band of all time and were integral to the development of 1960 s coun tercul ture and popular music ' s recognition as an art form . They were led by primary songwriters Lennon and McCartney . It is without a doubt that the Beatles were influential in rock and roll .</figDesc><table><row><cell>-Amy Schumer a week later</cell></row><row><cell>-James a week later</cell></row><row><cell>-Jimmy Mack a week later</cell></row><row><cell>-McNair a week later</cell></row><row><cell>-Miley Cyrus a week later</cell></row><row><cell>-Morgan a week later</cell></row><row><cell>-NBC a week later</cell></row><row><cell>-New Jersey a week later</cell></row><row><cell>-New Jersey Turnpike a week later</cell></row><row><cell>-Night Live a week later</cell></row><row><cell>-SNL a week later</cell></row><row><cell>-Season 41 a week later</cell></row><row><cell>-Tracy Morgan a week later</cell></row><row><cell>-Twitter a week later</cell></row><row><cell>Gold Output</cell></row><row><cell>Morgan , Tracy Morgan</cell></row><row><cell>ReCoRD AMA prompt()-chain Example</cell></row><row><cell>answer()</cell></row><row><cell>Complete the paragraph .</cell></row><row><cell>Model Choices</cell></row></table><note><p><p>Context</p>Context : ( CNN ) --Saif al -</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Dynamic question answering dataset that asks questions about current world facts.Kasai et al. [2022] Train Size: 90, Test Size: 187 RealTime QA Few Shot Input Answer the question given the articles . Article 1: Walmart is slashing prices on clothing and other products -CNN New York ( CNN Business ) Many shoppers have pulled back on buying clothing and other discretionary items as the highest inflation in four decades pinches their pocketbooks . Article 2: Retail slowdown : Target cuts vendor orders , slashes prices as it ... Associated Press NEW YORK . Article 3: Stores have too much stuff . That means discounts are coming | CNN ... New York ( CNN Business ) . Article 4: GM reports strong sales but says it ' s prepared for possible recession ... New York ( CNN Business ) . Article 5: Target is ramping up discounts . Here ' s why -CNN New York ( CNN Business ) . Question : Which major US retailer announced this week it is slashing prices on clothing and other products ? Answer : " Walmart " Article 1: Article 1: JetBlue announces a deal to buy Spirit Airlines . Fares could surge . Article 2: JetBlue -Spirit merger : Airlines have complaints over flights and fees Christopher Elliott Special to USA TODAY . Article 3: JetBlue announces a deal to buy Spirit Airlines | CNN Business The announcement comes a day after Spirit pulled the plug on a deal to merge with Frontier . Article 4: Spirit and Frontier pull plug on deal , setting stage for JetBlue to buy ... New York ( CNN Buiness ) . Article 5: Frontier Airlines , Spirit Airlines announce budget airline merger Budget airlines</figDesc><table><row><cell>-Amy Schumer a week later</cell></row><row><cell>-James a week later</cell></row><row><cell>-Jimmy Mack a week later</cell></row><row><cell>-McNair a week later</cell></row><row><cell>-Miley Cyrus a week later</cell></row><row><cell>-Morgan a week later</cell></row><row><cell>-NBC a week later</cell></row><row><cell>-New Jersey a week later</cell></row><row><cell>-New Jersey Turnpike a week later</cell></row><row><cell>-Night Live a week later</cell></row><row><cell>-SNL a week later</cell></row><row><cell>-Season 41 a week later</cell></row><row><cell>-Tracy Morgan a week later</cell></row><row><cell>-Twitter a week later</cell></row><row><cell>Model Output</cell></row><row><cell>Morgan , Tracy Morgan</cell></row><row><cell>G.15 RealTime QA</cell></row><row><cell>Description:</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>We draw inspiration from<ref type="bibr" target="#b8">Wu et al. [2022]</ref> and focus on task-agnostic and scalable prompt-chains.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>We do not use rank-classification scoring, which<ref type="bibr" target="#b0">Brown et al. [2020]</ref> and<ref type="bibr" target="#b4">Sanh et al. [2022]</ref> use to reduce task complexity, barring the tasks with explicit multiple-choice options (ReCORD, StoryCloze and COPA).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://github.com/realtimeqa/realtimeqa_public</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The computation required in this work was provided by <rs type="person">Together Computer</rs> (together.xyz). We are grateful to the Numbers Station (numbersstation.ai), Snorkel (snorkel.ai), <rs type="funder">Stanford Center for Research on Foundation Models</rs> (crfm.stanford.edu), and <rs type="funder">Stanford HAI</rs> (hai.stanford.edu) organizations for the resources that supported this work. We thank <rs type="person">Karan Goel</rs>, <rs type="person">Maya Varma</rs>, <rs type="person">Joel Johnson</rs>, <rs type="person">Kawin Ethayarajh</rs>, <rs type="person">Niladri Chatterji</rs>, <rs type="person">Alex Ratner</rs>, and <rs type="person">Rishi Bommasani</rs> for their helpful feedback and discussions. We gratefully acknowledge the support of <rs type="funder">DARPA</rs> under Nos. <rs type="grantNumber">FA86501827865</rs> (SDH) and FA86501827882 (ASED); <rs type="funder">NIH</rs> under No. <rs type="grantNumber">U54EB020405</rs> (Mobilize), <rs type="funder">NSF</rs> under Nos. <rs type="grantNumber">CCF1763315</rs> (<rs type="affiliation">Beyond Sparsity</rs>), CCF1563078 (Volume to Velocity), and 1937301 (RTML); <rs type="funder">ONR</rs> under No. <rs type="grantNumber">N000141712266</rs> (<rs type="affiliation">Unifying Weak Supervision</rs>); the <rs type="funder">Moore Foundation</rs>, <rs type="funder">NXP</rs>, <rs type="funder">Xilinx</rs>, <rs type="affiliation">LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF</rs>, <rs type="funder">Accenture</rs>, <rs type="funder">Ericsson</rs>, <rs type="funder">Qualcomm</rs>, <rs type="funder">Analog Devices</rs>, the <rs type="funder">Okawa Foundation, American Family Insurance</rs>, <rs type="person">Google Cloud</rs>, <rs type="affiliation">Swiss Re, Brown Institute for Media Innovation</rs>, <rs type="funder">Department of Defense (DoD)</rs> through the <rs type="funder">National Defense Science and Engineering Graduate Fellowship (NDSEG) Program</rs>, <rs type="funder">Fannie and John Hertz Foundation</rs>, <rs type="funder">National Science Foundation Graduate Research Fellowship Program</rs>, <rs type="funder">Texas Instruments</rs>, and members of the <rs type="institution">Stanford DAWN</rs> project: <rs type="affiliation">Teradata, Facebook, Google, Ant Financial, NEC, VMWare, and Infosys</rs>. SA is supported by a <rs type="grantName">Stanford Graduate Fellowship</rs>. LO is supported by an <rs type="funder">Intelligence Community Postdoctoral Fellowship</rs>. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of <rs type="affiliation">DARPA</rs>, <rs type="funder">NIH</rs>, <rs type="institution">ONR</rs>, or the <rs type="institution">U.S. Government</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_dAn3tnK">
					<idno type="grant-number">FA86501827865</idno>
				</org>
				<org type="funding" xml:id="_kW2bHQe">
					<idno type="grant-number">U54EB020405</idno>
				</org>
				<org type="funding" xml:id="_Jmatv6X">
					<idno type="grant-number">CCF1763315</idno>
				</org>
				<org type="funding" xml:id="_akqBb43">
					<idno type="grant-number">N000141712266</idno>
				</org>
				<org type="funding" xml:id="_yFzH7XU">
					<orgName type="grant-name">Stanford Graduate Fellowship</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summarize the passage .</head><p>Passage : China overtakes United States as top destination for foreign investment ( AFP ) . AFP -China overtook the United States as a top global destination for foreign direct investment ( FDI ) in 2003 while the Asia -Pacific region attracted more investment than any other developing region , a UN report said . Summarize : the passage " Passage ": The passage is about foreign direct investment .</p><p>Passage : Colangelo resigns as CEO of D -Backs . Jerry Colangelo has resigned his position as chief executive officer of the Arizona Diamondbacks , effective immediately , handing the reins of the organization to CEO Elect Jeff Moorad . Summarize : the passage " Passage ": The passage is about the Arizona Diamondbacks .</p><p>Passage : 3 injured in plant fire in Japan . TOKYO , Aug . 20 ( Xinhuanet ) --Fire broke out Friday at a tire plant belonging to Bridgestone Corp . in Amagi , western Fukuoka Prefecture of Japan , leaving 13 people injured . Summarize : the passage " Passage ": The passage is about a plant fire .</p><p>Passage : The Race is On : Second Private Team Sets Launch Date for Human Spaceflight ( SPACE . com ) .</p><p>SPACE . com -TORONTO , Canada --A second team of rocketeers competing for the #36;10 million Ansari X Prize , a contest for privately funded suborbital space flight , has officially announced the first launch date for its manned rocket . Summarize : the passage " Passage ":</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Output</head><p>The passage is about a rocket .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>answer()</head><p>Pick the correct category for the passage .</p><p>" Categories ": -World News -Sports -Business -Technology and Science Passage : China overtakes United States as top destination for foreign investment ( AFP ) . AFP -China overtook the United States as a top global destination for foreign direct investment ( FDI ) in 2003 while the Asia -Pacific region attracted more investment than any other developing region , a UN report said . Summary : The passage is about foreign direct investment . The summary " Summary " fits " Category ": Business Passage : Colangelo resigns as CEO of D -Backs . Jerry Colangelo has resigned his position as chief executive officer of the Arizona Diamondbacks , effective immediately , handing the reins of the organization to CEO Elect Jeff Moorad . Summary : The passage is the Arizona Diamondbacks . The summary " Summary " fits " Category ": Sports Passage : 3 injured in plant fire in Japan . TOKYO , Aug . 20 ( Xinhuanet ) --Fire broke out Friday at a tire plant belonging to Bridgestone Corp . in Amagi , western Fukuoka Prefecture of Japan , leaving 13 people injured . Summary : The passage is about a plant fire . The summary " Summary " fits " Category ": World News Passage : The Race is On : Second Private Team Sets Launch Date for Human Spaceflight ( SPACE . com ) .</p><p>SPACE . com -TORONTO , Canada --A second team of rocketeers competing for the #36;10 million Ansari X Prize , a contest for privately funded suborbital space flight , has officially announced the first launch date for its manned rocket . Summary : The passage is about a rocket . The summary " Summary " fits " Category ":</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold Output technology and science</head><p>Matter was a London music venue and nightclub that opened in September 2008 , after three years of planning . A 2 ,600 capacity live music venue and nightclub , it was the second project for owners Cameron Leslie and Keith Reilly , founders of the London club Fabric . Matter is the third venue to open at The O in south -east London . Question : The owners own more than one London club . True , False , or Neither ? True Whitechapel is a British television drama series produced by Carnival Films , in which detectives in London ' s Whitechapel district dealt with murders which replicated historical crimes . The first series was first broadcast in the UK on 2 February 2009 and depicted the search for a modern copycat killer replicating the murders of Jack the Ripper . Question : Some of the victims depicted in Whitechapel were women True , False , or Neither ? Neither Nannina de ' Medici (14 February 1448 -14 May 1493) , born Lucrezia de ' Medici , was the second daughter of Piero di Cosimo de ' Medici and Lucrezia Tornabuoni . She was thus the elder sister of Lorenzo de ' Medici . She married Bernardo Rucellai . Her father ' s name was Piero , so she is sometimes known as Lucrezia di Piero de ' Medici . Question : Nannina de ' Medici is sometimes known as Ivanka Trump True , False , or Neither ? False There is a little Shia community in El Salvador . There is an Islamic Library operated by the Shia community , named " Fatimah Az -Zahra ". They published the first Islamic magazine in Central America : " Revista Biblioteca Islamica ". Additionally , they are credited with providing the first and only Islamic library dedicated to spreading Islamic culture in the country . Question : The community is south of the United States . True , False , or Neither ? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold Output</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Output</head><p>Is the community south of the United States ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>answer()</head><p>Context : According to Biraben , the plague was present somewhere in Italy and affected 1 ,200 people . Question : Based on the context , Did the plague affect people in Europe ? Answer : yes , people in Italy , Europe Context : Policies aiming at controlling unemployment and in particular at reducing its inequality -associated effects support economic growth . Question : Based on the context , Is confidence a factor in increasing self -esteem ? Answer : unknown Context : The term " matter " is used throughout physics in a bewildering variety of contexts : for example , one refers to " condensed matter physics " , " elementary matter " , " partonic " matter , " dark " matter , " anti " -matter , " strange " matter , and " nuclear " matter . Question : Based on the context , Is anti -matter made of electrons ? Answer : Unknown Context : There is a little Shia community in El Salvador . There is an Islamic Library operated by the Shia community , named " Fatimah Az -Zahra ". They published the first Islamic magazine in Central America : " Revista Biblioteca Islamica ". Additionally , they are credited with providing the first and only Islamic library dedicated to spreading Islamic culture in the country . Question : Based on the context , Is the community south of the United States ? Answer :</p><p>Gold Output true</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.4 ANLI R3</head><p>Description: Adversarially mined natural language inference dataset from Wikipedia, News and other data sources. <ref type="bibr" target="#b34">Nie et al. [2020]</ref> Train Size: 100459, Test Size: 1200</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ANLI R3 Few Shot</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>And that means that the local law enforcement officials need help at the federal level . Programs like Project Exile where the federal government intensifies arresting people who illegally use guns . And we haven ' t done a very good job of that at the federal level recently . Question : There are only federal enforcement officials . True , False , or Neither ? False Scary Dream &lt; br &gt; Tom woke up in a cold sweat . He was shaking and scared . He realized he had just had a scary dream . Tom was too afraid to fall back asleep . Instead he stayed up all night . Question : Tom experienced a bad nightmare that kept him from sleeping . Statement : This headline leads to more information that is behind a paywall . Question :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Output</head><p>Does this headline lead to more information that is behind a paywall ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>answer()</head><p>Answer the question . If there is no evidence in the context , return " Unknown ".</p><p>Context : According to Biraben , the plague was present somewhere in Italy and affected 1 ,200 people . Question : Based on the context , Did the plague affect people in Europe ? Answer : yes , people in Italy , Europe Context : Policies aiming at controlling unemployment and in particular at reducing its inequality -associated effects support economic growth . Question : Based on the context , Is confidence a factor in increasing self -esteem ? Answer : unknown Context : The term " matter " is used throughout physics in a bewildering variety of contexts : for example , one refers to " condensed matter physics " , " elementary matter " , " partonic " matter , " dark " matter , " anti " -matter , " strange " matter , and " nuclear " matter . Context : Drinking in public --Drinking in public is legal in England and Wales --one may carry a drink from a public house down the street ( though it is preferred that the user requests a plastic glass to avoid danger of breakage and because the taking of the glass could be considered an offence of Theft as only the drink has been purchased ) , and one may purchase alcohol at an off -licence and immediately begin drinking it outside . Separately , one may drink on aeroplanes and on most National Rail train services , either purchasing alcohol onboard or consuming one ' s own . Question : is it legal to drink in public in london Answer : Yes Context : Harry Potter and the Escape from Gringotts --Harry Potter and the Escape from Gringotts is an indoor steel roller coaster at Universal Studios Florida , a theme park located within the Universal Orlando Resort . Similar to dark rides , the roller coaster utilizes special effects in a controlled -lighting environment and also employs motion -based 3 -D projection of both animation and live -action sequences to enhance the experience . The ride , which is themed to the Gringotts Wizarding Bank , became the flagship attraction for the expanded Wizarding World of Harry Potter when it opened on July 8 , 2014. Question : is harry potter and the escape from gringotts a roller coaster ride ? True or False ? Answer :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold Output</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>True</head><p>BoolQ AMA prompt()-chain Example</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>answer()</head><p>Provide the answer to the question from the passage .</p><p>Passage : When Judy and Jack went to school , they got in trouble with their teacher for being late . I didn ' t think it was very fair . Question : Did she think it was fair ? Answer : No Passage : If inflation is occurring , leading to higher prices for basic necessities such as gas by 2 dollars . Do you think that inflation is good for society ? Question : Is inflation good for society ? Answer : Maybe Passage : Put yourself out there . The more time you spend dating and socializing , the more likely you will find a boyfriend you like . Question : Does socializing help you find a boyfriend ? Answer : Yes Passage : Modify the arachnids , said the researchers . Change their bodies and conditions , and you could get fibres like glass , still monofilament , but with logarithmic progressions of possibilities of strength and flexibility , and the ability to resonate light -particles or sound -waves undistorted , scarcely weakened over thousands of miles . Who said the arachnids had to be totally organic ? Question : Did arachnids have to be totally organic ? Answer :</p><p>Gold Output false G.8 COPA Description: Casual reasoning dataset where task is to select the alternative that more plausibly has a causal relation with the premise. <ref type="bibr" target="#b30">Wang et al. [2019]</ref> Train Size: 400, Test Size: 100</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COPA Few Shot</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Pick the more likely continuation to the following sentence .</p><p>Context : The truck crashed into the motorcycle on the bridge so The motorcyclist died .</p><p>Context : The customer came into the boutique because The window display caught her eye .</p><p>Context : The print on the brochure was tiny so The man put his glasses on .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context : The</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Choices</head><p>-woman became famous so -photographers followed her Passage : TY KU -TY KU is an American alcoholic beverage company that specializes in sake and other spirits . The privately -held company was founded in 2004 and is headquartered in New York City New York . While based in New York TY KU ' s beverages are made in Japan through a joint venture with two sake breweries . Since 2011 TY KU ' s growth has extended its products into all 50 states . Summarize : the passage " Passage ":</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold Output</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Output</head><p>The passage is about a company .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>answer()</head><p>Pick one category for the following text . Ferry in Scotland that capsized during a rescue attempt with the loss of her entire crew of eight men . The Mona was built in 1935 and in her time saved 118 lives . Summary : The passage is about a lifeboat . The summary " Summary " fits " Category ": mean of tra nsport ation Passage : Sayonara mo Ienakatta Natsu -Sayonara mo Ienakatta Natsu is an album by Mikuni Shimokawa released on July 4 2007 by Pony Canyon . This album consists of eleven songs ; several new songs and some songs which were previously released as singles . Summary : The passage is about a album . The summary " Summary " fits " Category ": album Passage : TY KU -TY KU is an American alcoholic beverage company that specializes in sake and other spirits . The privately -held company was founded in 2004 and is headquartered in New York City New York . While based in New York TY KU ' s beverages are made in Japan through a joint venture with two sake breweries . Since 2011 TY KU ' s growth has extended its products into all 50 states . Summary : The passage is about a company . The summary " Summary " fits " Category ":</p><p>Gold Output company G.10 DROP Description: A reading comprehension benchmark requiring discrete reasoning over paragraphs. <ref type="bibr" target="#b40">Dua et al. [2019]</ref> Train Size: 77409, Test Size: 9536</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DROP Few Shot</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Context : According to Biraben , the plague was present somewhere in Europe in every year between 1346 and 1671 Question : Where was the plague present ? Answer : somewhere in Europe Context : Policies aiming at controlling unemployment and in particular at reducing its inequality -associated effects support economic growth . Question : What ' s one factor in increasing self -esteem ? Answer : Unknown Context : The term " matter " is used throughout physics in a bewildering variety of contexts : for example , one refers to " condensed matter physics " , " elementary matter " , " partonic " matter , " dark " matter , " anti " -matter , " strange " matter , and " nuclear " matter . Question : What is another name for anti -matter ? Answer : Unknown nearly everyone in the White House told us , they immediately knew it was not an accident . The Secret Service initiated a number of security enhancements around the White House complex . The officials who issued these orders did not know that there were additional hijacked aircraft , or that one such aircraft was en route to Washington . These measures were precautionary steps taken because of the strikes in New York . The FAA and White House Teleco nf e re nc e s . The FAA , the White House , and the Defense Department each initiated a multiagency tele confer ence before 9:30. Because none of these teleconferences -at least before 10:00 -included ... Question : Based on the previous passage , To what did the CIA and FAA begin participating in at 9:40? Is " Coffee hour " a correct answer ? Answer : No Passage : What causes a change in motion ? The application of a force . Any time an object changes motion , a force has been applied . In what ways can this happen ? Force can cause an object at rest to start moving . Forces can cause objects to speed up or slow down . Forces can cause a moving object to stop . Forces can also cause a change in direction . In short , forces cause changes in motion . The moving object may change its speed , its direction , or both . We know that changes in motion require a force . We know that the size of the force determines the change in motion . How much an objects motion changes when a force is applied depends on two things . It depends on the strength of the force . It also depends on the objects mass . Think about some simple tasks you may regularly do . You may pick up a baseball . This requires only a very small force . Question : Based on the previous passage , Would the mass of a baseball affect how much force you have to use to pick it up ? Is " Yes " a correct answer ? Answer :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold Output yes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MultiRC AMA prompt()-chain Example</head><p>Passage : Sara wanted to play on a baseball team . She had never tried to swing a bat and hit a baseball before . Her Dad gave her a bat and together they went to the park to practice . Sara wondered if she could hit a ball . She wasn ' t sure if she would be any good . She really wanted to play on a team and wear a real uniform . She couldn ' t wait to get to the park and test out her bat . When Sara and her Dad reached the park , Sara grabbed the bat and stood a few steps away from her Dad . Sara waited as her Dad pitched the ball to her . Her heart was beating fast . She missed the first few pitches . She felt like quitting but kept trying . Soon she was hitting the ball very far . She was very happy and she couldn ' t wait to sign up for a real team . Her Dad was very proud of her for not giving up . Question : Based on the previous passage , Who pitched the ball to Sara and where did it occur ? Is " Her dad did in the park " a correct answer ? Answer : yes Passage : The Vice President stated that he called the President to discuss the rules of engagement for the CAP . He recalled feeling that it did no good to establish the CAP unless the pilots had instructions on whether they were authorized to shoot if the plane would not divert . He said the President signed off on that concept . The President said he remembered such a conversation , and that it reminded him of when he had been an interceptor pilot . , become expelled for the publication of an off -campus underground paper . As a result , a philosophy professor , Dr . Jonathon Barnett , resigns his teaching position and decides to become an advocate for the c ounter cultur e youth movement and , specifically , the use of LSD . The hippies of the Haight -Ashbury district first see him as a hero and then as something even more . Dr . Barnett even makes an appearance on the Joe Pyne TV show to voice his support of the hippie community and the use of LSD . One scheming young man sees the opportunity to build Dr . Barnett as the head of a cult centered around the use of LSD . He hopes to earn profit from the users , Dr . Barnett ' s speeches known as '' happenings , '' and their lifestyles . At a massive LSD -fueled dance , Patricia begins to have a bad trip Which leads to an argument between her and Pat , ultimately splitting the couple up ... Question : Based on the previous passage , Why did Dr . Barnett resign from teaching ? Is " Patricia expulsion " a correct answer ? Answer : yes Passage : I wondered if that were my case --if I rode out for honour , and not for the pure pleasure of the riding . And I marvelled more to see the two of us , both lovers of one lady and eager rivals , burying for the nonce our feuds , and with the same hope serving the same cause . We slept the night at Aird ' s store , and early the next morning found Ringan . A new Ringan indeed , as unlike the buccaneer I knew as he was unlike the Quaker . He was now the gentleman of Breadalbane , dressed for the part with all the care of an exquisite . He rode a noble roan , in his Spanish ... Question : Based on the previous passage , Who is described as both buccaneer and cavalier ? Is " Quaker " a correct answer ? Answer : no Passage : What causes a change in motion ? The application of a force . Any time an object changes motion , a force has been applied . In what ways can this happen ? Force can cause an object at rest to start moving . Forces can cause objects to speed up or slow down . Forces can cause a moving object to stop . Forces can also cause a change in direction . In short , forces cause changes in motion . The moving object may change its speed , its direction , or both . We know that changes in motion require a force . We know that the size of the force determines the change in motion . How much an objects motion changes when a force is applied depends on two things . It depends on the strength of the force . It also depends on the objects mass . Think about some simple tasks you may regularly do . You may pick up a baseball . This requires only a very small force . Question : Based on the previous passage , Would the mass of a baseball affect how much force you have to use to pick it up ? Is " Yes " a correct answer ? Answer : yes G.12 Natural Questions (NQ) Description: Open-domain question answering that contains questions from real users. <ref type="bibr">Kwiatkowski</ref>  Frye says , that he ( a homeopathy expert ) and Iris Bell recently studied homeopathic treatment of fibromyalgia . A new analysis -comparing published studies of homeopathic drugs to matched , randomly selected studies of medical drugs -suggests that these apparent homeopathic drug effects are merely placebo effects . Question : What really irks Frye and other doctors of homeopathy , however , is that homeopathic remedies are not supposed to be used like medical drugs . True or False ? False Security forces were on high alert after an election campaign in which more than 1 ,000 people , including seven election candidates , have been killed . Question : Security forces were on high alert after a campaign marred by violence . True or False ? Statement : Security forces were on high alert after a campaign marred by violence . Question :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold Output</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Output</head><p>Were security forces on high alert after a campaign marred by violence ? answer() Answer the question . If there is no evidence in the context , return " Unknown ".</p><p>Context : Jenna ' s 10 th birthday was yesterday evening and at least 10 of her friends attended the party . Question : Did 10 friends attend Jenna ' s party ? Answer : Unknown , at least 10 Context : The bullies attacked John when he was walking through the elementary school parking lot and then got sent to the teacher ' s office . Question : Did the bullies attack John in the teacher ' s office ? Answer : No , parking lot Context : WISS discovered a new monkey disease in a remote tribe in the Amazon rainforrest last week . It was highly contagious . Question : Did WISS discover a new disease ? Answer : Yes , new monkey disease Context : Security forces were on high alert after an election campaign in which more than 1 ,000 people , including seven election candidates , have been killed . Question : Were security forces on high alert after a campaign marred by violence ? Answer :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold Output</head><p>True G.14 ReCoRD Description: Reading comprehension dataset which requires commonsense reasoning. <ref type="bibr" target="#b30">Wang et al. [2019]</ref> Train Size: 100730, Test Size: 10000 Question : What is the capital city of Japan ? Answer : Tokyo Article : 5 things to know for June 13: Gun laws , January 6 , Covid , White ... If your day doesn ' t start until you ' re up to speed on the latest headlines , then let us introduce you to your new favorite morning fix . Sign up here for the '5 Things ' newsletter . ( CNN ) Just imagine what a relief it would be if you could use the same charging cable for all of your devices --your phone , laptop , earbuds , camera , tablet , portable speaker , etc . Well , in a huge step to reduce cable clutter and waste , European regulators say that Apple and other smartphone makers will be required to support a single common charging standard for all mobile devices as early as the fall of 2024. But Apple hates the idea ( shocker ) because that means about a billion devices will become obsolete . Article : 5 things to know for March 11: Ukraine , Pandemic , MLB , North ... If your day doesn ' t start until you ' re up to speed on the latest headlines , then let us introduce you to your new favorite morning fix . Sign up here for the '5 Things ' newsletter . ( CNN ) America , the " land of the free ," is getting quite costly . Prices for gas , food and housing --which are all necessary expenses --are spiking across the country . Gas prices have risen 38% over the past year , and rising prices in pandemic -related sectors , such as travel and dining , are also expected as the US recovers from the Omicron wave of Covid -19. Here ' s what you need to know to Get Up to Speed and On with Your Day . Article : Wi -Charge / consists of a transmitter and a receiver . Transmitter connects to a standard power outlet and converts electricity into infrared laser beam . Receivers use a miniature photo -voltaic cell to convert transmitted light into electrical power . Receivers can be embedded into a device or connected into an existing charging port . The transmitter automatically identifies chargeable receivers and start charging . Several devices can charge at the same time . According to Wi -Charge it can deliver several watts of power to a device at several meters away . The core technology is based on a " distributed laser resonator " which is formed by the re tr o re fl e ct or s within the Article : Mobile broadband / added in 2005. CDPD , CDMA2000 EV -DO , and MBWA are no longer being actively developed . In 2011 , 90% of the world ' s population lived in areas with 2 G coverage , while 45% lived in areas with 2 G and 3 G coverage , and 5% lived in areas with 4 G coverage . By 2017 more than 90% of the world ' s population is expected to have 2 G coverage , 85% is expected to have 3 G coverage , and 50% will have 4 G coverage . A barrier to mobile broadband use is the coverage provided by the mobile service networks . This may mean no mobile network or that service is limited to Text : in terms of execution this movie is careless and unfocused . Sentiment : negative Text : ... a pretentious and ultimately empty examination of a sick and evil woman . Sentiment : negative Text : the film 's plot may be shallow , but you ' ve never seen the deep like you see it in these harrowing surf shots . Sentiment : positive Text : a gob of drivel so sickly sweet , even the eager consumers of moore 's pasteurized ditties will retch it up like rancid creme brulee . Sentiment : Peter was excited to go to the Sanders rally in New Hampshire . As Peter entered the arena it was full of thousands of people . When Peter saw Bernie he cheered as loudly as possible . He felt thrilled to be there . A ) He couldn ' t wait to vote for him . B ) He was a staunch republican . Answer : He couldn ' t wait to vote for him . My friends all love to go to the club to dance . They think it ' s a lot of fun and always invite . I finally decided to tag along last Saturday . I danced terribly and broke a friend ' s toe . A ) My friends decided to keep inviting me out as I am so much fun . B ) The next weekend , I was asked to please stay home . Answer :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold Output</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Choices</head><p>-My friends decided to keep inviting me out as I am so much fun .</p><p>-The next weekend , I was asked to please stay home .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold Output</head><p>The next weekend , I was asked to please stay home . Passage : My roommate was sick . She stayed home from work and school . She slept all day long and by the end of the day , she was feeling better . Question : Did the rest help her ? Answer : Yes , she slept and felt better .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Story</head><p>Passage : Andy had always wanted a big kids bike . When he turned six Year ' s old he asked for a bike for his birthday . He did not know how to ride a bike . On Andy ' s birthday his mother gave him a bike . Question : Did he cry all night ? Answer : No , Andy was happy because he got a bike .</p><p>Passage : My friends all love to go to the club to dance . They think it ' s a lot of fun and always invite . I finally decided to tag along last Saturday . I danced terribly and broke a friend ' s toe . Question : Did I stay home the next weekend ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Choices</head><p>-My friends decided to keep inviting me out as I am so much fun .</p><p>-The next weekend , I was asked to please stay home .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold Output</head><p>The next weekend , I was asked to please stay home .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.18 WSC</head><p>Description: Task that requires readining a sentence with a pronoun and selecting the referent of that pronoun from a list of choices. <ref type="bibr" target="#b30">Wang et al. [2019]</ref> Train Size: 554, Test Size: 104</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WSC Few Shot</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Passage : Mark was close to Mr . Singer 's heels . He heard him calling for the captain , promising him , in the jargon everyone talked that night , that not one thing should be damaged on the ship except only the ammunition , but the captain and all " his " crew had best stay in the cabin until the work was over . Question : In the passage above , does the pronoun " his " refer to Mark ? Answer : No Passage : Tom gave Ralph a lift to school so " he " wouldn ' t have to walk . Question : In the passage above , does the pronoun " he " refer to Ralph ? Answer : Yes Passage : This book introduced Shakespeare to Ovid ; it was a major influence on " his " writing . Question : In the passage above , does the pronoun " his " refer to Shakespeare ? Answer : Yes Passage : The large ball crashed right through the table because " it " was made of styrofoam . Question : In the passage above , does the pronoun " it " refer to the table ? Answer : Yes WSC AMA prompt()-chain Example question() Extract the phrase containing the pronoun .</p><p>Passage : Jane ' s mom went to the shop to buy Jane a backpack for " her " first day of kindergarten . Extract : phrase containing " her ": " her " first day Passage : The musicians performed in the park and the crowd loved " them ". The crowd cheered for them . Extract : phrase containing " them ": crowd loved " them " Passage : Jeff gave his son some money because " he " wanted to buy lunch . Extract : phrase containing " he ": " he " wanted to buy Passage : The large ball crashed right through the table because " it " was made of styrofoam . Extract : phrase containing " it ": </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Output</head><p>John Kasich is the current governor of Ohio .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>answer()</head><p>Answer the question . In " She heard the sound of voices in the hall ." , synonyms for the word " sound " are : -noise</p><p>In " Enter the secret code ." , synonyms for the word " code " are : -password</p><p>In " She acted in a play on Broadway " , synonyms for the word " play " are : -show</p><p>In " An emerging professional class ." , synonyms for the word " ' class '" are :</p><p>Model Output</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">On the opportunities and risks of foundation models</title>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simran</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><surname>Sydney Von Arx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeannette</forename><surname>Michael S Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><surname>Brunskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Calibrate before use: Improving few-shot performance of language models</title>
		<author>
			<persName><forename type="first">Tony</forename><forename type="middle">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09690v2</idno>
		<idno>arXiv:2104.08315</idno>
		<ptr target="https://arxiv.org/pdf/2102.09690.pdf" />
	</analytic>
	<monogr>
		<title level="m">Surface form competition: Why the highest probability answer isn&apos;t always right</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.02155</idno>
		<title level="m">Training language models to follow instructions with human feedback</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multitask prompted training enables zero-shot task generalization</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Raja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Chain of thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11903</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Can language models learn from explanations in context?</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">K</forename><surname>Lampinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishita</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Stephanie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kory</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">Henry</forename><surname>Mathewson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonia</forename><surname>Tessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">L</forename><surname>Crwswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><forename type="middle">X</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Hill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Reframing instructional prompts to gptk&apos;s language</title>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.07830</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts</title>
		<author>
			<persName><forename type="first">Tongshuang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carrie</forename><forename type="middle">Jun</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI Conference on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Maieutic prompting: Logically consistent reasoning with recursive explanations</title>
		<author>
			<persName><forename type="first">Jaehun</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianhui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faeze</forename><surname>Brahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Le Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2205.11822" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">How can we know what language models know? Transactions of the</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Araki Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">It&apos;s not just size that matters: Small language models are also few-shot learners</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07118v2</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Snorkel: Rapid training data creation with weak supervision</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Ehrenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Fries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
		<idno type="DOI">10.14778/3157794.3157797</idno>
		<ptr target="https://doi.org/10.14778%2F3157794.3157797" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2017-11">nov 2017</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="269" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning dependency structures for weak supervision models</title>
		<author>
			<persName><forename type="first">Paroma</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Re</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v97/varma19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019-06">Jun 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow</title>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5297715</idno>
		<ptr target="https://doi.org/10.5281/zenodo.5297715" />
		<imprint>
			<date type="published" when="2021-03">March 2021</date>
		</imprint>
	</monogr>
	<note>If you use this software, please cite it using these metadata</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Opt: Open pre-trained transformer language models</title>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjali</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2205.01068" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Avanika</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurel</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.09911</idno>
		<title level="m">Can foundation models wrangle your data?</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Can foundation models help us achieve perfect secrecy?</title>
		<author>
			<persName><forename type="first">Simran</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mcclandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">Scaling laws for neural language models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m">Scaling language modeling with pathways</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Benchmarking generalization via in-context instructions on 1,600+ language tasks</title>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pegah</forename><surname>Alipoormolabashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeganeh</forename><surname>Kordi</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01652</idno>
		<title level="m">Finetuned language models are zero-shot learners</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Is a question decomposition unit all we need?</title>
		<author>
			<persName><forename type="first">Pruthvi</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2205.12538" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Antonia</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murray</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.09712</idno>
		<title level="m">Selection-inference: Exploiting large language models for interpretable logical reasoning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">What makes good incontext examples for gpt-3?</title>
		<author>
			<persName><forename type="first">Jiachang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Self-consistency improves chain of thought reasoning in language models</title>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Data programming: Creating large training sets, quickly</title>
		<author>
			<persName><forename type="first">Christopher M De</forename><surname>Alexander J Ratner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Selsam</surname></persName>
		</author>
		<author>
			<persName><surname>R?</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2016/file/6709" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
	<note>e8d64a5f47269ed5cea9f625f7ab-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Training complex models with multi-task weak supervision</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Braden</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Dunnmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shreyash</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1810.02840" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast and three-rious: Speeding up weak supervision with triplet methods</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayee</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Hooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kayvon</forename><surname>Fatahalian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Re</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v119/fu20a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Hal</forename><surname>Daum?</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Iii</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</editor>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07">Jul 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">A</forename><surname>Fries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Braden</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.02318v1</idno>
		<title level="m">Language models in the loop: Incorporating prompting into weak supervision</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">SuperGLUE: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno>1905.00537</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Training compute-optimal large language models</title>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelia</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Sifre</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2203.15556" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Lsdsem 2017 shared task: The story cloze test</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2206.07682" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourselevel Semantics</title>
		<editor>
			<persName><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">James</forename><surname>Allen</surname></persName>
		</editor>
		<meeting>the 2nd Workshop on Linking Models of Lexical, Sentential and Discourselevel Semantics</meeting>
		<imprint>
			<date type="published" when="2017">2022. 2017</date>
			<biblScope unit="page" from="46" to="51" />
		</imprint>
	</monogr>
	<note>Emergent abilities of large language models</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adversarial nli: A new benchmark for natural language understanding</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D13-1170" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10">October 2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 25th international conference on world wide web</title>
		<meeting>the 25th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoichi</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Le Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.13332</idno>
		<title level="m">Realtime qa: What&apos;s the answer right now? arXiv preprint</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Natural questions: a benchmark for question answering research</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><forename type="middle">N</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D13-1160" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10">October 2013</date>
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs</title>
		<author>
			<persName><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Webgpt: Browser-assisted question-answering with human feedback</title>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchir</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyna</forename><surname>Eloundou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Button</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno>CoRR, abs/2112.09332</idno>
		<ptr target="https://arxiv.org/abs/2112.09332" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR, abs/1910.10683</idno>
		<ptr target="http://arxiv.org/abs/1910.10683" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Promptsource: An integrated development environment and repository for natural language prompts</title>
		<author>
			<persName><forename type="first">H</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Xin</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nihal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abheesht</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taewoon</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Saiful Bari</surname></persName>
		</author>
		<author>
			<persName><surname>Fevry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.01279</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Ethical and social risks of harm from language models</title>
		<author>
			<persName><forename type="first">Laura</forename><surname>Weidinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maribeth</forename><surname>Rauh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conor</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myra</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Borja</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atoosa</forename><surname>Kasirzadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.04359</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName><surname>Huggingface</surname></persName>
		</author>
		<ptr target="https://huggingface.co/models" />
		<imprint>
			<date type="published" when="2021-11">Nov 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<ptr target="https://openai.com/api/" />
		<imprint>
			<date type="published" when="2021-11">Nov 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Robust principal component analysis?</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><forename type="middle">J</forename><surname>Cand?s</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Wright</surname></persName>
		</author>
		<idno type="DOI">10.1145/1970392.1970395</idno>
		<ptr target="https://doi.org/10.1145/1970392.1970395" />
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<idno type="ISSN">0004-5411</idno>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2011-06">jun 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/W04-1013" />
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004-07">July 2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Know what you don&apos;t know: Unanswerable questions for squad</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Boolq: Exploring the surprising difficulty of natural yes/no questions</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2924" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The commitmentbank: Investigating projection in naturally occurring discourse</title>
		<author>
			<persName><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandy</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judith</forename><surname>Tonhauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of Sinn und Bedeutung</title>
		<meeting>Sinn und Bedeutung</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="107" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Choice of plausible alternatives: An evaluation of commonsense causal reasoning</title>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Roemmele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cosmin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Bejan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">S</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI spring symposium: logical formalizations of commonsense reasoning</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="90" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Looking beyond the surface: A challenge set for reading comprehension over multiple sentences</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="252" to="262" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
