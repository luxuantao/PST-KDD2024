<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond Grids: Learning Graph Representations for Visual Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yin</forename><surname>Li</surname></persName>
							<email>yin.li@wisc.edu</email>
						</author>
						<author>
							<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
							<email>abhinavg@cs.cmu.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Biostatistics &amp; Medical Informatics Department of Computer Sciences</orgName>
								<orgName type="institution">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">The Robotics Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Beyond Grids: Learning Graph Representations for Visual Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">47786B667859EE65E1E3437DE1479001</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose learning graph representations from 2D feature maps for visual recognition. Our method draws inspiration from region based recognition, and learns to transform a 2D image into a graph structure. The vertices of the graph define clusters of pixels ("regions"), and the edges measure the similarity between these clusters in a feature space. Our method further learns to propagate information across all vertices on the graph, and is able to project the learned graph representation back into 2D grids. Our graph representation facilitates reasoning beyond regular grids and can capture long range dependencies among regions. We demonstrate that our model can be trained from end-to-end, and is easily integrated into existing networks. Finally, we evaluate our method on three challenging recognition tasks: semantic segmentation, object detection and object instance segmentation. For all tasks, our method outperforms state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep convolutional networks have been tremendously successful for visual recognition <ref type="bibr" target="#b0">[1]</ref>. These deep models stack many local operations of convolution and pooling. The assumption is that this stacking will not only provide a strong model for local patterns, but also create a large receptive field to capture long range dependencies, e.g., contextual relations between an object and other elements of the scene. However, this approach for modeling context is highly inefficient. A recent study <ref type="bibr" target="#b1">[2]</ref> showed that even after hundreds of convolutions, the effective receptive field of a network's units is severely limited. Such a model may fail to incorporate global context beyond local regions.</p><p>Instead of the "deep stacking", one appealing idea is using image regions for context reasoning and visual recognition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. This paradigm builds on the theory of perceptual organization, and starts by grouping pixels into a small set of coherent regions. Recognition and context modeling are often postulated as an inference problem on a graph structure <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>-with regions as vertices and the similarity between regions as edges. This graph thus encodes dependencies between regions. These dependencies are of much longer range than those are captured by local convolutions.</p><p>Inspired by region based recognition, we propose a novel approach for capturing long range dependencies using deep networks. Our key idea is to move beyond regular grids, and learn a graph representation for a 2D input image or feature map. This graph has its vertices defining clusters of pixels ("regions"), and its edges measuring the similarity between these clusters in a feature space. Our method further learns to propagate messages across all vertices on this graph, making it possible to share global information in a single operation. Finally, our method is able to project the learned graph representation back into 2D grids, and thus is fully compatible with existing networks. Specifically, our method consists of Graph Projection, Graph Convolution and Graph Re-projection. Graph projection turns a 2D feature map into a graph, where pixels with similar features are assigned to the same vertex. It further encodes features for each vertex and computes an adjacency matrix for each sample. Graph convolution makes use of convolutions on a graph structure <ref type="bibr" target="#b10">[11]</ref>, and updates vertex features based on the adjacency matrix. Finally, graph re-projection interpolates the vertex features into a 2D feature map, by reverting the pixel-to-vertex assignment from the projection step.</p><p>We evaluate our method on several challenging visual recognition tasks, including semantic segmentation, object detection and object instance segmentation. Our method consistently improves state-of-the-art methods. For semantic segmentation, our method improves a baseline fully convolutional network <ref type="bibr" target="#b11">[12]</ref> by ∼7%. And our results slightly outperform the state-of-the-art context modeling approaches <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. For object detection and instance segmentation, our method improves the strong baseline of Mask RCNN <ref type="bibr" target="#b14">[15]</ref> by ∼1%. Note that a 1% improvement is significant on COCO (even doubling the number of layers provides 1-2% improvement). More importantly, we believe that our method offers a new perspective in designing deep models for visual recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Major progress has been made for visual recognition with the development of deep models. Deep networks have been widely used for image classification <ref type="bibr" target="#b0">[1]</ref>, semantic segmentation <ref type="bibr" target="#b11">[12]</ref>, object detection <ref type="bibr" target="#b15">[16]</ref> and instance segmentation <ref type="bibr" target="#b14">[15]</ref>. However, even after hundreds of convolution operations, these network may fail to capture long range context in the input image <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>Several recent works have thus developed deep architectures for modeling visual context. For example, dilated convolutions are attached to deep networks to increase the size of their receptive fields <ref type="bibr" target="#b16">[17]</ref>. A global context vector, pooled from all spatial positions, can be concatenated to local features <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b12">13]</ref>. These new features thus encode both global context and local appearance. Moreover, local features across different scales can be fused to encode global context <ref type="bibr" target="#b18">[19]</ref>. However, all previous methods still reside in a regular 2D feature map with the exception of <ref type="bibr" target="#b19">[20]</ref>. The non-local operation in <ref type="bibr" target="#b19">[20]</ref> constructed a densely connected graph with pairwise edges between all pixels. Therefore, their method is computational heavy for high resolution feature maps, and is less desirable for tasks like semantic segmentation. Our methods differs from these approaches by moving beyond regular grids and learning an efficient graph representation with a small number of vertices.</p><p>Our method is inspired by region based recognition. This idea can date back to Gestalt school of visual perception. In this setting, recognition is posed as labeling image regions. Examples include segmentation <ref type="bibr" target="#b20">[21]</ref>, object recognition <ref type="bibr" target="#b2">[3]</ref>, object detection <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> and scene geometry estimation <ref type="bibr" target="#b23">[24]</ref>. Several works addressed context reasoning among regions. Context can be encoded via a decomposition of regions <ref type="bibr" target="#b5">[6]</ref>, or via features from neighborhood regions <ref type="bibr" target="#b6">[7]</ref>. Our graph representation resembles the key idea of a region graph in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8]</ref>, where vertices are regions and edges encode relationships between regions. While previous approaches did not consider deep models, our model embeds a region graph in a deep network. More recently, region based recognition has been revisited in deep learning <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. Nonetheless, these methods considered grouping as a pre-processing step, and did not learn a graph representation as our method. In contrast, our method provides a novel deep model for learning graph representations of 2D visual data Furthermore, our method is related to learning deep models on graph structure <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29]</ref>. Specifically, graph convolutional networks <ref type="bibr" target="#b10">[11]</ref> are used to propagate information on our learned graph. However, our method focuses on learning graph representations rather than developing message passing methods on the graph. Finally, our graph projection step draws inspirations from nonlinear feature encoding methods, such as VLAD and Fisher Vectors <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>. These methods have been visited in the context of deep models <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b13">14]</ref>. However, previous methods focused on global encoding of local features, and did not consider the case of a graph representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this section, we present our method on learning graph representations for visual recognition. We start with an overview of our key ideas, followed by a detailed derivation of the proposed graph convolutional unit. Finally, we discuss the learning of our method and present approaches for incorporating our model into existing networks for recognition tasks. Figure <ref type="figure">1</ref>: Overview of our approach. Our Graph Convolutional Unit (GCU) projects a 2D feature map into a sample-dependent graph structure by assigning pixels to the vertices of the graph. GCU then passes information along the edges of graph and update the vertex features. Finally, these new vertex features are projected back into 2D grids based on the pixel-to-vertex assignment. GCU learns to reason beyond regular grids, captures long range dependencies across the 2D plane, and can be easily integrated into existing networks for recognition tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>For simplicity, we consider an input 2D feature map X of size H × W from a single sample. Our method can easily extend to batch size ≥ 1 or 3D feature maps (e.g., videos). X are the intermediate responses of a deep convolutional network. x ij ∈ R d thus indexes the d dimensional feature at pixel (i, j). Our proposed Graph Convolutional Units (GCU) consists of three operations.</p><p>• Graph Projection G proj . G proj projects X into a graph G = (V, E) with its vertices as V and edges as E. Specifically, G proj assigns pixels with similar features to the same vertex. This assignment is soft and likely groups pixels into coherent regions. Pixel features are further aggregated within each vertex, and form the vertex features Z ∈ R d×|V| for graph G.</p><p>Based on Z, we measure the distance between vertices, and compute the adjacency matrix. Moreover, we store the pixel-to-vertex assignments and will use them to re-project the graph back to 2D grids. • Graph Convolution G conv . G conv performs convolutions on the graph G by propagating features Z along the edges of the graph. G conv makes use of graph convolutions as <ref type="bibr" target="#b10">[11]</ref> and can stack multiple convolutions with nonlinear activation functions. When G is densely connected, G conv has a receptive field of all vertices on the graph, and thus is able to capture the global context of the input. G conv outputs the transformed vertex features Z ∈ R |V|× d.</p><p>• Graph Reprojection G reproj . G reproj maps the new features Z back into the 2D grid of size (H × W ). This is done by "inverting" the assignments from the projection step. The output X of G reproj will be a 2D feature map with dimension d at each position (i, j). Thus, X is compatible with a regular convolutional neural network.</p><p>Figure <ref type="figure">1</ref> presents an overview of our method. In a nutshell, our GCU can be expressed as</p><formula xml:id="formula_0">X = GCU(x) = Greproj(Gconv(Gproj(X))).<label>(1)</label></formula><p>It is more intuitive to consider our method in terms of pixels and regions. In GCU, "pixels" are assigned to vertices based on their feature vectors. Thus, each vertex defines a cluster of pixels, i.e., a "region" in the image. Each region will re-compute its feature by pooling over all its pixels. And the similarity between regions is estimated based on the pooled region features, and thus define the structure of a region graph. Inference can then be performed on the graph by passing messages between regions and along the edges that connect them. This inference will update the feature for each region and can connect regions that are far away in the 2D space. The updated region features can then be projected back to pixels by linearly interpolation between regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph Convolutional Unit</head><p>We now describe the details of our graph projection, convolution and reprojection operations.</p><p>Graph Projection G proj first assigns feature vectors X to a set of vertices, parameterzied by W ∈ R d×|V| and Σ ∈ R d×|V| , with the number of vertices |V| pre-specified. Each column w k ∈ R d of W specifies an anchor point for the vertex k. Specifically, we compute a soft-assignment q k ij of a feature vector x ij to w k by</p><formula xml:id="formula_1">q k ij = exp -(x ij -w k )/σ k 2 2 /2 k exp (-(x ij -w k )/σ k 2 2 /2) ,<label>(2)</label></formula><p>where σ k is the column vector of Σ and / is the element-wise division. We constrain the range of each element in σ k to (0, 1) by defining σ k as the output of a sigmoid function. Eq 2 computes the weighted Euclidean distance between all x ij and w k , and creates a soft-assignment using softmax function. We denote Q ∈ R HW ×|V| as the soft assignment matrix from pixel to vertices, with each row vector q ij such that k q k ij = 1. Moreover, we encode features z k for each vertex k by</p><formula xml:id="formula_2">z k = z k z k 2 , z k = 1 ij q k ij ij q k ij (xij -w k ) /σ k .<label>(3)</label></formula><p>Each z k is a weighted average of the residuals between feature vectors x ij to the vertex parameter w k . z k is further L2 normalized to get the feature vector z k for vertex k. z k thus forms the kth columns of the feature matrix Z ∈ R d×|V| . We further compute the graph adjacency matrix as A = Z T Z. With normalized z k , A k,k in the adjacency matrix is the pairwise cosine similarity between the feature vectors z k and z k . Note that removing the coefficients 1/ ij q k ij does not impact the normalized feature z k , yet will change the way that the gradients are computed. Eq 3 is inspired by nonlinear feature encoding methods, such as VLAD or Fisher Vectors <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b30">31]</ref>. This connection is more obvious if we consider w k as the cluster center and σ k as its variance (assuming a diagonal covariance matrix). In this case, the L2 normalization is exactly the intranormalization in <ref type="bibr" target="#b33">[34]</ref>. We note that our encoding is different from VLAD or Fisher Vectors as we do not concatenate z k as a global representation of X. Instead, we derive graph structure from Z and keep individual z k as vertex features.</p><p>Eq 3 can be viewed as multiple parallel yet competing affine transforms, followed by weighted average pooling. And thus each z k provides a different snapshot of the input X. Moreover, if w k and σ k are computed as per batch mean and variance for cluster k, each affine transform becomes batch normalization <ref type="bibr" target="#b34">[35]</ref>. This link between fisher vector and batch normalization is discussed in <ref type="bibr" target="#b35">[36]</ref>.</p><p>To summarize, the outputs of our graph projection operation are (1) the adjacency matrix A, (2) the vertex features Z and (3) the pixel-to-vertex assignment matrix Q. Moreover, our graph projection operation introduce 2|V|d new parameters. With a small number of vertices (e.g., 32) and a moderate feature dimension (e.g., 1024), the number of added parameters is small in comparison to those in the rest of a deep network. Moreover, every step in this project operation is fully differentiable. Thus, chain rule can be used for the derivatives of the input x and the parameters (W and Σ). In practice, we reply on automatic differentiation for back propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Convolution</head><p>We make use of graph convolution G conv from <ref type="bibr" target="#b10">[11]</ref> to further propagate information on the graph. Specifically, for a single graph convolution with its parameter W g ∈ R d× d, the operation is defined as</p><formula xml:id="formula_3">Z = f (AZ T Wg),<label>(4)</label></formula><p>where f can be any nonlinear activation functions. We use the Batch Normalization <ref type="bibr" target="#b34">[35]</ref> with Rectified Linear Units for our models. For all our experiments, we use a single graph convolution yet stacking multiple graph convolutions is a trivial extension. While each graph convolution has parameters of size (d × d), it remains highly efficient with a small number of vertices. Note that our adjacency matrix is computed per sample, and thus our graph representation is sample-dependent and will get updated during training. This is different from the settings in <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29]</ref>, where a sample-independent graph is pre-computed and remains unchanged during training.</p><p>Graph Reprojection Our graph reprojection operation G reproj takes the inputs of transformed vertex features Z and the assignment matrix Q, and produces 2D feature map X. Ideally, we have to invert the assignment matrix Q, which is unfortunately unfeasible. Instead, we compute pixel features of X using a re-weighting of the vertex features Z, given by X = Q ZT . G reproj thus linearly interpolates 2D pixel features based on their region assignments and does not have any parameters. Note that even if two pixels are assigned to the same vertex, they will have different features after reprojection. Thus, GCU is likely to preserve the spatial details of the signal. Finally, these projection results can be integrated into existing networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning Graph Representations</head><p>Our model is fully differentiable and can be trained from end-to-end. However, we find that learning GCUs faces an optimization challenge. Let us consider a corner case where the model assigns most of the input pixel features x ij to a single vertex k. In this setting, the GCU will degenerate to a linear function W g (x ij -w k )/σ k . As this rare case seems to be unlikely, we find that the model can be trapped to several modes. For example, the model will always assign the whole image with a single vertex, but uses different vertices for different images. To address this issue, we propose two strategies to regularize the learning of GCU.</p><p>Initialization by Clustering We initialize the W and Σ in graph projection operations by clustering the input feature maps. Specifically, we use K-Means clustering to get the centers for each column w k of W . We also estimate the variance along each dimension and setup the column vectors σ k of Σ. Note that our model always start with a pre-trained network. Thus, K-Means will produce semantically meaningful clusters. And this initialization does not require labeled data. Once initialized, we use gradient descent to update W and Σ, and avoid tracking batch statistics as <ref type="bibr" target="#b34">[35]</ref>. We found that this initialization is helpful for stable training and gives slightly better results than random initialization.</p><p>Regularization by Diversity We find it beneficial to directly regularize the assignment between pixels to vertices. Specifically, we propose to add a graph diversity loss function that matches the distribution of the assignments p(Q) = ij q k ij ∈ R |V| to a prior p. This is given by</p><formula xml:id="formula_4">L div = KL(p(Q)||p)<label>(5)</label></formula><p>where KL is the Kullback-Leibler divergence between p(Q) and p. This regularization term is highly flexible as it allows us to inject any prior distributions. We assume that p follows a uniform distribution. This prior enforces that each vertex is used with equal frequency, and thus prevents the learning of "empty" vertices. We find that a small coefficient (0.05) of this loss term is sufficient. Our GCU can be easily wrapped into a graph block and incorporated into an existing network. Specifically, we define a graph block as <ref type="bibr" target="#b5">(6)</ref> where GCU k denote a GCU with k vertices and ⊕ can be either a residual connection or a concatenation operation. A residual connection allows us to insert GCU into a network without changing its behavior (by using a zero initialization of the batch normalization after G conv ). Concatenating features supplements the original map X with global context captured at different granularity. We explore both architectures in our experiments. We use the residual connection with a single GCU for object detection and instance segmentation, and concatenate features from multiple GCUs for semantic segmentation. Details of these architectures are shown in Fig 2 . 

Computational Complexity We summarize the computational complexity of our graph block. More importantly, we compare the complexity to 2D convolutions and non-local networks <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Graph Blocks</head><note type="other">Projection Graph Convolution Unprojection Projection Graph Convolution Unprojection Graph Convolution Unprojection Projection</note><formula xml:id="formula_5">X = X ⊕ GCU k 1 (X) ⊕ ... ⊕ GCU kn (X),</formula><p>• Complexity: For a feature map of size H × W with dimension d, our graph projection has a complexity of O(HW d|V|), where |V| is the number of vertices on the graph. The graph convolution is O(|V|d 2 + |V| 2 d) and the reprojection takes O(HW d|V|)) if we keep the output feature dimension the same as the inputs.</p><p>• Comparison to convolutions. The graph projection, convolution and reprojection operations have roughly the same complexity as 1x1 convolutions with output dimension |V| (assuming HW ≥ d for high resolution feature maps). Thus, the complexity of a single GCU is approximately equivalent to the stacking of three 1x1 convolutions.</p><p>• Comparison to non-local networks. For the same setting, the non-local operation <ref type="bibr" target="#b19">[20]</ref> has a complexity of O(H 2 W 2 d). With a high resolution feature map (large HW ) and a small |V|, this is almost quadratic to our GCU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We present our experiments and discuss the results in this section. We test our method on three important recognition tasks: (1) semantic segmentation, (2) object detection and instance segmentation.</p><p>Our experiments are thus organized into two parts.</p><p>We also explored different ways of incorporating our method in the experiments. For semantic segmentation, we concatenate multiple GCU outputs (Fig 2 <ref type="figure">Bottom</ref>). In this case, our method has to be accomplished with extra convolutions for recognition, and thus can be considered an novel context model. For object detection and instance segmentation, we incorporate GCUs with residual connections (Fig 2 <ref type="figure">Top</ref>) into the Mask RCNN framework <ref type="bibr" target="#b14">[15]</ref>. Here, our method does not change the original networks and thus serves as a new plugin unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Semantic Segmentation</head><p>We now present our results on semantic segmentation. We introduce the benchmark and implementation details, and present an ablation study of the GCU. More importantly, we compare our model to a set of baselines and discuss the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset and Benchmark</head><p>We use ADE20K dataset <ref type="bibr" target="#b36">[37]</ref> for semantic segmentation. ADE20K contains 22K densely labeled images. The benchmark includes 150 semantic categories with both stuff (i.e., wall, sky) and objects (i.e., car, person). The categories are fine-grained and their number of samples follows a long tailed distribution. Therefore, this dataset is very challenging. We follow the same evaluation protocol as <ref type="bibr" target="#b12">[13]</ref> and train our method on the 20K training set. We report the pixel level accuracy and mean Intersection over Union (mIoU) on the 2K validation set.</p><p>Implementation Details Our base model attaches 4 GCUs to the last block of a backbone network and concatenates their outputs, followed by convolutions for pixel labeling. These GCUs have <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b31">32)</ref> vertices and output dimensions of d = 256. And these numbers are chosen to roughly match the number of parameters and operations as in <ref type="bibr" target="#b12">[13]</ref>. We use ResNet 50/101 <ref type="bibr" target="#b37">[38]</ref> pre-trained on ImageNet <ref type="bibr" target="#b38">[39]</ref> as our backbone network. Similar to <ref type="bibr" target="#b12">[13]</ref>, we add dilation to the last two residual blocks, thus the output is down-sampled by a factor of 8. We upsample the result to original resolution using bilinear interpolation. As in <ref type="bibr" target="#b12">[13]</ref>, we crop the image into a fixed size (505x505) with data augmentations (random flip, rotation, scale) and train for 120 epochs. We also add an auxiliary loss after the 4th residual block with a weight of 0.4 as <ref type="bibr" target="#b12">[13]</ref>. The network is trained using SGD with batch size 16 (across 4 GPUs), learning rate 0.01 and momentum 0.9. We also adapt the power decay for learning rate schedule <ref type="bibr" target="#b39">[40]</ref>, and enable synchronized batch normalization. For inference, we average network outputs from multiple scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>We provide an ablation study of the GCU using the task of semantic segmentation.</p><p>The results are reported on ADE20K and with pre-trained ResNet 50 as the backbone. First, we vary the number of GCUs. With dilated convolutions, the backbone itself has a mIoU of 35.6%. Adding a single GCU with 2 vertices to the backbone achieves Figure <ref type="figure">3</ref>: Visualization of segmentation results on ADE20K (with ResNet 50). Our method produces "smoother" maps-regions that are similar are likely to be labeled as the same category.</p><p>Backbone Method PixAcc% mIoU% VGG16 <ref type="bibr" target="#b41">[42]</ref> FCN-8s <ref type="bibr" target="#b11">[12]</ref> 71.32 29.39 SegNet <ref type="bibr" target="#b40">[41]</ref> 71.00 21.64 DilatedNet <ref type="bibr" target="#b16">[17]</ref> 73.55 32.31 CascadeNet <ref type="bibr" target="#b36">[37]</ref> 74.52 34.90</p><p>Res50 <ref type="bibr" target="#b37">[38]</ref> Dilated FCN 76.51 35.60 PSPNet <ref type="bibr" target="#b12">[13]</ref> 80.76 42.78 EncNet <ref type="bibr" target="#b13">[14]</ref> 79 the run-time and memory cost, yet does not to improve the performance. Second, we evaluate our initialization and regularization schemes. Our base model (4 GCUs) without regularization and initialization has a mIoU of 41.34. Our regularization improves the result by 0.39% (41.73%). Our initialization further adds another 0.87% (42.60%). Thus, both the diversity loss and the clustering help to improve the performance.</p><p>Baselines We further compare our method with a set of baselines. These baselines are organized as</p><p>• Dilated FCN: This is the backbone network of our method, where we added dilation to a ResNet. It is also a variant of DeepLab <ref type="bibr" target="#b39">[40]</ref>.</p><p>• Context Models: We include results from recent context models for deep networks. Specifically, we compare to state-of-the-art results from PSPNet <ref type="bibr" target="#b12">[13]</ref>, RefineNet <ref type="bibr" target="#b18">[19]</ref> and Enc-Net <ref type="bibr" target="#b13">[14]</ref>. These are close competitors of our method.</p><p>• Other Methods: We also report results from <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b18">19]</ref> for reference.</p><p>Results and Discussions Our main results are summarized in Table <ref type="table" target="#tab_1">1</ref>. Our method (GCU) improves the backbone Dilated FCN network by 7% in mIoU. With ResNet 50, our result on mIoU is comparable to PSPNet. With ResNet 101, our method is 1.5% better than PSPNet and 4.6% higher than RefineNet in mIoU. We also notice that our pixel level accuracy is consistently lower than PSPNet by 0.2-1.2%. One possibility is that GCU will produce "diffused" pixel features. This is because the output features of GCU are linearly interpolated from region features, which are averaged across pixels. We visualize our results in Fig <ref type="figure">3</ref> and find that our method does tend to over-smooth the outputs (see the missing clock in the left column). A similar property is also observed in region based recognition <ref type="bibr" target="#b9">[10]</ref>. Even a good grouping may decrease the performance if the recognition goes wrong. For example, in the middle column of Fig 3, our method mis-classified the "building" region as "house" and has a lower score than the baseline Dilated FCN. Nonetheless, our method is able to assign the same category to the pixels on the building surface, which are previously divided in pieces.</p><p>Thus far, we have described our method by taking the analogy of region based recognition. However, we must point out that our method is trained without supervision of regions. And there is no guarantee that it will learn a valid representation of regions or region graphs. To further diagnose our method, we create visualizations of the assignment matrix in GCU (see <ref type="bibr">Fig 4)</ref>. It is interesting to see that our method does learn to identify some meaningful components of the scene. For example, with 2 vertices, the network seems to build up the concept of foreground vs. background. With 4 vertices, there seems to be a weak correlation between the assignments and the spatial layout (e.g., pink for ground, and yellow for vertical surfaces). As the number of vertices grows, the assignment begins to over-segment the image, creating superpixel-like regions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Object Detection and Instance Segmentation</head><p>We now present our benchmark and results on object detection and segmentation.</p><p>Dataset and Benchmark For both object detection and instance segmentation, we use COCO dataset from <ref type="bibr" target="#b42">[43]</ref>. COCO is by far the most challenging dataset for object detection and instance segmentation. COCO includes more than 160K images, where object bounding boxes and masks are annotated. We report the standard COCO metrics including AP (averaged over IoU thresholds), and AP 50 , AP 75 (AP at different IoU thresholds) for both boxes and masks. As in previous work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, we train using the union of 80k train images and a 35k subset of val images (trainval35k), and report results on the remaining 5k val images (minival).</p><p>Implementation Details For both tasks, we attach 4 GCUs to the last four residual blocks in ResNet 50/101 <ref type="bibr" target="#b37">[38]</ref> with FPN <ref type="bibr" target="#b44">[45]</ref>. These GCUs have 32 vertices and output dimensions d = [256, 512, 1024, 2048] that matches the feature dimensions of the network. Our GCUs are added with residual connections and zero initialization after the last convolution of the residual block and before FPN layers. We train the model using SGD with a batch size of 8 across 4 GPUs. Following the training schedule (x1) in <ref type="bibr" target="#b43">[44]</ref>, we linearly scale the training iterations (180K) and initial learning rate (0.01) based on our batch size. The learning rate is decreased by 10 at 120/160K iterations. We also freeze the batch normalization layers. Other implementation details for training and inference are kept the same as <ref type="bibr" target="#b14">[15]</ref>. Note that only random flip is used for data augmentation during training. And our results are reported without test time augmentation (e.g., multi-scale or flip). They can be further incorporated for to improve performance. It is also possible to further boost the performance for the baseline and our method by training for longer (as the x2 scheme in <ref type="bibr" target="#b43">[44]</ref>).</p><p>Baselines Our method is a plugin unit for Mask RCNN. Our baselines thus include</p><p>• Mask RCNN: This is the result reported in <ref type="bibr" target="#b19">[20]</ref>. This version is slightly better than the original Mask RCNN <ref type="bibr" target="#b14">[15]</ref> by replacing the stage-wise training with end-to-end training. • Mask RCNN + NL: This is the result of adding non-local operations to the backbone network of Mask RCNN <ref type="bibr" target="#b19">[20]</ref>. This operation is designed to capture long range dependencies. • Detectron: This is the open source version of Mask RCNN <ref type="bibr" target="#b43">[44]</ref> with end-to-end training and careful learning rate schedule. Our method builds on top of this implementation.</p><p>Results and Discussions Our results for both tasks are summarized in Table <ref type="table" target="#tab_2">2</ref>. Our method consistently improves the baseline Mask RCNN (Detectron) results by ∼1% for both detection and segmentation, and for both ResNet 50 and 101. This trend of improvement is also observed by adding non-local networks. We have to emphasis that our baseline (Detectron) is a well-optimized version of Mask RCNN. Thus, our improvements are non-trivial. Moreover, we present visualizations of our results and compare them to the Mask RCNN (Detectron) in Fig 5 . By modeling context using a graph representation, our method is able to find objects that are previously missing (the "boat" in first row), resolve ambiguity in region classification ("truck" vs "bus" in second row) and help to better estimate the spatial extent of objects (third row). One of the failure modes of our model is the missing detection of small, out-of-context objects, as the "skis" in the sky (zoom in to see in last row). We hypothesis that this is again due to the "diffused" local features in GCU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have presented a novel deep model for learning graph representations from 2D visual data. Our method transforms a 2D feature map into a graph structure, where the vertices define regions and edges capture the relationship between regions. Context modeling and recognition can be done using this graph structure. In this case, our method resembles the key idea behind region based recognition. Our model thus addresses pixel grouping, region representation, context modeling and recognition under the same framework. We have evaluated our method on several challenging visual recognition tasks. Our results outperformed state-of-the-art methods. Through careful analysis of these results, we demonstrated that our model is able to learn primitive grouping of scene components (such as foreground vs. background), and further leverage these components for recognition. Our method thus provides a revisit to the region based recognition in the era of deep learning. We hope our work will offer useful insights in re-thinking the design of visual representations in deep models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architectures of two different graph blocks. Top: Single GCU with a residual connection can be incorporated an existing network; Bottom: Concatenation of multiple parallel GCUs introduces a new context model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Visualization of the assignment matrix in GCU for semantic segmentation (with ResNet 50). From left to right: input image, pixelto-vertex assignments with 2, 4, 8 and 32 vertices. Pixels with the same color are assigned to the same vertex. Vertices are colored consistently across images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Building Sky Floor Tree Ceiling Bed Window Grass Person Table Plant Curtain Chair Clock House Lamp Skyscraper Pillow Coffee Bench Stool Brand Car</head><label></label><figDesc></figDesc><table><row><cell>GT</cell></row><row><cell>FCN</cell></row><row><cell>GCU</cell></row><row><cell>(ours)</cell></row></table><note><p>39.43%-a ∼4% boost. Using two GCUs with (2, 4) vertices reaches 40.92%. And our base model (4 GCUs with (2, 4, 8, 32) vertices) has 42.60%. Alternatively, if we increase the number of vertices in the last GCU of our base model (from 32 to 64). The mIoU score stays similar to the base model (42.58% vs. 42.60%). Adding more nodes increases</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results of semantic segmentation on ADE20K. mIoU scores within 0.5% of the best result are marked. With ResNet 50, our method improves Dilated FCN by 7%. With ResNet 101, our method outperforms PSPNet by 1.5%.</figDesc><table><row><cell></cell><cell></cell><cell>.73</cell><cell>41.11</cell></row><row><cell></cell><cell>GCU (ours)</cell><cell>79.51</cell><cell>42.60</cell></row><row><cell></cell><cell>RefineNet [19]</cell><cell>-</cell><cell>40.20</cell></row><row><cell>Res101 [38]</cell><cell>PSPNet [13] EncNet [14]</cell><cell>81.39 81.69</cell><cell>43.29 44.65</cell></row><row><cell></cell><cell>GCU (ours)</cell><cell>81.19</cell><cell>44.81</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results of object detection and instance segmentation on COCO dataset. We compare our single-model results to state-of-the-art methods on COCO minival. Scores of AP box and AP seg with in 0.5% of the best result are marked. Our method (GCU) improves the strong baseline of Mask RCNN by ∼1% across different networks.</figDesc><table><row><cell>Backbone</cell><cell>Method</cell><cell>AP box AP box 50</cell><cell>AP box 75</cell><cell>AP seg AP seg 50</cell><cell>AP seg 75</cell></row><row><cell></cell><cell>Mask RCNN [15, 20]</cell><cell cols="2">38.0 59.6 41.0</cell><cell cols="2">34.6 56.4 36.5</cell></row><row><cell>ResNet 50 [38]</cell><cell cols="3">Mask RCNN + NL [20] Mask RCNN(Detectron) [15, 44] 37.7 59.2 40.9 39.0 61.1 41.9</cell><cell cols="2">35.5 58.0 37.4 33.9 55.8 35.8</cell></row><row><cell></cell><cell cols="3">Mask RCNN(Detectron) + GCU 38.7 60.5 41.7</cell><cell cols="2">34.7 57.2 36.5</cell></row><row><cell></cell><cell>Mask RCNN [15, 20]</cell><cell cols="2">39.5 61.3 42.9</cell><cell cols="2">36.0 58.1 38.3</cell></row><row><cell>ResNet 101 [38]</cell><cell cols="3">Mask RCNN + NL [20] Mask RCNN(Detectron) [15, 44] 40.0 61.8 43.7 40.8 63.1 44.5</cell><cell cols="2">37.1 59.9 39.2 35.9 58.3 38.0</cell></row><row><cell></cell><cell cols="3">Mask RCNN(Detectron) + GCU 41.1 63.2 44.9</cell><cell cols="2">36.9 59.8 39.0</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This work was supported by ONR MURI N000141612007, Sloan Fellowship, Okawa Fellowship and ONR Young Investigator Award to AG. The authors thank Xiaolong Wang for many helpful discussions, and Jianping Shi for sharing implementation details of PSPNet.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* The work was done when Y. Li was at CMU. 32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recognition using regions</title>
		<author>
			<persName><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic segmentation using regions and parts</title>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object recognition by sequential figure-ground ranking</title>
		<author>
			<persName><forename type="first">João</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stacked hierarchical labeling</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Class segmentation and object localization with superpixel neighborhoods</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Fulkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Beyond categories: The visual memex model for reasoning about object relationships</title>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alyosha</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Region-based segmentation and object detection</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Associative hierarchical crfs for object class image segmentation</title>
		<author>
			<persName><forename type="first">L'ubor</forename><surname>Ladický</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ambrish Tyagi, and Amit Agrawal. Context encoding for semantic segmentation</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristin</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R-Cnn</forename><surname>Faster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><surname>Parsenet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<title level="m">Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust higher order potentials for enforcing label consistency</title>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L'ubor</forename><surname>Ladický</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Object detection by labeling superpixels</title>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bottom-up segmentation for top-down detection</title>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Geometric context from a single image</title>
		<author>
			<persName><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName><forename type="first">Clement</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Superpixel convolutional networks using bilateral inceptions</title>
		<author>
			<persName><forename type="first">Raghudeep</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kappler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TNN</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep fisher networks for large-scale image classification</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep TEN: Texture encoding network</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristin</forename><surname>Dana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">All about VLAD</title>
		<author>
			<persName><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Training faster by separating modes of variation in batch-normalized models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mahdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mubarak</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02892</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scene parsing through ADE20K dataset</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitnick</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Detectron</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
