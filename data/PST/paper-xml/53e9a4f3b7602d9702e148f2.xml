<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">What Do Usability Evaluators Do in Practice? An Explorative Study of Think-Aloud Testing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Mie</forename><surname>Nørgaard</surname></persName>
							<email>mien@diku.dk</email>
						</author>
						<author>
							<persName><forename type="first">Kasper</forename><surname>Hornbaek</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<addrLine>University Park</addrLine>
									<region>Pennsylvania</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">What Do Usability Evaluators Do in Practice? An Explorative Study of Think-Aloud Testing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">817C7F09C0B3C4BBCCA8568DA713E271</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Usability evaluation</term>
					<term>think aloud testing</term>
					<term>industrial software development</term>
					<term>user-centered design H.5.2 Information Interfaces and Presentation (e.g.</term>
					<term>HCI): User Interfaces-Evaluation/Methodology; D.2.2 Software Engineering: Design Tools and Techniques</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Think-aloud testing is a widely employed usability evaluation method, yet its use in practice is rarely studied. We report an explorative study of 14 think-aloud sessions, the audio recordings of which were examined in detail.</p><p>The study shows that immediate analysis of observations made in the think-aloud sessions is done only sporadically, if at all. When testing, evaluators seem to seek confirmation of problems that they are already aware of. During testing, evaluators often ask users about their expectations and about hypothetical situations, rather than about experienced problems. In addition, evaluators learn much about the usability of the tested system but little about its utility. The study shows how practical realities rarely discussed in the literature on usability evaluation influence sessions. We discuss implications for usability researchers and professionals, including techniques for fast-paced analysis and tools for capturing observations during sessions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Methods for usability evaluation are one of the successes of human-computer interaction: they are widely used and in many cases improve the usability of the software to which they are applied. According to recent surveys <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b34">35]</ref>, think-aloud testing (TA) is widely used and valued by usability evaluators. Numerous studies have been made of usability evaluation methods in general, and of TA testing in particular <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref>; for recent reviews see <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref>. In our view, however, these studies are biased in two respects. First, most studies do not take place in a practical software development context, but in a laboratory-style set-up with non-expert participants. While such studies give insight into benefits and drawbacks of particular evaluation methods, they miss how practical realities of software development shape the use of evaluation methods <ref type="bibr" target="#b36">[37]</ref>. Second, studies of usability evaluation tend to focus on coarse measures of outcomes such as the number of problems identified; they rarely describe the process of evaluation in detail. One exception is diary studies of usability evaluation, such as <ref type="bibr" target="#b23">[24]</ref>, which have provided valuable input on how evaluation methods are used. In a 2004 keynote, John called for more studies of the process of using HCI methods <ref type="bibr" target="#b21">[22]</ref>, seemingly dissatisfied with the current literature.</p><p>Addressing the two biases above, this paper reports an explorative study of how TA testing is practiced. We do so by observing the setting up, carrying out, and handling of results from TA sessions in professional consultancies or software development organizations. Inspired by grounded theory and verbal protocol analysis, we analyze and summarize data with two expected benefits. For usability researchers, we intend the paper to deliver insights into some issues of practical usability work. For usability professionals, we identify some of the problems and tradeoffs they face, hoping that this may assist the planning and conducting of future TA tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RELATED WORK</head><p>The question of how TA testing is done in practice is related to studies (a) describing experiences from real-life usability evaluation or (b) presenting detailed information on the process of usability evaluation. Below we review this research and discuss the extent to which it helps understand the practice of TA testing.</p><p>One group of studies describes real-life usability evaluation. Some of these studies systematically collect data through observation and interviews of usability specialists and other stakeholders in software development projects, see for example <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b35">36]</ref>. These studies focus on factors that facilitate or impede usability evaluations and the impact of their results. They have identified several strategic concerns in real-life usability evaluation, such as the need for users to be involved throughout the design process to facilitate useful contributions <ref type="bibr" target="#b35">[36]</ref> or that the organization of usability work, to some extent, shape usability results <ref type="bibr" target="#b18">[19]</ref>. They do not, however, in detail discuss how evaluations are undertaken.</p><p>Other studies have focused more on tactical issues of usability evaluation, see for example <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34]</ref>. These issues include how to make the results of usability evaluations such as TA testing impact software development <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b30">31]</ref> and how to deliver feedback that is useful to developers <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17]</ref>. As an example, Molich et al. <ref type="bibr" target="#b26">[27]</ref> discussed how the usability reports produced by nine teams of mostly professional evaluators differ in content. They found great variation in selection of tasks for usability tests and in reporting of results. Studies of tactical issues of usability evaluation rarely describe the process but focus mainly on the outcome of usability evaluation.</p><p>Equally interesting are studies where professionals report how practical circumstances have forced them to adapt and develop the evaluation procedures they use, see for example <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38]</ref>. Spencer <ref type="bibr" target="#b31">[32]</ref>, for example, described how the evaluation technique cognitive walkthrough was modified to better fit the realities of the software development organization in which he worked. Those realities include time pressure and a defensive attitude among participants in the walkthrough. Spencer reported that the modified technique worked better in his organization. Such studies provide interesting observations on factors influencing practical usability work, such as the influence of a particular kind of product on the decisions about which evaluation method to use <ref type="bibr" target="#b37">[38]</ref>. Yet, they lack the methodological rigor of the studies mentioned above and may not provide general lessons for usability research.</p><p>Another group of studies has focused on the process of usability evaluation. Mostly, the academic literature on usability evaluation has been concerned with the outcome of evaluation in the form of problem lists or suggestions for redesigns. A few studies, however, have reported diary studies of usability evaluation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24]</ref>. In those studies, evaluators typically keep a diary in which they make notes on their planning, conducting and reporting of an evaluation. John and Packer <ref type="bibr" target="#b23">[24]</ref> showed how participants in a diary study made severity judgments based on personal judgment rather than on the usability evaluation technique used. Hornbaek and Frøkjaer <ref type="bibr" target="#b17">[18]</ref> argued that the evaluation process observed in their diary study was complex, with participants identifying usability problems not just while conducting the actual evaluation, but also during planning and reporting of the evaluation. The studies referenced above, however, look only at non-expert evaluators outside an industrial software development context. These studies, and studies where the evaluator fill out forms during evaluation <ref type="bibr" target="#b7">[8]</ref>, present the most detailed data on evaluation currently available. We know of no studies that have systematically observed and analyzed usability evaluation, for example using video. Overall, it appears that studies looking at real-life usability evaluation place little focus on describing the process of usability evaluation; studies of the evaluation process look at somewhat artificial evaluation settings with diaries as the data-collection method with the finest granularity.</p><p>The paper by Boren and Ramey <ref type="bibr" target="#b3">[4]</ref> is a notable exception to these shortcomings. Boren and Ramey observed TA sessions in two companies, and related their observations to what some consider the theoretical basis of TA testing, the work of Ericsson and Simon <ref type="bibr" target="#b11">[12]</ref>. The analysis by Boren and Ramey showed discrepancies between the observed TA testing and the recommendations of Ericsson and Simon. While the work of Boren and Ramey has given unique insights to usability research, it is limited in that they reported mainly discrepancies to Ericsson and Simon's prescriptions (in particular about prompting the user), and not more general issues confronting a usability specialist conducting an evaluation.</p><p>Attempting to broaden the focus of Boren and Ramey's paper we next present an explorative study concerning how usability evaluations are conducted in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EXPLORING THE USE OF THINK ALOUD PROTOCOL</head><p>The question guiding the study is: what do usability evaluators do in practice? To get a better understanding of this we observed 14 TA test sessions in seven companies. We chose to focus on TA testing because it is widely used and because observing analytic usability evaluation, such as heuristic evaluation, presents methodological difficulties (e.g., concerning introspection) that we wanted to avoid. Our data comprise mainly audio recordings of the setting up, running and analysis of the TA sessions. Our intention is not to reprehend the practice of usability testing. Rather, we aim to explore what usability evaluators do so as to (a) sensitize usability research to industrial practice and (b) help evaluators understand better the strengths and weaknesses of what they do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Companies Participating in the Study</head><p>Seven companies agreed to participate in the study by letting us observe how they conduct TA tests. The companies were recruited among Danish enterprises that either offer usability evaluation as consultancy or integrate usability evaluation in their systems development. Table <ref type="table" target="#tab_0">1</ref> provides a summary of the companies; their names replaced by the letters A through G.</p><p>Our sample comprises three companies that provide usability evaluations solely to customers outside of the company and work with information technology as part of their core business (companies B, D, F). Two of the companies in the sample (companies A, C) perform usability evaluation both in-house and to customers outside of the company. These two companies have information technology and systems development as their core business. Finally, two of the companies solely perform usability evaluation in-house (companies E, G); while both companies have a strong presence online, their core business is in the service sector. The companies vary in size from 2 to 8500. They had varying levels of experience with usability evaluation; some of the evaluators we observed had only worked with usability for one year, while one had been conducting usability evaluations for eight years. Four companies evaluated running prototypes (companies A, C, E, F), two companies evaluated deployed applications (companies B, D), and company G evaluated paper prototypes. All tests observed were formative tests in that they were usability evaluations with users seeking to investigate issues such as concept, tools and navigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Collection</head><p>Methodologically we were inspired by grounded theory which dictates that researchers should not initiate an investigation on the basis of a list of hypotheses <ref type="bibr" target="#b29">[30]</ref>. Our data collection was thus broad and open-ended. We tried to participate in as many of the activities surrounding the usability evaluations as possible, wanting to probe how the TA protocol is put into practice. Data was collected over a period of three months and the focus of attention developed during this time, as suggested by <ref type="bibr" target="#b32">[33]</ref> and <ref type="bibr" target="#b29">[30]</ref>.</p><p>The core of our data is the observations, field notes, and audio recordings from 14 TA sessions, that is, the period of time from the arrival of the test participant until that participant leaves. These sessions were distributed among the companies as shown in Table <ref type="table" target="#tab_0">1</ref>; the number of sessions we could observe was largely dictated by practical circumstances. In all sessions, except those of company G, two evaluators from the company were present. On average, an evaluation consisted of a series of six sessions, of which we typically participated in two. The sessions we participated in were placed both at the beginning, middle and end of the series. In one session, the recording made from an observation room was of such poor quality that it allowed only sporadic transcription of the interaction between user and evaluator.</p><p>When possible, discussions, analysis, and informal conversations among usability evaluators before and after the test sessions were also observed and recorded. Sometimes customers (i.e., the persons who commissioned the test) were also present and took part in these discussions (e.g., company B). In two cases we recorded when usability evaluators delivered test results to the customers (companies C and G). In two cases we collected reports, summaries or notes that documented the tests (companies F, G). In two cases (companies A and C) we additionally conducted semi-structured interviews with the persons responsible for the usability work in the company.</p><p>The data collection described above resulted in, among other material, 24 hours and 54 minutes of audio recordings. Below we focus on the test sessions and the discussions immediately following tests -we only mention material from feedback sessions, usability reports, and the semi-structured interviews, when it corroborates findings from the core data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Analysis</head><p>Analysis was conducted in three phases. First we segmented the recordings applying descriptive keywords to each segment. Second we re-evaluated segments and keywords in order to adjust keywords or apply new ones. Third we analyzed and tried to form a coherent interpretation of segments that shared keywords. We explain this procedure more thoroughly below, and briefly relate it to grounded theory <ref type="bibr" target="#b29">[30]</ref> and Chi's proposal for how to analyze verbal protocols <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Segmenting and open coding of the recordings</head><p>The audio recordings were initially divided into 641 segments. One segment could concern a usability evaluator analyzing the test results, or explaining how to ensure scientifically valid test results. A segment could last from a few seconds to several minutes. We chose to do only a partial transcription of the recordings, but listened repeatedly to the segments during our analysis.</p><p>In order to code the segments, keywords were attached to each segment allowing us to analyze and group segments. Thirty-five keywords were generated as the study proceeded. Some segments regarded more than one interesting topic and hence got more keywords attached to it. This process is similar to open coding in grounded theory <ref type="bibr" target="#b29">[30]</ref> or to Chi's <ref type="bibr" target="#b5">[6]</ref> phase of developing or choosing a coding scheme or formalism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Re-evaluating and crosschecking the coding</head><p>In order to ensure that a segment contained evidence for a specific keyword, the coding was carried out in two iterations, one by each of the authors. Disagreements or questions about the attachment of a keyword to a segment were discussed before attaching an existing or creating a new keyword. This is similar to Chi's phase of operationalizing evidence in the protocols <ref type="bibr" target="#b5">[6]</ref> and, in part, to axial coding in grounded theory <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synthesizing and interpreting the data</head><p>Groups of segments, which shared the same keyword, were analyzed to identify the most interesting areas and thus reduce the size of data. For interesting areas, we looked for the observations that were most surprising to us, or seemed to contrast the literature on usability research and textbook recommendations on how to do a usability evaluation. Such areas were selected for further analysis and interpretation. This phase is similar to Chi's phases of seeking patterns in the mapped formalism <ref type="bibr" target="#b5">[6]</ref> or selective coding in grounded theory <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RESULTS</head><p>The following section describes our results organized in six areas. Table <ref type="table" target="#tab_2">2</ref> summarizes these areas and the main findings within each of them. The areas concern (1) analysis of the results from a session, (2) confirmation of known issues, (3) practical realities, (4) questions asked during a test, (5) laboratory-style scientific standards, and (6) uncovering usability problems or utility concerns. Below we present each area in turn. For findings we give the number of sessions in which they were observed. We use sessions rather than segments as an indication of frequency, because the number of segments is strongly influenced by the nature of a session, especially how much the evaluator and the user talks, how much they jump between topics, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of Results from a Test Session</head><p>The first area concerns how usability evaluators analyze test sessions. By analysis we mean the task of understanding and agreeing upon important observations from a session. Analysis also includes attempts to understand the causes of those observations, interpret user behavior and find design solutions to observed problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>None of the sessions included attempts to carry out a</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Area of attention Main finding N Example of observations and quotes</head><p>Analysis is unstructured 9 Scattered fragments of analysis; no systematic approach used Analysis is incomplete 9 Does not identify causes or solutions; restricts discussion to user traits</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of results from a test session</head><p>Analysis as a summary with the user 3 "Let's sum up"; selecting a few problems for further questioning; listing key findings Looking for known issues 8 "Now, I am just looking for ammunition"; develops ideas of problems before testing; tasks and questions designed to point out known issues</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Confirmation of known issues as a test' focus</head><p>Practitioners have foreseen problems 5 "We have a gut feeling", "I told you so"  structured analysis of the results immediately after the session, for example by systematically agreeing on and then analyzing, say, the ten most prominent observations of user difficulties. However, as we have not in this study covered every step from test design to final report, we are not able to say if analysis took place later.</p><p>One evaluator did carry out a semi-structured analysis in the last minutes of three sessions though, focusing on summarizing key findings while the user was present: Three other evaluators (in a total of five sessions) also tried to sum up a few problematic topics and return to those topics for further questioning before ending the session. However, we did not encounter any systematic attempt to cover the most important observations directly after a session.</p><p>After a session had finished, the most common activity was that usability evaluators, and in four sessions also customers, discussed the session. We observed how they presented overall impressions intertwined with a general discussion about the system, social talk, observations, ideas for re-design, and occasionally analysis of the problems. To illustrate, an 11 minutes long discussion of a session was shaped as follows:</p><p>Impressions of user attitude, discussing problems with prototype (3 min); Identification of one problem, analysis, summary of observations from session (2 min); Talk about old ideas, identify two problems, analysis (2 min); Discussion of recommendations and re-design (30 sec); Customer calls-and gets a short general summary (1.5 min); Summary of findings combined with general talk (2.5 min).</p><p>After this discussion, one evaluator went on to write a summary of findings to the customer. In other sessions the evaluators would just have a short conversation about general impressions before leaving the room, and thus ending the attempt to carry out an immediate analysis.</p><p>In nine sessions we saw examples of incomplete analysis. By incomplete analysis we mean remarks or observations that, if they were intended to assist in uncovering usability problems and solutions to such problems, needed to be elaborated and discussed. In seven sessions, for instance, evaluators would quickly characterize a user as being for example confused or insecure, but fail to follow up on this characterization or even identify what made the user become confused or insecure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Confirmation of Known Issues as a Test Focus</head><p>The evaluators made comments before, during and after sessions, which let us to believe that they held more or less strong ideas about usability problems of the particular system being tested, even before commencing on the test. These ideas appear to shape the design of tasks and the questions raised during a test session. While such ideas are natural and may be important hypotheses, they sometime appear to focus the test on a particular topic or hypothesis. This delicate balance seem difficult to master.</p><p>After a session one evaluator stated, for example, that the test should provide proof for the conclusions in a usability report, which she had already begun writing:</p><p>C1: "I think we agree on many of the issues" C2: "Yes -I have already written the chapter, I just need the ammunition".</p><p>A total of four evaluators stated that they had a more or less clear idea of the usability problems before commencing a test. In an interview another evaluator said that usability tests in some cases merely serve to confirm the evaluators' assumptions:</p><p>A: "When we design a test we practically always have a gut feeling where it will fail […] in a way it is just an 'I told you so'-kind of thing, but it is nice to be able to document it".</p><p>The quotes suggest that usability evaluators see a need to support expert opinion with something more concrete when presenting customers with advice on usability. This may lead to tests that in part serve only to confirm.</p><p>In addition to these expressed opinions, it also appears that the actual activities of a test are sometimes chosen to confirm, or at least explore, areas known to be problematic. Questions and tasks within a test, for example, would be chosen to explore well-known issues. This led to test situations where evaluators literally waited for the user to point to the problem area. A1 explained to us how a certain task that required the entry of percentages most likely would cause problems. During the test, the user did actually spot the problem, and the response from the evaluator suggested almost a relief that the user did so:</p><p>U: [Typing] A1: "So you just added minus 10 on both lines?" U: "…And then I got 20%.....WHAT?" A1: "Yes" [laughs out conformingly]</p><p>In another session, in response to a user severely criticizing a particular functionality, the usability evaluator broke out in laughter and said "this is really good", suggesting to us, that this issue was already anticipated as being problematic. In this way, 8 of the 14 sessions had examples of evaluators directly or indirectly expressing that they were confirmed in their preconceived opinions about usability problems.</p><p>It is hard to say whether a test focused at confirmation influences how evaluators interpret the observations they make during a test. An evaluator from company A noted after a session; "we really wanted to test this because we are confident it will fail, he [the participant] managed it, but I am sure others will not". The quote suggests that the expectation to find the problem in future tests could overshadow the possible interesting observation that at least one user successfully used a particular part of the interface. We return to discuss the balance between known issues and new findings in the discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Practical Realities Influencing Tests</head><p>The study revealed numerous practical problems that usability evaluators experience when testing. In 12 sessions we observed examples of such problems or practical realities. These include system failures, users not showing up for a session, disturbing surroundings, and technical problems with recording devices. Despite such problems the evaluators managed to carry out all of the sessions.</p><p>Data show that the practical realities surrounding a test are produced by many factors, some out of the evaluators' control. In eight sessions, for example, we observed severe technical problems interfering with the session. As an example one session had a technical problem approximately every five minutes, each resulting in a break in workflow.</p><p>In two sessions problems arose because the customer had failed to provide the required number of test participants, thus forcing the evaluators to quickly find a solution in order to carry through the test within the scheduled time:</p><p>F1: "The next user is one of my old friends […]" F2: "[…] they are not the first ones we choose, but if the customer fail to recruit [when they have agreed to do so] then we take whomever we can get."</p><p>Six sessions had problems with unfinished prototypes or last-minute changes to the prototype. One evaluator noted: D1: "Some things will, if not done properly, affect the users' perception rather dramatically…A log-in name should not be "WaddleFish", it's such a developerkind-of-thing to make up funny log-in names like that"</p><p>Unfinished prototypes or prototypes recently changed are two reasons that evaluators often were confused or in doubt about the functionality of the prototype. In seven sessions evaluators stated that they were not familiar with aspects of the prototype's functionality: G1: "Now…let us see…[searches in prototype paper sheets]…these are brand new, so I have not looked at them before"</p><p>In sum, severe practical problems in some sessions lead to a continual interruption of the participants' attempts to complete their tasks. In this study, the practical realities influencing tests are much more frequent and severe than one would expect from textbooks or research papers on usability evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Questions Asked During a Test</head><p>The study showed variations in the types of questions asked by the evaluators. We analyzed these to understand which kinds of information usability evaluators are interested in, and to discuss later the validity of the information gained by different kinds of questions.</p><p>A large number of questions were reminders to keep talking like "Hmmm" and "Yes?". These kinds of questions were omnipresent and should be uncontroversial.</p><p>Equally unsurprising is the many questions that simply try to elicit what the user is currently doing, or what problems the user is facing, for example "What is happening?", "What are you looking for?", or "What is the problem?". Many of these questions concerned the users' experienced problems in solving concrete tasks.</p><p>We encountered evaluators asking questions that differed dramatically from how Ericsson and Simon <ref type="bibr" target="#b11">[12]</ref>, and in part also Boren and Ramey <ref type="bibr" target="#b3">[4]</ref>, suggest to interact with test participants. Some questions concerned, for example, nonexistent parts of the system, such as asking how the user would use a mouse to interact with a paper prototype or what the user would feel about having to create a user profile in order to be able to use the system.</p><p>Other questions appeared speculative or hypothetical. One evaluator asked, for example, "Do you think you would go back to the front page at some point?" and "Let us say that something here [in a list of articles] would interest you…" (both F1), asking the user to continue on this assumption. Some questions urge users to look back in time and remember their thoughts, that is, retrospective questions.</p><p>For example "Did you notice this column [when you were here before]?" (F1), or "Do you remember if you got what you expected from the web shop?" (E1).</p><p>Questions about the user's expectation of the system were also frequent, for example: "What would you expect to see?" or "How many would you expect to find?" (both from company G). Questions about the expectations of the system were often asked in the beginning of the In 13 sessions we observed one or more questions of the five kinds described above. In contrast to the experienced problems discussed earlier they did not concern problems experienced as part of solving a task, but rather imagined, indirectly experienced or expected problems. This intensive probing for such problems surprised us.</p><p>Thirteen sessions showed another kind of question, best characterized as leading questions. One evaluator, for instance, asked a question aiming at a certain issue of interest and the user would without much trouble solve the task or answer the question as anticipated:</p><p>[The user has pressed play to see an episode of a series of video clips in a media player:] G1: "What would happen when this episode was over?" User: "The series would end" G1: The evaluators made several remarks suggesting that they find validity to be of great importance when testing. The concepts of validity upon which evaluators rely seem primarily to be those of scientific experiments, such as keeping the same procedure throughout a test, using representative subjects, and using elaborate questionnaires to get information on users' satisfaction. Note that we here mainly describe the evaluators' beliefs; in the discussion we will look closer at the relation of these views to those presented by the literature.</p><p>Evaluators from three companies (representing five sessions) emphasized that one should not change a test design between the sessions of a test. Changing a test design could include making changes to questions, tasks, prototype and choice of language. One evaluator, for example, stated the importance of maintaining the same tasks and phrasings of questions throughout a TA test even though it was evident after a few test sessions that the users misunderstood some of the tasks.</p><p>C1: "I think it is really annoying that we already now can see problems, which we cannot correct as we go along…but we have to make sure that all users get the same questions"</p><p>In three sessions we observed how the fact that evaluators were trying to adhere to laboratory-style validity resulted in rigid and artificial procedures. For instance, we observed a session where Danish evaluators asked questions in English to a Danish user. The aim was to make test conditions similar among Scandinavian participants. In another session, evaluators tried to collect data about the system through a series of questions (e.g., "I will be more effective with the system") that users should rate on a one-to-seven scale. These questions are similar to instruments for measuring subjective satisfaction typically used in laboratory-style experiments. While such scales certainly have their uses, in this case they seemed to contradict what had happened during the session minutes before. This observation was supported by the evaluator:</p><p>A1: "When users rate statements […] we take the results with a kilo of salt. This guy -it is a pretty good score right but […] in the beginning he was right-clicking all over the place and he mentioned that he did not like the buttons disappearing…" Thus, the questions were seemingly included to adhere to some perception of how scientific user testing should be conducted. In this case, the answers were apparently not used, but had they been, it might have lead to a deemphasis of the user difficulties just observed.</p><p>In sum, the attempt to adhere to scientific standards in some cases lead to rigid or artificial procedures that appeared unnecessary given the influence of practical realities and the rather informal analysis of test results mentioned earlier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uncovering Usability Problems or Utility Concerns</head><p>All sessions in the study would naturally include segments where usability problems were identified, including problems with scrolling, positioning of information, how links should be emphasized, how the user was prompted for information several times, etc. Other segments concern the utility of the system, for example which tasks the system should support or whether tasks from the test were unrealistic with regard to how the user usually uses the system or would want to use the system <ref type="bibr" target="#b27">[28]</ref>. We observed utility concerns being discussed in 10 sessions.</p><p>In seven sessions we observed how the evaluator asked more or less specific questions concerning the utility of the system. Consider the following example: F1: "Lets look at the article again...What would you typically do?" User: "I would pass it on...if it was fun and interesting…" F1: "Like printing it?" U: "No just by word of mouth…" F1: "Word of mouth. Ok…" U: "…unless it was really good -then I would forward it electronically..." F1: "Would you ever print articles?" U: "No....I actually save them […]" F1: "So…do you copy the text and paste it into a Word document?" U: "Yes, I could do that"</p><p>Ten sessions had examples of users who were pointing to utility problems like the following from company C: U: "[reads question loud:] 'Where would I look for an employee?'.... I would use a phonebook [which is not a part of the system]" Some users specifically pointed to areas of the system, which they found failed to support their workflow, for example from company E: "This is just to tell you that I would not do it like this".</p><p>In 13 sessions we observed how problems relating to usability seemed to be favored over problems relating to the utility of the system. A remark from a user about not wanting to solve a task in the way suggested by the system did for example not result in an attempt to investigate that utility problem further; nor did it get reported to the customer during the feedback session we observed. This study suggests that utility problems are much less frequently examined than usability problems. Given the little attention problems regarding utility got in the sessions we observed, we do not expect them to be treated more thoroughly in discussions that we did not attend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DISCUSSION</head><p>To sum up, this study shows that careful and systematic analysis of usability problems rarely take place immediately after the sessions in which they occur. Evaluators do not always, either, ensure that they agree on even the most important observations from a test. In addition, many tests appear to search also-and sometimes mainly-for confirmation of issues known beforehand or observed in other tests. Most of the sessions we observed were affected by practical realities such as incomplete prototypes and evaluators' limited experience with the system being tested. The questions raised by the evaluators during the test varied, but some questions appeared hypothetical and probed only users' expectations and not the problems they actually experienced. Some evaluators seemed to regard TA testing as a scientific laboratory-style method resulting in rigid and artificial procedures when conducting the test. Finally, seemingly important observations about the utility of the system being evaluated were made during sessions. These were infrequent, however, compared to results and discussions concerning usability issues.</p><p>Most surprising to us is the lack of systematic analysis while the results of a test are still fresh in mind. As we have not covered every step from test design to final report in this study, we are unable to rule out whether analysis were done at a later stage. Still, the fact that evaluators rarely check whether they agree on the most important observations from a session adds to the picture of analysis as being a weak part of the evaluation process. Work on the evaluator effect <ref type="bibr" target="#b15">[16]</ref> show that evaluators observing the same test find substantially different usability problems, making collecting and discussing different views of the main observations important. Summaries of the main observations by the evaluator while the test participant was present worked well-similarly to the idea of cooperative usability testing <ref type="bibr" target="#b12">[13]</ref>. However, using this or similar techniques to agree on observations from a test does not in itself reveal usability problems, the causes of those problems, or possible remedies for them.</p><p>Perhaps the lack of systematic analysis is understandable, given the scarce advice about analysis of usability tests we receive from textbooks and introductions about how to do a TA study. Molich <ref type="bibr" target="#b25">[26]</ref>, for example, used 2 pages of his 33-page instruction on how to do TA testing to discuss analysis. Dumas and Redish <ref type="bibr" target="#b10">[11]</ref> used around 31 pages of their 404-page textbook on analysis. It appears desirable that usability research develops and validates techniques supporting fast-paced analysis. Usability evaluators would be well advised to more systematically relate and discuss their observations when they are fresh in mind. Evaluators might take up using post-it notes for capturing observations during a session, and analyze these immediately after the session. They might find it rewarding to prioritize these post-its, possibly together with the user, to develop a common understanding, and discuss problems and feasible solutions.</p><p>The extent to which usability practitioners already before testing had a clear idea of the usability problems to be found was surprising. Interestingly, recommendations are made in the literature (e.g., <ref type="bibr" target="#b10">[11]</ref>, p. 160) about looking for known problems. Some views of the psychology of confirmation suggest that as a result of this, evaluators are very likely to confirm what they are looking for, perhaps failing to make other equally important observations. If the answer is not known with confidence prior to testing, we agree with the practice of exploring these explicit questions in the test. However, if usability issues are already known with such confidence that the practitioner is only "looking for ammunition", why test at all? Finding the balance between on the one hand testing specific areas of concern and on the other hand exploring the system in a more open manner seems to be an important but difficult challenge to evaluators.</p><p>The practical realities surrounding the tests we observed are far from the expectations about the test situation presented in textbooks such as <ref type="bibr" target="#b10">[11]</ref>. Techniques and tools that are usable under such less-than-ideal circumstances are needed, for example to enable the analysis of observations in the usually short time available between sessions. Evaluators should for their part consider preparing material to be used on the fly in case of system failure.</p><p>Given the work of Boren and Ramey <ref type="bibr" target="#b3">[4]</ref>, we had expected open and varied questions. Quite surprisingly we saw hypothetical questions, abstract questions, leading questions, and plain impossible-to-answer questions: in short, questions that did not aim at understanding problems experienced by the user, but rather at encouraging users to predict possible problems. On the one hand this suggests that evaluators may be looking for information about feelings and perceptions, which cannot be gained from a traditional TA testing. On the other hand we feel obliged to point out that some of the questions we encountered could never produce useful answers.</p><p>Questions about "first impressions", "what would you expect to be there [e.g., on the next page]", or "what do you feel about this" may imply that evaluators need researchers to provide more valid and systematic ways of probing for, say, participants' feelings of trust. Evaluators are advised to pay closer attention to the way they phrase their questions.</p><p>Questions probing for information about utility also seem to warrant further investigation. Molich <ref type="bibr" target="#b25">[26]</ref> suggested to ask test participants about their impressions of the tasks after a TA session. In two sessions we observed how useful discussions about the users' real-life tasks developed from such a question being asked during a test session. However when the same type of question appeared at the end of a session as advised by Molich, it became more general and received also a general answer. We suggest for researchers to provide further techniques for initiating discussions about utility during tests, which would help address the concern that usability testing might "tune a user interface at the tail end of design, to clean up any rough edges or unnecessary difficulty in understanding or interacting with the interface" [2, p.373], instead of concern the user' tasks or needs. In order to understand and discuss how to improve the utility of a system evaluators may find it helpful to question the system's utility and ask users how they usually go about solving a specific task.</p><p>The study suggests a belief amongst some evaluators that usability testing is science, and therefore must meet the same criteria as science. Iivari <ref type="bibr" target="#b18">[19]</ref> recently reported an explorative study in which similar attitudes were present among some usability professionals, "staid researchers" in Iivari's terms. The insistence on, for example, not changing tasks or procedure during a test appears rigid and counter-productive. We encourage evaluators to change set-up or make alterations to the prototype in the middle of a test if they believe it will help them answer important questions about the use of the system. Since TA testing is not a classical laboratory-style scientific testing method evaluators may feel they need to support the formative test results with summative measures. This need for bolstering a usability claim is discussed by <ref type="bibr" target="#b4">[5]</ref> who points at highlights videos as one way of providing such evidence. Researchers are encouraged to search for other, less expensive methods, for backing up usability results.</p><p>Acknowledging the work of Boren and Ramey <ref type="bibr" target="#b3">[4]</ref> this study aims at providing a needed description of how usability evaluation is conducted in practice. Two limitations are worth mentioning. First, we have only collected data in seven companies. Obviously, there are great variations in how usability work is conducted in those companies, which we have not touched upon. A goal for future work should be to collect more coarse-grained data, which would capture the process of usability evaluation in a greater number of companies. Second, we have mainly focused on test sessions. Thus, we did not explore the relation between test sessions and the feedback given to customers; nor did we collect any material on the planning of tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSION</head><p>We have presented an explorative study of how usability professionals conduct think-aloud tests. It suggests that think-aloud tests might not get sufficiently analyzed. We see a tendency that evaluators end up focusing too much on already known problems, and that the questions they ask during a test seem to concern problems that the user expects, rather than problems actually experienced during the test. The tests were to some extent shaped by practical realities and by some evaluators' adherence to a strict, laboratory-style procedure. Finally evaluators seem to prioritize problems regarding usability over problems regarding utility, when they conduct think-aloud tests.</p><p>We encourage further work on methods for fast-paced analysis. Methods and procedures for investigating the utility and probing for users' perception of a system may also be of value for evaluators. Practitioners are advised to more systematically capture and discuss observations from a test. Questions about the practical relevance of the system evaluated could be one way to address utility issues. Investigating problems that are experienced rather than expected may also improve think-aloud tests.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>F1:</head><label></label><figDesc>"Let's sum up: The front page [should] maybe emphasize what they have in mind […] and the logo [gesturing where a logo should be]….and eventually [we should] list these sections. And the picture behind [we should] make it a bit more interesting. The editorial ends down here [points]…"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 . The companies participating in the study and the test sessions observed within each company.</head><label>1</label><figDesc></figDesc><table><row><cell cols="2">Company Total no. of</cell><cell>Test</cell><cell>Evaluators</cell><cell>Evaluators'</cell><cell>Customer</cell></row><row><cell></cell><cell>employees</cell><cell>sessions</cell><cell>present</cell><cell>experience</cell><cell>of test</cell></row><row><cell></cell><cell>(no. of</cell><cell>observed</cell><cell>during</cell><cell>in years</cell><cell>results</cell></row><row><cell></cell><cell>employees</cell><cell></cell><cell>tests</cell><cell></cell><cell></cell></row><row><cell></cell><cell>working with</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>usability)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A</cell><cell>810 (6)</cell><cell>2</cell><cell>2</cell><cell>1 -6</cell><cell>Intern</cell></row><row><cell>B</cell><cell>2 (2)</cell><cell>1</cell><cell>2</cell><cell>1 -8</cell><cell>Extern</cell></row><row><cell>C</cell><cell>165 (3)</cell><cell>1</cell><cell>2</cell><cell>2.5 -6</cell><cell>Intern</cell></row><row><cell>D</cell><cell>7 (7)</cell><cell>1</cell><cell>2</cell><cell>1 -6</cell><cell>Extern</cell></row><row><cell>E</cell><cell>8500 (7)</cell><cell>4</cell><cell>2</cell><cell>4 -6</cell><cell>Intern</cell></row><row><cell>F</cell><cell>16 (3)</cell><cell>3</cell><cell>2</cell><cell>1.5 -6</cell><cell>Extern</cell></row><row><cell>G</cell><cell>3464 (8)</cell><cell>2</cell><cell>1</cell><cell>6.5</cell><cell>Intern</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 . Overview of results. N refers to the number of sessions in which a finding was made (out of 14 sessions in total)</head><label>2</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>"It was just a short version of the series or what?" U: "[…]I have pressed to see the whole series...Ah! I have pressed to see the whole series [...] something [other episodes] could come afterwards […]"</figDesc><table /><note><p>Trying to Meet Laboratory-Style Scientific Standards</p></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Stakeholder Forest: Designing an Expense Application for the Enterprise</title>
		<author>
			<persName><forename type="first">J</forename><surname>Arnowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dorsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heidelberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI 2005</title>
		<meeting>CHI 2005</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="941" to="956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">H</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Holtzblatt</surname></persName>
		</author>
		<author>
			<persName><surname>Design</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Morgan Kaufman Publishers</publisher>
			<pubPlace>San Francisco</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Why Usability for Lost or Usability in in-House Software Development</title>
		<author>
			<persName><forename type="first">I</forename><surname>Boivie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Åborg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Persson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Löfberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interacting with Computers</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="623" to="639" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Thinking Aloud: Reconciling Theory and Practice</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Boren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Professional Communication</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="261" to="277" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Role of Highlights Video in Usability Testing: Rhetorical and Generic Expectations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yeats</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technical Communications</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quantifying Qualitative Analyses of Verbal Data: A Practical Guide</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Learning Sciences</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="271" to="315" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The Human-Computer Interaction Handbook</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cockton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lavery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Woolrych</surname></persName>
		</author>
		<editor>Jacko, J. A. &amp; Sears, A.</editor>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Lawrence Erlbaum Associates</publisher>
			<biblScope unit="page" from="1118" to="1138" />
		</imprint>
	</monogr>
	<note>Inspection-Based Evaluations</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Changing Analysts&apos; Tunes: The Surprising Impact of a New Instrument for Usability Inspection Method Assessment</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cockton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Woolrych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hidemarch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HCI 2003</title>
		<meeting>HCI 2003</meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="145" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The Human-Computer Interaction Handbook</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dumas</surname></persName>
		</author>
		<editor>Jacko, J. A. &amp; Sears, A.</editor>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Lawrence Erlbaum Associates</publisher>
			<biblScope unit="page" from="1093" to="1117" />
		</imprint>
	</monogr>
	<note>User-Based Evaluations</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Describing Usability Problems: Are We Sending the Right Message?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Molich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jefferies</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="24" to="29" />
		</imprint>
	</monogr>
	<note>interactions, 4 (2004</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Practical Guide to Usability Testing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Redish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intellect</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Ericsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Simon</surname></persName>
		</author>
		<title level="m">Protocol Analysis: Verbal Reports As Data</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
	<note>Revised Edition</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cooperative Usability Testing: Complementing Usability Tests With User-Supported Interpretation Sessions</title>
		<author>
			<persName><forename type="first">E</forename><surname>Frøkjaer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hornbaek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Extended Abstracts of ACM Conference on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1383" to="1386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Making a Difference -a Survey of the Usability Profession in Sweden</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gulliksen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Boivie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Persson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hektor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Herulf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Nordichi</title>
		<meeting>Nordichi</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="207" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">User Testing in Industry: A Case Study of Laboratory, Workshop, and Field Tests</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hertzum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ERCIM Workshop on User Interfaces for All</title>
		<meeting>ERCIM Workshop on User Interfaces for All</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="59" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The Evaluator Effect: A Chilling Fact About Usability Evaluation Methods</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hertzum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="421" to="443" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Comparing Usability Problems and Redesign Proposals As Input to Practical Systems Development</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hornbaek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frøkjaer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI&apos;2005</title>
		<meeting>CHI&apos;2005</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="391" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Two Psychology-Based Usability Inspection Techniques Studied in a Diary Experiment</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hornbaek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frøkjaer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Nordichi</title>
		<meeting>Nordichi</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Realistic Humanists&apos; or &apos;Staid Researchers&apos;? An Analysis of Usability Work in Software Product Development</title>
		<author>
			<persName><forename type="first">N</forename><surname>Iivari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interact 2005, Edizioni Guiseppe Laterza</title>
		<meeting>Interact 2005, Edizioni Guiseppe Laterza</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="418" to="430" />
		</imprint>
	</monogr>
	<note>Usability Specialists -&apos;a Mommy Mob</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Two Case Studies in Using Cognitive Walkthroughs for Interface Evaluation</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>John</surname></persName>
		</author>
		<idno>CMU-CS-00-132</idno>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">User Interface Evaluation in the Real World: A Comparison of Four Techniques</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jeffries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wharton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Uyeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI&apos;91</title>
		<meeting>CHI&apos;91</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="119" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Beyond the UI: Product, Process and Passion</title>
		<author>
			<persName><forename type="first">B</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Nordichi</title>
		<meeting>Nordichi</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="285" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Evaluating a Multimedia Authoring Tool</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Mashyna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society of Information Science</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1004" to="1022" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning and Using the Cognitive Walkthrough Method: a Case Study Approach</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Packer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI&apos;95</title>
		<meeting>CHI&apos;95</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="429" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Comparison of Empirical Testing and Walkthrough Methods in Usability Interface Evaluation</title>
		<author>
			<persName><forename type="first">C.-M</forename><surname>Karat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fiegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI&apos;92</title>
		<meeting>CHI&apos;92</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="397" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Rolf</forename><surname>Molich</surname></persName>
		</author>
		<ptr target="www.dialogdesign.dk" />
		<title level="m">User testing, Discount user testing</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Comparative Usability Evaluation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Molich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Ede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kaasgaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Karyukin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behaviour &amp; Information Technology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="65" to="74" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
		<author>
			<persName><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Usability Engineering</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<pubPlace>San Francisco, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Finding Usability Problems Through Heuristic Evaluation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI&apos;92</title>
		<meeting>CHI&apos;92</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="373" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Grounded Theory of the Flow Experiences of Web Users</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="347" to="363" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Making a Difference -The Impact of Inspections</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sawyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Flanders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI&apos;96</title>
		<meeting>CHI&apos;96</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="376" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The Streamlined Cognitive Walkthrough Method, Working Around Social Constraints Encountered in a Software Development Company</title>
		<author>
			<persName><forename type="first">R</forename><surname>Spencer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI&apos;2000</title>
		<meeting>CHI&apos;2000</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="353" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Basics of Qualitative Research -Techniques and Procedures for Developing Grounded Theory</title>
		<author>
			<persName><forename type="first">A</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Corbin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Sage Publications</publisher>
			<pubPlace>California</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Usability Testing -on a Budget: a NASA Usability Test Case Study</title>
		<author>
			<persName><forename type="first">M</forename><surname>Szczur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behaviour &amp; Information Technology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="106" to="118" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Survey of User-Centered Design Practice</title>
		<author>
			<persName><forename type="first">K</forename><surname>Vredenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Carey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI 2002</title>
		<meeting>CHI 2002</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="472" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Helping and Hindering User Involvement -a Tale of Everyday Design</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bekker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI&apos;97</title>
		<meeting>CHI&apos;97</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Evaluating Usability Methods: Why the Current Literature Fails the Practitioner, interactions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wixon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="29" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Usability Testing in a Competive Market: Lessons Learned</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zirkler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Ballman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behaviour and Information Technology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="191" to="197" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note>The columns on the last page of</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
