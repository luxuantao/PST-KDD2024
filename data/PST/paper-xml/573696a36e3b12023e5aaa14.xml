<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SymNMF: nonnegative low-rank approximation of a similarity matrix for graph clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Da</forename><surname>Kuang</surname></persName>
							<email>da.kuang@cc.gatech.edu</email>
						</author>
						<author>
							<persName><forename type="first">•</forename><surname>Sangwoon</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Haesun</forename><surname>Park</surname></persName>
							<email>hpark@cc.gatech.edu</email>
						</author>
						<author>
							<persName><forename type="first">S</forename><surname>Yun</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computational Science and Engineering</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<postCode>30332-0765</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematics Education</orgName>
								<orgName type="institution">Sungkyunkwan University</orgName>
								<address>
									<addrLine>Jongro-gu</addrLine>
									<postCode>110-745</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Korea Institute for Advanced Study</orgName>
								<address>
									<addrLine>Dongdaemun-gu</addrLine>
									<postCode>130-722</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SymNMF: nonnegative low-rank approximation of a similarity matrix for graph clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">284F41208DC217421C0BEC35C7086FEA</idno>
					<idno type="DOI">10.1007/s10898-014-0247-2</idno>
					<note type="submission">Received: 27 January 2014 / Accepted: 20 October 2014</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nonnegative matrix factorization (NMF) provides a lower rank approximation of a matrix by a product of two nonnegative factors. NMF has been shown to produce clustering results that are often superior to those by other methods such as K-means. In this paper, we provide further interpretation of NMF as a clustering method and study an extended formulation for graph clustering called Symmetric NMF (SymNMF). In contrast to NMF that takes a data matrix as an input, SymNMF takes a nonnegative similarity matrix as an input, and a symmetric nonnegative lower rank approximation is computed. We show that SymNMF is related to spectral clustering, justify SymNMF as a general graph clustering method, and discuss the strengths and shortcomings of SymNMF and spectral clustering. We propose two optimization algorithms for SymNMF and discuss their convergence properties and computational efficiencies. Our experiments on document clustering, image clustering, and image segmentation support SymNMF as a graph clustering method that captures latent linear and nonlinear relationships in the data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dimension reduction and clustering are two of the key tasks in machine learning and data analytics. Suppose a collection of n data items with m features is represented in a matrix X ∈ R m×n . In a low rank approximation, we are given a desired reduced rank k which is typically much smaller than m and n, and we are to find C ∈ R m×k and G ∈ R n×k such that the difference between X and the product CG T is minimized as X ≈ CG T .</p><p>(</p><formula xml:id="formula_0">)<label>1</label></formula><p>This minimization problem can be formulated using various difference or distance measures.</p><p>In this paper, we will focus on the Frobenius norm based minimization, i.e. min C,G X -CG T F .</p><p>(</p><formula xml:id="formula_1">)<label>2</label></formula><p>In nonnegative Matrix factorization (NMF), nonnegativity is imposed on the factors C and G, i.e. we are to solve min</p><formula xml:id="formula_2">C≥0,G≥0 X -CG T 2 F ,<label>(3)</label></formula><p>where C ∈ R m×k + , G ∈ R n×k + , and R + denotes the set of nonnegative real numbers. The NMF can be defined for any matrix, but it makes more sense to consider NMF when the matrix X is nonnegative. Throughout this paper, we will assume that X ∈ R m×n + is nonnegative. NMF has been shown to be an effective method in numerous applications <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35]</ref>. In this paper, we will focus on the role of NMF as a clustering method. Note that NMF is posed as a constrained low rank approximation method, and accordingly, is a method for dimension reduction. However, dimension reduction and clustering are closely related. The following interpretation of the results of the low rank approximation illustrates this point: We consider the columns of C are the new basis for the reduced k-dimensional space for X , and each column of G T provides the k-dimensional representation of the corresponding column of X in the space spanned by the columns of C.</p><p>In the case of singular value decomposition (SVD), the columns of C are ordered in a way that the first column is the most dominant vector (the leading left singular vector) that captures the largest variation in the data, and the next column is the second most dominant vector and orthogonal to the leading singular vector, etc. Therefore, the columns of C do not "equally" represent the column space spanned by the data matrix X . In addition, the two factors C and G T can have negative elements, and thus it will be difficult to interpret the i-th column of G T as a "proportion distribution" with which the i-th data item has the component in the corresponding basis vector in C. On the other hand, the columns of C in NMF cannot have negative signs, and accordingly cannot "cancel out" some directions that the more dominant columns of C may represent. Accordingly, the columns of C more or less "equally" represent the data set and each column in the factor G T can be viewed as a distribution with which the i-th data item has the component in the corresponding column of C. Since we can use G T to derive an assignment of the n data points into k groups, clustering can be viewed as a special type of dimension reduction. The NMF gives a soft clustering result as explained above but we can also interpret the result as a hard clustering by assigning the i-th data point to the j-th cluster when the largest element among all components of the i-th column of G T lies in the j-th position. For example, when NMF is applied to document clustering, the basis vectors in C represent k topics, and the coefficients in the i-th column of G T indicate the degrees of membership for x i , the i-th document. NMF is well-known for the interpretability of the latent space it finds <ref type="bibr" target="#b37">[38]</ref>.</p><p>Another way to illustrate the capability of NMF as a clustering method is by observing its relationship to the objective function of the classical K-means clustering, which is arguably the most commonly used clustering method:</p><formula xml:id="formula_3">min n i=1 x i -c g i 2 2 ,<label>(4)</label></formula><p>where x 1 , . . . , x n are the columns of X, c 1 , . . . , c k are the k centroids, and g i = j when the i-th data point is assigned to the j-th cluster (1 ≤ j ≤ k). Consider K-means formulated as a dimension reduction problem <ref type="bibr" target="#b29">[30]</ref>:</p><formula xml:id="formula_4">min G∈{0,1} n×k ,G1 k =1 n X -CG T 2 F ,<label>(5)</label></formula><p>where 1 k ∈ R k×1 , 1 n ∈ R n×1 are column vectors whose elements are all 1's. In the formulation <ref type="bibr" target="#b4">(5)</ref>, columns of C are the cluster centroids, and the single nonzero element in each column of G T indicates the clustering assignment. NMF as a clustering method has been proved to be superior to K-means on many types of data, including documents <ref type="bibr" target="#b63">[64]</ref>, images <ref type="bibr" target="#b7">[8]</ref>, and microarray data <ref type="bibr" target="#b26">[27]</ref>. Although K-means and NMF have the same objective function X -CG T 2  F with different constraints, i.e. G ∈ {0, 1} n×k , G1 k = 1 n in the case of K-means, and C ≥ 0 and G ≥ 0 in the case of NMF, each has its best performance on different kinds of data sets. In order to apply NMF to the appropriate data sets, we must know the limitation of its capability in clustering. Most clustering methods have a clearly defined objective function to optimize such as <ref type="bibr" target="#b4">(5)</ref> and <ref type="bibr" target="#b2">(3)</ref>. However, clustering is difficult to formulate mathematically in order to discover the hidden pattern <ref type="bibr" target="#b32">[33]</ref>. Each clustering method has its own conditions under which it performs well. For example, K-means assumes that data points in each cluster follow a spherical Gaussian distribution <ref type="bibr" target="#b17">[18]</ref>. In contrast, the NMF formulation (3) provides a better low-rank approximation of the data matrix X than the K-means formulation <ref type="bibr" target="#b4">(5)</ref>.</p><p>If k ≤ rank(X ), since rank(X ) ≤ nonnegative-rank(X ) <ref type="bibr" target="#b3">[4]</ref> and the low rank approximation by NMF gives a smaller objective function value when the columns of C (the cluster representatives) are linearly independent, it is for the best interest of NMF to produce linearly independent cluster representatives. This explains our earlier discovery that NMF performs well when different clusters correspond to linearly independent vectors <ref type="bibr" target="#b33">[34]</ref>. The following artificial example illustrates this point. See Fig. <ref type="figure" target="#fig_0">1</ref>, where the two cluster centers are along the same direction therefore the two centroid vectors are linearly dependent. While NMF still approximates all the data points well in this example, no two linearly independent vectors in a two-dimensional space can represent the two clusters shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Since K-means and NMF have different conditions under which each of them does clustering well, they may generate very different clustering results in practice. We are motivated by Fig. <ref type="figure" target="#fig_0">1</ref> to mention that the assumption of spherical K-means is that data points in each cluster follow a von Mises-Fisher distribution <ref type="bibr" target="#b2">[3]</ref>, which is similar to the assumption of NMF.</p><p>Therefore, NMF, originally a dimension reduction method, is not always a preferred clustering method. The success of NMF as a clustering method depends on the underlying data set, and its most success has been around document clustering <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b63">64]</ref>. In a document data set, data points are often represented as unit-length vectors <ref type="bibr" target="#b46">[47]</ref> and embedded in a linear subspace. For a term-document matrix X , a basis vector c j is interpreted as the term distribution of a single topic. As long as the term distributions of k topics are linearly independent, which are usually the case, NMF can extract the ground-truth clusters well. However, NMF has not been as successful in image clustering. For image data, it was shown that a collection of images tends to form multiple 1-dimensional nonlinear manifolds <ref type="bibr" target="#b59">[60]</ref>, one manifold for each cluster. This does not satisfy NMF's assumption on cluster structures, and therefore NMF may not identify correct clusters.</p><p>In this paper, we study a more general formulation for clustering based on NMF, called Symmetric NMF (SymNMF), where an n × n nonnegative and symmetric matrix A is given as an input instead of a nonnegative data matrix X . The matrix A contains pairwise similarity values of a similarity graph, and is approximated by a lower rank matrix HH T instead of the product of two lower rank matrices CG T . High-dimensional data such as documents and images are often embedded in a low-dimensional space, and the embedding can be extracted from their graph representation. We will demonstrate that SymNMF can be used for graph embedding and clustering and often performs better than spectral methods in terms of multiple evaluation measures for clustering.</p><p>The rest of this paper is organized as follows. In Sect. 2, we review previous work on nonnegative factorization of a symmetric matrix and introduce the novelty of the directions proposed in this paper. In Sect. 3, we present our new interpretation of SymNMF as a clustering method. In Sect. 4, we show the difference between SymNMF and spectral clustering in terms of their dependence on the spectrum. In Sects. 5 and 6, we propose two algorithms for SymNMF: A Newton-like algorithm and an alternating nonnegative least squares (ANLS) algorithm, and discuss their efficiency and convergence properties. In Sect. 7, we report experiment results on document and image clustering that illustrate that SymNMF is a competitive method for graph clustering. In Sect. 8, we apply SymNMF to image segmentation and show the unique properties of the obtained segments. In Sect. 9, we discuss future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>In SymNMF, we look for the solution</p><formula xml:id="formula_5">H ∈ R n×k + , min H ≥0 f (H ) = A -HH T 2 F ,<label>(6)</label></formula><p>given A ∈ R n×n + with A T = A and k. The integer k is typically much smaller than n. In our graph clustering setting, A is called a similarity matrix: The (i, j)-th entry of A is the similarity value between the i-th and j-th nodes in a similarity graph, or the similarity value between the i-th and j-th data items.</p><p>The above formulation has been studied in a number of previous papers. Ding et al. <ref type="bibr" target="#b14">[15]</ref> transformed the formulation of NMF (3) to a symmetric approximation A -HH T 2  F where A is a positive semi-definite matrix, and showed that it has the same form as the objective function of spectral clustering. Li et al. <ref type="bibr" target="#b40">[41]</ref> used this formulation for semi-supervised clustering where the similarity matrix was modified with prior information. Zass and Shashua <ref type="bibr" target="#b68">[69]</ref> converted a completely positive matrix <ref type="bibr" target="#b4">[5]</ref> to a symmetric doubly stochastic matrix A and used the formulation (6) to find a nonnegative H for probabilistic clustering. They also gave a reason why the nonnegativity constraint on H was more important than the orthogonality constraint in spectral clustering. He et al. <ref type="bibr" target="#b22">[23]</ref> approximated a completely positive matrix directly using the formulation <ref type="bibr" target="#b5">(6)</ref> with parallel update algorithms. In all of the above work, A was assumed to be a positive semi-definite matrix. Other related work that imposed additional constraints on H includes <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b65">66]</ref>.</p><p>The SymNMF formulation has also been applied to non-overlapping and overlapping community detection in real networks <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b71">72]</ref>. For example, Nepusz et al. <ref type="bibr" target="#b49">[50]</ref> proposed a formulation similar to <ref type="bibr" target="#b5">(6)</ref> with sum-to-one constraints to detect soft community memberships; Zhang et al. <ref type="bibr" target="#b71">[72]</ref> proposed a binary factorization model for overlapping communities and discussed the pros and cons of hard/soft assignments to communities. The adjacency matrix A involved in community detection is often an indefinite matrix.</p><p>Catral et al. <ref type="bibr" target="#b8">[9]</ref> studied whether W H T is symmetric and W = H , when W and H are the global optimum for the problem min W,H ≥0 A -W H T 2  F where A is nonnegative and symmetric. Ho <ref type="bibr" target="#b23">[24]</ref> in his thesis related SymNMF to the exact SymNMF problem A = HH T . Both of their theories were developed outside the context of graph clustering, and their topics are beyond the scope of this paper. Ho <ref type="bibr" target="#b23">[24]</ref> also proposed a 2n-block coordinate descent algorithm for <ref type="bibr" target="#b5">(6)</ref>. Compared to our two-block coordinate descent framework described in Sect. 6, Ho's approach introduced a dense n × n matrix which destroys the sparsity pattern in A and is not scalable.</p><p>Almost all the work mentioned above employed multiplicative update algorithms to optimize their objective functions with nonnegativity constraints. However, this type of algorithms does not have the property that every limit point is a stationary point <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b41">42]</ref>, and accordingly their solutions are not guaranteed to be local minima. In fact, the results of multiplicative update algorithms (e.g. <ref type="bibr" target="#b15">[16]</ref>) only satisfy part of the KKT condition, but do not satisfy all the components of the KKT condition, for example, the sign of the gradient vector. In the three papers <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b70">71]</ref> that used gradient descent methods for optimization that did reach stationary point solutions, they performed the experiments only on graphs with up to thousands of nodes.</p><p>In this paper, we study the formulation (6) from different angles:</p><p>1. We focus on a more general case where A is a symmetric indefinite matrix and represents a general graph. Examples of such an indefinite matrix include a similarity matrix for highdimensional data formed by the self-tuning method <ref type="bibr" target="#b69">[70]</ref> as well as the pixel similarity matrix in image segmentation <ref type="bibr" target="#b55">[56]</ref>. Real networks have additional structures such as the scale-free properties <ref type="bibr" target="#b58">[59]</ref>, and we will not include them in this work. 2. We focus on hard clustering and will give an intuitive interpretation of SymNMF as a graph clustering method. Hard clustering offers more explicit membership and easier visualization than soft clustering <ref type="bibr" target="#b71">[72]</ref>. Unlike <ref type="bibr" target="#b14">[15]</ref>, we emphasize the difference between SymNMF and spectral clustering instead of their resemblance. 3. We propose two optimization algorithms that converge to stationary point solutions for SymNMF, namely Newton-like algorithm and ANLS algorithm. We also show that the new ANLS algorithm scales better to large data sets.</p><p>4. In addition to experiments on document and image clustering, we apply SymNMF to image segmentation using 200 images in the Berkeley Segmentation Data Set <ref type="bibr" target="#b0">[1]</ref>. To the best of our knowledge, our work represents the first attempt for a thorough evaluation of nonnegativity-based methods for image segmentation.</p><p>Overall, we conduct a comprehensive study of SymNMF in this paper, covering from foundational justification for SymNMF for clustering, convergent and scalable algorithms, to real-life applications for text and image clustering as well as image segmentation. The Newton-like algorithm and some of the analysis of spectral clustering was first proposed in our previous work <ref type="bibr" target="#b33">[34]</ref>. We include them in this paper for completeness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Interpretation of SymNMF as a graph clustering method</head><p>Just as the nonnegativity constraint in NMF makes it interpretable as a clustering method, the nonnegativity constraint H ≥ 0 in (6) also gives a natural partitioning as well as interpretation of SymNMF. Now we provide an intuitive explanation of why this formulation is expected to extract cluster structures.</p><p>Figure <ref type="figure">2</ref> shows an illustrative example of SymNMF, where we have rearranged the rows and columns of A without loss of generality. If a similarity matrix has cluster structures embedded in it, several diagonal blocks (two diagonal blocks in Fig. <ref type="figure">2</ref>) with large similarity values will appear. In order to approximate this similarity matrix with low-rank matrices and simultaneously extract cluster structures, we can approximate these diagonal blocks separately because each diagonal block indicates one cluster. As shown in Fig. <ref type="figure">2</ref>, it is straightforward to use an outer product hh T to approximate a diagonal block. Because h is a nonnegative vector, it serves as a cluster membership indicator: Larger values in h indicate stronger memberships to the cluster corresponding to the diagonal block. When multiple such outer products are added up together, they approximate the original similarity matrix, and each column of H represents one cluster. Due to the nonnegativity constraints in SymNMF, only "additive", or "non-subtractive", summation of rank-1 matrices is allowed to approximate both diagonal and off-diagonal blocks.</p><p>On the contrary, Fig. <ref type="figure">3</ref> illustrates the result of low-rank approximation of A without nonnegativity constraints. In this case, when using multiple hh T outer products to approximate A, cancellations of positive and negative numbers are allowed. Without nonnegativity enforced on h's, the diagonal blocks need not be approximated separately by each term hh T . The elements in a vector h can have any sign (+, 0, -) and magnitude, though the summation of all hh T terms, i.e. HH T , can approximate the large diagonal blocks and small off-diagonal blocks well. Thus, h cannot serve as a cluster membership indicator. In this case, the rows of the low-rank matrix H contain both positive and negative numbers and can be used for graph embedding. In order to obtain hard clusters, we need to post-process the embedded data points such as applying K-means clustering. This reasoning is analogous to the contrast between NMF and SVD <ref type="bibr" target="#b37">[38]</ref>.</p><p>SymNMF is flexible in terms of choosing similarities between data points. We can choose any similarity measure that describes the cluster structure well. In fact, the formulation of NMF (3) can be related to SymNMF when A = X T X in (6) <ref type="bibr" target="#b14">[15]</ref>. This means that NMF implicitly chooses inner products as the similarity measure, which is not always suitable to distinguish different clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SymNMF and spectral clustering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Objective functions</head><p>Spectral clustering represents a large class of graph clustering methods that rely on eigenvector computation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b55">56]</ref>. Now we will show that spectral clustering and SymNMF are closely related in terms of the graph clustering objective but fundamentally different in optimizing this objective.</p><p>Many graph clustering objectives can be reduced to a trace maximization form <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b35">36]</ref>:</p><formula xml:id="formula_6">max trace( H T A H ),<label>(7)</label></formula><p>where H ∈ R n×k (to be distinguished from H in the SymNMF formulation) satisfies H T H = I, H ≥ 0, and each row of H contains one positive entry and at most one positive entry due to H T H = I . Clustering assignments can be drawn from H accordingly. Under the constraints on H T H = I, H ≥ 0, we have <ref type="bibr" target="#b14">[15]</ref>:</p><formula xml:id="formula_7">max trace H T A H ⇔ min trace(A T A) -2trace H T A H + trace(I ) ⇔ min trace A -H H T T A -H H T ⇔ min A -H H T 2 F .</formula><p>This objective function is the same as <ref type="bibr" target="#b5">(6)</ref>, except that the constraints on the low-rank matrices H and H are different. The constraint on H makes the graph clustering problem NP-hard <ref type="bibr" target="#b55">[56]</ref>, therefore a practical method relaxes the constraint to obtain a tractable formulation. In this respect, spectral clustering and SymNMF can be seen as two different ways of relaxation: While spectral clustering retains the constraint H T H = I , SymNMF retains H ≥ 0 instead. These two choices lead to different algorithms for optimizing the same graph clustering objective <ref type="bibr" target="#b6">(7)</ref>, which are shown in Table <ref type="table" target="#tab_0">1</ref>. </p><formula xml:id="formula_8">Ĥ =I A -Ĥ Ĥ T 2 F min H ≥0 A -HH T 2 F</formula><p>Step 1</p><p>Obtain the global optimal Ĥn×k by computing k leading eigenvectors of A Obtain a solution H using an optimization algorithm</p><p>Step 2 Scale each row of Ĥ (No need to scale rows of H )</p><p>Step 3 Apply a clustering algorithm to the the rows of Ĥ , a k-dimensional embedding</p><p>The largest entry in each row of H indicates the clustering assignments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Spectral clustering and the spectrum</head><p>Normalized cut is a widely-used objective for spectral clustering <ref type="bibr" target="#b55">[56]</ref>. Now we describe some scenarios where optimizing this objective may have difficulty in identifying correct clusters while SymNMF could be potentially better.</p><p>Although spectral clustering is a well-established framework for graph clustering, its success relies only on the properties of the leading eigenvalues and eigenvectors of the similarity matrix A. It was pointed out in <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b57">58]</ref> that the k-dimensional subspace spanned by the leading k eigenvectors of A is stable only when |λ k (A)-λ k+1 (A)| is sufficiently large, where λ i (A) is the i-th largest eigenvalue of A. Now we show that spectral clustering could fail when this condition is not satisfied but the cluster structure may be perfectly represented in the block-diagonal structure of A. Suppose A is composed of k = 3 diagonal blocks, corresponding to three clusters:</p><formula xml:id="formula_9">A = ⎡ ⎣ A 1 0 0 0 A 2 0 0 0 A 3 ⎤ ⎦ . (<label>8</label></formula><formula xml:id="formula_10">)</formula><p>If we construct A as in the normalized cut, then each of the diagonal blocks A 1 , A 2 , A 3 has a leading eigenvalue 1. We further assume that λ 2 (A i ) &lt; 1 for all i = 1, 2, 3 in exact arithmetic. Thus, the three leading eigenvectors of A correspond to the diagonal blocks A 1 , A 2 , A 3 respectively. However, when λ 2 (A 1 ) and λ 3 (A 1 ) are so close to 1 that it cannot be distinguished from λ 1 (A 1 ) in finite precision arithmetic, it is possible that the computed eigenvalues</p><formula xml:id="formula_11">λ j (A i ) satisfy λ1 (A 1 ) &gt; λ2 (A 1 ) &gt; λ3 (A 1 ) &gt; max( λ1 (A 2 ), λ1 (A 3 )).</formula><p>In this case, three subgroups are identified within the first cluster; the second and the third clusters cannot be identified, as shown in Fig. <ref type="figure">4</ref> where all the data points in the second and third clusters are mapped to (0, 0, 0). Therefore, eigenvectors computed in a finite precision cannot always capture the correct low-dimensional graph embedding. Now we demonstrate the above scenario using a concrete graph clustering example.  <ref type="figure"></ref>and<ref type="figure">(c, d</ref>) plots of the similarity matrix A. Suppose the scattered points form the first cluster, and the two tightly-clustered groups correspond to the second and third clusters. We employ the widely-used Gaussian kernel <ref type="bibr" target="#b60">[61]</ref> and normalized similarity values <ref type="bibr" target="#b55">[56]</ref>: </p><formula xml:id="formula_12">e i j = exp - x i -x j 2 2 σ 2 , A i j = e i j d -1/2 i d -1/2 j , (<label>9</label></formula><formula xml:id="formula_13">)</formula><p>where x i 's are the two-dimensional data points,</p><formula xml:id="formula_14">d i = n s=1 e is (1 ≤ i ≤ n),</formula><p>and σ is a parameter set to 0.05 based on the scale of data points. In spectral clustering, the rows of  In this example, the original data points are mapped to three unique points in a new space. However, the three points in the new space do not correspond to the three clusters in Fig. <ref type="figure">5a</ref>. In fact, out of the 303 data points in total, 290 data points are mapped to a single point in the new space.</p><p>Let us examine the leading eigenvalues, shown in Table <ref type="table" target="#tab_1">2</ref>, where the fourth largest eigenvalue of A is very close to the third largest eigenvalue. This means that the second largest eigenvalue of a cluster, say λ 2 (A 1 ), would be easily identified as one of λ 1 (A 1 ), λ 1 (A 2 ), and λ 1 (A 3 ). The mapping of the original data points shown in Fig. <ref type="figure">5b</ref> implies that the computed three largest eigenvalues come from the first cluster. This example is a noisier case of the scenario in Fig. <ref type="figure">4</ref>.</p><p>On the contrary, we can see from Fig. <ref type="figure">5c</ref>, d that the block-diagonal structure of A is clear, though the within-cluster similarity values are not on the same scale. Figure <ref type="figure">6</ref> shows the comparison of clustering results of spectral clustering and SymNMF in this case. SymNMF is able to separate the two tightly-clustered groups more accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">A condition on SymNMF</head><p>How does the spectrum of A possibly influence SymNMF? We have seen that the solution of SymNMF relies on the block-diagonal structure of A, thus it does not suffer from the situations in Sect. 4.2 where the eigengap between the k-th and (k + 1)-th eigenvalues is small. We will also see in later sections that algorithms for SymNMF do not depend on eigenvector computation. However, we do emphasize a condition that SymNMF must satisfy in order to make the formulation <ref type="bibr" target="#b5">(6)</ref> valid. This condition is related to the spectrum of A, specifically the number of nonnegative eigenvalues of A. Note that A is only assumed to be symmetric and nonnegative, and is not necessarily positive semi-definite, therefore may have both positive and negative eigenvalues. On the other hand, in the approximation A -HH T F , HH T is always positive semi-definite and has rank at most k, therefore HH T would not be a good Algorithm 1 Framework of the Newton-like algorithm for SymNMF: min H ≥0 f (x) = A -HH T 2 F 1: Input: number of data points n, number of clusters k, n ×n similarity matrix A, reduction factor 0 &lt; β &lt; 1, acceptance parameter 0 &lt; σ &lt; 1, and tolerance parameter 0 &lt; μ &lt;&lt; 1 2: Initialize x, x (0) ← x 3: repeat 4: Compute scaling matrix S 5:</p><p>Step size α = 1 6: while true do 7:</p><formula xml:id="formula_15">x new = [x -αS∇ f (x)] + 8: if f (x new ) -f (x) ≤ σ ∇ f (x) T (x new -x) then 9: break 10: end if 11:</formula><p>α ← βα 12: end while 13: x ← x new 14: until ∇ P f (x) ≤ μ ∇ P f (x (0) ) 15: Output: x approximation if A has fewer than k nonnegative eigenvalues. We assume that A has at least k nonnegative eigenvalues when the given size of H is n × k.</p><p>This condition on A could be expensive to check. Here, by a simple argument, we claim that it is practically reasonable to assume that this condition is satisfied given a similarity matrix. Again, we use the similarity matrix A in (8) as an example. Suppose we know the actual number of clusters is three, and therefore H has size n × 3. Because A is nonnegative, each of A 1 , A 2 , A 3 has at least one nonnegative eigenvalue according to Perron-Frobenius theorem <ref type="bibr" target="#b3">[4]</ref>, and A has at least three nonnegative eigenvalues. In a real data set, A may become much noisier with small entries in the off-diagonal blocks of A. The eigenvalues are not dramatically changed by a small perturbation of A according to matrix perturbation theory <ref type="bibr" target="#b57">[58]</ref>, hence A would also have at least k nonnegative eigenvalues if its noiseless version does. In practice, the number of positive eigenvalues of A is usually much larger than that of negative eigenvalues, which is verified in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">A Newton-like algorithm for SymNMF</head><p>In this section, we will present an optimization algorithm to compute SymNMF where A is nonnegative and symmetric. The objective function in ( <ref type="formula" target="#formula_5">6</ref>) is a fourth-order non-convex function with respect to the entries of H , and has multiple local minima. For this type of problem, it is difficult to find a global minimum; thus a good convergence property we can expect is that every limit point is a stationary point <ref type="bibr" target="#b6">[7]</ref>. We could directly apply standard gradient search algorithms, which lead to stationary point solutions; however, they suffer from either slow convergence or expensive computation cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Algorithm framework</head><p>First, we introduce our notations for clarity.</p><formula xml:id="formula_16">Let H = [h 1 , . . . , h k ] ∈ R n×k + . A vector x of length nk is used to represent the vectorization of H by column, i.e. x = vec(H ) = [h T 1 , . . . , h T k ] T ∈ R nk×1 + .</formula><p>For simplicity, functions applied on x have the same notation as functions applied on H , i.e. f (x) ≡ f (H ). [•] + denotes the projection to the nonnegative </p><formula xml:id="formula_17">S (t) = I nk×nk S (t) = ∇ 2 E f (x (t) ) -1 Convergence Linear (zigzagging) Quadratic Complexity O(n 2 k)/iteration O(n 3 k 3 )/iteration</formula><p>orthant, i.e. replacing any negative element of a vector to be 0. Superscripts denote iteration indices, e.g.</p><formula xml:id="formula_18">x (t) = vec(H (t)</formula><p>) is the iterate of x in the t-th iteration. For a vector v, v i denotes its i-th element. For a matrix M, M i j denotes its (i, j)-th entry; and</p><formula xml:id="formula_19">M [i][ j]</formula><p>denotes its (i, j)th n × n block, assuming that both the numbers of rows and columns of M are multiples of n. M 0 refers to positive definiteness of M. We define the projected gradient ∇ P f (x) at x as <ref type="bibr" target="#b42">[43]</ref>:</p><formula xml:id="formula_20">∇ P f (x) i = (∇ f (x)) i , if x i &gt; 0; (∇ f (x)) i + , if x i = 0. (<label>10</label></formula><formula xml:id="formula_21">)</formula><p>Algorithm 1 describes a framework of gradient search algorithms applied to SymNMF, based on which we developed our Newton-like algorithm. This description does not specify iteration indices, but updates x in-place. The framework uses the "scaled" negative gradient direction as search direction. Except the scalar parameters β, σ, μ, the nk ×nk scaling matrix S (t) is the only unspecified quantity. Table <ref type="table" target="#tab_2">3</ref> lists two choices of S (t) that lead to different gradient search algorithms: projected gradient descent (PGD) <ref type="bibr" target="#b42">[43]</ref> and projected Newton (PNewton) <ref type="bibr" target="#b6">[7]</ref>.</p><p>PGD sets S (t) = I throughout all the iterations. It is known as one of steepest descent methods, and does not scale the gradient using any second-order information. This strategy often suffers from the well-known zigzagging behavior, thus has slow convergence rate <ref type="bibr" target="#b6">[7]</ref>. On the other hand, PNewton exploits second-order information provided by the Hessian ∇ 2 f (x (t) ) as much as possible. PNewton sets S (t) to be the inverse of a reduced Hessian at x (t) . The reduced Hessian with respect to index set R is defined as:</p><formula xml:id="formula_22">(∇ 2 R f (x)) i j = δ i j , if i ∈ R or j ∈ R; ∇ 2 f (x) i j , otherwise,<label>(11)</label></formula><p>where δ i j is the Kronecker delta. Both the gradient and the Hessian of f (x) can be computed analytically:</p><formula xml:id="formula_23">∇ f (x) = vec 4(HH T -A)H , ∇ 2 f (x) [i][ j] = 4 δ i j (HH T -A) + h j h T i + (h T i h j )I n×n .</formula><p>We introduce the definition of an index set E that helps to prove the convergence of Algorithm 1 <ref type="bibr" target="#b6">[7]</ref>:</p><formula xml:id="formula_24">E = {i|0 ≤ x i ≤ , (∇ f (x)) i &gt; 0} , (<label>12</label></formula><formula xml:id="formula_25">)</formula><p>where depends on x and is usually small (0 &lt; &lt; 0.01) <ref type="bibr" target="#b25">[26]</ref>. In PNewton, S (t) is formed based on the reduced Hessian ∇ 2 E f (x (t) ) with respect to E. However, because the computation of the scaled gradient S (t) ∇ f (x (t) ) involves the Cholesky factorization of the reduced Hessian, PNewton has a very large computational complexity of O(n 3 k 3 ), which is prohibitive. Therefore, we propose a Newton-like algorithm that exploits second-order information in an inexpensive way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Improving the scaling matrix</head><p>The choice of the scaling matrix S (t) is essential to an algorithm that can be derived from the framework described in Algorithm 1. We propose two improvements on the choice of S (t) , yielding new algorithms for SymNMF. Our focus is to efficiently collect partial second-order information but meanwhile still effectively guide the scaling of the gradient direction. Thus, these improvements seek a tradeoff between convergence rate and computational complexity, with the goal of accelerating SymNMF algorithms as an overall outcome.</p><p>Our design of new algorithms must guarantee the convergence. Since the algorithm framework still follows Algorithm 1, we would like to know what property of the scaling matrix S (t) is essential in the proof of the convergence result of PGD and PNewton. This property is described by the following lemma:</p><formula xml:id="formula_26">Definition 1 A scaling matrix S is diagonal with respect to an index set R, if S i j = 0, ∀i ∈ R and j = i [6].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 1 Let S be a positive definite matrix which is diagonal with respect to</head><formula xml:id="formula_27">E. If x ≥ 0 is not a stationary point, there exists ᾱ &gt; 0 such that f [x -αS∇ f (x)] + &lt; f (x), ∀0 &lt; α &lt; ᾱ. (modified from [6])</formula><p>Lemma 1 states the requirement on S (t) , which is satisfied by the choices of S (t) in both PGD and PNewton. It guides our development of new ways to choose S (t) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Improvement 1: fewer Hessian evaluations</head><p>A common method for reducing computation cost related to S (t) is to periodically update S (t)  or evaluate S (t) only at the 1st iteration (chord method) <ref type="bibr" target="#b25">[26]</ref>. However, this method cannot be directly used in the framework of Algorithm 1, because S (t) is not necessarily diagonal with respect to E (t) if E (t) = E (1) , and the requirement for convergence is violated.</p><p>Our way to delay the update of S (t) is to evaluate S (t) only when E (t) changes. More precisely,</p><formula xml:id="formula_28">S (t) = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ S (t-1) , if E (t) = E (t-1) ; ∇ 2 E f (x (t) ) -1 , if E (t) = E (t-1)</formula><p>and</p><formula xml:id="formula_29">∇ 2 E f (x (t) ) 0; I nk×nk , otherwise.<label>(13)</label></formula><p>Note that because f (x) is non-convex, we have to set</p><formula xml:id="formula_30">S (t) = I when ∇ 2 E f (x (t)</formula><p>) is not positive definite, which can be checked during its Cholesky factorization. We expect that this improvement can reduce the number of Hessian evaluations and Cholesky factorizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Improvement 2: cheaper Hessian evaluations</head><p>The second improvement in choosing S (t) is inspired by the recently proposed coordinate gradient descent (CGD) method for solving covariance selection <ref type="bibr" target="#b67">[68]</ref>. When CGD is directly applied to SymNMF, it updates one column of H in each iteration while the other columns are fixed, and the search direction is typically determined by solving a quadratic programming problem. The CGD method introduces additional overhead when determining the search direction; however, it implies a possibility of using second-order information without evaluating the entire Hessian.</p><p>Inspired by the incremental update framework of CGD, we propose to choose S (t) to be a block-diagonal matrix in our batch update framework in Algorithm 1. Specifically,</p><formula xml:id="formula_31">S (t) [i][ j] = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 0, if i = j; ∇ 2 E f (x (t) ) [i][ j] -1 , if i = j and ∇ 2 E f (x (t) ) [i][ j] 0; I n×n , otherwise.<label>(14)</label></formula><p>Intuitively speaking, the i-th n × n diagonal block of S (t) corresponds to variables in the i-th column of H , and S (t) only involves second-order information within each column of H . This choice of S (t) has two advantages over the choice in PNewton algorithm: First, the computational complexity in each iteration is O(n 3 k), much lower than the complexity of PNewton if k is not too small. Second, we can exploit partial second-order information even though the n diagonal blocks of ∇ 2 E f (x (t) ) are not all positive definite, whereas PNewton requires the positive definiteness of all the n diagonal blocks as a necessary condition.</p><p>Our final strategy for solving SymNMF ( <ref type="formula" target="#formula_5">6</ref>) is to combine Improvement 1 and Improvement 2. Note that the requirement on S (t) described in Lemma 1 is satisfied in both of the improvements, and also in their combination. Thus, convergence is guaranteed in all of these variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">An ANLS algorithm for SymNMF</head><p>In this section, we propose another optimization algorithm for SymNMF that converges to stationary points, a necessary condition for local minima. The algorithm is based on an alternative formulation of SymNMF, where it becomes straightforward to use the two-block coordinate descent framework that has been shown efficient for standard NMF <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b42">43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Two-block coordinate descent framework</head><p>We first briefly review the two-block coordinate descent framework <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b42">43]</ref> for standard NMF problems shown in (3): min C≥0,G≥0 X -CG T 2  F , which has our desired convergence property that every limit point is a stationary point.</p><p>Separating the unknowns in C and G in the NMF formulation (3) into two blocks, we obtain the following subproblems:</p><p>1. Fix G and solve min C≥0 GC T -X T 2 F . 2. Fix C and solve min G≥0 CG T -X 2 F . Each subproblem is a nonnegative least squares problem with multiple right-hand sides (NLS for short), and many efficient procedures have been developed to solve NLS, e.g. active-set method <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b36">37]</ref>, block pivoting <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>, PGD <ref type="bibr" target="#b42">[43]</ref>, etc. The key requirement in this framework is to obtain the optimal solution in each subproblem (see more discussions in <ref type="bibr" target="#b27">[28]</ref>). This way, the original NMF formulation (3) has been reduced to an alternating NLS problem (ANLS for short).  0) , H (0) ) F 7: Output: H</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Framework of the ANLS algorithm for SymNMF: min W,H ≥0</head><formula xml:id="formula_32">A -W H T 2 F + α W -H 2</formula><formula xml:id="formula_33">← arg min H ≥0 W √ α I k H T - A √ αW T F 6: until ∇ P g(W, H ) F ≤ μ ∇ P g(W (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">A nonsymmetric formulation for SymNMF</head><p>In SymNMF, it is difficult to separate the nk unknowns in a straightforward way as in NMF, because the two factors H and H T contain the same set of unknowns. We propose to reformulate SymNMF in the context of NMF <ref type="bibr" target="#b23">[24]</ref>:</p><formula xml:id="formula_34">min W,H ≥0 g(W, H ) = A -W H T 2 F + α W -H 2 F , (<label>15</label></formula><formula xml:id="formula_35">)</formula><p>where A still represents the n × n similarity matrix, W, H are two low-rank factors of size n × k, and α &gt; 0 is a scalar parameter for the tradeoff between the approximation error and the difference of W and H . Here we force the separation of unknowns by associating the two factors with two different matrices. If α has a large enough value, the solutions of W and H will be close enough so that the clustering results will not be affected whether W or H are used as the clustering assignment matrix.</p><p>The nonsymmetric formulation can be easily cast into the two-block coordinate descent framework after some restructuring. In particular, we have the following subproblems for <ref type="bibr" target="#b14">(15)</ref>:</p><formula xml:id="formula_36">min W ≥0 H √ α I k W T - A √ α H T F ,<label>(16)</label></formula><p>min</p><formula xml:id="formula_37">H ≥0 W √ α I k H T - A √ αW T F , (<label>17</label></formula><formula xml:id="formula_38">)</formula><p>where 1 k ∈ R k×1 is a column vector whose elements are all 1's, and I k is the k × k identity matrix. Note that we have assumed A = A T . Solving subproblems ( <ref type="formula" target="#formula_36">16</ref>) and ( <ref type="formula" target="#formula_37">17</ref>) in an alternate fashion will lead to a stationary point solution, as long as an optimal solution is returned for every NLS subproblem encountered. We simplify and summarize this algorithm in Algorithm 2.</p><p>If W and H are expected to indicate more distinct cluster structures, sparsity constraints on the rows of W and H can also be incorporated into the nonsymmetric formulation easily, by adding L 1 regularization terms <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>:</p><formula xml:id="formula_39">min W,H ≥0 g(W, H ) = A -W H T 2 F + α W -H 2 F + β n i=1 w i 2 1 + β n i=1 h i 2 1 , (<label>18</label></formula><formula xml:id="formula_40">)</formula><p>where α, β &gt; 0 are regularization parameters, w i , h i are the i-th rows of W, H respectively, and • 1 denotes vector 1-norm. Consequently, the two subproblems for <ref type="bibr" target="#b17">(18)</ref> in the two-block coordinate descent framework are:</p><formula xml:id="formula_41">min W ≥0 ⎡ ⎣ H √ α I k √ β1 T k ⎤ ⎦ W T - ⎡ ⎣ A √ α H T 0 ⎤ ⎦ F ,<label>(19)</label></formula><p>min</p><formula xml:id="formula_42">H ≥0 ⎡ ⎣ W √ α I k √ β1 T k ⎤ ⎦ H T - ⎡ ⎣ A √ αW T 0 ⎤ ⎦ F . (<label>20</label></formula><formula xml:id="formula_43">)</formula><p>We can even use just one L 1 regularization term in <ref type="bibr" target="#b17">(18)</ref>, that is,</p><formula xml:id="formula_44">β n i=1 w i 2 1 or β n i=1 h i 2 1</formula><p>, since W and H are sufficiently close; however, using one or two L 1 regularization terms does not make much difference computationally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Implementation</head><p>Now we describe an efficient implementation of the ANLS algorithm for SymNMF. Our algorithm reduces to solving the NLS problem in line 5 of Algorithm 2. Consider a form of NLS with simplified notation: min G≥0 CG T -X 2 F . In many algorithms for NLS, the majority of time cost comes from the computation of C T C and X T C. For example, in the active-set method <ref type="bibr" target="#b27">[28]</ref> and block pivoting method <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>, we need to form the normal equation:</p><formula xml:id="formula_45">C T CG T = C T X.</formula><p>In PGD <ref type="bibr" target="#b42">[43]</ref>, we need to compute the gradient:</p><formula xml:id="formula_46">∇ G = 2G(C T C) -2X T C.</formula><p>For more details of these algorithms for NLS, please refer to the original papers <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Our strategy to solve the NLS problem in Algorithm 2 is to precompute C T C and X T C:</p><formula xml:id="formula_47">C T C = W T W + α I k , X T C = A T W + αW</formula><p>without forming X = A √ αW T directly. Though this change sounds trivial, it is very costly to form X directly when A is large and sparse, especially when A is stored in the "compressed sparse column" format such as in Matlab and the Python scipy package. In our experiments, we observed that our strategy had considerable time savings in the iterative Algorithm 2.</p><p>For choosing the parameter α, we can gradually increase α from 1 to a very large number, for example, by setting α ← 1.01α. We can stop increasing α when W -H F / H F is negligible (say, &lt;10 -8 ).</p><p>Theoretically, both the Newton-like algorithm and the ANLS algorithm are valid algorithms for SymNMF for any nonnegative and symmetric matrix A. In practice, however, when a similarity matrix A is very sparse and the efficiencies of these two algorithms become very different. The Newton-like algorithm does not take into account the structure of SymNMF formulation <ref type="bibr" target="#b5">(6)</ref>, and a sparse input matrix A cannot contribute to speeding up the algorithm because of the formation of the dense matrix HH T in intermediate steps. On the contrary, in the ANLS algorithm, many algorithms for the NLS subproblem <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b42">43]</ref> can often benefit from the sparsity of similarity matrix A automatically. This benefit comes from sparsedense matrix multiplication inside these algorithms such as AH as well as the absence of large dense matrices such as HH T . Therefore, we recommend using the ANLS algorithm for a sparse input matrix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments on document and image clustering</head><p>In this section, we show the performances of SymNMF on a number of text and image data sets, and compare SymNMF with the standard forms and variations of NMF, spectral clustering, and K-means. The SymNMF formulation is a nonconvex minimization problem. If we apply Newton-like algorithm or ANLS algorithm which is described in Sects. 5 and 6 to SymNMF, then it can find a local minimal solution but may not find a global one. Hence we need a global optimization method. Our proposed global optimization method for experiments on document and image clustering is based on a multistart global optimization algorithm <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54]</ref> that combines random sampling with a local search procedure. That is, we choose 20 initial points uniformly within the nonnegative orthant and a local search procedure is applied to every initial point for improving it. We use Newton-like algorithm and ANLS algorithm for our local search method. Throughout the experiments, we use Matlab 7.9 (R2009b) with an Intel Xeon X5550 quad-core processor and 24GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Data preparation</head><p>We construct a sparse graph for each data set. Using sparse graphs makes large-scale clustering possible in terms of efficiency. We take the following three steps to form the similarity matrix:</p><p>1. Construct a complete graph. The edge weights between graph nodes are defined according to the type of data set.</p><p>-For text data, all the document vectors are normalized to have unit 2-norm. The edge weight is the cosine similarity between two document vectors:</p><formula xml:id="formula_48">e i j = x T i x j , (i = j). (<label>21</label></formula><formula xml:id="formula_49">)</formula><p>-For image data, the self-tuning method <ref type="bibr" target="#b69">[70]</ref> is used:</p><formula xml:id="formula_50">e i j = exp - x i -x j 2 2 σ i σ j , (i = j), (<label>22</label></formula><formula xml:id="formula_51">)</formula><p>where each data point has a local scale σ i , as opposed to a global scale σ in <ref type="bibr" target="#b8">(9)</ref>. σ i is set to be the Euclidean distance between x i and its k-th neighbor. We use k = 7 as suggested in <ref type="bibr" target="#b69">[70]</ref>.</p><p>Note that we enforce self-edge weights e ii = 0 (1 ≤ i ≤ n) in all cases <ref type="bibr" target="#b50">[51]</ref>. 2. Sparsify the graph. We only keep the edges that connect a node to its q nearest neighbors.</p><p>More precisely, let</p><formula xml:id="formula_52">N (i) = j|x j is one of the q nearest neighbors of x i , j = i . (<label>23</label></formula><formula xml:id="formula_53">)</formula><p>Edge weights in the sparse graph are defined as:</p><formula xml:id="formula_54">êi j = e i j , if i ∈ N ( j)or j ∈ N (i); 0, otherwise. (<label>24</label></formula><formula xml:id="formula_55">)</formula><p>We choose q = log 2 n + 1 as suggested in <ref type="bibr" target="#b60">[61]</ref>. 3. Form the similarity matrix. We compute the normalized similarity values as in the normalized cut <ref type="bibr" target="#b50">[51]</ref>:</p><formula xml:id="formula_56">A i j = êi j d -1/2 i d -1/2 j , (<label>25</label></formula><formula xml:id="formula_57">)</formula><p>where</p><formula xml:id="formula_58">d i = n s=1 êis (1 ≤ i ≤ n).</formula><p>Note that the similarity matrix A constructed as above is symmetric, nonnegative, and usually indefinite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Data sets</head><p>Document clustering was conducted on the following labeled corpuses: 1. TDT2 1 contains 10,212 news articles from various sources (e.g. NYT, CNN, VOA) in 1998. 2. Reuters 2 contains 21,578 news articles from the Reuters newswire in 1987. 3. From the newly-released Reuters news collection RCV1 3 [40] that contains over 800,000 articles in 1996-1997, we selected the training set containing 23,149 articles. Labels are assigned according to a topic hierarchy, and we only considered leaf topics as valid labels. 4. The research paper collection NIPS14-16 4 contains 420 NIPS papers in 2001-2003 <ref type="bibr" target="#b20">[21]</ref>, which are associated with labels indicating the technical area (algorithms, learning theory, vision science, etc). For all these data sets, documents with multiple labels are discarded in our experiments. In addition, clusters representing different topics are highly unbalanced in size. We selected the largest 20, 20, 40, 9 clusters from these data sets respectively. While TDT2 and the two Reuters data sets were well maintained, the NIPS data set was extracted from PS and PDF files, resulting in very noisy texts, which can be seen from the list of terms available online (see footnote 4). For example, its vocabulary includes many symbols frequently used in formulas which are not semantically meaningful.</p><p>Image clustering was conducted on object and face recognition data sets: 1. COIL-20 5 contains gray-scale images of 20 objects, rescaled to 64×64 size. The viewpoints are equally spaced in the entire 360 • range, resulting in 72 images for each object. 2. ORL 6 contains 400 face images of 40 persons with different facial expressions and slightly-varing pose. 3. From Extended YaleB 7 face data set (with the original YaleB data included) <ref type="bibr" target="#b38">[39]</ref>, we selected 2,414 frontal face images of 38 persons, with different illumination conditions. 4. From PIE 8 face data set <ref type="bibr" target="#b56">[57]</ref>, we selected 232 frontal face images of 68 persons, with different facial expressions. Compared to other variations in PIE data set such as illumination and lighting conditions, different facial expressions represent more variations in faces and the images are embedded in multiple manifolds <ref type="bibr" target="#b59">[60]</ref>; moreover, only 3-4 images are available for each person, which makes clustering more challenging. Though ORL and the selected subset of PIE are not large-scale, they share the same characteristics: High variations within each class, with a handful of images per class. For all the image data sets, the identity information of the objects or faces is used as ground-truth labels. The statistics of the processed document and image data sets are summarized in Table <ref type="table" target="#tab_4">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Algorithms for comparison</head><p>We experimented with a large variety of clustering algorithms for a comprehensive comparison. The algorithms in our experiment can be divided into four categories:  1. K-means variants (All these K-means variants include a batch-update phase and an additional online-update phase in each run <ref type="bibr" target="#b17">[18]</ref>. We use both phases.)</p><p>-Standard K-means (KM) The input matrix is constructed as follows. For text data, each column of the tf-idf matrix X <ref type="bibr" target="#b46">[47]</ref> is scaled to have unit 2-norm; in addition, X is transformed into its normalized-cut weighted version X D -1/2 <ref type="bibr" target="#b63">[64]</ref>, where D is defined in Sect. 3 with e i j = x T i x j . For image data, each column of X is scaled to the [0, 1] interval.</p><p>-Spherical K-means (SKM) Unlike standard K-means that uses Euclidean distance as the dissimilarity measure, spherical K-means uses 1cos(x i , x j ); therefore any scaling of columns of X does not take effect. Spherical K-means was proposed for document clustering, where cosine similarity is often a better measure than Euclidean distance <ref type="bibr" target="#b13">[14]</ref>. As mentioned in Sect. 1, we believe that spherical K-means has a closer relationship to NMF than standard K-means. -Kernel K-means (KKM) Kernel K-means is a graph clustering method based on Kmeans. We use the weighted kernel K-means algorithm described in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b35">36]</ref> that minimizes the normalized cut objective. Because K is generally indefinite, the condition for convergence is violated. We terminate the algorithm as soon as the objective function value stops decreasing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">NMF variants</head><p>-NMF We use the ANLS algorithm with block pivoting method for NMF <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. The same input matrix as in standard K-means is used. The hard clustering result is indicated by the largest entry in each row of H . -GNMF Cai et al. <ref type="bibr" target="#b7">[8]</ref> proposed Graph-regularized NMF (GNMF) by adding a graphtheoretic penalty term to (3) that takes neighboring relationship into account, so that the resulting method is better at clustering on manifolds. We use the algorithm and the parameters suggested in <ref type="bibr" target="#b7">[8]</ref>. The input matrix is constructed in the same way as in standard K-means. However, the neighboring relationship based on the sparse graph is generated using the original data matrix, i.e. without the scaling of each x i . The clustering result is obtained by treating the rows of H as graph embedding and applying spherical K-means to the embedded points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Spectral clustering variants</head><p>-NJW algorithm (SpNJW) This refers to the algorithm proposed in Ng et al. <ref type="bibr" target="#b50">[51]</ref>. The rows of the k leading eigenvectors of A, where each row is normalized to have unit 2-norm, are used as the graph embedding of data points. Standard K-means is used in the final step to obtain clustering results, which is initialized by randomly choosing k samples as centroids. -YS algorithm (SpYS) This refers to the algorithm proposed in Yu and Shi <ref type="bibr" target="#b66">[67]</ref>. The clustering results are obtained by finding the optimal orthogonal transformation of H = D -1/2 H into a partition matrix <ref type="bibr" target="#b66">[67]</ref>, where columns of H are the k leading eigenvectors of A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SymNMF</head><p>We observed that the Newton-like algorithm for SymNMF gives better clustering quality on image data (more details in Sect. 7.5). On text data, however, the Newton-like algorithm is not efficient enough due to large problem sizes, and only the ANLS algorithm is applicable. When reporting the results, we use the general name "SymNMF" to refer to the algorithm of choice.</p><p>For the Newton-like algorithm (Algorithm 1), we use parameters β = 0.1, σ = 0.1. We also empirically observe that choosing in <ref type="bibr" target="#b11">(12)</ref> to be a fixed value 10 -16 makes the Newton-like algorithm faster while having little influence on the clustering quality. For the ANLS algorithm, we solve the formulation (15), i.e. without sparsity constraints on W, H (Algorithm 2). We empirically observe that it is sufficient to use a fixed parameter α = 1 in <ref type="bibr" target="#b14">(15)</ref> to obtain a negligible W -H F / H F . Note that the choice of a large enough value of α should be aligned with the scale of the similarity values in A. In our experiments, the matrix A contains normalized similarity values <ref type="bibr" target="#b24">(25)</ref>, thus the maximum possible value in A is 1, and most of the entries of A are smaller than 1. Finally, in both of our algorithms, the tolerance parameter μ in the stopping criteria is set to 10 -4 and the maximum iteration count is set to 10,000 so that the outputs are stationary points.</p><p>For each data set, we run each algorithm 20 times with different random initializations and the known number of clusters k as input. Algorithms in the same category have the same initializations. In other words, the multistart global optimization method is applied with the described method in each category as for a local search procedure. Although the data sets are labeled, the labels are used only when evaluating the clustering quality, not by the clustering algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Clustering quality</head><p>We use clustering accuracy, the percentage of correctly clustered items given by the maximum bipartite matching, to evaluate the clustering quality (see more details in <ref type="bibr" target="#b63">[64]</ref>). The average and maximum clustering accuracy over the 20 runs are shown in Tables <ref type="table" target="#tab_5">5</ref> and<ref type="table" target="#tab_6">6</ref>, respectively. The maximum clustering accuracy for an algorithm is determined by the solution with the smallest objective function value among the 20 runs. We have the following observations:</p><p>1. SpYS and SymNMF achieves the highest clustering quality more frequently than other methods. Note that SpYS was proposed as a more principled way to obtain hard clustering than SpNJW, from the k leading eigenvectors of A <ref type="bibr" target="#b66">[67]</ref>. Conceptually, both SpYS and SymNMF facilitate interpretation of the low-rank matrix that is used to approximate the graph similarity matrix, so that we can obtain hard clustering results directly from the low-rank matrix. However, comparing Tables <ref type="table" target="#tab_5">5</ref> and<ref type="table" target="#tab_6">6</ref>, we observe that by employing the multistart global optimization method and picking the solution with the smallest objective function value, SpYS achieves higher accuracy than the average for four out of eight data sets, while SymNMF and SpNJW achieve higher accuracy than the average for 5 data sets, implying that the objective functions in SymNMF and SpNJW are slightly better proxies for the clustering problem than that in SpYS. For each data set, the highest accuracy and any other accuracy within the range of 0.01 from the highest accuracy are marked bold The average metrics over all the data sets are marked italic 2. GNMF in our experiments does not show as dramatic improvement over SpNJW as the results reported in <ref type="bibr" target="#b7">[8]</ref> where only maximum clustering accuracy was reported. One possible reason is that in <ref type="bibr" target="#b7">[8]</ref>, full graphs with cosine similarity are used, whereas we use sparse graphs and different similarity measures for better scalability and clustering quality (Sect. 7.1). 3. The K-means variants give exceedingly high accuracy on the RCV1 data set. We need more study to have a good explanation of their performances, for example, in what cases cosine dissimilarity is a better choice of distance measure than Euclidean distance. Note that RCV1 is the only data set where spherical K-means has the highest accuracy, and also the only data set where NMF performs better than almost all the other low-rank approximation methods (GNMF, SpNJW, SpYS, SymNMF). This consistency corroborated with our observation that spherical K-means has a closer relationship to NMF than standard K-means, and seems to explain why spherical K-means is often used as an initialization strategy for NMF <ref type="bibr" target="#b62">[63]</ref>. We mentioned in Sect. 7.3 that the ANLS algorithm for SymNMF handles large data sets more efficiently, and the Newton-like algorithm achieves higher clustering accuracy. Here we discuss this tradeoff between efficiency and quality. The different properties exhibited by the two algorithms can be attributed to their different convergence behaviors, though both algorithms converge to stationary point solutions. In Fig. <ref type="figure" target="#fig_6">7</ref>, we use COIL-20 data set to study their convergence by plotting the objective function f (H ) and the projected gradient ∇ P f (H ) F throughout the iterations. As we could expect, f (H ) is non-increasing in both algorithms; on the contrary, ∇ P f (H ) F is not guaranteed to drop in every iteration but is used to check stationarity.</p><p>The Newton-like algorithm shows a divergent behavior in the initial stage of iterations, because the formulation ( <ref type="formula" target="#formula_5">6</ref>) is nonconvex and the search step degrades to a steepest descent direction. However, when the intermediate iterate becomes close to a local minimum, the Hessian matrix becomes positive definite and the second-order information begins to help guide the search. Thus after this point, the algorithm converges very quickly to an accurate stationary point. In contrast, the ANLS algorithm shows a quick drop in both ∇ P f (H ) F and f (H ) when the algorithm starts. However, near the final stage, it converges slowly to the appointed stationarity level. Overall, the Newton-like algorithm produces more accurate solutions and better clustering quality; however, it is overall less efficient than the ANLS algorithm due to heavier computational cost per iteration. We compare their clustering quality and timing performance in Table <ref type="table" target="#tab_7">7</ref>, with μ = 10 -4 in the stopping criterion in both algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Image segmentation experiments</head><p>In this section, we explore the application of SymNMF to image segmentation. Image segmentation methods have been heavily relying on spectral clustering <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48]</ref>. We will demonstrate that SymNMF produces segmentation results that are closer to human-marked boundaries compared to spectral clustering. To the best of our knowledge, this is the first systematic evaluation of SymNMF applied to image segmentation. The input is a nonnegative and symmetric matrix that contains similarity values between pairs of pixels; the output is a clustering of pixels where each cluster corresponds to a region. In the graph represented by a pixel similarity matrix A, a pixel is only connected to the pixels within some neighborhood. Thus, the input matrix A is typically a sparse matrix. The similarity value between two neighboring pixels can be computed based on brightness, color, and texture cues <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b47">48]</ref>. The similarity value characterizes the discontinuity along the line connecting the two pixels and can be trained by a logistic model using human-marked boundaries as ground-truth <ref type="bibr" target="#b18">[19]</ref>.</p><p>Spectral clustering is one of the most common methods that solve the graph clustering problem in image segmentation. As we explained in Sects. 3 and 4, because eigenvectors contain both positive and negative numbers in general, they cannot be used as cluster indicators directly. A variety of methods have been proposed to post-process the graph embedding-the continuous-valued eigenvectors-to obtain closed regions. In contrast, the low-rank matrix H in the solution of SymNMF can not only be used as graph embedding, but also derive graph clustering results directly.</p><p>In the current paper, our focus is the gain in segmentation quality by replacing spectral clustering with SymNMF. We follow the steps in an early paper <ref type="bibr" target="#b18">[19]</ref> to construct the similarity matrix as well as post-process the graph embedding when the produced low-rank matrix is viewed as graph embedding. The post-processing steps are:</p><p>1. Run K-means on the embedded points to generate an oversegmentation of an image.</p><p>The oversegmentations are called superpixels and denoted as o 1 , . . . , o K , where K is an integer larger than the rank k of the low-rank matrix. 2. Build a contracted graph on the superpixels and represent it by a K × K similarity matrix W . The edge weight between the I -th and J -th superpixels (1 ≤ I, J ≤ K ) is defined as:</p><formula xml:id="formula_59">W I J = i∈o I j∈o J A i j . (<label>26</label></formula><formula xml:id="formula_60">)</formula><p>3. Recursively split the contracted graph to produce a hierarchy of regions <ref type="bibr" target="#b45">[46]</ref>.</p><p>We note that the baseline segmentation algorithm <ref type="bibr" target="#b18">[19]</ref> used in our comparison between spectral clustering and SymNMF is not the best algorithm to date (for example, see <ref type="bibr" target="#b0">[1]</ref>). However, we chose this baseline algorithm in order to simplify the experiment setting and make the comparison more visible. In our current workflow, both spectral and SymNMF use the same similarity matrix as an input; the resulting low-rank matrices are interpreted as either graph embedding to produce a hierarchy of regions or graph clustering to produce a flat partitioning of an image into regions. With more recent segmentation algorithms such as <ref type="bibr" target="#b0">[1]</ref>, the low-rank matrices would be interpreted in a more sophisticated way so that we do not know which component of the segmentation algorithm contributes to the gain in segmentation quality. We expect that the comparison result shown in this section will carry on to other segmentation algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Data and software</head><p>We use the Berkeley Segmentation Data Set 500<ref type="foot" target="#foot_0">9</ref> (BSDS500) <ref type="bibr" target="#b0">[1]</ref> and choose the 200 color images used in <ref type="bibr" target="#b18">[19]</ref>. The size of the original images is 481 × 321. We resized the images to 240 × 160 to be consistent with the experiments in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>We compute the pixel similarity matrices and post-process the embedded points using the Berkeley Segmentation Engine. <ref type="foot" target="#foot_1">10</ref> We use the default settings: The number of eigenvectors in spectral clustering k (and also the lower rank in SymNMF) is set to 16; the number of oversegmentations K is set to 51. The neighborhood of a pixel is modified from default to a round disk centered at the pixel with radius of 20 pixels. The resulting similarity matrix has size n × n where n = 38,400 and 44 million nonzeros. The same similarity matrix is given as an input to both spectral clustering and SymNMF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Evaluation methods</head><p>The evaluation of segmentation results is based on the evaluation of boundary detection. In the experiments on document and image clustering, solving SymNMF and interpreting the low-rank result matrix as a cluster indicator yield a hard clustering of items. In order to evaluate SymNMF in the context of image segmentation and compare its performance with that of spectral clustering, we introduce our way to transform the hard clustering results to soft boundaries. First, we generate a probability of boundary (P b ) image from multiple segmentations of an image. Second, we evaluate the P b image against human-marked boundaries.</p><p>-We consider the following four ways to obtain multiple segmentations:  -The data set includes a couple of human-marked boundaries for each image for evaluation.</p><p>The P b image has values in the [0, 1] interval. We can produce a binary boundary image using a threshold value t (0 &lt; t &lt; 1). Then the precision P is calculated as the fraction of true boundary pixels among all the detected boundary pixels; the recall R is calculated as the fraction of detected boundary pixels among all the true boundary pixels. The F-measure is defined as 2P R/(P + R). We can draw a precision-recall curve using a series of threshold values (see more details in <ref type="bibr" target="#b48">[49]</ref>). The best F-measure on this curve is regarded as a summary performance metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Results</head><p>We show the precision-recall curves for Spectral-Embed, SymNMF-Embed, Spectral-NJW, and SymNMF-Clust in Fig. <ref type="figure" target="#fig_7">8</ref>. Using the best F-measure as the summary metric, both SymNMF versions have better segmentation quality than either of the spectral clustering methods. SymNMF-Embed is much better than Spectral-Embed in the high-recall low-precision area, with the highest recall approaching 0.8.  SymNMF-Clust is much better than Spectral-Embed in the high-precision low-recall area, and consistently better than Spectral-Embed along the curve. When the threshold value t is close to 1, we can be much more confident about the detected regions using SymNMF-Clust than using Spectral-Embed.</p><p>SymNMF-Clust is only marginally better than Spectral-NJW, but is consistently better along the precision-recall curve.</p><p>Figure <ref type="figure">9</ref> shows several exemplar images from the BSDS500 data set. The segmentation results are consistent with our findings in the precision-recall curve. We notice that Spectral-Embed often subdivides a large flat area with uniform colors into multiple regions (grass, sky, etc.). This is a well-known problem of image segmentation methods that rely on K-means to post-process the eigenvectors, and the reason is that the embedded points for the pixels in those areas vary smoothly <ref type="bibr" target="#b0">[1]</ref>. On the contrary, SymNMF-Clust often leaves those areas intact, which implies that the low-rank matrix generated by SymNMF is a better cluster indicator. Figure <ref type="figure" target="#fig_0">10</ref> shows the pixels plotted in the lower dimensional space produced by spectral clustering and SymNMF for a single image, which seems to support our reasoning above. We also notice that SymNMF-Clust tends to identify a few very small regions that correspond to noise in an image. This means that setting k larger than needed will not degrade its segmentation quality. If we remove the regions whose areas are smaller than some threshold, we will see that many of the remaining regions correspond to meaningful objects.</p><p>In summary, we can use SymNMF-Clust to detect salient objects and use SymNMF-Embed to discover more detailed segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>In this paper, we studied SymNMF: min H ≥0 A -HH T 2  F as a graph clustering method that is suitable for clustering data points embedded in linear and nonlinear manifolds. Our method extends the applicability of NMF to more general cases, where data relationship is not described by distances in vector space but by similarity values in a latent space. Unlike previous work on SymNMF that imposed various additional constraints on the matrix H , we showed that with nonnegativity constraints only, H can be well interpreted as a cluster indicator matrix. We justified SymNMF to be a valid graph clustering method by showing that it originates from the same formulation as spectral clustering but relaxes the constraint on H differently. While spectral clustering methods require post-processing the eigenvector-based data representation to obtain hard clusters, SymNMF does not depend on the spectrum and finds cluster memberships directly from H . Compared to previous work on the extension of NMF to a positive semi-definite and nonnegative matrix, our approach only assumes that A is symmetric and nonnegative.</p><p>We developed two algorithms for SymNMF, a Newton-like algorithm and an ANLS-based algorithm, which should be used in different cases for best practices but both guaranteed to converge to stationary point solutions. We discussed the tradeoff between clustering quality and efficiency when choosing an algorithm for SymNMF. On one hand, the Newton-like algorithm often produces more accurate solutions and higher-quality clustering results, but is more appropriate when the problem size n is small, e.g. n &lt; 3,000. On the other hand, the ANLS algorithm is especially efficient for a sparse input matrix A and is scalable to very large data sets, e.g. n ≈ 10 6 . For large-scale clustering, we have to construct a sparse similarity matrix instead of a dense one. For example, with n = 10 5 data points, it is difficult to store a dense similarity matrix (∼75 GB) into the main memory of a contemporary machine.</p><p>We have shown the promise of SymNMF in document clustering and image clustering. We also conducted a comprehensive evaluation of SymNMF for image segmentation on 200 natural images. Overall, we developed a general framework in this paper, one with minimal constraints and flexible enough for extension. One limitation of our formulation is that an indefinite matrix A could be approximated by a positive semi-definite matrix HH T . Its effect requires further study; however, we have not seen evidences that the clustering performance degraded due to this limitation. The proposed algorithms can be easily parallelized, for example, in the Newton-like algorithm, the evaluation and Cholesky factorization of different diagonal blocks of the Hessian can run in parallel; and in the ANLS algorithm, the nonnegative least squares problem with different right-hand sides can be made parallel as well.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 An example with two ground-truth clusters, with different clustering results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 Fig. 3</head><label>23</label><figDesc>Fig. 2 An illustration of SymNMF formulation min H ≥0 A -HH T 2 F . Each cell is a matrix entry. Colored region has larger values than white region. Here n = 7 and k = 2. (Color figure online)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 5 shows (a) the original data points; (b) the embedding generated by spectral clustering;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 3 .Fig. 5 A</head><label>435</label><figDesc>Fig. 4 Three leading eigenvectors of the similarity matrix in (8) when λ3 (A 1 ) &gt; max( λ1 (A 2 ), λ1 (A 3 )). Here we assume that all the block diagonal matrices A 1 , A 2 , A 3 have size 3 × 3. Colored region has nonzero values. (Color figure online)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 Fig. 6</head><label>56</label><figDesc>Fig. 6 Clustering results for the example in Fig. 5: a Spectral clustering, b SymNMF</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>F 1 : 5 :</head><label>15</label><figDesc>Input: number of data points n, number of clusters k, n × n similarity matrix A, regularization parameter α &gt; 0, and tolerance parameter 0 &lt; μ &lt;&lt; 1 2: Initialize H 3: repeat 4: W ← H Solve an NLS problem: H</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7</head><label>7</label><figDesc>Fig. 7 Convergence behaviors of SymNMF algorithms, generated from a single run on COIL-20 data set with the same initialization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8</head><label>8</label><figDesc>Fig. 8 Examples of the original images and P b images from BSDS500. Pixels with brighter color in the P b images have higher probability to be on the boundary. a Original, b Spectral-Embed, c Spectral-NJW, d SymNMF-Embed, e SymNMF-Clust</figDesc><graphic coords="25,49.59,56.30,340.12,244.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>curves [F=0.5772] Spectral-Embed @(0.524,0.643) t=0.55 [F=0.6000] SymNMF-Embed @(0.570,0.634) t=0.67 [F=0.5836] Spectral-NJW @(0.496, 0.709) t=0.16 [F=0.5942] SymNMF-Clust @(0.510,0.711) t=0.<ref type="bibr" target="#b15">16</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 1 -Fig. 10</head><label>9110</label><figDesc>Fig. 9 Precision-recall curves for image segmentation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Algorithmic steps of spectral clustering and SymNMF clustering</figDesc><table><row><cell></cell><cell>Spectral clustering</cell><cell>SymNMF</cell></row><row><cell>Objective</cell><cell>min Ĥ T</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Leading eigenvalues of the similarity matrix based on</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Comparison of PGD and PNewton for solving minH ≥0 A -HH T 2 F , H ∈ R n×k</figDesc><table><row><cell>+</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>Data sets used in</figDesc><table><row><cell>experiments</cell><cell>Data set</cell><cell>Dimension</cell><cell># Data points</cell><cell># Clusters</cell></row><row><cell></cell><cell>TDT2</cell><cell>26,618</cell><cell>8,741</cell><cell>20</cell></row><row><cell></cell><cell>Reuters</cell><cell>12,998</cell><cell>8,095</cell><cell>20</cell></row><row><cell></cell><cell>RCV1</cell><cell>20,338</cell><cell>15,168</cell><cell>40</cell></row><row><cell></cell><cell>NIPS14-16</cell><cell>17,583</cell><cell>420</cell><cell>9</cell></row><row><cell></cell><cell>COIL-20</cell><cell>64 × 64</cell><cell>1,440</cell><cell>20</cell></row><row><cell></cell><cell>ORL</cell><cell>69 × 84</cell><cell>400</cell><cell>40</cell></row><row><cell></cell><cell>Extended YaleB</cell><cell>56 × 64</cell><cell>2,414</cell><cell>38</cell></row><row><cell></cell><cell>PIE-expression</cell><cell>64 × 64</cell><cell>232</cell><cell>68</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>Average clustering accuracy for document and image data setsFor each data set, the highest accuracy and any other accuracy within the range of 0.01 from the highest accuracy are marked bold The average metrics over all the data sets are marked italic</figDesc><table><row><cell></cell><cell>KM</cell><cell>SKM</cell><cell>KKM</cell><cell>NMF</cell><cell>GNMF</cell><cell>SpNJW</cell><cell>SpYS</cell><cell>SymNMF</cell></row><row><cell>TDT2</cell><cell>0.6711</cell><cell>0.6755</cell><cell>0.6837</cell><cell>0.8505</cell><cell>0.7955</cell><cell>0.7499</cell><cell>0.9050</cell><cell>0.8934</cell></row><row><cell>Reuters</cell><cell>0.4111</cell><cell>0.3332</cell><cell>0.3489</cell><cell>0.3731</cell><cell>0.4460</cell><cell>0.3114</cell><cell>0.4986</cell><cell>0.5094</cell></row><row><cell>RCV1</cell><cell>0.3111</cell><cell>0.3888</cell><cell>0.3831</cell><cell>0.3797</cell><cell>0.3592</cell><cell>0.2723</cell><cell>0.2743</cell><cell>0.2718</cell></row><row><cell>NIPS14-16</cell><cell>0.4602</cell><cell>0.4774</cell><cell>0.4908</cell><cell>0.4918</cell><cell>0.4908</cell><cell>0.4987</cell><cell>0.5026</cell><cell>0.5086</cell></row><row><cell>COIL-20</cell><cell>0.6184</cell><cell>0.5611</cell><cell>0.2881</cell><cell>0.6312</cell><cell>0.6304</cell><cell>0.6845</cell><cell>0.7899</cell><cell>0.7258</cell></row><row><cell>ORL</cell><cell>0.6499</cell><cell>0.6500</cell><cell>0.6858</cell><cell>0.7020</cell><cell>0.7282</cell><cell>0.7127</cell><cell>0.7752</cell><cell>0.7798</cell></row><row><cell>Extended YaleB</cell><cell>0.0944</cell><cell>0.0841</cell><cell>0.1692</cell><cell>0.1926</cell><cell>0.2109</cell><cell>0.1862</cell><cell>0.2254</cell><cell>0.2307</cell></row><row><cell>PIE-expression</cell><cell>0.7358</cell><cell>0.7420</cell><cell>0.7575</cell><cell>0.7912</cell><cell>0.8235</cell><cell>0.7966</cell><cell>0.7375</cell><cell>0.7517</cell></row><row><cell>ALL</cell><cell>0.4940</cell><cell>0.4890</cell><cell>0.4759</cell><cell>0.5515</cell><cell>0.5606</cell><cell>0.5265</cell><cell>0.5886</cell><cell>0.5839</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc>Maximum clustering accuracy for document and image data sets</figDesc><table><row><cell></cell><cell>KM</cell><cell>SKM</cell><cell>KKM</cell><cell>NMF</cell><cell>GNMF</cell><cell>SpNJW</cell><cell>SpYS</cell><cell>SymNMF</cell></row><row><cell>TDT2</cell><cell>0.7878</cell><cell>0.7531</cell><cell>0.7502</cell><cell>0.8761</cell><cell>0.8439</cell><cell>0.8046</cell><cell>0.9060</cell><cell>0.9059</cell></row><row><cell>Reuters</cell><cell>0.5001</cell><cell>0.3047</cell><cell>0.3828</cell><cell>0.3839</cell><cell>0.4053</cell><cell>0.3096</cell><cell>0.4985</cell><cell>0.4957</cell></row><row><cell>RCV1</cell><cell>0.3392</cell><cell>0.3956</cell><cell>0.3844</cell><cell>0.3762</cell><cell>0.3682</cell><cell>0.2695</cell><cell>0.2743</cell><cell>0.2771</cell></row><row><cell>NIPS14-16</cell><cell>0.5071</cell><cell>0.4833</cell><cell>0.5048</cell><cell>0.5000</cell><cell>0.4786</cell><cell>0.4952</cell><cell>0.5024</cell><cell>0.5048</cell></row><row><cell>COIL-20</cell><cell>0.6917</cell><cell>0.6125</cell><cell>0.3569</cell><cell>0.6653</cell><cell>0.6590</cell><cell>0.7347</cell><cell>0.7986</cell><cell>0.7847</cell></row><row><cell>ORL</cell><cell>0.6675</cell><cell>0.6500</cell><cell>0.7125</cell><cell>0.7200</cell><cell>0.7225</cell><cell>0.7700</cell><cell>0.7725</cell><cell>0.7900</cell></row><row><cell>Extended YaleB</cell><cell>0.0903</cell><cell>0.0816</cell><cell>0.1785</cell><cell>0.1980</cell><cell>0.2171</cell><cell>0.1864</cell><cell>0.2299</cell><cell>0.2307</cell></row><row><cell>PIE-expression</cell><cell>0.7759</cell><cell>0.7586</cell><cell>0.7629</cell><cell>0.7845</cell><cell>0.8190</cell><cell>0.8060</cell><cell>0.7888</cell><cell>0.7543</cell></row><row><cell>ALL</cell><cell>0.5450</cell><cell>0.5049</cell><cell>0.5041</cell><cell>0.5630</cell><cell>0.5642</cell><cell>0.5470</cell><cell>0.5964</cell><cell>0.5929</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7</head><label>7</label><figDesc>Clustering accuracy and timing of the Newton-like and ANLS algorithms for SymNMF Image segmentation is an important task in computer vision that organizes an image into a non-overlapping set of closed regions. It can be viewed as a graph clustering problem:</figDesc><table><row><cell></cell><cell cols="2">Newton-like algorithm</cell><cell>ANLS algorithm</cell><cell></cell></row><row><cell></cell><cell>Accuracy</cell><cell>Time (s)</cell><cell>Accuracy</cell><cell>Time (s)</cell></row><row><cell>COIL-20</cell><cell>0.7258</cell><cell>53.91</cell><cell>0.7195</cell><cell>8.77</cell></row><row><cell>ORL</cell><cell>0.7798</cell><cell>4.30</cell><cell>0.7713</cell><cell>1.97</cell></row><row><cell>Extended YaleB</cell><cell>0.2307</cell><cell>163.6</cell><cell>0 .2296</cell><cell>23.47</cell></row><row><cell>PIE-expression</cell><cell>0.7517</cell><cell>13.48</cell><cell>0.6836</cell><cell>4.43</cell></row><row><cell cols="5">Experiments are conducted on image data sets with parameter μ = 10 -4 and the reported measures are</cell></row><row><cell cols="2">averaged over 20 random runs</cell><cell></cell><cell></cell><cell></cell></row><row><cell>8.1 Overview</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>1 .</head><label>1</label><figDesc>Spectral-Embed Compute the eigenvectors associated with the 16 largest eigenvalues and treat them as a graph embedding. Generate a hierarchy of regions following the procedures in Sect. 8.1. Each level of the hierarchy determines a segmentation of the image. 2. SymNMF-Embed Solve SymNMF with k = 16 and treat the rows of H as a graph embedding. Generate a hierarchy of regions following the procedures in Sect. 8.1.Each level of the hierarchy determines a segmentation of the image. 3. Spectral-NJW For each k = 2, 3, . . . , 16, compute the eigenvectors associated with the k largest eigenvalues, denoted as a matrix Ĥ ∈ R n×k . Apply K-means to the rows of each matrix Ĥ , and the clustering result corresponds to a segmentation. 4. SymNMF-Clust Solve SymNMF with k = 2, 3, . . . , 16 and treat each matrix H as a cluster indicator. For each k, the clustering result corresponds to a segmentation.</figDesc><table /><note><p>Spectral-Embed and SymNMF-Embed produces 50 segmentations for each image. Spectral-NJW and SymNMF-Clust produces 15 segmentations for each</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_0"><p>http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_1"><p>http://www.cs.berkeley.edu/~fowlkes/BSE/.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work of the first and third authors was supported in part by the National Science Foundation (NSF) Grants CCF-0808863 and the Defense Advanced Research Projects Agency (DARPA) XDATA program Grant FA8750-12-2-0309. The work of the second author was supported by the TJ Park Science Fellowship of POSCO TJ Park Foundation. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF, the DARPA, or the NRF.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Clustering by left-stochastic matrix factorization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fazel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML &apos;11: Proceedings of the 28nd International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Clustering on the unit hypersphere using von Mises-Fisher distributions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1345" to="1382" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Plemmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nonnegative Matrices in the Mathematical Sciences</title>
		<meeting><address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shaked-Monderer</surname></persName>
		</author>
		<title level="m">Completely Positive Matrices</title>
		<meeting><address><addrLine>River Edge, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Projected newton methods for optimization problems with simple constraints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Control Optim</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="221" to="246" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nonlinear Programming, 2nd edn</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Athena Scientific</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>Belmont, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graph regularized nonnegative matrix factorization for data representation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1548" to="1560" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On reduced rank nonnegative matrix factorization for symmetric matrices</title>
		<author>
			<persName><forename type="first">M</forename><surname>Catral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Plemmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra Appl</title>
		<imprint>
			<biblScope unit="volume">393</biblScope>
			<biblScope unit="page" from="107" to="126" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spectral k-way ratio-cut partitioning and clustering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schlag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. CAD Integr. Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1088" to="1096" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Utopian: User-driven topic modeling based on interactive nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1992" to="2001" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spectral segmentation with multiscale graph decomposition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Benezit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR &apos;05: Proceedings of the 2005 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1124" to="1131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A Unified View of Kernel K-Means, Spectral Clustering and Graph Cuts</title>
		<author>
			<persName><forename type="first">I</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<idno>TR-04-25</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>University of Texas at Austin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Concept decompositions for large sparse text data using clustering</title>
		<author>
			<persName><forename type="first">I</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Modha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="143" to="175" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the equivalence of nonnegative matrix factorization and spectral clustering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM &apos;05: Proceedings of the SIAM International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="606" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nonnegative matrix factorization for combinatorial optimization: spectral clustering, graph matching, and clique finding</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM &apos;08: Proceedings of the 8th IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convex and semi-nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="55" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Pattern Classification. Wiley-Interscience</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">How Much Does Globalization Help Segmentation?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno>UCB/CSD- 4-1340</idno>
		<imprint>
			<date type="published" when="2004">2004</date>
			<pubPlace>Berkeley</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchical clustering of hyperspectral images using rank-two nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gillis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2066" to="2078" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Euclidean embedding of co-occurrence data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="2265" to="2295" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Accelerating the Lee-Seung Algorithm for Non-Negative Matrix Factorization</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Gonzales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno>TR05-02</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Rice University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Symmetric nonnegative matrix factorization: algorithms and applications to probabilistic clustering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zdunek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2117" to="2131" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Nonnegative Matrix Factorization Algorithms and Applications</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Ho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>Universite catholique de Louvain</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bounded matrix factorization for recommender system</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ishteva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="491" to="511" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Iterative Methods for Linear and Nonlinear Equations</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Kelley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<pubPlace>Philadelphia, PA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sparse non-negative matrix factorizations via alternating non-negativity-constrained least squares for microarray data analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1495" to="1502" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Nonnegative matrix factorization based on alternating non-negativity-constrained least squares and the active set method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix. Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="713" to="730" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Algorithms for nonnegative matrix and tensor factorizations: a unified view based on block coordinate descent framework</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Glob. Optim</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="285" to="319" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Sparse Nonnegative Matrix Factorization for Clustering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<idno>GT-CSE- 08-01</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Georgia Institute of Technology</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Toward faster nonnegative matrix factorization: a new algorithm and comparisons</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM &apos;08: Proceedings of the 8th IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast nonnegative matrix factorization: An active-set-like method and comparisons</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3261" to="3281" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An impossibility theorem for clustering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="446" to="453" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Symmetric nonnegative matrix factorization for graph clustering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM &apos;12: Proceedings of the SIAM International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="106" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast rank-2 nonnegative matrix factorization for hierarchical document clustering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;13: Proceedings of the 19th ACM International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="739" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semi-supervised graph clustering: a kernel approach</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML &apos;05: Proceedings of the 22nd Internatioal Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="457" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Solving Least Squares Problems</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Lawson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Hanson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974">1974</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning the parts of objects by non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">401</biblScope>
			<biblScope unit="page" from="788" to="791" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Acquiring linear subspaces for face recognition under variable lighting</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="684" to="698" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rcv1: A new benchmark collection for text categorization research</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Solving consensus and semi-supervised clustering problems using nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM &apos;07: Proceedings of the 7th IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="577" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On the convergence of multiplicative update algorithms for nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1589" to="1596" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Projected gradient methods for nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2756" to="2779" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A hybrid heuristic for the diameter constrained minimum spanning tree problem</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lucena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Glob. Optim</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="363" to="381" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Semi-supervised clustering algorithm for community structure detection in complex networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. A Stat. Mech. Appl</title>
		<imprint>
			<biblScope unit="volume">389</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="187" to="197" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Contour and texture analysis for image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="27" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning to detect natural image boundaries using local brightness, color, and texture cues</title>
		<author>
			<persName><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="530" to="549" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV &apos;01: Proceedings of the 8th IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fuzzy communities and the concept of bridgeness in complex networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nepusz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petróczi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Négyessy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bazsó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">107</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">On spectral clustering: analysis and an algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="849" to="856" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Text mining using non-negative matrix factorizations</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Pauca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shahnaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Plemmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM &apos;04: Proceedings of the SIAM International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="452" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Stochastic global optimization methods, part II: multi level methods</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rinnooy Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Timmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="57" to="78" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Global optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rinnooy Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Timmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbooks in Operations Research and Management Science</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Kan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Todds</surname></persName>
		</editor>
		<meeting><address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<publisher>North Holland</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="631" to="662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Document clustering using nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shahnaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Pauca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Plemmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Manag</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="373" to="386" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The CMU pose, illumination, and expression database</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bsat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1615" to="1618" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Matrix Perturbation Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Exploring complex networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Strogatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">410</biblScope>
			<biblScope unit="page" from="268" to="276" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Comput</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Community discovery using nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min. Knowl. Discov</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="493" to="521" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Improving non-negative matrix factorizations through structured initialization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Curry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dougherty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2217" to="2232" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Document clustering based on non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;03: Proceedings of the 26th International ACM Conference on Research and Development in Informaion Retrieval</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="267" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Clustering by nonnegative matrix factorization using graph random walk</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Dikmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1088" to="1096" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Clustering by low-rank doubly stochastic matrix decomposition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML &apos;12: Proceedings of the 29nd International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Multiclass spectral clustering</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV &apos;03: Proceedings of the 9th IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="313" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A block coordinate gradient descent method for regularized convex separable optimization and covariance selection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Toh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="331" to="355" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A unifying approach to hard and probabilistic clustering</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV &apos;05: Proceedings of the 10th IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="294" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Self-tuning spectral clustering</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1601" to="1608" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Overlapping community detection via bounded nonnegative matrix trifactorization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;12: Proceedings of the 18th ACM International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="606" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Overlapping community detection in complex networks using symmetric binary matrix factorization</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">803</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
