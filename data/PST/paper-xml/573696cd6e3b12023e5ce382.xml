<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gradient-based Hyperparameter Optimization through Reversible Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
							<email>maclaurin@physics.harvard.edu</email>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
							<email>dduvenaud@seas.harvard.edu</email>
						</author>
						<author>
							<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
						</author>
						<title level="a" type="main">Gradient-based Hyperparameter Optimization through Reversible Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tuning hyperparameters of learning algorithms is hard because gradients are usually unavailable. We compute exact gradients of cross-validation performance with respect to all hyperparameters by chaining derivatives backwards through the entire training procedure. These gradients allow us to optimize thousands of hyperparameters, including step-size and momentum schedules, weight initialization distributions, richly parameterized regularization schemes, and neural network architectures. We compute hyperparameter gradients by exactly reversing the dynamics of stochastic gradient descent with momentum.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Machine learning systems abound with hyperparameters. These can be parameters that control model complexity, such as L 1 and L 2 penalties, or parameters that specify the learning procedure itself -step sizes, momentum decay parameters and initialization conditions. Choosing the best hyperparameters is both crucial and frustratingly difficult.</p><p>The current gold standard for hyperparameter selection is gradient-free model-based optimization <ref type="bibr" target="#b34">(Snoek et al., 2012;</ref><ref type="bibr" target="#b6">Bergstra et al., 2011;</ref><ref type="bibr">2013;</ref><ref type="bibr" target="#b20">Hutter et al., 2011)</ref>. Hyperparameters are chosen to optimize the validation loss after complete training of the model parameters. These approaches have demonstrated that automatic tuning of hyperparameters can yield state-of-the-art performance. However, in general they are not able to effectively optimize more than 10 to 20 hyperparameters.</p><p>Why not use gradients? Reverse-mode differentiation allows gradients to be computed with a similar time cost to the original objective function. This approach is taken al-Proceedings of the 32 nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W&amp;CP volume 37. Copyright 2015 by the author(s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training loss W e ig h t 1 W e i g h t 2</head><p>Initial weights Meta-iteration 1 Meta-iteration 2 Meta-iteration 3</p><p>Figure <ref type="figure">1</ref>. Hyperparameter optimization by gradient descent. Each meta-iteration runs an entire training run of stochastic gradient descent to optimize elementary parameters (weights 1 and 2). Gradients of the validation loss with respect to hyperparameters are then computed by propagating gradients back through the elementary training iterations. Hyperparameters (in this case, learning rate and momentum schedules) are then updated in the direction of this hypergradient.</p><p>most universally for optimization of elementary<ref type="foot" target="#foot_1">1</ref> parameters. The problem with taking gradients with respect to hyperparameters is that computing the validation loss requires an inner loop of elementary optimization, which makes naïve reverse-mode differentiation infeasible from a memory perspective. Section 2 describes this problem and proposes a solution, which is the main technical contribution of this paper.</p><p>Gaining access to gradients with respect to hyperparamters opens up a garden of delights. Instead of straining to eliminate hyperparameters from our models, we can embrace them, and richly hyperparameterize our models. Just as having a high-dimensional elementary parameterization gives a flexible model, having a high-dimensional hyperparameterization gives flexibility over model classes, regularization, and training methods. Section 3 explores these new opportunities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Contributions</head><p>• We give an algorithm that exactly reverses stochastic gradient descent with momentum to compute gradients with respect to all continuous training parameters.</p><p>• We show how to efficiently store only the information needed to exactly reverse learning dynamics. For example, when the momentum term is 0.9, this method reduces the memory requirements of reverse-mode differentiation of hyperparameters by a factor of 200.</p><p>• We show that these gradients allow optimization of validation loss with respect to thousands of hyperparameters. For example, we optimize fine-grained learning-rate schedules, per-layer initialization distributions of neural network parameters, per-input regularization schemes, and per-pixel data preprocessing.</p><p>• We provide insight into learning procedures by examining optimized learning-rate schedules and initialization procedures, comparing them to standard advice in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Hypergradients</head><p>Reverse-mode differentiation (RMD) has been an asset to the field of machine learning <ref type="bibr" target="#b24">(LeCun et al., 1989</ref>) (see the appendix for a refresher). The RMD method, known as "backpropagation" in the deep learning community, allows the gradient of a scalar loss with respect to its parameters to be computed in a single backward pass. This increases the computational burden by only a factor of two over evaluating the loss itself, regardless of the number of parameters.</p><p>Obtaining the same sort of information by either forwardmode differentiation or brute force finite differences would require a separate pass for each parameter and would make deep learning entirely infeasible.</p><p>Applying RMD to hyperparameter optimization was proposed by <ref type="bibr" target="#b3">Bengio (2000)</ref> and <ref type="bibr" target="#b2">Baydin &amp; Pearlmutter (2014)</ref>, and applied to small problems by <ref type="bibr" target="#b12">Domke (2012)</ref>. However, the naïve approach fails for real-sized problems because of memory constraints. RMD requires that intermediate variables be maintained in memory for the reverse pass. Evaluating the validation loss requires training the model, which may require many elementary iterations. Conventional RMD stores this entire training trajectory, w 1 ...w T in memory. In large neural networks, the amount of memory required to store the millions of parameters being trained is typically close to the amount of physical RAM available <ref type="bibr" target="#b37">(Sutskever et al., 2014)</ref>. If storing the parameter vector takes ∼1GB, and the parameter vector is updated tens of thousands of times (the number of mini batches times the number of epochs) then storing the learning history is unmanageable even with physical storage.</p><p>Imagine that we could exactly trace a training procedure backwards, starting from the trained parameter values and working back to the initial parameters. Then we could recompute the learning trajectory on the fly during the reverse pass of RMD rather than storing it in memory. This is not possible in general, but we will show that for the popular training procedure of stochastic gradient descent with momentum, we can do exactly this, storing a small number of auxiliary bits to handle finite precision arithmetic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Reversible learning with exact arithmetic</head><p>Stochastic gradient descent (SGD) with momentum (Algorithm 1) can be seen as a physical simulation of a system moving through a series of fixed force fields indexed by time t. With exact arithmetic this procedure is reversible. This lets us write Algorithm 2, which reverses the steps in Algorithm 1, interleaved with computations of gradients. It outputs the gradient of a function of the trained weights f (w) (such as the validation loss) with respect to the initial weights w 1 , the learning-rate and momentum schedules, and any other hyperparameters which affect training gradients.</p><p>Algorithm 1 Stochastic gradient descent with momentum 1: input: initial w 1 , decays γ, learning rates α, loss function L(w, θ, t) 2: initialize v 1 = 0 3: for t = 1 to T do 4:</p><formula xml:id="formula_0">g t = ∇ w L(w t , θ, t)</formula><p>evaluate gradient 5: w t+1 = w t + α t v t update position 7: end for 8: output trained parameters w T Algorithm 2 Reverse-mode differentiation of SGD 1: input: w T , v T , γ, α, train loss L(w, θ, t), loss f (w) 2: initialize dv = 0, dθ = 0, dα t = 0, dγ = 0 3: initialize dw = ∇ w f (w T ) 4: for t = T counting down to 1 do 5:</p><formula xml:id="formula_1">v t+1 = γ t v t − (1 − γ t )</formula><formula xml:id="formula_2">dα t = dw T v t 6: w t−1 = w t − α t v t 7: g t = ∇ w L(w t , θ, t)</formula><p>exactly reverse gradient descent operations 8:</p><formula xml:id="formula_3">v t−1 = [v t + (1 − γ t )g t ]/γ t 9: dv = dv + α t dw 10: dγ t = dv T (v t + g t ) 11: dw = dw − (1 − γ t )dv∇ w ∇ w L(w t , θ, t) 12: dθ = dθ − (1 − γ t )dv∇ θ ∇ w L(w t , θ, t)</formula><p>13: dv = γ t dv 14: end for 15: output gradient of f (w T ) w.r.t w 1 , v 1 , γ, α and θ Computations of steps 11 and 12 both require a Hessian-vector product, but these can be computed exactly by applying RMD to the dot product of the gradient with a vector <ref type="bibr" target="#b31">(Pearlmutter, 1994)</ref>. Thus the time complexity of reverse SGD is O(T ), the same as forward SGD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Reversible learning with finite precision arithmetic</head><p>In practice, Algorithm 2 fails utterly due to finite numerical precision. The problem is the momentum decay term γ. Every time we apply step 8 to reduce the velocity, we lose information. Assuming we are using a fixed-point representation,<ref type="foot" target="#foot_2">2</ref> each multiplication by γ &lt; 1 shifts bits to the right, destroying the least significant bits. This is more than a pedantic concern. Attempting to carry out the reverse training requires repeated multiplication by 1/γ. Errors accumulate exponentially, and the reversed learning procedure ends far from the initial point (and usually overflows). Do we need γ &lt; 1? Unfortunately we do. γ &gt; 1 results in unstable dynamics, and γ = 1, recovers the leapfrog integrator <ref type="bibr" target="#b19">(Hut et al., 1995)</ref>, a perfectly reversible set of dynamics, but one that does not converge.</p><p>This problem is quite a deep one: optimization necessarily discards information. Ideally, optimization maps all initializations to the same optimum, a many-to-one mapping with no hope of inversion. Put another way, optimization moves a system from a high-entropy initial state to a low-entropy (hopefully zero entropy) optimized final state.</p><p>It is interesting to consider the analogy with physical dynamics. The γ term is analogous to a drag term in the simulation of Hamiltonian dynamics. Having γ &lt; 1 corresponds to dissipative dynamics which generates heat, increases the entropy of the environment and is not therefore not reversible. But we must have dissipation in order for our system to converge to equilibrium.</p><p>If we want to reverse the dynamics, there is no choice but to store the extra bits discarded by the γ operation. But we can at least try to be parsimonious about the number of extra bits we store. This is what the next section addresses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Optimal storage of discarded entropy</head><p>This section gives the technical details of how to efficiently store the information discarded each time the momentum decay operation (Step 8) is applied.</p><p>If γ = 0.5, we can simply store the single bit that falls off at each iteration, and if γ = 0.25 we could store two bits. But for fine-grained control over γ we need a way to store the information lost when we multiply by, say, γ = 0.9, which will be less than one bit on average. Here we give a procedure which achieves exactly this.</p><p>We represent the velocity v and parameter w vectors with 64-bit integers. With an implied radix point this can be a fixed-point representation of the reals. We represent γ as a rational number, n/d. When we divide each v by d we use integer division. In order to be able to reverse the process we just need to store the remainder, v modulo s, in some "information buffer", B. If B were an integer and n = 2, the remainder r would just be a single bit, and we could store it in B by left-shifting B's bits and adding r. For arbitrary n, we can do the base-n analogue of this operation: multiply B by n and add r. Eventually, B will overflow. We need a way to either detect this, store the bits, and start a fresh integer, or else we can just use an arbitrary size integer that grows as needed. (Python's "long" integer type supports this). This procedure allows division by n while storing the remainder in log 2 (n) bits on average.</p><p>When we multiply by the numerator of n/d we don't need to store anything extra, since integer division will bring us back to exactly the same point anyway. But the procedure as it stands would store three bits when γ = 7/8, whereas it should store less than one (log 2 (8/7) = 0.19). Our solution is the following: when we multiply v by n, there is an opportunity to add a nonnegative integer smaller than n to the result without affecting the reverse process (integer division by n). We can get such an integer from the information buffer by dividing it by n and recording B modulo n. We are using the velocity v as an information buffer itself! Algorithm 3 illustrates the entire process.</p><p>Algorithm 3 Exactly reversible multiplication by a ratio 1: Input:</p><formula xml:id="formula_4">Information buffer i, value c, ratio n/d 2: i = i × d make room for new digit 3: i = i + (c mod d) store digit lost by division 4: c = c ÷ d divide by denominator 5: c = c × n multiply by numerator 6: c = c + (i mod n) add digit from buffer 7: i = i ÷ n</formula><p>shorten information buffer 8: return updated buffer i, updated value c</p><p>We could also have used an arithmetic coding scheme for our information buffer <ref type="bibr">(MacKay, 2003, Chapter 6)</ref>. How much does this procedure save us? When γ = 0.98, we will have to store only 0.029 bits on average. Compared to storing a new 32-bit integer or floating-point number at each iteration, this reduces memory requirements by a factor of one thousand.</p><p>The standard way to save memory in RMD is checkpointing. Checkpointing stores the entire parameter vector on only a fraction of the training steps, and recomputes the missing steps of the training procedure (forwards) as needed during the backward pass. However, this would require too much memory to be practical for large neural nets trained for thousands of minibatches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>In typical machine learning applications, only a few hyperparameters (less than 20) are optimized. Since each experiment only yields a single number (the validation loss), the search rapidly becomes more difficult as the dimension of the hyperparameter vector increases. In contrast, when hypergradients are available, the amount of information gained from each training run grows along with the number of hyperparameters, allowing us to optimize thousands of hyperparameters. How can we take advantage of this new ability?</p><p>This section shows several proof-of-concept experiments in which we can more richly parameterize training and regularization schemes in ways that would have been previously impractical to optimize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Gradient-based optimization of gradient-based optimization</head><p>Modern neural net training procedures often employ various heuristics to set learning rate schedules, or set their shape using one or two hyperparameters set by crossvalidation <ref type="bibr" target="#b11">(Dahl et al., 2014;</ref><ref type="bibr" target="#b36">Sutskever et al., 2013)</ref>. These schedule choices are supported by a mixture of intuition, arguments about the shape of the objective function, and empirical tuning.</p><p>To more directly shed light on good learning rate schedules, we jointly optimized separate learning rates for every single learning iteration of training of a deep neural network, as well as separately for weights and biases in each layer. Each meta-iteration trained a network for 100 iterations of SGD, meaning that the learning rate schedules were specified by 800 hyperparameters (100 iterations × 4 layers × 2 types of parameters). To avoid learning an optimization schedule that depended on the quirks of a particular random initialization, each evaluation of hypergradients used a different random seed. These random seeds were used both to initialize network weights and to choose mini batches. The network was trained on 10,000 examples of MNIST, and had 4 layers, of sizes 784, 50, 50, and 50.</p><p>Because learning schedules can implicitly regularize networks <ref type="bibr" target="#b14">(Erhan et al., 2010)</ref>, for example by enforcing early stopping, for this experiment we optimized the learning rate schedules on the training error rather than on the validation set error. a fixed learning rate for all layers and iterations, it chose a learning rate of 2.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-optimization strategies</head><p>We experimented with several standard stochastic optimization methods for metaoptimization, including SGD, RMSprop <ref type="bibr" target="#b38">(Tieleman &amp; Hinton, 2012)</ref>, and minibatch conjugate gradients. The results in this section used Adam <ref type="bibr" target="#b21">(Kingma &amp; Ba, 2014)</ref>, a variant of RMSprop that includes momentum. We typically ran for 50 meta-iterations, and used a meta-step size of 0.04.  Optimizing weight initialization scales We optimized a separate weight initialization scale hyperparameter for each type of parameter (weights and biases) in each layer -a total of 8 hyperparameters. Results are shown in Figure <ref type="figure">5</ref>.</p><p>Interestingly, the initialization scale chosen for the first layer weights matches a heuristic which says to choose an initialization scale of 1/ √ N , where N is the number of weights in the layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Optimizing regularization parameters</head><p>Regularization is often important for generalization performance. Typically, a single parameter controls a single L 2 norm or sparsity penalty on the entire parameter vector of a neural network. Because different types of parameters in different layers play different roles, it is reasonable to suspect that separate regularization hyperparameter for each parameter type would improve performance. Indeed, <ref type="bibr" target="#b34">Snoek et al. (2012)</ref> optimized separate regularization parameters for each layer in a neural network, and found that it improved performance.</p><p>We can take this idea even further, and introduce a separate regularization penalty for each individual parameter in a neural network. We use a simple model as an examplelogistic regression, which can be seen as a neural network without a hidden layer. We choose this model because every weight corresponds to an input-pixel and output-label pair, meaning that these 7,840 hyperparameters might be relatively interpretable. Figure <ref type="figure" target="#fig_3">6</ref> shows a set of regularization hyperparameters learned for a logistic regression network. Because each parameter corresponds to a particular input, this regularization scheme could be seen as a generalization of automatic relevance determination <ref type="bibr" target="#b26">(MacKay &amp; Neal, 1994)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Optimizing training data</head><p>We can use Algorithm 2 to take the gradient with respect to any parameter the training procedure depends on. This includes the training data, which can be viewed as just another set of hyperparameters. By chaining gradients through transformations of the data, we can compute gradients of the validation objective with respect to data preprocessing, weighting, or augmentation procedures.</p><p>We demonstrate a simple proof-of-concept where an entire training set is learned by gradient descent, starting from blank images. Figure <ref type="figure" target="#fig_4">7</ref> shows a training set, the pixels of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Optimizing initial parameters</head><p>The last remaining parameter to SGD is the initial parameter vector. Treating this vector as a hyperparameter blurs the distinction between learning and meta-learning. In the extreme case where all elementary learning rates are set to zero, the training set ceases to matter and the meta-learning procedure exactly reduces to elementary learning on the validation set. Due to philosophical vertigo, we chose not to optimize the initial parameter vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Learning continuously parameterized architetures</head><p>Many of the notable successes in deep learning have come from novel architectures adapted to particular domains: convolutional neural nets, recurrent neural nets and multitask neural nets. We can think of these architectures as hard constraints that force particular weights to be zero and tie particular pairs of weights together. By softening these hard architectural constraints we can form continuous (but very high-dimensional) parameterizations of architecture. Having access to hypergradients makes learning these softened architectures feasible.</p><p>We illustrate this "architecture learning" with a multitask learning problem, the Omniglot data set <ref type="bibr" target="#b22">(Lake, 2014)</ref>. This data set consists of 28x28 pixel greyscale images of characters from 50 alphabets with up to 55 characters in each alphabet but only 15 examples of each character. Rather than learning a separate neural net for each alphabet, a multitask approach would be for all the neural nets to share a single first layer, pooling statistical strength to learn generic Gabor-like filters, while maintaining separate higher layers specific to each alphabet.</p><p>We can parameterize any architecture based on weight tying or weight absence with a pairwise quadratic penalty on the weights, w T Aw, where A is a number-of-weights by number-of-weights matrix. Learning this enormous matrix is clearly infeasible but we can implicitly build such a matrix from lower dimensional structures of manageable size.</p><p>For the Omniglot problem, we learn a penalty for each alphabet pair, separately for each neural net layer. Thus, for ten three-layer neural networks, the penalty matrix A is fully described by three ten-by-ten matrices. An architecture with fully independent nets for each alphabet corresponds to three diagonal matrices while an architecture with a mutual lower layer corresponds to two diagonal ma-Rotated Original trices for the upper layers and a matrix of all ones for the lowest layer (Figure <ref type="figure" target="#fig_6">9</ref>). We use five alphabets from the Omniglot set. To see whether our multitask learning system is able to learn high level similarities as well as low-level similarities, we repeat these five alphabets with the images rotated by 90 degrees (Figure <ref type="figure" target="#fig_5">8</ref>) to make ten alphabets total.</p><p>Figure <ref type="figure" target="#fig_6">9</ref> shows the learned penalties (normalized by row and column to have ones on the diagonal, akin to a correlation matrix). We see that the lowest layer has been partially shared, across all alphabets equally, with the upper layers much less shared. Interestingly, the top layer penalty learns to share weights between the rotated alphabets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Implementation Details</head><p>Automatic differentiation (AD) software packages such as Theano <ref type="bibr" target="#b1">(Bastien et al., 2012;</ref><ref type="bibr" target="#b5">Bergstra et al., 2010)</ref> are mainstays of deep learning, significantly speeding up development time by providing gradients automatically. Since we required access to the internal logic of RMD in order to implement Algorithm 2, we implemented our own automatic differentiation package for Python, available at github.com/HIPS/autograd. This package differentiates standard Numpy <ref type="bibr" target="#b28">(Oliphant, 2007)</ref> code, and can differentiate code containing while loops, branches, and even gradient evaluations.</p><p>Code for all experiments in this paper is available at github.com/HIPS/hypergrad.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Limitations</head><p>Back-propagation for training neural networks has several pitfalls that were later addressed by analysis and engineering. Likewise, the use of hypergradients also has several apparent difficulties that need to be addressed before it becomes practical. This section explores several issues with this technique that became apparent in our experiments.</p><p>When are gradients meaningful? <ref type="bibr" target="#b4">Bengio et al. (1994)</ref> noted that "learning long-term dependencies with gradient descent is difficult." Our situation is even worse: We are using gradients to optimize functions which depend on their hyperparameters through hundreds of iterations of SGD.</p><p>To make things worse, each elementary iteration's gradient itself depends on forward-and then back-propagation through a neural network. Thus the same issues that sometimes make elementary learning difficult are compounded.</p><p>For example, <ref type="bibr">Pearlmutter (1996, Chapter 4)</ref> showed that large learning rates induce chaotic behavior in the learning dynamics, making the gradient uninformative about the medium-term shape of the training objective. This phenomenon is related to the exploding-gradient problem <ref type="bibr" target="#b29">(Pascanu et al., 2012)</ref>.</p><p>Figure <ref type="figure" target="#fig_7">10</ref> illustrates this phenomenon when training a neural network having 2 hidden layers for 50 elementary iterations. We partially addressed this problem in our experiments by initializing learning rates to be relatively small, and stopping meta-optimization when the magnitude of the meta-gradient began to grow.</p><p>Overfitting How many hyperparameters can we fruitfully optimize? One limitation is overfitting the validation objective, in the same way that optimizing too many parameters can overfit the training objective. However, the same rules of thumb still apply -the size of the validation set, assuming examples are i.i.d., gives a rough guide to Bottom: Gradient of loss with respect to learning rate. When the learning rate is high, the gradient becomes uninformative about the medium-term behavior of the function. To maintain stability during meta-learning, we initialize using a small learning rate so as to approach the minimum from the left. how many hyperparameters can be optimized.</p><p>Discrete parameters Of course, gradients are not necessarily useful for optimizing discrete hyperparameters such as the number of layers, or hyperparameters that affect discrete changes such as dropout regularization parameters. Some of these difficulties could be addressed by parameterizing apparently discrete choices in a continuous manner. For instance, the per-hidden-unit regularization of section 3.2 is an example of a continuous way to choose the number of hidden units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related work</head><p>The most closely-related work is <ref type="bibr" target="#b12">Domke (2012)</ref>, who derived algorithms to compute reverse-mode derivatives of gradient descent with momentum and L-BFGS, using them to update the hyperparameters of CRF image models. However, his approach relied on naïve caching of all parameter vectors w 1 , w 2 , . . . , w T , making it impractical for large models with many training iterations. <ref type="bibr" target="#b23">Larsen et al. (1998</ref><ref type="bibr" target="#b13">), Eigenmann &amp; Nossek (1999)</ref>, <ref type="bibr" target="#b9">Chen &amp; Hagan (1999)</ref>, <ref type="bibr" target="#b3">Bengio (2000)</ref>, <ref type="bibr" target="#b0">Abdel-Gawad &amp; Ratner (2007)</ref>, and <ref type="bibr" target="#b15">Foo et al. (2008)</ref> showed that gradients of regularization parameters are available in closed form when training has converged exactly to a local minimum. In contrast, our procedure can compute exact gradients of any type of hyperparameter, whether or not learning has converged. <ref type="bibr" target="#b8">Chapelle et al. (2002)</ref> introduced a differentiable bound on the SVM loss in order to be able to compute derivatives with respect to hundreds of hyperparameters, including weighting parameters for each input dimension in the kernel. However, this bound was not tight, since optimizing the SVM objective requires a discrete selection of training points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Support vector machines</head><p>Bayesian methods For Bayesian models with a closedform marginal likelihood, gradients with respect to all continuous hyperparameters are usually available. For example, this ability has been used to construct complex kernels for Gaussian process models <ref type="bibr">(Rasmussen &amp; Williams, 2006, Chapter 5)</ref>. Variational inference also allows gradient-based tuning of hyperparameters in Bayesian neural-network models such as deep Gaussian processes <ref type="bibr" target="#b17">(Hensman &amp; Lawrence, 2014)</ref>. However, it does not provide gradients with respect to training parameters.</p><p>Gradients with respect to Markov chain parameters <ref type="bibr" target="#b33">Salimans et al. (2014)</ref> tune the step-size and mass-matrix parameters of Hamiltonian Monte Carlo by chaining gradients from a lower bound on the marginal likelihood through several iterations of leapfrog dynamics. Because they used only a small number of steps, all intermediate values could be stored naïvely. Our reversible-dynamics memory-tape approach could be used to dramatically extend the number of HMC iterations used in this approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Extensions and future work</head><p>Bayesian optimization with gradients Hypergradients could be used with parallel, model-based optimization of hyperparameters. For example, Gaussian-process-based optimization methods could incorporate gradient information <ref type="bibr" target="#b35">(Solak et al., 2003)</ref>. Such methods could make use of parallel evaluations of hypergradients, which might be too slow to evaluate in a sequential manner.</p><p>Reversible elementary computation Recurrent neural network models can require so much memory to differentiate that checkpointing is required simply to compute their elementary gradients <ref type="bibr" target="#b27">(Martens &amp; Sutskever, 2012)</ref>. Reversible computation might offer memory savings for some architectures. For example, evaluations of Long Short-Term Memory <ref type="bibr" target="#b18">(Hochreiter &amp; Schmidhuber, 1997)</ref> or a Neural Turing Machines <ref type="bibr" target="#b16">(Graves et al., 2014)</ref> rely on long chains of mostly-small updates of parameters. Exactly reversing these dynamics might allow more memory-efficient elementary gradient evaluations of their outputs on very long input sequences.</p><p>Exactly reversing other learning methods The memory saving trick from Section 2.3 could presumably be applied to other momentum-based variants of SGD such as RM-Sprop <ref type="bibr" target="#b38">(Tieleman &amp; Hinton, 2012)</ref> or Adam <ref type="bibr" target="#b21">(Kingma &amp; Ba, 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we derived a computationally efficient procedure for computing gradients through stochastic gradient descent with momentum. We showed how the approximate reversibility of learning dynamics can be used to drastically reduce the memory requirement for exactly backpropagating gradients through hundreds of training iterations.</p><p>We showed how these gradients allow the optimization of validation loss with respect to thousands of hyperparameters, something which was previously infeasible. This new ability allows the automatic tuning of most details of training neural networks. We demonstrated the tuning of detailed training schedules, regularization schedules, and neural network architectures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Figure2. A learning-rate training schedule for the weights in each layer of a neural network, optimized by hypergradient descent. The optimized schedule starts by taking large steps only in the topmost layer, then takes larger steps in the first layer. All layers take smaller step sizes in the last 10 iterations. Not shown are the schedules for the biases or the momentum, which showed less structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure  3shows the elementary and meta-learning curves that generated the hyperparameters shown in Figure2.How smooth are hypergradients? To demonstrate that the hypergradients are smooth with respect to time steps in the training schedule, Figure4shows the hypergradient with respect to the step size training schedule at the beginning of training, averaged over 100 random seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Figure 4. The initial gradient of the cross-validation loss with respect to the training schedule, averaged over 100 random weight initializations and mini batches. Colors correspond to the same layers as in Figure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Optimized L2 regularization hyperparameters for each weight in a logistic regression trained on MNIST. The weights corresponding to each output label (0 through 9 respectively) have been rendered separately. High values (black) indicate strong regularization.</figDesc><graphic url="image-1.png" coords="5,312.65,71.67,188.16,77.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Figure7. A dataset generated purely through meta-learning. Each pixel is treated as a hyperparameter, which are all optimized to maximize validation-set performance. Training labels are fixed in order from 0 to 9. Some optimal pixel values are negative.</figDesc><graphic url="image-2.png" coords="5,312.58,587.50,185.61,76.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Top: Example characters from 5 alphabets taken from the Omniglot dataset. Bottom: Those same alphabets with each character rotated by 90 • . Distinguishing characters within each of these 10 alphabets constitute the 10 tasks in our multi-task learning experiment.</figDesc><graphic url="image-3.png" coords="6,326.94,67.06,198.90,133.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Figure9. Results of the Omniglot multitask experiment. Each matrix shows the degree of weight sharing between each pair of tasks for that layer. Top: A separate network is trained independently for each task. Middle: The lowest-level features were forced to be shared. Bottom: The degree of weight sharing between tasks was optimized by hyperparameter optimization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Top: Loss after training as a function of learning rate.Bottom: Gradient of loss with respect to learning rate. When the learning rate is high, the gradient becomes uninformative about the medium-term behavior of the function. To maintain stability during meta-learning, we initialize using a small learning rate so as to approach the minimum from the left.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">†  The order of these two authors is random. See github.com/hips/author-roulette</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">Since this paper is about hyperparameters, we use "elementary" to unambiguously denote the other sort of parameter, the "parameter-that-is-just-a-parameter-and-not-a-hyperparameter".</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">We assume fixed-point representation to simplify the discussion (and the implementation).<ref type="bibr" target="#b10">Courbariaux et al. (2014)</ref> show that fixed-point arithmetic is sufficient to train deep networks. Floating point representation doesn't fix the problem, it just defers the loss of information from the division step to the addition step.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Christian Steinruecken, Oren Rippel, and Matthew James Johnson for helpful discussions. We also thank Brenden Lake for graciously providing the Omniglot dataset. Thanks to Jason Rolfe for helpful feedback. We thank Analog Devices International and Samsung Advanced Institute of Technology for their support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Adaptive optimization of hyperparameters in L2-regularised logistic regression</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Abdel-Gawad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Ratner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><surname>Pascal</surname></persName>
		</author>
		<author>
			<persName><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName><surname>Arnaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic differentiation of algorithms for machine learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Baydin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Pearlmutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AutoML Workshop at the International Conference on Machine Learning (ICML)</title>
				<meeting>the AutoML Workshop at the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gradient-based optimization of hyperparameters</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1889" to="1900" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName><surname>Frédéric</surname></persName>
		</author>
		<author>
			<persName><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><surname>Pascal</surname></persName>
		</author>
		<author>
			<persName><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName><surname>Turian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Warde-Farley</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for Scientific Computing Conference (SciPy)</title>
				<meeting>the Python for Scientific Computing Conference (SciPy)</meeting>
		<imprint>
			<date type="published" when="2010-06">June 2010</date>
		</imprint>
	</monogr>
	<note>Oral Presentation</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Algorithms for hyper-parameter optimization</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName><surname>Rémi</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Yoshua</surname></persName>
		</author>
		<author>
			<persName><surname>Kégl</surname></persName>
		</author>
		<author>
			<persName><surname>Balázs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="115" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Choosing multiple parameters for support vector machines</title>
		<author>
			<persName><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sayan</forename><surname>Mukherjee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="131" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Optimal use of regularization and cross-validation in neural network modeling</title>
		<author>
			<persName><forename type="first">Dingding</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">T</forename><surname>Hagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1275" to="1280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Low precision arithmetic for deep learning</title>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Pierre</forename><surname>David</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7024</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><surname>Navdeep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1231</idno>
		<title level="m">Multi-task neural networks for QSAR predictions</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generic methods for optimization-based modeling</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Domke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="318" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gradient based adaptive regularization</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Eigenmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><forename type="middle">A</forename><surname>Nossek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1999 IEEE Signal Processing Society Workshop on Neural Networks</title>
				<meeting>the 1999 IEEE Signal Processing Society Workshop on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning?</title>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Yoshua</surname></persName>
		</author>
		<author>
			<persName><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName><surname>Pierre-Antoine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Samy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient multiple hyperparameter learning for log-linear models</title>
		<author>
			<persName><forename type="first">Chuan-Sheng</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuong</forename><forename type="middle">B</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="377" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Nested variational compression in deep Gaussian processes</title>
		<author>
			<persName><forename type="first">James</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><forename type="middle">D</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1370</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long shortterm memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Building a better leapfrog</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Makino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mcmillan</surname></persName>
		</author>
		<idno type="DOI">10.1086/187844</idno>
	</analytic>
	<monogr>
		<title level="j">Astrophysical Journal, Part 2 -Letters</title>
		<imprint>
			<biblScope unit="volume">443</biblScope>
			<biblScope unit="page" from="L93" to="L96" />
			<date type="published" when="1995-04">April 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sequential model-based optimization for general algorithm configuration</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Holger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyton-Brown</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LION-5</title>
				<meeting>LION-5<address><addrLine>Kevin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">6683</biblScope>
			<biblScope unit="page" from="507" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Towards more human-like concept learning in machines: Compositionality, causality, and learning-to-learn</title>
		<author>
			<persName><forename type="first">Brenden</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adaptive regularization in neural network modeling</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><surname>Svarer</surname></persName>
		</author>
		<author>
			<persName><surname>Claus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><surname>Nonboe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><surname>Kai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="113" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Information theory, inference, and learning algorithms</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Mackay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Automatic relevance determination for neural networks</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
		<author>
			<persName><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
		<respStmt>
			<orgName>Cambridge University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">In Technical Report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Training deep and recurrent networks with hessian-free optimization</title>
		<author>
			<persName><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="479" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Python for scientific computing</title>
		<author>
			<persName><forename type="first">Travis</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing in Science &amp; Engineering</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="10" to="20" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Understanding the exploding gradient problem</title>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.5063</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">An investigation of the gradient descent process in neural networks</title>
		<author>
			<persName><forename type="first">Barak</forename><surname>Pearlmutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast exact multiplication by the Hessian</title>
		<author>
			<persName><forename type="first">Barak</forename><forename type="middle">A</forename><surname>Pearlmutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="147" to="160" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Gaussian Processes for Machine Learning</title>
		<author>
			<persName><forename type="first">Carl</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.6460</idno>
		<title level="m">Markov chain Monte Carlo and variational inference: Bridging the gap</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Practical Bayesian optimization of machine learning algorithms</title>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larochelle</forename><surname>Hugo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2960" to="2968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Derivative observations in Gaussian process models of dynamic systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Solak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Murray Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Leithead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Leith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1057" to="1064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML-13)</title>
				<meeting>the 30th International Conference on Machine Learning (ICML-13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Quoc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Lecture 6.5-RmsProp: Divide the gradient by a running average of its recent magnitude. Coursera: Neural Networks for Machine Learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
