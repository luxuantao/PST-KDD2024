<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MSA Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Roshan</forename><surname>Rao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jason</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>Verkuil</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Joshua</forename><surname>Meier</surname></persName>
						</author>
						<author>
							<persName><forename type="first">John</forename><forename type="middle">F</forename><surname>Canny</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><surname>Sercu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Rives</surname></persName>
						</author>
						<title level="a" type="main">MSA Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1101/2021.02.12.430858</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised protein language models trained across millions of diverse sequences learn structure and function of proteins. Protein language models studied to date have been trained to perform inference from individual sequences. The longstanding approach in computational biology has been to make inferences from a family of evolutionarily related sequences by fitting a model to each family independently. In this work we combine the two paradigms. We introduce a protein language model which takes as input a set of sequences in the form of a multiple sequence alignment. The model interleaves row and column attention across the input sequences and is trained with a variant of the masked language modeling objective across many protein families. The performance of the model surpasses current state-ofthe-art unsupervised structure learning methods by a wide margin, with far greater parameter efficiency than prior state-of-the-art protein language models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Unsupervised models learn protein structure from patterns in sequences. Sequence variation within a protein family conveys information about the structure of the protein <ref type="bibr" target="#b52">(Yanofsky et al., 1964;</ref><ref type="bibr" target="#b2">Altschuh et al., 1988;</ref><ref type="bibr" target="#b13">GÃ¶bel et al., 1994)</ref>. Since evolution is not free to choose the identity of amino acids independently at sites that are in contact in the folded three-dimensional structure, patterns are imprinted onto the sequences selected by evolution. Constraints on the structure of a protein can be inferred from patterns in related sequences. The predominant unsupervised approach is to fit a Markov Random Field in the form of a Potts Model to a family of aligned sequences to extract a coevolutionary 1 UC Berkeley 2 Work performed during internship at FAIR. 3 Facebook AI Research 4 New York University. Code and weights available at https://github.com/facebookresearch/ esm. Correspondence to: Roshan Rao &lt;rmrao@berkeley.edu&gt;, Alexander Rives &lt;arives@fb.com&gt;. signal <ref type="bibr" target="#b23">(Lapedes et al., 1999;</ref><ref type="bibr" target="#b46">Thomas et al., 2008;</ref><ref type="bibr" target="#b50">Weigt et al., 2009)</ref>.</p><p>A new line of work explores unsupervised protein language models <ref type="bibr" target="#b1">(Alley et al., 2019;</ref><ref type="bibr" target="#b40">Rives et al., 2020;</ref><ref type="bibr" target="#b15">Heinzinger et al., 2019;</ref><ref type="bibr">Rao et al., 2019)</ref>. This approach fits large neural networks with shared parameters across millions of diverse sequences, rather than fitting a model separately to each family of sequences. At inference time, a single forward pass of an end-to-end model replaces the multistage pipeline, involving sequence search, alignment, and model fitting steps, standard in bioinformatics. Recently, promising results have shown that protein language models learn secondary structure, long-range contacts, and function via the unsupervised objective <ref type="bibr" target="#b40">(Rives et al., 2020)</ref>, making them an alternative to the classical pipeline. While small and recurrent models fall well short of state-of-the-art <ref type="bibr">(Rao et al., 2019)</ref>, the internal representations of very large transformer models are competitive with Potts models for unsupervised structure learning <ref type="bibr" target="#b40">(Rives et al., 2020;</ref><ref type="bibr">Rao et al., 2021)</ref>.</p><p>Potts models have an important advantage over protein lan-guage models during inference. The input to the Potts model is a set of sequences. Inference is performed by fitting a model that directly extracts the covariation signal from the input. Current protein language models take a single sequence as input for inference. Information about evolutionary variation must be stored in the parameters of the model during training. As a result, protein language models require many parameters to represent the data distribution well.</p><p>In this work, we unify the two paradigms within a protein language model that takes sets of aligned sequences as input, but shares parameters across many diverse sequence families. Like prior protein language models operating on individual sequences, the approach benefits from learning from common patterns across protein families, allowing information to be generalized and transferred between them. By taking sets of sequences as input, the model gains the ability to extract information during inference, which improves the parameter efficiency.</p><p>We introduce the MSA Transformer, a model operating on sets of aligned sequences. The input to the model is a multiple sequence alignment. The architecture interleaves attention across the rows and columns of the alignment as in axial attention <ref type="bibr">(Ho et al., 2019)</ref>. We propose a variant of axial attention which shares a single attention map across the rows. The model is trained using the masked language modeling objective. Self supervision is performed by training the model to reconstruct a corrupted MSA.</p><p>We train an MSA Transformer model with 100M parameters on a large dataset (4.3 TB) of 26 million MSAs, with an average of 1192 sequences per MSA. The resulting model surpasses current state-of-the-art unsupervised structure learning methods by a wide margin, outperforming Potts models and protein language models with 650M parameters. The model improves over state-of-the-art unsupervised contact prediction methods across all multiple sequence alignment depths, with an especially significant advantage for MSAs with lower depth. Information about the contact pattern emerges directly in the tied row attention maps. Evaluated in a supervised contact prediction pipeline, features captured by the MSA Transformer outperform trRosetta <ref type="bibr" target="#b51">(Yang et al., 2019)</ref> on the CASP13 and CAMEO test sets. We find that high precision contact predictions can be extracted from small sets of diverse sequences, with good results from as few as 8-16 sequences. We investigate how the model performs inference by independently destroying the covariation or sequence patterns in the input, finding that the model uses both signals to make predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Unsupervised Contact Prediction The standard approach to unsupervised protein structure prediction is to identify pairwise statistical dependencies between the columns of an MSA, which are modeled as a Potts model Markov Random Field (MRF). Since exact inference is computationally intractable, a variety of methods have been proposed to efficiently fit the MRF, including mean-field inference <ref type="bibr">(Morcos et al., 2011)</ref>, sparse-inverse covariance estimation <ref type="bibr" target="#b18">(Jones et al., 2012)</ref>, and the current state-of-the-art pseudolikelihood maximization <ref type="bibr" target="#b3">(Balakrishnan et al., 2011;</ref><ref type="bibr" target="#b10">Ekeberg et al., 2013;</ref><ref type="bibr" target="#b37">Seemayer et al., 2014)</ref>. In this work we use Potts models fit with psuedolikelihood maximization as a baseline, and refer to features generated from Potts models as "co-evolutionary features." Making a connection with the attention mechanism we study here, <ref type="bibr">Bhattacharya et al. (2020)</ref> show that a single layer of self-attention can perform essentially the same computation as a Potts model.</p><p>Deep Models of MSAs Several groups have proposed to replace the shallow MRF with a deep neural network. Riesselman et al. ( <ref type="formula">2018</ref>) train deep variational autoencoders on MSAs to predict function. <ref type="bibr" target="#b33">Riesselman et al. (2019)</ref> train autoregressive models on MSAs, but discard the alignment, showing that function can be learned from unaligned sequences. In contrast to our approach which is trained on many MSAs, these existing models are trained on a single set of related sequences and do not provide a direct method of extracting protein contacts.</p><p>Supervised Structure Prediction Supervised structure prediction using deep neural networks has driven groundbreaking progress on the protein structure prediction problem <ref type="bibr" target="#b38">(Senior et al., 2019;</ref><ref type="bibr">Jumper et al., 2020)</ref>. Initial models used coevolutionary features <ref type="bibr" target="#b49">(Wang et al., 2017;</ref><ref type="bibr" target="#b24">Liu et al., 2018;</ref><ref type="bibr" target="#b51">Yang et al., 2019;</ref><ref type="bibr" target="#b38">Senior et al., 2019;</ref><ref type="bibr">Adhikari &amp; Elofsson, 2020)</ref>. Recently MSAs have been proposed as input to supervised structure prediction methods. <ref type="bibr" target="#b27">Mirabello &amp; Wallner (2019)</ref> and <ref type="bibr">Kandathil et al. (2020)</ref> study models which take MSAs as input directly, respectively using 2D convolutions or GRUs to process the input. More recently, AlphaFold2 <ref type="bibr">(Jumper et al., 2020)</ref> uses attention to process MSAs in an end-to-end model supervised with structures.</p><p>The central difference in our work is to model a collection of MSAs using unsupervised learning. This results in a model that contains features potentially useful for a range of downstream tasks. We use the emergence of structure in the internal representations of the model to measure the ability of the model to capture biology from sequences. This is a fundamentally distinct problem setting from supervised structure prediction. The MSA Transformer is trained in a purely unsupervised manner and learns contacts without being trained on protein structures.</p><p>Large protein sequence databases contain billions of sequences and are undergoing exponential growth. Unsupervised methods can directly use these datasets for learning, while supervised methods are limited to supervision from the hundreds of thousands of crystallized structures. Unsupervised methods can learn from regions of sequence space not covered by structural knowledge.</p><p>Protein Language Models Protein language modeling has emerged as a promising approach for unsupervised learning of protein sequences. <ref type="bibr" target="#b4">Bepler &amp; Berger (2019)</ref> combined unsupervised sequence pre-training with structural supervision to produce sequence embeddings. <ref type="bibr" target="#b1">Alley et al. (2019)</ref> and <ref type="bibr" target="#b15">Heinzinger et al. (2019)</ref> showed that LSTM language models capture some biological properties. Simultaneously, <ref type="bibr" target="#b40">Rives et al. (2020)</ref> proposed to model protein sequences with self-attention, showing that transformer protein language models capture accurate information of structure and function in their representations. <ref type="bibr">Rao et al. (2019)</ref> evaluated a variety of protein language models across a panel of benchmarks concluding that small LSTMs and transformers fall well short of features from the bioinformatics pipeline.</p><p>A combination of model scale and architecture improvements has been instrumental to recent successes in protein language modeling. <ref type="bibr" target="#b11">Elnaggar et al. (2020)</ref> study a variety of transformer variants. <ref type="bibr" target="#b40">Rives et al. (2020)</ref> show that large transformer models produce state-of-the-art features across a variety of tasks. Notably, the internal representations of transformer protein language models are found to directly represent contacts. <ref type="bibr">Vig et al. (2020)</ref> find that specific attention heads of pre-trained transformers correlate directly with protein contacts. <ref type="bibr">Rao et al. (2021)</ref> combine multiple attention heads to predict contacts more accurately than Potts models, despite using just a single sequence for inference.</p><p>Alternatives to the masked language modeling objective have also been explored, such as conditional generation <ref type="bibr" target="#b26">(Madani et al., 2020)</ref> and contrastive loss functions <ref type="bibr">(Lu et al., 2020)</ref>. Most relevant to our work, <ref type="bibr">Sturmfels et al. (2020)</ref> and <ref type="bibr" target="#b40">Sercu et al. (2020)</ref> study alternative learning objectives using sets of sequences for supervision. <ref type="bibr">Sturmfels et al. (2020)</ref> extended the unsupervised language modeling to predict the position specific scoring matrix (PSSM) profile. <ref type="bibr" target="#b40">Sercu et al. (2020)</ref> used amortized optimization to simultaneously predict profiles and pairwise couplings. However, prior work on protein language models has not considered sets of sequences as inputs to the language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>Transformers are powerful sequence models capable of passing information from any position to any other position <ref type="bibr" target="#b47">(Vaswani et al., 2017)</ref>. However, they are not trivially applied to a set of aligned sequences. Naively concatenating M sequences of length L in an MSA would allow attention across all sequences, but the (M L) 2 self-attention maps would be prohibitively memory-intensive. The main con-tribution of this paper is to extend transformer pre-training to operate on an MSA, while respecting its structure as an M Ã L character matrix.</p><p>We describe the input MSA as a matrix x â R M ÃL , where rows correspond to sequences in the MSA, columns are positions in the aligned sequence, and entries x mi take integer values<ref type="foot" target="#foot_0">1</ref> encoding the amino acid identity of sequence m at position i. After embedding the input, each layer has a R M ÃLÃd state as input and output. For the core of the transformer, we adapt the axial attention approach from <ref type="bibr">Ho et al. (2019)</ref> and <ref type="bibr" target="#b6">Child et al. (2019)</ref>. This approach alternates attention over rows and columns of the 2D state (see Fig. <ref type="figure" target="#fig_0">1</ref>). This sparsity pattern in the attention over the MSA brings the attention cost to O(LM 2 ) for the column attention, and O(M L 2 ) for the row attention.</p><p>Feedforward Layers We deviate from <ref type="bibr">Ho et al. (2019)</ref> in the interleaving of the feedforward layers. Rather than applying a feedforward layer after each row or column attention, we apply row and column attention followed by a single feedforward layer (see Fig. <ref type="figure" target="#fig_0">1</ref>). This choice follows more closely the transformer decoder architecture <ref type="bibr" target="#b47">(Vaswani et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tied Row Attention</head><p>The standard implementation of axial attention allows for independent attention maps for each row and column of the input. However, in an MSA each sequence should have a similar structure; indeed, directcoupling analysis exploits this fact to learn contact information. To leverage this shared structure we hypothesize it would be beneficial to tie the row attention maps between the sequences in the MSA. As an additional benefit, tied attention reduces the memory footprint of the row attentions from O(M L 2 ) to O(L 2 ).</p><p>Let M be the number of rows, d be the hidden dimension and Q m , K m be the matrix of queries and keys for the m-th row of input. We define tied row attention (before softmax is applied) to be</p><formula xml:id="formula_0">M m=1 Q m K T m Î»(M, d)<label>(1)</label></formula><p>The denominator Î»(M, d) would be the normalization constant â d in standard scaled-dot product attention. In tied row attention, we explore two normalization functions to prevent attention weights linearly scaling with the number of input sequences:</p><formula xml:id="formula_1">Î»(M, d) = M â d (mean normaliza- tion) and Î»(M, d) = â M d (square-root normalization).</formula><p>Our final model uses square-root normalization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training Objective</head><p>We adapt the masked language modeling objective <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref> to the MSA setting. The loss for an MSA x, and masked MSA x is as follows:</p><formula xml:id="formula_2">L MLM (x; Î¸) = (m,i)âmask log p(x mi | x; Î¸)<label>(2)</label></formula><p>The probabilities are the output of the MSA transformer, softmax normalized over the amino acid vocabulary independently per position i in each sequence m. We consider masking tokens uniformly at random over the MSA or masking entire columns of the MSA, and achieve the best performance by masking tokens uniformly at random over the MSA (Table <ref type="table" target="#tab_7">6</ref>). Note that the masked token can be predicted not only from context amino acids at different positions but also from related sequences at the same position.</p><p>Pre-training Dataset Models are trained on a dataset of 26 million MSAs. An MSA is generated for each UniRef50 <ref type="bibr" target="#b45">(Suzek et al., 2007)</ref> sequence by searching UniClust30 <ref type="bibr" target="#b28">(Mirdita et al., 2017)</ref> with HHblits <ref type="bibr">(Steinegger et al., 2019)</ref>.</p><p>The average depth of the MSAs is 1192. See Fig. <ref type="figure" target="#fig_7">8</ref> for MSA depth distribution. MSA Subsampling During Inference At inference time, memory is a much smaller concern. Nevertheless we do not provide the full MSA to the model as it would be computationally expensive and the model's performance can decrease when the input is much larger than that used during training. Instead, we explore four strategies for subsampling the sequences provided to the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models and Training</head><p>â¢ Random: This strategy parallels the one used at training time, and selects random sequences from the MSA (ensuring that the reference sequence is always included).</p><p>â¢ Diversity Maximizing: This is a greedy strategy which starts from the reference and adds the sequence with highest average hamming distance to current set of sequences.</p><p>â¢ Diversity Minimizing: This strategy is equivalent to the Diversity Maximizing strategy, but adds the sequence with lowest average hamming distance. It is used to explore the effects of diversity on model performance.</p><p>â¢ HHFilter: This strategy applies hhfilter <ref type="bibr">(Steinegger et al., 2019)</ref> with the -diff M parameter, which returns M or more sequences that maximize diversity (the result is usually close to M ). If more than M sequences are returned we apply the Diversity Maximizing strategy on top of the output. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>We study the MSA Transformer in a panel of structure prediction tasks, evaluating unsupervised contact prediction from the attentions of the model, and performance of features in supervised contact and secondary structure prediction pipelines.</p><p>To calibrate the difficulty of the masked language modeling task for MSAs, we compare against two simple prediction strategies using the information in the MSA: (i) column frequency baseline, and (ii) nearest sequence baseline. These baselines implement the intuition that a simple model could use the column frequencies to make a prediction at the masked positions, or copy the identity of the missing character from the most similar sequence in the input. Table <ref type="table">5</ref> reports masked language modeling performance. The MSA Transformer model (denoising accuracy of 56.6) significantly outperforms the PSSM (accuracy 38.9) and nearest-neighbor (accuracy 39.1) baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Unsupervised Contact Prediction</head><p>Unsupervised contact prediction has been used to measure the ability of transformer protein language models to capture information about protein structure in their internal representations. We compare to two state-of-the-art transformer protein language models: ESM-1b <ref type="bibr" target="#b40">(Rives et al., 2020)</ref> with 650M parameters and ProTrans-T5 <ref type="bibr" target="#b11">(Elnaggar et al., 2020)</ref> with 3B parameters. We follow the methodology of Rao et al. ( <ref type="formula">2021</ref>) using the same validation set of 14,842 structures and corresponding MSAs. We fit a logistic regression to identify a sparse combination of attention heads that represent contacts. At inference time, we use hhfilter to subsample 256 sequences. For the single-sequence protein language models we use the sequence belonging to the structure as input. We also compare against Potts models using the APC-corrected <ref type="bibr" target="#b9">(Dunn et al., 2008)</ref> Frobenius norm of the coupling matrix computed on the MSA <ref type="bibr" target="#b20">(Kamisetty et al., 2013)</ref>.</p><p>Table <ref type="table" target="#tab_1">1</ref> compares unsupervised contact prediction performance of the models. The MSA Transformer significantly outperforms all baselines, increasing top-L long-range con-  <ref type="bibr" target="#b14">(Haas et al., 2018)</ref> and CASP13-FM <ref type="bibr" target="#b41">(Shrestha et al., 2019)</ref>. The CASP13-FM test set consists of 31 free modeling domains (from 25 targets); the CAMEO hard targets are a set of 131 domains (out of which we evaluate on the 129 that fit within the 1024 character maximum context length of the model). On the CASP13-FM test set, unsupevised contact prediction with the MSA Transformer (42.7 top-L long-range precision) is competitive with the trRosetta base model (42.4 top-L long-range precision), a fully supervised structure prediction model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Supervised Contact Prediction</head><p>Used independently, features from current state-of-the-art protein language models fall short of co-evolutionary features from Potts models on supervised contact prediction tasks <ref type="bibr" target="#b40">(Rives et al., 2020)</ref>.</p><p>We evaluate the MSA Transformer as a component of a supervised structure prediction pipeline. Following <ref type="bibr" target="#b40">Rives et al. (2020)</ref>, we train a deep residual network with 32 preactivation blocks, each with a filter size of 64, using learning rate 0.001. The network is supervised with binned pairwise distance distributions (distograms) using the trRosetta training set <ref type="bibr" target="#b51">(Yang et al., 2019)</ref> of 15,051 MSAs and structures.</p><p>We evaluate two different ways of extracting features from the model. In the first, we use the outer concatenation of the output embedding of the query sequence. In the second, we combine the outer concatenation with the symmetrized row self-attention maps. For comparison, we train the same residual network over co-evolutionary features from Potts models <ref type="bibr" target="#b37">(Seemayer et al., 2014)</ref>. Additionally we compare to features from state-of-the-art protein language models ESM-1b and ProTrans-T5 using the outer concatenation of the sequence embeddings. We also compare to trRosetta <ref type="bibr" target="#b51">(Yang et al., 2019)</ref>, a state-of-the-art supervised structure prediction method prior to AlphaFold2 <ref type="bibr">(Jumper et al., 2020)</ref>.</p><p>The MSA Transformer produces a substantial improvement over co-evolutionary features for supervised contact prediction. Table <ref type="table" target="#tab_3">3</ref> shows a comparison between the models on the CASP13-FM and CAMEO test sets. The best MSA Transformer model, using the combination of attention maps with features from the final hidden layer, outperforms all other models including the trRosetta baseline model (which uses 36 residual blocks) and the trRosetta full model (which uses 71.6 Â± 0.1 MSA Transformer 72.9 Â± 0.2 61 residual blocks, data augmentation via MSA subsampling, and predicts inter-residue orientations). No model ensembling was used in the evaluation of the trRosetta models. Table <ref type="table" target="#tab_9">8</ref> gives additional comparisons with LSTM and transformer protein language models available in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Secondary Structure Prediction</head><p>To further evaluate the quality of representations generated by the MSA Transformer, we train a state-of-the-art downstream head based on the Netsurf architecture <ref type="bibr" target="#b22">(Klausen et al., 2019)</ref>. The downstream model is trained to predict 8-class secondary structure from the pretrained representations. We evaluate models on the CB513 test set <ref type="bibr" target="#b7">(Cuff &amp; Barton, 1999)</ref>. The models are trained on the Netsurf training dataset. Representations from the MSA Transformer (72.9%) surpass the performance of HMM profiles (71.2%) and ESM-1b embeddings (71.6%) (Table <ref type="table" target="#tab_4">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>We perform an ablation study over seven model hyperparameters, using unsupervised contact prediction on the vali- dation set for evaluation. For each combination of hyperparameters, a model is pre-trained with the masked language modeling objective for 100k updates. Training curves for the models are shown in Fig. <ref type="figure" target="#fig_8">9</ref> and Top-L long-range precision is reported in Table <ref type="table" target="#tab_7">6</ref>.</p><p>The ablation studies show the use of tied attention plays a critical role in model performance. After 100k updates, a model trained with square-root normalized tied attention outperforms untied attention by more than 17 points and outperforms mean normalized tied-attention by more than 6 points on long-range contact precision.</p><p>Parameter count also affects contact precision. A model with half the embedding size (384) and only 30M parameters reaches a long-range precision of 52.8 after 100k updates, 3.5 points lower than the base model, yet 11.7 points higher than the state-of-the-art 650M parameter single-seequence model. See Appendix A.3 for further discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Model Analysis</head><p>We examine how the model uses its input MSA in experiments to understand the role of sequence diversity, attention patterns, and covariation in the MSA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Effect of MSA diversity</head><p>The diversity of the input sequences strongly influences inference of structure. We explore three inference time strategies to control the diversity of the input sequence sets: (i) diversity maximizing, (ii) diversity minimizing, and (iii) random selection (see Section 3). Fig. <ref type="figure">4</ref> shows average performance across the test set for each selection strategy as the number of sequences used for input increases. Two approaches to maximize diversity, greedy hamming distance maximization and hhfilter, perform equivalently. Both strategies surpass ESM-1b performance with just 16 input sequences. In comparison, the diversity minimizing strategy, hamming distance minimization, performs poorly, requiring 256 sequences to surpass ESM-1b. Random selection performs well, although it falls behind the diversity maximizing strategies. The qualitative effects of MSA diversity are illustrated in Fig. <ref type="figure" target="#fig_3">3</ref>, where the addition of just one high-diversity sequence outperforms the addition of 31 low-diversity sequences.</p><p>In principle, the model's attention could allow it to identify and focus on the most informative parts of the input MSA. We find row attention heads that preferentially attend to highly variable columns. We also identify specific column attention heads that attend to more informative sequences. In this experiment random subsampling is used to select inputs for the model. Fig. <ref type="figure" target="#fig_4">5</ref> compares the distribution of attention weights with two measures of MSA diversity: (i) per-column entropy of the MSA; and (ii) computed sequence weights (Appendix A.13). Per column entropy gives a measure of how variable a position is in the MSA. Computed sequence weights measure how informative a sequence is in the context of the other sequences in the MSA. Sequences with few similar sequences receive high weights. The maximum average Pearson correlation between a row attention head and column entropy is 0.59. The maximum average Pearson correlation between a column attention head and sequence weights is 0.58. These correlations between attention weights and measures of MSA diversity suggest the model is specifically looking for informative sequences when processing the input. 5ahw, chain: A) of model performance after independently shuffling each column of an MSA to destroy covariance information, and after independently permuting the order of positions to destroy sequence patterns. The MSA Transformer maintains reasonable performance under both conditions. A Potts model fails on the covariance-shuffled MSA, while a single-sequence language model (ESM-1b) fails on the position-shuffled sequence. Right: Model performance before and after shuffling, binned by depth of the original (non-subsampled) MSA. 1024 sequence selected with hhfilter are used as input to MSA Transformer and Potts models. MSAs with fewer than 1024 sequences are not considered in this analysis. Average Top-L long-range precision drops from 52.9 (no ablation) to 15.9 (shuffled covariance) and 27.9 (shuffled positions) respectively. A Null (random guessing) baseline is also considered. Potts model performance drops to the Null baseline under the first condition and ESM-1b performance drops to the Null baseline under the second condition. The MSA Transformer produces reasonable predictions under both scenarios, implying it uses both modes of inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Attention Corresponds to Protein Contacts</head><p>In Section 4.1, we use the heads in the model's tied row attention directly to predict contacts in the protein's threedimensional folded structure. Following <ref type="bibr">Rao et al. (2021)</ref> we fit a sparse logistic regression to the model's row attention maps to identify heads that correspond with contacts. Fig. <ref type="figure" target="#fig_6">7</ref> shows the weight values in the learned sparse logistic regression fit using 20 structures. A sparse subset (55 / 144) of heads are predictive of protein contacts. The most predictive heads are concentrated in the final layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Inference: Covariance vs. Sequence Patterns</head><p>Potts models and single-sequence language models predict protein contacts in fundamentally different ways. Potts models are trained on a single MSA; they extract information directly from the covariance between mutations in columns of the MSA. Single-sequence language models do not have access to the MSA, and instead make predictions based on patterns seen during training. The MSA Transformer may use both covariance-based and pattern-based inference. To disentangle the two modes, we independently ablate the covariance and sequence patterns in the model's input via random shuffling. To ensure that there is enough information in the input for covariance-based extraction to succeed, we subsample each MSA to 1024 sequences using hhfilter, and apply the model to unshuffled and shuffled inputs. To avoid the confound of some MSAs having fewer sequences, we only consider MSAs with at least 1024 sequences.</p><p>To remove covariance information, we randomly permute the values in each column of the MSA. This preserves percolumn amino acid frequencies (PSSM information) while destroying pairwise correlations between columns. Under this condition, Potts model performance drops to the random guess baseline. Since ESM-1b takes a single sequence as input, the permutation trivially produces the same sequence, and the result is unaffected. Unlike the Potts model, the MSA Transformer retains some ability to predict contacts, which increases sharply as a function of MSA Depth. This indicates that the model can make predictions from patterns in the sequence profile in the absence of covariance.</p><p>To remove sequence patterns seen during training, we randomly permute the order of positions (columns) in the MSA. This preserves all covariance information between pairs of columns, but results in an input that is highly dissimilar to a real protein. Under this condition, Potts model performance is unaffected since its parameterization is invariant to sequence order. ESM-1b performance drops to the random guess baseline. The MSA Transformer does depend on sequence order, and predicts spurious contacts along the diagonal of the reordered sequence. When predicted contacts with sequence separation &lt; 6 are removed, the remaining predictions align with the correct contacts. Model performance does not increase significantly with MSA depth. This shows the model can predict directly from covariance when presented with sequence patterns unobserved in training.</p><p>Together these ablations independently destroy the information used by Potts models and single-sequence language models, respectively. Under both conditions, the MSA Transformer maintains some capability to predict contacts, demonstrating that it uses both modes of inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>Prior work in unsupervised protein language modeling has focused on inference from individual sequences. We study an approach to perform inference from a set of aligned sequences in an MSA. We use axial attention to efficiently parameterize attention over the rows and columns of the MSA. This approach enables the model to extract information from dependencies in the input set and generalize patterns across MSAs. We find the internal representations of the model enable state-of-the-art unsupervised structure learning with an order of magnitude fewer parameters than current protein language models. While supervised methods have produced breakthrough results for protein structure prediction <ref type="bibr">(Jumper et al., 2020)</ref>, unsupervised learning provides a way to extract the information contained in massive datasets of sequences produced by low cost gene sequencing. Unsupervised methods can learn from billions of sequences, enabling generalization to regions of sequence space not covered by structural knowledge.</p><p>Models fit to MSAs are widely used in computational biology including in applications such as fitness landscape prediction <ref type="bibr" target="#b34">(Riesselman et al., 2018)</ref>, pathogenicity prediction <ref type="bibr" target="#b44">(Sundaram et al., 2018;</ref><ref type="bibr">Frazer et al., 2020)</ref>, remote homology detection <ref type="bibr" target="#b17">(Hou et al., 2018)</ref>, and protein design <ref type="bibr">(Russ et al., 2020)</ref>. The improvements we observe for structure learning suggest the unsupervised language modeling approach here could also apply to these problems.</p><p>Prior work in protein language models has established a link between the scale of models and performance across a variety of prediction tasks related to structure and function <ref type="bibr" target="#b40">(Rives et al., 2020)</ref>. Further scaling the approach studied here in the number of parameters and input sequences is a potential direction for investigating the limits of unsupervised learning for protein sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Unsupervised Contact Prediction</head><p>For unsupervised contact prediction, we adopt the methodology from <ref type="bibr">Rao et al. (2021)</ref>, which shows that sparse logistic regression trained on the attention maps of a single-sequence transformer is sufficient to predict protein contacts using a small number (between 1 â 20) of training structures. To predict the probability of contact between amino acids at position i and j, the attention maps from each layer and head are independently symmetrized and corrected with APC <ref type="bibr" target="#b9">(Dunn et al., 2008)</ref>. The input features are then the values Älhij for each layer l and head h. The models have 12 layers and 12 heads for a total of 144 attention heads. An L1-regularization coefficient of 0.15 is applied. The regression is trained on all contacts with sequence separation â¥ 6. 20 structures are used for training. Trained regression weights are shown in Fig. <ref type="figure" target="#fig_6">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Dataset Generation</head><p>For the unsupervised training set we retrieve the UniRef-50 <ref type="bibr" target="#b45">(Suzek et al., 2007)</ref> database dated 2018-03. The UniRef50 clusters are partitioned randomly in 90% train and 10% test sets. For each sequence, we construct an MSA using HHblits, version 3.1.0. <ref type="bibr">(Steinegger et al., 2019)</ref> against the UniClust30 2017â10 database <ref type="bibr" target="#b28">(Mirdita et al., 2017)</ref>. Default settings are used for HHblits except for the the number of search iterations (-n), which we set to 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Ablation Studies</head><p>Ablation studies are conducted over a set of seven hyperparameters listed in Table <ref type="table" target="#tab_7">6</ref>. Since the cost of an exhaustive search over all combinations of hyperparameters is prohibitive, we instead train an exhaustive search over four of the hyperparameters (embedding size, block order, tied attention, and masking pattern) for 10k updates. The best run is then selected as the base hyperparameter setting for the Table <ref type="table">5</ref>. Validation perplexity and denoising accuracy on UniRef50 validation MSAs. PSSM probabilities and nearest-neighbor matching are used as baselines. To compute perplexity under the PSSM, we construct PSSMs using the input MSA, taking the cross-entropy between the PSSM and a one-hot encoding of the masked amino acid. When calculating PSSM probabilities, we search over pseudocounts in the range [10 â10 , 10), and select 10 â2 , which minimizes perplexity. For denoising accuracy, the argmax for each column is used. For nearest-neighbor matching, masked tokens are predicted using the values from the sequence with minimum hamming distance to the masked sequence. This does not provide a probability distribution, so perplexity cannot be calculated. MSAs with depth 1 are ignored, since the baselines fail in this condition. Perplexity ranges from 1 for a perfect model to 21 for a uniform model selecting over the common amino acids and gap token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Perplexity For the full ablation study, each model is trained for 100k updates using a batch size of 512. Contact prediction on the trRosetta dataset <ref type="bibr" target="#b51">(Yang et al., 2019)</ref> is used as a validation task. Precision after 100k updates is reported in Table <ref type="table" target="#tab_7">6</ref> and the full training curves are shown in Fig. <ref type="figure" target="#fig_8">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Supervised Contact Prediction</head><p>The model with best hyperparameters is then further trained to 125k updates. The performance of this model is reported in Table <ref type="table" target="#tab_8">7</ref>. Potts <ref type="bibr" target="#b3">(Balakrishnan et al., 2011)</ref>, TAPE transformer <ref type="bibr">(Rao et al., 2019)</ref>, ESM-1b <ref type="bibr" target="#b40">(Rives et al., 2020)</ref>, ProtBERT-BFD, and ProTrans-T5 <ref type="bibr" target="#b11">(Elnaggar et al., 2020)</ref> are used as unsupervised contact prediction comparisons.  ever, we do not find a statistically significant difference between masking 15% or 20% of the positions. Therefore, we use a masking percentage of 15% in all other studies for consistency with ESM-1b and previous masked language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.10. MSA Positional Embedding</head><p>An MSA is an unordered set of sequences. However, due to the tools used to construct MSAs, there may be some pattern to the ordering of sequences in the MSA. We therefore examine the use of a learned MSA positional embedding in addition to the existing learned sequence positional embedding. The positional embedding for a sequence is then a learned function of its position in the input MSA (not in the full MSA). Subsampled sequences in the input MSA are sorted according to their relative ordering in the full MSA. We find that the inclusion of an MSA positional embedding does modestly increase model performance, and therefore include it in our final model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Left: Sparsity structure of the attention. By constraining attention to operate over rows and columns, computational cost is reduced from O(M 2 L 2 ) to O(LM 2 ) + O(M L 2 ) where M is the number of rows and L the number of columns in the MSA. Middle: Untied row attention uses different attention maps for each sequence in the MSA. Tied row attention uses a single attention map for all sequences in the MSA, thereby constraining the contact structure. Ablation studies consider the use of both tied and untied attention. The final model uses tied attention. Right: A single MSA Transformer block. The depicted architecture is from the final model, some ablations alter the ordering of row and column attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Left: Top-L long-range contact precision (higher is better) MSA Transformer vs. Potts model (left) and ESM-1b (right) on 14,842 proteins. Each point represents a single protein and is colored by the depth of the full MSA for the sequence. The Potts model receives the full MSA as input, ESM-1b receives only the reference sequence, and the MSA Transformer receives an MSA subsampled with hhfilter to a maximum of 256 sequences. The MSA Transformer outperforms both models for the vast majority of sequences. Right: Characterization of long-range contact precision performance for MSA Transformer, ESM-1b, and Potts model as a function of MSA depth. Sequences are binned by MSA depth into 10 bins, average performance in each bin along with 95% confidence interval is shown. Model performance generally increases with MSA depth, but the MSA Transformer performs very well on sequences with low-depth MSAs, rivaling Potts model performance on MSAs 10x larger.</figDesc><graphic url="image-1.png" coords="4,90.36,67.06,416.16,124.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2 shows the top-L long-range precision distribution across all structures, comparing the MSA Transformer with Potts models and ESM-1b. The MSA Transformer matches or exceeds Potts models on 97.5% of inputs and matches or exceeds ESM-1b on 90.2% of inputs. Fig. 2 also shows unsupervised contact performance as a function of MSA depth. The model outperforms ESM-1b and Potts models across all MSA depths and has a significant advantage for lower depth MSAs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Contact prediction from a small set of input sequences. Predictions are compared under diversity minimizing and diversity maximizing sequence selection strategies. Visualized for 4zjp chain A. Raw contact probabilities are shown below the diagonal, top L contacts are shown above the diagonal. (blue: true positive, red: false positive, grey: ground-truth contacts). Top-L long-range contact precision below each plot. Contact precision improves with more sequences under both selection strategies. Maximizing the diversity enables identification of long-range contacts from a small set of sequences.</figDesc><graphic url="image-2.png" coords="6,127.50,67.06,341.88,149.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure5. Left: Average correlation between row-attention and column entropy. This is computed by taking an average over the first dimension of each L Ã L row-attention map and computing correlation with per-column entropy of the MSA. Right: Average correlation between column-attention and sequence weights. This is computed by taking an average over the first two dimensions for each LÃM ÃM column-attention map and computing correlation with sequence weights (see Appendix A.13). Both quantities are measures of MSA diversity. The relatively high correlation (&gt; 0.55) of some attention heads to these measures suggests the model explicitly looks at diverse sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. The MSA Transformer uses both covariance and similarity to training sequences to perform inference. Left: Examples (pdbid: 5ahw, chain: A) of model performance after independently shuffling each column of an MSA to destroy covariance information, and after independently permuting the order of positions to destroy sequence patterns. The MSA Transformer maintains reasonable performance under both conditions. A Potts model fails on the covariance-shuffled MSA, while a single-sequence language model (ESM-1b) fails on the position-shuffled sequence. Right: Model performance before and after shuffling, binned by depth of the original (non-subsampled)MSA. 1024 sequence selected with hhfilter are used as input to MSA Transformer and Potts models. MSAs with fewer than 1024 sequences are not considered in this analysis. Average Top-L long-range precision drops from 52.9 (no ablation) to 15.9 (shuffled covariance) and 27.9 (shuffled positions) respectively. A Null (random guessing) baseline is also considered. Potts model performance drops to the Null baseline under the first condition and ESM-1b performance drops to the Null baseline under the second condition. The MSA Transformer produces reasonable predictions under both scenarios, implying it uses both modes of inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Weight values of learned sparse logistic regression trained on 20 structures. A sparse subset (55 / 144) of contact heads, largely in the final layers, are predictive of protein contacts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Distribution of MSA depths in the MSA Transformer training set. Average MSA depth is 1192 and median MSA depth is 1101.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Training curves for MSA Transformer with different hyperparameters. See Section 4.4 for a description of each hyperparameter searched over. ESM-1b training curve, ESM-1b final performance (after 505k updates), and average Potts performance are included as dashed lines for comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Average long-range precision for MSA and singlesequence models on the unsupervised contact prediction task.</figDesc><table><row><cell>Model</cell><cell>L</cell><cell>L/2</cell><cell>L/5</cell></row><row><cell>Potts</cell><cell cols="3">39.3 52.2 62.8</cell></row><row><cell>TAPE ProTrans-T5 ESM-1b</cell><cell cols="3">11.2 14.0 17.9 35.6 46.1 57.8 41.1 53.3 66.1</cell></row><row><cell cols="4">MSA Transformer 57.7 72.5 83.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Unsupervised contact prediction on CASP13 and CAMEO (long-range precision). Note the large improvement of MSA Transformer over classical Potts models and ESM-1b.</figDesc><table><row><cell></cell><cell cols="2">CASP13-FM</cell><cell cols="2">CAMEO</cell></row><row><cell>Model</cell><cell>L</cell><cell>L/5</cell><cell>L</cell><cell>L/5</cell></row><row><cell cols="5">ProTrans-T5 ESM-1b Potts MSA Transformer 42.7 65.3 40.5 59.1 16.6 26.7 25.3 42.6 16.8 30.1 30.7 52.3 17.0 31.8 23.9 42.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Supervised contact prediction on CASP13 and CAMEO (long-range precision). * Uses outer-concatenation of the query sequence representation as features. â  Additionally uses the row attention maps as features.tact precision by a full 16 points over the previous state-ofthe-art. Table2shows results on harder test sets CAMEO hard targets</figDesc><table><row><cell></cell><cell cols="2">CASP13-FM</cell><cell cols="2">CAMEO</cell></row><row><cell>Model</cell><cell>L</cell><cell>L/5</cell><cell>L</cell><cell>L/5</cell></row><row><cell>trRosetta base trRosetta full</cell><cell cols="4">42.4 66.3 49.3 74.0 49.6 76.2 51.1 74.9</cell></row><row><cell cols="5">ProTrans-T5 ESM-1b Co-evolutionary MSA Transformer  *  53.3 81.7 52.4 77.9 24.1 40.6 40.0 63.7 27.2 48.4 42.0 65.1 40.1 65.2 47.3 72.1</cell></row><row><cell cols="5">MSA Transformer  â  57.1 86.1 54.9 79.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>CB513 8-class secondary structure prediction accuracy.</figDesc><table><row><cell>Model</cell><cell>CB513</cell></row><row><cell>Netsurf</cell><cell>72.1</cell></row><row><cell>HMM Profile ProTrans-T5 ESM-1b</cell><cell>71.2 Â± 0.1 71.4 Â± 0.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Hyperparameter search on MSA Transformer. P@L is long-range (s â¥ 24) precision on unsupervised contact prediction followingRao et al. (2021). Precision is reported after 100k updates.</figDesc><table><row><cell>D</cell><cell>Block</cell><cell cols="4">Tied Masking Mask p MSA Pos Emb</cell><cell>Subsample</cell><cell>P@L</cell></row><row><cell cols="2">768 Row-Column 384 Column-Row</cell><cell>Sqrt None Mean</cell><cell>Uniform Column</cell><cell>0.15 0.2</cell><cell>No Yes</cell><cell>Log-uniform 56.3 52.8 55.7 42.1 50.1 38.8 56.6 57.3 Full 56.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Average precision on 14842 test structures for MSA and single-sequence models trained on 20 structures.</figDesc><table><row><cell></cell><cell cols="3">6 â¤ sep &lt; 12</cell><cell cols="3">12 â¤ sep &lt; 24</cell><cell></cell><cell>24 â¤ sep</cell><cell></cell></row><row><cell>Model</cell><cell>L</cell><cell>L/2</cell><cell>L/5</cell><cell>L</cell><cell>L/2</cell><cell>L/5</cell><cell>L</cell><cell>L/2</cell><cell>L/5</cell></row><row><cell>Potts</cell><cell cols="9">17.2 26.7 44.4 21.1 33.3 52.3 39.3 52.2 62.8</cell></row><row><cell>TAPE ProtBERT-BFD ProTrans-T5 ESM-1b</cell><cell cols="9">9.9 12.3 16.4 10.0 12.6 16.6 11.2 14.0 17.9 20.4 30.7 48.4 24.3 35.5 52.0 34.1 45.0 57.4 20.1 30.6 48.5 24.6 36.1 52.4 35.6 46.1 57.8 21.6 33.2 52.7 26.2 38.6 56.4 41.1 53.3 66.1</cell></row><row><cell cols="10">MSA Transformer 25.8 41.2 65.2 32.1 49.2 71.9 57.7 72.5 83.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>Supervised Contact Prediction performance on CASP13-FM and CAMEO-hard targets. Reported numbers are long-range (s â¥ 24) contact precision. Three variants of the MSA Transformer are included for comparison: * unsupervised model, â  supervised model using final hidden representations of the reference sequence as input, â¡ supervised model using final hidden representations of reference sequence and all attention maps as input. Baseline and final trRosetta models are also included for comparison.42.7 53.9 65.3 40.5 50.8  59.1 MSA Transformer â  53.3 70.2 81.7 52.4 67.0 77.9 MSA Transformer â¡ 57.1 73.6 86.1 54.9 69.6 79.8</figDesc><table><row><cell></cell><cell></cell><cell cols="2">CASP13-FM</cell><cell></cell><cell>CAMEO</cell><cell></cell></row><row><cell>Model</cell><cell>L</cell><cell>L/2</cell><cell>L/5</cell><cell>L</cell><cell>L/2</cell><cell>L/5</cell></row><row><cell>Unirep SeqVec TAPE ProtBERT-BFD ProTrans-T5 ESM-1b Co-evolutionary</cell><cell cols="6">10.5 11.8 14.7 17.0 22.0 28.0 12.5 15.7 18.3 21.2 28.4 37.5 13.0 16.0 19.0 15.5 19.7 25.6 22.0 28.1 34.3 35.4 46.5 57.6 24.1 31.5 40.6 40.0 51.9 63.7 27.2 35.0 48.4 42.0 54.1 65.1 40.1 52.5 65.2 47.3 60.9 72.1</cell></row><row><cell>trRosetta base trRosetta full</cell><cell cols="6">42.4 54.4 66.3 49.3 62.8 74.0 49.6 64.9 76.2 51.1 64.7 74.9</cell></row><row><cell>MSA Transformer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*  </note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The final vocab size is</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="29" xml:id="foot_1">, consisting of 20 standard amino acids, 5 non-standard amino acids, the alignment character '.', gap character '-', the start token, and the mask token</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Nicholas Bhattacharya, Zeming Lin, Sergey Ovchinnikov, and Neil Thomas for valuable input on the paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The best MSA Transformer outperforms all other methods by a wide margin, increasing long-range precision at L by a full 16 points. See below for a discussion of all seven hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Embedding Size (D)</head><p>Since the MSA Transformer is provided with more information than single sequence protein language models, it is possible that many fewer parameters are needed to learn the data distribution. To test this hypothesis we train a model with half the embedding size (384 instead of 768) resulting in 30M total parameters. The resulting model achieves a Top-L long-range precision of 52.8 after 100k updates, 3.5 points lower than the baseline model. This suggests that model size is still an important factor in contact precision, although also shows that a model with fewer than 30M parameters can still outperform 650M and 3B parameter single-sequence models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6. Masking Pattern</head><p>We consider two strategies for applying BERT masking to the MSA: uniform and column. Uniform masking applies masking uniformly at random across the MSA. Column masking always masks full columns of the MSA. This makes the training objective substantially more difficult since the model cannot look within a column of an MSA for information about masked tokens. We find that column masking is significantly worse (by almost 20 points) than uniform masking. One possible explanation is that column masking removes all direct signal for training column attention. It is possible that a combination of uniform and column attention could improve performance further; we leave this for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7. Block Ordering</head><p>Row attention followed by column attention slightly outperforms column attention followed by row attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8. Tied Attention</head><p>We consider three strategies for row attention: untied, mean normalization, and square root normalization (see Section 3). We find that tied attention substantially outperforms untied attention and that square root normalization outperforms mean normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.9. Masking Percentage</head><p>As the MSA Transformer has more context than single sequence models, its training objective is substantially easier than that of single sequence models. Therefore, we explore whether increasing the masking percentage (and thereby increasing task difficulty) would improve the model. How-  The fraction of attention on an amino acid k is defined as the average over the dataset of a lh i 1{xi == k}, where xi is a particular token in the input MSA and a lh is the attention in a particular layer and head. KL Divergence is large for early layers but decreases in later layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.11. Subsample Strategy</head><p>At training time we explore two subsampling strategies. The first strategy is adapted from Yang et al. ( <ref type="formula">2019</ref>): we sample the number of output sequences from a log-uniform distribution, with a maximum of N/L sequences to avoid exceeding the maximum tokens we are able to fit in GPU memory. Then, we sample that number of sequences uniformly from the MSA, ensuring that the reference sequence is always chosen. In the second strategy, we always sample the full N/L sequences from the MSA. In our hyperparameter search, most models use the first strategy, while our final model uses the second. We find no statistically significant difference in performance between the two strategies. However, it is possible that the log-uniform strategy would help prevent overfitting and ultimately perform better after more training.</p><p>The CCMpred implementation of Potts <ref type="bibr" target="#b3">(Balakrishnan et al., 2011;</ref><ref type="bibr" target="#b10">Ekeberg et al., 2013)</ref>, UniRep <ref type="bibr" target="#b1">(Alley et al., 2019)</ref>, Se-qVec <ref type="bibr" target="#b15">(Heinzinger et al., 2019)</ref>, TAPE transformer <ref type="bibr">(Rao et al., 2019)</ref>, ESM-1b <ref type="bibr" target="#b40">(Rives et al., 2020)</ref>, ProtBERT-BFD, and ProTrans-T5 <ref type="bibr" target="#b11">(Elnaggar et al., 2020)</ref> are used as supervised contact prediction comparisons. In Table <ref type="table">8</ref> we show the complete results for long-range precision over the CASP-13 FM targets and CAMEO-hard domains referenced in <ref type="bibr" target="#b51">(Yang et al., 2019)</ref>. All baseline models are trained for 200 epochs with a batch size of 4.</p><p>A.12. Attention to Amino Acids <ref type="bibr">Vig et al. (2020)</ref> examine the distribution of amino acids attended to by single-sequence models. The attention in single-sequence models is roughly equivalent to the rowattention in our model, but there is no column-attention analogue. We therefore examine the distribution of amino acids attended to by the column attention heads. In Fig. <ref type="figure">10</ref> we show the KL-divergence between the distribution of attention across amino acids and the background distribution of amino acids. The divergence is large for earlier layers in the model but decreases in later layers, suggesting the model stops focusing on the amino acid identities in favor of focusing on other properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.13. Sequence Weights</head><p>Sequence reweighting is a common technique used for fitting Potts models which helps to compensate for data bias in MSAs <ref type="bibr">(Morcos et al., 2011)</ref>. Informally, sequence reweighting downweights groups of highly similar sequences to prevent them from having as large of an effect on the model. The sequence weight w i is defined as,</p><p>where x i , x j are the i-th and j-th sequences in the MSA, d hamming is the hamming distance between two sequences normalized by sequence length, and w i is the sequence weight of the i-th sequence.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DEEPCON: Protein contact prediction using dilated convolutional neural networks with dropout</title>
		<author>
			<persName><forename type="first">B</forename><surname>Adhikari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elofsson</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btz593</idno>
		<ptr target="https://academic.oup.com/bioinformatics/article/36/2/470/5540673" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unified rational protein engineering with sequence-only deep representation learning</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Alley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Khimulya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alquraishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Church</surname></persName>
		</author>
		<idno type="DOI">10.1101/589333v1</idno>
		<ptr target="https://www.biorxiv.org/content/10.1101/589333v1" />
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1315" to="1322" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Coordinated amino acid changes in homologous protein families</title>
		<author>
			<persName><forename type="first">D</forename><surname>Altschuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vernet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Berti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nagai</surname></persName>
		</author>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/3237684" />
	</analytic>
	<monogr>
		<title level="j">Protein engineering</title>
		<idno type="ISSN">0269-2139</idno>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="193" to="199" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning generative models for protein fold families</title>
		<author>
			<persName><forename type="first">S</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kamisetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Langmead</surname></persName>
		</author>
		<idno type="DOI">10.1002/prot.22934</idno>
		<ptr target="http://doi.wiley.com/10.1002/prot.22934" />
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1061" to="1078" />
			<date type="published" when="2011">4 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning protein sequence embeddings using information from structure</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bepler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Berger</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1902.08661https://arxiv.org/abs/1902.08661" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Single Layers of Attention Suffice to Predict Protein Contacts</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dauparas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ovchinnikov</surname></persName>
		</author>
		<idno type="DOI">10.1101/2020.12.21.423882</idno>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>CoRR, abs/1904.10509</idno>
		<ptr target="http://arxiv.org/abs/1904.10509" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Evaluation and improvement of multiple sequence methods for protein secondary structure prediction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Cuff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Barton</surname></persName>
		</author>
		<idno type="DOI">10.1002/(SICI)1097-0134(19990301)34:4508</idno>
		<ptr target="https://pubmed.ncbi.nlm.nih.gov/10081963/" />
	</analytic>
	<monogr>
		<title level="m">AID-PROT10 3.0.CO;2-4</title>
				<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="508" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mutual information without the influence of phylogeny or entropy dramatically improves residue contact prediction</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Wahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Gloor</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btm604</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="333" to="340" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improved contact prediction in proteins: Using pseudolikelihoods to infer Potts models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ekeberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>LÃ¶vkvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weigt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Aurell</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevE.87.012707</idno>
	</analytic>
	<monogr>
		<title level="j">Nonlinear, and Soft Matter Physics</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Physical Review E -Statistical</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Prot-Trans: Towards Cracking the Language of Life&apos;s Code Through Self-Supervised Deep Learning and High Performance Computing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elnaggar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heinzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rihawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gibbs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Feher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Angerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bhowmik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2007.06225" />
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-scale clinical interpretation of genetic variants using evolutionary data and deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Frazer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Notin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Marks</surname></persName>
		</author>
		<idno type="DOI">10.1101/2020.12.21.423785</idno>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Correlated mutations and residue contacts in proteins</title>
		<author>
			<persName><forename type="first">U</forename><surname>GÃ¶bel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Valencia</surname></persName>
		</author>
		<idno type="DOI">10.1002/prot.340180402</idno>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/8208723http://doi.wiley.com/10.1002/prot.340180402" />
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Genetics</title>
		<idno type="ISSN">0887-3585</idno>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="309" to="317" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Continuous Automated Model EvaluatiOn (CAMEO) complementing the critical assessment of structure prediction in CASP12</title>
		<author>
			<persName><forename type="first">J</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barbato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Behringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mostaguir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gumienny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schwede</surname></persName>
		</author>
		<idno type="DOI">10.1002/prot.25431</idno>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="387" to="398" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Suppl</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modeling the language of life -Deep Learning Protein Sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heinzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elnaggar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nechaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Matthes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
		<idno type="DOI">10.1101/614313v3</idno>
		<ptr target="https://www.biorxiv.org/content/10.1101/614313v3" />
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">614313</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1912.12180" />
		<title level="m">Axial Attention in Multidimensional Transformers. arXiv, 12 2019</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">DeepSF: deep convolutional neural network for mapping protein sequences to folds</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Adhikari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btx780</idno>
		<ptr target="https://academic.oup.com/bioinformatics/article/34/8/1295/4708302" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<idno type="ISSN">1367-4803</idno>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1295" to="1303" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">PSICOV: precise structural contact prediction using sparse inverse covariance estimation on large multiple sequence alignments</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W A</forename><surname>Buchan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cozzetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btr638</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<idno type="ISSN">1460-2059</idno>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="184" to="190" />
			<date type="published" when="2012">1 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">High Accuracy Protein Structure Prediction Using Deep Learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tunyasuvunakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Å½Ã­dek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bridgland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A A</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pacholska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btr638</idno>
		<ptr target="https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btr638" />
	</analytic>
	<monogr>
		<title level="m">Fourteenth Critical Assessment of Techniques for Protein Structure Prediction</title>
				<imprint>
			<publisher>Abstract Book</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note>URL</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Assessing the utility of coevolution-based residue-residue contact predictions in a sequence-and structure-rich era. Proceedings of the National Academy of Sciences of the United States of America</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kamisetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ovchinnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baker</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1314045110</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="15674" to="15679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning-based prediction of protein structure using learned representations of multiple sequence alignments</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kandathil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Greener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Jones</surname></persName>
		</author>
		<idno type="DOI">10.1101/2020.11.27.401232</idno>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">NetSurfP-2.0: Improved prediction of protein structural features by integrated deep learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Klausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Jespersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Jurtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>SÃ¸nderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O A</forename><surname>Sommer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Marcatili</surname></persName>
		</author>
		<idno type="DOI">10.1002/prot.25674</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/abs/10.1002/prot.25674" />
	</analytic>
	<monogr>
		<title level="m">Proteins: Structure, Function, and Bioinformatics</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="520" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Correlated Mutations in Models of Protein Sequences: Phylogenetic and Structural Effects</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Lapedes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Giraud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Stormo</surname></persName>
		</author>
		<idno type="DOI">10.2307/4356049</idno>
		<ptr target="http://www.jstor.org/stable/4356049" />
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="236" to="256" />
		</imprint>
	</monogr>
	<note type="report_type">Lecture Notes-Monograph Series</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Enhancing Evolutionary Couplings with Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Palmedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cels.2017.11.014</idno>
		<ptr target="https://pubmed.ncbi.nlm.nih.gov/29275173/" />
	</analytic>
	<monogr>
		<title level="j">Cell Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="74" />
			<date type="published" when="2018">1 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Self-Supervised Contrastive Learning of Protein Representations By Mutual Information Maximization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moses</surname></persName>
		</author>
		<idno type="DOI">10.1101/2020.09.04.283929</idno>
		<ptr target="https://doi.org/10.1101/2020.09.04.283929" />
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ProGen: Language Modeling for Protein Generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Eguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2004.03497" />
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">End-to-end Deep Learning using raw Multiple Sequence Alignments</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mirabello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wallner</surname></persName>
		</author>
		<author>
			<persName><surname>Rawmsa</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0220182</idno>
		<ptr target="https://dx.plos.org/10.1371/journal.pone.0220182" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<idno type="ISSN">1932-6203</idno>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">e0220182</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Uniclust databases of clustered and deeply annotated protein sequences and alignments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mirdita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Von Den Driesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Galiez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Soding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steinegger</surname></persName>
		</author>
		<idno type="DOI">10.1093/nar/gkw1081</idno>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D170" to="D176" />
			<date type="published" when="2017">1 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Direct-coupling analysis of residue coevolution captures native contacts across many protein families</title>
		<author>
			<persName><forename type="first">F</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pagnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bertolino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zecchina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Onuchic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weigt</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1111471108</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences of the United States America</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">49</biblScope>
			<biblScope unit="page">2011</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Evaluating Protein Transfer Learning with TAPE</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Song</surname></persName>
		</author>
		<idno>doi: 10</idno>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
				<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
		<respStmt>
			<orgName>Cold Spring Harbor Laboratory</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<idno type="DOI">10.1101/676825</idno>
		<ptr target="https://doi.org/10.1101/676825http://arxiv.org/abs/1906.08230" />
		<title level="m">URL</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Transformer protein language models are unsupervised structure learners. ICLR, pp</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ovchinnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rives</surname></persName>
		</author>
		<idno type="DOI">10.1101/2020.12.15.422761</idno>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Accelerating Protein Design Using Autoregressive Generative Models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Riesselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-E</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kollasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mcmahon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Manglik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marks</surname></persName>
		</author>
		<idno type="DOI">10.1101/757252v1</idno>
		<ptr target="https://www.biorxiv.org/content/10.1101/757252v1" />
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">757252</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep generative models of genetic variation capture the effects of mutations</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Riesselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Marks</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41592-018-0138-4</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="816" to="822" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="DOI">10.1101/622803v3</idno>
		<ptr target="https://www.biorxiv.org/content/10.1101/622803v3" />
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An evolutionbased model for designing chorismate mutase enzymes</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Figliuzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barrat-Charlaix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Socolich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hilvert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cocco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weigt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganathan</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aba3304</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">369</biblScope>
			<biblScope unit="issue">6502</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">CCMpred -Fast and precise prediction of protein residue-residue contacts from correlated mutations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Seemayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>SÃ¶ding</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btu500</idno>
		<ptr target="https://pubmed.ncbi.nlm.nih.gov/25064567/" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="3128" to="3130" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Å½Ã­dek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W R</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bridgland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Penedones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Crossan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<idno type="DOI">10.1002/prot</idno>
		<title level="m">Protein structure prediction using multiple deep neural in the 13th Critical Assessment of Protein Structure Prediction (CASP13). Proteins: Structure, Function, and Bioinformatics</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="1141" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<idno type="DOI">10.1002/prot.25834</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/abs/10.1002/prot.25834" />
		<title level="m">URL</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verkuil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rives</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=U6Xpa5R-E1" />
	</analytic>
	<monogr>
		<title level="j">Neural Potts Models. MLCB</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Assessing the accuracy of contact predictions in CASP13</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fajardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fidelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kryshtafovych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Monastyrskyy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fiser</surname></persName>
		</author>
		<idno type="DOI">10.1002/prot.25819</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/abs/10.1002/prot.25819" />
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<idno type="ISSN">0887-3585</idno>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1058" to="1068" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">HH-suite3 for fast remote homology detection and deep protein annotation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirdita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>VÃ¶hringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Haunsberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>SÃ¶ding</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12859-019-3019-7</idno>
		<ptr target="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3019-7" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Profile Prediction: An Alignment-Based Pre-Training Task for Protein Sequence Models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sturmfels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Rajani</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2012.00195" />
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Predicting the clinical impact of human mutation with deep neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Padigepati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Mcrae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Kosmicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fritzilas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hakenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Batzloglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Farh</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41588-018-0167-z</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Genetics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1161" to="1170" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Comprehensive and nonredundant UniProt reference clusters</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Suzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcgarvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Uniref</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btm098</idno>
		<ptr target="http://www.uniprot.org" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1282" to="1288" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Graphical models of residue coupling in protein families</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bailey-Kellogg</surname></persName>
		</author>
		<ptr target="https://pubmed.ncbi.nlm.nih.gov/18451428/" />
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Å</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdfhttps://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Rajani</surname></persName>
		</author>
		<idno type="DOI">10.1101/2020.06.26.174417</idno>
		<ptr target="http://arxiv.org/abs/2006.15222" />
	</analytic>
	<monogr>
		<title level="j">BERTology Meets Biology: Interpreting Attention in Protein Language Models. bioRxiv</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Accurate Novo Prediction of Protein Contact Map by Ultra-Deep Learning Model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1005324</idno>
		<ptr target="https://dx.plos.org/10.1371/journal.pcbi.1005324" />
	</analytic>
	<monogr>
		<title level="j">PLOS Computational Biology</title>
		<idno type="ISSN">1553-7358</idno>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">e1005324</biblScope>
			<date type="published" when="2017">1 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Identification of direct residue contacts in protein-protein interaction by message passing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weigt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Szurmant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hwa</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.0805923106</idno>
		<ptr target="https://www.pnas.org/content/106/1/67https://www.pnas.org/content/106/1/67.abstract" />
		<imprint>
			<date type="published" when="2009">1 2009</date>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="67" to="72" />
		</imprint>
		<respStmt>
			<orgName>Proceedings of the National Academy of Sciences of the United States of America</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Improved protein structure prediction using predicted inter-residue orientations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Anishchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ovchinnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harvard</surname></persName>
		</author>
		<idno type="DOI">10.1101/846279v1</idno>
		<ptr target="https://www.biorxiv.org/content/10.1101/846279v1" />
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">846279</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Protein Structure Relationships Revealed By Mutational Analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yanofsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thorpe</surname></persName>
		</author>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/14224506" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<idno type="ISSN">0036-8075</idno>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="issue">3651</biblScope>
			<biblScope unit="page" from="1593" to="1594" />
			<date type="published" when="1964">12 1964</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
