<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Billion-Scale Pretraining with Vision Transformers for Multi-Task Visual Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-08-12">12 Aug 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Josh</forename><surname>Beal</surname></persName>
							<email>jbeal@pinterest.com</email>
						</author>
						<author>
							<persName><forename type="first">Hao-Yu</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dong</forename><surname>Huk</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Park</forename><surname>Andrew</surname></persName>
							<email>andrew@pinterest.com</email>
						</author>
						<author>
							<persName><forename type="first">Zhai</forename><surname>Dmitry</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kislyuk</forename><surname>Pinterest</surname></persName>
							<email>dkislyuk@pinterest.com</email>
						</author>
						<title level="a" type="main">Billion-Scale Pretraining with Vision Transformers for Multi-Task Visual Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-08-12">12 Aug 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2108.05887v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large-scale pretraining of visual representations has led to state-of-the-art performance on a range of benchmark computer vision tasks, yet the benefits of these techniques at extreme scale in complex production systems has been relatively unexplored. We consider the case of a popular visual discovery product, where these representations are trained with multi-task learning, from use-case specific visual understanding (e.g. skin tone classification) to general representation learning for all visual content (e.g. embeddings for retrieval). In this work, we describe how we (1) generate a dataset with over a billion images via large weakly-supervised pretraining to improve the performance of these visual representations, and (2) leverage Transformers to replace the traditional convolutional backbone, with insights into both system and performance improvements, especially at 1B+ image scale. To support this backbone model, we detail a systematic approach to deriving weaklysupervised image annotations from heterogenous text signals, demonstrating the benefits of clustering techniques to handle the long-tail distribution of image labels. Through a comprehensive study of offline and online evaluation, we show that large-scale Transformer-based pretraining provides significant benefits to industry computer vision applications. The model is deployed in a production visual shopping system, with 36% improvement in top-1 relevance and 23% improvement in click-through volume. We conduct extensive experiments to better understand the empirical relationships between Transformer-based architectures, dataset scale, and the performance of production vision systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual representation learning is a core foundation in online content search and recommendation systems <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b18">19]</ref>. In recent years, we have increasingly seen the paradigm of training a single, high-capacity, deep neural network model jointly across many heterogeneous tasks, especially in in-dustry settings with complex use-cases <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b2">3]</ref>. There are multiple benefits, including <ref type="bibr" target="#b0">(1)</ref> the ability to jointly utilize large amounts of strongly and weakly supervised data, instead of training domain-specific models in isolation, sometimes leading to better performance compared with domainspecific models <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b1">(2)</ref> an infrastructure benefit due to reduced operational overhead to maintain fewer models, and (3) a reliable source model to build upon for downstream modeling tasks via transfer learning.</p><p>Many search, recommendation, and content understanding tasks require a representation capturing both visual and semantic components (Figure <ref type="figure" target="#fig_0">1</ref>). This work focuses on the single multi-task image representation model powering visual understanding for a widely-used visual discovery product, referred to as the "Unified Visual Embedding". This model powers tens of production use cases and is an area of substantial investment. Some production use-cases include:</p><p>• Retrieval: the embeddings are used through Approximate Nearest Neighbors in several retrieval systems, including Visual Search and Visual Shopping.</p><p>• Features: the embeddings are used as features in other content understanding models that need information extracted from images. Such models can be purely visual (e.g. skin-tone classification), aggregating multiple images (e.q. video understanding), or combining other signals such as text and graph structure <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b35">36]</ref>. It is a computationally efficient way to leverage visual signals in model training and serving.</p><p>In our paper we describe the implementation, experimentation, and productionization of large scale pretraining on over a billion images, along with the adoption and deployment of a Transformer-based architecture (Vision Transformers), supplanting a CNN architecture approach that has been an industry standard in recent years. We focus on a weakly supervised label preparation methodology along with ablations including dataset size variation to construct the critical dataset Annotations-1.3B, and describe how our billion-scale pretrained Vision Transformer model benefits the multi-task image representation model, along with the end-to-end production system relevance, and engagement impact through human judgement and A/B experimentation. We conclude with insights into generalization (fewshot, cross-domain) performance. To the best of our knowledge, this is the first large-scale industry application of pure Transformer-based models for image retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Visual Search Systems</head><p>Visual search has been widely adopted in social and ecommerce applications, including Facebook <ref type="bibr" target="#b29">[30]</ref>, Pinterest <ref type="bibr" target="#b40">[41]</ref>, eBay <ref type="bibr" target="#b36">[37]</ref>, Google, Microsoft <ref type="bibr" target="#b14">[15]</ref>, Alibaba <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>, and Amazon <ref type="bibr" target="#b46">[47]</ref>. In prior work, GrokNet <ref type="bibr" target="#b2">[3]</ref> and Shop the Look <ref type="bibr" target="#b25">[26]</ref> have described the recent details of industrialscale visual search systems. These visual search systems leverage multi-task learning to optimize a single embedding for different applications in the organization. The visual embedding models powering these systems are trained using deep metric learning, including classification-based methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">40]</ref> and triplet-loss-based methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Large-Scale Pretraining</head><p>Large-scale image datasets have proven to be useful in the pretraining of general visual representations. Some of the earliest work in this area involved pretraining AlexNet <ref type="bibr" target="#b17">[18]</ref> on YFCC100M <ref type="bibr" target="#b30">[31]</ref>, showing that pretraining on a large dataset of Flickr images could perform comparably to ILSVRC-2012 <ref type="bibr" target="#b6">[7]</ref>, a fully supervised dataset of 1.3 million images. The quality of the text metadata has limited the effectiveness of YFCC100M, and in subsequent work, many of the improvements stem from the selection of higher quality sources of textual supervision. JFT-300M <ref type="bibr" target="#b27">[28]</ref> is a large-scale dataset developed at Google, consisting of around 300M images and 18k labels in the dataset taxonomy. The labels for each image are derived from a mixture of web signals. Recently this dataset has been leveraged in the BigTransfer <ref type="bibr" target="#b16">[17]</ref> work to achieve state-of-the-art results on ILSVRC-2012 (hereafter referred to as ImageNet-1k) and the Visual Task Adaptation Benchmark (VTAB) <ref type="bibr" target="#b41">[42]</ref>. Facebook <ref type="bibr" target="#b20">[21]</ref> investigated pretraining on billions of Instagram images using hashtags as a form of weak supervision, achieving competitive ImageNet-1k performance. Large subsets of Instagram images, selected randomly without bias to the hashtag distribution, have also been useful in the context of self-supervised learning <ref type="bibr" target="#b4">[5]</ref>. OpenAI has explored the effectiveness of image-to-caption matching in CLIP <ref type="bibr" target="#b23">[24]</ref>, introducing the WebImageText dataset of approximately 400 million (image, text) pairs. The webly-supervised pretraining paradigm offers benefits to more complex vision tasks, such as object detection <ref type="bibr" target="#b1">[2]</ref> and video understanding <ref type="bibr" target="#b26">[27]</ref>. Similar to CLIP, the task of pairing videos with their as-sociated title, description, and other metadata, has led to state-of-the-art results on action recognition tasks when using a dataset of 70 million YouTube videos <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Transformer Architectures</head><p>Transformer <ref type="bibr" target="#b32">[33]</ref> architectures have become the state-ofthe-art solution for many natural language processing tasks, based on a simple and scalable application of multi-head attention. BERT <ref type="bibr" target="#b7">[8]</ref> and GPT-3 <ref type="bibr" target="#b3">[4]</ref> have demonstrated that these architectures have a large pretraining capacity, with few signs of saturating performance as the dataset size and model size continue to increase. Fully Transformer-based architectures have recently been successful for image classification, with Vision Transformers <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32]</ref> achieving state-of-the-art ImageNet-1k performance, while offering compelling advantages in training compute and memory efficiency relative to the equivalent ResNet <ref type="bibr" target="#b10">[11]</ref> architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">System Overview</head><p>Figure <ref type="figure" target="#fig_0">1</ref> (A) describes the overall setup for Unified Visual Embeddings, which includes optimization objectives spanning across many modes of classification (single-class, multilabel, multi-label softmax <ref type="bibr" target="#b20">[21]</ref>), metric learning (normalized sampled softmax <ref type="bibr" target="#b39">[40]</ref>, distance weighted sampling <ref type="bibr" target="#b33">[34]</ref>) and auxiliary regularization losses. To evaluate model improvements, we compare both qualitative and quantitative evaluation metrics across tens of datasets. Visual Search <ref type="bibr" target="#b15">[16]</ref> remains one of the most important applications of unified visual embeddings, where performance is predominantly determined by the representation quality and is the focus of our offline evaluations and A/B experiments.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> (B) describes an overview of our modifications to the prior unified embedding setup with our billion-scale weakly supervised pretraining and end-to-end Transformer encoder. We go into details of our methodology below, describing these two components that each led to significant improvements across the majority of the evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Billion-Scale Weakly Supervised Dataset</head><p>Motivated by prior work leveraging user-provided text for large-scale weak supervision <ref type="bibr" target="#b20">[21]</ref>, we leverage multiple text understanding models to create our billion-scale dataset. We apply term clustering techniques and filter the candidates according to visual concreteness and top-level interest-match in order to create our Annotations-1.3B dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Label Generation</head><p>We leverage multiple content understanding models to derive weakly-supervised image annotations from over a billion   web images. Figure <ref type="figure" target="#fig_1">2</ref> depicts a high-level overview of our process, which is described in detail below.</p><p>Annotation Selection: We use high-confidence outputs of a keyword prediction model as the basis for our label generation method. To simplify the subsequent steps in the pipeline, we select only the canonical, non-sensitive English keywords, referred to as "annotations." Furthermore, we select a confidence score threshold such that annotations are discarded if their score falls below the threshold. This ensures a high level of precision in the resulting output. Similar to hashtag-based pretraining approaches <ref type="bibr" target="#b20">[21]</ref>, it is possible that applicable annotations are missing from the signal output due to incompleteness of the text candidate sources (e.g., content is missing a title and description).</p><p>Visual Dictionary Restriction: One challenge in working with the predicted annotations is that some terms may be helpful for general content understanding, but are less helpful for learning visual features due to the abstract nature of the terms. The relationship between the visual concreteness of terms and the ability of machine learning algorithms to learn cross-modal relationships based on these terms has been investigated in prior work <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b12">13]</ref>. However, manually assigning a concreteness score to each term in the annotations dictionary would involve a large amount of human effort. Therefore, we developed an algorithmic approach to defining a concreteness score for each term.</p><p>For each term in the annotations dictionary, we train a semi-supervised binary classifier to distinguish the applicability of that term to a given image. We sample images with an annotation matching that term at score greater than the high-confidence threshold, and sample an equal number of random negative examples that do not have a matching annotation. We train and evaluate a lightweight embedding-based MLP classifier for each term using the production unified visual embedding as input to the model. The top-1 accuracy of the term's classifier serves as the visual concreteness score for the term, since the performance reflects the general ability of a model to distinguish the applicability of the term based on the visual information only.</p><p>Applying this method, we analyze the visual concreteness of 218,879 terms in the annotations dictionary. In Figure <ref type="figure" target="#fig_2">3</ref>, we present the distribution of the concreteness scores. We identify a decision boundary for the concreteness scores based on a quantitative analysis of the pretraining performance after removal of the low-quality terms from the dictio- nary. The visual dictionary consists of all terms not falling below the decision boundary. During the label generation procedure, if an annotation is not present in the visual dictionary, it is removed from further processing.</p><p>Annotation Clustering: Another challenge with the predicted annotations is the large size of the label space, which consists of hundreds of thousands of terms. This creates practical challenges for training and motivates the use of clustering techniques to reduce the size of the label space. Our method first uses a classification model to map each annotation to its corresponding top-level interests ("L1 interests"). There are 24 high level interests (e.g., home decor, food and drinks, fashion) in the human-curated taxonomy. Within each of the L1 interest groups, k-means clustering is applied to the text embeddings of the terms within that interest group in order to yield a set of cluster identifiers. Each image is mapped to the cluster identifiers corresponding to its previously selected annotations. Note that each annotation may map to multiple cluster identifiers, since each annotation may map to multiple L1 interests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L1 Interest Restriction:</head><p>We apply a classification model to map each image to top-level interests ("L1 interests") based on the image's associated text metadata and user interactions. We remove the cluster identifiers where the L1 interest of that cluster fails to match the L1 interest of the image. This helps to address polysemy in the annotations space. Per the example in Figure <ref type="figure" target="#fig_1">2</ref>, the keyword "jaguar" could be used in reference to an animal, a vehicle, or a fashion item, whereas the top-level interests for the image help to reduce the ambiguity of the textual content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training Datasets</head><p>We follow the label generation procedure to derive the Annotations-1.3B pretraining dataset from the application's image corpus. This dataset consists of 1329M images with 18k labels in the taxonomy and approximately 2.88 labels per image. We use a near-duplicate removal process to ensure that each image that is present in the training dataset is unique and non-overlapping with the validation sets.</p><p>The pretrained backbone network is used for initialization of the encoder in the Unified Embedding fine-tuning process. The multi-task training setup for the Unified Embedding follows the previous description in <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b25">26]</ref>. Compared to the prior multi-task setting <ref type="bibr" target="#b25">[26]</ref>, there are three additional classification datasets: Image Style, Skin Tone, and Home Decor Color. Figure <ref type="figure" target="#fig_0">1</ref> depicts the relationship between the pretraining setting and the multi-task training setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Model Architecture</head><p>We experiment with two families of model architectures: CNN-based ResNext architectures <ref type="bibr" target="#b34">[35]</ref> and Transformerbased ViT architecture <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32]</ref>. In this work, we use the ResNeXt-101 32x8d variant for all our CNN-based experiments (as it is the baseline in production), thus we will refer to it in short as ResNeXt-101 throughout this paper. We use the ViT-Base model variants in our experiments, which contain 12 Transformer encoder layers, 12 attention heads, hidden size of 768, and MLP size of 3072 (86M parameters) <ref type="bibr" target="#b8">[9]</ref>. We consider the ViT-B/32 and ViT-B/16 variants, which use an image patch size of 32x32 and 16x16, respectively. Due to limitations on GPU memory and compute, and the quadratic complexity of the Vision Transformer model with respect to input sequence length, it was not practical to evaluate variants with smaller patch sizes. For pretraining, we use a prediction head trained with the multi-label softmax loss <ref type="bibr" target="#b20">[21]</ref>, which yielded better results than training with per-cluster sigmoid outputs and binary cross entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Offline Evaluation</head><p>We studied the effect of large-scale pretraining through the transfer performance of the fine-tuned Unified Visual Embedding on the key downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Retrieval Evaluation</head><p>We use three offline evaluation sets described in the previous works <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b25">26]</ref> to measure the image retrieval performance of the fine-tuned Unified Visual Embedding: Visual Shopping (VS), Flashlight (F), and Lens (L). The metric average Precision@20 is used for Lens and Flashlight tasks, and metric P@1 is used for fine-grained Visual Shopping task. In this work, we use the methodology described in Shiau et al. <ref type="bibr">[</ref>  <ref type="table">1</ref>: Summary of Unified Embedding retrieval performance for different pretraining datasets and model architectures. "VS" = Visual Shopping Precision@1, "F" = Flashlight Average Precision@20, "L" = Lens Average Precision@20, "C" = Average Precision@1 of ten image classification tasks. "RN-101" = ResNeXt-101, "IN-1k" = ImageNet-1k, and "ANN-1.3B" = Annotations-1.3B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Label to collect a Visual Shopping offline evaluation set with ∼40k (query, product) pairs plus ∼110k distractor set.</p><p>In Table <ref type="table">1</ref>, we see that improvements to the pretraining procedure yield consistent benefits for the CNN-based backbone. Switching from ImageNet-1k to a generic largescale dataset (IG-940M) yielded an average relative improvement of +15.5% across the three retrieval tasks of interest. When using a domain-specific large-scale dataset of comparable scale (Annotations-1.3B), we obtained a further +8.5% average relative improvement to the retrieval performance. Finally, by leveraging the Vision Transformer model architecture with large pretraining capacity, we obtained an additional +6.7% average relative improvement to the retrieval performance. The finding is consistent with <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32]</ref>, when the pretraining dataset is small, CNN-based backbone has slight advantage. Vision Transformer starts to outperform when we use the Annotation-1.3B pretraining dataset.</p><p>We studied the impact of design choices in the label generation procedure Table <ref type="table" target="#tab_2">2</ref>. The decisions to limit the selected terms to those in the visual dictionary and restrict the selected clusters to those matching the top-level interests of the content item yielded improvements to the Visual Shopping retrieval performance. Furthermore, reducing the size of the clustered label space benefits the computational efficiency of the prediction head in pretraining.</p><p>Per the results in Table <ref type="table" target="#tab_3">3</ref>, we found that the Vision Transformer backbone can take advantage of higher-resolution inputs (384x384 images) in the fine-tuning process, whereas the ResNeXt-101 backbone did not see consistent benefits from higher-resolution inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Classification Evaluation</head><p>We observed consistent improvements across classification tasks when replacing the ImageNet-1k backbone with one that is pretrained on a large-scale dataset. For this evaluation, we measured the average P@1 across 10 tasks, namely, Camera Categories, Home Decor Color, Fashion Color, Pattern, Fabric, Image Style, Lowerbody Length, Dress Style, and Skin Tone. We found that using a ResNeXt-101 model pretrained on a generic large-scale dataset (IG-940M) yielded an average improvement of +2.7% across these tasks, while pretraining on a domain-specific large-scale dataset (Annotations-1.3B) yielded a larger average improvement of +3.6% over the ImageNet-1k baseline model. By adopting the ViT-B/16 backbone, we obtained the largest average improvement of +4.5% over the baseline model.</p><p>Furthermore, we observed large improvements for important downstream tasks where no training data had yet been integrated. The unified visual embedding is a key part of a near-duplicate image detection system <ref type="bibr" target="#b9">[10]</ref>, and we measure the performance of the visual embedding for this application using R@P95 on a held-out eval set. For the ResNeXt-101 model architecture, performance improved from 42.4% to 60.8% as the pretraining dataset is varied from ImageNet-1k to IG-940M. Furthermore, we found that as the pretraining dataset is varied from IG-940M to Annotations-1.3B, performance improved again from 60.8% to 84.9%. This task improvement was achieved without any specific data collection or optimization for the target use case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Human Relevance</head><p>We deployed large-scale pretrained models to a visual shopping system, and measured the real-world relevance improvement with different pretraining datasets and model architectures through end-to-end human relevance evaluation as shown in Table <ref type="table" target="#tab_4">4</ref>. We sampled roughly 8,000 traffic weighted user image queries in production, with half of the queries from fashion and half from home decor domain. For each model variant, we compute binary embeddings for the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pretraining</head><p>Offline VS P@1 E2E Extremely Similar@1 E2E Similar@1 Production <ref type="bibr" target="#b25">[26]</ref>   We report click (C-vol) and click-through volume (CT-vol) relative improvement over our current production system. We also report relative improvement of number of users who click (C-er) or click-through (CT-er) on the product recommendations.</p><p>queries and retrieve top-1 results from the shopping product corpus. The pairs of (query, product) are then rated by inhouse human evaluators. We reported two levels of relevance. The pair is rated extremely similar when all key attributes, such as color, pattern and materials, match. The pair is rated similar when 1-2 attributes have minor mismatches. The general improvement trend found in offline evaluation experiments holds true for the industry scale visual shopping application. With CNN-based backbone, when we update the pretraining dataset from ImageNet-1k, to IG-940M, to Annotations-1.3B, the end-to-end extremely similar@1 metrics improves from 14.9% to 19.1% (∼ 28% relative improvement). With Transformer-based backbone, we can better leverage the largest pretraining dataset Annotations-1.3B, and further improves the metrics to 23.9% (∼ 60% relative improvement). Comparing to the current production model, the end-to-end extremely similar@1 metrics improves ∼38% relative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">A/B Experiments</head><p>We take two model variants for online visual shopping A/B experiment -ResNeXt-101 and ViT-B/16 224x pretrained on Annotations-1.3B dataset. The relevance improvement from large-scale pretraining translates to better user engagement as shown in  <ref type="table" target="#tab_4">4</ref>), it is computationally more expensive. For this application, ViT-B/16 224x provides better trade-offs between the retrieval performance and the computational requirements of the model in training and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Computational Efficiency</head><p>There are three model architectures that we consider for production deployment: ResNeXt-101, ViT-B/16 224x and ViT-B/16 384x. We benchmark various practical aspects of the computational efficiency on a single GPU shown in Table <ref type="table" target="#tab_7">6</ref>. The detailed environment setup for running the benchmark is found in the appendix.</p><p>For batch operations such as distributed training and batch inference, ResNeXt-101 and ViT-B/16 224x have similar throughput, and thus cost similarly to train the model and compute the embeddings for the visual search corpus. For real-time applications such as visual shopping, we run single image inference. ViT-B/16 224x has the lowest single image inference latency. Overall, we found that ViT-B/16 224x model is the most computationally efficient model to deploy in the production environment. ViT-B/32</p><p>Figure <ref type="figure">4</ref>: Retrieval performance (Visual Shopping P@1) with respect to the size of the pretraining dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Pretraining Dataset Scale</head><p>The sample count of the pretraining dataset has a significant impact on the Unified Visual Embedding retrieval performance. In Figure <ref type="figure">4</ref>, we notice a consistent trend of improvements as the size of the pretraining dataset is increased. When training with 1% of the Annotations-1.3B dataset, the ResNeXt-101 model architecture has a slight performance advantage (+0.4%) relative to the ViT-B/16 model architecture. However, as the sample count increases, the performance advantage of the ViT-B/16 model becomes more clear. These findings are consistent with the results in Table <ref type="table">1</ref>, where pretraining on ImageNet-1k vs. Annotations 1.3B yields an absolute difference of 12.8% in VS P@1 performance for the ResNeXt-101 model architecture, whereas the absolute difference in performance of 25.5% for the ViT-B/32 model architecture is significantly greater. For future work, it would be valuable to study the performance tradeoffs at even larger pretraining dataset sizes, especially with parallel scaling of the Transformer model capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Pretraining Label Distribution</head><p>Figure <ref type="figure" target="#fig_4">5</ref> highlights the Zipfian <ref type="bibr" target="#b45">[46]</ref> distribution of Annotations-1.3B and the non-Zipfian distribution of other common pretraining datasets, including ImageNet-1k and ImageNet-21k. To handle the large class imbalance of webly-supervised data, it has been demonstrated in prior work that resampling according to the inverse square root of class frequency yields improvements to the learning of rare classes and can improve the transfer performance of the pretrained representation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref>. We explored the benefits of this dataset resampling technique for the Annotations-1.3B dataset, finding that resampling the dataset for ViT-B/16 224x pretraining improved the VS P@1 from 41.0% to 53.1% as compared to uniform sampling of the dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Few-Shot Learning</head><p>Given the significant costs associated with collecting human labels, we study the extent to which pretraining can improve the sample efficiency of fine-tuning as a way to help reduce these costs for future label collection efforts. We consider the Image Style multi-label classification task as an example for our analysis. This dataset consists of 72k images and has an average of 6.2k labels per category. The dataset categories, such as "Screenshot," "Mosaic," and "Infographic," are useful for content understanding at scale. In Figure <ref type="figure" target="#fig_5">6</ref>, we present the Precision@1 performance for the Image Style task head when varying the number of images per category <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr">100,</ref><ref type="bibr">250</ref>, 500, all) used in fine-tuning. During fine-tuning, the other Unified Visual Embedding dataset sizes remain the same.</p><p>As suggested in the context of language modeling <ref type="bibr" target="#b11">[12]</ref>, rotation, viewpoint, and environment in the dataset construction. In Figure <ref type="figure">7</ref> we see that generalization performance improves as the sample count of the pretraining dataset is increased from 13M to 1.3B and as the spatial resolution is increased in the comparison between ViT-B/32 and ViT-B/16. Per Table <ref type="table">7</ref>, without any specific optimization for the ImageNet taxonomy in the dataset construction, Annotations-1.3B achieves competitive transfer performance on the tasks relative to other large-scale image datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented a scalable approach for pretraining with over a billion images in order to improve a production Unified Visual Embedding model. By leveraging heterogeneous sources of textual supervision in a principled fashion, we constructed a large-scale image dataset known as Annotations-1.3B that obtained strong transfer performance and enabled adoption of the state-of-the-art Vision Transformer architecture. The embedding yielded significant improvements to the Visual Shopping system when deployed in production and demonstrated strong advantages across a variety of use cases. This work suggests the promise of further scaling of the Transformer-based pretraining paradigm as a way to systematically improve complex computer vision applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (A) The overall architecture for Unified Visual Embeddings<ref type="bibr" target="#b40">[41]</ref>, consisting of one backbone convolutional neural network model consuming a variety of datasets including classification and metric learning across a set of loss and regularization functions. The embedding is consumed by a variety of customers across retrieval, as an input feature, and for fine-tuning domain-specific models. We also show (B) our proposed methodology with grey boxes denoting unchanged components. We introduce billion-scale image pretraining to produce a strong backbone encoder and leverage the Vision Transformer encoder as a replacement of the CNN backbone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of the label generation procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visual concreteness distribution for terms in the annotations dictionary and visual dictionary threshold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Label distribution of image datasets, including ImageNet-1k, ImageNet-21k, and Annotations-1.3B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Large-scale pretraining yields better transfer performance in the small data regime for the Image Style task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Example of retrieval results using the control (production) model, the ResNeXt-101 Annotations-1.3B model, and the ViT-B/16 Annotations-1.3B model. The ViT model generally matches more similar product results.</figDesc><graphic url="image-54.png" coords="8,287.52,231.80,61.29,63.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>. L1 Interest Restriction jaguar</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>jaguar</cell></row><row><cell></cell><cell>panthera onca</cell></row><row><cell></cell><cell>jaguar print</cell></row><row><cell></cell><cell>best design</cell></row><row><cell></cell><cell>2. Visual Dictionary Restriction</cell></row><row><cell></cell><cell>ANIMALS : CLS_1</cell></row><row><cell></cell><cell>VEHICLES: CLS_2</cell></row><row><cell></cell><cell>W_FASHION: CLS_3</cell></row><row><cell></cell><cell>M_FASHION: CLS_4</cell></row><row><cell>3. Annotation Clustering</cell><cell>4</cell></row></table><note>1. Annotation Selection (EN, jaguar, 0.90) (EN, panthera onca, 0.90) (EN, jaguar print, 0.95) (EN, best design, 0.92) (EN, print design, 0.75) →ANIMALS:CLS_1 panthera onca → ANIMALS:CLS_1 jaguar → VEHICLES:CLS_2 jaguar print → W_FASHION:CLS_3 jaguar print → M_FASHION:CLS_4</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Table</figDesc><table><row><cell>Model</cell><cell cols="2">Pretraining VS</cell><cell>F</cell><cell>L</cell><cell>C</cell></row><row><cell>RN-101</cell><cell>IN-1k</cell><cell cols="4">39.6 59.7 17.2 85.2</cell></row><row><cell>RN-101</cell><cell>IG-940M</cell><cell cols="4">46.7 67.6 20.2 87.9</cell></row><row><cell>RN-101</cell><cell cols="5">ANN-1.3B 52.4 70.8 22.7 88.8</cell></row><row><cell cols="2">ViT-B/32 IN-1k</cell><cell cols="4">29.2 44.7 15.2 82.3</cell></row><row><cell cols="6">ViT-B/32 ANN-1.3B 46.4 68.9 24.9 86.5</cell></row><row><cell cols="6">ViT-B/16 ANN-1.3B 54.7 74.3 26.7 89.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>26]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of retrieval performance and pretraining label count for variants of the label generation procedure. Improvements are relative to variant in previous row.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Count</cell><cell>VS</cell></row><row><cell cols="2">Annotation Clustering</cell><cell></cell><cell>34k</cell><cell>-</cell></row><row><cell cols="2">+ Visual Dictionary</cell><cell></cell><cell>26k</cell><cell cols="2">+0.9%</cell></row><row><cell cols="2">+ L1 Interest Restrict</cell><cell></cell><cell>18k</cell><cell cols="2">+1.1%</cell></row><row><cell>Model</cell><cell cols="2">Resolution VS</cell><cell>F</cell><cell>L</cell><cell>C</cell></row><row><cell>RN-101</cell><cell>224x</cell><cell cols="4">52.4 70.8 22.7 88.8</cell></row><row><cell>RN-101</cell><cell>384x</cell><cell cols="4">52.3 71.4 22.5 89.0</cell></row><row><cell cols="2">ViT-B/16 224x</cell><cell cols="4">53.1 73.7 26.3 89.2</cell></row><row><cell cols="2">ViT-B/16 384x</cell><cell cols="4">54.7 74.3 26.7 89.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Summary of retrieval performance for different input image resolutions and model architectures.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Visual Shopping end-to-end retrieval performance using Unified Embedding with different pretraining datasets and model architectures. The end-to-end system retrieval performance is measured by our in-house human evaluators. Two levels of relevance (Extremely Similar and Similar) are shown in the table.</figDesc><table><row><cell></cell><cell></cell><cell>IG-940M</cell><cell></cell><cell>44.7</cell><cell>17.3</cell><cell>39.0</cell></row><row><cell cols="2">ResNeXt-101</cell><cell>ImageNet-1k</cell><cell></cell><cell>39.6</cell><cell>14.9</cell><cell>32.1</cell></row><row><cell cols="2">ResNeXt-101</cell><cell>IG-940M</cell><cell></cell><cell>46.7</cell><cell>17.9</cell><cell>36.8</cell></row><row><cell cols="2">ResNeXt-101</cell><cell cols="2">Annotations-1.3B</cell><cell>52.4</cell><cell>19.1</cell><cell>38.8</cell></row><row><cell cols="2">ViT-B/16 224x</cell><cell cols="2">Annotations-1.3B</cell><cell>53.1</cell><cell>23.6</cell><cell>40.6</cell></row><row><cell cols="2">ViT-B/16 384x</cell><cell cols="2">Annotations-1.3B</cell><cell>54.7</cell><cell>23.9</cell><cell>42.3</cell></row><row><cell>Model</cell><cell cols="2">C-vol CT-vol</cell><cell>C-er</cell><cell>CT-er</cell></row><row><cell>Production [26]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ResNeXt-101</cell><cell>+10%</cell><cell>+8%</cell><cell>+7%</cell><cell>+7%</cell></row><row><cell>ViT-B/16 224x</cell><cell cols="4">+22% +23% +17% +22%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Visual Shopping A/B experiment using large-scale Annotations-1.3B pretrained Unified Embedding with CNNbased and transformer-based architectures.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>. ViT-B/16 224x pretrained</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Comparison of computation efficiency for different model architectures. Train and inference throughput are measured with maximum batch size for a single GPU. Latency refers to single-image batch inference. "ViT" = ViT-B/16.</figDesc><table><row><cell>on Annotations-1.3B improves both the volume of click-</cell></row><row><cell>throughs and the number of users who click-through the</cell></row><row><cell>recommendations by more than 20%.</cell></row><row><cell>Though ViT-B/16 384x scores higher in terms of rele-</cell></row><row><cell>vance metrics (Table</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>The authors would like to thank Eric Tzeng, Raymond Shiau, Kofi Boakye, Vahid Kazemi, and Chuck Rosenberg for valuable discussions regarding the paper, and the anonymous reviewers and ACs for their helpful suggestions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>large-scale pretraining can serve as an effective multiplier of the size of each fine-tuning dataset. When fine-tuned with only 500 samples per category, the ResNeXt-101 model pretrained on Annotations-1.3B achieves a similar level of performance (94.6%) as compared to the ResNeXt-101 model pretrained on ImageNet-1k and fine-tuned on the full dataset (94.7%). Furthermore, when the smaller-size dataset was used in conjunction with the Annotations-1.3B pretrained Vision Transormer backbone, it was able to outperform the full-size dataset used in conjunction with the ImageNet-1k pretrained backbone (95.0%). In the extreme case of finetuning with only 5 samples per category, the large-scale pretrained backbone yields a +20% absolute improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Cross-Domain Generalization</head><p>The ability of the pretrained model to generalize is important as the requirements for the Unified Visual Embedding continue to evolve over time. We study the cross-domain generalization performance of our pretraining approach by analyzing the transfer performance on image classification benchmarks, including ImageNet-1k and ObjectNet <ref type="bibr" target="#b0">[1]</ref>, a challenging real-world test set that controls for the biases of Pretraining is implemented using PyTorch on 8 p3dn.24xlarge Amazon EC2 instances with a total of 64 Tesla V100 GPUs, while the fine-tuning uses PyTorch on a single p3dn.24xlarge Amazon EC2 instance with 8 Tesla V100 GPUs. We use DistributedDataParallel for multi-GPU training. We use automatic mixed precision for all of our experiments, and channels-last memory format for the ResNeXt experiments, in order to improve the training throughput. All of our model training runs and performance benchmarks use PyTorch 1.7.1, CUDA 11.0, and cuDNN 8.</p><p>Vision Transformer pretraining uses a warmup phase of 10k steps, total batch size of 8192, base learning rate (LR) of 8e-4, and linear decay LR schedule of 2 epochs in length, such that around 2.6B images are processed during the main phase of training. We train using the AdamW <ref type="bibr" target="#b19">[20]</ref> optimizer with a weight decay value of 0.05. Vision Transformer Unified Visual Embedding fine-tuning uses a warmup phase of 5k steps, base LR of 0.24, and cosine decay LR schedule of 20 epochs in length. We fine-tune using the SGD optimizer with a base LR of 0.24 and weight decay of 1e-4 for the non-sparse parameters. Vision Transformer ImageNet finetuning uses a warmup phase of 5k steps, base LR of 0.03, cosine decay LR schedule of 50k steps, SGD optimizer, and zero weight decay.</p><p>ResNeXt-101 pretraining uses a warmup phase of 15k steps, total batch size of 12288, base learning rate of 0.03, and step LR schedule of 20 steps and γ = 0.5. We train using the LARS <ref type="bibr" target="#b38">[39]</ref> optimizer with a weight decay value of 1e-4. The hyperparameters of the ResNeXt-101 Unified Visual Embedding fine-tuning are largely the same as <ref type="bibr" target="#b25">[26]</ref>, except the base learning rate is 0.03.</p><p>For pretraining, we use the Inception <ref type="bibr" target="#b28">[29]</ref> random crop strategy, whereas for Unified Visual Embedding fine-tuning we apply horizontal mirroring, random crops, and color jitter to the resized images. For ImageNet fine-tuning we directly apply the data augmentation strategy that is specified in the Vision Transformer work.</p><p>For ablations on the sample count of the pretraining dataset, we linearly interpolate the training schedule length between the minimum and maximum value, i.e., 100 epochs on the 13M dataset and 2 epochs on the 1.3B dataset.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</title>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Alverio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Toward transformer-based object detection</title>
		<author>
			<persName><forename type="first">Josh</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kislyuk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09958</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">GrokNet: Unified computer vision model trunk and embeddings for commerce</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sami</forename><surname>Alsheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yina</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Pizzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karun</forename><surname>Henning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omkar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fedor</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><surname>Borisyuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ilya Sutskever, and Dario Amodei</title>
				<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06987</idno>
		<title level="m">A short note on the kinetics-700 human action dataset</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Evolution of a web-scale near duplicate image detection system</title>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Gusev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajing</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01293</idno>
		<title level="m">Scaling laws for transfer</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Quantifying the visual concreteness of words and topics in multimodal datasets</title>
		<author>
			<persName><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep metric learning using triplet network</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Ailon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIMBAD</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="84" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Web-scale responsive visual search at bing</title>
		<author>
			<persName><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Komlev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">(</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiapei</forename><surname>Stephen) Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meenaz</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Merchant</surname></persName>
		</author>
		<author>
			<persName><surname>Sacheti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visual search at pinterest</title>
		<author>
			<persName><forename type="first">Yushi</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kislyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Tavel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Related pins at pinterest: The evolution of a real-world recommender system</title>
		<author>
			<persName><forename type="first">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Shiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">C</forename><surname>Kislyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yushi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName><forename type="first">Yair</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Shop the look: Building a large scale visual shopping system at pinterest</title>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Shiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao-Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Li Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eileen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunlong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">C</forename><surname>Stroud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.14937</idno>
		<title level="m">Learning video representations from textual web supervision</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">MSURU: Large scale ecommerce image classification with weakly supervised search data</title>
		<author>
			<persName><forename type="first">Yina</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fedor</forename><surname>Borisyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddarth</forename><surname>Malreddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Kirshner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Yfcc100m: The new data in multimedia research</title>
		<author>
			<persName><forename type="first">Bart</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerald</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damian</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="64" to="73" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multisage: Empowering gcn with contextualized multi-embeddings on webscale multipartite networks</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikil</forename><surname>Pancha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visual search at ebay</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajinkya</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yury</forename><surname>Bubnov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadi</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robinson</forename><surname>Piramuthu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Large batch training of convolutional networks</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Classification is a strong baseline for deep metric learning</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao-Yu</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning a unified embedding for visual search at pinterest</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao-Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Rosenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A large-scale study of representation learning with the visual task adaptation benchmark</title>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Ruyssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><forename type="middle">Susano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04867</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Large-scale visual search with binary distributed graph at alibaba</title>
		<author>
			<persName><forename type="first">Kang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changxu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Large scale long-tailed product recognition system at alibaba</title>
		<author>
			<persName><forename type="first">Xiangzeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">PinText: A multitask text embedding system in pinterest</title>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">The psycho-biology of language: An introduction to dynamic philology</title>
		<author>
			<persName><forename type="first">George</forename><surname>Kingsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zipf</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Routledge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A flexible large-scale similar product identification system in e-commerce</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lixi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michinari</forename><surname>Momma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikai</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD Workshop on Industrial Recommendation Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
