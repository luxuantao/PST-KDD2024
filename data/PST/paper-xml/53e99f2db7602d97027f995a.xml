<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PARALLEL ALGORITHMS FOR DENSE LINEAR ALGEBRA COMPUTATIONS*</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Gallivant</surname></persName>
						</author>
						<author>
							<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Plemmons$</surname></persName>
						</author>
						<author>
							<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Sameht</surname></persName>
						</author>
						<author>
							<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Gallivan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Plemmons</surname></persName>
						</author>
						<author>
							<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Sameh</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Center for Supercomputing Research and Development</orgName>
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>Illinois</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Departments of Computer Science and Mathematics</orgName>
								<orgName type="institution">North Carolina State University</orgName>
								<address>
									<postCode>27695-8206</postCode>
									<settlement>Raleigh</settlement>
									<region>North Carolina</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PARALLEL ALGORITHMS FOR DENSE LINEAR ALGEBRA COMPUTATIONS*</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0548249D6C2C076A45BB0D237F91C1FA</idno>
					<note type="submission">Received by the editors March 6, 1989; accepted for publication (in revised form) October 31, 1989.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>AMS(MOS) subject classifications. 65-02</term>
					<term>65F05</term>
					<term>65F15</term>
					<term>65F20</term>
					<term>65N20</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scientific and engineering research is becoming increasingly dependent upon the development and implementation of efficient parallel algorithms on modern high-performance com- puters. Numerical linear algebra is an indispensable tool in such research and this paper attempts to collect and describe a selection of some of its more important parallel algorithms. The purpose is to review the current status and to provide an overall perspective of parallel algorithms for solving dense, banded, or block-structured problems arising in the major areas of direct solution of linear systems, least squares computations, eigenvalue and singular value computations, and rapid elliptic solvers. A major emphasis is given here to certain computational primitives whose efficient execution on parallel and vector computers is essential in order to obtain high performance algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>with a modest number of processors and distributed memory architectures such as the hypercube.</p><p>Since the amount of literature in these areas is very large we have attempted to select representative work in each. As a result, the topics and the level of detail at which each is treated can not help but be biased by the authors' interest. For exam- ple, considerable attention is given here to the discussion and performance analysis of certain computational primitives and algorithms for high performance machines with hierarchical memory systems. Given recent developments in numerical software technology, we believe this is appropriate and timely.</p><p>Many important topics relevant to parallel algorithms in numerical linear algebra are not discussed in this survey. Iterative methods for linear systems are not men- tioned since the recent text by Ortega <ref type="bibr" target="#b138">[137]</ref> contains a fairly comprehensive review of that topic, especially as it relates to the numerical solution of partial differential equa- tions. Parallel algorithms using special techniques for solving generally sparse prob- lems in linear algebra will also not be considered in this particular survey. Although significant results have recently been obtained, the topic is of sufficient complexity and importance to require a separate survey for adequate treatment. The organization of the rest of this paper is as follows. Section 2 briefly discusses some of the important aspects of the architecture and the way in which they influence algorithm design. Section 3 contains a discussion of the decomposition of algorithms into computational primitives of varying degrees of conplexity. Matrix multiplica- tion, blocksize analysis, and triangular system solvers are emphasized. Algorithns for LU and LU-like factorizations on both shared and distributed memory systems are considered in 4. Parallel factorization schemes for block-tridiagonal systems, which arise in numerous application areas, are discussed in detail. Section 5 concerns parallel orthogonal factorization methods on shared and distributed memory systems for solving least squares problems. Recursive least squares computations, on local memory hypercube architectures, are also discussed in terms of applications to com- putations in control and signal processing. Eigenvalue and singular value problems are considered in 6. Finally, 7 contains a review of parallel techniques for rapid elliptic solvers of importance in the solution of separable elliptic partial differential equations. In particular, recent domain decomposition, block cyclic reduction, and boundary integral domain decomposition schemes are examined.</p><p>2. Architectures of interest. To satisfy the steadily increasing demand for computational power by users in science and engineering, supercomputer architects have responded with systems that achieve the required level of performance via pro- gressively complex synergistic effects of the interaction of hardware, system software (e.g., restructuring compilers and operating systems), and system architecture (e.g., multivector processors and multilevel hierarchical memories). Algorithm designers are faced with a large variety of system configurations even within a fairly generic architectural class such as shared memory multivector processors. Furthermore, for any particular system in the architectural class, a CRAY-2 or Cedar <ref type="bibr" target="#b118">[117]</ref>, the algo- rithm designer encounters a complex relationship between performance, architectural parameters (cache size, number of processors), and algorithmic parameters (method used, blocksizes). As a result, codes for scientific computing such as numerical linear algebra take the form of a parameterized family of algorithms that can respond to changes within a particular architecture, e.g., changing the size of cluster or global memory on Cedar, or when moving from one member of an architectural family to another, e.g., Cedar to CRAY-2. The latter adaptation may, of course, involve chang- ing the method used completely, say from Gaussian elimination with partial pivoting to a decomposition based on pairwise pivoting.</p><p>There are several consequences of such a situation. First, algorithm designers must be sensitive to architecture/algorithm mapping issues and any discussion of parallel numerical algorithms is incomplete if these issues are not addressed. Second, one of the main thrusts of parallel computing research must be to change the situa- tion. That is, if scientific computing is to reap the full benefits of parallel processing, cooperative research involving expertise in the areas of parallel software development (debugging, restructuring compilers, etc.), numerical algorithms, and parallel architec- tures is required to develop parallel languages and programming environments along with parallel computer systems that mitigate this architectural sensitivity. Such co- operative work is underway at several institutions. The architecture that first caused a widespread and substantial algorithm redesign activity in numerical computing is the vector processor. Such processors exploit the concept of pipelining computations. This technique decomposes operations of inter- est, e.g., floating point multiplication, into multiple stages and implements a pipelined functional unit that allows multiple instances of the computation to proceed simulta- neously one in each stage of the pipe. Such parallelism is typically very fine-grain and requires the identification in algorithns of large amounts of homogeneous work ap- plied to vector objects. Fortunately, numerical linear algebra is rich in such operations and the vector processor can be used with reasonable success. From the point of view of the functional unit, the basic algorithmic parameter that influences performance is the vector length, i.e., the number of elements on which the basic computation is to be performed. Architectural parameters that determine the performance for a partic- ular vector length include cycle time, the number of stages of the pipeline, as well as any other startup costs involved in preparing the functional unit for performing the computations. Various models have been proposed in the literature to characterize the relationship between algorithmic and architectural parameters that determine the performance of vector processors. Perhaps the best known is that of Hockney and Jesshope <ref type="bibr" target="#b101">[100]</ref>.</p><p>The Cyber 205 is a memory-to-memory vector processor that has been success- fully used for scientific computation. On every cycle of a vector operation multiple operands are read from memory, each of the functional unit stages operate on a set of vector elements that are moving through the pipe, and an element of the result of the operation is written to memory. Obviously, the influence of the functional unit on algorithmic parameter choices is not the only consideration required. Heavy demands are placed on the memory system in that it must process two reads and a write (along with any other control I/O) in a single cycle. Typically, such demands are met by using a highly interleaved or parallel memory system with M &gt; 1 memory modules whose aggregate bandwidth matches or exceeds that demanded by the pipeline. Ele- ments of vectors are then assigned across the memory modules in a simple interleaved form, e.g., v(i) is in module mod M, or using more complex skewing schemes <ref type="bibr" target="#b195">[193]</ref>.</p><p>As a result, the reference pattern to the memory modules generated by accessing elements of a vector is crucial in determining the rate at which the memory system can supply data to the processor. The algorithmic parameter that encapsulates this information is the stride of vector access. For example, accessing the column of an array stored in column-major order results in a stride of 1 while accessing a row of The details of the architectural tradeoffs involved in a vector processor are somewhat surprisingly subtle and complex. For an excellent discussion of some of them see <ref type="bibr" target="#b176">[174]</ref>. Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php the same array requires a stride of lda where lda is the leading dimension of the array data object.</p><p>Not all vector processors are implemented with the three computational memory ports (2 reads/1 write) required by a memory-to-memory processor. The CRAY- 1, one CPU of a CRAY-2 and one computational element of an Alliant FX/8 are examples of register-based vector processors that have a single port to memory and, to compensate for the loss in data transfer bandwidth, provide a set of vector registers internal to the processor to store operands and results. 2 Each of the registers can hold a vector of sufficient length to effectively use the pipelined functional units available. The major consequence of this, considered in detail below, is that such processors require careful management of data transfer between memory and register in order to achieve reasonable performance. In particular, care must be taken to reuse a register operand several times before reloading the register or to accumulate as many partial results of successive computations in the same register before storing the values to memory, i.e., reducing the number of loads and stores, respectively. Some register-based vector processors also use two other techniques to improve performance. The first is the use of parallelism across functional units and ports.</p><p>Multiple instructions that have no internal resource conflict, e.g., adding two vector registers with the result placed in a third and loading of a fourth register from memory, are executed simultaneously, making as much use of the available resources as possible. This influences kernel design in that careful ordering of assembler level instructions can improve the exploitation of the processor. The second technique is essentially functional unit parallelism with certain re- source dependences managed by the hardware at runtime. The technique is called chaining and it allows the result of one operation to be routed into another operation as an operand while both operations are active. For example, on a machine without chaining, loading a vector from memory into a register and adding it to another reg- ister would require two distinct nonoverlapped vector operations and therefore two startup periods, etc. Chaining allows the elements of the vector loaded into the first register to be made available, after a small amount of time, for use by the adder before the load is completed. Essentially, it appears as if the vector addition was taking one of its operands directly from memory. For processors that handle chaining of instruc- tions automatically at runtime, careful consideration of the order of instructions used in implementing an algorithm or kernel is required. Some other vector processors, however, make the chaining of functional units and the memory port an explicit part of the vector instruction set. For example, the Alliant FX/8 allows one argument of a vector instruction to be given as an address in memory, thereby chaining the memory port and the appropriate functional units. The best example of this is the workhorse of its instruction set, the triad, which computes v v2 / ax, where v and V2 are vector registers, a is a scalar, and x is a vector in memory. This instruction explicitly chains the floating point multiplier and adder and the memory port. Such instruction constructs greatly simplify the exploitation of the chaining capabilities of a vector processor at the cost of the loss of a certain amount of flexibility. While vector processors have been used and can deliver substantial performance for many computations, the quest for even more speed led to the availability and continuing development of MIMD multiprocessors and multivector processors. The processors on such machines are capable of executing arbitrary code segments in As local memory sizes increase, the architecture moves toward the distributed end of the architectural spectrum. Not surprisingly, the ability of the network/memory system to supply data to the multiple processors at a sufficient rate is one of the key components of performance of shared memory architectures. As a result, the orga- nization and proper exploitation of this system must be carefully considered when designing high-performance algorithms. The generic organization in Fig. <ref type="figure" target="#fig_0">1</ref> shows a highly interleaved or parallel mem- ory system connected to the processors. This connection can take on several forms. Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php For a small number of processors and memory modules, p, a high-performance bus or crossbar switch can provide complete connectivity and reasonable performance.</p><p>Unfortunately, such networks quickly become too costly as p increases. For larger systems, it is necessary to build scalable networks out of several smaller, completely connected switches such as (s x s)-crossbars. The 2-network of Lawrie <ref type="bibr" target="#b120">[119]</ref> can connect p s k processors and melnory modules with k network stages. Each stage comprises s k-1 (s x s)-crossbars, for a total of O(plogsp switches. As with vector processors, data skewing schemes and access stride manipulation are important in balancing the memory bandwidth achieved with the aggregate computational rate of the processors. Ideally, the two should balance perfectly; in practice, keeping the two within a small multiple is achievable for numerical linear algebra computations via the skewing and stride manipulations or with the introduction of local memory (discussed below). As p increases, however, the latency for each memory access grows as O(k).</p><p>Fortunately, the addition of architectural features such as data prefetch mechanisms and local menory can provide some mitigation of this problem.</p><p>As mentioned above, one of the ways in which the performance of a large shared memory system can be improved is the introduction of local memories or caches with each processor. The idea is similar to the use of registers within vector processors in that data can be kept for reuse in small fast memory private to each processor. If sufficient data locality<ref type="foot" target="#foot_2">3</ref> is present in the computations the processor can proceed at a rate consistent with the data transfer bandwidth of the cache rather than the lower effective bandwidth of the large shared memory due to latency and conflicts.</p><p>One difference between local memories/caches and vector registers, however, is that registers have a prescribed shape and must be used, for the most part, in vector operations; they must contain and be operated on as a vector v E Nm where rn is the vector length. On the other hand, local memory or caches can contain, up to a point, arbitrary data objects with no constraint on type or use. These differences can strongly affect the way that these architectural features influence algorithm parameter choices.</p><p>Another feature which can significantly influence the performance of an algorithm on a shared menory machine is the architectural support for synchronization of processors. These mechanisms are required for the assignment of parallel work to a processor and enforcing data dependences to ensure correct operation once the assignment is made. The support found on the various multiprocessors varies consid- erably. Some provide special purpose hardware for controlling small grain tasks on a moderate number of processors and simple TEST-AND-SET<ref type="foot" target="#foot_3">4</ref> synchronization in mem- ory, e.g., the Alliant FX/8. Others provide more complex synchronization processors at the memory module or network level with capabilities such as FETCH-AND-OP or the Zhu-Yew primitives used on Cedar <ref type="bibr" target="#b198">[196]</ref>. Finally, there are some which are ori- ented toward large-grain task parallelism which rely more on system-software-based synchronization mechanisms with relatively large cost to coordinate multiple tasks within a user's job, often at the same time with the tasks of other users.</p><p>The discussion above clearly shows that the optimization of algorithms for shared memory multivector architectures involve the consideration of the tradeoffs concern- a total peak rate of approximately 94 Mflops. The startup times for the vector in- structions can reduce this rate significantly. For example, the vector triad instruction v v + cx (the preferred instruction for achieving high performance in many codes) has a maximum performance of 68 Mflops. Each CE has eight 32-element vector reg- isters and eight floating point scalar registers as well as other integer registers. The CE's are connected by a concurrency control bus (used as a synchronization facility). This mechanism allows an iteration of a parallel loop to be assigned to a processor within in time equivalent to a few floating point operations and provides synchroniza- tion support from lower iterations to higher iterations with a cost of a few cycles. As a result, the CE's can cooperate efficiently on parallel loops with iterations with a granularity of a small number of floating point operations.</p><p>There is only one memory port on each CE, like the CRAY-1 and a single CPU of the CRAY-2, therefore management of the vector registers is crucial. The CE's share the physical memory as well as a write-back cache that allows up to eight simultaneous accesses per cycle. The size of the cache can be configured from 64KB up to 512KB. The cache and the four-way interleaved main memory are connected through the main memory bus. Most of the detailed performance information for shared memory machines given below was obtained on this machine.</p><p>Distributed memory architectures can be roughly characterized in a fashion sim- ilar to that used above for shared memory. In particular, there are two major factors that distinguish them from shared memory architectures. These are the mode of memory access and the mode of synchronization.</p><p>On p-processor distributed memory machines with an aggregate memory size M each user-controlled processor has direct access to its local memory only, typically of size M/p. Accessing any other memory location requires the active participation of another user-controlled processor. As a result of this idea of direct interaction between processors to exchange data, distributed memory architectures are often identified by the topology of the connections between processors. Figure <ref type="figure" target="#fig_0">1</ref> illustrates three popular connection schemes. The ring topology (b) uses a linear nearest-neighbor bidirectional connection, essentially a linear array with a wrap-around connection between the first and last processor, while the mesh connection (c) provides two-dimensional nearest neighbor connections (wrap-around meshes are also used extensively). Both of these simple topologies work quite well for many numerical linear algebra algorithms. In particular, several algorithms are presented below for ring architectures. The hyper- A four-dimensional cube is illustrated in (d). The connection patterns are, as the name implies, local connections in an arbitrarily dimensioned space. In general, a k-dimensional cube has 2 k processors (vertices) each of which is connected to k other processors. It can be constructed from two (k-1)-dimensional cubes by simply con- necting corresponding vertices. As a result of this construction, the nodes have a very natural binary numbering scheme based on a Gray code. This construction also demonstrates one of the basic scalability problems of the hypercube in that the num- ber of connections for a particular processor grows as the size of the cube increases as opposed to the constant local connection complexity of the simpler mesh and ring topologies. Many of the more common topologies, such as rings and meshes, can be embedded into a hypercube of appropriate dimension. In fact, many of the hyper- cube algorithms published use the cube as if it were one of the simpler topologies. Synchronization on a distributed memory architecture, due to the memory access- ing paradigm, is accomplished via a data flow mechanism rather than the indivisible update used in a large shared memory system.    cessor when due to its position in its local code the processor decides a computation is to be performed and all of the memory transactions involving operands for the computation in remote memory modules are complete. (These transactions are the interaction between the processors associated with the local memory and the remote memory modules mentioned above.) Clearly, since the synchronization is so enmeshed in the control and execution of interprocessor communication, the major algorithmic reorganization that can alter the efficiency of the synchronization on distributed mem- ory machines is the partitioning of the computations (or similarly the data) so as to reduce the synchronization overhead required.</p><p>As we would expect, the algorithm/architecture mapping questions for a dis- tributed memory machine change appreciably from those of shared memory. Since the machines tend to have more, but less powerful, processors, a key aspect of al- gorithm organization is the exposure of large amounts of parallelism. Once this is accomplished the major task is the partitioning of the data and the computations onto the processors. This partitioning must address several tradeoffs.</p><p>To reduce total execution time, a suitable balance must be achieved between the amount of communication required and efficient spreading of the parallel computa- tions across the machine. One indicator of the efficient partitioning of the computa- tions and data is the relationship between the load balance across processors and the amount of communication between processors. Typically, although not necessarily, a more balanced load produces a more parallel execution of the computations, ignoring for a moment delays due to communication. On the other hand, dispersing the com- putations over many processors may increase the amount of communication required and thereby negate the benefit of parallelism. The property of data locality, which was very significant for shared memory ma- chines in the management of registers and hierarchical memory systems, is also very important for some distributed memory machines in achieving the desired balance. Ideally, we would like to partition the data and computations across the processors and memory modules in such a way that a small amount of data is exchanged be- tween processors at each stage of an algorithm, followed by the use of the received data in operations on many local data. As a result the cost of communication is com- pletely amortized over the subsequent computations that make use of the data. If the partitioning of the computations and data also results in a balanced computational load the algorithm proceeds near the aggregate computational rate of the machine. This is, of course, identical to the hierarchical memory problem of amortizing a fetch of a data operand from the farthest level of memory by combining it with several operands in the nearest. Therefore many of the discussions to follow concerning the data-transfer-to-operations ratios that are motivated by shared hierarchical memory considerations are often directly applicable to the distributed memory case, although, as is shown below, there is often a tradeoff between data locality and the amount of exploitable parallelism.</p><p>Of course, there is a spectrum of architectures and a particular machine tends to have characteristics of both shared and distributed memory architectures. For these hybrid architectures efficient algorithms often involve a combination of techniques used to achieve high performance on the two extremes. An example of such an architecture that is used in this paper to facilitate the discussion of these algorithms is the Cedar system being built at the University of Illinois Center for Supercomputing Research and Development (see Fig. <ref type="figure" target="#fig_12">2</ref>). It consists of clusters of vector processors connected to a large interleaved shared global memory access to which can be accelerated Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php by data prefetching hardware. At this level it looks much like a conventional shared memory processor. However, each cluster is, in turn, a shared memory multivector processor, a slightly modified Alliant FX/8, whose cluster memory is accessible only by its CE's. The size of the cluster memory is fairly large and therefore the aggregate makes up a considerable distributed memory system. Consequently, the Cedar ma- chine is characterized by its hierarchical organization in both memory and processing capabilities. The memory hierarchy consists of: vector registers ,private to each vector processor; cache and cluster memory shared by the processors within a cluster; and global memory shared by all processors in the system. Three levels of parallelism are also available: vectorization at the individual processor level, concurrency within each cluster, and global concurrency across clusters. Control and synchronization mecha- nisms between clusters are supported at two levels of granularity. The larger consists of large-grain tasks and multitasking synchronization primitives such as event waiting and posting similar to CRAY large-grain primitives. These primitives are relatively high cost in that they affect the state of the task from the point of view of the op- erating system, e.g., a task waiting for a task-level event is marked as blocked from execution and removed from the pool of tasks considered by the operating system when allocating computational resources. The second and lower-cost control mech- anism is the SDOALL loop (for spread DOALL) which provides a self-scheduling loop structure whose iterations are grabbed and executed at the cluster level by helper tasks created at the initiation of the user's main task. Each iteration can then use the smaller grain parallelism and vectorization available within the cluster upon which it is executing. The medium grain SDOALL loop is ideal for moderately tight intercluster communication such as that required at the highest level of control in multicluster primitives with BLAS-like functionality that can be used in iterations such as the hybrid factorization routir presented in 4. Hardware support for synchronization between clusters on a much tighter level than the task events is supplied by synchronization processors, one per global memory module, which implements the Zhu-Yew synchronization primitives <ref type="bibr" target="#b198">[196]</ref>.</p><p>3. Computational primitives. 3.1. Motivation. The development of high-performance codes for a range of architectures is greatly simplified if the algorithms under consideration can be de- composed into computational primitives of varying degrees of complexity. As new architectures emerge, primitives with the appropriate functionality which exploit the novel architectural features are chosen and used to develop new forms of the algorithms. Over the years, such a strategy has been applied successfully to the develop- ment of dense linear algebra codes. These algorithms can be expressed in terms of computational primitives ranging from operations on matrix elements to those involv- ing submatrices. As the pursuit of high performance has increased the complexity of computer architectures, the need to exploit this richness of decomposition has been reflected in the evolution of the Basic Linear Algebra Subroutines (BLAS).</p><p>The investigation of dense matrix algorithms in terms of decomposition into lower- level primitives such as the three levels of the BLAS has several advantages. First, for many presently available machines the computational granularity represented by single instances of the BLAS primitives from one of the levels or multiple instances executing simultaneously is sufficient for investigating the relative strengths and weak- nesses of the architecture with respect to dense matrix computations. Consequently, since the primitive's computational complexity is manageable, it is possible to probe at an architecture/software level which is free of spurious software considerations Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php such as ways of tricking a restructuring compiler/code generator combination into producing the code we want. Thus, allowing meaningful conclusions to be reached about the most effective way to use a new machine. 5 Second, it aids in the identi- fication of directions in language and restructuring technologies that would help in the implementation of high-performance scientific computing software. For example, matrix-manipulation constructs are already included in many proprietary extensions to Fortran due to the need for higher-level constructs to achieve high performance on some machines. Third, detailed knowledge of the efficient mapping of primitives to different architectures provides a way of thinking about algorithm design that facili- tates the rapid generation of new versions of an algorithm by the direct manipulation of its algebraic formulation. (See the discussion of triangular system solvers below for a simple example.) Fourth, exposing the weaknesses of an architecture for the execu- tion of basic primitives provides direction for architectural development. Finally, it simplifies the design of numerical software for nonexpert users. This typically occurs through the use of total primitives, i.e., primitives which hide all of the architectural details crucial to performance from the user. Code is designed in terms of a sequential series of calls to primitives which use all of the resources of the machine in the best way to achieve high performance. When such a strategy is possible a certain amount of performance portability is achieved as well. Unfortunately, many important archi- tectures do not lend themselves to total primitives. Even in this case, however, the hiding of parts of the architecture via partial primitives is similarly beneficial. A user need only deal with managing the interaction of the partial primitives which may or may not execute simultaneously.</p><p>In this section, computational primitives from each level of the BLAS hierarchy are discussed and analyses of their efficiency on the architectures of interest in this paper are presented in various degrees of detail. Based on the discussion in 2 which indicates that the investigation of data locality is of great importance for both shared and distributed memory machines, special attention is given to identifying the strengths and weaknesses of each primitive in this regard and its relationship to the amount of exploitable parallelism. 3.2. Architecture/algorithm analysis methodology. The design of efficient computational primitives and algorithms that exploit them requires an understanding of the behavior of the algorithm/primitive performance as a function of certain system parameters (cache size, number of processors, etc.). It is particularly crucial that the analysis of this behavior identifies any contradictory trends that require tradeoff consideration, and the limits of performance improvement possible via a particular technique such as blocking. Additionally, preferences within a set of primitives can be identified by such an analysis, e.g., on certain architectures a rank-1 BLAS2 primitive does not perform as well as a matrix-vector multiplication. Ideally, the analysis should also yield insight into techniques a compiler could use to automatically restructure code to improve performance, e.g., on hierarchical memo.ry systems <ref type="bibr" target="#b64">[63]</ref>, [75]. In this paper we are mostly concerned with analyses that concern the effects of hierarchical (registers, cache or local memory, global metnory) or distributed memory systems.</p><p>As indicated earlier, the consideration of data locality and its relationship to the exploitable parallelism in an algorithm is a key activity in developing high- performance algorithms for both hierarchical shared memory and distributed memory Very loosely speaking this is usually the assembler level, i.e., the level at which the user has direct control over performance-critical Mgorithm/architecture tradeoffs. Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php architectures. In this section, we point out some performance modeling efforts con- cerning these tradeoffs that have appeared in the literature and present a summary of the techniques used on hierarchical shared memory architectures to produce some of the results discussed in later sections.</p><p>Several papers have appeared recently which discuss modeling the influence of a hierarchical memory on numerical algorithms, e.g., <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b77">[76]</ref>, <ref type="bibr" target="#b100">[99]</ref>, <ref type="bibr" target="#b102">[101]</ref>. Earlier work on virtual memory systems also discusses similar issues, e.g., the work of McKellar and</p><p>Coffman <ref type="bibr" target="#b132">[131]</ref>, and Trivedi [185], <ref type="bibr" target="#b188">[186]</ref>. In fact, the work of Trivedi performs many of the analyses for virtual memory systems that were later needed for both BLAS2 and BLAS3 such as the effect of blocking, loop orderings in the LU factorization, and prefetching. The details and assumptions for the hierarchical memory case, however, differ enough to require the further investigation that has taken place. Of particular interest here are studies by the groups at the University of Illinois on shared memory multivector processors (the Cedar Project) <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b67">[66]</ref>, [67], <ref type="bibr" target="#b106">[105]</ref> and at the California Institute of Technology on hypercubes (the Caltech Concurrent Computation Pro- gram) [59]- <ref type="bibr" target="#b62">[61]</ref>. In these studies performance analyses were developed to express the influence of the blocksizes, used in both the matrix multiplication primitives and the block algorithms built from them, on performance in terms of architectural parame- ters.</p><p>Gallivan, Jalby, Meier, and Sameh [67], <ref type="bibr" target="#b106">[105]</ref> proposed the use of a decoupling methodology to analyze in terms of certain architectural parameters the trends in the relationship between the performance and the blocksizes used when implementing BLAS3 primitives and block algorithms on a shared memory multivector processor.</p><p>In particular, they considered an architecture comprising a moderate number (p) of vector processors that share a small fast cache or local memory and a larger slower global memory. (The analysis is easily altered for the private cache or local memory case.) An example of such an architecture is the Alliant FX/8. In their methodology, two time components, whose sum is the total time for the algorithm, are analyzed separately. A region in the parameter space, i.e., the space of possible blocksize choices, that provides near-optimal behavior is produced for each time component. The intersection of these two regions yields a set of blocksizes that should give near- optimal performance for the time function as a whole.</p><p>The first component considered is called the arithmetic time and is denoted Ta.</p><p>This time represents the raw computational speed of the algorithm and is derived by ignoring the hierarchical nature of the memory system: it is the time required by the algorithm given that the cache is infinitely large. The second component of the time function considered is the degradation of the raw computational speed of the algorithm due to the use of a cache of size CS and a slower main memory. This component is called the data loading overhead and is denoted At. The components Ta and At are respectively proportional to the number of arithmetic operations and data transfers, from memory to cache, required by the algorithm; therefore, the total time for the algorithm is T Ta + At naT"a + ntTt,</p><p>where n and nl are the number of operations and data transfers, andand vl are the associated proportionality constants or the "average" times for an operation and data load. Note that no assumptions have been made concerning the overlap (or lack thereof) of computation and the loading of data in order to write T as a sum of these two terms. Tt. This overlap effect can cause Tt to vary from zero, for machines which have a perfect prefetch capability from memory to cache, to tt, where tt is the amount of time it takes to transfer a single data element, for machines which must fetch data on demand sequentially from memory to cache.</p><p>The analysis of Ta considers the performance of the algorithm with respect to the architectural parameters of the multiple vector processors and the register-cache hierarchy under the assumption of an infinite cache. For some machines, the register- cache hierarchy is significant enough to require another application of the decoupling methodology with the added constraint of the shape of the registers. Typically, how- ever, the analysis involves questions similar to those discussed concerning the BLAS2 below.</p><p>Rather than considering At directly, the second portion of the analysis attempts a more modest goal. The data loading overhead can be analyzed so as to produce a region in the parameter space where the relative cost of the data loading At/Ta is small. This analysis is accomplished by expressing Al/Ta in terms of two ratios: a cache-miss ratio and a cost ratio. Specifically, A</p><p>--= Ta where # nt/na is the cache-miss ratio and A Tt/Ta is the cost ratio. For the purposes of qualitative analysis, A can be bounded under various assumptions (average case, worst case, etc.) and trends in the behavior of the primitive or algorithm derived in terms of architectural parameters via the consideration of the behavior of the cache- miss ratio it as a function of the algorithm's blocksizes. The utility of the results of the decoupling form of analysis depends upon the fact that the intersection of the near-optimal regions for each term is not empty or at least that the arithmetic time does not become unacceptably large when using parameter values in the region where small relative costs for data loading are achieved. For some algorithms this is not true; reducing the arithmetic time may directly conflict with reducing the relative cost of data loading. In somecases, a technique known as multilevel blocking can mitigate these conflicts <ref type="bibr" target="#b68">[67]</ref>. In other cases, more machine- specific tradeoff studies must be performed. These studies typically involve probing the interaction of data motion to and from the various levels of memory and the underlying hardware to identify effective tradeoffs <ref type="bibr" target="#b65">[64]</ref>, <ref type="bibr" target="#b66">[65]</ref>.</p><p>On distributed memory machines, analyses in the spirit of the decoupling method- ology can be performed. Fox, Otto, and Hey <ref type="bibr" target="#b60">[59]</ref>, <ref type="bibr" target="#b62">[61]</ref> analyzed the efficiency of the broadcast-multiply-roll matrix multiplication algorithm and other numerical linear al- gebra algorithms on hypercUbes in terms of similar parameters. In particular, they expressed efficiency in terms of the number of matrix elements per node (blocksize), the number of processors and a cost ratio tcomm/tItop which gives the relative cost of communication to computation. Johnsson and Ho <ref type="bibr" target="#b111">[110]</ref> presented a detailed analysis of matrix multiplication on a hypercube with special attention to the complexity of the communication primitives required and the associated data partitioning. 3.3. First and second-level BLAS. The first level of the BLAS comprises vector-vector operations such as dotproducts, c -xTy, and vector triads (SAXPY), y y =t: ax <ref type="bibr" target="#b122">[121]</ref>. This level was used to implement the numerical linear algebra package LINPACK <ref type="bibr" target="#b38">[38]</ref>. These primitives possess a simple one-dimensional parallelism especially suitable for vector processors with sufficient memory bandwidth to tolerate the high ratio of memory references to operations; it for the triad and it 1 Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php for the dotproduct. The superiority of the dotproduct is due to the fact that it is a reduction operation that writes a scalar result after accumulating it in a register.</p><p>The triad, on the other hand, produces a vector result and must therefore write n elements to memory in addition to reading the 2n elements of the operands. For vector processors, performance tuning is limited to adjusting the vector length and stride of access. On multivector processors, both primitives are easily decomposed into several smaller versions of themselves for parallel execution. For the triad, # + 2-, note that the fetch of c becomes more significant, and tt , 1 + for the dotproduct, where n p is the number of processors. As the number of processors increases to a maximum of n, the preference for the dotproduct over the triad is reversed. For p n the triad requires O(1) time with # 2 while the dotproduct requires O(log n) with # 2. Such a reversal often occurs when considering large numbers of processors relative to the dimension of the primitive. The dependences graph of the reduction operation and its properties that produced a small # for a limited number of processors scale very poorly as p increases and translate directly into a relative increase in the amount of memory traffic required on a shared memory architecture and interprocessor communication on a distributed memory machine. (For a distributed memory machine, whether or not the reversal of preference occurs can depend strongly on the initial partitioning of the data.)</p><p>The advent of architectures with more than a few processors and high-performance register-based vector processors with limited processor-memory bandwidth such as the CRAY-1 exposed the limitations of the first level of the BLAS. New implementations of dense numerical linear algebra algorithms were developed which paid particular attention to vector register management and an emphasis on matrix-vector primitives resulted <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b57">[56]</ref>. This problem was later analyzed in a more systematic way in <ref type="bibr" target="#b42">[42]</ref> and resulted in the definition of the extended BLAS or BLAS2 <ref type="bibr" target="#b40">[40]</ref>. Architectures with a more substantial number of processors were also more efficiently used since matrix-vector operations consist essentially of multiple BLAS1 primitives that can be executed in parallel-roughly speaking they possess two-dimensional parallelism.</p><p>The second level of the BLAS includes computations involving O(n2) operations such as a matrix-vector multiplication, y ,-y :l: Ax, and a rank-1 update, A A :t: xyT.</p><p>Note that these primitives subsume the triad and dotproduct BLAS1 primitives and become those primitives in the limit as one of the dimensions of A tends to 1. These primitives improve data locality in the sense that the number of memory references per operation can be reduced by accumulating the results of several vector operations in a vector register before writing to memory as in matrix-vector multiplication or by keeping in registers operands common to successive vector operations as in a rank-1 update. The two techniques, however, do not result in similar improvements in data locality. In general, it is preferable to write algorithms for register-based multivector processors in terms of matrix-vector multiplications rather than rank-1 updates.</p><p>To see this, consider first the efficiency of implementing the two BLAS2 primitives as a set of BLAS1 primitives each of the order of the matrix. (For the rank-1 it is only possible to use the triad; the matrix-vector multiplication allows a choice of primitives.) If the matrix dimensions nl and n2 are larger than the register size 6 of any of the processors there is no possibility of efficient register reuse and the value of # remains at the disappointing BLAS1 level. For problems where either nl or n2 is smaller than the register size, however, it is possible to reuse the registers in such a way that both primitives achieve their theoretical minimum values of #; # 1 + 1/2nl + 1/2n2 for the rank-1 update and # 1/2 + 1/2nl + 1/n2 for the matrix-vector product. For the small rank-l, this local optimal is achieved by reading the small vector into vector register once and reusing it to form a triad with each row or column of the matrix in turn. As a result, each element of the matrix and the two vectors are loaded into the processor exactly once and the elements of the matrix are written exactly once the optimal data transfer behavior for a rank-1 update.</p><p>For the matrix-vector product, the technique depends upon whether n or n2 is the small dimension. If it is n2 then a technique similar to the rank-1 update is used.</p><p>The vector x is loaded into a register once. Each row of A is read in turn and used in an inner product calculation with x in the register, and the result is then added to the appropriate element of y and written back to memory. Every data element is read and written the minimum number of times. If the small dimension is nl then a slightly different technique is used. The result of the operation, y, is accumulated in a vector register, thereby suppressing the writes back to memory of partial sums.</p><p>As long as n or n2 do not get very small, which implies that the primitives are degenerating into a first level primitive, the values are an improvement compared to their limiting first level primitives. Of course, the rank-1 update still has a value of # similar to the dotproduct BLAS1 primitive, but it has the advantage of more exploitable parallelism. If these results could be maintained for arbitrary n and n2, the superiority of the BLAS2 on register-based multivector processors would be established.</p><p>To show that this is indeed possible, we will exploit the richness of structure present in linear algebra computations and partition the primitives into smaller ver- sions of themselves. This is accomplished by partitioning A into klk2 submatrices Aij E ml xm2, where it is assumed for simplicity that n kim with ki and m integers, and partitioning x and y conformally. The blocksizes which determine the partitioning are chosen so that the smaller instances of the primitives are locally optimal with respect to their values of #.</p><p>The rank-1 update is thus reduced to kk2 independent small rank-1 updates.</p><p>The resulting global # value for the entire rank-1 update is # 1 + 1/2ml + 1/2m2. Now consider its behavior as p, the number of register-based vector processors used, increases. For small and moderate p, one of the blocksizes, say m, could be taken equal to the corresponding dimension of the matrix, n (the choice of m or m2 simply depends upon the shape of the matrix and the exact number of processors). It follows that # 1 + 1/2r + 1/2n where r is the register length. As p increases further, a true two-dimensional partitioning must be used. So we set p kk2 which balances the computational load and the amount of data required by each processor. Since the register size determines the largest vector object we can work with and extra transfers to and from registers translate directly into additional time, we make m -1 + m-I as small as possible under the constraint that either ml &lt;_ r or m2 _&lt; r, depending on the implementation chosen for the register-based smaller rank-1 update. Consequently, P # 1 + x--------(ml -'}-m2) znln2 and the algorithm requires O(mm2) time. At the limit of available parallelism, p n ln and the rank-1 update requires O(1) time with # 2. This is the same as the best BLAS1 primitive. This is not surprising since in the limit each processor is doing essentially the same scalar computation as the BLAS1 is that in the BLAS2 case there is much more exploitable parallelism. Note also that at some point while increasing the number of processors the vector length used by each processor will fall below the breakeven point for the use of the vector capability of the processor, and the switch should be made to scalar mode.</p><p>A similar decomposition technique can be used for the matrix-vector product primitive y y + Ax. The matrix is partitioned into submatrices Aij E ]ml m2 and partitioning x and y conformally. The resulting algorithm is</p><formula xml:id="formula_1">do i-1, kl Yi Yi + AilXl +''" + AikXk2 end do</formula><p>All of the basic computations z z + Aijxj can proceed in parallel with a fan-in dependence graph required on the update of the yi if k2 &gt; 1. As before, for a small to moderate number of processors one of the mi can be set to the register length and the other to the remaining dimension of A. If 1 then no synchronization is required since k2 n2 and the loop can execute in parallel. The resulting global # is</p><formula xml:id="formula_2">1 1 1</formula><p>where r is the vector length. If i-2</p><p>In the latter case, kl is equal to 1 and synchronization is required. However, since the number of processors is assumed small the partial sums from local matrix-vector products can be accumulated in a vector of length n private to each processor (not necessarily a register). After all processors are finished accumulating their partial sums, a simple fan-in of the results can be done. The time required is O(rnrn2).</p><p>Note that on a moderate number of processors the matrix-vector primitive is twice as efficient as the rank-1 primitive of the same size. Consequently, when implement- ing algorithms with BLAS2 primitives on a register-based multivector architecture with a moderate number of processors, a matrix-vector product-based algorithm will significantly outperform the same algorithm based on a rank-1 update.</p><p>As with the rank-1 update it is possible to derive an estimate of the time and the value of # for the case where a two-dimensional partitioning is used with p kk2.</p><p>In this case, not only must the transfers be computed for the small matrix-vector products performed by each of the processors, but also the transfers associated with the k independent fan-in trees which sum together the partial sums into the final values of y for 1 _&lt; &lt;_ kl. The time required is O(mrn2) + O(ml log2 k) with</p><formula xml:id="formula_3">#= + l+m2 12 1</formula><p>As with all of the other primitives, when p is as large as possible, in this case p n ln2, the value of # increases to approximately 2. Due to the reduction nature of the matrix- vector product, its time has a lower bound of O(log n2).</p><p>The results above demonstrate several important points about first-and second- level BLAS primitives. The most important is that for register-based multivector Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php processors with a moderate number of processors, there can be a significant difference between the performance of a given algorithm when implemented in terms of the four primitives discussed above. This performance order is given from worse to best in terms of decreasing values of tt. The triad with #does far too many spurious data transfers to be of use on a processor with a single port to memory. The dotproduct improves the ratio to # 1 but not all processors have high-performance capabilities.</p><p>The BLAS2 rank-1 update primitive also has # , 1 but it does not depend upon efficient reduction operations on vector registers being available on a processor and its extra dimension of parallelism makes it more flexible than the previous primitives.</p><p>By far, however, the preferred primitive for such an architecture is the matrix-vector product due to its superior register management.</p><p>The second observation from the results above is how the preferences can reverse when the architecture used is radically altered. In this case we considered increasing the number of register-based vector processors available to the maximum needed.</p><p>It was shown that in the limit all have similar register-memory transfer behavior and the nonreduction operations have a distinct advantage if it is assumed that the data and computations have been partitioned ideally. This last point is crucial. Our discussions implicitly assumed a shared memory architecture when increasing the number of processors. While the results do hold for certain distributed memory architectures, they can be very sensitive to the assumptions concerning initial data partioning. If for some reason the data had been partitioned in a different way the trends need not be the same.</p><p>3.4. Third-level BLAS.</p><p>3.4.1. Motivation. The highest level of the BLAS is motivated by the use of memory hierarchies. On such systems, only the lowest level of the hierarchy (or in some cases the two lowest, e.g., registers and cache) are able to supply data at the computational bandwidth of the processors. Hence, data locality must be exploited to allow computations to involve mostly data located in the lowest levels. This allows the cost of the data transfer between levels to be amortized over several operations performed at the computational bandwidth of the processors. This problem of data reuse in the design of algorithms has been studied since the beginning of scientific computing. Early machines, which had small physical memories, required the use of secondary storage such as tape or disk to hold all of the data for a problem. Similar considerations were also needed on later machines with paged virtual memory systems. The block algorithms developed for such architectures relied on transferring large submatrices between different levels of storage, with prepaging in some cases, and localizing operations to achieve acceptable performance.</p><p>Of course, the resulting matrix-matrix primitives could have been used in algorithms for the machines which motivated the BLAS2. Indeed, as Calahan points out <ref type="bibr" target="#b23">[23]</ref>, the use of matrix-matrix modules was considered when developing algorithms for the CRAY-1. The hierarchy, however, was not distinct enough to achieve a significant advantage over BLAS2 primitives. The introduction of the CRAY X-MP and its ad- ditional memory ports delayed even further the move to the next level of the BLAS. It was finally caused by the availability of high-performance architectures which rely on the use of a hierarchical memory system and with more profound performance consequences when not used correctly. Agarwal and Gustavson designed matrix mul- tiplication primitives and block algorithms for solving linear systems to exploit the cache memory on the IBM 3090 in the iatter part of 1984. These evolved into the algorithms contained in ESSL, first released in the middle of 1985, for the IBM 3090 Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php with vector processing capabilities <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b85">[84]</ref>, <ref type="bibr" target="#b131">[130]</ref>, and more recently for the multi- processor version of the architecture <ref type="bibr" target="#b1">[2]</ref>. A numerical linear algebra library based on block methods was developed and its performance analyzed in terms of architectural parameters in 1985 and early 1986 for a single cluster of the Cedar machine, the multivector processor Alliant FX/8 <ref type="bibr" target="#b8">[9]</ref>, [105], [156]. At approximately the same time, Calahan developed block LU factorization algorithms for one CPU of the CRAY-2 <ref type="bibr" target="#b23">[23]</ref>. In 1985, Bischof and Van Loan developed the use of block Householder reflectors in computing the QR factorization and presented results on an FPS-164/MAX [16].</p><p>The development of these routines and numerical linear algebra libraries clearly demonstrated that a third level of primitives, or BLAS3, based on matrix-matrix com- putations was required to achieve high performance on the emerging architectures.</p><p>Such primitives achieve a significant improvement in data locality, i.e., the data local- ity is no longer effectively independent of problem size as it is for the first two levels of the BLAS. Third-level primitives perform O(n3) operations on O(n2) data, and they increase the parallelism available by yet another dimension by essentially consisting of multiple independent BLAS2 primitives.</p><p>Since the reawakening of interest in block methods for linear algebra, many pa- pers have appeared in the literature considering the topic on various machines, e.g., <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b45">[44]</ref>, <ref type="bibr" target="#b150">[149]</ref>. The techniques have become so accepted that some manufacturers now provide high-performance libraries which contain block methods and matrix-matrix primitives. Some, such as Alliant, provide matrix multiplication intrinsics i:. their concurrent/vector processing extensions to Fortran. In 1987, an effort began to stan- dardize for Fortran 77 the BLAS3 primitives and block methods for numerical linear algebra <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b39">[39]</ref>. 3.4.2. Some algorithms. The most basic BLAS3 primitive is a simple matrix operation of the form (3) C C + AB, where C, A, and B are nl n3, nl n2, and n2 n3 matrices, respectively. Clearly, this primitive subsumes the rank-1 update, (he 1), and matrix-vector multiplication, (n3 1), BLAS2 primitives. In block algorithms, it is most often used as a rank-w update (n2 w &lt;&lt; n, n3) or a matrix multiplied by several vectors (n3 w &lt; nl, n2).</p><p>The analysis of the parallel complexity of such a computation has been the subject of much study. In this section we give a brief summary of some generic algorithms and mention some implementations on various machines that have appeared recently in the literature.</p><p>The basic scalar computation can be expressed as do r ln3 do s 1, n do t 1, n Cs,r Cs,r n u as,tbt,r end do end do end do where cs,r, as,t, and bt,r denote the elements of C, A respectively B.</p><p>There are three basic generic approaches to performing these computations which correspond to different choices of orderings of the loops. They are called the inner, Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php middle, and outer product methods due to the fundamental kernels used and corre- spond to the following code segments: inner_product" dor= 1, n3 do s 1, nl C'8r end do end do Cs,r + inner_prod(as,,, b,,r) middle_product:</p><p>dor= 1,n3 end do c,,r + Ab,,r and outer_product:</p><formula xml:id="formula_4">do t 1, rt2 C C + a,,<label>tbTt,, end do.</label></formula><p>Each has its advantages and disadvantages for various problem shapes and architec- tures. All have immediate generalizations involving submatrices. These issues are discussed in the literature, e.g., <ref type="bibr" target="#b101">[100]</ref>, <ref type="bibr" target="#b138">[137]</ref>, in several places and will not be repeated here. We do note, however, that for register-based vector and multivector processors with one port to memory, the middle product algorithm facilitates the efficient use of the vector registers and data bandwidth to cache of each processor, and exploits the chaining of the multiplier, adder, and data fetch available on many systems. This is accomplished by performing, possibly in parallel, multiple matrix-vector products the preferred BLAS2 primitive for vector register management. When the vector pro- cessors are such that register-register operations are significantly faster than chained operations from local memory or cache, a more sophisticated two-level generalization of the blocking strategy discussed below can be used to achieve high performance. Madsen, Rodrigue, and Karush considered, for use on the CDC STAR-100 vector processor, a slightly more exotic matrix multiplication based on storing and manip- ulating the diagonals of matrices <ref type="bibr" target="#b128">[127]</ref>. Their motivation was mitigating the perfor- mance degradation of the algorithms above for banded matrices and the difficulties in accessing the transpose of a matrix on some machines.</p><p>The BLAS3 primitive implemented for a single cluster of the Cedar machine [66], [67], [105] and applicable to machines with a moderate number of reasonably coupled multivector processors with a shared cache implements a block version of the basic matrix multiplication loops. It proceeds by partitioning the matrices C, A, and B into submatrices Cj, A, and Bkj whose dimensions are rnl x m3, m x m2, and rn: x m3, respectively. The basic loop is of the form do 1, kl do k 1, k2 do j 1, k3 where nl klml, rt2 k2m2, and n3 knrn3, and kl, k2, and k3 are assumed to be positive integers for simplicity.</p><p>The block operations Cij Cij + Ak * Bkj possess a large amount of concur- rent and vectorizable computations, so the algorithm proceeds by dedicating the full resources of the p vector processors to each of the block operations in turn. The kernel block multiplication can be computed by any of the basic concurrent/vector algorithms. As noted above the middle product algorithm whi(/h performs several multiplications of Aik and columns of Bj in parallel is well suited for register-based architectures like the Alliant FX/8, hence it is assumed in the analysis below.</p><p>There are, of course, several possible orderings of the block loops and several other kernels that can be used for the block operations. 7 If, for example, the processors are not tightly coupled enough parallelism can moved to the block level. This can also be useful in the case of private caches or local memories for each processor. As is shown below this particular ordering (or one trivially related to it) is appropriate for use in the block algorithms discussed in later sections. However, when developing a robust BLAS3 library, kernels for the block operations which differ from those discussed below and alternate orderings must be analyzed so that selection of the appropriate form of the routine can be done at runtime based on the shape of the problem. This is especially important for cases with extreme shapes, e.g., guaranteeing smooth performance characteristics as the shapes become BLAS2-1ike.</p><p>Clearly, if the number of processors are increased to p nlnn3 the inner product form of the algorithm can generate the result in O(log. n) time. For a shared memory machine, such an approach would place tremendous strain on a highly interleaved or parallel memory systems. As mentioned earlier, one way that such strain is mitigated is by assigning elements of structured variables to the memory banks in such a way as to minimize the chance of conflicts when accessing certain subsections of the data.</p><p>For the inner product algorithm it is particularly important that the row and columns of matrices be accessible in a conflict free manner. One of the easiest memory module mapping strategies that achieves this goal dates back to the ILLIAC IV ( <ref type="bibr" target="#b115">[114]</ref>, <ref type="bibr" target="#b116">[115]</ref>, also see <ref type="bibr" target="#b117">[116]</ref>). The technique is called the skewed storage scheme. In it the elements of each row of a matrix are assigned in an interleaved fashion across the memory modules. However, when assigning the first element of a row it is placed in the memory module that is skewed by one from the module that contained the first element of the previous row. Any row or column of a matrix can now be accessed in a conflict free fashion. Matrix multiplication algorithms for the distributed memory ILLIAC IV were developed based on this scheme which can be easily adapted to the shared memory situation. If we are willing to sacrifice some numerical stability, fast schemes which use less than O(n3) operations can be used to multiply two matrices. In <ref type="bibr" target="#b96">[95]</ref>, Higham has analyzed this loss of stability for Strassen's method <ref type="bibr" target="#b177">[175]</ref> and concluded that it does not preclude the effective use of the method as a BLAS3 kernel. Recently, Bailey has   The j k ordering of the block loops, for example, produces distinctly different blocksizes and shapes <ref type="bibr" target="#b106">[105]</ref>. Its use can be motivated by the desire to keep a block of C in cache while accumulating its final value. This iInplies that a block of A must reside in the cache simultaneously thereby altering the optimal shapes. considered the use of Strassen's method to multiply matrices on the CRAY-2 <ref type="bibr" target="#b6">[7]</ref>. The increased performance compared to CRAY's MXM library routine is achieved via the reduced operation count implicit in the method and the careful use of local memory via an algorithm due to Calahan. Speedups as high as 2.01 are reported compared to CRAY's library routine on a single CPU. Bailey also notes that the algorithm is very amenable to use on multiple CPU's of the CRAY-2 although no such results are presented.</p><p>The broadcast-multiply-roll algorithm for matrix multiplication described and an- alyzed by Fox et al. is representative of distributed memory algorithms <ref type="bibr" target="#b60">[59]</ref>- <ref type="bibr" target="#b62">[61]</ref>.</p><p>(For other distributed memory algorithms see <ref type="bibr" target="#b79">[78]</ref>, <ref type="bibr" target="#b130">[129]</ref>, [135].) Consider the calcu- lation of C C + AB where A, B, C E nn. Assume the processors are connected as a two-dimensional wrap-around mesh and the square subblock with index (i, j) of each matrix starts out in the memory of the processor correspondingly indexed. The algorithm consists of v/ steps each of which consists of broadcast, multiply, and roll phases. In particular, on step (i 0,..., v-1) the processor in each row owning Aj,(j+i)modv/-d broadcasts it to the rest of the processors in the row which store it in a local work array T. Each processor then multiplies T by the subblock of B presently in its memory and adds it to the subblock of C that it owns. The final phase of each step consists of rolling the matrix B up one row in the mesh with appropriate wrap-around at the ends of the mesh. In other words, each processor transmits the subblock of B it has in its memory to the processor in the same column of the mesh but one row up. The repetition of this three-phase step v/ times corresponds to the number of steps required to let each subblock of B return to its original processor. Finally, Johnsson and Ho' have considered the implementation of matrix multi- plication on a hypercube <ref type="bibr" target="#b111">[110]</ref>. In this work they consider the implementation of the computational primitive in terms of communication primitives some of which implic- itly perform computations as the data move through the cube. As a result, users can write their algorithms as a sequence of calls to these data motion primitives in a fashion similar to the method advocated with respect to the computational primitives discussed above.</p><p>3.4.3. Blocksize analysis. In this section we summarize the application of the decoupling methodology to the matrix multiplication algorithm for the single cluster of the Cedar machine described above. Recall that the block level loops were do 1, kl do k 1, k2 do j 1, k3</p><p>Vii Vii + Aik * Bkj end do end do end do where nl klml, rt2 k2m2, and n3 k3m3, and kl, k2, and k3 are assumed to be positive integers. Each block operation Cij Cij + Aik * Bkj uses the resources of the p vector processors by performing matrix-vector products in parallel.</p><p>Values of m l, m2, and m3 which yield near-optimal values of the arithmetic time for the kernel can be determined by an analysis similar to those presented above for the BLAS2. The essential tradeoffs require balancing the parallel and vector processing capabilities and the bandwidth restrictions due to the single port to memory on each processor. For the Alliant FX/8, the values of m, m2, and rn3 chosen according to the preceding reasoning are: m 32k or is large; m2 &gt;_ 16 to 32 depending on the Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php overhead surrounding the accumulation; and m3 8k or is large.</p><p>The reduction of the data loading overhead reduces to a simple constrained min- imization problem. Since the submatrices Aik are associated with the inner loop, it is assumed that each Aik is loaded once and kept in cache for the duration of the j loop. Similarly, it is assumed that each of the Cij and Bkj are loaded into cache repeatedly. Note that the conservative approach is taken in that no distinction is made between reads and writes in that , is set under the pessimistic assumption that anything loaded has to be written back whether or not it was updated. Some cases where this distinction becomes important are discussed below.</p><p>It is easily seen by considering the number of transfers required that the cache- miss ratio, it, is given by</p><formula xml:id="formula_5">1 1 1 (4) 2ml 2n3</formula><p>The theoretical minimum, given an infinite cache, is</p><formula xml:id="formula_6">1 1 1 --+ + 2nl 2n3</formula><p>Constraints for the optimization of the terms involving ml and m2 are generated by determining what amount of data must fit into cache at any given time and requir- ing that this quantity be bounded by the cache size CS. The final set of constraints come from the fact that the submatrices cannot be larger than the matrices being multiplied. Therefore, the minimization of the number of loads performed by the BLAS3 primitive is equivalent to the solution of the minimization problem <ref type="bibr" target="#b4">(5)</ref> min fl(ml,rrt2) m -1 A-m -1 subject to m2(ml + p) &lt;_ CS</p><formula xml:id="formula_7">1 &lt; rni &lt; n 1 &lt; m2 &lt; n2,</formula><p>where CS is the cache size and p is the number of processors. The constraints trace a rectangle and an hyperbola in the (ml, rn2)-plane.</p><p>The solution to the minimization problem separates the (nl,n.) plane into four distinct regions; two of which are of interest for the rank-w update and matrix-times-w- vectors primitives, and general large dense matrix multiplication (see [67] for details).</p><p>These can be summarized as: 1. The value of m3 is arbitrary and taken to be n3. Note that since the near-optimal region for the arithmetic time component was unbounded in the positive direction, there is a nontrivial intersection between it and the near-optimal region for the data loading component. This implies that, except for some boundary cases where nl, n2, and/or n3 become srnall, the decoupling method- ology does yield a strategy which can be used to choose near-optimal blocksizes for BLAS3 primitives. (The troublesome boundary cases can be handled by altering the block-loop ordering or choosing a different form of the block multiplication kernel.)</p><p>For the rank-w primitive this results in a partitioning of the form</p><formula xml:id="formula_8">(6) + B, Ck Ck Ak</formula><p>where the blocksizes are given by the case above with rt2 w and small. Note that the block loops simplify to do/= 1, k Ci Ci + A * B end do and parallelism at the block-loop level becomes trivially exploitable when necessary.</p><p>Also note that each blockof the matrix C is read and written exactly once implying that this blocking maintains the minimum number of writes back to main memory.</p><p>For large dense matrix multiplication and for the matrix-times-w-vectors primitive the partitioning is Once again block parallelism is obviously exploitable when needed. Note however that the blocks of C are written to several times. In general, these writes are not signifi- cant since the blocksizes have been chosen to reduce the significance of all transfers (including these writes) to a negligible level. The i-j-k block loop ordering can be used and analyzed in a similar fashion if it is desirable to accumulate a block of C in local memory. The blocksizes that result are, of course, different from the one shown above (see <ref type="bibr" target="#b106">[105]</ref>).</p><p>The key observation with respect to the behavior of # for BLAS3 primitives is that it decreases hyperbolically as a function of rnl and rn.. (This assumes this particular block loop ordering but similar statements can be made about the others.) + + Therefore, assuming that na is much larger than (large dense matrix multi- plication), data loading overhead can be reduced to O(1/x/-C-). This limit on the cache-miss ratio reduction due to blocking is consistent with the bound derived in Hong and Kung <ref type="bibr" target="#b102">[101]</ref>. For BLAS3 primitives where one of the dimensions is smaller than the others, with value denoted w, the data loading overhead is a satisfactory O(1/w).</p><p>The hyperbolic nature of the data loading overhead implies that reasonable per- formance can be achieved without increasing the blocksizes to the near-optimal values given above. Of course, exactly how large m and rn must be in order to reduce the data loading overhead to an acceptable amount depends on the cost ratio of the machine under consideration. The existence of a lower bound on the cache-miss ratio achievable by blocking does, however, have implications with respect to the blocksizes used in block versions of linear algebra algorithms.</p><p>The expression for the data loading overhead based on (2) and ( <ref type="formula">4</ref>) is also of the correct form for matrix multiplication primitives blocked for register usage in that hyperbolic behavior is also seen. The actual optimization process must be altered. The use of registers imposes shape constraints on blocksize choices and it is often more convenient not to decouple the two components of time. For the most part, however, the conclusions stated here still hold.</p><p>For hypercubes, the analysis of Fox, Otto, and Hey [61] derives a result in the same spirit as <ref type="bibr" target="#b7">(8)</ref>. They show that the efficiency (speedup divided by the number of processors) of the broadcast-multiply-roll matrix multiplication algorithms is 1 1 -(c/vf)tom/tfo where tcomm, tIlop, and n are the cost for communication of data, cost of a floating point operation, and the number of matrix elements stored locally in each proces- sor (hence bounded by the local memory size). The constant c is 1 for the square subblock decomposition but is v/-fi/2 for the row decomposition, where p is the num- ber of processors, indicating the superiority of square blocks for this type of matrix multiplication algorithm. 3.4.4. Preferred BLAS3 primitives. The preceding analysis also allows the issue of superiority of one BLAS3 primitive compared to the others to be addressed. Consider the comparison of the rank-primitive to the primitive which multiplies a matrix by w vectors. If w 1 this is the BLAS2 comparison discussed earlier and for the shared memory multivector processor analyzed above the matrix-vector multipli- cation primitive should be superior. On the other hand, if w n, the two primitives are identical and no preference should be predicted by the analysis. Hence, the anal- ysis should result in a preference which is parameterized by w with end conditions consistent with these two observations.</p><p>To make such a comparison we will restrict ourselves to the multivector shared hierarchical memory case considered above and to four partitionings of the primitives which exploit the knowledge that w is small compared to the other dimensions of the Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php matrices involved (denoted h and below). Such a strategy was proposed in <ref type="bibr" target="#b106">[105]</ref> and has been demonstrated effective on the Alliant FX/8. We will also distinguish between elements which are only read from memory into cache and those which require reading and writing. This allows us to be more precise than the conservative bounding of the cost of data transfer presented above. Also note that this affects the value of the cost ratio A in that it need not be as large as required above.</p><p>The partitioning of the rank-w update used is of the form given above in <ref type="bibr" target="#b5">(6)</ref> but the values of the blocksizes are altered to reflect the more accurate analysis obtained by differentiating between reads and writes. (The qualitative conclusions of the previous analysis do not change.) Three different partitionings for the primitive which multiplies a matrix by w vectors are analyzed. Each is appropriate under various assumptions about the architecture and shape of the problem.</p><p>It is assumed that the primitives make use of code to perform the basic block operations which has been optimized for register-cache transfer and is able to maintain efficient use of the lowest levels of the hierarchy as the shape of the problem changes, i.e., the arithmetic time Ta has been parameterized according to w and the code adjusted accordingly. In this case, the source of differences in the performance of the two primitives is the amount of data transfer required between cache and main memory which is given by the ratio #. Below we derive and compare the value of # for each of the four implementations of the primitives.</p><p>The rank-w update computes C C =k AB where C E hx, A E hx, and B Nxt. The partitioning used is shown in <ref type="bibr" target="#b5">(6)</ref> where Ci Nmxt, Ai Nmxw, km h, and m is the blocksize which must be determined. Note that we have used the knowledge of the analysis above to fix two blocksizes at w and 1. The computations requires 2hlw operations and the block loops are of the form doi-1, k Ci Ci + Ai B end do.</p><p>The primitive requires hi / hw / klw loads from memory and hl writes back to memory. This partitioning/primitive combination is denoted Form-1.</p><p>The second primitive also computes C -C =k AB. In this case, however, C Nhw, A E Nh, and B .A s noted above, three partitionings are considered.</p><p>The first two are of the form shown in <ref type="bibr" target="#b6">(7)</ref>. Both have the block loop form doi--1, k do j= 1, m C C + Aj Bj end do end do.</p><p>They differ in the constraints placed on the blocksizes.</p><p>The first version, denoted Form-2, results from applying the analysis of the pre- vious section to the i-k-j loop ordering of the original triply nested loop form of the matrix multiplication primitive. One of the blocksizes is fixed at w. Specifically, the partitioning is such that A m1,2, klml h, k2m2 l, and C and B are dimensioned conformally. The blocksizes m and m2 are determined under the sim- plified constraint of mm2 &lt;_ CS. Form-2 requires hl / hlw(m / m ) loads and Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php k2hw writes to memory.</p><p>The second version, denoted Form-3, results from analyzing the i-j-k loop ordering of the original triply nested loop form of the matrix multiplication primitive as in <ref type="bibr" target="#b106">[105]</ref>. As before, one of the blocksizes is fixed at w. The partitioning is such that Ai E mlm2, klml h, k2m2 l, and Ci and B are dimensioned conformally. The blocksizes ml and m2 are determined under the constraint of m (m2 + w) &lt;_ CS. This constraint is generated by requiring the accumulation in cache of a block C which implies that a C and the Aj contributing to the product must fit in cache simultaneously. In <ref type="bibr" target="#b106">[105]</ref> it is shown that this partitioning sets m to the value T where T is determined via the analysis of register-cache transfer cost. This simplifies the minimization problem and leaves only m to be determined. Form-3 requires hl / hw + hlwm loads hw writes to memory. Additionally, it requires (k2 1)hw writes to cache due to the local accumulation of Ci.  The third version, denoted Form-4, applies the i-k-j ordering to the transpose of the matrix multiplication to determine blocksizes. This form is valuable for certain architecture/shape combinations. The resulting partitioning is of the form (9) C C +/-A1 where A hm, B mw and km = 1. The constraint mw &lt;_ CS is applied.</p><p>Form-4 requires hl + lw + 2khw loads and khw writes to memory. The block loops simplify to Performance of square matrix multiplication on an Alliant FX/8.</p><p>Table <ref type="table">1</ref> lists the results of analyzing each of the four forms presented above. The generic form of # is given in terms of the dimensions of the problem and the blocksizes used as well as its optimal value. Since the results of the analysis of the primitives given above and the analysis of the block methods which use them indicate that w vr-# represents a limit point on performance improvement the optimal # evaluated there is also given. Finally, the value of the blocksizes which give the optimal data loading cost are also listed. The values show clearly the well-known inferiority of the rank-w by a factor of 2 when w is near 1, i.e., in the near-BLAS2 regime. However, as w increases, the fact that one is up to a factor of two more than the other (though this multiple rapidly reduces as well) quickly becomes irrelevant since the relative cost of data transfer to computational work has become an insignificant performance consideration. As a result, given these partitionings and an architecture satisfying the assumptions of the analysis, we would not expect significant performance differences between the two primitives when w and the size of the matrices are large enough.</p><p>Such observations have been verified on an Alliant FX/8. Consequently, one would not expect the performance of the block algorithms that use the two BLAS3 primitives, Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php e.g., a block LU algorithm, to be significantly different for sufficiently large problems s.</p><p>It would also be expected that the trend in preference for non-reduction types of computations as the number of processors or the cost of processor synchronization increases seen with BLAS2 primitives carry over to the BLAS3. 3.4.5. Experimental results. The performance benefits of using BLAS3 primi- tives and carefully selecting blocksizes in their implementation has been demonstrated in the literature. In this section, we report briefly on experimental results on the A1- liant FX/8. The experiments were performed executing the particular kernel many times and averaging to 'arrive at an estimate of the time spent in a single instance of the kernel. This technique was used to minimize the experimental error present on the Alliant when measuring a piece of code of short duration. As a consequence of this technique, the curves have two distinct parts. The first is characterized by a peak of high performance. This is the region where the kernel operates on a problem which fits in cache. The performance rate in this region gives some idea of the arithmetic component of the time function. It is interesting to compare this peak to the rest of the curve which corresponds to the kernel operating on a problem whose data is initially in main memory. When the asymptotic performance in the second region is close to the peak in cache the number of loads is being managed effectively.</p><p>Figure <ref type="figure">3</ref> illustrates the effect of blocksize on the performance of the BLAS3 prim- itive C C-AB where all three matrices are square and of order n. The blocksizes used for each curve are from low to high performance m 32, m2 32, and m3 32; ml 64, m2 64, and m3 64; and m 128, m2 96, and m3 n. It is clear from the asymptotic performance of the top curve that a significant portion of peak performance can be achieved by choosing the correct blocksizes. In this case an asymptotic rate of just below 52 Mflops is achieved on a machine with a peak rate, including vector startup, of 68 Mflops. Figures <ref type="figure">4</ref> and<ref type="figure" target="#fig_4">5</ref> show the performance of various rank-k updates. The parameters m2 and m3 are taken as k and n as recommended by the analysis of the BLAS3 primitive. The parameter m is taken to be 96 and 128 in the two figures, respectively. This parameter is kept constant for each figure to allow a fair comparison between the performances of the various kernels. Further, the BLAS3 analysis recommends m (CS/k) p. In fact, for the values of k considered here, if m _&gt; 96 then the term in the expression for the number of loads for the rank-k kernel which involves m is not significant compared to the term involving m2. These curves clearly show that increasing k yields increased performance and a significant portion of the effective peak computational rate is achievable. Also note that as k increases the difference in performance of two successive rank-k kernels diminishes. Indeed, the k 96 curve was not included in Fig. <ref type="figure">4</ref> since it delivers performance virtually identical to the k 64 kernel.</p><p>It is instructive to compare the performance of the rank-k kernel to typical BLAS and BLAS2 kernels. The BLAS kernels .-xTy and y y + cx achieve 11 Mflops and 7 Mflops, respectively, with their arguments in main memory. The BLAS2 matrix- vector product kernel achieves 18 to 20 Mflops.</p><p>3.5. Triangular system solvers. Solving triangular systems of linear equa- tions, whether dense or sparse, is encountered in numerous applications. Even though the solution process consumes substantially less time than the associated factoriza- s As is discussed later, when the ratio of the blocksize to the problem, w/n, is small other tradeoffs must be considered in the performance of block algorithms. tion stage, we often wish to solve these triangular systems repeatedly with different right-hand sides but with the same triangular matrix. Hence, it is vital to solve them as efficiently as possible on the architecture at hand.</p><p>There are two classical sequential algorithms for solving a lower triangular system Lx f, where L [AO], f [],</p><p>x [] and i,j 1, 2,..., n. They differ in the fact that one is oriented towards rows, and the other columns. These algorithms are:  As is shown below, these two algorithms are the basis for many adaptations suitable for various vector and parallel architectures.</p><p>3.5.1. Shared-memory triangular system solvers. The inner loops of the row-and column-oriented versions vectorize trivially to yield algorithms based re- spectively on the BLAS operations of SAXPY and DOTPRODUCT. We refer to these algorithms as the row-sweep or forward-sweep, and the column-sweep <ref type="bibr" target="#b117">[116]</ref>.</p><p>Each step of the row-sweep algorithm requires less data motion than the corre- sponding step in the column-sweep algorithm; the DOTPRODUCT primitive reads two vectors and produces a scalar while the SAXPY reads two vectors and writes a third back to memory. If the vector processor has adequate bandwidth then, theoretically Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php at least, this should not be an important distinction. In practice, however, the reduced data traffic of the DOTPRODUCT may be preferable. (This assumes, of course, that the implementation of the DOTPRODUCT is not particularly expensive. 9) The row-sweep algorithm can suffer from the fact that it accesses rows of the matrix. This can be remedied by storing the transpose of the lower triangular matrix, although in some cases this may not be an option, e.g., when the data placement has been determined by some other portion of the algorithm of which the triangular solve is a component.</p><p>For register-based vector processors with limited bandwidth to memory such as the CRAY-1 or a single processor of the Alliant FX/8 each of which has a single port to memory, the performance degradation due to excessive register transfers of the vector algorithms described above can be severe. Block forms of the algorithms must be considered. Let L () L, f(0) f, and let each of L(J), x(J), and f(J) be of order (n j), j 0,. , n 1 where with L ), x j), and I ) being each of order u (we assume that u divides n). Note that this blocking allows the registers to be used efficiently. The matrix-vector product which updates the right-hand side vector is blocked in the fashion described above to allow the accumulation in a vector register of the result of p vector operations before writing the register to memory rather than the one write per two reads of the triads in the nonblocked column-sweep. Similarly, the column-sweep algorithm can accumulate the solution to the triangular system L)xJ)= fJ)in vector registers resulting in a data flow between registers and memory identical to that of a block of the matrix-vector product with the exception that the vector length reduces by one for each of the operations.</p><p>A block row-sweep algorithm can also be derived which reduces the amount of register-memory traffic even further. Using the notation above, partition L so that each block row is of the form [Ci, <ref type="bibr">Li,O]</ref> where Ci E x(i-l) and Li E x. Let x (xT, ...,xTp)T,x (j) (xT, ,X) T, and f (fT,..., fTp)T, where xi, fi</p><p>The block algorithm is: B_Row_Sweep 9 On some machines this is not necessarily a good assumption. The Alliant FX/8 has a con- siderable increase in the startup cost of the dotproduct compared to that of the triad instruction.</p><p>Similarly, CRAY machines implement the dotproduct in a two-stage process. The first accumulates 64 partial sums in a vector register and the second reduces these sums to a scalar. The first phase has the memory reference pattern mentioned above but the second is memory intensive and its cost can be significant for smaller vectors. This algorithm requires only one or two vector writes per block row computation depending upon whether or not the result of the matrix-vector product is left in registers for the triangular-solve primitive to use. This algorithm is characterized by the use of short and wide matrix-vector operations rather than the tall and narrow shapes of the block column-sweep. It is, of course, quite straightforward to combine the two approaches to use a more consistent shape throughout the algorithm. Another triangular solver, which is also suited for both shared and distributed memory multiprocessors, is that based on the DO-ACROSS notion. For example, in the above sequential form of the column-oriented algorithm, the main point of a DO- ACROSS is that computing each (j need not wait for the completion of the whole inner iteration j + 1,..., n. In fact, one processor may compute j soon after another processor has computed Cj := Cj )j,j-lj-1. To minimize the synchro- nization overhead in a DO-ACROSS and efficiently use registers or local memory, the computation is performed by blocks. For example, if L [Lpq], x {Xp}, f {fp}, and p, q 1,..., 4, where each block is of order n/4, then the DO-ACROSS on two processors may be illustrated as shown in Fig. <ref type="figure">6</ref>. Vectorization can be exploited in each of the calculations shown if each processor has vector capabilities. The particular parallel schedule used in the DO-ACROSS approach is, of course, highly dependent on the efficiency of the synchronization mechanisms provided on the multiprocessor of interest.</p><p>All of the methods presented thus far in this section can be viewed as reorgani- zations of the task graph in Fig. <ref type="figure">7</ref>. The row-oriented algorithm executes each row in turn starting from the top and tasks within each row from left to right. The column-oriented, on the other hand, executes each column in turn starting from the left and tasks within a column from the top-to bottom. The row and column sweeps FIG. <ref type="figure">7</ref>. Triangular system solution dependence graph. on a vector machine merely vectorize the tasks within a row or column, respectively. Block versions of the algorithm interpret each node as corresponding to computations involving a submatrix rather than a single element. Careful consideration of the task graph, however, reveals certain limitations of all methods based upon it. Suppose that each node represents the operation on a submatrix of order rn and n km. The dependence graph implies that the maximum number of processors that can ever be active at the same time is k-1. Further, the dependence graph has a critical path with O(k) length which establishes a fundamental limit to the speed at which these algorithms can solve a triangular system. To go faster we need a new dependence graph which relates the solution x to the data L and f.</p><p>The new dependence graph can be generated from recognizing the algebraic char- acterization of the columnand row-sweep algorithms. The algorithms can be easily described algebraically in terms of elementary unit lower triangular matrices. For example, assuming without loss of generality that i 1, it follows that where Ni I-lieT, Mj I-ejv, li is the vector corresponding to column in L with the 1 on the diagonal removed and vj is similarly constructed from row j of L. It is easy to see from the algebraic structure of Ni and Mj that multiplying them by a vector corresponds to the computational primitives of a triad and dotproduct, respectively. It follows immediately that the column-sweep and row-sweep algorithms are specified algebraically by (here with n 8):</p><p>:))))))) and (Ms (M7 (M6 (M5 (M4(M3(M2f))))))). The grouping of computations makes clear the source of the O(n) critical path in the dependence graph. Also a simple application of associativity can generate two algorithms that have a much shorter critical path. Specifically, the column-sweep expression can be transformed into (N y) ).</p><p>Note the logarithmic nature of the critical path. The algorithm specified is called the product form and is due to Sameh and Brent <ref type="bibr" target="#b161">[159]</ref>. Instead of performing the product (Nn-l"'" N2N1)f in (n-l) stages, we may form it in O(log 2 n) stages. It can be shown by careful consideration of the structure of the matrices at each stage that the critical path has a length of k2/2 + 3k/2 floating point operations where k log 2 n. Such an improvement is not without cost, however. The algorithm requires approximately n3/lO + O(n2) operations and n3/68 + O(n2) processors. It is therefore typically not appropriate for an architecture with a limited number of processors such as those of interest here. For a discussion of the numerical stability of this algorithm see <ref type="bibr" target="#b189">[187]</ref>.</p><p>Note that thus far we have assumed only one right-hand side vector. The BLAS3</p><p>primitive triangular solver assumes that multiple right-hand side vectors and solu- tions are required. This, of course, provides the necessary data locality for high performance on a hierarchical memory system. The generalization of the algorithms above are straightforward and the blocksizes (the number and order of right-hand sides solved in a stage of the,algorithm) can be analyzed in a fashion similar to the matrix multiplication primitives.</p><p>For banded lower triangular systems in which the bandwidth m (the number of subdiagonals with nonzero entries) is small, column-sweep algorithms are ineffective on vector or parallel computers. Consider such a system Lx f, where L is par- titioned as a block-bidiagonal matrix with diagonal submatrices L and subdiagonal submatrices R_I, 1,..., n/m, where L and Ri_ are lower and upper triangular, respectively. Premultiplying both sides of Lx f by D diag(L-) we obtain the system L()x f(o) where L () is block bidiagonal with identities of order m on the diagonal and matrices G ) L-_Rj on the subdiagonal, and f(0)= Of. Note that we do not invert the Lj's, but obtain f(0) and G ) by solving triangular systems using one of the above parallel algorithms. We repeat the process by multiplying both sides of L()x f(0) by D () diag((n))-) where (LO))_l (Ira _(0) In and f(log(n/m)) x. The required number of arith- metic operations is O(m2n log(n/2m)) resulting in a redundancy of O(m log(n/2m)), e.g., see <ref type="bibr" target="#b161">[159]</ref>. Given m2n/2 + O(mn) processor, however, those operations can be completed in O(log m log n) time. This algorithm offers opportunities for both vector and parallel computers. At the first stage we have n/m triangular systems to solve, each of order m, for (m + 1)   right-hand sides except for the first system which has only one right-hand side. In the subsequent stages we have several matrix-matrix and matrix-vector multiplications, with the last stage consisting of only one matrix-vector multiplication, in which the matrix is of order (n/2 m). Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php An alternative scheme, introduced by Chen, Kuck, and Sameh <ref type="bibr" target="#b27">[27]</ref>, may be de- scribed as follows. Let the banded lower triangular matrix L be partitioned as where LI R2 L2 /3 L3 1p Lp n )0 0 and each Li is of order (n/p) &gt; &gt; rn and each Ri is upper triangular of order m. If the right-hand side f and the solution x are partitioned accordingly, then after solving the triangular systems LlXl fl and Li [U, g] the original system is reduced to x g in which is of the form where Let Wi x zi gi hi ri in which Wi is a matrix of order m and ri, zi are vectors of rn elements each. Thus, solving the above system reduces to solving a smaller triangular system of order rap,</p><formula xml:id="formula_9">Im Zl rl W2 Im Z2 r2</formula><p>Wp Im Zp rp After solving this system by the previous parallel scheme, for example, we can retrieve the rest of the elements of the solution vector x by obvious matrix-vector multiplica- tions. The algorithm requires approximately 4rn2n operations which, given i5 rnp processors, can be completed in time 2/5-1m2n + 3-lmn + O(m2). See [189] for a discussion of the performance of this algorithm applied to lower bidiagonal systems and the attendant numerical stability properties. Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php is emphasized. In addition, the study in <ref type="bibr" target="#b54">[53]</ref> has improved upon the cyclic type algorithms in <ref type="bibr" target="#b90">[89]</ref>. <ref type="bibr" target="#b3">4</ref>. LU factorization algorithms. The goal of the LU decomposition is to fac- tor an n n-matrix A into a lower triangular matrix L and an upper triangular matrix U. This factorization is certainly one of the most used of all numerical linear com- putations. The classical LU factorization <ref type="bibr" target="#b84">[83]</ref> can be expressed in terms any of the three levels of the BLAS, and techniques needed to achieve high performance for both shared and distributed memory systems have been considered in great detail in the literature. In this section we review some of these techniques for the LU and LU-like factorizations for dense and block tridiagonal linear systems. 4.1. Shared-memory algorithms for dense systems. In this subsection we consider some of the approaches used in the literature for implementing the LU fac- torization of a matrix A E n n on shared-memory multivector processors such as the CRAY-2, CRAY X-MP, and Alliant FX/8. To simplify the discussion of the effects of hierarchical memory organization, we move directly to the block versions of the algorithms. Throughout the discussion w denotes the blocksize used and the more fa- miliar BLAS2-based versions of the algorithms can be derived by setting w 1. Four different organizations of the computation of the classical LU factorization without pivoting are presented with emphasis on identifying the computational primitives involved in each. The addition of partial pivoting is then considered and a block generalization of the LU factorization (L and U being block triangular) is presented for use with diagonally dominant matrices. Finally, the results of an analysis of the architecture/algorithm mapping of this latter algorithm for a multivector processor with a hierarchical memory are also examined along with performance results from the literature. 4.1.1. The algorithms. The are several ways to organize the computations for calculating the LU factorization of a matrix. These reorganizations are typically listed in terms of the ordering of the nested loops that define the standard computation. The essential differences between the various forms are: the set of conputational primitives required, the distribution of work among the primitives, and the size and shape of the subproblems upon which the primitives operate. Since architectural characteristics can favor one primitive over another, the choice of computational organization can be crucial in achieving high performance. Of course, this choice in turn depends on a careful analysis of the architecture/primitive mapping. Systematic comparisons of the reorganizations have appeared in various contexts in the literature. Trivedi considered thegn in the context of virtual memory systems in combination with other performance enhancement techniques <ref type="bibr" target="#b187">[185]</ref>, <ref type="bibr" target="#b188">[186]</ref>. Dongarra,  Gustavson, and Karp <ref type="bibr" target="#b42">[42]</ref> and more recently Ortega [137] compare the orderings for vector machines such as the CRAY-1 where the key problem is the efficient exploita- tion of the register-based organization of the processor and the single port to memory.</p><p>Ortega has also considered the problem on highly parallel computers <ref type="bibr" target="#b138">[137]</ref>. Papers have also appeared that are concerned with comparing the reorderings given a par- ticular machine/compiler/library combination, e.g., see <ref type="bibr" target="#b164">[162]</ref>. In general, most of the conclusions reached in these papers can be easily understood and parameterized by analyses of the computational primitives and the algorithms in the spirit of those in the previous section and below.</p><p>4.1.1.1. Version 1. Version 1 of the algorithm assumes that at step the LU factorization of the leading principal submatrix of dimension (i-1), Ai-1 Li-IUi-1, is available. The next w rows of L and w columns of U are computed during step to produce the factorization of the leading principal submatrix of order iw. Clearly, after k n/w such steps the factorization LU A results.</p><p>The basic step of the algorithm can be deduced by considering the following partitioning of the factorization of the matrix Ai E o.,x:</p><formula xml:id="formula_10">(Ai-lC) Ai BT H M T L2 0 U2</formula><p>where H is a square matrix of order w and the rest of the blocks are dimensioned conformally. The basic step of the algorithm consists of four phases:</p><p>(i) Solve for G: C Li-IG C.</p><p>(ii) Solve for M: B U_IM B.</p><p>(iii) Update" H H-11TG.</p><p>(iv) Factor H -L2 U2 H.</p><p>(The arrow is used to represent the portion of the array which is overwritten by the new information obtained in each phase.) Clearly, repeating this step on successively larger submatrices will produce the factorization of A E Nn.</p><p>In each step, solving the triangular system requires 2wh 2 operations, the update of H requires 2hw 2 and the factorization requires O(wS), where h (i-1)w. Early stages of the algorithm are dominated by the factorization primitive. The later stages, where most of the work is done, is dominated by solving triangular systems with w right-hand side vectors. This dominance is particularly pronounced when the BLAS2 (w 1) version of the algorithm is used. Note also that when w 1 the use of the triangular solver allows efficient use of the vector registers on vector processors like the CRAY-1 or a single CE of the Alliant FX/8 which have single ports to memory.</p><p>4.1.1.2. Version 2. Version 2 of the algorithm assumes that the first (i-1)w columns of L and rows of U are known at the start of step i, and that the transformations necessary to compute this information have been applied to the submatrix A n-nin the lower right-hand corner of A that has yet to be reduced. The algorithm proceeds by producing the next w columns and rows of L and U, respectively, and computing Ai+1. This is a straightforward block generalization of the standard rank-l-based Gaussian elimination algorithm.</p><p>Assume that the factorization of the matrix A N'-'is partitioned as follows:</p><p>Ai (All A12 ) (511 0 )( Ull U12 )</p><formula xml:id="formula_11">A21 A. Le I 0 A i+1</formula><p>where A is square and of order w and the other submatrices are dimensioned confor- mally. Ll,L21 and U12 are the desired w columns and rows of L and U and identity defines A i+ 1.</p><p>The basic step of the algorithm consists of:</p><p>(i) Factor" All --L11U11 All.</p><p>(ii) Solve for L21" n21 --UTllLT21 AT21</p><p>(iii) Solve for U2" A12 LU12 A2.</p><p>(iv) Update: A22 -A22-L2UI2.</p><p>Clearly, the updated A22 is A + and the algorithm proceeds by repeating the above four phases. This version of the algorithm is dominated by the rank-w update of the submatrix A22 E (n-iw)(n-iw). Note that the triangular systems that must be solved are of order w with many right-hand sides as opposed to the large systems which are solved in Version 1. As in Version 1 the factorization primitive operates on systems of order w. As is well known and obvious from the analysis of the previous section, the BLAS2 version, based on the rank-1 update, is not the preferred form for register-based vector or multivector processors with a single port to memory due to poor register usage. 4.1.1.3. Version 3. Version 3 of the algorithm can be viewed as a hybrid of the first two versions. Like Version 2, it is assumed that the first (i-1)w columns of L and rows of U are known at the start of step i. It also assumes, like Version 1, that the transformations that produced these known columns and rows must be applied elements of A which are to be transformed into the next w columns and rows of L and U. As a result, Version 3 does not update the remainder of the matrix at every step.</p><p>Consider the factorization:</p><p>where A 11 is a square matrix of order (i-1)w and the rest are partitioned conformally. By our assumptions, Lll, L21, U, and U2 are known and the first w columns of L22 and the first w rows of U22 are to be computed. Since Version 3 assumes that none of the update A22 --A22 L21U12 has occurred in the first i-1 steps of the algorithm, the first part of step is to perform the update to the portion upon which the desired columns of L22 and rows of U22 depend. This is then followed by the calculation of the columns and rows.</p><p>To derive the form of the computations, suppose the update of A22 where H and/2/are square matrices of order w and the other submatrices are dimen- sioned conformally.</p><p>Step then has two major phases" Calculate H, B, and C; and calculate 1, ,21, DI, and 2. As a result, at the end of stage i, the first iw rows and columns of the triangular factors of A are known.</p><p>Let L2 [M1T, MT] T and U12 [M3, M4], where M1 and M3 consist of the first w rows and columns of the respective matrices. The first phase of step computes (i) [HT, BT] T '-[HT, BT] T [T, T]T L21M3.</p><p>(ii) C C T T MM4.</p><p>In the second phase, the first w rows and columns of the factorization of the updated A22 are then given by: (i) Factor: H *--11]'11 H.</p><p>(ii) Solve for ,2" B T2 BT. The work in this version of the algorithm is split between a matrix multiplication prinitive, a triangular solver, and a factorization primitive; the latter two of which are applied to systems of order w. Note, however, that the matrix multiplication primitive is applied to a problem which has the shape of a large matrix applied to w vectors (or the transpose of such a problem). Hence, for w 1 this version of the algorithm becomes a form which uses the preferred BLAS2 primitive matrix-vector multiplication. Although, as noted above, when w is nontrivial the preference for this block form over Version 2 does not necessarily follow. 4.1.1.4. Version 4. Version 4 of the algorithm assumes that at the beginning of step the first (i 1)w columns of L and U are known.</p><p>Step computes the next w columns of the two triangular factors. Consider the factorization</p><formula xml:id="formula_12">(AI A2)(LI 0 )(UI A A A22 L2 L22 0 U22</formula><p>where All is a square matrix of order (i-1)w and the rest are partitioned conformally.</p><p>By our assumptions, LI, L21, and U are known.</p><p>Let L, U, and A be the matrices of dimension n x w formed of the first w columns of [0, L]T, [U, U]T, and [A2, A2]T, respectively. (These are also columns (i 1)w + 1 through iw of L, U, and A.) Consider the partitioning G 0 where , , nd 2 re squre matrices of order wih and lower and upper triangular respectively.</p><p>Step calculates L and U by applying all of he ransformions from seps 0 i-1 to A and then fcoring rectangular mrix. Specifically, sep comprises he computations:</p><p>(i) Solve for M: LM .</p><p>(iii) Factor" A2 LU A2.</p><p>(iv) Sov or : G . This version of the algorithm requires he solution.of lrge ringular system wih w right-hand sides s well s a small riangulr system of order w wih many right-hand sides. The factorization kernel operates on a system of order . As wih Version 3 he matrix multiplication primitive operates on a problem wih he shape of a large matrix times vectors and the facorizion of a system of order and he same observations apply. This version Mso has the feature ha i works exclusively with columns of A which can be advantageous in some Fortran and virtual memory environments.</p><p>4.1..5. Partial pivoting. PariM pivoting cn be easily added o Versions 2, 3, and 4 of the algorithm. Sep of each of he versions requires he LU facorizaion of a rectangular matrix M G w, where h n-(i-1)w. Specifically, step computes Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php where ]-11 and [)'11 are, respectively, lower and upper triangular matrices of order w. In the versions above without pivoting, this calculation could be split into two pieces: the factorization of a system of order w, 11'11 M1; and the solution of a triangular system of order w with h-w right-hand sides. (These computations are: (i) and (ii) in Version 2; (i) and (ii) of the second phase of Version 3; and (iii) and (iv) of Version 4.) When partial pivoting is added to the versions of the algorithm these computations at each step cannot be separated and are replaced by a single primitive which produces the factorization of a rectangular matrix with permuted rows, i.e., PM P M2 L21 where P is a permutation matrix. This primitive is usually cast as a BLAS2 version of one of the versions above. Note, however, a fundamental difference compared to the nonpivoting versions. The ability to split the factorization of the tall matrix into smaller BLAS3-based components in the latter case has benefits with respect to hierarchical memory usage, since w is usually taken so that such systems fit in cache or local memory, see <ref type="bibr" target="#b23">[23]</ref>, [67]. In the case of pivoting, these operations are performed via BLAS2 primitives repeatedly updating a matrix which can not be kept locally.</p><p>As a result, the arithmetic component of time and the data transfer overhead both increase. In fact, a conflict between their reductions occurs. This situation is similar to that seen in the block version of Modified Gram Schmidt and Version 5 of the factorization algorithm, both discussed below along with a solution. (Although in the latter case, the source of difficulties is slightly different.)</p><p>The information contained in the permutations associated with each step, Pi, can be applied in various ways. For example, the permutation can be applied immediately to the transformations of the previous steps, which are stored in the elements of the array A to the left of the active area for step i, and to the elements of the array A which have yet to reach their final form, which, of course, appear to the right of the active area for step i. The application to either portion of the matrix nay also be delayed. The update of the elements of the array which have yet to reach their final form could be delayed by maintaining a global permutation matrix which is then applied to only the elements required for the next step. Similarly, the application to the transformations from steps 1 through i-1 could be suppressed and the Pi could be kept separately and applied incrementally in a modified forward and backward substitution routine. 4.1.1.6. Version 5. A block generalization. In some cases it is possible to use a block generalization of the classical LU factorization in which L and U are lower and upper block triangular matrices, respectively. The use of such a block generalization is most appropriate when considering systems which do not require pivoting for stability, e.g., diagonally dominant or symmetric positive definite. This algorithm decomposes A into a lower block triangular matrix L and an upper block triangular matrix U with blocks of the size w by w (it is assumed for simplicity that n kco, k &gt; 1). Assume that A is diagonally dominant and consider the factorization"</p><formula xml:id="formula_13">(All A12 )( I 0 )( All A12 ) A A21 A22 L21 I 0 B</formula><p>where All is a square matrix of order w. The block LU algorithm is given by: (i) All AI (ii) A2 -L2 A21AI (iii) A22 --B A22-L2A12</p><p>(iv) Proceed recursively on the matrix B.</p><p>Statements (i) and (ii) can be implemented in several ways. Since A is assumed to be diagonally dominant, explicit inversion of the diagonal blocks can be done either via the Gauss-Jordan algorithm <ref type="bibr" target="#b144">[143]</ref> or an LU decomposition without pivoting. In the latter case, the computations in step (i) above are replaced by solving two triangular systems of order w with many right-hand sides. (Due to parallel processing, the Gauss-- Jordan scheme, historically frowned upon, has recently been the subject of renewed interest. See <ref type="bibr" target="#b34">[34]</ref> for a discussion of its application, with appropriate modifications, to general nonsymmetric systems of equations.) If the Gauss-Jordan kernel is used, as is assumed below, the block LU algorithm is more expensive by a factor of approximately (1 + 2/k2) than the classical LU factorization which requires about 2n3/3 operations. In this form, the above block algorithm uses three primitives: a Gauss-Jordan inversion (or LU decomposition), A AB, and a rank-w update.</p><p>Note that when w 1 this form of the algorithm becomes the BLAS2 version based on rank-1 updates. As with Versions 1-4, which produce the classical LU factorization, the computations of Version 5 can be reorganized so that different com- binations of BLAS3 primitives and different shapes of submatrices are used. For example, the main BLAS3 primitive can be changed from a rank-w update into a matrix multiplying w row or column vectors. As noted above, the importance of such a reorganization depends highly on the architecture in question. There are two general aspects of the block LU decomposition through which the blocksize oa n/k influences the arithmetic time: the number of redundant operations (applicable when the Gauss-Jordan approach is used); and the relationship, as a function of w, between the performance of each of the primitives and the distribution of work among the primitives. The redundancy factor of (1 + 2/k2) and the fact that the number of operations performed in the Gauss-Jordan primitive is an increasing function of w cause the arithmetic time component to prefer smaller blocksizes for small and moderately sized systems. For those systems, increasing w and therefore decreasing k clearly exacerbates the two problems noted above to such a degree that the effect is dominant compared to the reduction in data transfer overhead gained by increasing the blocksize. As the order of the system increases, however, these effects become secondary to data transfer considerations.</p><p>The data transfer overhead of the algorithm is most conveniently analyzed by writing the algorithm's cache-miss ratio as the weighted average of the cache-miss ratios of the various instances of each primitive. The weights are the ratio of the number of operations in the particular instance of the primitive to the total number of operations required. In practice, some of the local cache-miss ratios are zero due to the interaction between the instances of the primitives; this occurs when the remaining Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php part of the matrix to be decomposed approaches the size of the cache and later instances of primitives find an increasing proportion of their data left in cache by earlier instances. In [67] the results are derived using the conservative assumption of no interaction between instances of primitives. Note that without a model of the data transfer properties of the primitives such an analysis at the algorithmic level is impossible. This does not imply that blocksizes cannot be set effectively based on observed perfornance data of the primitives for various shapes and sizes of problems. Such a black box tuning approach is quite useful in practice, but it does not provide any explanation as to why the performance is as observed. This can only be done by considering the architecture/algorithm mapping of the primitives and the implications of combining them in the manner specified by the particular version of the factorization algorithm used.</p><p>The behavior on the interval 1 _&lt; w _&lt; , where CS denotes cache size, roughly separates into three regimes. For small values of , i.e., w &lt;_ 16, the cache-miss ratio is of the form:</p><formula xml:id="formula_14">1</formula><p>where /1 is proportional to 1/n and q'R is a function of w which is bounded by a small constant. This result is expected since the computations are dominated by the rank-w update which achieves a similar cache-miss ratio. In particular, it is clear that the data locality of a BLAS2 version, w 1, is very poor. In the middle of the interval of interest the cache-miss ratio is of the form: 1 where r/2 is proportional to 1In. Finally, when w vf-C--, the cache-miss ratio is</p><formula xml:id="formula_15">1 # v/-C-R -{-</formula><p>where 3 is proportional to I/n. The ratio # becomes a rapidly increasing function once w exceeds until it reaches, at the point w n, the cache-miss ratio of the algorithm of a BLAS2-based version of the Gauss-Jordan algorithm which has a value of approximately . The exact point where this transition to rapidly increasing occurs is dependent on the implementation of the Gauss-Jordan primitive, but, any decrease in # between w and the transition point is typically insignificant.</p><p>4.1.3. Experimental results. The various versions of the algorithms have ap- peared in different, contexts in the literature. Here we list some representative papers and then consider in more detail the performance of Version 5 and its relationship to the trends predicted via the blocksize analysis presented above.</p><p>The column-oriented BLAS2 form of Version 4 was used by Fong and Jordan on the CRAY-1 <ref type="bibr" target="#b57">[56]</ref>. The results of using a BLAS2 form of Version 3 on the CRAY-1 and one CPU of a CRAY X-MP were given by Dongarra and Eisenstat in <ref type="bibr" target="#b41">[41]</ref>. Dongarra and Hewitt discuss the use of a rank-3-based approach on four CPU's of a CRAY X-MP <ref type="bibr" target="#b46">[45]</ref>. Calahan demonstrated the power of the block form of Version 3 (with and without pivoting) on the hierarchical memory system of one CPU of a CRAY-2. Agarwal and Gustavson have extended their work which led to single CPU block algorithms for the IBM 3090 by considering parallel forms of the BLAS3 primitives and LU factorization on an IBM ES/3090 model 600S <ref type="bibr" target="#b1">[2]</ref>. In particular, they discuss Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php the use of parallel block methods in a multitasking environment where the user is not necessarily guaranteed control of all (or any fixed subset) of the six processors in the system. Radicati, Robert, and Sguazzero have presented the results of a rank-k-based code on an IBM 3090 multivector processor for one to six processors <ref type="bibr" target="#b150">[149]</ref>. The block form of Version 4 was also considered in a virtual memory setting by Du Croz et al. in <ref type="bibr" target="#b51">[50]</ref> and used as a model of a block LU factorization in the BLAS3 standard proposal by Dongarra et al. <ref type="bibr" target="#b39">[39]</ref>. The performance trends for Version 5 predicted via the decoupling analysis sum- marized above have been verified in [67]. Figure <ref type="figure" target="#fig_8">8</ref> illustrates the performance of the block LU algorithm for diagonally dominant matrices for various blocksizes on an A1liant FX/8 [67]. The performance was computed using the nonblock version operation count. The actual rate is, therefore, higher for the block methods.</p><p>The curves in Fig. <ref type="figure" target="#fig_8">8</ref> clearly show the trends predicted by the analysis above. The significant improvement over BLAS2-based routines by a small amount of blocking can be seen in the peribrmance of the w 8 curve and comparing it to the 7 to 10 Mflops possible via a BLAS2-based Version 2 code or the 15 to 17 Mflops of a BLAS2-based Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Version 3 code. As expected, for any fixed order of the system, performance improves as w is increased until an optimal is reached. For small systems, increasing beyond this value causes performance degradation due to the conflict between reducing # and efficiently distributing work among the primitives. For larger systems, the conflict reduces and performance is maintained until w exceeds vf-C--s. The conflict between arithmetic time and data loading overhead minimization which produces the shifting of the preferred blocksize as a function of n can be mit- igated somewhat by using a double-level blocking [67]. This conflict has been delib- erately exacerbated in these experiments by using a Fortran implementation of the Gauss-Jordan primitive and assembler coded BLAS3 routines.</p><p>There are two basic approaches to double-level blocking: inner-to-outer and outer- to-inner. Both require a pair of blocksizes (, w). The outer-to-inner approach replaces the operation of the Gauss-Jordan primitive on a system of order w with a block LU factorization using the inner blocksize 0. The inner-to-outer approach begins with a block LU factorization with blocksize which is determined largely by the arithmetic time analysis and which is typically smaller than the single-level 10ad analysis would recommend. Several rank-0 updates are then grouped together into a rank-w in order to improve the data loading overhead. The decoupling methodology can be used to determine pivot row interchange doi-k+l,n-1 broadcast the column just computed and pivot index receive the column just computed and pivot index interchange for (all columns j &gt; k that I own) doi=k+l,n-1 enddo A modification of RSRP, which we refer to as RSCP, Row Storage with Column Pivoting, consists of searching the current pivot row for the element with maximum modulus, and then exchanging columns to bring this element to the diagonal. The RSCP algorithm can be readily seen as nothing more than the dual of algorithm CSRP.</p><p>Geist and Heath <ref type="bibr" target="#b79">[78]</ref> indicate that both RSCP and CSRP yield essentially identical speedup on an Intel iPSC hypercube. In fact, Geist and Heath conclude that, in the absence of such techniques as loop unrolling, LU factorization with partial pivoting is most efficient when pipelining is used to mask the cost of pivoting. In particular, the two schemes that can most easily be pipelined are: pivoting by interchanging rows when the matrix is distributed across the processors by columns (algorithm CSRP), and pivoting by interchanging columns when the matrix is distributed across the processors by rows (algorithm RSCP).</p><p>4.2.2. Pairwise pivoting. Gaussian elimination with pairwise pivoting is an alternative to LU factorization which is attractive on a variety of distributed memory architectures including systolic arrays since it introduces parallelism into the pivoting strategy. 1 Such a pivoting strategy dates back to Wilkinson's work on Gaussian elimination using the ACE computer with its limited amount of memory <ref type="bibr" target="#b63">[62]</ref>. The main idea is rather simple. If U T [1,''', tn] and V T [/21,''',/]n] are two row vectors, then we can choose a stabilized elementary transformation P so as to annihilate either #1 or /21, whichever is smaller in magnitude.</p><p>either the identity of order 2 or (e2, el) so that Here, P is S vT 0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22</head><p>]n One of the many possible annihilation schemes for reducing a nonsingular matrix A of order n to upper triangular form is illustrated in Fig. <ref type="figure" target="#fig_10">10</ref> for n 8. (The elements marked with can all be eliminated simultaneously on step i.)</p><p>Such a triangularization scheme requires 2n-3 stages in which each stage consists of a maximum of [n/2] independent stabilized transformations. It is ideally suited for a ring of processors <ref type="bibr" target="#b159">[157]</ref> or other systolic arrays <ref type="bibr" target="#b81">[80]</ref>. Note, however, that it does not produce an LU factorization of the matrix. L is replaced by a product of matrices in which each one can be readily inverted. One possible drawback of this pivoting strategy is that the upper bound on the growth factor is the square of that of partial pivoting <ref type="bibr" target="#b170">[168]</ref>, <ref type="bibr" target="#b171">[169]</ref>. Our extensive numerical experiments indicate that, as is the case with partial pivoting, such growth is rarely encountered in practice. In that sense, our experience contradicts some conclusions of Trefethan and Schreiber <ref type="bibr" target="#b186">[184]</ref> indicating that some further work is required to reconcile this seeming inconsistency.</p><p>The above annihilation scheme was originally motivated by a parallel Givens reduction introduced in <ref type="bibr" target="#b163">[161]</ref> and now used extensively in applications such as signal processing for recursive least squares conputations. This parallel Givens reduction was later generalized for a ring of processors <ref type="bibr" target="#b160">[158]</ref>.</p><p>4.2.3. A hybrid scheme. In order to design factorization schemes for multi- cluster machines, such as Cedar, in which each cluster is a parallel computer with tightly coupled processors, we must combine the strategies outlined above for both shared and distributed memory models. Breaking the problem among the clusters so as to minimize intercluster communication while maintaining load balancing is an issue faced by users of distributed memory architectures. Cedar's advantage is the existence of a shared global memory.</p><p>The shared memory block LU algorithm and the BLAS3 primitives, discussed above, are concerned with achieving high performance on an architecture like a single Cedar cluster. While these algorithms and kernels form an invaluable building block for algorithms on the Cedar system and the conclusions of the analysis are applicable over a fairly wide range of multivector architectures, care must be taken not to gen- eralize these conclusions too far. For example, on a single Cedar cluster (and similar architectures) routines for many of the basic linear algebra tasks encountered in prac- tice can be designed as a series of calls to BLAS3 kernels and BLAS2-implemented algorithms thereby masking all of the architectural considerations of parallelism, vec- torization, and communication. This method of algorithm design, however, cannot be generalized to all hierarchical shared memory machines. One of the main reasons for this is the fact that an algorithm designed via this method may have problems with an inappropriate choice of task granularity and the resulting excessive com- munication requirements. The need to introduce double-level blocking forms of the algorithm indicated the onset of such a problem on a Cedar cluster: the attempt to spread the BLAS2-implemented kernel across the processors in a cluster introduced serious limitations on the performance of the block algorithm. When this problem Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php becomes extreme, other forms of the algorithm must be used which typically involve reorganizing the block computations to more efficiently map the algorithm to the architecture via tasks of coarser granularity with more attention focused on minimiz- ing the required communication. Typically this involves some notion of pipelining (possibly multidimensional) at the block level, e.g., see <ref type="bibr" target="#b13">[14]</ref>, [157].</p><p>An example of such a situation is the solution of a dense linear system using more than one cluster of Cedar (possibly a subset of the total number available). In this case the algorithm design must take into account that intercluster communication is rather costly. There are several possible designs for such an algorithm. One of the most straightforward is based on the outer-to-inner double-level block form presented above. The block computations can be pipelined across clusters using the necessary Cedar synchronization primitives. A second possibility uses the control structure of the pipelined Givens factorization on a ring of processors described in <ref type="bibr" target="#b160">[158]</ref>. A block of rows rather than a single row is communicated between processors and the row rotation is replaced with a block Gaussian elimination procedure. The remainder of this section discusses another algorithm, due to Sameh <ref type="bibr" target="#b159">[157]</ref>, for solving dense linear systems on a multiple cluster architecture which requires a relatively small amount of intercluster communication. For simplicity a four-cluster Cedar is assumed. Let A, a nonsingular matrix of order n, be partitioned as AT (AT A, AT3, A)</p><p>where Ai resides in the ith cluster nemory. The algorithm consists of two major stages. In the first stage, using a block-LU scheme with partial pivoting, each Ai is factored into the form PA LU for 1, 2, 3, 4 where Pi is a permutation, L is unit lower triangular, and U is upper trapezoidal.</p><p>Assuming, without loss of generality, that each U has a nonsingular upper trian- gular part, the factorization of A may be completed in the second stage which consists of 3n/4 computational waves pipelined across the four clusters. These computational waves comprise three groups of n/4 waves. During the kth group the latest values for the rows of Uk are used by clusters k + 1 to 4 in a pipelined fashion to further reduce their segments of the decomposition. It should be noted that cluster k is idle during the kth group of waves and the remainder of the algorithm since the other clusters will update the rows of Uk that it has produced and placed in global memory. (For example, cluster 1 only performs the initial reduction of A1 and is then released for other tasks within the application code of which solving the system is a part or the tasks of other users since Cedar is a multiuser system.) The first group of n/4 com- putational waves which use the rows of U1 produced by cluster 1 is described below.</p><p>The pattern of the remaining two groups follows trivially.</p><p>Wave 1. Let Uk =--[#k,j]. The first row of U1 is transmitted via the global memory to cluster 2 where it is used, with pairwise pivoting, to annihilate the first element of the (possibly new due to pairwise pivoting) first row of U2, #,1. The updated first row of U1 is then transmitted to cluster 3 so as to annihilate #3, and then to cluster 4 where 4 #1,1 is eliminated with the final version of the first row of U1 residing in global memory. Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php As soon as #k,1 is annihilated in cluster k, k 2, 3, 4, the nonzero portion of Uk is a n/4 (n-1) upper Hessenberg matrix, e.g., for n 24 it is of the form</p><formula xml:id="formula_16">X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X</formula><p>The cluster then proceeds to reduce Uk to upper trapezoidal form through a pipelined Gaussian elimination process using pairwise pivoting.</p><p>Waves 2 _ j &lt;_ n/4. Similar to the first wave, the jth row of U1 is transmitted to clusters 2, 3, and 4 to annihilate #,j,2 l,j3, and #4,, respectively. After these annihilations occur, each cluster reduces Uk, which at this point is upper Hessenberg, to upper trapezoidal form.</p><p>Note that after this first group of computational waves U is in its final form in global memory. The matrix U2 is in its penultimate form since it will only change due to the pairwise pivoting done by clusters 3 and 4 in the second group of computational waves. This implies that cluster 2 is now available for other work. The second and third computational groups proceed in the same way as the first did with each cluster fetching the appropriate row from the source matrix, U2 followed by U3, transforming Uk to upper Hessenberg form and then reducing it back to an upper trapezoidal matrix. This basic form of the algorithn possesses many levels of communication and computation granularity and can be modified to improve utilization of a multicluster architecture. For example, ._ the whole Cedar machine were devoted to such a dense solver, simple interleaving of block rows of A would enhance load balancing among the clusters.</p><p>4.3. Block tridiagonal linear systems. Block tridiagonal systems arise in nu- merous applications one example being the numerical handling of elliptic partial differential equations via finite element discretization. Often, solving such linear sys- tems constitutes the major computational task. Hence, efficient algorithms for solving these systems on vector and parallel computers are of importance. Using block ver- sions of Gaussian elimination for block tridiagonal systems seems a natural extension of the efficient dense solvers discussed above. Some of the early work may be found in <ref type="bibr" target="#b193">[191]</ref> and the survey by Heller <ref type="bibr" target="#b91">[90]</ref>. A more recent study of block Gaussian elimination on the Alliant FX/8 for solving such systems [11] indicates the importance of efficient dense solvers and the underlying BLAS3 as components for block tridiagonal solvers.</p><p>If the size of the blocks is small, i.e., a narrow-banded system, such forms of Gaussian elimination offer little potential vectorization and parallelization. Similar to the above discussions for banded triangular systems, a partitioning scheme, referred to as the spike algorithm below, for handling tridiagonal systems on vector or parallel computers was introduced in <ref type="bibr" target="#b163">[161]</ref>, where Givens reductions were used to handle the diagonal blocks. Later, Wang <ref type="bibr" target="#b194">[192]</ref> considered the simpler problem of diagonally dominant systems and gave essentially the same form of the algorithm modified to use Gaussian elimination (made possible by the assumption of diagonal dominance) and a different method for the elimination of the spikes. Several studies have generalized this partitioning scheme to narrow-banded systems, e.g., see <ref type="bibr" target="#b47">[46]</ref>, <ref type="bibr" target="#b48">[47]</ref>, <ref type="bibr" target="#b121">[120]</ref>, <ref type="bibr" target="#b133">[132]</ref> and the recent book by Ortega <ref type="bibr" target="#b138">[137]</ref>. The main idea of this partitioning scheme may be outlined as follows. Let the linear system under consideration be denoted by Ax f, where A is a banded diag- onally dominant matrix of order n. It is assumed that the number of superdiagonals rn &lt;&lt; n is equal to the number of subdiagonals and that, for simplicity of presenta- tion, n pq. On a sequential machine such a system would be solved via Gaussian elimination, see <ref type="bibr" target="#b38">[38]</ref> for example. The algorithm described below assumes p CPU's of a CRAY X-MP or CRAY-2, or a Cedar system with p clusters. Here, for the sake of illustration, p is taken to be 4.</p><p>Let the matrix A be partitioned into the block-tridiagonal form with block row [Ci, Ai, Bi] and conformally x and f, e.g., A B 0 0 where E-(k,0), F (0,), in which/i and/i are matrices of rn columns given by /i A -1 ( 0 and and will, in general, be full. In stage 1, i, /i, and gi are obtained by solving the associated linear systems. In each cluster 2_&lt; k_&lt; 4wesolve 2m + 1 linear systems of the formAkv r, while clusters 1 and 4 each solves rn + 1 linear systems of the same form. Note that no intercluster communication is needed.</p><p>The method of solution used on each cluster (Alliant FX/8) for these 4 systems with multiple right-hand sides, varies with m. For rn &lt; 8 a variant of the spike algorithm is used. For 8 _&lt; rn _&lt; 16 (approximately), block cyclic reduction is the most effective and for larger m a block Gaussian elimination is recommended <ref type="bibr" target="#b10">[11]</ref>. where Pi, Qi, Si, and Ti m,. Also, let gi and xi be conformally partitioned:</p><formula xml:id="formula_17">gi wi xi Zi h2i-1 Y2i-1</formula><p>The structure of the resulting partitioned system is such that the unknown vectors yj, 1 _&lt; j _&lt; 6 (each of order m) are disjoint from the rest of the unknowns. In other words, the m equations above and the m equations below each of the 3 partitioning lines form an independent system of order 6m, which is referred to as the reduced system Ky h,</p><formula xml:id="formula_18">Im T1 Yl hi P2 Im $2 Y2 h2 Q2 Im T2 Y3 h3 P3 Im $3 ya h4 Q3 Im T3 Y5 h5 P4 Im Y6 h6</formula><p>Since A is diagonally dominant, it can be shown that the reduced system is also diagonally dominant and hence there are a number of options available for solving it. Typically, it is small enough to be sent to a single Cedar cluster and solved with an appropriate algorithm.</p><p>When it is large enough to warrant a multicluster approach the reduced-system approach could be applied again. Note, however, that the bandwidth of the system has doubled compared to the original system. Block-column permutations can reduce the bandwidth back to its original value but this destroys diagonal dominance and pivoting will usually be required to solve the permuted reduced system. It is also possible to use all of the clusters to solve the reduced system via an iterative technique such as Orthomin(k) <ref type="bibr" target="#b48">[47]</ref>.</p><p>Finally, if the original linear system is suJficiently diagonally dominant, we can ignore the matrices Qi and Si as IISllo and IIQill are much smaller than IITllc and I[Pl[, respectively. This results in a block-diagonal reduced system in which each block is of the form Pk+l Im Stage 3. Once the yi's are obtained, the rest of the components of the solution vector of the original system may be retrieved as follows:</p><p>for 1_&lt; k_&lt;4, Zk Wk Mk Y2k-3 Nk Y2k and Yo ho-S, y2, y7 hT Q4 y.</p><p>Provided that the yi's are stored in the global memory, this stage requires no inter- cluster communication.</p><p>In addition to reporting on the performance results for this algorithm on the A1liant FX/8, [11] also reports on the performance achieved on four CPU's of a CRAY X-MP/416. Using four partitions on a system of order 16384 with blocksize 32, a speedup relative to itself of 3.8 was achieved indicating an efficient use of the micro- tasking capabilities and memory system of the machine. The speedup compared to a block LU algorithm on one CPU was approximately 2.</p><p>There are several modifications and reorganizations possible of the spike algorithm for solving banded systems discussed above. These can be used to alter the form of the algorithm to more efficiently map to a variety of shared memory architectures.</p><p>For one such alternative see <ref type="bibr" target="#b157">[155]</ref>. Also, if the system is symmetric positive definite, Dongarra and Johnsson <ref type="bibr" target="#b47">[46]</ref> have discussed how the algorithm can be modified to obtain a reduced system that is symmetric positive definite as well.</p><p>An analysis of the parallel and numerical aspects of a two-sided Gaussian elimi- nation for solving tridiagonal systems has been given recently by van der Vorst <ref type="bibr" target="#b190">[188]</ref>.</p><p>The work by Johnsson [108], <ref type="bibr" target="#b110">[109]</ref> is representative of organization of concurrent algorithms for solving tridiagonal and narrow banded systems on distributed memory machines with various connection topologies, e.g., two-dimensional arrays, shuffle- exchange networks and boolean cubes. Fox et al. have also considered the problem of banded systems on hypercubes. In <ref type="bibr" target="#b61">[60]</ref>, they provide a detailed performance analysis of the problem.</p><p>5. Least squares. In solving the linear least squares problem:</p><p>(10) min where A is an m n matrix of rank n, (m &gt;_ n), it is often necessary to obtain the factorization, in which Q is an orthogonal matrix and R is a nonsingular upper triangular ma- trix of order n. Such a factorization may be realized on multiprocessors via plane rotations, see <ref type="bibr" target="#b49">[48]</ref>, <ref type="bibr" target="#b160">[158]</ref>, and <ref type="bibr" target="#b163">[161]</ref>, elementary reflectors, see [16] and [158], or the Modified Gram-Schmidt algorithm, see <ref type="bibr" target="#b8">[9]</ref>. (Although the latter algorithm is more commonly associated with the calculation of an orthogonal basis of the range of A.) In the section concerning shared memory multiprocessors, block versions of House- holder reduction and the modified Gram-Schmidt algorithm are presented, as well as a pipelined Givens reduction for updating matrix factorization. For distributed memory multiprocessors, organization of Givens and Householder reductions on a ring of processors convey the main ideas needed for implementation on hypercubes and locally connected distributed memory architectures.</p><p>5.1. Shared-memory algorithms. T k=l,"" n, then it is possible to generate elementary reflectors Pk I-akUkUk, such that forming PkAk produces the kth row of R and the (m-k) (n-k) matrix</p><formula xml:id="formula_19">(k+) (k+) A+I [ak+l ,n</formula><p>by annihilating all but the first element in a(k k). The two basic tasks in such a procedure are <ref type="bibr" target="#b172">[170]</ref>: (i) generation of the reflector Pk such that Pka(k k) (Pka, 0,..., 0)T, k 1, 2,-.., n; and (ii) updating the remaining (n-</p><formula xml:id="formula_20">_(kT1)T k) columns, Pka k) (pkj,aj</formula><p>)T, j k + 1,...,n. On a parallel computer, reflector Pk+l may be generated even before task (ii) for stage k is finished. While an organization that allows such an overlap is well suited for some shared memory machines and for a distributed memory multiprocessor such as a ring of processors, e.g., see <ref type="bibr" target="#b160">[158]</ref>, it does not offer the data locality needed in a hierarchical shared memory system such as that of an Alliant FX/8. A block scheme proposed by Bischof and Van Loan <ref type="bibr" target="#b15">[16]</ref>, see also the related papers <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b36">[36]</ref>, <ref type="bibr" target="#b147">[146]</ref>, [163], offers such data locality. This scheme depends on the fact that the product of k elementary reflectors Qe (Pk,"" ,P2,P), where Pi Im wiTTy, can be expressed as a rank-k update of the identity of order m, i.e., Qk Im VkU[, where V1 U1 w, Vj (PjVj_I,Wj) and Uj (Uj-l,Uj), for j 2,...,k.</p><p>The block algorithm may be described as follows. Let the rn n matrix (m _&gt; n)</p><p>whose orthogonal factorization is desired be given by A [A, B],</p><p>where A is of rank n, and A consists of the first k columns of A. Next, proceed with the usual Householder reduction scheme by generating the k elementary reflectors P through Pk such that (P...P)A 0 where R is upper triangular of order k without modifying the matrix B. If we accu- mulate the product Qk Pk... P1 I-VkU as each Pi is generated, the matrix B is updated via which relies on the high efficiency of one of the most important kernels in BLAS3, that of a rank-k update. The process is then repeated on the modified B with another well-chosen block size, and so on until the factorization is completed. It may also be desirable to accumulate the various Qk's, one per block, to obtain the orthogonal matrix, Q, that triangularizes A. It was shown in <ref type="bibr" target="#b15">[16]</ref> that this block algorithm is as numerically stable as the classical Householder scheme. The block scheme, however, requires roughly (1 + 2/p) times the arithmetic operations needed by the classical sequential scheme, where p n/k is the number of blocks (assuming a uniform block size throughout the factorization). Bischof and Van Loan report the performance of the block algorithm at 18 Mflops for large square matrices (n 1000) on an FPS-164/MAX with a single MAX board and note that an optimized LINPACK QR running on an FPS- 164 without MAX boards would achieve approximately 6 Mflops. An example, of the performance achieved by a BLAS3 implementation of the block Householder algorithm (PQRDC) compared to a BLAS2 version (DQRDC) on an Alliant FX/8, <ref type="bibr" target="#b86">[85]</ref>, is shown in Fig. <ref type="figure" target="#fig_13">11</ref>. The performance shown is computed using the nonblock algorithm operation count.</p><p>Most recently, Schreiber and Van Loan have considered a more ecient storage scheme for the product of Householder matrices <ref type="bibr" target="#b166">[164]</ref>. They describe the compact WY representation of the orthogonal matrix Q which is of the form</p><formula xml:id="formula_21">Q I + YTYT,</formula><p>where Y E mn is a lower trapezoidal matrix and T E nn is a upper triangular The representation requires only mn storage locations and can be computed in a stable fashion.</p><p>5.1.2. A block-modified Gram-Schmidt algorithm. The goal of this algo- rithm is to factor an m n matrix A of maximal rank into an orthonormal rn x n matrix Q and an upper triangular R of order n where m &gt; n and A is of maximal rank. Let A be partitioned into two blocks A1 and B where A1 consists of w columns of order m, with Q and R partitioned accordingly:</p><p>(AI, B)-(Q,P) 0 The algorithm is given by: (i) A (ii) R12 QTIB, (iii) B1 B Q1R12.</p><p>(iv) Apply the algorithm recursively to produce B PR22.</p><p>If n kw, step (i) is performed k times and steps (ii) and (iii) are each performed k-1 times.</p><p>Three primitives are needed for the jth step of the algorithm: a QR decompo- sition (assumed here to be a modified Gram-Schmidt routine MGS); a matrix multiplication AB; and a rank-w update of the form C C-AB. The primitives allow for ideal decomposition for execution on a limited processor shared memory ar- chitecture. The BLAS2 version of the modified Gram-Schmidt algorithm is obtained when w 1 or w n, and a double-level blocking version of the algorithm is derived in a straightforward manner by recursively calling the single-level block algorithm to perform the QR factorization of the m x w matrix A.</p><p>Jalby and Philippe have considered the stability of this block algorithm <ref type="bibr" target="#b107">[106]</ref> and Gallivan et al. have analyzed the performance as a function of blocksize [67]. Below, a summary of this blocksize analysis is presented along with experimental results on an Alliant FX/8 of single and double-level versions of the algorithm.</p><p>The analysis is more complex than that of the block LU algorithm for diagonally dominant matrices discussed above, but the conclusions are similar. This increase in complexity is due to the need to apply a BLAS2-based MGS primitive to an rn w matrix at every step of the algorithm. As with the block version of the LU factoriza- tion with partial pivoting, this portion of each step makes poor use of the cache and increases the amount of work done in less efficient BLAS2 primitives. The analysis of the arithmetic time component clearly shows that the potential need for double-level blocking is more acute for this algorithm than for the diagonally dominant block LU factorization on problems of corresponding size.</p><p>The behavior of the algorithm with respect to the number of data loads can be discussed most effectively by considering approximations of the cache-miss ratios. For the interval 1 &lt;_ w &lt;_ ,, CS/m the cache-miss ratio is where is proportional to l/n, which achieves its minimum value m/(2CS) at w 1.</p><p>Under certain conditions the cache-miss ratio continues to decrease on the interval &lt; w &lt; n where it has the form 1 (7) W(nl__ 1 / #ww 1--+n -+ where /2 is proportional to l/n, which reaches its minimum at a point less than and increases thereafter, as expected. (See [67] for details.) When w n the cache- miss ratio for the second interval is 1/2 corresponding to the degeneration from a BLAS3 method to a BLAS2 method. The composite cache-miss ratio function over both intervals behaves like a hyperbola before reaching its minimum; therefore the cache-miss ratio does not decline as rapidly in latter parts of the interval as it does near the beginning. 12. Performance of one-level block MGS on an Alliant FX/8.</p><p>A load analysis of the double-level algorithm shows that double-level blocking either reduces or preserves the cache-miss ratio of the single-level version while im- proving the performance with respect to the arithmetic component of time.</p><p>Figures <ref type="figure" target="#fig_21">12</ref> and<ref type="figure" target="#fig_16">13</ref> illustrate, respectively, the results of experiments run on an Alliant FX/8, using single-level and double-level versions of the algorithm applied to square matrices. The cache size on this particular machine is 16K double precision words. For the range of n, the order of the matrix, shown in Fig. <ref type="figure" target="#fig_7">12</ref>, the single-level optimal blocksize due to the data loading analysis starts at w 64, decreases to w 21 for n 768, and then increases to w 28 at n 1024. Analysis of the arithmetic time component recommends the use of a blocksize between w 16 and w 32. Therefore, due to the hyperbolic nature of # and the arithmetic time component analysis it is expected that the performance of the algorithm should increase until w 32. The degradation in performance as w increases beyond this point to, say w 64 or 96, should be fairly significant for small and moderately sized systems due to the rather large portion of the operations performed by the BLAS2 MGS primitive. The results of the experiments confirm the trends predicted by the theory. The version using w 32 is clearly superior. The performance for w 8 is uniformly dismal across the entire interval since the blocksize is too small for both data loading overhead and arithmetic time considerations. Note that as n increases the gap in performance between the w 32 version and the larger blocksize versions narrows. This is due to both arithmetic time considerations as well as data loading. As noted above, for small systems, the distribution of operations reduces the performance of the larger blocksize version; but, as n increases, this effect decreases in importance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mflops</head><p>(Note that this narrowing trend is much slower than that observed for the block LU Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php algorithm. This is due to the fact that the fraction of the total operations performed in the slow primitive is win for the block Gram-Schmidt algorithm and only w2/n 2 for the block LU.) Further, for larger systems, the optimal blocksize for data loading is an increasing function of n; therefore, the difference in performance between the three larger blocksizes must decrease.</p><p>Figure <ref type="figure" target="#fig_16">13</ref> shows the increase in performance which results from double-level block- ing. Since the blocksize indicated by arithmetic time component considerations is between 16 and 32 these two values were used as the inner blocksize . For 0 16 the predicted outer blocksize ranges from w 64 up to w 128; for 0 32 the range is w 90 to w 181. (Recall that the double-level outer blocksize is influ- enced by the cache size only by virtue of the fact that is used as a maximum cutoff point.) For these experiments the outer blocksize of w 96 was used for two reasons. First, it is a reasonable compromise for the preferred outer blocksize given the two values of 0. Second, the corresponding single-level version of the algorithm, i.e., (0,w) (96, 96), did not yield high-performance and a large improvement due to altering 0 would illustrate the power of double-level blocking. (To emphasize this point the curve with (0, w) (96, 96) is included.) The curves clearly demonstrate that double-level blocking can improve the performance of the algorithm significantly.</p><p>(See [67] for details.)</p><p>5.1.3. Pipelined Givens rotations. While the pipelined implementation of Givens rotations is traditionally restricted to distributed memory and systolic type architectures, e.g., <ref type="bibr" target="#b81">[80]</ref>, it has been successful on shared memory machines in some settings. In <ref type="bibr" target="#b49">[48]</ref> a version of the algorithm was implemented on the HEP and com- pared to parallel methods based on Householder transformations. Rather than using the standard row-oriented synchronization pattern, the triangular matrix R was parti- tioned into a number of segments which could span row boundaries. Synchronization of the update of the various segments was enforced via the HEP's full-empty mech- anism. The resulting pipelined Givens algorithm was shown to be superior to the Householder based approaches.</p><p>Gallivan and Jalby have implemented a version of the traditional systolic algorithm (see <ref type="bibr" target="#b81">[80]</ref>) adapted to efficiently exploit the vector registers and cache of the Alliant FX/8. The significant improvement in performance of a structural mechanics code due to Berry and Plemmons, which uses weighted least squares methods to solve stiffness equations, is detailed in <ref type="bibr" target="#b9">[10]</ref> (see also [144], <ref type="bibr" target="#b146">[145]</ref>).</p><p>The hybrid scheme for LU factorization discussed earlier for cluster-based shared memory architectures converts easily to a rotation-based orthogonal factorization, see <ref type="bibr" target="#b159">[157]</ref>. Chu and George have considered a variation of this scheme for shared memory architectures <ref type="bibr" target="#b31">[31]</ref>. The difference is due to the fact that Sameh exploited the hybrid nature of the clustered memory and kept most of the matrix stored in a distributed fashion while pipelining between clusters the rows used to eliminate elements of the matrix. Chu and George's version keep these rows local to the processors and move the rows with elements to be eliminated between processors.</p><p>5.2. Distributed memory multiprocessors.</p><p>5.2.1. Orthogonal factorization. Our purpose in this section is to survey par- allel algorithms for solving (10) on distributed memory systems. In particular, we discuss some algorithms for the orthogonal factorization of A. Several schemes have been proposed in the past for the orthogonal factorization of matrices on distributed memory systems. Many of them deal with systolic arrays and require the use of O(n2) [80] all consider Givens reduction and require a triangular array of O(n2) processors, while Luk <ref type="bibr" target="#b126">[125]</ref> uses a mesh connected array of O(n2) processors. Sameh <ref type="bibr" target="#b160">[158]</ref>, on the other hand, considers both Givens and Householder reduction on a ring of pro- cessors in which the number of processors is independent of the problem size. Each processor possesses a local memory with one processor only handling the input and output. Figure <ref type="figure" target="#fig_11">14</ref> shows the organization of Givens reduction on three processors for a rectangular matrix of seven rows and five columns on such a ring. Each column depicts the operations taking place in each processor. An entry ij, j &lt; i, indicates the rotation of rows and j so as to annihilate the ith element of row j.</p><p>Recall that the classical Householder reduction may be described as follows. Let aj(k) denote the jth column of Ak, where Ak+l QkAk in which Qk diag(Ik, Pk).</p><p>Here, Ak is upper triangular in its first (k-1) rows and columns with Pk being the elementary reflector of order (m-k + 1) that annihilates all the elements below the diagonal of the kth column of Ak. Then Householder reduction on the same matrix and ring architecture as above may be organized as shown in Fig. <ref type="figure" target="#fig_17">15</ref>. Here, a Pk alone indicates generation of the kth elementary reflector.</p><p>Modi and Clarke <ref type="bibr" target="#b135">[134]</ref> have suggested a greedy algorithm for Givens reduction and the equivalent ordering of the rotations, but do not consider a specific architecture or communication pattern. Cosnard, Muller, and Robert <ref type="bibr" target="#b32">[32]</ref> have shown that the greedy algorithm is optimal in the number of timesteps required. Theoretical studies and comparisons of such algorithms for Givens reduction have been given by Pothen, Somesh, and Vemulapati <ref type="bibr" target="#b149">[148]</ref> and by Elden <ref type="bibr" target="#b55">[54]</ref>. We now briefly survey some of these algorithms that have been implemented on current commercially available distributed memory multiprocessors.</p><p>In chronological order, we begin with the work of Chamberlain and Powell <ref type="bibr" target="#b25">[25]</ref>.</p><p>In this study the coefficient matrix A is stored by rows across the processors in the usual wrap fashion and most of the rotations involve rows within one processor in a type of divide-and-conquer scheme. However, it is necessary to carry out rotations  involving rows in different processors, which they call merges. They describe two ways of implementing the merges and compare them in terms of load balance and communication overhead. Numerical tests were made on an Intel iPSC hypercube with 32 processors based on 80287 floating point coprocessors to illustrate the practicality of their algorithms. The schemes used here are very similar the basic approach suggested originally by Golub, Plemmons, and Sameh <ref type="bibr" target="#b82">[81]</ref> and developed further in <ref type="bibr" target="#b146">[145]</ref>. We note that Katholi and Suter <ref type="bibr" target="#b113">[112]</ref> have also adopted this approach in developing an orthogonal factorization algorithm for shared memory systems, and have performed tests on a 30 processor Sequent Balance computer.</p><p>Chu and George <ref type="bibr" target="#b30">[30]</ref> have also suggested and implemented algorithms for perform- ing the orthogonal factorization of a dense rectangular matrix on a hypercube multi- processor. Their recommended scheme involves the embedding of a two-dimensional grid in the hypercube network, and their analysis of the algorithm determines how the aspect ratio of the embedded processor grid should be chosen in order to minimize the execution time or storage usage. Another feature of the algorithm is that redundant computations are incorporated into a communication scheme which takes full advantage of the hypercube connection topology; the data is always exchanged between neighboring processors. Extensive computational experiments which are reported by the authors on a 64-processor Intel hypercube support their theoretical performance analysis results.</p><p>Finally in this section we mention two studies which directly compare the results of implementations of Givens rotations with Householder transformations on local memory systems. Pothen and Raghavan <ref type="bibr" target="#b148">[147]</ref> have compared the earlier work of Pothen, Somesh, and Vemulapati <ref type="bibr" target="#b149">[148]</ref> on a modified version of a greedy Givens scheme with a standard row-oriented version of Householder transformations. Their tests seem to indicate that Givens reduction is superior on such an architecture. Kim, Agrawal, and Plemmons <ref type="bibr" target="#b114">[113]</ref>, however, have developed and tested a row-block version of the Householder transformation scheme which is based upon the divide-and-conquer approach suggested by Golub, Plemmons, and Sameh <ref type="bibr" target="#b82">[81]</ref> (see also <ref type="bibr" target="#b29">[29]</ref>). The tests by Kim, AgrawM, and Plemmons on a 64-processor Intel hypercube clearly favor their modified Householder transformation scheme. 5.2.2. Recursive least squares. In recursive least squares (RLS) it is required to recalculate the least squares solution vector x when observations (i.e., equations) Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php are successively added to or deleted from (10) without resorting to complete refactor- ization of the matrix A. For example, in many applications infornation continues to arrive and must be incorporated into the solution x. This is called updating. Alternatively, it is sometimes important to delete old observations and have their effects excised from x. This is called downdating. Applications of RLS updating and down- dating include robust regression in statistics, modification of the Hessian matrix in certain optimization schemes, and in estimation methods in adaptive signal processing and control.</p><p>There are two main approaches to solving RLS problems; the information matrix method based on modifying the triangular matrix R in <ref type="bibr" target="#b10">(11)</ref>, and the covariance matrix method based instead on modifying the inverse R-1. In theory, the information matrix method is based on modifying the normal equations matrix ATA, while the covariance matrix method is based on modifying the covariance matrix P (ATA) -.</p><p>The covariance matrix P measures the expected errors in the least squares solution x to <ref type="bibr" target="#b9">(10)</ref>. The Cholesky factor Rfor P is readily available in control and signal processing applications.</p><p>Various algorithms for modifying R in the information matrix approach due to updating or downdating have been implemented on a 64-node Intel hypercube by Henkel, Heath, and Plemmons <ref type="bibr" target="#b93">[92]</ref>. They make use of either plane rotations or hy- perbolic type rotations.</p><p>The process of modifying least squares computations by updating the covariance matrix P has been used in control and signal processing for some time in the context of linear sequential filtering. We begin with estimates for P R-R -T and x, and update R -1 to /-and x to : at each recursive timestep. Recently Pan and</p><p>Plemmons <ref type="bibr" target="#b141">[140]</ref> have described the following parallel scheme.</p><p>Algorithm (Covariance Updating). Given the current least squares estimate vector x, the current factor L =_ R -T of P (ATA) -1 and the observation yTx a being added, the algorithm computes the updated factor /-of/5 and the updated least squares estimate vector 2 as follows:</p><p>1. Form the matrix vector product 12)</p><p>a Ly.</p><p>2. Choose plane rotations Qi, to form and update L (15)</p><p>1 u(a yTx). As the recursive least squares computation proceeds, replaces L, 2 replaces x, a new equation is added, and the process returns to step 1. An efficient parallel im- plementation of this algorithm on the hypercube distributed-memory system making use of bidirectional data exchanges and some redundant computation is given in <ref type="bibr" target="#b94">[93]</ref>.</p><p>Steps 1 and 3 are highly parallelizable and effective implementation details of step 2 on a hypercube are given in <ref type="bibr" target="#b94">[93]</ref>.</p><p>Table <ref type="table" target="#tab_33">2</ref> shows the speedup and efficiency on an iPSC/2 hypercube (4 MB of memory for each processor) for a single phase of the algorithm on a test problem of One complete recursive update is performed. Here, the speedup is speedup size n 1024. given by, with the corresponding efficiency, time on 1 processor time on p processors' efficiency speedup</p><p>An alternative hypercube implementation of the RLS scheme of Pan and Plem- mons <ref type="bibr" target="#b141">[140]</ref> has been given by Chu and George [311. Eigenvalue and singular value problems.</p><p>6.1. Eigenvalue problems. Solving the algebraic eigenvalue problem, either standard Ax Ax, or generalized Ax ,Bx, is an important and potentially time- consuming task in numerous applications. In this brief review, only the dense case is considered for both the symmetric and nonsymmetric problems. Most of the parallel algorithms developed for the dense eigenvalue problem have been aimed at the stan- dard problem. Algorithms for handling the generalized eigenvalue problem on shared or distributed memory multiprocessors are very similar to those used on sequential machines. Reduction of the symmetric generalized eigenvalue problem to the stan- dard form is achieved by a Cholesky factorization of the symmetric positive definite matrix B which is well-conditioned in most applications. This reduction process can be made efficient on shared memory multiprocessors, for example, by adopting a block Cholesky scheme similar to the block LU decomposition discussed earlier to obtain the Cholesky factor L of B and to explicitly form the matrix L-1AL -T using the appropriate BLAS3. For the nonsymmetric generalized eigenvalue problems where the matrix B is known to be often extremely ill-conditioned in many applications, there is no adequate substitute to Moler and Stewart's QZ-scheme <ref type="bibr" target="#b137">[136]</ref>. On a shared memory multiprocessor, the most efficient stage is the initial one of reducing B to the upper triangular form. Dispensing thus with the generalized eigenvalue problems, the Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php remainder of the section will be divided between procedures that depend on reduction to a condensed form, and Jacobi or Jacobi-like schemes for both the symmetric and nonsymmetric standard eigenvalue problems.</p><p>6.1.1. Reduction to a condensed form. We start with the nonsymmetric case. For the standard problem the first step, after balancing, is the reduction to upper Hessenberg form via orthogonal similarity transformations. These usually consist of elementary reflectors which could yield high computational rates on vector machines provided appropriate BLAS2 kernels are used. On parallel computers with hierarchical memories, block versions of the classical scheme, e.g., see <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b45">[44]</ref>, <ref type="bibr" target="#b87">[86]</ref>, yield higher performance than BLAS2-based versions. Such block schemes are similar to those discussed above for orthogonal factorization, and their use does not sacrifice numerical stability. Block sizes can be as small as 2 for certain architectures. For the sake of illustration we present a simplified scheme for this block reduction to the upper Hessenberg form, where we assume that the matrix A is of order n where n ku + 2. 10 do j= 1, k do i= (j-1)u + 1, ju Obtain an elementary reflector Pi I-wiw such that Pi annihilates the last n-i-1 elements of the ith column of A Construct:</p><formula xml:id="formula_22">Ui (Ui-1, wi) V Pi Vi-1, w Yi (Y-1, Awl) Zi YiT i + ifi=jugotol0 ai+l (I-ViUiT)(ai+l Yizi) enddo A(ju + l'n) (I-V,ujT,)(A(ju + l'n)-Yj,Zj,) enddo.</formula><p>Here, Zm consists of the last (n rn) rows of Vm. This block scheme requires more arithmetic operations than the classical algorithm using elementary reflectors by a factor of roughly 1 + 1/k. Performance of the block scheme on the Alliant FX/8 is shown in Fig. <ref type="figure" target="#fig_18">16</ref>  <ref type="bibr" target="#b87">[86]</ref>. The performance shown is based on the operation count of the nonblock algorithm.</p><p>The next stage is that of obtaining the eigenvalues of the resulting upper Hessenberg matrix via the QR-algorithm with an implicit shifting strategy. This algorithm consists mainly of chasing a bulge represented by a square matrix of order 3 whose diagonal lies along the subdiagonal of the upper Hessenberg matrix. This in turn affects only 3 rows and columns of the Hessenberg matrix, leaving little that can be gained from vectorization, and to a lesser extent, parallelization. Stewart has consid- ered the implementation of this basic iteration on a linear array of processors <ref type="bibr" target="#b174">[172]</ref>.</p><p>More recently, a block implementation with multiple QR shifts was proposed by Bai and Demmel <ref type="bibr" target="#b5">[6]</ref> which yields some advantage for vector machines such as the Convex C-1 and Cyber 205.</p><p>If we are seeking all of the eigenvectors as well, the performance of the algorithm is enhanced since the additional work required consists of computations that are Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php amenable to vector and/or parallel processing; that of updating the orthogonal matrix used to reduce the original matrix to Hessenberg form. Similarly, the most common method for handling the standard dense symmetric eigenvalue problem consists of first reducing the symmetric matrix to the tridiagonal form via elementary reflectors followed by handling the tridiagonal eigenvalue prob- lem. Such reduction can be achieved by a minor modification of the above block reduction to the Hessenberg form. On 1 CPU of a CRAY X-MP, with an 8.5 ns clock, a BLAS2 implementation of Householder tridiagonalization using rank-2 updates (see <ref type="bibr" target="#b43">[43]</ref>) yields a computational rate of roughly 200 Mflops for matrices of order 1000 (see Fig. 17 <ref type="bibr" target="#b88">[87]</ref>. The performance of Eispack's TRED2 is also presented in the figure for comparison. Figure <ref type="figure" target="#fig_11">18</ref> shows a comparison of the performance of this BLAS3-based block reduction with a BLAS2-based reduction on the Alliant FX/8 [86]. As before, the performance is computed based on the nonblock version operation count. Once the tridiagonal matrix is obtained two approaches have been used, on se- quential machines, for obtaining its eigenvalues and eigenvectors. If all the eigenvalues are required a QR-based method is used. The classical procedure is inherently sequen- tial, offering nothing in the form of vectorization or parallelism. Recently, Dongarra</p><p>and Sorensen <ref type="bibr" target="#b50">[49]</ref>, adapted an alternative due to Cuppen <ref type="bibr" target="#b33">[33]</ref> for the use on multipro-  In its sinplest form, the main idea of the algorithm may be outlined as follows.</p><p>Let T (i, ai, +1) be the symmetric tridiagonal matrix under consideration, where we assume that none of its off-diagonal elenents/ vanishes. Assuming that it is of order 2m, it can be written as, e e m where each T is tridiagonal of order m, 7 is a "carefully" chosen scalar, and e is the ith column of the identity of order m. This in turn can be written as, T diag(T1, T2) + ')'VV T in which the scalar and the column vector v can be readily derived. Now, we have two tasks: namely obtaining the spectral decomposition of T1 and T2, i.e., Ti QiDQ, 1, 2, where Q is an orthogonal matrix of order rn and D is diagonal. Thus, if Q diag(Q1, Q2) and D diag(D1,D2), then T is orthogonally similar to a rank-1 Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php perturbation of a diagonal matrix, i.e., QTQ T D + OZZ T, where p and z are trivially obtained from -), and z. The eigenvalues of T are thus the roots of (A) 1 + pzT(D-/I)-lz and its eigenvectors are given by, ui "r(D ,i I)z, where -lID hilIl2. This module may be used recursively to produce a parallel counterpart to Cuppen's algorithm [3al as demonstrated in <ref type="bibr" target="#b50">[49]</ref>. For example, if the tridiagonal matrix T is of order 2km, then the algorithm will consist of obtaining the spectral decmnpo- sition of 2 k tridiagonal matrices each of order m, followed by k stages in which stage j consists of applying the above module simultaneously to 2 k-j pairs of tridiagonal matrices in which each is of order 2J-lm.</p><p>If eigenvalues only (or all those lying in a given interval) or selected eigenpairs are desired, then a bisection-inverse iteration combination is used, e.g., see Wilkinson</p><p>and Reinsch <ref type="bibr" target="#b197">[195]</ref> or Parlett <ref type="bibr" target="#b142">[141]</ref>. Such a combination has been adapted for the Illiac IV parallel computer, e.g., see <ref type="bibr" target="#b119">[118]</ref> and [1021, and later for the Alliant FX/8, see <ref type="bibr" target="#b124">[123]</ref>. This modification depends on a multisectioning strategy in which the interval containing the desired eigenvalues is divided into (p-1) subintervals where p is the number of processors. Using the Sturm sequence property we can simultaneously de- termine the number of eigenvalues contained in each of the (p-1) subintervals. This is accomplished by having each processor evaluate the well-known linear recurrence leading to the determinant of the tridiagonal matrix T-#I or the corresponding nonlinear recurrence so as to avoid overor underflow, e.g., see <ref type="bibr" target="#b142">[141]</ref>. This process is repeated until all the eigenvalues, or clusters of computationally coincident eigenval- ues, are separated. This "isolation" stage is followed by the "extraction" stage where the separated eigenvalues are evaluated using a root finder which is a hybrid of pure bisection and the combination of bisection and the secant methods, namely the ZE-ROIN procedure due to Brent and Dekker, see <ref type="bibr" target="#b59">[58]</ref>. If eigenvectors are desired, then the final stage consists of a combination of inverse iteration and orthogonalization for those vectors corresponding to poorly separated eigenvalues.</p><p>This scheme proved to be the most effective on the Alliant FX/8 for obtaining all or few of the eigenvalues only. Compared to its execution time on one CE, it achieves a speedup of 7.9 on eight CE's, and is more than four times faster than Eispack's TQL1, e.g., see <ref type="bibr" target="#b169">[167]</ref> or <ref type="bibr" target="#b197">[195]</ref>, for the tridiagonal matrix [-1,2,-1] of order 500 with the same achievable accuracy for the eigenvalues. Even if all the eigenpairs of the above tridiagonal matrix are required, this multisectioning scheme is more than 13 times faster than the best BLAS2-based version of Eispack's TQL2, 27 times faster than Eispack's pair Bisect and Tinvit, and five times faster than its nearest com- petitor, parallel Cuppen's procedure <ref type="bibr" target="#b50">[49]</ref>, with the same accuracy in the computed eigenpairs. For matrices with clusters of poorly separated eigenvalues, however, the multisectioning algorithm may not be competitive if all the eigenpairs are required with high accuracy. For example, for the well-known  FIG. 18. Reduction to tridiagonal form on Alliant FX/8. see <ref type="bibr" target="#b196">[194]</ref>, which have pairs of very close eigenvalues, the multisectioning method re- quires roughly twice the time required by the parallel Cuppen's procedure in order to achieve the same accuracy for all the eigenpairs.</p><p>Further studies by Simon <ref type="bibr" target="#b168">[166]</ref> demonstrate the robustness of the above multisec- tioning strategy conpared to other bisection-inverse iteration combinations proposed in <ref type="bibr" target="#b7">[8]</ref>. Also, comparisons between the above multisectioning scheme and parallel Cup- pen's algorithm have been given by Ipsen and Jessup on hypercubes <ref type="bibr" target="#b104">[103]</ref> indicating the effectiveness of multisectioning on distributed memory multiprocessors for cases in which the eigenvalues are not pathologically clustered.</p><p>6.1.2. Jacobi and Jacobi-like schemes. An alternative to reduction to a con- densed form is that of using one of the Jacobi schemes for obtaining all the eigenvalues or all the eigenvalues and eigenvectors. Work on such parallel procedures dates back to the Illiac IV distributed memory parallel computer, e.g., see <ref type="bibr" target="#b154">[152]</ref>. Algorithms for han- dling the two-sided Jacobi scheme for the symmetric problem, which are presented in that work, exploit the fact that independent rotations can be applied simultaneously. Furthernore, several ordering schemes of these independent rotations are presented that minimize the number of orthogonal transformations (i.e., direct sum of rotations) Efficient direct methods for solving the finite-difference approximation of the Pois- son equation on the unit square have been developed by Buneman <ref type="bibr" target="#b20">[20]</ref>, Hockney <ref type="bibr" target="#b97">[96]</ref>, <ref type="bibr" target="#b98">[97]</ref>, and nuzbee, Golub, and Nielson <ref type="bibr" target="#b22">[22]</ref>. The most effective sequential algorithm combines the block cyclic reduction and Fourier analysis schemes. This is Hockney's FACR(1) algorithm <ref type="bibr" target="#b98">[97]</ref>. Excellent reviews of these methods on sequential machines have been given by Swarztrauber <ref type="bibr" target="#b179">[177]</ref> and Temperton <ref type="bibr" target="#b184">[182]</ref>, [183]. In [177] it is shown that the asymptotic operation count for FACR(1) on an n n grid is O(n21og21og2n), and is achieved when the number of the block cyclic reduction steps preceding Fourier analysis is taken approximately as (log21og2n). Using only cyclic reduction, or Fourier analysis, to solve the problem on a sequential machine would require O(n21og2n) arith- metic operations.</p><p>Buzbee <ref type="bibr" target="#b21">[21]</ref> observed that Fourier analysis, or the matrix decomposition Pois- son solver (MD-Poisson solver), is ideally suited for parallel computation. It consists of performing a set of independent sine transforms, and solving a set of indepen- dent tridiagonal systems. On a parallel computer consisting of n 2 processors, with an arbitrarily powerful interconnection network, the MD-Poisson solver for the two- dimensional case requires O(log2n parallel arithmetic steps <ref type="bibr" target="#b162">[160]</ref>. It can be shown, <ref type="bibr" target="#b143">[142]</ref> and [173], that a perfect shuffle interconnection network is sufficient to keep the communication cost to a minimum. Ericksen <ref type="bibr" target="#b56">[55]</ref> considered the implementation of FACR(1), [97], and CORF, <ref type="bibr" target="#b22">[22]</ref>, on the ILLIAC IV; and Hockney [98] compared the performance of FACR(l) on the CRAY-1, Cyber-205, and the ICL-DAP.</p><p>7.1. A domain decomposition MD-scheme. We consider first the MD-algor- ithm for solving the 5-point finite difference approximation of the Poisson equation on the unit square with a uniform n n grid, where for the sake of illustration we consider only Dirichlet boundary conditions. The multiprocessor version algorithm presented below can be readily modified to accommodate Neumann and periodic boundary conditions. Using natural ordering of the grid points, we obtain the well-known linear system of order n2: where T [-1, 4,-1] is a tridiagonal matrix of order n. This parallel MD-scheme consists of 3 stages <ref type="bibr" target="#b156">[154]</ref>: Stage 1. Each cluster j, 1 _&lt; j _&lt; 4 (a four cluster Cedar is assumed), forms the subvectors f(j-1)q+l, f(j-lq+2,'",faqofthe right-hand side, where q-n/4. Next each cluster j obtains 0 (gj_l)q+,...,g), where gk Qfk, in which , is the eigenvector matrix of T.</p><p>This amounts to performing in each cluster q sine transforms each of length n. Now where each cluster memory contains one block row. Here, v (v(j_l)q+l,.--Vq) with vk Quk, M I-In, A,--In] is a block tridiagonal matrix of order qn, and This system, in turn, may be reduced to,</p><formula xml:id="formula_23">G Iqn F 2 h2 G Iqn F )3 3 G Iqn )4</formula><p>h4 T where it}" (hj_)q+,..., hjq), F and G are given by" Mj 9j, 1 &lt;_ j &lt;_ 4, MF E, and MG ET. Observing that M consists of n independent tridiagonal matrices Tk [--1, Ak,--1] each of order q, where Ak 4-2cos(k/[n + 1]), k 1,2,...,n, the right-hand side of the above system is obtained by solving in each cluster j the n independent systems Tkrk 8k, for k 1,2, n, where Sk T for ekg(j_l)q+i, and rk T e k h(j-)q+i, 1, 2,..., q, and 1 &lt;_ j &lt;_ 4. Here, i and ei are the ith columns of Iq and In, respectively.</p><p>The matrices F and G can be similarly obtained by solving, in each cluster j, the independent systems TkCk 1, and Tkdk q, for k 1, 2,..., n. Since Tk is a Toeplitz matrix, however, we have Ck Jdk, where J [q,... ,1], see <ref type="bibr" target="#b112">[111]</ref> for example. As a result, in order to obtain F and G we need only solve in each cluster the n systems Tkdk q, k 1, 2,..., n. Hence, F and G are of the form, (n) (k)</p><p>,'",'Yi ), in which "/i eck, for 1,2,..-,q, and k 1,2,...,n.</p><p>Stage 2. From the structure of (7.1) it is seen that the three pairs of n equations above and below each partition are completely decoupled from the rest of the n 2 . This reduced system, of order 6n, consists of interlocking blocks of the form: I F 0 0 F In 0 F F 0 I F 0 0 F I 0 F F 0 I F F I</p><p>This system, in turn, comprises n independent pentadiagonal systems each of order 6, which can be solved in a very short time.</p><p>Stage 3. Now, that the subvectors Vkq, Vkq+l, k 1,2,3, are available, each cluster j obtains V(j-1)q+i h(j-1)q+i (Fiv(j-1)q n t-Fq-i+l Vjq+l for 2,3,...,q-1, where v0 Vaq+i 0. Finally, each cluster j retrieves the q subvectors U(j_l)q+ Qv(j-)q+i, for 1, 2,...,q, of the solution via q sine transforms, each of length n.</p><p>Note that one of the key computational kernels in this algorithm is the calculation of multiple sine transformations. In order to design an efficient version of this kernel it is necessary to perform an analysis of the influence of the memory hierarchy similar to that presented above for the block LU algorithm. Such an analysis is contained in 7.2. A modified block cyclic reduction. The discretization of the separable elliptic equation Ou Ou <ref type="bibr" target="#b15">(16)</ref> a(X)x2 + b(x)-x + c(x)u + --Oy 2 f(x, y) with Dirichlet boundary conditions and a five-point stencil on a naturally ordered n x m grid defined on a rectangular region leads to a system of the form Au f. In this case A is the n block tridiagonal matrix diag[-I, A,-I], where A, I are respectively tridiagonal and identity matrices of order m. Block cyclic reduction (BCR) dates back to the work of Hockney and was presented in <ref type="bibr" target="#b22">[22]</ref> in its stabilized form due to Buneman. The work in <ref type="bibr" target="#b178">[176]</ref>, <ref type="bibr" target="#b180">[178]</ref>, <ref type="bibr" target="#b182">[180]</ref> resulted in the development of FISHPAK, a package based on BCR for the solution of ( <ref type="formula">16</ref>) and extensions thereof. BCR is a rapid elliptic solver (RES) having sequential computational complexity O(nm log n).</p><p>Assuming that n 2 k 1, the idea of the method for reduction steps r 1,..., k 1 is to combine the current 2 k-r+ 1 vectors into 2 k-1 ones, and then solve a system of the form p2r--l (A)X Y</p><p>where Y E Nmx (2 k--) and p2-1 (A) is a Chebyshev polynomial of degree 2 -1 in A. Since its roots A} -1) are known, it can be written in product form, where each factor is tridiagonal. Hence the system to be solved becomes <ref type="bibr" target="#b17">(17)</ref> H (A-,r-1)I)[Xll... Clearly as r increases, the effectiveness of a parallel or vector machine to handle <ref type="bibr" target="#b17">(17)</ref> decreases rapidly.</p><p>A parallel version of BCR was recently discovered <ref type="bibr" target="#b71">[70]</ref>, [181]. In summary, the method is based in expressing the matrix rational function [p2-1 (A)] -1 as a partial fraction, i.e., as a linear combination of the 2 r-1 components (A-Ar-1)I) -1 2r-1 (,'-i) A-i) <ref type="bibr" target="#b7">(8)</ref> [I I2--i]  {Pit (r--1)</p><p>Coefficients ci are equal to 1/ 2- )) and can be derived analytically.</p><p>Figure <ref type="figure" target="#fig_23">19</ref> shows the performance of the parallel and standard BCR on the Alliant FX/8. For a discussion of parallel BCR on distributed memory machines see <ref type="bibr" target="#b74">[73]</ref>, <ref type="bibr" target="#b181">[179]</ref>.</p><p>Partial fraction decomposition can also be applied to the parallel solution of parabolic equations. See <ref type="bibr" target="#b72">[71]</ref>, <ref type="bibr" target="#b73">[72]</ref> for details. 7.3. Boundary integral domain decomposition. A new method (BIDD) was recently proposed for the solution of Laplace's equation <ref type="bibr" target="#b69">[68]</ref>, [69]. The method is Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php characterized by the decoupling of the problem into independent subproblems on sub- domains. An approximation 5 to the solution u is sought as a finite linear combination loglz-wjl of V2u 0: of N fundamental solutions <ref type="bibr" target="#b129">[128]</ref> Cj(z)-- <ref type="bibr" target="#b19">(19)</ref> N j=l For a given set of N points wj lying outside the domain, a E NN is computed to minimize IIg-Gallo for some norm p. G E NvxN is the influence matrix consisting of fundamental solutions based at wj for each boundary point, g Nv consists of boundary values for u. Once a has been computed, the solution at any # points on the domain is t Ha, with H ttxg being the influence matrix for the # points. Choosing these # points to be subdomain boundary points, we can compute the solution by applying the elliptic solvers most suitable for each subdomain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIG. 1 .</head><label>1</label><figDesc>FIG.1. Some memory/processor topologies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Downloaded 11 /FIG. 4 .</head><label>114</label><figDesc>FIG. 4. Performance of rank-k update with ml 96 on an Alliant FX/8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FIG. 5 .</head><label>5</label><figDesc>FIG. 5. Performance of rank-k update with ml 128 on an Alliant FX/8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>block column-sweep algorithm may then be described as" The B_Col_Sweep p_n_ do j =O,p-2 solve Li{)xJ) f}J) via Col_Sweep or Row_Sweep end do solve L(p-1)x (p-i) f(P-i) via Col_Sweep or Row_Sweep.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>L () and f() D()f () are obtained by simple multiplication.Eventually, L (lg(n/m))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>4. 1 . 2 .</head><label>12</label><figDesc>Performance analysis.Gallivan et al.  have applied the decoupling meth- odology to Version 5<ref type="bibr" target="#b68">[67]</ref>. Their results demonstrate many of the performance trends observed in the literature for the various forms of block methods. A summary of the important points follows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>FIG. 8 .</head><label>8</label><figDesc>FIG. 8. Performance of block LU on an Alliant FX/8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>FIG. 9 .</head><label>9</label><figDesc>FIG. 9. Performance of double-level block LU on an All!ant FX/8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>FIG. 10 .</head><label>10</label><figDesc>FIG. 10. Annihilation scheme for n8.   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>1 (</head><label>1</label><figDesc>a banded matrix of order q nip and bandwidth 2m + p-1, in which /i and (i+1 are lower and upper triangular matrices, respectively, each of order rn.The algorithm consists of three stages.Stage 1. If both sides of Ax f were premultiplied by diag(A-1, AI, A -) we obtain a system of the form Iq</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Stage 2 .</head><label>2</label><figDesc>Let/i and/i be partitioned, in turn, as follows</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>5. 1 . 1 .</head><label>11</label><figDesc>A block Householder reduction. If A A1 [a 1) a ) a(n )]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Downloaded 11 /FIG. 11 .</head><label>1111</label><figDesc>FIG. 11. Performance of block Householder algorithm on an Alliant FX/8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>FIG. 12. Performance of one-level block MGS on an Alliant FX/8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>FIG. 13 .</head><label>13</label><figDesc>FIG. 13. Performance of two-level block MGS on an Alliant FX/8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>FIG. 15 .</head><label>15</label><figDesc>FIG. 15. Householder reflectors on a three processor ring.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>FIG. 16 .</head><label>16</label><figDesc>FIG. 16. Reduction to Hessenberg form on Alliant FX/8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>FIG. 17 .</head><label>17</label><figDesc>FIG. 17. Reduction to tridiagonal form on CRAY X-MP (1 CPU).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Q [( 2 /</head><label>2</label><figDesc>In + 1])12sin(lmr/[n + 11)1, 1,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>FIG. 19 .</head><label>19</label><figDesc>FIG. 19. Parallel and standard BCR on n x n grid on Alliant FX/8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head></head><label></label><figDesc>[yll''" lY2/-r_l].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php cube connection is perhaps the most discussed distributed memory topology recently.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Commercially available hypercubes include those by Ametek, Intel, and NCUBE.</figDesc><table><row><cell>Cluster</cell><cell>Cluster</cell><cell>Cluster</cell><cell>Cluster</cell></row><row><cell></cell><cell></cell><cell cols="2">To Global Network</cell></row><row><cell></cell><cell>Cluster Memory</cell><cell></cell><cell></cell></row><row><cell></cell><cell>M emJry Bus</cell><cell></cell><cell></cell></row><row><cell>IP Cache</cell><cell>CE Cache</cell><cell></cell><cell></cell></row><row><cell>Interactive Processor</cell><cell>Interactive Processor</cell><cell>Cluster Switch</cell><cell></cell></row><row><cell></cell><cell cols="2">Computational</cell><cell>Computational</cell></row><row><cell></cell><cell></cell><cell>FAement</cell><cell>Hlement</cell></row><row><cell></cell><cell cols="2">Concurrency Control Bus</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">hLliant Designed</cell></row></table><note><p><p><p>FIG.</p>2</p>. The Cedar multiprocessor.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Computations </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>can </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>The effect of such overlapping is seen through a reduction in</figDesc><table><row><cell>Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php It follows that the relative cost of transferring data decreases rapidly and reaches a global minimum of the form</figDesc><table><row><cell>(8)</cell><cell>A</cell><cell>pA</cell><cell>A</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE</head><label></label><figDesc>Comparison of the four forms of the BLAS3 primitives.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Note that if parallelism across the blocks is used this form requires synchronization (which is typically done on a subblock level).</figDesc><table><row><cell></cell><cell>55</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>I,\ /</cell></row><row><cell></cell><cell>5O</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>m2</cell><cell>,m --'li, m8</cell></row><row><cell></cell><cell>45</cell><cell cols="2">II II II t! // , AI, V ',-ll P\I V' , N-%''o"'', " ," Ill' L t 64, m. 64, m3 B4</cell></row><row><cell>Mflops</cell><cell>40</cell><cell>----, I</cell></row><row><cell></cell><cell></cell><cell></cell><cell>\,/..,,%,</cell></row><row><cell></cell><cell>35</cell><cell></cell><cell>,m 32,n3 = 2</cell></row><row><cell></cell><cell>30</cell><cell></cell><cell>.J</cell><cell>L</cell></row><row><cell></cell><cell>25</cell><cell>128</cell><cell>256 384 512 640 768 896 1024</cell></row></table><note><p><p>do/= 1, k C C + Ai Bi end do.</p>FIG. 3. </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php PARALLEL DENSE LINEAR ALGEBRA ALGORITHMS</figDesc><table><row><cell>Row_oriented: do i= 2, n do j= 1, i-1 enddo enddo -and</cell></row><row><cell>Column_oriented</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table><row><cell>Proc. 1</cell><cell>Proc. 2</cell></row><row><cell>solve Lllxl f</cell><cell></cell></row><row><cell>f2 f2 L:x</cell><cell></cell></row><row><cell></cell><cell>solve L22x2 f2</cell></row><row><cell>f4 +--f4-L41Xl</cell><cell>f3 +--f3-L32x2</cell></row><row><cell>solve L33x3 f3</cell><cell>f4 +-f4 L42x2</cell></row><row><cell>f4 e--f4-L43x3</cell><cell></cell></row><row><cell>solve L44x4 f4</cell><cell></cell></row><row><cell cols="2">FIG. 6. Two processor DO-ACROSS synchronization pattern.</cell></row><row><cell>n p -g</cell><cell></cell></row><row><cell cols="2">solve LlXl -bl via Col_Sweep or Row_Sweep</cell></row><row><cell>do j 2,p</cell><cell></cell></row><row><cell>fj fy Cjx(j_l)</cell><cell></cell></row></table><note><p>solve Ljxj fj via Col_Sweep or Row_Sweep end do.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head></head><label></label><figDesc>Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table><row><cell>PARALLEL DENSE LINEAR ALGEBRA ALGORITHMS</cell><cell>95</cell></row><row><cell>-1</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head></head><label></label><figDesc>Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table><row><cell>else</cell></row><row><cell>enddo.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head></head><label></label><figDesc>Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head></head><label></label><figDesc>Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head></head><label></label><figDesc>Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table><row><cell>for 1&lt;k&lt;3.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head></head><label></label><figDesc>Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head></head><label></label><figDesc>Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table /><note><p>matrix.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head></head><label></label><figDesc>Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Ii0 K.A. GALLIVAN, R. J. PLEMMONS, AND A. H. SAMEH</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head></head><label></label><figDesc>Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30"><head></head><label></label><figDesc>Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Proc. 1 Proc. 2 Proc. 3 , where n is the number of columns of the matrix. For instance, Ahmed, Delosme, and Morph [4], Bojanczyk, Brent, and Kung [17], and Gentleman and Kung</figDesc><table><row><cell>21</cell><cell></cell><cell></cell></row><row><cell>31</cell><cell>2</cell><cell></cell></row><row><cell>41</cell><cell>32</cell><cell></cell></row><row><cell>51</cell><cell>42</cell><cell>3</cell></row><row><cell>61</cell><cell>52</cell><cell>43</cell></row><row><cell>71</cell><cell>62</cell><cell>53</cell></row><row><cell>4</cell><cell>72</cell><cell>63</cell></row><row><cell>54</cell><cell></cell><cell>73</cell></row><row><cell>64</cell><cell>5</cell><cell></cell></row><row><cell>74</cell><cell>65</cell><cell></cell></row><row><cell></cell><cell>75</cell><cell>6</cell></row><row><cell></cell><cell></cell><cell>76</cell></row><row><cell cols="3">FIG. 14. Givens reduction on a three processor ring.</cell></row></table><note><p>processors</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_31"><head></head><label></label><figDesc>Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Proc. 1 Proc. 2 Proc. 3</figDesc><table><row><cell>P Pla 1) Pla 1) P2 pa(4 1 D _(2)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_32"><head></head><label></label><figDesc>Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table /><note><p>2 x -g</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_33"><head>TABLE 2</head><label>2</label><figDesc>Speedup and efficiency on the iPSC/2 for a problem of size n 1024.</figDesc><table><row><cell cols="3">Number of Processors Speedup Efficiency</cell></row><row><cell>P</cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>4</cell><cell>3.90</cell><cell>0.98</cell></row><row><cell>16</cell><cell>15.06</cell><cell>O.94</cell></row><row><cell>64</cell><cell>48.6O</cell><cell>0.76</cell></row><row><cell>6.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_34"><head></head><label></label><figDesc>Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php cessors. This algorithm obtains all the eigenvalues and eigenvectors of the symmetric tridiagonal matrix.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_35"><head></head><label></label><figDesc>Wilkinson matrices W1+27, e.g., Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table><row><cell></cell><cell>35</cell><cell>.=I</cell><cell>t.</cell></row><row><cell></cell><cell>3O</cell><cell></cell><cell>:-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>BLAS3-based</cell></row><row><cell>Mflops</cell><cell>25 2O</cell><cell cols="2">"i 'I I 1 -//, 7/., ....T , I1' '\ I1' \ II \_.,,. II II '\ II '\ II ""..:</cell></row><row><cell></cell><cell>15</cell><cell></cell></row><row><cell></cell><cell>I0</cell><cell>,' l ' i ' , /</cell><cell>',', ''., &lt; , . -' , I-...,,</cell></row><row><cell></cell><cell>0</cell><cell cols="2">128 256 384 512 640 768 896 1024</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_36"><head></head><label></label><figDesc>Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php handling the Laplace equation on irregular domains that consist of regular domains; examples of such domains are the right-angle or T-shapes.</figDesc><table /><note><p><p>within each sweep. Much more work has been done since on this parallel</p>two-sided   </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_37"><head></head><label></label><figDesc>Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table><row><cell cols="3">PARALLEL DENSE LINEAR ALGEBRA ALGORITHMS</cell><cell>125</cell></row><row><cell>we have the system</cell><cell></cell><cell></cell></row><row><cell>M E E T M E E T M E E T M</cell><cell>)1 2 3 4</cell><cell>1 2 (13 !}4</cell></row><row><cell></cell><cell></cell><cell>T</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_38"><head></head><label></label><figDesc>Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table /><note><p><p>equations</p><ref type="bibr" target="#b163">[161]</ref></p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Scientific Research under grant AFOSR-88-0285 and by the National Science Foundation under grant DMS-85-21154.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Some register-based vector processors also have multiple ports to memory in an attempt to have the best of both worlds, e.g., one CPU of a CRAY X-MP. Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>A computation is said to have high data locality if the ratio of the data elements to the number of operations is small.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>The TEST-AND-SET operation allows for the indivisible action of accessing a memory location, testing its value, and setting the location if the test succeeds. It can be used as the basic building block of most synchronization primitives. Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p>K. A. GALLIVAN R. J. PLEMMONS, AND A. H.SAMEH   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>The term register size does not necessarily mean the vector length of a single vector register. It can also refer to the aggregate size of all of the vector registers used in a processor in a given implementation of the primitive. Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_6"><p>Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_7"><p>Pairwise pivoting can also be useful on shared memory machines to break the bottleneck caused by partial pivoting discussed earlier.Downloaded</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_8"><p>11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_9"><p>(1979), pp. 352-364. Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This research was supported by the Department of Energy under grant DE-FG02-85ER25001 and the National Science Foundation under grants NSF-MIP-8410110 and NSF-CCR-8717942.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> 3.5.2. <p>Distributed-memory triangular system solvers. A large number of papers have appeared for handling triangular systems on distributed memory archi- tectures (mainly rings and hypercubes), e.g., see Sameh <ref type="bibr" target="#b160">[158]</ref>, Romine and Ortega <ref type="bibr" target="#b153">[151]</ref>, Heath and Romine <ref type="bibr" target="#b90">[89]</ref>, <ref type="bibr">Li and</ref> Coleman <ref type="bibr" target="#b123">[122]</ref> and Eisenstat et al.  <ref type="bibr" target="#b54">[53]</ref>. Most are variations on the basic algorithms above adapted to exploit the distributed nature of the architectures. For such architectures, it is necessary to distinguish whether a given triangular matrix L is stored across the individual processor memories by rows or by columns. For example, suppose that the matrix [L, f] is stored by rows, then the above column-sweep algorithm becomes: Row_Storage: doj=l,n if is one of my row indices then communicate(broadcast, fan-out) y to each processor doi-j/ 1,n if is one of my row indices then enddo enddo.</p><p>Note first that the computations in the inner loop can be executed in parallel, and that on a hypercube with p 2 u processors, the fan-out communication can be accomplished in u stages. If the lower triangular matrix L is stored by columns then the column-sweep algorithm will cause excessive interprocessor communication. A less communication intensive column storage oriented algorithm has been suggested in <ref type="bibr" target="#b152">[150]</ref> and <ref type="bibr" target="#b153">[151]</ref>. Such an algorithm is based upon the classical sequential Row_sweep algorithm shown above.</p><p>In implementing the column storage algorithm on an Intel iPSC hypercube, for example, information is gathered into one processor from all others via a fan-in op- eration fan_in(T,i). Such an operation enables the processor whose memory contains column to receive the sum of all the T's over all processors. The parallel column storage algorithm can be described as follows:</p><p>if is one of my column indices then ( ) /.</p><p>enddo.</p><p>Here, during stage of the algorithm, the pseudo-routine fan_in(T, i) collects and sums the partial inner productsfrom each processor, leaving the result r] in the processor containing column i. Further modifications to the basic row-and column- oriented triangular solvers on distributed memory systems have been studied in <ref type="bibr" target="#b123">[122]</ref>,</p><p>there a communication scheme which allows for ring embedding into a hypercube show that these techniques do mitigate the conflict between reducing the arithmetic time component and the data loading overhead (see [67] for details). Figure <ref type="figure">9</ref> demon- strates that the use of inner-to-outer form of double-level blocking can indeed improve performance. Note that that double-level version yields performance higher than all of the single-level implementations of Fig. <ref type="figure">8</ref> over the entire interval.</p><p>4.2. Distributed-memory algorithms. Our objective here is to describe the effects that the data-storage and pivoting schemes have on the efficiency of the LU factorization of a dense matrix A (aid) on distributed memory systems. The related parallel Cholesky schemes will not be discussed in this section; for an example, see Heath <ref type="bibr" target="#b89">[88]</ref>. We also describe some LU-like factorization schemes that are useful on distributed memory and hybrid architectures. 4.2.1. LU factorization. A number of papers have appeared in recent years describing various parallel LU factorization schemes on such architectures, e.g., see   Ipsen, Saad, and Schultz <ref type="bibr" target="#b105">[104]</ref>, Chu and George <ref type="bibr" target="#b28">[28]</ref>, Geist and Heath [77], [78], and Geist and Romine <ref type="bibr" target="#b80">[79]</ref>. We will concentrate here only on the work of Geist and Romine.</p><p>Consider the two basic storage schemes: storage of A by rows and by columns.</p><p>The row storage case is considered first. Adopting the terminology of Geist and Romine <ref type="bibr" target="#b80">[79]</ref>, we refer to the following scheme as RSRP, Row Storage with Row Piv- oting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RSRP:</head><p>each processor executes the following, do k-O,n-1 determine row pivot update permutation vector if (I own pivot row) fan-out(broadcast) pivot row else receive pivot row for (all rows &gt; k that I own) doj=k+l,n-1 aid aid )ikakj enddo enddo.</p><p>In most of the early work, row storage for the coefficient matrix was chosen principally because no efficient parallel algorithms were then known to exist for the subsequent forward and backward sweeps if the coefficient matrix were to be stored by columns. But, as discussed earlier, recent triangular solvers for distributed memory multiprocessors have removed the main reason for preferring row storage. Next, the Column Storage with Row Pivoting (CSRP) scheme is given by: CSRP: each processor executes the following do k---O,n-1 if (I own column k) Downloaded 11/27/14 to 129.120.242.61. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Jacobi scheme for the symmetric eigenvalue problem. These have been motivated primarily by the emergence of systolic arrays, e.g., see Brent and Luk <ref type="bibr" target="#b18">[18]</ref>. A most important byproduct of such investigation of parallel Jacobi schemes is a result due to Luk and Park <ref type="bibr" target="#b127">[126]</ref>, where they show the equivalence of various parallel Jacobi orderings to the classical sequential cyclic by row ordering for which Forsythe and Henrici <ref type="bibr" target="#b58">[57]</ref> proved convergence of the method. Also, in <ref type="bibr" target="#b154">[152]</ref> a Jacobi-like algorithm for solving the nonsymmetric eigenvalue problem due to Eberlein <ref type="bibr" target="#b52">[51]</ref>, has been modified for parallel computations, primarily for the Illiac IV. More recent related parallel schemes, aimed at distributed memory multiprocessors as well, have been developed by Stewart <ref type="bibr" target="#b173">[171]</ref> and Eberlein <ref type="bibr" target="#b53">[52]</ref> for the Schur decomposition of nonsymmetric matrices.</p><p>Unlike the two-sided Jacobi scheme, for the symmetric eigenvalue problem, the one-sided Jacobi scheme due to Hestenes <ref type="bibr" target="#b95">[94]</ref> requires only accessing of the columns of the matrix under consideration. This feature makes it more suitable for shared memory multiprocessors with hierarchical organization such as the Alliant FX/8. This procedure may be described as follows. Given a symmetric nonsingular matrix A of order n and columns ai, 1 &lt; &lt; n, obtain through an iterative process an orthogonal matrix V such that AV=S where S has orthogonal columns within a given tolerance. The orthogonal matrix V is constructed as the product of plane rotations in which each is chosen to orthogonalize a pair of columns, where &lt; j, so that diT@ 0 and Ildill2 &gt; Ildjll2. This is accomplished as follows, if /3&gt;0 / otherwise, Here, a 2aTaj, J Ila ll -Ila l12 2, and 7 V/a2 +/2. Several schemes can be used to select the order of the plane rotations. Shown below is the pattern for one sweep for a matrix of order n 8 an annihilation scheme related to those recommended in [1521,  where each sweep consists of n orthogonal transformations each being the direct sum of no more than [n/2J independent plane rotations. An integer k 8, for example, denotes that the column pairs <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b7">8)</ref>, <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b6">7)</ref>, (4,6) can be orthogonalized simultaneously by 3 independent rotations. After convergence of this iterative process, usually in a few sweeps, the matrix V yields a set of approximate eigenvectors from which the eigenvalues may be obtained via Rayleigh quotients. If the matrix A is positive- definite, however, then its eigenvalues are taken as the 2-norms of the columns of S. Note that if A is not known to be nonsingular, we treat the eigenvalue problem fi.x (A +)x, where A +hi, with c being the smallest number chosen such that is positive definite. On an Alliant FX/8, this Jacobi scheme is faster than algorithms that depend on tridiagonalization, with the same size residuals, for matrices of size less than 150 or for matrices that have few clusters of almost coincident eigenvalues.</p><p>Finally, a block generalization of the two-sided Jacobi scheme has been considered by Van Loan <ref type="bibr" target="#b192">[190]</ref> and Bischof <ref type="bibr" target="#b12">[13]</ref> for distributed memory multiprocessors. The convergence of cyclic block Jacobi methods has been discussed by Shroff and Schreiber   [165]. 6.2. Singular-value problems. Several algorithms have been developed for ob- taining the singular-value decomposition on vector and parallel computers. The most robust of these schemes are those that rely first on reducing the matrix to the bidiag- onal form, i.e., by using the sequential algorithm due to Golub and Reinsch <ref type="bibr" target="#b83">[82]</ref>. The most obvious implementation of the reduction to the bidiagonal form on a parallel or vector computer follows the strategy suggested by Chan  <ref type="bibr" target="#b26">[26]</ref>. The matrix is first reduced to the upper triangular form via the block Householder reduction, suggested in the previous section, leading to the achievement of high performance. This is then followed by the chasing of zeros via rotation of rows and columns to yield a bidiagonal matrix. The application of the subsequent plane rotations has to proceed sequentially but some benefit due to vectorization can still be realized.</p><p>Once the bidiagonal matrix is obtained a generalization of Cuppen's algorithm (e.g., see <ref type="bibr" target="#b108">[107]</ref>) may be used to obtain all the singular values and vectors. Similarly, a generalization of the multisectioning algorithm may be used to obtain selected singular values and vectors.</p><p>Luk has used the one-sided Jacobi scheme to obtain the singular-value decom- position on the Illiac IV <ref type="bibr" target="#b125">[124]</ref> and block variations of Jacobi's method have been attempted by Bischof on IBM's LCAP system <ref type="bibr" target="#b12">[13]</ref>.</p><p>For tall and narrow matrices with certain distributions of clusters of singular values and/or extreme rank deficiencies, Jacobi schemes may also be used to efficiently obtain the singular-value decomposition of the upper triangular matrix resulting from the orthogonal factorization via block Householder transformations. The same one- sided Jacobi scheme discussed above has proved to be most effective on the hierarchical memory system of the Alliant FX/8. Such a procedure results in a performance that is superior to the best vectorized version of Eispack's or LINPACK routines which are based on the algorithm in <ref type="bibr" target="#b83">[82]</ref>. Experiments showed that the block-Householder reduction and the one-sided Jacobi scheme combination is up to five times faster, on the Alliant FX/8, than the best BLAS2-version of LINPACK's routine for matrices of order 16000 128 <ref type="bibr" target="#b11">[12]</ref>. 7. Rapid elliptic solvers. In this section, we review parallel schemes for rapid elliptic solvers. We start with the classical Matrix Decomposition (MD), and Block- Cyclic Reduction (BCR) schemes for separable elliptic P.D.E.'s on regular domains. This is followed by a Boundary Integral-based </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IBM Engineering and Scientific Subroutine Library Guide and Reference</title>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>IBM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A parallel implementation of matrix multiplication and LU factorization on the IBM 3090</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gustavson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Aspects of Computation on Asynchronous Parallel Processors</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Wright</surname></persName>
		</editor>
		<meeting><address><addrLine>North-Holland, Amsterdam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="217" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A model for hierarchical memory</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Alpern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Snir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 19th ACM Symp. Theory of Computing</title>
		<meeting>19th ACM Symp. Theory of Computing</meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="305" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Highly concurrent computing structures for matrix arithmetic and signal processing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Delosme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Morph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="65" to="82" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Algorithm and performance notes for block LU factorization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Armstrong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. Par. Processing</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Bailey</surname></persName>
		</editor>
		<meeting>Intl. Conf. Par. essing</meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="161" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">On a block implementation of Hessenberg multishift QR iterations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Demmel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<pubPlace>New York</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Courant Institute of Mathematical Sciences, New York University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Extra high speed matrix multiplication on the CRAY-2</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="603" to="607" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Optimizing Givens&apos; algorithm for multiprocessors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="601" to="602" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Parallel numerical algorithms on the Cedar system</title>
		<author>
			<persName><forename type="first">M</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gallivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Harrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jalby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Philippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">And</forename><forename type="middle">A</forename><surname>Sameh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CONPAR 86</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Handler</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Algorithms and experiments for structural mechanics on high performance architectures</title>
		<author>
			<persName><forename type="first">_</forename><forename type="middle">M</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Plemmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Appl. Mech. Engrg</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="487" to="507" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiprocessor schemes for solving block tridiagonal linear systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sameh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. J. Supercomputer Appl</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="37" to="57" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Parallel algorithms for the singular value and dense symmetric eigenvalue problems</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">761</biblScope>
			<pubPlace>Urbana, IL</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Center for Supercomputing Research and Development, University of Illinois</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. CSRD Rept</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Computing the singular value decomposition on a distributed system o] vector processors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bischof</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<pubPlace>Ithaca, NY</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Science, Cornell University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. TR86-798</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A pipelined block QR decomposition algorithm</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of Third SIAM Conf. on Par. Processing for Scientific Computing</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Rodrigue</surname></persName>
		</editor>
		<meeting>of Third SIAM Conf. on Par. essing for Scientific Computing<address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">QR factorization algorithms for coarse-grained distributed systems</title>
		<idno>TR 88-939</idno>
		<imprint>
			<date type="published" when="1988">1988</date>
			<pubPlace>Ithaca, NY</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Science, Cornell University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The WY representation for products of Householder matrices</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Van Loan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="2" to="13" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Redistribution subject to SIAM license or copyright</title>
		<idno>Downloaded 11/27/14 to 129.120.242.61</idno>
		<ptr target="http://www.siam.org/journals/ojsa.php" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Numerically stable solution of dense systems of linear equations using mesh-connected processors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bojanczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Brent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="95" to="104" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The solution of singular-value and symmetric eigenvalue problems on multiprocessor arrays</title>
		<author>
			<persName><forename type="first">R</forename><surname>Brent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Luk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="69" to="84" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">QR-factorization of partitioned matrices</title>
		<author>
			<persName><forename type="first">O</forename></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Johnsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Appl. Mech. Engrg</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="153" to="172" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A compact non-iterative Poisson solver</title>
		<author>
			<persName><forename type="first">O</forename><surname>Buneman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969">1969</date>
			<biblScope unit="volume">294</biblScope>
			<pubPlace>Stanford, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Stanford University Institute for Plasma Research</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. Report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A fast Poisson solver amenable to parallel co&apos;reputation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Buzbee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput., C</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="793" to="796" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On direct methods for solving Poisson&apos;s equation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Buzbee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nielson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="627" to="656" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Block-oriented local-memory-based linear equation solution on the CRAY-2: uniprocessor algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Calahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. Par. Processing</title>
		<meeting>Intl. Conf. Par. essing<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="375" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A collection of equation solving codes for the CRAY-1</title>
		<author>
			<persName><forename type="first">D</forename><surname>Calahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ames</surname></persName>
		</author>
		<author>
			<persName><forename type="first">And</forename><forename type="middle">E</forename><surname>Sesek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep. SEL</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<date type="published" when="1979">1979</date>
			<pubPlace>Ann Arbor, MI</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Michigan</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">QR factorization for linear least squares on the hypercube</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chr. Michelsen Institute</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="1986">1986</date>
			<pubPlace>Bergen, Norway</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
	<note>CCS</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An improved algorithm for computing the singular value decomposition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Software</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="72" to="83" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Practical parallel band triangular system solvers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kuck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">And</forename><forename type="middle">A</forename><surname>Sameh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Software</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="270" to="277" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gaussian elimination with partial pivoting and load balancing on a multiprocessor</title>
		<author>
			<persName><forename type="first">E</forename><surname>Chu And N</surname></persName>
		</author>
		<author>
			<persName><surname>George</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Comput</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="65" to="74" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A balanced submatrix merging algorithm for multiprocessor architectures</title>
		<idno>CS-88-45</idno>
		<imprint>
			<date type="published" when="1988">1988</date>
			<pubPlace>Waterloo, Canada</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Faculty Of Mathematics, University of Waterloo</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m">QR factorization of a dense matrix on a hypercube multiprocessor</title>
		<imprint/>
	</monogr>
	<note>Tech</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Updating and downdating the inverse of a Cholesky factor on a hypercube multiprocessor</title>
		<idno>CS-88-46</idno>
		<imprint>
			<date type="published" when="1988">1988</date>
			<pubPlace>Waterloo, Canada</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Science, University of Waterloo</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Parallel QR decomposition of a rectangular matrix</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cosnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">And</forename><forename type="middle">Y</forename><surname>Robert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Math</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="239" to="249" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A divide and conquer method for the symmetric tridiagonal eigenproblem</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cuppen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numet. Math</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="177" to="195" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Rehabilitation of the Gauss-Jordan algorithm</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dekker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hoffman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<pubPlace>University of Amsterdam, Amsterdam</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep. TR86-28</note>
	<note>Dept. of Mathematics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Prospectus for the development of a linear algebra library for highperformance computers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Croz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Greenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hammarling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sorensen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<pubPlace>Argonne, IL</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Mathematics and Computer Science Div., Argonne National Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. TM-97</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A new formulation of the hypermatrix Householder-QR decomposition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dietrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cornput. Methods Appl. Mech. Engrg</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="273" to="280" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
		<title level="m">Workshop on the Level 3 BLAS</title>
		<meeting><address><addrLine>Argonne, IL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
		<respStmt>
			<orgName>Mathematics and Computer Science Div., Argonne National Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. TM-89</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bui'ch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Moler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LINPACK User&apos;s Guide, Society for Industrial and Applied Mathematics</title>
		<imprint>
			<date type="published" when="1979">1979</date>
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A proposal for a set of Level 3 basic linear algebra subprograms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Croz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Duff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">And</forename><forename type="middle">S</forename><surname>Hammarling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<pubPlace>Argonne, IL</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Mathematics and Computer Science Div., Argonne National Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. TM-88</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A proposal for an extended set of Fortran basic linear algebra subprograms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Croz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hammarling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">And</forename><forename type="middle">R</forename><surname>Hanson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<pubPlace>Argonne, IL</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Mathematics and Computer Science Div., Argonne National Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. TM-41</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Squeezing the most out of an algorithm in CRAY Fortran</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eisenstat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Software</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="219" to="230" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Implementing linear algebra algorithms for dense matrices on a vector pipeline machine</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gustavson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">And</forename><forename type="middle">A</forename><surname>Karp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="91" to="112" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Squeezing the most out of eigenvalue solvers on high-performance computers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hammarling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaufman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<pubPlace>Argonne, IL</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Mathematics and Computer Science Div., Argonne National Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. TM-46</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Redistribution subject to SIAM license or copyright</title>
		<idno>Downloaded 11/27/14 to 129.120.242.61</idno>
		<ptr target="http://www.siam.org/journals/ojsa.php" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Block reduction of matrices to condensed form for eigenvalue computations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hammarling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sorensen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<pubPlace>Argonne, IL</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Mthematics nd Computer Science Div., Argonne NationM Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. TM-99</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Implementing dense linear algebra algorithms using multitasking on the CRAY X-MP/4</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hewltt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="347" to="350" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Solving banded systems on a parallel processor</title>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Johnsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PrMlel Comput</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="219" to="246" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">On some parallel banded system solvers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dongarra And" N</surname></persName>
		</author>
		<author>
			<persName><surname>Sameh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="223" to="235" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Implementation of some concurrent algorithms for matrix factorization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sameh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sorensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Comput</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="25" to="34" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A fully parallel algorithm for the symmetric eigenvalue problem</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sorensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="139" to="154" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Solving large full sets of linear equations in a paged virtual store</title>
		<author>
			<persName><forename type="first">J</forename><surname>Croz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nugent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">And</forename><forename type="middle">D</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Software</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="527" to="536" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A Jacobi-like method for the automatic computation of eigenvalues and eigenvectors o.f an arbitrary matrix</title>
		<author>
			<persName><forename type="first">P</forename><surname>Eberlein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Soc. Indust. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="74" to="88" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">On the Schur decomposition of a matrix for parallel computation</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cornput</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="167" to="174" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Modified cyclic algorithms for solving triangular systems on distributed memory multiprocessors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eisenstat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Henkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Romine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="589" to="600" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">A parallel QR decomposition algorithm</title>
		<author>
			<persName><forename type="first">L</forename><surname>Elden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<pubPlace>Linkoping, Sweden</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Linkoping University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. Lith-MAT-R-1988-02</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Iterative and direct methods for solving Poisson&apos;s equation and their adaptability to Illiac IV</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ericksen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972">1972</date>
			<biblScope unit="volume">60</biblScope>
			<pubPlace>Urbana, IL</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Center for Advanced Computations, University of Illinois</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. CAC Doc</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Some linear algebra algorithms and their performance on CRAY-1</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977">1977</date>
			<pubPlace>Los Alamos National Laboratory, Los Alamos, NM</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep. LA-6774</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The cyclic Jacobi method for computing the principal values of a complex matrix</title>
		<author>
			<persName><forename type="first">G</forename><surname>Forsythe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Henrici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Amer. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Forsythe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malcolm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Moler</surname></persName>
		</author>
		<title level="m">Computer Methods for Mathematical Computations</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Domain decomposition in distributed and shared memory environments</title>
		<author>
			<persName><forename type="first">G</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1987 Intl. Conf. Supercomputing</title>
		<title level="s">Lecture Notes in Comput. Sci.</title>
		<meeting>1987 Intl. Conf. Supercomputing<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1987">1987</date>
			<biblScope unit="volume">297</biblScope>
			<biblScope unit="page" from="1042" to="1073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Ftx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lyzenga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">And</forename><forename type="middle">D</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Solving Problems on Concurrent Processors</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="1988">1988</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Matrix algorithms on a hypercube I: matrix multiplication</title>
		<author>
			<persName><forename type="first">G</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">And</forename><forename type="middle">A</forename><surname>Hey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Comput</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="17" to="31" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Goodwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Olver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wilkinson</surname></persName>
		</author>
		<title level="m">Modern Computing Methods</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Philosophical Library</publisher>
			<date type="published" when="1961">1961</date>
		</imprint>
	</monogr>
	<note>First edition</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Strategies for cache and local memory management by global program transformation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gallivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">And</forename><forename type="middle">W</forename><surname>Jalby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Parallel Dist. Computing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="587" to="616" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Behavioral characterization of multiprocessor memory systems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gallivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jalby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Malony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wijshoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1989 ACM SIGMETRICS Conf. on Measuring and Modeling Computer Systems</title>
		<meeting>1989 ACM SIGMETRICS Conf. on Measuring and Modeling Computer Systems<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="79" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Performance prediction of loop constructs on multiprocessor hierarchical memory systems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gallivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jalby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Malony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wijshoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1989 Intl. Conf. Supercomputing</title>
		<meeting>1989 Intl. Conf. Supercomputing<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="433" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The use of BLAS3 in linear algebra on a parallel processor with a hierarchical memory</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gallivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jalby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">And</forename><forename type="middle">U</forename><surname>Meier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1079" to="1084" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Impact of hierarchical memory systerns on linear algebra algorithm design</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gallivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jalby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">And</forename><forename type="middle">A</forename><surname>Sameh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Level 3 BLAS Workshop</title>
		<imprint>
			<date type="published" when="1987">1988. January 1987</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="12" to="48" />
		</imprint>
		<respStmt>
			<orgName>Argonne National Labortory</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Boundary integral domain decomposition on hierarchical memory multiprocessor</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gallopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1988 Intl. Conf. Supercomputing</title>
		<meeting>1988 Intl. Conf. Supercomputing<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="488" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Fast Laplace solver by boundary integral-based domain decomposition</title>
		<ptr target="http://www.siam.org/journals/ojsa.php" />
	</analytic>
	<monogr>
		<title level="m">Third SIAM Conf. on Par. Processing for Scientific Computing</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Rodrigue</surname></persName>
		</editor>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<publisher>Society for Industrial and Applied Mathematics</publisher>
			<date type="published" when="1988">27/14 to 129.120.242. 1988</date>
			<biblScope unit="volume">61</biblScope>
		</imprint>
	</monogr>
	<note>Proc. of Downloaded</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Parallel block cyclic reduction algorithm for the fast solution of elliptic equations</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gallopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Saad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l. Conf. on Supercomputing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="143" to="160" />
			<date type="published" when="1987">1989. 1987</date>
			<pubPlace>Athens, Greece</pubPlace>
		</imprint>
	</monogr>
	<note>Parallel Comput.</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Efficient parallel solution of parabolic equations: explicit methods</title>
	</analytic>
	<monogr>
		<title level="m">at the Fourth SIAM Conf. Parallel Processing for Scientific Computing</title>
		<meeting><address><addrLine>Urbana, IL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989-06">June 1989</date>
		</imprint>
		<respStmt>
			<orgName>Supercomputing Research and Development, University of Illinois</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rap</note>
	<note>To be presented</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Efficient parallel solution of parabolic equations: implicit methods</title>
	</analytic>
	<monogr>
		<title level="m">at the Fourth SIAM Conf. Parallel Processing for Scientific Computing</title>
		<meeting><address><addrLine>Urbana, IL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989-06">June 1989</date>
		</imprint>
		<respStmt>
			<orgName>Supercomputing Research and Development, University of Illinois</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rap</note>
	<note>To be presented</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Some fast elliptic solvers for parallel architectures and their complexities</title>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l. J. High Speed Comput</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="113" to="141" />
			<date type="published" when="1989-05">May 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">The influence of memory hierarchy on algorithm organization: programming FFTs on a vector multiprocessor</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jalby ; L. Jamieson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Douglass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Characteristics of Parallel Algorithms</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="277" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">On the problem of optimizing data transfers for complex memory systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jalby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gallivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1988 Intl. Conf. Supercomputing</title>
		<meeting>1988 Intl. Conf. Supercomputing<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="238" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">On the impact of communication complexity on the design of parallel numerical algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Rosendale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1180" to="1195" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Parallel Cholesky factorization on a hypercube multiprocessor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<pubPlace>Oak Ridge, TN</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Oak Ridge National Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. ORNL-6190</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Matrix factorization on a hypercube</title>
	</analytic>
	<monogr>
		<title level="m">Hypercube Multiprocessors</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Heath</surname></persName>
		</editor>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1986">1986. 1986</date>
			<biblScope unit="page" from="161" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">LU factorization algorithms on distributed memory multiprocessot architectures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Romine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="639" to="649" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Matrix triangularization by systolic arrays</title>
		<author>
			<persName><forename type="first">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Kung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE 298, Real Time Signal Processing</title>
		<meeting>SPIE 298, Real Time Signal essing<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1981">1981</date>
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Parallel block schemes for large-scale least squares computations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Plemmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">And</forename><forename type="middle">A</forename><surname>Sameh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Speed Computing, Scientific Applications and Algorithm Design</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Wilhelmson</surname></persName>
		</editor>
		<meeting><address><addrLine>Urbana, IL</addrLine></address></meeting>
		<imprint>
			<publisher>University of Illinois Press</publisher>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="180" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Singular value decomposition and least squares solutions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Reinsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Math</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="403" to="420" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Van Loan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matrix</forename><surname>Computations</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>The Johns Hopkins University Press</publisher>
			<pubPlace>Baltimore</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">F</forename><surname>Gustavson</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>private communication</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Programming with the BLAS</title>
		<author>
			<persName><forename type="first">W</forename><surname>Harrod ; L. Jamieson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Douglass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Characteristics of Parallel Algorithms</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="253" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">A block scheme for reduction to condensed form</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">696</biblScope>
			<pubPlace>Argonne, IL</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Center for Supercomputing Research and Development, University of Illinois</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rap. CSRD Rapt</note>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Harten</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>private communication</note>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Parallel Cholesky factorization in message passing multiprocessor environments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<pubPlace>Oak Ridge, TN</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Oak Ridge National Lab.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rap. ORNL-6150</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Parallel solution of triangular systems on distributed memory multiprocessors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Romine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="558" to="588" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Some aspects of the cyclic reduction algorithm for block tridiagonal linear systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Heller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="484" to="496" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">A survey of parallel algorithms for numerical linear algebra</title>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="740" to="777" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Cholesky downdating on a hypercube</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tlenkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Plemmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hypercube Multiprocessors</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Fox</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1988">1988. 1988</date>
			<biblScope unit="page" from="1592" to="1598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Recursive least squares on a hypercube multiprocessor using the covariance factorization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Henkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Plemmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="1988">1988. 1990</date>
			<pubPlace>Raleigh, NC</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept. Computer Science, North Carolina State University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rap</note>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Inversion of matrices by biorthogonalization and related results</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hestenes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Gallivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Plemmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">And</forename><forename type="middle">A H</forename></persName>
		</author>
		<ptr target="http://www.siam.org/journals/ojsa.php" />
	</analytic>
	<monogr>
		<title level="j">SAMEH dust. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="51" to="90" />
			<date type="published" when="1958">27/14 to 129.120.242.61. 1958</date>
		</imprint>
	</monogr>
	<note>J. Soc. In-Downloaded</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Exploiting fast matrix multiplication with the Level 3 BLAS</title>
		<author>
			<persName><forename type="first">N</forename><surname>Higham</surname></persName>
		</author>
		<idno>TR 89-984</idno>
		<imprint>
			<date type="published" when="1989">1989</date>
			<pubPlace>Ithaca, NY</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Science, Cornell University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">A fast direct solution of Poisson&apos;s equation using Fourier analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hockney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JACM</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="95" to="113" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">The potential calculation and some applications</title>
	</analytic>
	<monogr>
		<title level="j">Methods Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="135" to="211" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Optimizing the FACR(I) Poisson solver on parallel computers</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. Par. Processing</title>
		<meeting>Intl. Conf. Par. essing<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Problem related performance parameters .for supercomputers, in Performance Evaluation of Supercomputers</title>
		<editor>J. Martin</editor>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Elsevier Science Publishers B.V</publisher>
			<biblScope unit="page" from="215" to="235" />
			<pubPlace>Amsterdam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jesshope</surname></persName>
		</author>
		<title level="m">Parallel Computers</title>
		<meeting><address><addrLine>Adam Hilger, Bristol</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
	<note>First edition</note>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">KUNG, I/O complexity: the red-blue pebble game</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th ACM Symp. Theory of Computing</title>
		<meeting>13th ACM Symp. Theory of Computing</meeting>
		<imprint>
			<date type="published" when="1981">1981</date>
			<biblScope unit="page" from="326" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">A parallel algorithm for symmetric tridiagonal eigenvalue problems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974">1974</date>
			<biblScope unit="volume">109</biblScope>
			<pubPlace>Urbana, IL</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Center for Advanced Computation, University of Illinois</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. CAC Doc</note>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Solving the symmetric tridiagonal eigenvalue problem on the hypercube</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ipsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jessup</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<pubPlace>New Haven, CT</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept..of Computer Science, Yale University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. RR-548</note>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Complexity of dense linear system solution on a multiprocessor ring</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ipsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra Appl</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="205" to="239" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Optimizing matrix operations on a parallel mulitprocessor with a hierarchical memory system</title>
		<author>
			<persName><forename type="first">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. Par. Processing</title>
		<meeting>Intl. Conf. Par. essing<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="429" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Loss of orthogonality in a Gram-Schmidt process</title>
		<author>
			<persName><forename type="first">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Philippe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep., IRISA</title>
		<imprint>
			<date type="published" when="1987">1987</date>
			<pubPlace>Rennes, France</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">A parallel algorithm for computing the singular value decomposition of a matrix</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jessup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sorensen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<pubPlace>Argonne, IL</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Mathematics and Computer Science Div., Argonne National Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. TM-102</note>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Solving narrow banded systems on ensemble architectures</title>
		<author>
			<persName><forename type="first">L</forename><surname>Johnsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Software</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="271" to="288" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Solving tridiagonal systems on ensemble architectures</title>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="354" to="392" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Algorithms for multiplying matrices of arbitrary shapes using shared memory primitives on Boolean cubes</title>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Ho</surname></persName>
		</author>
		<idno>YALEU/DCS/TR-569</idno>
		<imprint>
			<date type="published" when="1987">1987</date>
			<pubPlace>New Haven</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept. Computer Science, Yale University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Inversion of Toeplitz operators, innovations and orthogonal polynomials</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kailath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Morf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="106" to="119" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">QR factorization of a rectangular matrix</title>
		<author>
			<persName><forename type="first">C</forename><surname>Suter</surname></persName>
		</author>
		<idno>TR88-07</idno>
		<imprint>
			<date type="published" when="1988">1988</date>
			<pubPlace>Birmingham, AL</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Alabama at Birmingham</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Recursive least squares filtering for signal processing on distributed memory multiprocessors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Plemmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inter. J. Parallel Proc</title>
		<imprint>
			<date type="published" when="1988">1988. 1990</date>
			<pubPlace>Raleigh, NC</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Science, North Carolina State University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep</note>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Knowles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Okawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Muraoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wilhelmson</surname></persName>
		</author>
		<title level="m">Matrix operations on ILLIAC IV, Tech. Rep. ILLIAC IV Doc</title>
		<meeting><address><addrLine>Urbana, IL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1967">1967</date>
			<biblScope unit="volume">118</biblScope>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Science, University of Illinois</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">ILLIAC IV software and application programming</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kuck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput., C</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="758" to="770" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">The Structure of Computers and Computations</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="1978">1978</date>
			<publisher>John Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Parallel supercomputing today and the Cedar approach</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kuck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lawrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">And</forename><forename type="middle">A</forename><surname>Sameh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">231</biblScope>
			<biblScope unit="page" from="967" to="974" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Parallel computations of eigenvalues of real matrices</title>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sameh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IFIP Congress 1971</title>
		<meeting>IFIP Congress 1971<address><addrLine>North-Holland, Amsterdam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1972">1972</date>
			<biblScope unit="page" from="1266" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Access and alignment of data in an array processor</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lawrie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1145" to="1155" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">The computations and communication complexity of a parallel banded system solver</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lawrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sameh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans Math. Software</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="185" to="195" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Redistribution subject to SIAM license or copyright</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lawson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kincaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Krogh</surname></persName>
		</author>
		<idno>11/27/14 to 129.120.242.61</idno>
		<ptr target="http://www.siam.org/journals/ojsa.php" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Software</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="308" to="323" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
	<note>for Fortran use</note>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">A parallel triangular solver on a distributed memory multiprocessor</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Coleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="485" to="502" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">A multiprocessor algorithm .for the symmetric tridiagonal eigenvalue problem</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">And</forename><forename type="middle">A</forename><surname>Pe</surname></persName>
		</author>
		<author>
			<persName><surname>Sameh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="155" to="165" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Computing the singular value decomposition on the Illiac IV</title>
		<author>
			<persName><forename type="first">F</forename><surname>Luk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Software</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="524" to="539" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">A rotation method for computing the QR decomposition</title>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="452" to="549" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">A proof of convergence for two parallel Jacobi SVD algorithms</title>
		<author>
			<persName><forename type="first">F</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Matrix multiplication by diagonals on a vector/parallel processor</title>
		<author>
			<persName><forename type="first">N</forename><surname>Madsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rodrigue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">And</forename><forename type="middle">J</forename><surname>Karush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Process. Lett</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="41" to="45" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">The approximate solution of elliptic boundary-value problems by fundamental solutions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mathon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Johnston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="638" to="650" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Hypercube algorithms and implementations</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mcbryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Van De</surname></persName>
		</author>
		<author>
			<persName><surname>Velde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="227" to="287" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Engineering and scientific library for the IBM 3090 vector facility</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mccomb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Systems Journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="404" to="415" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Organizing matrices and matrix operations for paged memory systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mckellar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Coffman Jr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. ACM</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="153" to="165" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">A parallel partition method for solving banded systems of linear equations</title>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Comput</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="33" to="43" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">A survey of parallelism in numerical analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Miranker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="524" to="547" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">An alternative Givens ordering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Modi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Math</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="83" to="90" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Matrix computations on distributed memory multiprocessors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Moler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hypercube Multiprocessors</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Heath</surname></persName>
		</editor>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1986">1986. 1986</date>
			<biblScope unit="page" from="181" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">An algorithm for generalized matrix eigenvalue problems</title>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="241" to="256" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<title level="m" type="main">Introduction to Parallel and Vector Solution of Linear Systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ortega</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Plenum</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Solution of partial differential equations on vector and parallel computers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Voigt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="149" to="240" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<monogr>
		<title level="m" type="main">A bibliography on parallel and vector numerical algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Voigt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Romine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<pubPlace>Oak Ridge, TN</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Oak Ridge National Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. ORNL/TM-10998</note>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Least squares modifications with inverse factorizations: parallel implications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Plemmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Appl. Math</title>
		<imprint>
			<date type="published" when="1987">1987</date>
			<pubPlace>Raleigh, NC</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Science, North Carolina State University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep</note>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b142">
	<monogr>
		<title level="m" type="main">The Symmetric Eigenvalue Problem</title>
		<author>
			<persName><forename type="first">B</forename><surname>Parlett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">An adaption of the fast Fourier transform for parallel processing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pease</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JACM</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="252" to="264" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">On the stability of Gauss-Jordan elimination with pivoting</title>
		<author>
			<persName><forename type="first">G</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wilkinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="20" to="24" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">A parallel block scheme applied to computations in structural analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Plemmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Algebraic Discrete Methods</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="337" to="347" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Substructuring methods for computing the nullspace of equilibrium matrices</title>
		<author>
			<persName><forename type="first">R</forename><surname>Plemmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep., Center for Research in Sci. Comp</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<pubPlace>Raleigh, NC</pubPlace>
		</imprint>
		<respStmt>
			<orgName>North Carolina State University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">An ejZficient parallel scheme for minimizing a sum of Euclidean norms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Plemmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra Appl</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="71" to="85" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<monogr>
		<title level="m" type="main">Orthogonal factorization on a distributed memory multiprocessor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pothen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
		<respStmt>
			<orgName>Pennsylvania State University, Computer Science Dept., University Park, PA</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. CS-87-24</note>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Orthogonal factorization .on a distributed memory multiprocessor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pothen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Somesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Vemulapati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hypercube Multiprocessors 1987, M. T. Heath</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="587" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Dense linear systems Fortran solvers on the IBM 3090 vector multiprocessor</title>
		<author>
			<persName><forename type="first">G</forename><surname>Radicati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sguazzero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="377" to="384" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<monogr>
		<title level="m" type="main">Redistribution subject to SIAM license or copyright</title>
		<idno>Downloaded 11/27/14 to 129.120.242.61</idno>
		<ptr target="http://www.siam.org/journals/ojsa.php" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">The parallel solution of triangular systems on a hypercube</title>
		<author>
			<persName><forename type="first">C</forename><surname>Romine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hypercube Multiprocessors 1987</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Heath</surname></persName>
		</editor>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="552" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Parallel solution of triangular systems of equations</title>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ortega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="109" to="114" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">On Jacobi and Jacobi-like algorithms for a parallel computer</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sameh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Comp</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="579" to="590" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Numerical parallel algorithms a survey</title>
	</analytic>
	<monogr>
		<title level="m">High Speed Computer and Algorithm Organization</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Kuck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Lawrie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Sameh</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1977">1977</date>
			<biblScope unit="page" from="207" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<monogr>
		<title level="m" type="main">A fast Poisson solver for multiprocessors</title>
		<editor>Elliptic Problem Solvers II, G. Birkhoff and A. Schoenstadt</editor>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Academic Press</publisher>
			<biblScope unit="page" from="175" to="186" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<monogr>
		<title level="m" type="main">On two numerical algorithms for multiprocessors</title>
		<editor>High Speed Computation, J. Kowalik</editor>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Springer-Verlag</publisher>
			<biblScope unit="page" from="311" to="328" />
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Numerical algorithms on the Cedar system</title>
	</analytic>
	<monogr>
		<title level="m">Second SIAM Conference on Parallel Processing</title>
		<imprint>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">On some parallel algorithms on a ring of processors</title>
	</analytic>
	<monogr>
		<title level="j">Comput. Phys. Comm</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="159" to="166" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Solving the linear least squares problem on a linear array of processors, in Algorithmically-Specialized Parallel Computers</title>
	</analytic>
	<monogr>
		<title level="m">Proc. 1982 Purdue Univ</title>
		<meeting>1982 Purdue Univ<address><addrLine>West Lafayette, IN</addrLine></address></meeting>
		<imprint>
			<publisher>Workshop</publisher>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="191" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Solving triangular systems on a parallel computer</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sameh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Brent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1101" to="1113" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Parallel Poisson and biharmonic solvers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sameh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">And</forename><forename type="middle">D</forename><surname>Kuck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="219" to="230" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">On stable parallel linear systems solvers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sameh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kuck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JACM</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="81" to="91" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Programming style on the IBM vector facility considering both performance and flexibility</title>
		<author>
			<persName><forename type="first">H</forename><surname>Samukawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Systems Journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="453" to="474" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Block reflectors: theory and computation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Parlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="189" to="205" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">A storage-efficient WY representation for products of Householder transformations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Van Loan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="53" to="57" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<monogr>
		<title level="m" type="main">On the convergence of the cyclic Jacobi method for parallel block orderings</title>
		<author>
			<persName><forename type="first">G</forename><surname>Shroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schreiber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<pubPlace>Troy, NY</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Science, Rensselaer Polytechnic Institute</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. 88-11</note>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Bisection is not optimal on vector processors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="205" to="209" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ikebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Klema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Moler</surname></persName>
		</author>
		<title level="m">Matrix Eigensystem Routines: EISPACK Guide</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
	<note>Second edition</note>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Analysis of pairwise pivoting in Gaussianelimination</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sorensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput., C</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="274" to="278" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<monogr>
		<title level="m" type="main">A fast Gaussian elimination scheme and automated roundoff error analysis for SIME machines, Master&apos;s thesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Stern</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
			<pubPlace>Urbana, IL</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Science, University of Illinois</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<monogr>
		<title level="m" type="main">Introduction to Matrix Computations</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Stewart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973">1973</date>
			<publisher>Academic Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">A Jacob#like algorithm for computing the Schur decomposition of a nonhermitian matrix</title>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="853" to="864" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
	</analytic>
	<monogr>
		<title level="m">A parallel implementation of the QR-algorithm</title>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="187" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Parallel processing with the perfect shuffle</title>
		<author>
			<persName><forename type="first">H</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="153" to="161" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<monogr>
		<title level="m" type="main">High-Performance Computer Architecture, First edition</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Reading</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Gaussian-elimination is not optimal</title>
		<author>
			<persName><forename type="first">V</forename><surname>Strassen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Math</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="354" to="356" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">A direct method for the discrete solution of separable elliptic equations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Swarztrauber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1136" to="1150" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">The methods of cyclic reduction, Fourier analysis and the FACR algorithm for the discrete solution of Poisson&apos;s equation on a rectangle</title>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="490" to="501" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Algorithm 541: efficient Fortran subprograms for the solution of separable elliptic partial differential equations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Swarztrauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sweet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Software</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Vector and parallel methods for the direct solution o] Poisson&apos;s equation</title>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="241" to="263" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">A cyclic reduction algorithm for solving block tridiagonal systems of arbitrary dimension</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sweet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="707" to="720" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">A parallel and vector variant of the cyclic reduction algorithm</title>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="761" to="765" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Direct methods for the solution of the discrete Poisson equation: some comparisons</title>
		<author>
			<persName><forename type="first">C</forename><surname>Temperton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">On the FACR(l) algorithm for the discrete Poisson equation</title>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="314" to="329" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Average-case stability of Gaussian elimination</title>
		<author>
			<persName><forename type="first">L</forename><surname>Trefethan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schreiber</surname></persName>
		</author>
		<idno>RUU-CS-84-7</idno>
	</analytic>
	<monogr>
		<title level="j">Dept. of Mathematics</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
	<note>MIT</note>
</biblStruct>

<biblStruct xml:id="b187">
	<monogr>
		<title level="m" type="main">Prepaging and applications to structured array problems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Trivedi</surname></persName>
		</author>
		<idno>UIUCDCS- R-74-662</idno>
		<imprint>
			<date type="published" when="1974">1974</date>
			<pubPlace>Urbana, IL</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Science, University of Illinois</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">On the paging performance of array algorithms</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput., C</title>
		<imprint>
			<biblScope unit="page" from="938" to="947" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<monogr>
		<title level="m" type="main">On the accuracy of solving triangular systems in parallel</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tsao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<pubPlace>Cleveland, OH</pubPlace>
		</imprint>
		<respStmt>
			<orgName>NASA Lewis Research Center</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. ICOMP-88-19</note>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Analysis of a parallel solution method for tridiagonal linear systems</title>
		<author>
			<persName><forename type="first">N</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName><surname>Vorst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Comput</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="303" to="311" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Vectorization of linear recurrence relations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vorst</surname></persName>
		</author>
		<author>
			<persName><surname>Dekker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="27" to="35" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<monogr>
		<title level="m" type="main">The block Jacobi method for computing the singular value decomposition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Van Loan</surname></persName>
		</author>
		<idno>TR85-680</idno>
		<imprint>
			<date type="published" when="1985">1985</date>
			<pubPlace>Ithaca, NY</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Science, Cornell University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b193">
	<monogr>
		<title level="m" type="main">The influence of vector computer architecture on numerical algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Voigt</surname></persName>
		</author>
		<editor>High Speed Computer and Algorithm Organization, D. Kuck, D. Lawrie, and A. Sameh</editor>
		<imprint>
			<date type="published" when="1977">1977</date>
			<publisher>Academic Press</publisher>
			<biblScope unit="page" from="229" to="244" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">A parallel method for tridiagonal equations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Software</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="170" to="183" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<monogr>
		<title level="m" type="main">Data Organization in Parallel Computers</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A G</forename><surname>Wijshoff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Kluwer</publisher>
			<pubPlace>Boston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<monogr>
		<title level="m" type="main">The Algebraic Eigenvalue Problem</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wilkinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965">1965</date>
			<publisher>Clarendon Press</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wilkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Reinsch</surname></persName>
		</author>
		<title level="m">Handbook for Automatic Computation: Linear Algebra</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1971">1971</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">A scheme to enforce data dependences on large multiprocessor systems</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Yew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Soft. Engrg</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="726" to="739" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<monogr>
		<title level="m" type="main">Redistribution subject to SIAM license or copyright</title>
		<idno>Downloaded 11/27/14 to 129.120.242.61</idno>
		<ptr target="http://www.siam.org/journals/ojsa.php" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
