<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Requirements and Motivations of Low-Resource Speech Synthesis for Language Revitalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Aidan</forename><surname>Pine</surname></persName>
							<email>aidan.pine@nrc.ca</email>
						</author>
						<author>
							<persName><forename type="first">Dan</forename><surname>Wells</surname></persName>
							<email>dan.wells@ed.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Nathan</forename><surname>Thanyehténhas Brinklow</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Littell</surname></persName>
							<email>patrick.littell@nrc.ca</email>
						</author>
						<author>
							<persName><forename type="first">Korin</forename><surname>Richmond</surname></persName>
							<email>korin.richmond@ed.ac.uk</email>
						</author>
						<title level="a" type="main">Requirements and Motivations of Low-Resource Speech Synthesis for Language Revitalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the motivation and development of speech synthesis systems for the purposes of language revitalization. By building speech synthesis systems for three Indigenous languages spoken in Canada, Kanien'kéha, Gitksan &amp; SENĆOŦEN, we re-evaluate the question of how much data is required to build low-resource speech synthesis systems featuring state-of-the-art neural models. For example, preliminary results with English data show that a FastSpeech2 model trained with 1 hour of training data can produce speech with comparable naturalness to a Tacotron2 model trained with 10 hours of data. Finally, we motivate future research in evaluation and classroom integration in the field of speech synthesis for language revitalization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There are approximately 70 Indigenous languages spoken in Canada, from 10 distinct language families <ref type="bibr" target="#b44">(Rice, 2008)</ref>. As a consequence of the residential school system and other policies of cultural suppression, the majority of these languages now have fewer than 500 fluent speakers remaining, most of them elderly. Despite this, interest from students and parents in Indigenous language education continues to grow (Statistics Canada, 2016); we have heard from teachers that they are overwhelmed with interest from potential students, and the growing trend towards online education means many students who have not previously had access to language classes now do.</p><p>Supporting these growing cohorts of students comes with unique challenges for languages with few fluent first-language speakers. A particular concern of teachers is to provide their students with opportunities to hear the language outside of 1 National Research Council Canada 2 University of Edinburgh 3 Queen's University class. Text-to-speech synthesis technology (TTS) shows potential for supplementing text-based language learning tools with audio in the event that the domain is too large to be recorded directly, or as an interim solution pending recordings from firstlanguage speakers.</p><p>Development of TTS systems in this context faces several challenges. Most notable is the usual assumption that neural speech synthesis models require at least tens of hours of audio recordings with corresponding text transcripts to be trained adequately. Such a data requirement is far beyond what is available for the languages we are concerned with, and is difficult to meet given the limited time of the relatively small number of speakers of these languages. The limited availability of Indigenous language speakers also hinders the subjective evaluation methods often used in TTS studies, where naturalness of synthetic speech samples is judged by speakers of the language in question.</p><p>In this paper, we re-evaluate some of these challenges for applying TTS in the low-resource context of language revitalization. We build TTS systems for three Indigenous languages of Canada, with training data ranging from 25 minutes to 3.5 hours, and confirm that we can produce acceptable speech as judged by language teachers and learners. Outputs from these systems could be suitable for use in some classroom applications, for example a speaking verb conjugator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Language Revitalization</head><p>It is no secret that the majority of the world's languages are in crisis, and in many cases this crisis is even more urgent than conservation biologists' dire predictions for flora and fauna <ref type="bibr" target="#b48">(Sutherland, 2003)</ref>. However, the 'doom and gloom' rhetoric that often follows endangered languages over-represents vulnerability and under-represents the enduring strength of Indigenous communities who have refused to stop speaking their languages despite over a century of colonial policies against their use <ref type="bibr">(Pine and Turin, 2017)</ref>. Continuing to speak Indigenous languages is often seen as a political act of anti-colonial resistance. As such, the goals of any given language revitalization effort extend far beyond memorizing verb paradigms to broader goals of nationhood and self-determination <ref type="bibr">(Pitawanakwat, 2009; McCarty, 2018)</ref>. Language revitalization programs can also have immediate and important impacts on factors including community health and wellness <ref type="bibr">(Whalen et al., 2016; Oster et al., 2014)</ref>.</p><p>There is a growing international consensus on the importance of linguistic diversity, from the Truth &amp; Reconciliation Commission of Canada (TRC) report in 2015 which issued nine calls to action related to language, to 2019 being declared an International Year of Indigenous Languages by the UN, and 2022-2032 being declared an International Decade of Indigenous Languages. From 1996 to 2016, the number of speakers of Indigenous languages increased by 8% <ref type="bibr">(Statistics Canada, 2016)</ref>. These efforts have been successful despite a lack of support from digital technologies. While opportunities may exist for technology to assist and support language revitalization efforts, these technologies must be developed in a way that does not further marginalize communities <ref type="bibr">(Brinklow et al., 2019; Bird, 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Why TTS for Language Revitalization?</head><p>Our interest in speech synthesis for language revitalization was sparked during user evaluations of Kawennón:nis (lit. 'it makes words'), a Kanien'kéha verb conjugator <ref type="bibr" target="#b27">(Kazantseva et al., 2018)</ref> developed in collaboration between the National Research Council Canada and the Onkwawenna Kentyohkwa adult immersion program in Six Nations of the Grand River in Ontario, Canada. Kawennón:nis models a pedagogicallyimportant subset of verb conjugations in XFST <ref type="bibr" target="#b2">(Beesley and Karttunen, 2003)</ref>, and currently produces 247,450 unique conjugations.</p><p>The pronominal system is largely responsible for much of this productivity, since in transitive paradigms, agent/patient pairs are fused, as illustrated in Figure <ref type="figure">1</ref>.</p><p>In user evaluations of Kawennón:nis, students often asked whether it was possible to add audio to the tool, to model the pronunciation of unfamil- The research question that then emerged was 'what is the smallest amount of data needed in order to generate audio for all verb forms in Kawennón:nis'. Beyond Kawennón:nis, we anticipate that there are many similar language revitalization projects that would want to add supplementary audio to other text-based pedagogical tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Speech Synthesis</head><p>The last few years have shown an explosion in research into purely neural network-based approaches to speech synthesis <ref type="bibr" target="#b50">(Tan et al., 2021)</ref>. Similar to their HMM/GMM predecessors, neural pipelines typically consist of both a network predicting the acoustic properties of a sequence of text and a vocoder. The feature prediction network must be trained using parallel speech/text data where the input is typically a sequence of characters or phones that make up an utterance, and the output is a sequence of fixed-width frames of acoustic features. In most cases the predictions from the TTS model are log Mel-spectral features and a vocoder is used to generate the waveform from these acoustic features.</p><p>Much of the previous work on low resource speech synthesis has focused on transfer learning; that is, 'pre-training' a network using data from a language that has more data, and then 'fine-tuning' using data from the low-resource language. One of the problems with this approach is that the input space often differs between languages. As the inputs to these systems are sequences of characters or phones, and as these sequences are typically one-hot encoded, it can be difficult to devise a principled method for transferring weights from the source language network to the target if there is a difference between the character or phone inventories of the two languages. Various strategies have emerged for normalizing the input space. For example, <ref type="bibr" target="#b12">Demirsahin et al. (2018)</ref> propose a unified inventory for regional multilingual training of South Asian languages, while <ref type="bibr" target="#b52">Tu et al. (2019)</ref> compare various methods to create mappings between source and target input spaces. Another proposal is to normalize the input space between source and target languages by replacing one-hot encodings of text with multi-hot phonological feature encodings <ref type="bibr">(Gutkin et al., 2018; Wells and</ref><ref type="bibr" target="#b56">Richmond, 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Speech Synthesis for Indigenous Languages in Canada</head><p>There is extremely little published work on speech synthesis for Indigenous languages in Canada (and North America generally). A statistical parametric speech synthesizer using Simple4All was recently developed for Plains Cree <ref type="bibr">(Harrigan et al., 2019; Clark, 2014)</ref>. Although it was unpublished, two highschool students<ref type="foot" target="#foot_0">1</ref> created a statistical parametric speech synthesizer for Kanien'kéha by adapting eSpeak <ref type="bibr" target="#b13">(Duddington and Dunn, 2007)</ref>. We know of no other attempts to create speech synthesis systems for Indigenous languages in Canada. Elsewhere in North America, a Tacotron2 system has been built for Cherokee <ref type="bibr" target="#b11">(Conrad, 2020)</ref>, and some early work on concatenative systems for Navajo was discussed in a technical report <ref type="bibr" target="#b59">(Whitman et al., 1997)</ref>, as well as on Rarámuri <ref type="bibr" target="#b54">(Urrea et al., 2009)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Indigenous Language Data</head><p>Although the term 'low resource' is used to describe a wide swath of languages, most Indigenous languages in Canada would be considered 'lowresource' in multiple senses of the word, having both a low amount of available data (annotated or unannotated), and a relatively low number of speakers. Most Indigenous languages lack transcribed audio corpora, and fewer still have such data recorded in a studio context. Due to the limited number of speakers, creating these resources is non-trivial: there are limited amounts of text from which a speaker could read, and there are few people available who are sufficiently literate in the languages to transcribe recorded audio. Re-focusing speakers' limited time to these tasks presents a significant opportunity cost; they are often already over-worked and over-burdened in under-funded and under-resourced language teaching projects.</p><p>As mentioned in §2.1, language technology projects that aim to assist language revitalization and reclamation efforts must be centered around the primary goals of those efforts and ensure that the means of developing the technology do not distract or work against the broader sociopolitical goals. A primary stress point for many natural language processing projects involving Indigenous communities surrounds issues of data sovereignty. It is important that communities direct the development of these tools, and maintain control, ownership, and distribution rights for their data, as well as for the resulting speech synthesis models (Keegan, <ref type="bibr">2019; Brinklow, 2021</ref>. In keeping with this, the datasets described in this paper are not being released publicly at this time.</p><p>To test the feasibility of developing speech synthesis systems for Indigenous languages, we trained models for three unrelated Indigenous languages, Kanien'kéha ( §3.1), Gitksan ( §3.2), and SENĆOŦEN ( §3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Kanien'kéha</head><p>Kanien'kéha<ref type="foot" target="#foot_2">2</ref> (a.k.a. Mohawk) is an Iroquoian language spoken by roughly 2,350 people in southern Ontario, Quebec, and northern New York state <ref type="bibr">(Statistics Canada, 2016)</ref>. In 1979 the first immersion school of any Indigenous language in Canada was opened for Kanien'kéha, and many other very successful programs have been started since, including the Onkwawenna Kentyohkwa adult immersion program in 1999 <ref type="bibr" target="#b18">(Gomashie, 2019)</ref>.</p><p>In the late 1990s, a team of five Kanien'kéha translators worked with the Canadian Bible Society to translate and record parts of the Bible; one of the speakers on these recordings, Satewas, is still living. Translation runs in Satewas's family, with his great-grandfather also working on Bible translations in the 19th century. Later, a team of four speakers and learners, including this paper's third author, aligned the text and audio at the utterance level using Praat <ref type="bibr" target="#b4">(Boersma and van Heuven, 2001)</ref> and ELAN <ref type="bibr" target="#b7">(Brugman and Russel, 2004)</ref>.</p><p>While a total of 24 hours of audio were recorded, members of the Kanien'kéha-speaking community told us it would be inappropriate to use the voices of speakers who had passed away, leaving only recordings of Satewas's voice. Using a GMMbased speaker ID system <ref type="bibr" target="#b30">(Kumar, 2017)</ref>, we removed utterances by these speakers, then removed utterances that were outliers in duration (less than 0.4s or greater than 11s) and speaking rate (less than 4 phones per second or greater than 15), recordings with an unknown phase effect present, and utterances containing non-Kanien'kéha characters (e.g. proper names like 'Euphrades'). Handling utterances with non-Kanien'kéha characters would have required grapheme-to-phoneme prediction capable of dealing with multilingual text and code-switching which we did not have available. The resulting speech corpus comprised 3.46 hours of speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Gitksan</head><p>Gitksan<ref type="foot" target="#foot_3">3</ref> is one of four languages belonging to the Tsimshianic language family spoken along the Skeena river and its surrounding tributaries in the area colonially known as northern British Columbia. Traditional Gitksan territory spans some 33,000 square kilometers and is home to almost 10,000 people, with approximately 10% of the population continuing to speak the language fluently (First Peoples <ref type="bibr">' Cultural Council, 2018)</ref>.</p><p>As there were no studio-quality recordings of the Gitksan language publicly available, and as an intermediate speaker of the language, the first author recorded a sample set himself. In total, he recorded 35.46 minutes of audio reading isolated sentences from published and unpublished stories <ref type="bibr" target="#b16">(Forbes et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">SENĆOŦEN</head><p>The SENĆOŦEN language is spoken by the W ¯SÁNEĆ people on the southern part of the island colonially known as Vancouver Island. It belongs to the Coastal branch of the Salish language family. The W ¯SÁNEĆ community runs a worldfamous language revitalization program<ref type="foot" target="#foot_4">4</ref> , and uses an orthography developed by the late SENĆOŦEN speaker and W ¯SÁNEĆ elder Dave Elliott. While the community of approximately 3,500 has fewer than 10 fluent speakers, there are hundreds of learners, many of whom have been enrolled in years of immersion education in the language (First Peoples <ref type="bibr">' Cultural Council, 2018)</ref>.</p><p>As there were no studio-quality recordings of the SENĆOŦEN language publicly available, we recorded 25.92 minutes of the language with PENÁĆ David Underwood reading two stories originally spoken by elder Chris Paul.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Research Questions</head><p>Given the motivation and context for language revitalization-based speech synthesis, a number of research questions follow. Namely, how much data is required in order to build a system of reasonable pedagogical quality? How do we evaluate such a system? And, how is the resulting system best integrated into the classroom? In §4.1, we discuss the difficulty of evaluating TTS systems in low-resource settings. We then discuss preliminary results for English and Indigenous language TTS which show that acceptable speech quality can be achieved with much less training data than usually considered for neural speech synthesis ( §4.2). Finally, we suggest possible directions for pedagogical integration in section §4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Low-Resource Evaluation</head><p>One of the most significant challenges in researching speech synthesis for languages with few speakers is evaluating the models. For some Indigenous languages in Canada, the total number of speakers of the language is less than the number typically required for statistical significance in a listening test <ref type="bibr" target="#b57">(Wester et al., 2015)</ref>. While the number of speakers in these conditions is sub-optimal for statistical analysis, we have been told by the communities we work with that the positive assessment of a few widely respected and community-engaged language speakers would be practically sufficient to assess the pedagogical value of speech models in language revitalization contexts. For the experiments described in this paper, we ran listening tests for both Kanien'kéha and Gitksan with speakers, teachers, and learners, but were not able to run any such tests for SENĆOŦEN due to very few speakers with already busy schedules.</p><p>While some objective metrics do exist, such as Mel cepstral distortion (MCD, <ref type="bibr" target="#b29">Kubichek, 1993)</ref>, we do not believe they should be considered reliable proxies for listening tests. Future research on speech synthesis for languages with few speakers should prioritize efficient and effective means of evaluating results. In many cases, including in the experiment described in §4.2, artificial data constraints can be placed on a language with more data, like English, to simulate a low-resource scenario. While this technique can be insightful and it is tempting to draw universal conclusions, English is linguistically very different from many of the other languages spoken in the world. Accordingly, we should be cautious not to assume that results from these types of experiments will necessarily transfer or extend to genuinely low-resource languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">How much data do you really need?</head><p>The first question to answer is whether our Indigenous language corpora ranging from 25 minutes to 3.46 hours of speech are sufficient for building neural speech synthesizers. Due to the prominence of Tacotron2 <ref type="bibr" target="#b45">(Shen et al., 2018)</ref>, it seems that many people have assumed that the data requirements for training any neural speech synthesizer of similar quality must be the same as the requirements for this particular model. As a result, some researchers still choose to implement either concatenative or HMM/GMM-based statistical parametric speech synthesis systems in low-resource situations based on the assumption that a "sufficiently large corpus [for neural TTS] is unavailable" <ref type="bibr">(James et al., 2020, p. 298)</ref>. We argue that attention-based models such as Tacotron2 should not be used as a benchmark for data requirements among all neural TTS methods, as they are notoriously difficult to train and unnecessarily inflate training data requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Replacing attention-based weak duration models</head><p>Tacotron2 is an autoregressive model, meaning it predicts the speech parameters ŷt from both the input sequence of text x and the previous speech parameters y 1 , ..., y t−1 . Typically, the model is trained with 'teacher-forcing', where the autoregressive frame y t−1 passed as input for predicting ŷt is taken from the ground truth acoustic features and not the prediction network's output from the previous frame ŷt−1 . As discussed by <ref type="bibr" target="#b33">Liu et al. (2019)</ref>, such a system might learn to copy the teacher forcing input or disregard the text en-tirely, which could still optimize Tacotron2's root mean square error function over predicted acoustic features, but result in an untrained or degenerate attention network which is unable to properly generalize to new inputs at inference time when the teacher forcing input is unavailable. Attention failures represent a characteristic class of errors for models such as Tacotron2, for example skipping or repeating words from the input text <ref type="bibr" target="#b55">(Valentini-Botinhao and King, 2021)</ref>.</p><p>There have been many proposals to improve training of the attention network, for example by guiding the attention or using a CTC loss function to respect the monotonic alignment between text inputs and speech outputs <ref type="bibr">(Tachibana et al., 2018; Liu et al., 2019; Zheng et al., 2019; Gölge, 2020)</ref>. As noted by <ref type="bibr" target="#b33">Liu et al. (2019)</ref>, increasing the socalled 'reduction factor' -which applies dropout to the autoregressive frames -can also help the model learn to rely more on the attention network than the teacher forcing inputs, but possibly at the risk of compromising synthesis quality.</p><p>FastSpeech2 <ref type="bibr" target="#b42">(Ren et al., 2021)</ref>, and similar systems like FastPitch <ref type="bibr">(Łańcucki, 2021)</ref>, present an alternative to Tacotron2-type attentive, autoregressive systems with similar listening test results and without the characteristic errors related to attention. Instead of modelling duration using attention, they include an explicit duration prediction module trained on phone duration targets extracted from the training data. For the original FastSpeech, target phone durations derived from the attention weights of a pre-trained Tacotron2 system were used to provide phone durations <ref type="bibr" target="#b43">(Ren et al., 2019)</ref>. In low-resource settings, however, there might not be sufficient data to train an initial Tacotron2 in the target language in the first place. For Fast-Speech2, phone duration targets are instead extracted using the Montreal Forced Aligner <ref type="bibr">(MFA, McAuliffe et al., 2017)</ref>, trained on the same data as used for TTS model training. We have found MFA can provide suitable alignments for our target languages, even with alignment models being trained on only limited data.</p><p>Faster convergence of text-acoustic feature alignments has been found to speed up overall encoder-decoder TTS model training, as stable alignments provide a solid foundation for further training of the decoder. <ref type="bibr" target="#b1">Badlani et al. (2021)</ref> show this by adding a jointly-learned alignment framework to a Tacotron2 architecture, reducing time to convergence. In contrast, they found that replacing MFA duration targets in FastSpeech2 training offers no benefit -forced alignment targets already provide enough information for more timeefficient training compared to an attention-based Tacotron2 system. Relieving the burden of learning an internal alignment model also opens the door to more data-efficient training. For example, Perez-Gonzalez-de-Martos et al. ( <ref type="formula">2021</ref>) submitted a non-attentive model trained from forced alignments to the Blizzard Challenge 2021, where their system was found to be among the most natural and intelligible in subjective listening tests despite only using 5 hours of speech; all other submitted systems included often significant amounts of additional training data (up to 100 hours total).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Experimental Comparison of Data Requirements for Neural TTS</head><p>To investigate the effects of differing amounts of data on the attention network, and in preparation for training systems with our limited Indigenous language data sets, we trained five Tacotron2 models on incremental partitions of the LJ Speech corpus of American English <ref type="bibr" target="#b23">(Ito and Johnson, 2017)</ref>. We used the NVIDIA implementation 5 with default hyperparameters apart from a reduced batch size of 32 to fit the memory capacity of our GPU resources. We artificially constrained the training data such that the first model saw only the first hour of data from the shuffled corpus, the second model that same first hour plus another two hours (3 total) etc., so that the five models were trained on 1, 5 https://github.com/NVIDIA/tacotron2</p><p>3, 5, 10 and 24 (full corpus) hours of speech. The models were trained for 100k steps and, as seen in Figure <ref type="figure" target="#fig_0">2</ref>, using up to 5 hours of data the attention mechanism does not learn properly, resulting in degenerate outputs.</p><p>For comparison, we trained seven FastSpeech2 models with batch size 16 for 200k steps on 15 and 30 minute, 1, 3, 5, 10 and 24 hour incremental partitions of LJ Speech. Our model<ref type="foot" target="#foot_5">6</ref> is based on an open-source implementation <ref type="bibr" target="#b8">(Chien, 2021)</ref>, which adds learnable speaker embeddings and a decoder postnet to the original model, as well as predicting pitch and energy values at the phone rather than frame level. We also added learnable language embeddings for supplementary experiments in cross-lingual fine-tuning; while not reported in this paper, we refer the interested reader to Pine (2021) for discussion of these experiments. Motivated by concerns of efficiency in model training and inference, and the possibility of overfitting a large model to limited amounts of data, we further modified the base architecture to match the Light-Speech model presented in <ref type="bibr" target="#b34">Luo et al. (2021)</ref>. We removed the energy adaptor, replaced the convolutional layers in the encoder, decoder and remaining variance predictors with depthwise separable convolutions <ref type="bibr" target="#b26">(Kaiser et al., 2018)</ref> and matched encoder and decoder convolutional kernel sizes with <ref type="bibr" target="#b34">Luo et al. (2021)</ref>. This reduced the number of model parameters from 35M<ref type="foot" target="#foot_6">7</ref> to 11.6M without noticeable change in voice quality and sped up train- ing by 33% on GPU or 64% on CPU. For additional discussion of the accessibility benefits of these changes with respect to Indigenous language communities, see Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Results</head><p>We conducted a short (10-15 minute) listening test to compare the two Tacotron2 models that trained properly (10h, full) against the seven FastSpeech2 models. We recruited 30 participants through Prolific, and presented each with four MUSHRA-style questions where they were asked to rank the 9 voices along with a hidden natural speech reference <ref type="bibr" target="#b24">(ITU-R, 2003)</ref>. MUSHRA-style questions were used as a practical way to evaluate this large number of models. While it only took 30 minutes to recruit 30 participants using Prolific, the quality of responses was quite varied. We rejected two outright as they seemingly did not listen to the stimuli and left the same rankings for every voice. Even still, there was a lot of variation in responses from the remaining participants, as seen in Figure <ref type="figure" target="#fig_1">3</ref>. We tested for significant differences between pairs of voices using Bonferroni-corrected Wilcoxon signed rank tests. Pairwise test results are summarized in the heat map of their p-values in Figure <ref type="figure" target="#fig_2">4</ref>.</p><p>In the results from the pairwise analysis, we can see that natural speech is rated as significantly more natural than all synthetic speech samples. Naturalness ratings for the FastSpeech2 voices trained on 15m and 30m of data are significantly lower than all other voices, and significantly different from each other. The results for the remaining wise test between the model on the y-axis and the model on the x-axis. Darker cells show stronger sig-nificance; grey cells did not show a significant difference in listening test results. FS2 refers to models built with FastSpeech2, TT2 refers to models built with Tacotron2, and 'Ref' to reference recordings. Samples available at https://roedoejet.github.io/ msc_listening_tests_data/ voices, while showing consistent improvements in naturalness ratings as more data is added (as shown in Figure <ref type="figure" target="#fig_1">3</ref>), are not significantly different from each other. This is a relevant and important finding for low-resource speech synthesis because it shows that a FastSpeech2 voice built with 3 hours of data can achieve subjective naturalness ratings which are not significantly different from a Tacotron2 voice built with 24 hours of data. Similarly, the results of the listening test for our Fast-Speech2 voice built with 1 hour of data are not significantly different from our Tacotron2 voice built with 10 hours of data. Additionally, while all the FastSpeech2 voices were intelligible, all Tacotron2 models trained with less than 10 hours of data produced unintelligible speech.</p><formula xml:id="formula_0">Ref FS2 15m FS2 30m FS2 1hr FS2 3hr FS2 5hr FS2 10hr FS2 Full TT2 10hr F S 2 1 5 m F S 2 3 0 m F S 2 1 h r F S 2 3 h r F S 2 5 h r F S 2 1 0 h r F S 2 F u l l T T 2 1 0 h r T T 2 F u</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Indigenous Language Experiments</head><p>Despite the difficulty in evaluation ( §4.1), we built and evaluated a number of TTS systems for the Indigenous languages described in §3. We had a baseline concatenative model available for Kanien'kéha that we had previously built using Festival and Multisyn <ref type="bibr">(Taylor et al., 1998; Clark et al., 2007)</ref>. Additionally, we trained cold-start FastSpeech2 models for each language, as well as models fine-tuned for 25k steps from a multilin-gual, multispeaker FastSpeech2 model pre-trained on a combination of VCTK <ref type="bibr" target="#b60">(Yamagishi et al., 2019)</ref>, Kanien'kéha and Gitksan recordings. A rule-based mapping from orthography to pronunciation form was developed for each language using the 'g2p' Python library in order to perform alignment and synthesis at the phone-level instead of character-level <ref type="bibr">(Pine et al., Under Review)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Results</head><p>We carried out listening test evaluations of Gitksan and Kanien'kéha models. Participants were recruited by contacting teachers, learners and linguists with at least some familiarity with the languages.</p><p>For the Kanien'kéha listening test, 6 participants were asked to answer 20 A/B questions comparing synthesized utterances from the various models. We used A/B tests for more targeted comparisons between different systems, namely cold-start vs. fine-tuned and neural vs. concatenative. Results showed that 72.2% of A/B responses from participants preferred our FastSpeech2 model over our baseline concatenative model. In addition, 81.7% of A/B responses from participants preferred the cold-start to the model fine-tuned on the multispeaker, multi-lingual model, suggesting that the transfer learning approach discussed in §2.3 might not be necessary for models with explicit durations such as FastSpeech2 since they are relieved of the burden to learn an implicit model of duration through attention from limited data.</p><p>For the Gitksan listening test, we did not build a concatenative model as with Kanien'kéha and so we were not comparing different models, but rather just gathering opinions on the quality of the cold-start FastSpeech2 model. Accordingly, 10 MOS-style questions were presented to 12 participants for both natural utterances and samples from our FastSpeech2 model. The model received a 3.56 ± 0.26 MOS compared with a MOS for the reference recordings of 4.63 ± 0.19 as shown in Figure <ref type="figure" target="#fig_3">5</ref>. While both Kanien'kéha and Gitksan results seem to corroborate our belief that these models should be of reasonable quality despite limited training data, it is difficult to make any conclusive statement given the low number of eligible participants available for evaluation.</p><p>As the main goal of our efforts here is to eventually integrate our speech synthesis systems into a pedagogical setting, we also asked the 18 people who participated across Kanien'kéha and Gitk-   san listening tests directly whether they approved of the synthesis quality. As seen in Figure <ref type="figure" target="#fig_5">6</ref>, participant responses were generally positive; full responses are reported in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Integrating TTS in the Classroom</head><p>Satisfying the goal of adding supplementary audio to a reference tool like Kawennón:nis can be straightforwardly implemented by linking entries in the verb conjugator to pre-generated audio for the domain from a static server. This implementation also limits the potential of out of domain utterances that might be deemed inappropriate, which is an ethical concern in communities with low numbers of speakers where the identity of the 'model' speaker is easily determined.</p><p>However, the ability to synthesize novel utterances could be pedagogically useful. Students often come into contact with words or sentences which do not have audio, and teachers often have to prepare new thematic word lists or vocabulary lessons that could benefit from a more general purpose speech synthesis solution. In those cases, with community and speaker input, we might consider what controls would be necessary for the users of this technology. One potential solution is the variance adaptor architecture present in Fast-Speech2, allowing for phone-level control of duration, pitch and energy; an engaging demonstration of a graphical user interface for the corresponding controls in a FastPitch model is also available. <ref type="foot" target="#foot_7">8</ref> We would like to focus further efforts on designing a user interface for speech synthesis systems that satisfies ethical concerns while prioritizing language pedagogy as the fundamental use case.</p><p>In addition to fine-grained prosodic controls, we would like to explore the synthesis of hyperarticulated speech, as often used by language teachers when modelling pronunciation of unfamiliar words or sounds for students. This style of speech typically involves adjustment beyond the parameters of pitch, duration and energy, and is characterized by more careful enunciation of individual phones than is found in normal speech. This problem has parallels to the synthesis of Lombard speech <ref type="bibr" target="#b21">(Hu et al., 2021)</ref>, as used to improve intelligibility by speakers who find themselves in noisy environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we presented the first neural speech synthesis systems for Indigenous languages spoken in Canada. Subjective listening tests showed encouraging results for the naturalness and acceptability of voices for two languages, Kanien'kéha and Gitksan, despite limited training data availability (3.5 hours and 35 minutes, respectively). More extensive evaluation on English shows that the FastSpeech2 architecture can produce speech with similar quality to a Tacotron2 system using a fraction of the amount of speech usually considered for neural speech synthesis. Notably, a Fast-Speech2 voice trained on 1 hour of English speech achieved subjective naturalness ratings not significantly different from a Tacotron2 voice using 10 hours of data, while a 3-hour FastSpeech2 system showed no significant difference from a 24-hour Tacotron2 voice.</p><p>We attribute these results to the fact that Fast-Speech2 learns input token durations from forced alignments, rather than jointly learning to align linguistic inputs to acoustic features alongside the acoustic feature prediction task as in attention-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Compute, Accessibility, &amp; Environmental Impact</head><p>For reasons of environmental impact and accessibility, reducing the amount of computation required for both training and inference is important for any neural speech synthesis system, particularly so for Indigenous languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Accessibility, Training &amp; Inference Speed</head><p>While language revitalization efforts are mostly encouraging about integrating new technologies into curriculum, there is a growing awareness of the potential harms. Beyond assessing the benefits and risks of introducing a new technology into language revitalization efforts, communities are concerned with the way the technology is researched and developed, as this process has the ability to empower or disempower language communities in equal measure <ref type="bibr">(Alia, 2009; Brinklow et al., 2019)</ref>. The current model for developing speech synthesis systems is not very equitablemodels need to be run on GPUs by people with specialized training. For Indigenous communities to create speech synthesis tools for their languages, they should not be required to hand over their language data to a large government or corporate organization. A pre-training, fine-tuning pipeline could be attractive for this reason; communities could fine-tune their own models on a laptop if a multilingual/multi-speaker model were pre-trained on GPUs at a larger institution. Reducing the computational requirements for training and inference of these models could help ensure language communities have greater control over the process of the development of these systems, less dependence on governmental organizations or corporations, and more sovereignty over their data (Keegan, 2019). <ref type="bibr" target="#b47">Strubell et al. (2019)</ref> present an argument for equitable access to computational resources for NLP research; put another way, we might say that systems which require less compute are more accessible. Reducing the number of parameters in a neural TTS model should translate to increased efficiency, and might make the model less prone to overfitting when training on limited amounts of data. As discussed in §4.2.2, we modified the base implementation of FastSpeech2 from Chien (2021) closely following the lightweight alternative discovered through neural architecture search in <ref type="bibr" target="#b34">Luo et al. (2021)</ref>. These changes reduced the size of the model from Chien (2021) from 35M to 11.6M parameters, reduced the size of the stored model from 417 MB to 135 MB and significantly improved inference and train times as summarized in Table <ref type="table" target="#tab_1">1</ref>. We saw a 33% improvement in average batch processing times on the GPU during training, and 64% on the CPU, which may be even more relevant for Indigenous language communities with limited computational resources. During inference, we saw a 15% speed-up on GPU and 57% on CPU.</p><p>Results were timed by running the model for 300 repetitions and taking the mean. The GPU (Tesla V100-SXM2 16GB) was warmed up for 10 repetitions before timing started, and PyTorch's builtin GPU synchronization method was used to synchronize timing (which occurs on the CPU) with the training or inference running on the GPU. CPU tests were performed on an Intel(R) Xeon(R) CPU E5-2650 v2 @ 2.60GHz with 4 cores and 16GB memory reserved. All timings used a batch size of 16.</p><p>• Out of the two voices I hear, the first was clearer to understand</p><p>• Yes, voices sounds really good!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• yes</head><p>Gitksan responses:</p><p>• yes</p><p>• Yes, but the ones that have the most whistling or buzzing would be annoying.</p><p>• maybe?? I think for a talking dictionary people do want to hear original pronunciations, but it could be a useful interim solution or a way to do short phrases!</p><p>• Yes</p><p>• Yes.</p><p>• Assuming there is a single control for the last section of the survey/test, then some of the synthesised voices actually sound really good and I would be comfortable hearing those in an online dictionary where audio didn't exist for a particular word or phrase.</p><p>• yes</p><p>• The ones with higher ratings for sure, some of the lower ratings were just about the sound quality because that hampered hearing the speech quality. So I may have confounded the results with that, but point remains that it is always good to try to avoid poor audio recordings for online dictionaries</p><p>• Maybe/yes</p><p>• only ones rated fair or above fair</p><p>• Absolutely yes</p><p>• yes, as long as they were identified as synthesized</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization of Tacotron2 Attention Network Weights extracted after 100k steps trained on the LJ corpus. The weights of the attention network should be diagonal and monotonic as seen in subfigure (b). Subfigure (a) shows that the network trained on a 5 hour subset of the LJ corpus results in a degenerate attention network.</figDesc><graphic url="image-1.png" coords="6,94.25,77.69,158.54,122.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Box plot of survey data from MUSHRA questions comparing Tacotron2 (TT2) and FastSpeech2 (FS2) models with constrained amounts of training data. 'Ref' refers to reference recordings of natural speech.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Pairwise Bonferroni-corrected Wilcoxon signed rank tests between each pair of voices. Cells correspond to the significance of the result of the pairwise test between the model on the y-axis and the model on the x-axis. Darker cells show stronger sig-nificance; grey cells did not show a significant difference in listening test results. FS2 refers to models built with FastSpeech2, TT2 refers to models built with Tacotron2, and 'Ref' to reference recordings. Samples available at https://roedoejet.github.io/ msc_listening_tests_data/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Box plot of MOS results for Gitksan listening test. 'Ref' is the reference voice and 'Phone' is the phone-based FastSpeech2 neural model. Variable results for the reference voice are likely due to the natural speech recordings coming from a non-native speaker.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Responses from qualitative survey asking participants "Would you be comfortable with any of the voices you heard being played online, say for a digital dictionary or verb conjugator if no other recording existed?". No participants responded "no".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Mean and standard deviation of training and inference times for a single forward pass of baseline Fast-Speech2 and adapted models.</figDesc><table><row><cell></cell><cell>FastSpeech2</cell><cell>Adapted System</cell></row><row><cell>Training</cell><cell cols="2">GPU 90.52 ms (σ 3.31) CPU 7561.50 ms (σ 263.55) 2720.88 ms (σ 92.99) 60.04 ms (σ 1.70)</cell></row><row><cell>Inference</cell><cell>GPU 12.00 ms (σ 0.30) CPU 138.73 ms (σ 3.94)</cell><cell>10.23 ms (σ 0.78) 59.50 ms (σ 1.85)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://wiki.laptop.org/go/ Instructions_for_implementing_a_new_language_%</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22" xml:id="foot_1">22voice%22_for_Speak_on_the_XO   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">As there are different variations of spelling, we use the spelling used in the communities of Kahnawà:ke and Kahnesetà:ke throughout this paper</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">We use Lonnie Hindle and Bruce Rigsby's spelling of the language, which, with the use of 'k' and 'a' is a blend of upriver (gigeenix) and downriver (gyets) dialects</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4">4 https://wsanecschoolboard.ca/sencotenlanguage/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">https://github.com/roedoejet/FastSpeech2</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">In the implementation of Chien (2021); the original Fast-Speech2 is slightly smaller at 27M parameters.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7">https://fastpitch.github.io/ based architectures such as Tacotron2. Given forced alignments of sufficient quality, which we found to be achievable even by training a Montreal Forced Aligner model only on our limited Indigenous language training data, this makes for more data-efficient training of neural TTS systems than has generally been explored in previous work. These findings show great promise for future work in low-resource TTS for language revitalization, especially as they come from systems trained from scratch on such limited data, rather than pre-training on a high-resource language and subsequent fine-tuning on limited target language data.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to gratefully acknowledge the many people who worked to record the audio for the speech synthesis systems described in this project. In particular, Satewas Harvey Gabriel, and PENÁĆ David Underwood.</p><p>Much of the text and experimentation related to this paper was submitted as partial fulfillment of the first author's M.Sc. dissertation at the University of Edinburgh <ref type="bibr" target="#b39">(Pine, 2021)</ref>.</p><p>This work was supported in part by the UKRI Centre for Doctoral Training in Natural Language Processing, funded by the UKRI (grant EP/S022481/1) and the University of Edinburgh, School of Informatics and School of Philosophy, Psychology &amp; Language Sciences.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 CO2 Consumption</head> <ref type="bibr" target="#b47">Strubell et al. (2019)</ref> <p>also argue that NLP researchers should have a responsibility to disclose the environmental footprint of their research, in order for the community to effectively evaluate any gains and to allow for a more equitable and reproducible field.</p><p>All experiments for this paper requiring a GPU were run on the Canadian General Purpose Science Cluster (GPSC) in Dorval, Quebec. Experiments were all run on single Tesla V100-SXM2 16GB GPUs. <ref type="bibr" target="#b47">Strubell et al. (2019)</ref> provide the following equation for estimating CO 2 production:</p><p>where t is time, p t is total power for training, p c is average draw of CPU sockets, p r is average DRAM memory draw, g is the number of GPUs used in training and p g is the average draw from GPUs. In our case, we estimate t to be equal to 1,541.98 9 after summing the time for experiments based on their log files, p c is 75 watts, p r is 6 watts, g is 1, and p g is 250 watts, and the equation for grams of CO2 consumption is CO 2 = 34.5p t as the average carbon footprint of electricity distributed in Quebec is estimated at 9 Note this estimate is based on the total number of hours spent running experiments from the M.Sc. dissertation this paper draws its experiments from. There were additional models trained for experiments that are not discussed in this paper. As such, this is a generous overestimation of t.</p><p>34.5g CO2eq/kWh <ref type="bibr" target="#b32">(Levasseur et al., 2021)</ref>. This results in a total equivalent carbon consumption of 27,821.65 grams, roughly equivalent to driving a single passenger gas-powered vehicle for 110 kilometres according to the average rate of 404 grams/mile <ref type="bibr">(EPA, 2019)</ref>. This is a comparatively low CO2 consumption for over 1500 GPU hours, largely due to the low CO2/kWh output of Quebec electricity when compared with the 2019 USA average of 400g CO2eq/kWh (EPA, 2019). However, CO2 equivalents are just a proxy for environmental impact and should not be understood to comprehensively account for social and environmental impact. Hydroelectric dam projects in Quebec, like the ones powering the GPSC have a sordid and complex history in the province. Innu Nation Grand Chief Mary Ann Nui spoke to this when she commented that "over the past 50 years, vast areas of our ancestral lands were destroyed by the Churchill Falls hydroelectric project, people lost their land, their livelihoods, their travel routes, and their personal belongings when the area where the project is located was flooded. Our ancestral burial sites are under water, our way of life was disrupted forever. Innu of Labrador weren't informed or consulted about that project" (Innu-Atikamekw-Anishnabeg Coalition, 2020).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question:</head><p>"Would you be comfortable with any of the voices you heard being played online, say for a digital dictionary or verb conjugator if no other recording existed?" Kanien'kéha responses:</p><p>• Yes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• yes • Yes</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Valerie</forename><surname>Alia</surname></persName>
		</author>
		<title level="m">The New Media Nation: Indigenous Peoples and Global Communication</title>
				<imprint>
			<publisher>Berghahn Books</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>ned -new edition, 1 edition</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Badlani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Łancucki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10447</idno>
		<title level="m">One TTS Alignment To Rule Them All</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Kenneth</forename><forename type="middle">R</forename><surname>Beesley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lauri</forename><surname>Karttunen</surname></persName>
		</author>
		<title level="m">Finite State Morphology</title>
				<imprint>
			<publisher>CSLI Publications</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Decolonising speech and language technology</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
				<meeting>the 28th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3504" to="3519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Speak and unSpeak with PRAAT</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Boersma</surname></persName>
		</author>
		<author>
			<persName><surname>Vincent Van Heuven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Glot International</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">9/10</biblScope>
			<biblScope unit="page" from="341" to="347" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Indigenous language technologies: Anti-colonial oases in a colonizing (digital) world. WINHEC: International</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Thanyehténhas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brinklow</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Indigenous Education Scholarship</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="239" to="266" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Indigenous Language Technologies &amp; Language Reclamation in Canada</title>
		<author>
			<persName><forename type="first">Nathan Thanyehténhas</forename><surname>Brinklow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Littell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Delaney</forename><surname>Lothian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Pine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heather</forename><surname>Souter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Conference on Language Technologies for All</title>
				<meeting>the 1st International Conference on Language Technologies for All</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="402" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Annotating Multi-media/Multi-modal Resources with ELAN</title>
		<author>
			<persName><forename type="first">Hennie</forename><surname>Brugman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Russel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC&apos;04)</title>
				<meeting>the Fourth International Conference on Language Resources and Evaluation (LREC&apos;04)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Chung-Ming</forename><surname>Chien</surname></persName>
		</author>
		<idno>date: 2020-06-25T13:57:53Z</idno>
		<ptr target="https://github.com/ming024/FastSpeech2" />
		<title level="m">ming024/FastSpeech2</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Clark</forename><surname>Robert</surname></persName>
		</author>
		<title level="m">Simple4all. In Proc. Interspeech</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="1502" to="1503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multisyn: Open-domain unit selection for the festival speech synthesis system</title>
		<author>
			<persName><forename type="first">Korin</forename><surname>Robert Aj Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Richmond</surname></persName>
		</author>
		<author>
			<persName><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="317" to="330" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Michael</forename><surname>Conrad</surname></persName>
		</author>
		<ptr target="https://www.cherokeelessons.com/content/tacotron2-and-cherokee-tts/" />
		<title level="m">Tacotron2 and Cherokee TTS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A unified phonological representation of South Asian languages for multilingual text-tospeech</title>
		<author>
			<persName><forename type="first">Isin</forename><surname>Demirsahin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jansche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Gutkin</surname></persName>
		</author>
		<idno type="DOI">10.21437/SLTU.2018-17</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. The 6th Intl. Workshop on Spoken Language Technologies for Under-Resourced Languages (SLTU)</title>
				<meeting>The 6th Intl. Workshop on Spoken Language Technologies for Under-Resourced Languages (SLTU)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="80" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Duddington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reece</forename><surname>Dunn</surname></persName>
		</author>
		<ptr target="http://espeak.sourceforge.net/" />
		<title level="m">eSpeak: Speech Synthesizer</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Emissions &amp; generation resource integrated database</title>
		<ptr target="https://www.epa.gov/egrid" />
	</analytic>
	<monogr>
		<title level="m">EPA</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Report on the status of B</title>
		<ptr target="https://fpcc.ca/resource/fpcc-report-of-the-status-of-b-c-first-nations-languages-2018/" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Clarissa</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gitksan</forename><forename type="middle">Research</forename><surname>Lab</surname></persName>
		</author>
		<title level="m">Three Gitksan Texts. Papers for the International Conference on Salish and Neighbouring Languages</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="47" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Solving Attention Problems of TTS models with Double Decoder Consistency</title>
		<author>
			<persName><forename type="first">Eren</forename><surname>Gölge</surname></persName>
		</author>
		<ptr target="https://erogol.com/solving-attention-problems-of-tts-models-with-double-decoder-consistency/" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kanien&apos;keha / Mohawk Indigenous language revitalisation efforts in Canada</title>
		<author>
			<persName><forename type="first">Grace</forename><forename type="middle">A</forename><surname>Gomashie</surname></persName>
		</author>
		<idno type="DOI">10.7202/1060864ar</idno>
	</analytic>
	<monogr>
		<title level="j">McGill Journal of Education / Revue des sciences de l&apos;éducation de McGill</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="151" to="171" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">FonBund: A Library for Combining Cross-lingual Phonological Segment Data</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Gutkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jansche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatiana</forename><surname>Merkulova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
				<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2236" to="2240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Preliminary Plains Cree Speech Synthesizer</title>
		<author>
			<persName><forename type="first">Atticus</forename><surname>Harrigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antti</forename><surname>Arppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Mills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on the Use of Computational Methods in the Study of Endangered Languages Volume</title>
				<meeting>the 3rd Workshop on the Use of Computational Methods in the Study of Endangered Languages Volume<address><addrLine>Honolulu</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="64" to="73" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Whispered and Lombard Neural Speech Synthesis</title>
		<author>
			<persName><forename type="first">Qiong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Bleisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petko</forename><surname>Petkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuomo</forename><surname>Raitio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Marchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Lakshminarasimhan</surname></persName>
		</author>
		<idno type="DOI">10.1109/SLT48900.2021.9383454</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE Spoken Language Technology Workshop (SLT)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="454" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Export of Canadian Hydropower to the United States -First Nations in Québec and Labrador Unite to Oppose Hydro-Québec Project</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Innu-Atikamekw-Anishnabeg Coalition</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Keith</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><surname>Johnson</surname></persName>
		</author>
		<ptr target="https://keithito.com/LJ-Speech-Dataset/" />
		<title level="m">The LJ speech dataset</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Recommendation ITU-R BS.1534-1 -Method for the subjective assessment of intermediate quality level of coding systems</title>
		<author>
			<persName><surname>Itu-R</surname></persName>
		</author>
		<idno>ITU-R BS.1534-1</idno>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>International Telecommunication Union</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Developing resources for te reo Māori text to speech synthesis system</title>
		<author>
			<persName><forename type="first">Jesin</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabella</forename><surname>Shields</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebekah</forename><surname>Berriman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Keegan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Watson</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58323-1_32</idno>
	</analytic>
	<monogr>
		<title level="m">Text, Speech, and Dialogue</title>
				<editor>
			<persName><forename type="first">P</forename><surname>Sojka</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Kopeček</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Pala</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Horák</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="294" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Depthwise Separable Convolutions for Neural Machine Translation</title>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francois</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Kawennón:nis: the wordmaker for Kanyen&apos;kéha</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Kazantseva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Owennatekha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronkwe</forename><forename type="middle">'</forename><surname>Maracle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josiah</forename><surname>Maracle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Pine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Computational Modeling of Polysynthetic Languages</title>
				<meeting>the Workshop on Computational Modeling of Polysynthetic Languages<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="53" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Issues with Māori sovereignty over Māori language data</title>
		<author>
			<persName><forename type="first">Te</forename><surname>Taka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keegan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Let The Languages Live 2019 Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mel-cepstral distance measure for objective speech quality assessment</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kubichek</surname></persName>
		</author>
		<idno type="DOI">10.1109/PACRIM.1993.407206</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Pacific Rim Conference on Communications Computers and Signal Processing</title>
				<meeting>IEEE Pacific Rim Conference on Communications Computers and Signal Processing</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="125" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Spoken Speaker Identification based on Gaussian Mixture Models : Python Implementation</title>
		<author>
			<persName><forename type="first">Abhijeet</forename><surname>Kumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fastpitch: Parallel Text-to-Speech with Pitch Prediction</title>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Łańcucki</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP39728.2021.9413889</idno>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6588" to="6592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improving the accuracy of electricity carbon footprint: Estimation of hydroelectric reservoir greenhouse gas emissions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Levasseur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mercier-Blais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Prairie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Turpin</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.rser.2020.110433</idno>
	</analytic>
	<monogr>
		<title level="j">Renewable and Sustainable Energy Reviews</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="page">110433</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Peng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xixin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyin</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01145</idno>
		<title level="m">Maximizing Mutual Information for Tacotron</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">LightSpeech: Lightweight and Fast Text to Speech with Neural Architecture Search</title>
		<author>
			<persName><forename type="first">Renqian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinzhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5699" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mcauliffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michaela</forename><surname>Socolof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Mihuc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Sonderegger</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2017-1386</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<publisher>ISCA</publisher>
			<biblScope unit="page" from="498" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Community-based language planning: Perspectives from Indigenous language revitalization</title>
		<author>
			<persName><forename type="first">Teresa</forename><forename type="middle">L</forename><surname>Mccarty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Routledge handbook of language revitalization</title>
				<imprint>
			<publisher>Routledge</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="22" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cultural continuity, traditional Indigenous language, and diabetes in Alberta first nations: a mixed methods study</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Oster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Grier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rick</forename><surname>Lightning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Mayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><surname>Toth</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12939-014-0092-4</idno>
	</analytic>
	<monogr>
		<title level="j">International journal for equity in health</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">92</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">VRAIN-UPV MLLP&apos;s system for the Blizzard Challenge</title>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Perez-Gonzalez-De-Martos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Sanchis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfons</forename><surname>Juan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Blizzard Challenge 2021 Workshop</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Low Resource Speech Synthesis</title>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Pine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">M.Sc. dissertation</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Under Review. G i 2P i : Rule-based, index-preserving grapheme-tophoneme transformations. Aidan Pine and Mark Turin</title>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Pine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Littell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Joanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Huggins-Daines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fineen</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><forename type="middle">Antonio</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shankhalika</forename><surname>Srikanth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Delaisie</forename><surname>Torkornoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabrina</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1093/acrefore/9780199384655.013.8</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Language Revitalization. Oxford Research Encyclopedia of Linguistics</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Anishinaabemodaa Pane Oodenang: a qualitative study of Anishinaabe language revitalization as selfdetermination in Manitoba and Ontario</title>
		<author>
			<persName><forename type="first">Brock</forename><surname>Thorbjorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pitawanakwat</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Victoria</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">FastSpeech 2: Fast and High-Quality End-to-End Text to Speech</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">FastSpeech: Fast, Robust and Controllable Text to Speech</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangjun</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Indigenous languages in Canada</title>
		<author>
			<persName><forename type="first">Keren</forename><surname>Rice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Canadian Encyclopedia</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rif</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Agiomyrgiannakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4779" to="4783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<ptr target="https://www12.statcan.gc.ca/census-recensement/2016/dp-pd/index-eng.cfm" />
	</analytic>
	<monogr>
		<title level="m">Census of population</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Energy and Policy Considerations for Deep Learning in NLP</title>
		<author>
			<persName><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1355</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3645" to="3650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Parallel extinction risk and global distribution of languages and species</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature01607</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">423</biblScope>
			<biblScope unit="page" from="276" to="279" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Efficiently trainable text-tospeech system based on deep convolutional networks with guided attention</title>
		<author>
			<persName><forename type="first">Hideyuki</forename><surname>Tachibana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katsuya</forename><surname>Uenoyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunsuke</forename><surname>Aihara</surname></persName>
		</author>
		<idno type="DOI">10.1109/icassp.2018.8461829</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="4784" to="4788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Soong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.15561</idno>
		<title level="m">A Survey on Neural Speech Synthesis</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The architecture of the Festival speech synthesis system</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Caley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Third ESCA/COCOSDA Workshop (ETRW) on Speech Synthesis</title>
				<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="147" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">End-to-end Text-to-speech for</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan-Jui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Chieh</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hungyi</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Low-resource Languages by Cross-Lingual Transfer Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2075" to="2079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Towards the Speech Synthesis of Raramuri: A Unit Selection Approach based on Unsupervised Extraction of Suffix Sequences</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Urrea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Abel Herrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maribel</forename><forename type="middle">Alvarado</forename><surname>Camacho</surname></persName>
		</author>
		<author>
			<persName><surname>García</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research in Computing Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="243" to="256" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Detection and Analysis of Attention Errors in Sequenceto-Sequence Text-to-Speech</title>
		<author>
			<persName><forename type="first">Cassia</forename><surname>Valentini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Botinhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>King</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2021-286</idno>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2021</title>
				<imprint>
			<publisher>ISCA</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2746" to="2750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Cross-lingual Transfer of Phonological Features for Low-resource Speech Synthesis</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Korin</forename><surname>Richmond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th ISCA Speech Synthesis Workshop</title>
				<meeting>11th ISCA Speech Synthesis Workshop</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="160" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Are we using enough listeners? No!-an empirically-supported critique of Interspeech 2014 TTS evaluations</title>
		<author>
			<persName><forename type="first">Mirjam</forename><surname>Wester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cassia</forename><surname>Valentini-Botinhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustav</forename><forename type="middle">Eje</forename><surname>Henter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="3476" to="3480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Healing through language: Positive physical health effects of indigenous language use</title>
		<author>
			<persName><forename type="first">D</forename><surname>Whalen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Moss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daryl</forename><surname>Baldwin</surname></persName>
		</author>
		<idno type="DOI">10.12688/f1000research.8656.1</idno>
		<imprint>
			<date type="published" when="1000">2016. F1000Research</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">852</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">A Navajo Language Text-to-Speech Synthesizer</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Whitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Sproat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chilin</forename><surname>Shih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>AT&amp;T Bell Laboratories</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">CSTR VCTK Corpus: English Multispeaker Corpus for CSTR Voice Cloning Toolkit (version 0.92)</title>
		<author>
			<persName><forename type="first">Junichi</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Veaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kirsten</forename><surname>Mac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Donald</forename></persName>
		</author>
		<idno type="DOI">10.7488/ds/2645</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh, The Centre for Speech Technology Research (CSTR)</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Forward-Backward Decoding for Regularizing Endto-End TTS</title>
		<author>
			<persName><forename type="first">Yibin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shifeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">K</forename><surname>Soong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengqi</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="1283" to="1287" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
