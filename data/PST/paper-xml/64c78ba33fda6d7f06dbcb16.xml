<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This paper is included in the Proceedings of the 17th USENIX Symposium on Operating Systems Design and Implementation</title>
				<funder ref="#_jCwH5t6">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_ZTkjZH7">
					<orgName type="full">Amazon</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuke</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Boyuan</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University of Rochester</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tong</forename><surname>Geng</surname></persName>
							<affiliation key="aff5">
								<orgName type="laboratory">Pacific Northwest National Laboratory</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Barker</surname></persName>
							<affiliation key="aff5">
								<orgName type="laboratory">Pacific Northwest National Laboratory</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yufei</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<addrLine>July 10-12</addrLine>
									<postCode>2023 ?</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of California Santa Barbara</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Rochester</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">Pacific Northwest National Laboratory; Yufei Ding</orgName>
								<orgName type="institution">University of California Santa Barbara</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">This paper is included in the Proceedings of the 17th USENIX Symposium on Operating Systems Design and Implementation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The increasing size of input graphs for graph neural networks (GNNs) highlights the demand for using multi-GPU platforms. However, existing multi-GPU GNN systems optimize the computation and communication individually based on the conventional practice of scaling dense DNNs. For irregularly sparse and fine-grained GNN workloads, such solutions miss the opportunity to jointly schedule/optimize the computation and communication operations for high-performance delivery.</p><p>To this end, we propose MGG , a novel system design to accelerate full-graph GNNs on multi-GPU platforms. The core of MGG is its novel dynamic software pipeline to facilitate fine-grained computation-communication overlapping within a GPU kernel. Specifically, MGG introduces GNN-tailored pipeline construction and GPU-aware pipeline mapping to facilitate workload balancing and operation overlapping. MGG also incorporates an intelligent runtime design with analytical modeling and optimization heuristics to dynamically improve the execution performance. Extensive evaluation reveals that MGG outperforms state-of-the-art full-graph GNN systems across various settings: on average 4.41?, 4.81?, and 10.83? faster than DGL, MGG-UVM, and ROC, respectively. Kernel &amp; Runtime Manager GNN-tailored Pipeline Construct. ( ? ) GPU-aware Pipeline Mapping ( ? ) MGG SHMEM Library (e.g., NVSHMEM) Optimized Param. Graph Loader &amp; Model Initializer Runtime Param. Optimizer( ? ) Performance Feedbacks GNN Model Node Embedding Graph Structure NVIDIA DGX Multi-GPU Platform</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the recent years, graph-based deep learning has attracted lots of attention from the research and industry communities. Among various graph-learning methods, graph neural network (GNN) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b48">49]</ref> gets highlighted most due to its success in many deep learning tasks (e.g., node feature vector (embedding) generation for node classification <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19]</ref> and link prediction <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b41">42]</ref>). GNNs consist of several layers, where layer k + 1 computes the embedding for a node v based on the embeddings at the previous layer k (k ? 0) by applying</p><formula xml:id="formula_0">a (k+1) v = Aggregate (k+1) (h (k) u |u ? N(v) ? h (k) v ) h (k+1) v = Update (k+1) (a (k+1) v ) where h (k) v</formula><p>is the embedding of node v at layer k. The Aggregate function accumulates neighbors'(N(v)) embeddings of node v. The Update function consists of a fullyconnected NN layer. The neighbor aggregation (Aggregate) is the key bottleneck that dominates the overall computation due to its high computation sparsity and irregularity <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b49">50]</ref>. Compared with conventional graph analytics (e.g., random walk <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b38">39]</ref>), GNN features higher accuracy <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b48">49]</ref> and better generality <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b54">55]</ref> on various applications.</p><p>GNN computation on large input graphs (millions/billions of nodes and edges) usually counts on powerful multi-GPU platforms (e.g., NVIDIA DGX <ref type="bibr" target="#b34">[35]</ref>) for scaling up the performance. The multi-GPU system (that can potentially store all data required for the computation in the aggregate memory of all GPUs on a single machine) can benefit from aggregated memory capacity and bandwidth (HBM and NVLinks) with more GPUs. There is also a popular trend for state-ofthe-art hyper-scale systems employing GPU-centric building blocks. For example, the recent NVIDIA DGX SuperPod <ref type="bibr" target="#b32">[33]</ref> consists of 32?DGX-H100 servers (each with 8?H100). Unfortunately, the runtime performance of GNNs does not scale proportionally with the aggregated compute capability and memory capacity of the platform. This is mainly because the irregular and sparse local memory access of neighbor aggregation in the single-GPU settings now "scales" to more expensive inter-GPU communication (i.e., remote memory access). Such intensive inter-GPU communication becomes the new critical path of multi-GPU GNN execution and offsets the performance gains from multi-GPU computation parallelism.</p><p>Based on this observation, we highlight a more promising way of formalizing GNN computation on multi-GPU systems.</p><p>Our key insight is that GNN execution can be more precisely abstracted as a fine-grained dynamic software pipeline to encourage communication and computation overlapping, which will largely hide the communication cost. The opportunities for building such fine-grained pipelines widely exist at different granularities in GNNs. For instance, on a single graph node, the remote neighbor access can be overlapped with the local neighbor computation. Among different graph nodes, the remote neighbor access for certain nodes would potentially be overlapped with the local neighbor computation of some other nodes. However, prior research could hardly exploit such benefits since they rely on hardware and software infrastructures tailored for coarse-grained <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28]</ref> and regular communication patterns <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref>. To capitalize on the finegrained pipelining benefits, there are three major challenges.</p><p>The first challenge is how to craft the pipeline structure. A work-efficient pipeline for GNNs demands comprehensively considering multiple factors (e.g., the operations and the number/granularity of each pipeline stage) to best fit the GNN algorithm and multi-GPU computation/communication. The second challenge is how to map the pipeline to the GPU processing units. Given the GPU's architectural complexity (e.g., multi-granular processing units and multi-layer memory hierarchy), different mapping and primitive choices would bring performance and design flexibility tradeoffs. The third challenge is how to find and adapt toward the "optimal" pipeline configuration swiftly. Given the diversity of GNN inputs (e.g., graph structures) and hardware (e.g., different types/numbers of GPUs), pinpointing the best-off design configuration with high-performance delivery relies on combined insights from the properties of the software pipeline, GNN inputs, and GPU programming and execution paradigms.</p><p>To this end, we introduce a set of principles for multi-GPU GNN acceleration via a fine-grained dynamic software pipeline. To construct fine-grained pipelines, the original coarse-grained irregular GNN computation should be breakdown into fine-grained operations. The joint optimization of the GNN workload granularity and data layout should be carried out to facilitate operation overlapping. To map pipelines to GPUs, the proper GPU logical processing units (e.g., thread, warp, and block) should be selected for promoting GPU kernel efficiency and design flexibility. In addition, the right choice of communication primitives (e.g., NVSHMEM <ref type="bibr" target="#b35">[36]</ref>) should be determined to provide fine-grained inter-GPU communication support. To adapt pipelines dynamically, customized kernel templates with tunning knobs should be devised. This will help to maintain pipelining effectiveness across a diverse range of GNN inputs and hardware platform settings.</p><p>We crystallize the above principles into MGG 1 , a holistic system design and implementation for multi-GPU GNNs (Figure <ref type="figure" target="#fig_0">1</ref>). Given the GNN models and inputs, MGG will automat-1 https://github.com/YukeWang96/MGG-OSDI23-AE.git ically generate pipeline-centric GPU kernels for multi-GPU platforms and dynamically improve the kernel performance based on runtime feedback. The core of MGG is its Kernel &amp; Runtime Manager, which constructs GNN-tailored pipelines and maps such pipelines to proper communication primitives and GPU logical processing units. It can also dynamically orchestrate GPU kernels based on new configurations. MGG also incorporates a Runtime Parameter Optimizer, which will monitor the performance (e.g., latency) from the actual execution and generate new configurations for the next iteration based on the analytical performance model and optimization heuristics. To the best of our knowledge, we are the first to explore the potential of GPU kernel operation pipelining for accelerating irregular GNN workloads. Moreover, MGG can be generalized to other applications (e.g., deep-learning recommendation model (DLRM) <ref type="bibr" target="#b30">[31]</ref>) that are sharing similar irregular communication demands ( ?7.3).</p><p>Overall, we make the following contributions in this paper:</p><p>? We propose a GNN-tailored pipeline construction technique ( ?4) with pipeline-aware workload management and hybrid data placement, for efficient communicationcomputation pipelining in a GPU kernel.</p><p>? We introduce a GPU-aware pipeline mapping strategy ( ?5), encompassing warp-based mapping and pipelining, and specialized memory designs and optimizations to comprehensively promote kernel performance.</p><p>? We devise an intelligent runtime with lightweight analytical modeling and optimization heuristics to dynamically improve the performance of GNN training ( ?6).</p><p>? Comprehensive experiments demonstrate that MGG can outperform state-of-the-art multi-GPU GNN systems across various GNN benchmarks. Additionally, MGG can be generalized to other DL applications, like DLRM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recent deep-learning applications expand their scope from handling structured dense inputs (e.g., images) to unstructured sparse inputs (e.g., graphs). Along with such algorithmic/application expansion is the exploration of new system designs and optimizations for more efficient deep learning. One of the most important topics is the ability to handle large-scale inputs, which are usually out of the computation and memory capacity of one GPU. For scaling regular deep-learning applications, like dense DNNs, various abstractions (e.g., data and model parallel) and high-performance communication libraries (e.g., NCCL <ref type="bibr" target="#b33">[34]</ref>) have been developed. While the scaling approach for irregular GNN applications is still initial and suffers from unsatisfactory performance. Compared to scaling dense DNNs, scaling sparse GNNs is significantly more challenging. The irregular fine-grained sparse GNNs workload cannot fit the regular coarse-grained  workload abstraction for dense DNNs. The cost of irregular communication in GNNs cannot be easily amortized by simply batching more requests as dense DNNs due to their randomness and sparseness. Scaling strategies largely vary among different GNN inputs while tiling/schedule strategies would be reused across different inputs of dense DNNs. Therefore, an array of dedicated designs have been introduced to scale the sparse GNNs, focusing on three major directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D E C</head><formula xml:id="formula_1">G D A B D A C B C E F G A F D D E F C C A Overlapped Compute &amp; Comm SM M SM M SM M time D C A B C E F G D GPU-0 GPU-1 C E G F D B A Local Compute D E F G swap A B C D E F G A B C B C D A C C A B C A B E E F G F F A G D Compute D D C G E F D<label>(</label></formula><p>Operator Specialization for Sparse Communication: This is the mainstream solution that treats the communication as a standalone operator for irregularly sparse GNN communication (Figure <ref type="figure" target="#fig_2">2</ref>(a)). DGL <ref type="bibr" target="#b44">[45]</ref> is the state-of-the-art GNN framework and its most recent update incorporates PyTorch-Direct <ref type="bibr" target="#b27">[28]</ref> (a GNN-tailored communication design based on zero-copy memory <ref type="bibr" target="#b40">[41]</ref>) for large-scale GNN training across GPUs. Work from <ref type="bibr" target="#b5">[6]</ref> introduces a communication planning algorithm for distributed GNNs by considering links, communication, contention, and load balancing. However, these efforts optimize the communication standalone and thus miss the opportunities to jointly optimize computation and communication operations/schedules which can potentially reduce the overall latency and improve GPU utilization.</p><p>Algorithm Modification for no Communication: The second typical type is to eliminate irregular communication by altering algorithms <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b50">51]</ref>. They harness various algorithmic adaption solutions, such as neighbor sampling and mini-batch to prefetch the remote neighbors to local devices, and then train the GNN model in a data-parallel fashion as the traditional dense DNN. However, existing research <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref> shows that such an algorithmic modification would compromise the accuracy of GNN models compared to the original GNNs. It would also destabilize the algorithmic performance (e.g., the lower convergence speed and final accuracy) under different inputs and sampling configurations.</p><p>Schedule Transformation for Dense Communication: The third type is to transform irregular communication to regularized communication (e.g., AlltoAll, P2P), which has been optimized by existing communication kernels (Figure <ref type="figure" target="#fig_2">2</ref>(b)). ROC <ref type="bibr" target="#b17">[18]</ref> delegates communication to its underlying NVIDIA Legion runtime <ref type="bibr" target="#b4">[5]</ref>, which manages irregular remote neighbor access via a DMA engine. It batches fine-grained embeddings into large embedding tiles on CPUs to facilitate coarse-grained data movement between the host and GPUs. NeuGraph <ref type="bibr" target="#b25">[26]</ref> tiles the large node embedding matrices by rows (as embedding chunks) and then forwards each chunk to GPUs sequentially via coarse-grained P2P communication. P3 <ref type="bibr" target="#b11">[12]</ref> spots the potential of transforming irregular embedding communication to regular all-to-all communication for embedding column tiles. However, this type of effort would introduce many unnecessary data movements and non-trivial overhead to transform original algorithms and data inputs.</p><p>To sum up, existing designs explore solutions in a limited scope and have yet to extend their solution search to a broader context by exploring the synergy between the multi-GPU GNN workloads, GPU execution paradigms, and communication patterns. Therefore, these designs could hardly enjoy the full potential of multi-GPU platforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Motivation</head><p>Different from prior solutions, we propose a new view for multi-GPU GNN workload. We spot that by removing the explicit barrier between the computation and communication stage in multi-GPU GNNs, we can co-schedule the operations from both stages in a holistic way that can reduce the GPU resource idleness and promote performance (Figure <ref type="figure" target="#fig_2">2</ref>(c)). For example, when GPUs initiate remote access requests and are waiting for the arrival of remote data, the idle cycles of GPUs can be fulfilled by other local computing workloads. Such insight enables us to abstract the multi-GPU GNN workload as a fine-grained dynamic software pipeline for communication and communication overlapping. Specifically, "Fine-grained" means that the operations at each pipeline stage are tiny (e.g., the aggregation of one neighbor's embeddings) versus DNN layers."Dynamic" means that the division of computation into pipeline stages would vary among different inputs in contrast to DNNs with a relatively fixed pipeline. Such a new design is motivated by our three major observations.</p><p>GNN Workload Speciality: The first observation reveals the specialty of GNN workloads, which feature two major types of partial dependency that facilitate pipelining <ref type="bibr" target="#b0">[1]</ref>. The first type is the fine-grained neighbor aggregation dependency, where the neighbor embeddings of individual graph nodes are aggregated either sequentially or in parallel with proper synchronization. The second type is the dynamic execution </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPU Execution Characteristics:</head><p>The second observation highlights the characteristics of the GPU execution paradigm.</p><p>One key design principle of GPUs is their massive computation/communication parallelism to amortize the unit cost of individual computation/communication operations <ref type="bibr" target="#b39">[40]</ref>. The underlying mechanism of GPU hardware design to facilitate this is to simultaneously schedule multiple logical processing units (e.g., threads/warps/blocks) to share the hardware processing units (i.e., GPU SMs). Such a design provides the essential ingredient for pipelining, which is that computation and communication operations can co-run on the same units at the same time to fulfill the idle GPU cycles and maximize the utilization of the GPU hardware processing units. Moreover, with the precise control of GPU kernel launching parameters (e.g., the size of the block and shared memory), the effectiveness of co-running heterogeneous operations can be adjusted so that we can flexibly accommodate different inputs while maintaining high-performance delivery.</p><p>Multi-GPU Programming Support: The third observation features the recent advancement of the GPU communication technique and its programming support. The one highlighted most is the NVSHMEM <ref type="bibr" target="#b35">[36]</ref>, which provides GPU intra-kernel APIs for fine-grained (several to tens of bytes) inter-GPU communication (Listing 1). NVSHMEM is the main communication backend for MGG. Other existing techniques such as Zero-copy memory can also serve as an alternative to NVSHMEM for fine-grained communication. The performance will be similar while NVSHMEM offers better programmability. Some other traditional strategies for inter-GPU communication, would either offer too coarse-grained communication solutions (e.g., unified virtual memory <ref type="bibr" target="#b37">[38]</ref> uses KB-level communication granularity) or resort to the default communication strategies of existing multi-GPU-based runtime system (e.g., NVIDIA Legion <ref type="bibr" target="#b4">[5]</ref>) without GNN-tailored communication optimization.</p><p>These observations and insights motivate MGG, a holistic multi-GPU GNN system with a novel view of GNN workloads as an operation pipeline. MGG automates the pipeline construction, detailed pipeline mapping, and dynamic inputdriven pipeline adaption, to improve the GNN scaling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GNN-tailored Pipeline Construction</head><p>Constructing a GNN-tailored pipeline are facing two major challenges: 1) How to effectively partition and schedule multi-GPU GNN workloads so that pipeline efficiency can be maximized; 2) How to properly layout input so that the hierarchy of GNN inputs and the memory/storage of multi-GPU systems can be carefully matched to facilitate pipeline execution. MGG addresses these challenges with Pipeline-aware Workload Management and Hybrid GNN Data Placement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pipeline-aware Workload Management</head><p>Managing irregularly sparse GNN workloads for pipelining is challenging and could hardly benefit from the prior practice and exploration of the DNN pipeline <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>Difference from DNN pipeline First, balancing the GNN workloads among GPUs has to jointly optimize the computation capacity and the computation/communication irregularity. While the DNN pipeline only needs to balance the computation/memory capacity, since its pipeline stages are wellstructured and their inputs are regularly dense. Distributed DNNs require dense regular communication (e.g., Allreduce) that is naturally fit for existing GPU interconnects optimized for throughput and has been optimized by many libraries (e.g., NCCL). In contrast, distributed full-graph GNN (with the entire graph cached on GPUs) is much more challenging since it requires sparse irregular communication that is naturally at odds with the existing hardware interconnects, and fewer efforts have optimized its performance. Second, the GNN pipeline workload is more irregular and non-structural and can easily cause pipeline stalls/bubbles. For example, remote neighbor aggregation would have different stages (remote access + aggregation) compared with local neighbor aggregation (local access + aggregation), making it challenging to mix those two heterogeneous workloads. While in the DNN pipeline, all inputs should consistently pass through the same pipeline stages. Third, GNN pipeline stages are more finegrained (e.g., fetching individual embeddings) compared with coarse-grained layers (e.g., GEMMs and Convolutions) in the DNN pipeline. Such small workload granularity enables different pipeline stages to overlap with each other on GPU processing units, like Streaming Multiprocessors (SMs). In </p><p>(2) With the above insights, we propose a three-stage dynamic software pipeline design. The three stages include loading remote neighbors (LR), loading local neighbors (LL), and aggregation computation (AC). Aggregation of a certain neighbor will only take two stages. The remote neighbor aggregation will take the stage LR and AC while local neighbor aggregation will take the stage LL and AC. The stage-wise pipelining is achieved with two steps: 1) assigning aggregation workload to different GPU logical processing units (LPUs), like warps and blocks, and 2) scheduling different LPUs on the same GPU SM to overlap their execution. Threephase pipeline can generalize to different GNN models, which essentially consist of the different numbers of basic remote and local operations. For example, GCN has a lower localvs-remote operation ratio while GAT features a higher localversus-remote operation ratio. Three-phase pipeline can also capture differences among inputs. For instance, a more sparse graph will have a higher remote-to-local operation ratio.</p><formula xml:id="formula_3">1 2 1 2 LNP-0 LNP-2 LNP-3</formula><p>However, the direct construction and execution of such three-stage pipelines would be inefficient, because of its ignorance of GNN workload heterogeneity and irregularity on multi-GPU platforms. To address these challenges, MGG highlights a GNN-tailored pipeline construction strategy to build and optimize the software pipeline in three steps.</p><p>Step-1: Workload-aware inter-GPU pipeline workload balancing. This step aims to construct the "raw" pipeline and balance workloads among pipelines on different GPUs. Our insight is that GPUs with massive processing units (e.g., SMs) will serve many pipelines concurrently, and the key to maximizing GPU performance and utilization is to en-sure that each pipeline will get a similar amount of workload, thereby avoiding execution critical path on certain "long" pipelines. We, therefore, develop a range-constrained binary search algorithm (Algorithm 1) based on prior graph partitioning exploration <ref type="bibr" target="#b2">[3]</ref>. Our solution features a lower runtime cost to split the GNN input graph into chunks (one chunk per GPU) while balancing the number of edges within each chunk. Then the workload from the same chunk is grouped by nodes as workload partitions mixed local and remote neighbors (Figure <ref type="figure" target="#fig_17">3(b)</ref>). From its potential execution pipeline, we can see many idle cycles (indicated by blank spaces in different pipeline stages) which would result in low pipeline efficiency and GPU resource occupancy. Note that in the software pipeline, workloads from different partitions can be overlapped as they will be processed by different LPUs. While the workloads from the same partition are sequentially processed by one LPU and their relative order should be maintained even after being mixed with other partitions.</p><p>Step-2: Heterogeneity-aware pipeline bubble reduction. The pipeline constructed from the previous step is still inefficient due to its scattered workloads among stages, namely pipeline bubbles. The optimization in this step is to minimize such pipeline bubbles for better pipeline efficiency. The key is to reduce the heterogeneity of workload partitions that hinders effective overlapping. To achieve this, we categorize the sparse multi-GPU GNN computation into two types. The first type has local neighbor access only, which has shorter execution latency. The second type has remote neighbor access, which features high latency overhead. We delicately handle different types of workloads via grouping (Figure <ref type="figure" target="#fig_4">3</ref>(a)-1 ), where two separate CSRs for local and remote subgraphs will be built. The aggregation will be conducted on local and remote subgraphs separately and followed by a result synchronization at the end. Such a remote-local split is also backed by the fact that on platforms with all-to-all GPU interconnections (e.g, DGX-A100/H100), accessing different GPUs under the same data granularity has approximately equal communication cost <ref type="bibr" target="#b23">[24]</ref>. Such heterogeneity awareness in workload partitioning (Figure <ref type="figure" target="#fig_4">3</ref>(c)) enables a more densely overlapped workload between the stage LR and LL/AC.</p><p>Step-3: Granularity-aware intra-GPU pipeline enhancement. While the second optimization improves pipeline efficiency by reducing the workload heterogeneity, there is still plenty of room for further enhancement. The optimization in this step is to facilitate a more balanced workload distribution among pipeline stages. This key is to find the proper workload granularity for local and remote subgraphs so that those originally sequentially processed workload partitions can be overlapped. Our key observation is that nodes in the local/remote subgraphs would have a diverse number of neighbors. Such a specialty makes it challenging for massively parallel GPUs to harvest the real performance gains due to the imbalance workload and diverged execution flow. Therefore, we approximate such coarse-grained irregular workloads with fine-grained fixed-sized partitions so that the workload imbalance across nodes can be amortized. For example, with 2 neighbors per partition (Figure <ref type="figure" target="#fig_4">3</ref>(a)-2 ), we can get a more balanced workload among nodes in their local and remote neighbor aggregation. With such granularity awareness, the individual pipeline can be further condensed along its time axis with more overlapping of the LL and AC stage. (Figure <ref type="figure" target="#fig_17">3(d)</ref>). Meanwhile, the irregular workload can be more evenly distributed to GPU SMs for higher GPU utilization. On the other side, partition granularity should also be balanced with synchronization overhead, since more fine-grained partitioning can bring more parallelism at the cost of more synchronization overhead. This is because workloads from different partitions for the same target node need to be reduced via synchronization, like inter-thread shuffling and atomics.</p><p>MGG design can also be generalized to multiple machines with a minor adaptation. For example, in Figure <ref type="figure" target="#fig_17">3(d)</ref>, when there are inter-node (over Inifite-Band) remote neighbors (longer latency due to lower inter-node communication speed), the size of remote neighbor partitioning (RNP) should be adjusted to a smaller size (e.g., from 2 to 1 remote neighbor) to facilitate better overlapping with local computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hybrid GNN Data Placement</head><p>In collaboration with our multi-step pipeline construction, we introduce a hybrid GNN data placement strategy to exploit the benefits of different types of memory in SHMEM-enabled multi-GPU systems. The major impact of such hybrid placement on pipelining is two-fold. First, placing GNN data in different memory spaces will lead to different ratios of local and remote workloads, thus, affecting workload balance among pipelines. Second, different memory spaces will offer different access performances (e.g., latency), thereby, affecting the execution efficiency of the individual pipelines, such as the number of pipeline bubbles.</p><p>Our strategy focuses on two major aspects. Firstly, for workload balance among pipelines, we leverage NVSHMEM "shared" global memory to store the node embeddings (NEs) of the whole graph (Figure <ref type="figure" target="#fig_6">4</ref> left). Our major consideration here is that such shared global memory space can be accessed by all GPUs with the approximated equal access speed, which is vital to facilitate a more even distribution of remote workloads to GPUs in terms of their size and unit access costs. In addition, NEs are generally large in terms of size (due to high dimensionality), which are beyond the device memory limit of a single GPU. Therefore, NEs are ideal to be placed in shared global memory space with sufficient space (with aggregated memory of different GPUs), which also provides direct remote access support across GPUs. Specifically, we will partition the NEs of input graphs into n equal-sized partitions (where n is the number of GPUs) and place each of them in one GPU's shared global memory space.</p><p>Secondly, for the efficiency of individual pipelines, we allocate the "private" global memory space for storing partitioned graph structure (GP) data, which is only visible to kernels on the current GPU. Our key insight is that GP (e.g., edge lists), is all scalar values and usually small in size, and will only  Note that "NE-i" is the node embedding partition stored on the i-th GPU. "GP-i" is the neighbor partition processed by the i-th GPU. "GPU-i [lb, ub]" is the node-id range [lowerbound, upperbound] of the node embeddings on the i-th GPU.</p><p>be accessed by the local GPU. Therefore, GP is ideal to be placed in individual GPUs' DRAM. Such a placement is also important to reduce unnecessary and inefficient remote access on those tiny scalars for fewer pipeline bubbles. In our design, GP data (e.g., edges) from private GPU global memory will be processed by a address translation unit for fetching correct NEs on local/remote GPU since the NE indices are rebased to zero on each GPU (Figure <ref type="figure" target="#fig_6">4</ref> right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">GPU-aware Pipeline Mapping</head><p>Efficient pipelining also demands effective mapping of wellconstructed pipeline workload and their schedules to the lowlevel GPU logical processing units (e.g., GPU threads/warps/blocks) to overlap computation and communication. To achieve this, we propose Warp-based Mapping &amp; Pipelining and Specialized Memory Design &amp; Optimization to jointly optimize the pipeline execution efficiency, GPU utilization, and end-to-end design flexibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Warp-based Mapping &amp; Pipelining</head><p>An effective pipeline mapping demands comprehensive consideration of two major aspects. 1) Which type of GPU logical processing units (e.g., warps, blocks) should be used for pipeline workload partitions? We choose GPU warp as the basic working unit to handle the workload of each partition. This is because threads in a warp can collaboratively work on different dimensions of a node embedding simultaneously.</p><p>Whereas using a single or several threads (less than the size of a warp, 32 threads) would hardly explore the computation parallelism and would cause warp-level divergence. Besides, NVSHMEM remote access initiated by a warp of threads would merge the requests into one remote memory transaction to amortize the overhead. 2) Which pattern of mapping should be used for benefiting pipeline execution efficiency?</p><p>The most straightforward way is to continuously map the neighbor partitions from the local and remote workload list to GPU warps with continuous IDs (Figure <ref type="figure" target="#fig_8">5</ref>). However, this strategy would easily suffer from workload imbalance among GPU SMs. This is because warps with continuous IDs are  more likely to be placed into the same thread block, which is assigned to one SM for processing. Therefore, SMs assigned with warps for handling remote neighbor partitions would lead to much longer latency than SMs assigned with warps for processing local neighbor partitions. Such a workload imbalance would lead to poor GPU utilization and runtime execution performance.</p><p>To this end, we introduce our novel workload interleaving strategy to balance the workload among SMs on GPUs. Each warp of threads running on GPU would handle one or more pairs of local/remote workload partitions. To more precisely calibrate the warp-to-SM mapping for different pipeline stages to achieve efficient pipelining, we introduce a new metric -interleaving distance. We give examples with the interleaving distance equals 1 and 2 for illustration (Figure <ref type="figure" target="#fig_8">5</ref>). By mixing different types (both local and remote) of workload together, better GPU utilization can be achieved since when one warp is blocked for high-cost remote access, other warps that are working on local computation can still be served by the SMs warp scheduler for filling up these idle GPU cycles. Moreover, such a design would improve design flexibility. For instance, given an input graph with a selected neighbor partition size, we can adjust the size of interleaving distance and the workload per warp so that waiting cycles of the remote access can be hidden by the computation cycles of the neighbor aggregation. Thus, each warp can be fully utilized while the design can achieve sufficient parallelism.</p><p>MGG currently processes the neighbors of adjacent nodes (based on node-ids) to the same thread block where the same block will be scheduled on the same SM. If there are common remote neighbors for those adjacent nodes, their remote requests will be merged. Improving such locality requires reordering the graph nodes to maximize their common neighbors. Such an exploration is orthogonal to our current contribution. In future GPUs, there is a trend to explore the locality among independent processing units. For instance, in Hopper, several thread blocks can be grouped together as thread-block groups. We can explore the tradeoff between the locality benefits and group synchronization overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Specialized Memory Design &amp; Optim.</head><p>Efficient software pipelining also demands careful management of high-bandwidth shared memory for promoting data access efficiency and asynchronized primitives for exploiting intra-warp operation pipelining.</p><p>GPU SM Shared Memory Layout: Based on our MGG's warp-based workload design, we propose a block-level shared memory orchestration to maximize the performance gains. We have several key insights for such a dedicated memory layout design within each thread block. First, our neighborpartition-based workload will generate the intermediate results that can be cached at the high-speed shared memory for reducing the frequent low-speed global memory access. Second, NVSHMEM-based remote data access demands a local scratch-pad memory (e.g., registers, shared and global memory) to hold the remote data for local operations.</p><p>For the local neighbor aggregation, we reserve a shared memory space with D (D is the embedding dimension) floating-point numbers for embeddings of the target node in each neighbor partition so that threads from a warp can cache the intermediate results of partial reduction in shared memory. For the remote neighbor aggregation, the shared memory space is doubled 2 ? wpb ? D (wpb is the warps per block). The reason is that we need the first half wpb ? D for caching the partial aggregation results of each warp and the remaining for the remotely accessed neighbor embeddings. For each MGG kernel design, we will first identify the warplevel information, like warp IDs. Then within each thread block, we define the customized shared memory layout by splitting the contiguous shared memory address into three different parts for neighbor ids, partial aggregation results, and the remotely-fetched node embeddings. We use the dynamic shared memory for design flexibility since those parameters (e.g., wpb and D) can only be determined at runtime. During execution, we will first calculate the total shared memory size per block and then pass it as a kernel launching parameter.</p><p>Pipelined Memory Operation: ?5.1 have discussed assigning local (LNP) and remote (RNP) neighbor aggregation workloads to warps so that different warps can overlap their computation and communication to fully saturate the active cycles of the GPU SM scheduler. However, only exploiting the inter-warp communication-computation overlap is not enough to maximize the utilization of GPU resources. We further explore the overlapping of the computation and communication at the intra-warp level by carefully scheduling the memory operations. Figure <ref type="figure" target="#fig_9">6</ref>(a) shows the case with two LNPs and two RNPs by using the synchronized remote access, we can just sequentially process the two LNPs and the two RNPs. The long-latency remote access can happen only after the completion of its preceding LNP. This could lead to a longer GPU stall for memory operations and low GPU SM utilization. Our profiling also shows that without overlapping, the remote access usually dominates the overall execution (around 60% of overall latency) compared to the time for local data access plus the time for aggregation computation (around 40% of overall latency). Such observation justifies our design to mainly hide the latency from remote access.</p><p>To amortize the cost of remote access for each warp, we introduce asynchronized remote memory operations (Figure <ref type="figure" target="#fig_9">6(b)</ref>). This improved design consists of two major steps. First, we can simultaneously launch the local memory access while initializing the remote memory access for fetching the node embedding ( 1 ), therefore, the time for remote access can be amortized by the processing of LNP. Second, once the remote access is completed, the current warp will start aggregation on the remotely-fetched node embedding data <ref type="bibr" target="#b1">( 2 )</ref>. The next step will start the new iteration of the previous two steps, which will process a new pair of LNP and RNP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Intelligent Runtime Design</head><p>In this section, we will discuss our intelligent runtime design with performance/resource analytical modeling and heuristicbased cross-iteration optimization strategy.</p><p>Performance-Resource Analytical Modeling: The performance/resource model of MGG has two variables: workload per warp (WPW) and shared memory usage per block (SMEM), which can be measured by</p><formula xml:id="formula_4">WPW = 2 ? ps ? D ? dist, SMEM = ps ? wpb ? IntS + 2 ? wpb ? D ? FloatS (1)</formula><p>where ps, wpb, and D are the sizes of neighbor partition, warp per block, and node embedding dimension, respectively; dist is the interleaved distance of local/remote workloads ( ?5.1); IntS and FloatS are both 4 bytes on GPUs. To determine the value of the ps, wpb, and dist of a given input graph, we will first compute the total number of warps by using</p><formula xml:id="formula_5">numWarps = max{local, remote} dist<label>(2)</label></formula><p>where local and remote are the number of local and remote partitions, respectively. Then we compute the total number of blocks and the estimated block per SMs by using</p><formula xml:id="formula_6">numBlocks = numWarps wpb , blocksPerSM = numBlocks numSMs<label>(3)</label></formula><p>Later, based on our micro-benchmarking results on diverse datasets, we define our parameter search space and constraints: 1) ps ? [1 . . . 32] to balance the computation parallelism and synchronization overhead; 2) dist ? [1 . . . 16] to effectively overlap the computation and remote memory access; 3) wpb ? [1 . . . 16] to maintain SM warp scheduling flexibility for better occupancy and throughput; 4) numSMs ? c 1 , SMEM ? c 2 , where c 1 and c 2 are hardware constraints <ref type="bibr" target="#b47">[48]</ref>, e.g., NVIDIA A100 has 108 SMs and 164KB shared memory per SM.</p><p>Heuristic-based Cross Iteration Optimization To optimize the design of MGG, the parameter ps, dist, and wpb are initialized as the value 1 at the beginning. Then we optimize one parameter in each of the following iterations. First, we increase the ps to maximize the warp utilization. When further increasing the ps would also increase the latency, we would stop the search on ps and switch to dist. Second, we apply a similar strategy to locate the value of dist that can maximize the overlap of local computation and remote access. Third, we increase wbp to maximize the utilization of the entire SM. If any increase of wpb would increase the latency, we know that there may be too large thread blocks or too heavy workloads on individual warps that lower SM warp scheduling efficiency or computation parallelism. We would "retreat" (i.e., decrease) ps to its second-highest value if necessary and restart the increase of wpb. This optimization algorithm will stop when any decrease of ps and increase of wpb would lead to higher latency than the top-3 lowest latency. The latency of each iteration during the optimization will be recorded by a configuration lookup table. Finally, the configuration with the lowest latency will be applied.</p><p>This particular optimization order of parameters (ps, dist, and wpb) is based on two major aspects: (i) Spatially speaking, the granularity is from coarse-grained algorithm-level partitioning through ps, to medium-grained pipeline construction through dist (according to the partition plan), to fine-grained pipeline-to-warp fine-tuning through wpb (according to the pipeline design). (ii) Temporally speaking, the three optimizations are applied at loading-time (ps to decide layout), kernel initialization (dist to decide pipeline), and runtime (wpb to decide pipeline mapping), respectively.</p><p>The above parameter adaption for dynamic pipelining is vital for design/optimization generality. This is because the characteristics of graphs (#nodes/edges and embedding sizes) would lead to different efficiency of kernel pipelines. Our later experimental studies (as shown in Figure <ref type="figure" target="#fig_0">11</ref>) demonstrate its benefits with up to 70% of performance improvements.  <ref type="bibr" target="#b16">[17]</ref> 2,449,029 61,859,140 100 47 ogbn-proteins(PROT) <ref type="bibr" target="#b16">[17]</ref> 132,534 39,561,252 8 112 com-orkut(ORKT) <ref type="bibr" target="#b22">[23]</ref> 3,072,441 117,185,083 128 32</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Evaluation</head><p>Benchmarks &amp; Datasets Despite the diversity of GNN models, the fundamental computation and communication paradigm (vector-based scatter-gather operation) in multi-GPU GNNs remains the same. We evaluate two distinctive and representative GNN models on node classification tasks:</p><p>The first type of GNN model uses a non-discriminated neighbor aggregation strategy, where all neighbors contribute equally when doing the aggregation. We choose Graph Convolutional Network (GCN) <ref type="bibr" target="#b20">[21]</ref>, which is the most popular GNN model and is also the key backbone network for many other GNNs, such as GraphSAGE <ref type="bibr" target="#b15">[16]</ref> and Differentiable Pooling <ref type="bibr" target="#b51">[52]</ref>. We use 2 layers with 16 hidden dimensions for GCN, which is also the setting from the original paper <ref type="bibr" target="#b20">[21]</ref>. The computation of a 2-layer GCN can be expressed as</p><formula xml:id="formula_7">Z = Softmax( ? ReLU( ?XW 1 )W 2 ). (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>where ? is the adjacent matrix of the input graph with selfloop edges, and X is the input node embedding matrix, where X ? R N?D ; N is the number of nodes in a graph; D is the size of node embedding dimensions. W 1 and W 2 are trainable weight matrices in layer-1 and layer-2, respectively.</p><p>The second type uses a discriminated neighbor aggregation strategy, where neighbors would contribute differently depending on their calculated edge-specific features. We choose Graph Isomorphism Network (GIN) <ref type="bibr" target="#b48">[49]</ref>, which aims to distinguish the graph structure that cannot be identified by GCN. Each layer of GIN can be expressed as</p><formula xml:id="formula_9">h l+1 v = MLP l ((1 + ? l ) ?l + ? u?N (v) h l u ). (<label>5</label></formula><formula xml:id="formula_10">)</formula><p>where l is the layer ID and l ? {0, 1}, MLP is a fullyconnected neural network, h v is the node embedding for node v, and N (v) stands for the neighbors of node v. GIN mainly differs from GCN in its aggregation function, which introduces a weight parameter as the ratio of contribution from its neighbors and the node itself. In addition, GIN is the reference architecture for many other advanced GNNs with more edge properties, such as Graph Attention Network <ref type="bibr" target="#b42">[43]</ref>. For GIN evaluation, we use 5 layers with 64 hidden dimensions, which is also the setting used in the original paper <ref type="bibr" target="#b48">[49]</ref>. Graphs (Table <ref type="table" target="#tab_1">1</ref>) used in our evaluation are large in their number of nodes and edges that demand multi-GPU capability for effective GNN computation. #Class is the output dimension  (#labels) for the node classification task. #Dim is the embedding dimension of the input graph.</p><p>Baselines In this evaluation, we compared MGG with several existing systems that support large full-graph GNN (i.e., caching the entire graph on GPUs) on multi-GPU platforms. 1) Deep Graph Library (DGL) <ref type="bibr" target="#b44">[45]</ref> is the state-of-the-art framework for large-scale GNNs across GPUs. It leverages PyTorch-Direct <ref type="bibr" target="#b26">[27]</ref> as the communication backend for GPUinitiated zero-copy memory access <ref type="bibr" target="#b40">[41]</ref> to fetch neighbors embedding from the CPU host. 2) MGG-UVM <ref type="bibr" target="#b19">[20]</ref> is a GNN design by adapting MGG to leverage unified virtual memory (UVM). UVM has been highlighted in handling irregular graph computations (such as PageRank) on large graphs <ref type="bibr" target="#b19">[20]</ref>. However, <ref type="bibr" target="#b19">[20]</ref> is not open-sourced, we thus generalize the pipeline kernel designs and optimizations ( ?4 and ?5) of MGG to build such a UVM baseline and incorporate optimizations from <ref type="bibr" target="#b19">[20]</ref>. Note that UVM and zero-copy memory are different communication backends <ref type="bibr" target="#b0">[1]</ref>. Thus, MGG-UVM does not implement zero-copy data transfer. We remark UVM is the key communication protocol before the new hardware support for fine-grained direct GPU-GPU communication (e.g., NVSHMEM). UVM is more coarse-grained and will require the engagement of CPUs (e.g., host memory management) for communication. The reason to use MGG-UVM is to show that if there is no advanced hardware support (e.g., NVSHMEM) for fine-grained direct GPU-GPU communication, the benefits of our elaborated pipeline can be offset by UVM communication overhead. 3) ROC <ref type="bibr" target="#b17">[18]</ref> is a popular distributed multi-GPU system for full-graph computation. ROC highlights its learning-based partitioning and leverages NVIDIA Legion <ref type="bibr" target="#b4">[5]</ref> runtime for communication and task scheduling.</p><p>Other multi-GPU GNN designs, like NeuGraph <ref type="bibr" target="#b25">[26]</ref> and P3 <ref type="bibr" target="#b11">[12]</ref>, are not publicly available. Initially, we plan to evaluate MGG on AMD ROC_SHMEM <ref type="bibr" target="#b1">[2]</ref>. However, as indicated in its document, the existing ROC_SHMEM is an experimental prototype and is not officially ready to be applied in prac-tice due to very strict software limitations (e.g., only supports ROCm v4.3) and hardware (e.g., only supports AMD GFX9 GPUs), which are quite challenging to find and deploy and not supported by any existing GNNs frameworks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28]</ref> for comparison. We believe that once ROC_SHMEM becomes ready and generally applicable, MGG can be easily migrated to AMD multi-GPU platforms.</p><p>There is no existing design that can leverage GPU-to-GPU communication only for distributed full-graph GNN computation. We try our best to measure the best-possible baseline performance. DGL and ROC have longer latency in the earlier iteration due to cache warmup for node embedding on GPU memory. We thus perform warm up iterations until their per-iteration latency becomes stable, and then measure their performance with minimized CPU-GPU data movements.</p><p>Platforms &amp; Tools The implementation of MGG consists of ?9K LoC. We compile and link MGG with CUDA (v11.2), OpenMPI (v4.1.1), NVSHMEM (v2.0.3), and cuDNN (v8.2) library. Our major platform is an NVIDIA DGX-A100 with dual AMD Rome 7742 processors (each with 64 cores, 2.25 GHz), 1TB host memory, and 8?A100 GPUs (40 GB) connected via NVSwitch, which offers 600 GB/s GPU-to-GPU bi-directional bandwidth. For the modeling study, we also leverage DGX-1 with 4?V100 GPUs connected via NVLinks. We use NVIDIA NSight Compute to get the kernel-level profiling metrics. Speedup is averaged over 100 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">End-to-End Performance</head><p>Compared with DGL In this section, we will compare with the state-of-the-art DGL framework, which leverages PyTorch-Direct for cross-GPU communication. We evaluate different datasets and platform settings (with 4 and 8 A100 GPUs). As shown in Figure <ref type="figure" target="#fig_10">7</ref>, MGG outperforms DGL with averaged 4.25? and 4.57? speedups on GCN and GIN models, respectively. We also notice a trend that MGG demonstrates a more pronounced speedup with more GPUs. With the increasing number of GPUs, DGL suffers from heavy memory access contention, since multiple GPUs are initiating massive requests to access the neighbor embeddings on the CPU host memory. Another observation is that on GIN (D = 64) with higher hidden dimensionality for smaller datasets (e.g., PROD and PROT), the performance gap between DGL and MGG is smaller compared to GCN (D = 16) since as indicated in <ref type="bibr" target="#b27">[28]</ref>, zero-copy memory would be beneficial from more coarsegrained data movement (with larger embedding vector) that can saturate the PCIe cache line (128 Bytes). While such an advantage of DGL diminishes for those larger datasets (e.g., IT04 and PAPER) on GIN due to significantly increased sparsity and irregularity. In addition, compared with MGG, DGL assumes the one-size-fits-all communication strategy would work well for all input datasets. Therefore, it ignores the importance of the inputs and hardware properties, which would bring non-trivial (more than 30%) benefits ( ?7.2).   MGG can also be extended to cover other GNN models. The following results show the speedups of MGG over DGL on GraphSAGE with layerwise node neighbor sampling and GAT with dot-product edge attention. Table <ref type="table" target="#tab_5">2</ref> shows that the performance results of GAT and SAGE also agree with our prior observations on the GCN and GIN, demonstrating the generality and effectiveness of our proposed design and optimizations to handle more complex dataflow (e.g., edge attention and softmax) in multi-GPU GNN computation.</p><p>Despite that MGG (NVSHMEM) and DGL (with CPU-GPU zero-copy memory <ref type="bibr" target="#b40">[41]</ref>) both rely on GPU-initiated communication and overlap communication with computation, their underlying mechanism is different, and MGG shows more performance advantages. MGG can leverage inter-GPU communication while DGL can only rely on CPU-GPU communication with limited bandwidth. This makes the communication costs pronounced in DGL and offsets the performance gains from massive thread-level parallelism. This experiment also shows that MGG can serve as a drop-in replacement for the existing communication backend of DGL to improve large-scale full-graph GNN computation.</p><p>Compared with MGG-UVM In this experiment, we compare MGG with its UVM-based counterpart, MGG-UVM, which uses UVM in place of NVSHMEM for remote communication. Figure <ref type="figure" target="#fig_11">8</ref> shows that MGG achieves 4.58? speedup and 5.04? speedup on average compared to MGG-UVM on GCN and GIN, respectively. The MGG-UVM leverages the page-faulting-based remote data access that is more coarsegrained (around 4 KB) in comparison with a single node embedding size (less than 0.4KB), which leads to higher overhead and lower effective bandwidth usage per embedding </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GCN GIN</head><p>Figure <ref type="figure" target="#fig_15">9</ref>: Performance comparison with ROC with 8?A100.</p><p>transfer. Such an overhead would exacerbate with more GPUs and also make MGG-UVM challenging for GPU SM schedulers to effectively dispatch instructions for the next available warps. This is mainly because most of the warps wait for the long-cycle page-faulting and migration. We notice that with the increase of the dimension size (i.e., data movement granularity), the speedup over MGG-UVM becomes higher. We later found out that the increase of data-movement granularity actually increases the overall pagefault counts. This is because embedding vectors are generally stored continuously for memory efficiency instead of aligning with the size of memory pages. Therefore, increasing the size of individual embedding also increases the likelihood of triggering multiple pagefaults per embedding transfer.</p><p>Comparing among datasets, for graphs (e.g., PAPER) with more nodes/edges and lower average node degree, MGG would demonstrate more speedups since these graphs exhibit more irregular and sparse access that can not well fit into regular fix-sized pages. This also indicates the importance of amortizing communication overhead. Thanks to pipeline-centric workload management, we can effectively amortize such costs with careful operation scheduling.</p><p>We further measure two performance-critical GPU kernel metrics that are the key indicators of our pipeline efficiency ( ?4.1): Achieved Occupancy (the ratio of the average active warps per active cycle to the maximum number of warps supported in an SM) and SM utilization (the utilization of all available SMs on a single GPU). MGG improves SM utilization (by 21.15% on average) and occupancy (by 39.20% on average) compared to MGG-UVM. This indicates that MGG can effectively 1) distribute irregular workloads to SMs to balance workloads among pipelines and improve the overall GPU utilization, and 2) overlap the remote access and local aggregation computation from different warps to reduce pipeline bubbles and maximize SM occupancy.</p><p>Compared with ROC In this experiment, we compare MGG with ROC <ref type="bibr" target="#b17">[18]</ref> on their officially released GCN model implementation. We originally plan to evaluate both 4 and 8 GPU settings. However, ROC reports many out-of-memory (OOM) errors for those large graphs on GCN/GIN model and medium graphs on the GIN model due to its aggressive caching of those intermediate tensors on GPUs. Therefore, we keep our comparison to 8 GPUs. Performance-critical ROC runtime configurations (e.g., #CPU cores, GPU/host memory size) are optimized to fully utilize the DGX-A100. Figure <ref type="figure" target="#fig_15">9</ref> shows that MGG achieves averaged 12.30? and 9.35? speedups over ROC on GCN and GIN, respectively. MGG demonstrates a more pronounced speedup over ROC on the larger graph (e.g., IT04 and PAPER), which has more irregular neighbor embedding access. The Legion runtime of ROC relies on the DMA engine for bulky data (batched embeddings) transfer between host and GPU memory, leading to higher throughput but inferior latency performance. Besides, ROC relies on a separate communication-computation design, where computation happens after the full completion of communication. Such a design eliminates the opportunity to fill idle GPU cycles with computation during communication. In addition, the learning-based partitioning (to reduce communication) of ROC shows benefits on relatively smaller datasets (e.g., RDD and PROT) but hard to find optimal partition plans for large graphs due to the input structure complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Optimization Analysis</head><p>Neighbor Partitioning (NP) We compare MGG with a baseline design without applying the neighbor partitioning technique (i.e., each aggregation workload consists of all local/remote neighbors) on 4?A100. We apply the workload interleaving for both implementations and fix the warp-per-block size to 2 to eliminate the impact from other performancerelated factors. Figure <ref type="figure" target="#fig_12">10</ref>(a) shows higher latency (averaged 2.26?) for designs without applying neighbor partitioning, since the workload imbalance becomes more severe across different warps without neighbor partitioning, especially for those graphs with many remote access demands, leading to limited computing parallelism and GPU underutilization.</p><p>Workload Interleaving (WL) We compare MGG with a baseline design without workload interleaving (i.e., remote neighbor aggregation and local neighbor aggregation are mapped separately to the GPU warps. We fix the neighbor partition size to 16 and the warp-per-block size to 2. Figure <ref type="figure" target="#fig_12">10(b)</ref> shows that MGG consistently outperforms the non-interleaved baseline with an average of 1.89? speedup. Without interleaving the local/remote workload, the workload distribution would be highly skewed, where the heavy and intensive remote aggregation would be gathered on certain warps close to each other while the lightweight local aggregation would be gathered on some other warps close to each other. This leads to inefficient warp scheduling and higher latency. The solid black triangle with "E" is the searched "optimal" combination for ps and dist, while the black solid star with "E" is the searched "optimal" wpb given dist and ps.</p><p>Communication Primitives We adopt MGG with different NVSHMEM primitives at the thread, warp, and block levels. We fix the number of GPUs to 2, the hidden dimension to 16, the neighbor partition size to 2, and the distance of workload interleaving to 2. Figure <ref type="figure" target="#fig_12">10(c)</ref> shows that warp-level NVSH-MEM primitives (e.g., nvshmemx _ float _ warp _ get) for remote accessing can bring the lowest latency. For thread-level NVSHMEM primitives (e.g., nvshmem _ float _ get), it would not coalesce the remote memory access to reduce unnecessary transactions. For the block-level NVSHMEM primitives (e.g., nvshmemx _ float _ block _ get), the higher overhead comes from collaborating a block of threads for remote access, since thread blocks (usually consisting of multiple warps) is larger than a single warp, thus, leading to higher synchronization and scheduling cost. This study also shows that our choice of warp-level primitives strikes a good balance between memory access efficiency and scheduling flexibility.</p><p>Modeling and Optimization We further analyze the effectiveness of our lightweight analytical model for design space search. Specifically, three key parameters are studied, the size of neighbor partitioning (ps), the interleaving distance (dist), and the warps per block (wpb). We consider three different settings on a 2-layer GCN model: I: RDD on 4?A100 as the basic setting. II: RDD on 8?A100 to demonstrate the adaptability toward the different numbers of GPUs. III: RDD on 4?V100 <ref type="bibr" target="#b36">[37]</ref> to demonstrate the adaptability toward the different types of GPUs. We decompose searching results into two parts corresponding to the output of the second and third steps of the optimization discussed in ?6.</p><p>Figure <ref type="figure" target="#fig_0">11</ref> shows that our performance modeling and parameter selection strategy can pinpoint the low-latency design for the above three settings. The overall searching process only requires about 10 iterations to reach the final "optimal" settings. Note that here we show latency results for all possible settings for comparison. While in practice, we only need to traverse a small part of the whole design space (as indicated by the boxes touched by the dot lines). By comparing the final optimal runtime configuration setting and the initial configuration, we can see that modeling and cross-iteration optimization can decrease the execution time by up to 68%. In the end-to-end GNN training (usually more than 100 iterations), such a latency saving would also be significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Additional Study</head><p>Accuracy-latency Tradeoff This study will analyze the accuracy-latency tradeoff between GNNs with sampling and full-graph (w/o sampling) on 8?A100. Table <ref type="table" target="#tab_7">3</ref> shows an evident node classification accuracy increase (2% to 5%) of GNN w/o sampling over GNN w/ sampling. The accuracy of sampling-based GNN would be affected by many factors (e.g., sampling rate at each GNN layer and graph structure). It is thus highly tricky to choose the "optimal" value for those factors. Here we follow the conventional way for GNN sampling <ref type="bibr" target="#b44">[45]</ref>. The accuracy difference agrees with previous GNN algorithmic work <ref type="bibr" target="#b15">[16]</ref>. In many real-world applications (e.g, e-commerce), such an accuracy advantage of full-graph GNNs are be more preferred by users. Because even 1% accuracy would make significant profit gains when deploying services at scale while the latency penalty is relatively minor.</p><p>Generality to other applications The design of MGG can be generalized to other similar applications. We demonstrate the typical and popular deep-learning recommendation model (DLRM) <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b53">54]</ref> that has been widely used in the industry. In multi-GPU DLRM, the large embedding tables are partitioned by rows and stored in different GPUs. The DLRM inputs (embedding access queries) will request embeddings from tables on different GPUs and then apply operations (e.g., elementwise addition or dot product) on those fetched embeddings. Such embedding lookup is highly sparse and irregular and dominates (&gt; 80% latency <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b53">54]</ref>) the overall DLRM computation. We improve the mainstream DLRM system <ref type="bibr" target="#b30">[31]</ref> with the design and optimizations of MGG to accelerate embedding lookup and element-wise addition and compare with the original system (which relies on NCCL) <ref type="bibr" target="#b30">[31]</ref> under 4-GPU settings on the popular Criteo Kaggle <ref type="bibr" target="#b8">[9]</ref> dataset. Table <ref type="table" target="#tab_8">4</ref> shows that DLRM with MGG effectively reduces the lookup time (2.64?). The fine-grained remote access of MGG can reduce redundant inter-GPU traffic by using NCCL and offset the cost by massively parallel GPU-initiated communication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head><p>Deep Learning Pipelines: Despite the popularity of the pipeline concept in the conventional dense DL, the generalization of such a technique in sparse GNN computation is yet to be explored in-depth. PiPAD <ref type="bibr" target="#b43">[44]</ref> overlaps the communication (CPU-to-GPU) and processing (on GPUs) between adjacent graph partitions. Adopting this strategy, we will get designs as Figure <ref type="figure" target="#fig_4">3</ref>(c), which would still suffer from pipeline bubbles due to workload imbalance. vPipe <ref type="bibr" target="#b55">[56]</ref> dynamically assigns a DNN layer to certain pipeline stages during the runtime. It improves pipeline efficiency and GPU utilization for DNN models. However, adopting this approach in our fine-grained kernel pipeline would incur high overhead due to frequent workload reassignment and context switching. In addition, the pipeline bubbles in dense DNN are predictable, inputagnostic, and can be reduced offline. However, the pipeline bubble for GNN can only be figured out at runtime due to input dependency. It, therefore, demands careful online workload balance and a pipeline schedule/mapping. Graph Partitioning Strategies: Besides our current IDbased graph partitioning, our designs/optimizations could also be extended to support other graph partitioning strategies from prior graph processing and GNN work. There are several major categories. 1) Locality-driven partitioning (e.g., Gemini <ref type="bibr" target="#b56">[57]</ref> and Rabbit order <ref type="bibr" target="#b3">[4]</ref>) minimizes the communication/synchronization cost in distributed graph processing/GNN computing. Such partition strategies are orthogonal to our current design optimization. Despite it will reduce the total size of communication, the communication pattern remains the same with irregular, sparse, and finegrained data movements. Our MGG design can be modified to accommodate such reduced-communication cases through dynamic kernel re-configuration (e.g., fine-tuning the interleaving distance and warp-to-block mapping) to maximize communication and computation efficiency. 2) Workloaddriven partitioning (e.g., NeuGraph <ref type="bibr" target="#b25">[26]</ref> and CUBE <ref type="bibr" target="#b52">[53]</ref>) balances the irregular graph/GNN workload among different devices. This type of strategy typically maintains multiple replicas of nodes and node properties on different devices and synchronizes partial results in replicas after local computation on each device. Our current design be adapted to handle such cases by inserting device synchronization primitives (NVSHMEM collective communication primitives, such as nvshmem _ float _ sum _ reduce) for maintaining data consistency among different replicas. 3) Learning-based partitioning (e.g., ROC <ref type="bibr" target="#b17">[18]</ref>) dynamically learns an "optimal" partitioning strategy that can maximize the computation performance. Our current design/optimization can also support this partitioning strategy by incorporating the overhead of NVSHMEM remote memory access in the runtime prediction model when optimizing partitioning strategies online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>This paper presents MGG, a novel multi-GPU system design, and implementation to exploit the potential of leveraging GPU intra-kernel software pipeline for accelerating GNNs. MGG consists of GNN-tailored pipeline construction and GPU-aware pipeline mapping to facilitate workload balancing and operation overlapping, and an intelligent runtime design to dynamically improve the GNN runtime performance. Experiments show the advantages of MGG over state-of-the-art solutions and its generality towards other DL applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Artifact Appendix</head><p>MGG is a holistic runtime for exploiting intra-GPU-kernel communication-computation pipelining to accelerate multi-GPU GNNs. MGG consists of two parts. The first part is the host-side CPU program. It is responsible for dataset loading, runtime configuration generation, and invoking the GPU-side program. The second part is the device-side GPU program, called kernels. It is responsible for the major computation and communication of the GNN model on sparse neighboraggregation across GPUs and dense node-update phase within each GPU. MGG introduces GNN-tailored pipeline construction and GPU-aware pipeline mapping to facilitate workload balancing and operation overlapping.</p><p>? Code repository: Github 2 and Zenodo 3 .</p><p>? Hardware, OS &amp; Compiler:</p><p>-NVIDIA DGX-A100 with dual AMD Rome 7742 processors (each with 64 cores, 2.25 GHz), 1TB host memory, and 8?A100 GPUs (40 GB) connected via NVSwitch (600 GB/s). -Operating systems: Ubuntu 20.04+.</p><p>-Compilers: NVCC (v11.2), GCC (v7.5.0), -Libraries: CUDA (v11.2), OpenMPI (v4.1.1), NVSHMEM (v2.0.3), cuDNN (v8.2). -Datasets: SNAP <ref type="bibr" target="#b22">[23]</ref> and OGB <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Environment Setup</head><p>Step-1: Download libraries and datasets.</p><p>- Step-2. Run Initial Tests.</p><p>Please try below Section-3.4 and Section-3.5.</p><p>Step-3: Experiments.</p><p>-3.1. Compare with UVM (Fig. <ref type="figure" target="#fig_11">8a</ref> and Fig. <ref type="figure" target="#fig_11">8b</ref>).</p><p>./0 _ run _ MGG _ UVM _ 4GPU _ GCN.sh ./0 _ run _ MGG _ UVM _ 4GPU _ GIN.sh ./0 _ run _ MGG _ UVM _ 8GPU _ GCN.sh ./0 _ run _ MGG _ UVM _ 8GPU _ GIN.sh cd dgl _ pydirect _ internal/ ./launch _ docker.sh cd gcn/ ./0 _ run _ gcn.sh cd ../gin/ ./0 _ run _ gin.sh Results of DGL can be found at 1 _ dgl _ gin.csv and 1 _ dgl _ gcn.csv. MGG reference is in MGG _ GCN _ 8GPU.csv and MGG _ 8GPU _ GIN.csv. -3.3. Compare with ROC on 8xA100 (Fig. <ref type="figure" target="#fig_15">9</ref>).   Note that the results can be found at MGG _ WL _ study.csv. -3.6. Compare API (Fig. <ref type="figure" target="#fig_12">10c</ref>). python 4 _ MGG _ API.py Note that the results can be found at MGG _ API _ study.csv. -3.7. Design Space Search (Fig. <ref type="figure" target="#fig_0">11a</ref>). Results can be found at Reddit _ 4xA100 _ dist _ ps.csv, Reddit _ 4xA100 _ dist _ wpb.csv, Reddit _ 8xA100 _ dist _ ps.csv, Reddit _ 8xA100 _ dist _ wpb.csv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results can be found at</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of MGG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Different Multi-GPU GNN strategies for computation and communication. Note that red and green boxes indicate aggregation workload on remote and local neighbors. "SM" boxes with grey areas indicate potential idleness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) Pipeline-aware workload management. "LNP"/"RNP" indicate local/remote workload partitions. (b)(c)(d) Different strategies of workload decomposition and pipelining. Each box indicates a certain (local/remote) aggregation workload and its length indicates its relative latency. "LR": loading remote neighbors, "LL": loading local neighbors, "AC": aggregation computation. Each grey rectangular shadow indicates a workload partition to be processed by one GPU processing unit. (1) and (2) indicate that the same pipeline is chunked into two parts along its time axis due to space limitations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: MGG Storage Layout and Communication Pattern.Note that "NE-i" is the node embedding partition stored on the i-th GPU. "GP-i" is the neighbor partition processed by the i-th GPU. "GPU-i [lb, ub]" is the node-id range [lowerbound, upperbound] of the node embeddings on the i-th GPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Warp-based Mapping and Pipelining. Note that "LNP" refers to the local neighbor partitions; "RNP" refers to the remote neighbor partitions. Workload and Warps are matched based on colors. Tiny boxes in GPU SM indicate decomposed workload operations for overlapped execution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Illustration of (a) w/o and (b) w/ asynchronized primitives for overlapping computation and communication of an individual warp. Note that the length of each rectangular box indicates the estimated latency cost of each operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Performance comparison with DGL. Note that fullgraph PAPER on DGL requires A100-80GB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Performance comparison with MGG-UVM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Optimization Analysis: (a) Neighbor Partitioning; (b) Workload Interleaving; (c) Choice of Communication Primitives.</figDesc><graphic url="image-9.png" coords="13,341.38,192.03,108.15,53.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>1 Figure 11 :</head><label>111</label><figDesc>Figure 11: Parameter selection for three different settings. (a), (b), and (c) are for setting I, II, and III, respectively. Note that the left-side figures show the runtime latency for different combinations of ps and dist, while the right-side figures show the latency for different combinations of wpb and dist.The solid black triangle with "E" is the searched "optimal" combination for ps and dist, while the black solid star with "E" is the searched "optimal" wpb given dist and ps.</figDesc><graphic url="image-4.png" coords="13,462.57,314.52,92.17,54.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig _ 8 _Fig _ 8 _</head><label>88</label><figDesc>Fig _ 8 _ UVM _ MGG _ 4GPU _ GIN.csv, Fig _ 8 _ UVM _ MGG _ 8GPU _ GCN.csv, Fig _ 8 _ UVM _ MGG _ 8GPU _ GIN.csv -3.2.Compare with DGL (Fig.7aand Fig.7b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig _ 9 _</head><label>9</label><figDesc>Fig _ 9 _ ROC _ MGG _ 8GPU _ GIN.csv.-3.4. Compare NP with w/o NP (Fig.10a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>python 2 _</head><label>2</label><figDesc>MGG _ NP.pyNote that the results can be found at MGG _ NP _ study.csv. -3.5. Compare WL with w/o WL (Fig.10b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>python 3 _</head><label>3</label><figDesc>MGG _ WL.py</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>python 5 _</head><label>5</label><figDesc>MGG _ DSE _ 4GPU.py python 5 _ MGG _ DSE _ 8GPU.py</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="1,-9.00,-10.01,630.00,255.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-2.png" coords="1,-9.00,543.00,630.00,259.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>nvshmem _ team _ my _ pe(NVSHMEMX _</head><label></label><figDesc></figDesc><table><row><cell>Listing 1: NVSHMEM APIs in CUDA C.</cell></row><row><cell>// Initialize an NVSHMEM context on CPUs.</cell></row><row><cell>nvshmem _ init();</cell></row><row><cell>// Get the current GPU device ID on CPUs.</cell></row><row><cell>int gpu _ id = TEAM _ NODE);</cell></row><row><cell>// Set the GPU based on its device ID on CPUs.</cell></row><row><cell>cudaSetDevice(gpu _ id);</cell></row><row><cell>// Define NVSHMEM memory visible for all GPUs on CPUs.</cell></row><row><cell>d _ shared _ mem = (void * ) nvshmem _ malloc (num _ bytes);</cell></row><row><cell>// Define global memory visible only for the current GPU.</cell></row><row><cell>cudaMalloc((void ** ) &amp;d _ mem, num _ bytes);</cell></row><row><cell>// Remote access API called by a thread/warp/block.</cell></row><row><cell>__ device __ nvshmem _ float _ get _ {warp/block}(void * dst, const void * src, size _ t nelems, int src _ gpu _ id);</cell></row><row><cell>// Sync all GPUs within an NVSHMEM context on CPUs.</cell></row><row><cell>nvshmem _ barrier _ all();</cell></row><row><cell>// Release NVSHMEM objects on CPUs.</cell></row><row><cell>nvshmem _ free(d _ shared _ mem);</cell></row><row><cell>// Terminate the current NVSHMEM context on CPUs.</cell></row><row><cell>nvshmem _ finalize();</cell></row><row><cell>dependency on limited processing units, where different oper-</cell></row><row><cell>ations would compete for limited GPU resources (e.g., SMs)</cell></row><row><cell>during the runtime. Such two types of dependencies expose</cell></row><row><cell>new opportunities for us to amortize communication costs by</cell></row><row><cell>overlapping neighbor aggregation from different nodes.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Algorithm 1 :</head><label>1</label><figDesc>Range-constrained Binary Search. :Graph node pointer array (nPtr), edge list array (eList) , and the number of GPUs (numGPUs). output :list of graph edge split points (numGPUs -1).</figDesc><table><row><cell cols="2">1 outList = {};</cell></row><row><cell cols="2">2 lastPos = 0;</cell></row><row><cell cols="2">/ * Compute approximated #edges per GPU.</cell><cell>* /</cell></row><row><cell cols="2">3 ePerGPU = (len(eList) + numGPUs -1)/numGPUs;</cell></row><row><cell cols="2">4 for sId in [0, 1, ..., numGPUs -1] do</cell></row><row><cell>5</cell><cell cols="2">nid = binSearch(nPtr, ePerGPU, lastPos, numNodes);</cell></row><row><cell>6</cell><cell>lastPos = nid;</cell></row><row><cell>7</cell><cell>outList[sId] = nid;</cell></row><row><cell>8 end</cell><cell></cell></row><row><cell cols="2">9 return outList;</cell></row><row><cell cols="2">Function binSearch(nPtr, ePerGPU, lastPos,</cell></row><row><cell cols="2">numNodes):</cell></row><row><cell></cell><cell>i = lastPos;</cell></row></table><note><p>input / * Search split points on nPtr. * / j = numNodes; target = min(nPtr[i] + ePerGPU, nPtr[numNodes]); while i &lt; j do mid = (nPtr[i] + nPtr[ j])/2; if mid &gt; target then 17 j = (i + j)/2; else 19 i = (i + j)/2; end return i;</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Datasets for Evaluation.</figDesc><table><row><cell>Dataset</cell><cell>#Vertex</cell><cell cols="3">#Edge #Dim #Class</cell></row><row><cell>reddit(RDD) [45]</cell><cell>232,965</cell><cell>114,615,892</cell><cell>602</cell><cell>41</cell></row><row><cell>enwiki-2013(ENWIKI) [23]</cell><cell>4,203,323</cell><cell>202,623,226</cell><cell>300</cell><cell>12</cell></row><row><cell>it-2004 (IT04) [10]</cell><cell cols="2">41,291,594 1,150,725,437</cell><cell>256</cell><cell>64</cell></row><row><cell cols="3">ogbn-paper100M(PAPER) [12] 111,059,956 1,615,685,872</cell><cell>128</cell><cell>64</cell></row><row><cell>ogbn-products(PROD)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Additional performance comparison of MGG and DGL on GraphSAGE and GAT.</figDesc><table><row><cell>Model</cell><cell cols="2">RDD ENWIKI</cell><cell cols="3">IT04 PAPER PROD PROT ORKT</cell></row><row><cell>SAGE</cell><cell>4.97?</cell><cell cols="2">1.76? 1.99?</cell><cell>3.53? 7.05? 3.39?</cell><cell>3.53?</cell></row><row><cell>GAT</cell><cell>2.65?</cell><cell cols="2">1.62? 2.06?</cell><cell>3.04? 2.06? 3.39?</cell><cell>3.04?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Accuracy-Latency of GNNs w/ and w/o sampling.</figDesc><table><row><cell>Dataset</cell><cell>Accuracy w/ sampling</cell><cell>Accuracy w/o sampling</cell><cell>Latency (w/o vs. w/ sampling)</cell></row><row><cell>RDD</cell><cell>0.937</cell><cell>0.957</cell><cell>1.07?</cell></row><row><cell>PROT</cell><cell>0.776</cell><cell>0.825</cell><cell>1.25?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>DLRM<ref type="bibr" target="#b30">[31]</ref> with MGG in Embedding Lookup.</figDesc><table><row><cell cols="3">Implementation DLRM [31] DLRM (MGG)</cell></row><row><cell>Time (ms)</cell><cell>315.27</cell><cell>119.66</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>1.1. Download libraries. wget storage.googleapis.com/mgg _ data/local.tar.gz tar -zxvf local.tar.gz tar -zxvf local/nvshmem _ src _ 2.0.3-0/build _ cu112.tar.gz -1.2. Download datasets and Setup Baselines. wget storage.googleapis.com/mgg _ data/dataset.tar.gz tar -zxvf dataset.tar.gz cd dgl _ pydirect _ internal/ wget storage.googleapis.com/mgg _ data/graphdata.tar.gz &amp;&amp; tar -zxvf graphdata.tar.gz &amp;&amp; rm graphdata.tar.gz cd .. gsutil cp -r gs://mgg _ data/roc-new/ . -1.3. Launch Docker for MGG. build &amp;&amp; cd build &amp;&amp; cmake .. &amp;&amp; cd .. ./0 _ mgg _ build.sh 2 https://github.com/YukeWang96/MGG-OSDI23-AE.git 3 https://doi.org/10.5281/zenodo.7853945</figDesc><table><row><cell>cd docker</cell></row><row><cell>./launch.sh</cell></row><row><cell>-1.4. Compile MGG implementations.</cell></row></table><note><p>mkdir</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="10">Acknowledgment</head><p>We would like to appreciate the great help and support from OSDI shepherd and anonymous reviewers. This work was supported in part by <rs type="funder">NSF</rs><rs type="grantNumber">-2124039</rs> and CloudBank [32]. We also appreciate the generous help and support from <rs type="funder">Amazon</rs> <rs type="grantName">Faculty Research Award 2021 for Professor Yufei Ding and NVIDIA Graduate Fellowship</rs> <rs type="grantNumber">2022-2023</rs> for <rs type="person">Yuke Wang</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jCwH5t6">
					<idno type="grant-number">-2124039</idno>
				</org>
				<org type="funding" xml:id="_ZTkjZH7">
					<idno type="grant-number">2022-2023</idno>
					<orgName type="grant-name">Faculty Research Award 2021 for Professor Yufei Ding and NVIDIA Graduate Fellowship</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Runtime dependency analysis for loop pipelining in highlevel synthesis</title>
		<author>
			<persName><forename type="first">Mythri</forename><surname>Alle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Morvan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Derrien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Design Automation Conference (DAC)</title>
		<meeting>the 50th Annual Design Automation Conference (DAC)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Amd</forename><surname>Rocm</surname></persName>
		</author>
		<ptr target="https://github.com/ROCm-Developer-Tools/ROC_SHMEM" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Balanced graph partitioning</title>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Andreev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harald</forename><surname>R?cke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixteenth annual ACM symposium on Parallelism in algorithms and architectures (SPAA)</title>
		<meeting>the sixteenth annual ACM symposium on Parallelism in algorithms and architectures (SPAA)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rabbit order: Just-in-time parallel reordering for fast graph analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Arai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shiokawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yamamuro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Onizuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Iwamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Parallel and Distributed Processing Symposium (IPDPS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Legion: Expressing locality and independence with logical regions</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Treichler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elliott</forename><surname>Slaughter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis</title>
		<meeting>the International Conference on High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dgcl: an efficient communication library for distributed gnn training</title>
		<author>
			<persName><forename type="first">Zhenkun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yidi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaihao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth European Conference on Computer Systems (EuroSys)</title>
		<meeting>the Sixteenth European Conference on Computer Systems (EuroSys)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Link prediction approach to collaborative filtering</title>
		<author>
			<persName><forename type="first">Hsinchun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zan</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL)</title>
		<meeting>the 5th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">FastGCN: Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Criteo display ad challenge</title>
		<author>
			<persName><surname>Criteo</surname></persName>
		</author>
		<ptr target="https://kaggle.com/c/criteodisplay-ad-challenge" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The university of florida sparse matrix collection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Timothy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software (TOMS)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning graph representations with embedding propagation</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duran</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed deep graph learning at scale</title>
		<author>
			<persName><forename type="first">Swapnil</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand Padmanabha</forename><surname>Iyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Graph embedding in vector spaces by node attribute statistics</title>
		<author>
			<persName><forename type="first">Jaume</forename><surname>Gibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Valveny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horst</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The architectural implications of facebook&apos;s dnn-based personalized recommendation</title>
		<author>
			<persName><forename type="first">Udit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Reagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bradford</forename><surname>Cottel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><forename type="middle">M</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hempstead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hsien-Hsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheevatsa</forename><surname>Malevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving the accuracy, scalability, and performance of graph neural networks with roc</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sina</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd MLSys Conference</title>
		<meeting>the 3rd MLSys Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Graph classification and clustering based on vector space embedding</title>
		<author>
			<persName><forename type="first">Riesen</forename><surname>Kaspar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bunke</forename><surname>Horst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>World Scientific</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Batch-aware unified memory management in gpus for irregular workloads</title>
		<author>
			<persName><forename type="first">Hyojong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoong</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasun</forename><surname>Gera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramyad</forename><surname>Hadidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyesoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning spectral graph transformations for link prediction</title>
		<author>
			<persName><forename type="first">J?r?me</forename><surname>Kunegis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Lommatzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning (ICML)</title>
		<meeting>the 26th Annual International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Krevl</surname></persName>
		</author>
		<ptr target="https://snap.stanford.edu/data" />
		<title level="m">SNAP Datasets: Stanford large network dataset collection</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Evaluating modern gpu interconnect: Pcie, nvlink, nv-sli, nvswitch and gpudirect</title>
		<author>
			<persName><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuaiwen</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><forename type="middle">R</forename><surname>Tallent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Barker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Parallel and Distributed Systems (TPDS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pagraph: Scaling gnn training on large graphs via computation-aware caching</title>
		<author>
			<persName><forename type="first">Zhiqi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youshan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinlong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th ACM Symposium on Cloud Computing</title>
		<meeting>the 11th ACM Symposium on Cloud Computing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neugraph: parallel deep neural network computation on large graphs</title>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youshan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yafei</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference (ATC)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Pytorch-direct: Enabling gpu centric data access for very large graph neural network training with irregular accesses</title>
		<author>
			<persName><forename type="first">Seung</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sitao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mert</forename><surname>Hidayetoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eiman</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Mei</forename><surname>Hwu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.07956</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large graph convolutional network training with gpu-oriented data communication architecture</title>
		<author>
			<persName><forename type="first">Seung</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sitao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mert</forename><surname>Hidayetoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eiman</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Mei</forename><surname>Hwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pipedream: generalized pipeline parallelism for dnn training</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Harlap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">R</forename><surname>Nikhil R Devanur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><forename type="middle">B</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 27th ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Memory-efficient pipelineparallel dnn training</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep learning recommendation model for personalization and recommendation systems</title>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheevatsa</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hao-Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narayanan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongsoo</forename><surname>Sundaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Udit</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alisson</forename><forename type="middle">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Azzolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Dzhulgakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilia</forename><surname>Mallevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghai</forename><surname>Cherniavskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghuraman</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ansha</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Kondratenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianjie</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Misha</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><surname>Smelyanskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00091</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cloudbank: Managed services to simplify cloud access for computer science research and education</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vince</forename><surname>Kellen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shava</forename><surname>Smallen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Practice and Experience in Advanced Research Computing</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Dgx superpod</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<ptr target="https://nvidia.com/en-us/data-center/dgx-superpod/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Nvidia collective communication library (nccl</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<ptr target="https://developer.nvidia.com/nccl" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Nvidia dgx a100</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<ptr target="https://nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-dgx-a100-datasheet.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Nvshmem communication library</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<ptr target="https://developer.nvidia.com/nvshmem" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<author>
			<persName><surname>Tesla</surname></persName>
		</author>
		<ptr target="https://nvidia.com/en-us/data-center/v100/" />
		<imprint>
			<date>v100</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Unified memory for cuda beginners</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<ptr target="https://developer.nvidia.com/blog/unified-memory-cuda-beginners/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 20th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Optimization principles and application performance evaluation of a multithreaded gpu using cuda</title>
		<author>
			<persName><forename type="first">Shane</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">I</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><forename type="middle">S</forename><surname>Baghsorkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><forename type="middle">S</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">B</forename><surname>Kirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Mei W</forename><surname>Hwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 13th ACM SIGPLAN Symposium on Principles and practice of parallel programming (PPoPP)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Peer-to-peer &amp; unified virtual addressing</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Schroeder</surname></persName>
		</author>
		<ptr target="https://developer.download.nvidia.com/CUDA/training/cuda_webinars_GPUDirect_uva.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Towards time-aware link prediction in evolving social networks</title>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Tylenda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd workshop on social network mining and analysis</title>
		<meeting>the 3rd workshop on social network mining and analysis</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>Ralitsa Angelova, and Srikanta Bedathur</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pipad: Pipelined and parallel dynamic gnn training on gpus</title>
		<author>
			<persName><forename type="first">Chunyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Desen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuebin</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming (PPoPP)</title>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep graph library: Towards efficient and scalable deep learning on graphs</title>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Gnnadvisor: An efficient runtime system for gnn acceleration on gpus</title>
		<author>
			<persName><forename type="first">Yuke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyuan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gushu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">El-rec: efficient large-scale recommendation model training via tensor-train embedding table</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyuan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheevatsa</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Muthiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<ptr target="https://en.wikipedia.org/wiki/CUDA" />
		<title level="m">Nvidia gpu micro-architecture</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Hygcn: A gcn accelerator with hybrid architecture</title>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujing</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaochun</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhimin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongrui</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Gnnlab: a factored system for sample-based gnn training over gpus</title>
		<author>
			<persName><forename type="first">Jianbang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahai</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoniu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth European Conference on Computer Systems (EuroSys)</title>
		<meeting>the Seventeenth European Conference on Computer Systems (EuroSys)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 32nd International Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Exploring the hidden dimension in graph processing</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuehai</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weimin</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep learning based recommender system: A survey and new perspectives</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Every document owns its structure: Inductive text classification via graph neural networks</title>
		<author>
			<persName><forename type="first">Yufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueli</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongzhen</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">vpipe: A virtualized acceleration system for achieving efficient and scalable pipeline parallel dnn training</title>
		<author>
			<persName><forename type="first">Shixiong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xusheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuxian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Parallel and Distributed Systems (TPDS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Gemini: A computation-centric distributed graph processing system</title>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenguang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weimin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaosong</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
