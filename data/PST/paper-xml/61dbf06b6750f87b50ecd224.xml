<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CogKR: Cognitive Graph for Multi-hop Knowledge Reasoning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IEEE</orgName>
								<address>
									<addrLine>Jie Tang</addrLine>
									<settlement>Fellow, Fellow</settlement>
									<region>IEEE</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IEEE</orgName>
								<address>
									<addrLine>Jie Tang</addrLine>
									<settlement>Fellow, Fellow</settlement>
									<region>IEEE</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IEEE</orgName>
								<address>
									<addrLine>Jie Tang</addrLine>
									<settlement>Fellow, Fellow</settlement>
									<region>IEEE</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Teng</forename><surname>Tu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IEEE</orgName>
								<address>
									<addrLine>Jie Tang</addrLine>
									<settlement>Fellow, Fellow</settlement>
									<region>IEEE</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Letian</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IEEE</orgName>
								<address>
									<addrLine>Jie Tang</addrLine>
									<settlement>Fellow, Fellow</settlement>
									<region>IEEE</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IEEE</orgName>
								<address>
									<addrLine>Jie Tang</addrLine>
									<settlement>Fellow, Fellow</settlement>
									<region>IEEE</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IEEE</orgName>
								<address>
									<addrLine>Jie Tang</addrLine>
									<settlement>Fellow, Fellow</settlement>
									<region>IEEE</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CogKR: Cognitive Graph for Multi-hop Knowledge Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>cognitive graph</term>
					<term>knowledge graph representation and reasoning</term>
					<term>multi-hop reasoning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Inferring new facts from an existing knowledge graph with explainable reasoning processes is an important problem, known as knowledge graph (KG) reasoning. The problem is often formulated as finding the specific path that represents the query relation and connects the query entity and the correct answer. However, due to the limited expressiveness of individual paths, the majority of previous works failed to capture the complex subgraph structure in the graph. We propose CogKR that traverses the knowledge graph to conduct multi-hop reasoning. More specifically, motivated by the dual process theory from cognitive science, our framework is composed of an extension module and a reasoning module. By setting up a cognitive graph through iteratively coordinating the two modules, CogKR can cope with more complex reasoning scenarios in the form of subgraphs instead of individual paths. Experiments on three knowledge graph reasoning benchmarks demonstrate that CogKR achieves significant improvements in accuracy compared with previous methods while providing the explainable capacity. Moreover, we evaluate CogKR on the challenging one-shot link prediction task, exhibiting the superiority of the framework on accuracy and scalability compared to the state-of-the-arts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>K NOWLEDGE graphs (KGs) such as Freebase [1], NELL <ref type="bibr" target="#b1">[2]</ref>, and YAGO <ref type="bibr" target="#b2">[3]</ref> have been built in the last decade and nourished a wide range of downstream tasks, including relation extraction <ref type="bibr" target="#b3">[4]</ref>, question answering <ref type="bibr" target="#b4">[5]</ref>, dialogue systems <ref type="bibr" target="#b5">[6]</ref> and recommender systems <ref type="bibr" target="#b6">[7]</ref>. However, the incompleteness challenge that exists in most KGs seriously limits the accuracy of downstream tasks. This thus motivates a lot of works proposed for new fact inference in these years <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>.</p><p>Prior arts for KG reasoning can be roughly categorized into embedding-based methods <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> and path-based methods <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Embeddingbased methods collaboratively learn the distributed representations of entities and relations according to existing links in KGs. However, they usually consider only direct links and lack the ability of multi-hop reasoning, which involves multiple entities and facts in the reasoning process <ref type="bibr" target="#b9">[10]</ref>. Path-based methods instead leverage multi-hop path information of entity pairs to infer their underlying relations <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. But due to their poor generalization to unseen paths, the performances have largely been surpassed by embedding-based methods.</p><p>More recent advancements in multi-hop reasoning combine path-based methods with distributed representations <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> and reinforcement learning <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> • Zhengxiao Du, Teng Tu, and Letian Cheng are with the Department of Computer Science and Technology, Tsinghua University. E-mail: {zx-du20,tut19,chenglt19}@mails.tsinghua.edu.cn • Chang Zhou, Jiangchao Yao, Hongxia <ref type="bibr">Yang, and Jingren</ref> Zhou are with the DAMO Academy, Alibaba Group. E-mail: {ericzhou.zc,jiangchao.yjc,yang.yhx,jingren.zhou}@alibaba-inc.com • Jie Tang is with the Department of Computer Science and Technology, Tsinghua University. E-mail: jietang@tsinghua.edu.cn. Jie Tang is the corresponding author.</p><p>Manuscript received <ref type="bibr">March 14, 2021</ref> to improve the generalization ability. These methods have gained remarkable performance in many benchmarks. However, the expressiveness of these path-based methods is still limited to paths, which could only represent a small subset of first-order logical formulas <ref type="bibr" target="#b22">[23]</ref>. A more powerful form to encode multi-hop relations is the subgraph of the KG, which can handle more complex logical formulas. Unfortunately, it is still challenging to search and reason over exclusive subgraphs of the KGs efficiently. To bridge the gap, we may get some heuristics from the dual-process theory <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, one of the dominant theories in cognitive science that studies thoughts and reasoning processes. Specifically, according to the dual process theory, the reasoning system of human beings consists of two distinct processes, one to retrieve relevant information intuitively (System 1) and the other to reason over the collected information via a controllable, sequential, and logical reasoning process (System 2). Such two systems are also related to the capacity-limited working memory <ref type="bibr" target="#b27">[28]</ref>. System 1 updates the working memory with the retrieved content, while System 2 operates on the content of the working memory <ref type="bibr" target="#b23">[24]</ref>.</p><p>Inspired by the two-System structure of the dual process theory, we propose a novel framework CogKR to efficiently conduct multi-hop KG reasoning over subgraphs. The core of the proposed CogKR is the cognitive graph <ref type="bibr" target="#b28">[29]</ref>, a subgraph of the original KG iteratively expanded in the reasoning process, which resembles the working memory in human brains. Our framework combines two iterative processes, one to expand the subgraph with relevant entities and edges from the neighborhood, and the other to conduct relational reasoning based on the subgraph. The two processes, which resemble System 1 and System 2 in dual process theory, are iterated to search and reason over subgraphs of the KG. Moreover, the model is fully differentiable, such that the model can be trained efficiently via the Fig. <ref type="figure" target="#fig_2">1</ref>. An example of multi-hop KG reasoning. The aim is to find the entity that has the relation "Lives in" with the entity Player A. Blue circles and orange, bold lines represent the bridge entities and relations used in the reasoning process. By reasoning about the complex subgraph structure around Player A, we can find the correct answer City B.</p><p>stochastic gradient descent methods. Our contributions are summarized as follows:</p><p>• We propose CogKR, a novel framework for multihop KG reasoning based on the cognitive graph structure inspired by human cognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We show that CogKR could model the complex structure in a subgraph, improving the multi-hop reasoning ability while preserving the explainability simultaneously in knowledge graph completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We evaluate CogKR on the challenging task of oneshot link prediction in KGs. Our model achieves improvements on accuracy and scalability over the one-shot baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Knowledge Graph Embedding</head><p>Embedding methods for knowledge graphs have been extensively studied for KG completion. From a general perspective, entities and relations are represented as continuous vectors in latent space, and various scoring functions are defined for a fact (e s , r, e o ). RESCAL <ref type="bibr" target="#b29">[30]</ref> is one of the earlier works that model multi-relational data with vector representations. Following that, more advanced scoring functions have been proposed, such as vector difference <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b11">[12]</ref>, vector product <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b30">[31]</ref>, convolution <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b31">[32]</ref>, and tensor operation <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b32">[33]</ref>. Although these embedding approaches have achieved impressive results on several KG completion benchmarks, they have been shown to suffer from cascading errors when modeling multi-hop relations <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b33">[34]</ref>, which are indispensable for more complex reasoning tasks. Besides, since these methods all operate on latent space, their predictions are not interpretable.</p><p>Furthermore, these embedding methods usually assume enough training facts for all relations. One-shot link prediction on KGs has been proposed by <ref type="bibr" target="#b34">[35]</ref> to handle queries about a new relation type with only one training instance. They propose a similarity metric based on graph convolutional network <ref type="bibr" target="#b35">[36]</ref> and multi-step matching to compute scores for all the candidate facts. However, their model requires forward pass through neural networks for every candidate, which is computationally expensive or even intractable for large-scale KGs. Their method has been further extended to handle few-shot learning (more than one training instance is given) via aggregation network <ref type="bibr" target="#b36">[37]</ref> or optimization-based meta learning <ref type="bibr" target="#b37">[38]</ref>. Another line of research is to augment the KG embedding methods with the ability to handle unseen entities or relations by leveraging text descriptions <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Knowledge Graph Reasoning</head><p>Learning symbolic logic rules has been the mainstream to automatically infer new knowledge in its early days <ref type="bibr" target="#b41">[42]</ref>. In statistical relational learning, machine learning is combined with symbolic rules to handle uncertainty <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. The Path-Ranking Algorithm <ref type="bibr" target="#b14">[15]</ref> uses a random walk with restart mechanism to obtain paths between two entities and perform supervised classification of relations according to discrete path features. Various neural methods to learn firstorder logical rules have also been proposed recently <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>. Although logical formulas are easy to explain, they have largely been superseded by distributed vector representations due to the poor generalization ability.</p><p>To overcome the limit, many works <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> have proposed approaches that explicitly encode multi-step paths with deep learning, denoted as path-based methods. Such methods benefit from both the generalization of distributed representations and the explainability of logical rules. Both Chain-of-Reasoning <ref type="bibr" target="#b20">[21]</ref> and Compositional Reasoning <ref type="bibr" target="#b21">[22]</ref> infer underlying relations of two entities with neural networks taking multi-step paths found by random walk as input. Recently, DeepPath <ref type="bibr" target="#b9">[10]</ref> uses RLbased agents combined with pre-trained KG embeddings to find better paths for reasoning than those found by random walks. MINERVA <ref type="bibr" target="#b15">[16]</ref> reformulates the problem of predicting the missing entity in a new fact as a sequential decision problem of walking from one entity to reach the answer. MultihopKG <ref type="bibr" target="#b17">[18]</ref> augments MINERVA with reward reshaping and action dropout to overcome false-negative supervision and spurious paths. M-Walk <ref type="bibr" target="#b16">[17]</ref> uses Monte Carlo Tree Search to solve the reward sparsity problem. DIVA <ref type="bibr" target="#b10">[11]</ref> unifies path-finding and path-reasoning with variational inference. DIVINE <ref type="bibr" target="#b44">[45]</ref> uses generative adversarial imitation learning <ref type="bibr" target="#b45">[46]</ref> to learn reasoning policies and reward functions self-adaptively through imitating the demonstrations sampled from KGs. Compared with previous KG reasoning methods, our proposed CogKR bases reasoning on subgraphs that can capture the interaction of multiple paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Graph Neural Networks for Relational Learning</head><p>Graph neural networks (GNN) <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref> are a class of neural networks that model graphs and structure their computations accordingly. Recently, graph convolutional network (GCN) <ref type="bibr" target="#b35">[36]</ref> and its multiple variants <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref> have become the dominant methods in representation learning and semisupervised learning on graphs <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>. GNNs pretrained on large-scale unlabeled data via self-supervised learning can further improve the performance on downstream tasks <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>.</p><p>Although GNNs have been successfully applied to link prediction on graphs <ref type="bibr" target="#b54">[55]</ref>, most GNNs operate on homogeneous graphs, in which only one type of edge exists. On the contrary, in a typical KG, there are various relation types, i.e., types of edges. Even in works of GNN for heterogeneous graphs <ref type="bibr" target="#b55">[56]</ref>, the number of edge types is quite limited, far fewer than that in a typical KG. Therefore, it is not easy to directly apply GNNs to KG reasoning. To overcome the limit, R-GCN <ref type="bibr" target="#b56">[57]</ref> and its variants <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref> modify the GCN structure to use different parameters for different relation types. These methods only apply GNNs to the computation of entity embeddings, while the predictions are based on scoring functions of embedding-based methods. Therefore, their methods can be considered as more complicated ones of embedding-based methods, and cannot provide explanations for the predictions. GraIL <ref type="bibr" target="#b59">[60]</ref> reasons over local subgraph structures with GNNs to make inductive relation prediction between two entities. However, directly applying the method to knowledge graph completion can lead to infeasible time cost, since it needs to extract subgraphs for every candidate entity of a query. Furthermore, <ref type="bibr" target="#b60">[61]</ref> proposes an algorithm to synthesize benchmarks to evaluate the logical generalization of GNNs for relational learning. DPMPN <ref type="bibr" target="#b61">[62]</ref> proposes a two-GNN framework to encode both global and local graph structures coordinated by an attention module. Their approach mainly focuses on pruning message passing in GNN to improve scalability. Our module, which introduces dual-process theory to improve multi-hop reasoning, is conceptually simpler and provides clearer explanations of prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM FORMULATION</head><p>A knowledge graph G is represented as G = (E, R, T ), where E and R denote the entity set and the relation set. T is a set of triples {(e s , r, e o )} N E ✓ R ✓ E which denote the facts about the specific relation r " R from e s " E to e o " E. e s is called the head entity and e o is called the tail entity. We can treat G as a directed graph composing of entities as nodes and relations as different edge types. Then, the triple (e s , r, e o ) represents a directed edge of type r from e s to e o . There may be multiple edges of different types between two nodes. Following <ref type="bibr" target="#b15">[16]</ref>, we augment G with the inverse link (e o , r 1  , e s ) for any (e s , r, e o ) " T to enhance the connectivity.</p><p>For multi-hop knowledge graph reasoning, we target to leverage a reasoning model to predict the tail entity from a background KG G for each query (e s , r, ?). The reasoning model should traverse the knowledge graph, collect relevant evidence, and predict the correct answer based on the evidence. The evidence might include multiple entities and relations, which require multi-hop reasoning ability. Figure <ref type="figure" target="#fig_2">1</ref> illustrates an example of this task. Generally, in the training phase, we use a set of triples T train to train a reasoning model, so that for each (e s , r, e o ) in T train the predicting probability of e o given the query (e s , r, ?) is maximized.</p><p>Then, in the test phase, a different set of triples T test is used to evaluate the performance of the trained model. That is, given a query (e s , r, ?), we use the trained model to output prediction and compare it with the groudtruth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">APPROACH</head><p>In this section, we describe the proposed model for multihop KG reasoning and the training algorithm, discuss the connection with previous methods, and analyze the complexity of the algorithm.</p><p>Our whole framework for multi-hop KG reasoning problem is shown in Figure <ref type="figure" target="#fig_1">2</ref>. It takes a head entity e s and a relation r as input and predicts the correct tail entity e o via an explicit reasoning process. The problem is challenging, as in most cases, inferring unseen relationships usually involves complicated reasoning processes. We connect our study to the dual-process theory from cognitive science <ref type="bibr" target="#b24">[25]</ref>. Accordingly, the proposed model combines two iterative processes: retrieving information from the original KG (System 1) and reasoning over collected information (System 2). The retrieved information and reasoning results are stored in a unique structure called cognitive graph. In the following, we will introduce the cognitive graph, System 1 and System 2 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Cognitive Graph</head><p>The cognitive graph G is a subgraph of G that contains entities and edges selected from G as relevant evidence and latent representations for its entities as the reasoning results. Formally, G = (V, E, X), where V N E, E N T and X " R ∂V ∂✓d . Here X is the matrix of latent representations, whose each element X[e] for the entity e represents the semantic information of e in the reasoning process.</p><p>In the beginning, V only contains the initially given head entity e s . At each step t, the V and E in G are expanded based on currently involved nodes by System 1, and then the representations X for the expanded V are updated by System 2. To trace the currently involved entities, an attention flow on the graph <ref type="bibr" target="#b62">[63]</ref> is constructed. We define the attention distribution a t at step t, a probability distribution over entities in G, to represent the current focus. Given the query (e s , r, ?), the initial attention a 0 is focused on e s , that is, a 0 has 1 for e s and 0 for other entities. Then after each expansion step, we compute a t+1 based on a t to update the focus.</p><p>Compared with individual paths found by previous path-based reasoning methods, the cognitive graph is more expressive, since some complex reasoning processes may require the interaction among paths. For example, the pathfinding method cannot distinguish two paths e s  Then, the reasoning module (System 2) updates the latent representations of newly attended nodes (green), via neighborhood aggregation and computes the attention values to measure their importance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">System 1 (Expansion)</head><p>The function of System 1 is to collect relevant evidence from G for the subsequent reasoning. It iteratively leverages a progressive subgraph expansion to search the possible evidence of entities and relations. Specifically, at each step t, System 1 selects the involved entities F t 1 at the last step and expand G with part of F t 1 's outgoing edges and the corresponding nodes.</p><p>Given the attention at last step a t 1 , F t 1 is a collection of entities with probability mass larger than zero, i.e. F t 1 = {e∂a t 1 (e) &gt; 0}. For each entity e k in F t 1 , the candidate set to expand G consists of the outgoing edges of e k in G, termed as A t (e k ) = {(r, e)∂(e k , r, e) " G}. To avoid out-of-memory error during computation, we rank the edges in A t (e k ) according to the PageRank values of connected entities and cut the maximum number of edges in A t (e k ) by a threshold ⌘. To provide the agent with the option of staying at e k , we add a self-loop link to A t (e k ). After obtaining A t (e k ), we can build the candidate matrix A t (e k ) " R ∂A t (e k )∂✓3d by stacking the embeddings of all edges in A t (e k ). The embedding of an edge (r, e) is the concatenation of the entity embedding v e , the relation embedding v r , and the entity's latent representation X[e] (filled with 0 if e ä V ). Based on the neighborhood features A t of e k and e k 's own representation (X[e k ], v e k ), as well as the query relation embedding v r , the probabilities of selecting edges are computed as</p><formula xml:id="formula_0">s t (e k ) = (A t (e k )W 1 ) W 2 [X[e k ] h v e k h v r ]⌥ p t (e k ) = a t 1 (e k ) Softmax(s t (e k ))<label>(1)</label></formula><p>where W 1 " R 3d✓d , W 2 " R d✓3d are parameters, and h denotes concatenation.</p><p>The concatenation of the probability vectors p t (e) for all the entities e k in F t 1 , denoted as p t , represent a probability distribution over the outgoing edges of entities in F t 1 . From all of the outgoing edges, we select n edges with the largest probability values, denoted as E t . n is the action budget, i.e., the maximum number of selected edges at each step. After that, we add edges in E t to E and add entities that are connected with selected edges but never visited before to V , which implements the cognitive graph expansion. Note that, there are two differences compared with pathfinding methods. Firstly, when n &gt; 1, we select multiple edges at each step, so that the search subgraph can form a directed acyclic graph (DAG) rather than a single path. Secondly, the attention flow mechanism is deterministic and differentiable, which means that we could train the whole module end-to-end without reinforcement learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">System 2 (Reasoning)</head><p>After the expansion by System 1, the relational reasoning can be conducted by System 2 over the extended cognitive graph. To be more specific, we need System 2 to provide the following two operations: (1) update the latent representations of entities that are visited via the newly selected edges;</p><p>(2) adjust the attention distribution over entities according to selected edges.</p><p>For the first one, out of consideration about the generalization ability <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, we adopt the deep learning module instead of previous rule-based reasoning modules <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>. Concretely, considering the flexible structure of the cognitive graph, we compute the node representations with graph neural networks <ref type="bibr" target="#b46">[47]</ref>, which capture the dependence of graphs via message passing <ref type="bibr" target="#b63">[64]</ref> between the nodes of graphs:</p><formula xml:id="formula_1">X[e] = U (e, m e ) m e = = (e k ,r k )"E e M (e k , r k , e),<label>(2)</label></formula><p>where E e = {(e k , r k )∂(e k , r k , e) " E} is the ingoing edges of e in G, M (e k , r k , e) is the message vector passed from e k to e via relation r k , and U ( , ) is the node update function. Unlike conventional GNNs where the current layer of representations are computed from the previous layer(s), all the representations are computed in the same layer but in a sequential way. In this sense, the way we update node representations is similar to the homogeneous AC-GCN in <ref type="bibr" target="#b64">[65]</ref>, in which the GNN shares the same parameters across layers.</p><p>By extending the framework of <ref type="bibr" target="#b64">[65]</ref> to directed graph with more than one edge type, we can study the logical expressiveness of System 2 in terms of first-order predicate logic. Given a query (e s , r, ?), the rule to look for the correct answer can be expressed as a logical node classifier, which is given by a formula (x) with exactly one free variable x. For example, (x) ⇥= øx ¨¨⇥øx ¨⇥r</p><formula xml:id="formula_2">1 (x ¨, x ¨¨) 0 x ¨= e s 0 r 3 (x ¨¨, x) , (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where 0 is the logical AND operator and r(e 1 , e 2 ) represents that (e 1 , r, e 2 ) " T . The free variable x is evaluated over the entity set E and any entity e that makes the body part evaluate to true is the correct answer. With the definition of logical node classifier in hand, we can give the theorem about the logical expressiveness of System 2: Definition 1. The set of logical node classifiers is defined as:</p><formula xml:id="formula_4">• (x = e s ) " for r " R. • 1 (x) 0 2 (x) 0 ⇧ 0 n (x) " for 1 (x), 2 (x), ⇧, n (x) " • øx ¨(r(x ¨, x) 0 (x ¨)) for r " R, (x ¨) "</formula><p>Theorem 1. Every logical node classifier (x) " can be captured by our System 2.</p><p>The proof of the theorem is in Appendix A.</p><p>Considering the success of RNN models in path-based methods <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b21">[22]</ref>, we use GRU <ref type="bibr" target="#b65">[66]</ref>, a variant of RNN with a gating mechanism as the message function</p><formula xml:id="formula_5">M (e k , r k , e) = GRU(X[e k ], v r k h v e ),<label>(4)</label></formula><p>where</p><formula xml:id="formula_6">GRU(X[e k ], [v r k ; v e ])</formula><p>is the one-step update of GRU with the input of the previous hidden state X[e k ] and the relation-entity feature v r k h v e . We do not use more advanced RNN models such as LSTM <ref type="bibr" target="#b66">[67]</ref> that improve long-term memory since reasoning paths are usually short. The node update function is simply the average function</p><formula xml:id="formula_7">U (e, m e ) = 1 ∂E e ∂ m e<label>(5)</label></formula><p>It can be considered as an extension of the Path-RNN <ref type="bibr" target="#b21">[22]</ref>, with the ability to encode not only a single path but a complex subgraph.</p><p>For the second operation, we directly aggregate the probabilistic values of edges pointing to the entities to compute a new attention distribution over entities, which is formulated as follows ãt (e) = = (e ¨,r,e)"E t p t (e ¨, r, e) a t (e) = ãt (e) &lt; e ¨"E ãt (e ¨) <ref type="bibr" target="#b5">(6)</ref> Note that the normalization is necessary since the sum of the probabilities of E t might be smaller than 1 due to previous top-n selection. </p><formula xml:id="formula_8">a t ⇥ 0 ∂E∂ , V t ⇥ o 14:</formula><p>for (e ¨, r, e) in E t do 15:</p><p>a t (e) ⇥ a t (e) + p t (e ¨, r, e)</p><p>16:</p><formula xml:id="formula_9">V t ⇥ V t &lt; {e ¨} 17:</formula><p>end for 18:</p><p>if t j T then 19:</p><formula xml:id="formula_10">V ⇥ V &lt; V t ,E ⇥ E &lt; E t 20:</formula><p>Update X[V t ] with Equations ( <ref type="formula" target="#formula_1">2</ref>) and ( <ref type="formula" target="#formula_5">4</ref>)</p><formula xml:id="formula_11">21:</formula><p>a t ⇥ a t / &lt; e ¨at (e ¨)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22:</head><p>end if 23: end for 24: return argmax e"V a T (e)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Prediction and Optimization</head><p>In the previous sections, we describe how CogKR builds the cognitive graph and conducts relational reasoning via deep learning. In this section, we will introduce how the final answer is predicted and what objective is used to optimize the model.</p><p>Generally, the prediction of the tail entity e o in the query could be straightforward. At the last step T , we directly compute a T based on a T 1 as described in Sections 4.2 and 4.3 but without selecting n edges since the expansion of the cognitive graph is no need any more. Then, the a T is considered as the prediction distribution and we maximize the probability of e o being predicted as the correct entity. A cross-entropy loss fashion `(a T , e o ) = log a T (e o ) can be applied. However, one critical issue is that the prediction distribution a T does not have full support over the entity set E. As we only compute the probabilities for entities in the T -hop neighborhood of e s , the others are all a T (e) = 0. Moreover, since only top n edges are selected in the iterative process and the local nature of the attention mechanism, a T could be more sparse than the T -hop neighborhood. Therefore, it is quite possible that a T (e o ) = 0 in the initial or the intermediate training procedure. To avoid such ill-posed cases to the log function, we assume the probability of e o is a small number ✏ close to zero when a(e o ) = 0. So the final objective is defined as:</p><formula xml:id="formula_12">`(a T , e o ) = w log a T (e o ) a T (e o ) &gt; 0 log(1 + ✏ &lt; e a T (e)) a T (e o ) = 0<label>(7)</label></formula><p>We use the stochastic gradient descent to approximate the gradient descent on the full dataset in Equation <ref type="bibr" target="#b6">(7)</ref>. The complete algorithm is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">The difference with path-finding methods</head><p>As illustrated in Section 1, our framework has two main differences with path-finding methods. Firstly, our framework can search the knowledge graph in the form of a DAG instead of a single path. Secondly, our model can be trained end-to-end without reinforcement learning. There is also the third difference. According to <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, beam search is often used in the evaluation stage of path-finding methods to improve sample efficiency. The model rolls out n paths for a query, in which n edges that have the largest accumulated probability values are selected greedily at each step. The way our model explores the KG following attention is similar to beam search, but the explored entities and edges are integrated into the cognitive graph, instead of individual paths. This enables the interaction of multiple paths and avoids inconsistency between training and evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Explainability of cognitive graph</head><p>After predicting the correct answer e o for a query (e s , r, ?), we can extract a graphical explanation for the prediction from the cognitive graph. The explanation is generated by extracting the enclosing subgraph between the head entity e s and the predicted answer e o in the cognitive graph G. The enclosing subgraph of G between entities e s and e o is defined as the subgraph of G that consists of entities on a (directed) path between e s and e o and edges between the entities. From Section 4.3 we know that the subgraph between e s and e o can decide e o 's latent representation. Therefore it can provide graphical explanation for the prediction. In Section 5.4 we show examples of graphical explanations generated by CogKR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Complexity Analysis</head><p>To complete a query (e s , r, ?), embedding-based methods need to enumerate the whole entity set, so it takes O(∂E∂) time for every query. For a large KG containing millions of entities, this is highly expensive especially combined with complex scoring functions.</p><p>CogKR, on the other hand, utilizes the local structure of the KG to reduce the time complexity. For System 1, at each step, at most n entities are visited and for each entity we compute scores of at most ⌘ outgoing edges. Given the iteration times T , it thus takes O(T n⌘) time to complete graph expansion. Similarly for System 2, at each step, at most n nodes' latent representations are updated. To update each entity, we need to aggregate the messages from at most ∂E∂ edges. Therefore, it takes O(T n∂E∂) time for latent representation computation. Since at each step at most n edges are added to the cognitive graph, we have ∂E∂ &amp; T n. Overall, CogKR takes at most O(T n⌘ + T 2 n 2 ) time. Given predefined T, n and ⌘, the maximum time is a constant that does not depend on the entity number, and thus is scalable to handle the large KGs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENT</head><p>In this section, we provide empirical results to validate the effectiveness of CogKR on multi-hop KG reasoning. Firstly, we evaluate CogKR on three knowledge graph completion benchmarks, to show that CogKR could outperform both embedding-based and path-based baselines. Secondly, we evaluate CogKR on the one-shot link prediction task proposed recently, to show CogKR's superiority in accuracy and scalability on this challenging setting. Thirdly, we analyze the performance of CogKR from different perspectives, including the ablation study, the reasoning ability for different hops, the convergence speed, and the influence of hyperparameters. Finally, we conduct a case study over the graphical explanations extracted from cognitive graphs to show that CogKR could utilize the subgraph structure to conduct relational reasoning and provide explanations.  We use two public KG datasets, FB15K-237 <ref type="bibr" target="#b67">[68]</ref> and WN18RR <ref type="bibr" target="#b12">[13]</ref> for the KG completion task. These two datasets are sampled from FB15K <ref type="bibr" target="#b7">[8]</ref> and WN18 <ref type="bibr" target="#b7">[8]</ref> with inverse relations causing test set leakage removed. Therefore, they are more challenging and realistic. The original datasets, FB15K and WN18, have been shown to suffer from test set leakage due to inverse relations from the training set being present in the test set <ref type="bibr" target="#b12">[13]</ref>. To further validate that CogKR can tackle large-scale KGs, we also evaluate CogKR on YAGO3-10, a subset of of YAGO3 <ref type="bibr" target="#b68">[69]</ref> which consists of entities that have a minimum of 10 relations each. For all the datasets, we use the original data split, which is widely used in most papers. The dataset statistics are shown in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Knowledge Graph Completion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Experimental Setting</head><p>We add inverse relations into the training set as data augmentation. But these inverse relations are not added into the validation or test set on FB15K-237 and WN18RR, to avoid inconsistency in evaluation data with other papers <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b44">[45]</ref>. During training, given a training triple (e s , r, e o ), the corresponding edge and its inverse are masked for the model to avoid leakage. On FB15K-237, following <ref type="bibr" target="#b61">[62]</ref>, all the edges from e s to e o are also masked, forcing the model to learn a composite reasoning pattern rather than a singlehop pattern.</p><p>During the evaluation, for a test triple (e s , r, e o ), all the correct answers for the query (e s , r, ?) except e o are removed from the prediction. This is called the filtered setting <ref type="bibr" target="#b7">[8]</ref> and has been used by most papers, since it can provide a more reliable performance metric in the presence of multiple correct triples. We use Hits@1,3,10 and mean reciprocal rank (MRR), which are standard metrics for KB completion, as evaluation metrics. We compare CogKR with various state-of-the-arts. For embedding-based methods, we compare with TransE <ref type="bibr" target="#b7">[8]</ref>, DistMult <ref type="bibr" target="#b30">[31]</ref>, ComplEx <ref type="bibr" target="#b8">[9]</ref>, R-GCN <ref type="bibr" target="#b56">[57]</ref>, ConvE <ref type="bibr" target="#b12">[13]</ref>, Ro-tatE <ref type="bibr" target="#b13">[14]</ref>, and TuckER <ref type="bibr" target="#b32">[33]</ref>. Among the embedding-based methods, R-GCN also applies GNN to conduct relational reasoning over knowledge graphs. For path-based methods, we compare with NeuralLP <ref type="bibr" target="#b43">[44]</ref>, MINERVA <ref type="bibr" target="#b15">[16]</ref>, Multi-hopKG <ref type="bibr" target="#b17">[18]</ref>, M-Walk <ref type="bibr" target="#b16">[17]</ref> and DIVINE <ref type="bibr" target="#b44">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Hyperparameters</head><p>The dimensions of embeddings and node representations are set to 100 and 200 respectively. The entity and relation embeddings are randomly initialized. As two key hyperparameters, maximum step T is set to 4 on FB15K-237 and YAGO3-10 and 5 on WN18RR and action budget n is set to 64 on FB15K-237 and YAGO3-10 and 32 on WN18RR. The maximum neighbor number ⌘ is set to 256. The activation function is LeakyReLU in all the layers.</p><p>We use the ADAM optimization algorithm for model training with a learning rate of 1e-3. We also add L2 regularization with a weight decay of 0.0001. The batch size is 28 on FB15K-237 and YAGO3-10 and 64 on WN18RR. The maximum training step is 30,000 on WN18RR and YAGO3-10 and 80,000 on FB15K-237. We use the MRR on the validation set as the criteria to pick the best model checkpoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Result Analysis</head><p>Table <ref type="table" target="#tab_2">2</ref> reports the KG completion performance on FB15K-237 and WN18RR datasets. As shown on both datasets, CogKR produces consistent improvements over previous methods in terms of Hits@1, 3 and MRR. Path-based methods, including NeuralLP, MINERVA, MultihopKG, M-Walk, and DIVINE, generally perform worse than embeddingbased methods. Our model could outperform all the embedding-based and path-based methods. This demonstrates the effectiveness of CogKR with the structure of System 1 and System 2. Besides, the improvements of our model on FB15K-237 are particularly substantial in all the evaluated metrics, with relative improvements of 9.8% on Hits@1. We speculate that it can be related to the fact that on FB15K-237 there are no edges in the training set directly linking any pair of head and tail in the validation (or test) set. Therefore, the multi-hop reasoning ability is particularly important on this dataset, which is exactly what our model is good at. On the contrary, for a large portion (35.2%) of facts in the validation and test set of WN18RR, the head and the tail are linked by some edge (of a different relation) in the train set. Therefore, the multi-hop reasoning ability may not be so important, which limits the improvements we can achieve. We will further analyze the multi-hop reasoning performance on WN18RR in Section 5.3.2.</p><p>Table <ref type="table">3</ref> shows the experimental results on YAGO3-10. The dataset is not used in previous path-based methods. We select MultihopKG, which has the best performance among path-based methods in the previous experiment, as the pathbased baseline. CogKR can also outperform baselines on YAGO3-10, in terms of all the metrics except Hits@10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">One-shot Link Prediction on KGs</head><p>To further validate the effectiveness of CogKR, we evaluate the model on a more challenging task proposed recently: one-shot link prediction on KGs <ref type="bibr" target="#b34">[35]</ref>. In this task, we need to perform link prediction for relation types with only one training fact per relation. Since the information about each relation type is limited and vague, it poses more challenges to the model about its reasoning ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Datasets</head><p>Most benchmarks for knowledge graph completion, like FB15K-237 and WN18RR used in our previous experiment, are subsets of real-world KGs but do not contain sufficient relation types to train and evaluate one-shot learning algorithms <ref type="bibr" target="#b34">[35]</ref>. Therefore, we use the newly proposed NELL-One and Wiki-One datasets in <ref type="bibr" target="#b34">[35]</ref>, both of which are created from real-world KGs (NELL <ref type="bibr" target="#b1">[2]</ref> and Wikidata <ref type="bibr" target="#b69">[70]</ref> respectively) for one-shot relational learning. The datasets are created with a similar process: relations with less than 500 but more than 50 triples are selected as one-shot tasks and randomly divided into training, validation, and testing relations. The dataset statistics are shown in Table <ref type="table" target="#tab_3">4</ref>. Following <ref type="bibr" target="#b34">[35]</ref>, we use Hits@1,5,10 and MRR as evaluation metrics.</p><p>Note that the Wiki-One dataset is an order of magnitude larger than any other benchmark datasets in terms of the number of entities and relations. In practice, we found that the Wiki-One dataset suffers from sparsity and nonconnectivity in the backend KG. In the test set, 15.8% of the entity pairs are not connected at all, and the distances of the other 25.5% pairs are no less than 5. For these 41.3% pairs, we do not have any reasonable paths to infer their relations. To better evaluate the reasoning ability, we remove evaluation facts whose entity pairs' distances are no less than 5 in Wiki-One.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Experimental Setting</head><p>For For evaluation, another two sets of relations R valid and R test are given as the validation and test relations. For each relation r in R valid or R test , we select one triple (e s , r, e o ) as the support fact and use all the other pairs in D r for evaluation. For cross-validation, it is guaranteed that relations in R test will not appear in R train or R valid . Note that in <ref type="bibr" target="#b34">[35]</ref>, for a query (e s , r, e o ), they only rank entities in a filtered candidate set C e s ,r . We follow this setting and filter entities e ä C e s ,r from our prediction.</p><p>We compare with various baselines. For embeddingbased methods, we compare with TransE, DistMult, and ComplEx. More advanced embedding methods such as ConvE failed to scale to Wiki-One, as reported in <ref type="bibr" target="#b34">[35]</ref>. We also include MINERVA as the path-based baseline and GMatching <ref type="bibr" target="#b34">[35]</ref> as the one-shot baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Hyperparameters</head><p>To deploy CogKR on this task, we combine it with the oneshot model GMatching proposed in <ref type="bibr" target="#b34">[35]</ref>. GMatching learns to match the training triple (e s r , r, e o r ) and a candidate triple (e s , r, e o ) with GCN <ref type="bibr" target="#b35">[36]</ref>. Instead, we use the same architecture to map the training triple to a vector representation of r: (8)   which is then passed to CogKR as v r to answer the query (e s , r, ?), as described in Section 4.</p><formula xml:id="formula_13">! r = (W r (! e s r h ! e o r ) + b r ) ! e = (W s v e + b s + W c 1 ∂N e ∂ = (r k ,e k )"N e v r k h v e k ),</formula><p>On NELL-One, the dimensions of embeddings and node representations are set to 100 and 200 respectively. On Wiki-One, the dimensions are set to 50 and 100 due to the extremely large scale of the dataset. On both datasets, we use T = 3 and n = 16.</p><p>We use the ADAM optimization algorithm for model training with learning rates of 1e-3 for parameters of CogKR and 1e-4 for parameters of the entity encoder and the relation layer. We also add L2 regularization with a weight decay of 0.0001. The batch size is 160 on NELL-One and 24 on Wiki-One. The maximum training step is 10000 on NELL-One and 30000 on Wiki-One. We use the MRR on the validation set as the criteria to pick the best model checkpoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Result Analysis</head><p>Table <ref type="table" target="#tab_4">5</ref> reports the one-shot link prediction performance on NELL-One and Wiki-One datasets. On NELL-One, CogKR produces consistent improvements over previous works in all evaluation metrics. The absolute improvement is 7.4% for Hits@1 and 7.8% for MRR. On Wiki-One, our model also achieves improvements on Hits@1 and MRR, but does not show advantages against the embedding-based method GMatching on Hits@5, 10. We can also observe similar patterns in the performance of MINERVA. We speculate that when the information about the target is insufficient, multi-hop reasoning methods, including CogKR and MIN-ERVA, cannot conduct effective reasoning and lose their advantages. Nevertheless, CogKR can always perform better than MINERVA, confirming its improvements over pathbased methods. We also find that the traditional embedding methods do not perform well on Wiki-One, possibly due to the contrast between its extremely large scale and the scarcity of training data in the one-shot setting. Another observation is that the path-based method MINERVA can generally outperform embedding-based methods in the oneshot setting, contrary to its disadvantage on the fullysupervised setting in Section 5.1. We speculate that knowledge graph reasoning methods, including MINERVA and CogKR, are more suitable for one-shot link prediction on KGs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5">Inference Time</head><p>A disadvantage of embedding-based methods is that each candidate entity e for the query (e s , r, ?) needs to be evaluated with a scoring function. This is especially slow for GMatching <ref type="bibr" target="#b34">[35]</ref>, whose scoring function is a complex neural network that compares the candidate triple (e s , r, e) with the training triple (e s r , r, e o r ). This is almost infeasible for large-scale KGs that contain millions of entities.</p><p>To validate CogKR's advantage on time complexity, we compare the inference time of CogKR against DistMult  of graphs is even larger than that of paths. Path-KR is the variation of CogKR with n = 1, which means that the cognitive graph is reduced to a single path. According to the results, this hurts the performance greatly, leading to a 71.8% relative drop in Hits@1. Besides, we also created a variation of CogKR without System 2, which means no latent representations for nodes. As can be seen, removing System 2 has a smaller effect, but still causes a 25.5% relative loss of Hits@1. In summary, different components of CogKR all contribute to the final improvement in the KG reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Multi-hop Analysis</head><p>To evaluate the multi-hop reasoning ability, in Figure <ref type="figure">3a</ref>, we show the Hits@1 of CogKR on WN18RR, categorized by the shortest path lengths from the query entity to the correct answer. The baseline is MultihopKG <ref type="bibr" target="#b17">[18]</ref>, which has shown the best performance among path-based methods on WN18RR. We observe that CogKR outperforms such a strong baseline by 9.9-12.8% for facts that require 2, 3 or 4 reasoning steps. For facts that require only one-step reasoning, both methods can achieve accuracy close to 100%, and the improvement of our method is marginal. For facts with longer distances, both methods can hardly make effective predictions. It indicates the long reasoning paths are highly uncertain and hard to distill useful information. Above all, CogKR can improve the multi-hop reasoning ability for the appropriate length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Convergence Analysis</head><p>To evaluate the convergence speed of CogKR, we plot the model's performance on the validation set of WN18RR at different training points in Figure <ref type="figure">3b</ref>. The baseline is MultihopKG, which uses reinforcement learning with reward reshaping and action dropout to train path-finding agents <ref type="bibr" target="#b17">[18]</ref> and has the official implementation to trace the training process. We show the average of 3 runs and the 95% confidence interval. From Figure <ref type="figure">3b</ref>, compared with the path-finding method, the performance of our model can improve faster while the final performance is also better. This confirms our motivation of using differentiable training to efficiently search over the subgraph space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4">Hyperparameter Sensitivity</head><p>Finally, we analyze the influence of hyperparameters on the performance of CogKR. We mainly focus on three hyperparameters, the maximum step T , the maximum number of edges ⌘, and the action budget n. The performance of CogKR under different choices of T , ⌘, and n on FB15K-237 is shown in Figure <ref type="figure" target="#fig_4">4</ref>. T is the maximum length of reasoning. When T is small, e.g., T = 2, the expressiveness of CogKR is limited to the short reasoning paths, leading to a significant drop of accuracy. However, overlong reasoning paths, e.g., T = 5, are highly uncertain, also leading to a slight drop of accuracy. ⌘ is the maximum number of edges for entities in G. CogKR is not sensitive to the choice of ⌘, achieving high accuracy even when ⌘ = 64. n is the maximum number of selected edges at each step. According to Figure <ref type="figure" target="#fig_4">4</ref>, larger n, which indicates the topological structure of cognitive graphs could be more complex, generally improves the performance. However, when n is too large, the noise is also introduced into the cognitive graph, which may affect the convergence and lead to the improvement constrained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Qualitative Analysis</head><p>In Figure <ref type="figure">5</ref>, we give three examples of graphical explanations built by CogKR. According to Section 4.5.2, each graphical explanation is built by extracting all the paths from the head entity r in the query to the predicted tail entity in the cognitive graph.</p><p>From the examples, we can conclude: 1) the expressiveness of cognitive graphs is more powerful than individual paths. Graphs in three cases contain complex interaction of multiple paths that cannot be captured by path-finding methods. 2) the cognitive graph can provide graphical explanations for the prediction of CogKR. For example, in case (a), to predict the missing genre of the movie Wonder Boys, CogKR looks at the movie About Schmidt that shares two common genres with Wonder Boys.</p><p>In case (c), CogKR predicts that Michael Biehn acted in the movie Terminator 2, because Michael also acted in the movie The Terminator, which is its prequel and shares the same genre with Terminator 2. The three cases are all consistent with human cognition. We also show the statistics of the complete cognitive graphs on the test set in Table <ref type="table" target="#tab_7">8</ref>. We can observe that in a large portion of incorrect predictions by CogKR the correct answers are missing in the cognitive graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>This paper presents CogKR, a novel framework to tackle the multi-hop KG reasoning problem. Under the inspiration of the dual process theory in cognitive science, we organize the reasoning process with a cognitive graph, achieving more powerful reasoning ability than previous path-based methods and end-to-end training following gradient methods. Experimental results on both knowledge graph completion and one-shot link prediction benchmarks demonstrate the superiority of our framework. For further improvements, we find that reasoning-based methods often get stuck to the non-connectivity of KGs. Therefore, we will explore to improve System 1 by allowing the non-connected node expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX PROOF OF THEOREM 1</head><p>Let (x) be a logical formula in . sub( ) = 1 , 2 , ⇧, L is an enumeration of the sub-formulas of such that if k is a sub-formula of l then k &amp; l. Note that the formula x = e s has no sub-formula and must exist in the enumeration. We always represent it with 1 . We will build the System 2 such that component l of entity e's latent representation X[e] gets a value 1 if and only if the sub-formula l is satisfied for e. In this way, the last component of X[e] after the reasoning process will get a value 1 if and only if e satisfies since l = . Therefore our System 2 captures the formula .</p><p>We choose the message and update functions in our System 2 as:</p><formula xml:id="formula_14">M (e k , r k , e) = (AX[e k ] + Bv r k + b)<label>(9)</label></formula><p>U (e, m e ) = (C (m e ) + c)</p><p>where A, B, C " R L✓L and b, c " R L are defined next. is the activation function, which is the truncated ReLU activation, i.e., (x) = min(max(x, 0), 1). v r k is the one-hot encoding of relation type r k . The entries of the l-th rows of A, B, C and b, c are defined according to l as:</p><p>• If l = j 1 0 j 2 0 ⇧ 0 j n , then C lj 1 = C lj 2 = ⇧ = C lj n = 1 and c l = n + 1.</p><p>• If l (x) = øx ¨(r(x ¨, x) 0 j (x ¨)), then B lr k = A lj = 1, b l = 1, and C ll = 1, c l = 0. and all other values in the l-th entries of A, B, C and b, c are 0. We skip the case when l = (x = e s ), which is only possible for l = 1 and x = e s . We initialize the latent representations X[e s ] of e s with the first component as 1 and the other components as 0.</p><p>Next we prove that for every l " sub( ) and every entity e in G, (X[e]) l = 1 if and only if e satisfies . We prove it by induction on the entities in G in topological ordering. The first entity of topological sort can only be e s . Given that (X[e s ]) 1 = 1 and other components of X[e s ] are 0, we know that the property holds for e s . Now we assume that for every entity e k before e in the topological ordering, the property holds. Then we need to prove that for e the property also holds. This is quite straightforward given our definition of A, B, C and b, c. If If l = j 1 0 j 2 0 ⇧, 0 j n , without loss of generality, we assume that each sub-formula is in the form of øx ¨(r(x ¨, x)0 (x ¨)). The l-th component of X[e] is computed as:</p><formula xml:id="formula_16">(X[e]) l = ⇧ n = i=1 ( (m e )) j i n + 1↵<label>(12)</label></formula><p>Since ( (m e )) j i = 1 if and only if e satisfies j i , we have that (X[e]) l = 1 if and only if e satisfies j1 , j2 , ⇧, jn .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 e 1 e 1 r 2 e o and e s r 1 e 2 r 2 e</head><label>2112122</label><figDesc>o that interact at the entity e, and two independent paths e s r o . On the contrary, in the cognitive graph, the information from multiple paths could interact at the "bridge nodes", such as e in the previous case, which improves the expressiveness of CogKR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Overview of CogKR. The core is the cognitive graph, which consists of nodes with latent representations and edges linking them. The expansion module (System 1) selects relevant edges from the neighborhood of current nodes (purple), according to their latent representations. Then, the reasoning module (System 2) updates the latent representations of newly attended nodes (green), via neighborhood aggregation and computes the attention values to measure their importance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>each dataset, we have a set of training relations R train . For each relation r " R train , or R valid , R test , we have the corresponding fact set D r = {(e s , e o )∂(e s , r, e o ) " G}. To generate a training instance, we first sample a relation r from R train and then sample the training triple (e s r , r, e o r ) and the query triple (e s , r, e o ) from D r .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Hyperparameter sensitivity of CogKR on FB15K-237.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>l</head><label></label><figDesc>(x) = øx ¨(r(x ¨, x) 0 j (x ¨)), the l-th component of X[e] is computed as:(M (e k , r k , e)) l = ((X[e k ]) j + I(r k = r) 1)(X[e]) l = ( ( = (e k ,r k )"E e (M (e k , r k , e)) l ))(11)By induction hypothesis we know that (X[e k ]) j = 1 if and only if e k satisfies j . (X[e]) l = (∂{e k ∂(e k , r k ) " E e and r k = r and e k satisfies k }∂). Therefore (X[e]) l = 1 if and only if e satisfies l . We can also observe that ( (m e )) l = 1 if and only if e satisfies l .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1</head><label>1</label><figDesc>Graph-based KG Reasoning AlgorithmRequire: Query (e s , r, ?);A background KG G = (E, R, T ) 1: V ⇥ {e s }, E ⇥ o 2: a 0 ⇥ one hot(e s ), X[e s ] ⇥ 0 t (e k ) ⇥ score(A t (e k ); v e k , X[e k ], v r ) t (e k ) ⇥ a t 1 (e k )Softmax(s t (e k )) t ⇥ &lt; e k "F t 1 p t (e k ), E t ⇥ &lt; e k "F t 1 A t (e k )</figDesc><table><row><cell>7:</cell><cell></cell></row><row><cell>8:</cell><cell>end for</cell></row><row><cell>9:</cell><cell></cell></row><row><cell>10:</cell><cell>if t j T then</cell></row><row><cell>11:</cell><cell>E t ⇥ Top-k(E t , p t , n)</cell></row><row><cell>12:</cell><cell>end if</cell></row><row><cell>13:</cell><cell></cell></row></table><note>3: for t = 1 to T do 4: F t 1 ⇥ {e∂a t 1 (e) &gt; 0} 5: for entity e k in F t 1 do 6: s p p</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>Statistics of datasets in knowledge graph completion.</figDesc><table><row><cell>Dataset</cell><cell cols="2">#Ent. #Rel</cell><cell cols="2">#Train #Valid</cell><cell>#Test</cell></row><row><cell>FB15K-237</cell><cell>14,541</cell><cell>237</cell><cell>272,115</cell><cell cols="2">17,535 20,466</cell></row><row><cell>WN18RR</cell><cell>40,943</cell><cell>11</cell><cell>86,835</cell><cell>3,034</cell><cell>3,134</cell></row><row><cell cols="2">YAGO3-10 123,182</cell><cell cols="2">37 1,079,040</cell><cell>5,000</cell><cell>5,000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>KG reasoning results for FB15K-237 and WN18RR. Results of [π] are taken from<ref type="bibr" target="#b15">[16]</ref>. Results of [∑]are taken from the corresponding papers. Other results are obtained by running the official implementations.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">FB15K-237</cell><cell></cell><cell></cell><cell cols="2">WN18RR</cell></row><row><cell>Model</cell><cell></cell><cell></cell><cell>H@1</cell><cell>H@3</cell><cell>H@10</cell><cell>MRR</cell><cell>H@1</cell><cell>H@3</cell><cell>H@10</cell><cell>MRR</cell></row><row><cell cols="2">DistMult [π]</cell><cell></cell><cell>27.5</cell><cell>41.7</cell><cell>56.8</cell><cell>37.0</cell><cell>41.0</cell><cell>44.1</cell><cell>47.5</cell><cell>43.3</cell></row><row><cell cols="2">ComplEx [π]</cell><cell></cell><cell>30.3</cell><cell>43.4</cell><cell>57.2</cell><cell>39.4</cell><cell>38.2</cell><cell>43.3</cell><cell>48.0</cell><cell>41.5</cell></row><row><cell>R-GCN</cell><cell></cell><cell></cell><cell>19.9</cell><cell>32.0</cell><cell>47.5</cell><cell>29.1</cell><cell>7.6</cell><cell>11.8</cell><cell>18.4</cell><cell>11.1</cell></row><row><cell>ConvE</cell><cell></cell><cell></cell><cell>31.3</cell><cell>45.7</cell><cell>60.0</cell><cell>41.0</cell><cell>40.3</cell><cell>45.2</cell><cell>51.9</cell><cell>43.8</cell></row><row><cell>RotatE</cell><cell></cell><cell></cell><cell>33.1</cell><cell>48.2</cell><cell>63.7</cell><cell>43.4</cell><cell>44.6</cell><cell>52.1</cell><cell>60.3</cell><cell>49.9</cell></row><row><cell>TuckER</cell><cell></cell><cell></cell><cell>33.6</cell><cell>46.7</cell><cell>60.7</cell><cell>42.7</cell><cell>45.5</cell><cell>50.2</cell><cell>53.5</cell><cell>48.5</cell></row><row><cell cols="2">NeuralLP [π]</cell><cell></cell><cell>16.6</cell><cell>24.8</cell><cell>34.8</cell><cell>22.7</cell><cell>37.6</cell><cell>46.8</cell><cell>65.7</cell><cell>46.3</cell></row><row><cell cols="2">MINERVA[∑]</cell><cell></cell><cell>21.7</cell><cell>32.9</cell><cell>45.6</cell><cell>29.3</cell><cell>41.3</cell><cell>45.6</cell><cell>51.3</cell><cell>44.8</cell></row><row><cell cols="2">MultihopKG[∑]</cell><cell></cell><cell>32.9</cell><cell>-</cell><cell>54.4</cell><cell>39.3</cell><cell>43.7</cell><cell>-</cell><cell>54.2</cell><cell>47.2</cell></row><row><cell cols="2">M-Walk[∑]</cell><cell></cell><cell>16.5</cell><cell>24.3</cell><cell>-</cell><cell>23.2</cell><cell>41.4</cell><cell>44.5</cell><cell>-</cell><cell>43.7</cell></row><row><cell cols="2">DIVINE[∑]</cell><cell></cell><cell>22.3</cell><cell>33.1</cell><cell>-</cell><cell>29.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CogKR</cell><cell></cell><cell></cell><cell>36.9</cell><cell>49.2</cell><cell>61.4</cell><cell>44.9</cell><cell>48.4</cell><cell>54.3</cell><cell>60.6</cell><cell>52.3</cell></row><row><cell></cell><cell cols="2">TABLE 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">KG reasoning results for YAGO3-10. Results of ∂ are taken from [14].</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Other results are obtained by running the official implementation.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="4">H@1 H@3 H@10 MRR</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DistMult[∂]</cell><cell>24</cell><cell>38</cell><cell>54</cell><cell>34</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ComplEx[∂]</cell><cell>26</cell><cell>40</cell><cell>55</cell><cell>36</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ConvE[∂]</cell><cell>35</cell><cell>49</cell><cell>62</cell><cell>44</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RotatE[∂]</cell><cell>40.2</cell><cell>55.0</cell><cell>67.0</cell><cell>49.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TuckER</cell><cell>41.6</cell><cell>55.3</cell><cell>67.1</cell><cell>50.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">MultihopKG 40.5</cell><cell>53.7</cell><cell>63.4</cell><cell>48.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CogKR</cell><cell>47.7</cell><cell>57.3</cell><cell>64.4</cell><cell>53.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4</head><label>4</label><figDesc>Statistics of datasets in one-shot link prediction.</figDesc><table><row><cell>Dataset</cell><cell cols="2">#Ent. #Rel</cell><cell cols="2"># Triples # Tasks</cell></row><row><cell>NELL-One</cell><cell>68,545</cell><cell>358</cell><cell>181,109</cell><cell>67</cell></row><row><cell>Wiki-One</cell><cell>4,838,244</cell><cell cols="2">822 5,859,240</cell><cell>183</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 5</head><label>5</label><figDesc>One-shot KG reasoning results for NELL and Wikidata.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">NELL-One</cell><cell></cell><cell></cell><cell cols="2">Wiki-One</cell></row><row><cell>Model</cell><cell></cell><cell>H@1</cell><cell></cell><cell>H@5</cell><cell>H@10</cell><cell>MRR</cell><cell>H@1</cell><cell>H@5</cell><cell>H@10</cell><cell>MRR</cell></row><row><cell>TransE</cell><cell></cell><cell>4.4</cell><cell></cell><cell>14.9</cell><cell>29.6</cell><cell>11.1</cell><cell>2.5</cell><cell>4.3</cell><cell>5.2</cell><cell>3.5</cell></row><row><cell>ComplEx</cell><cell></cell><cell>9.4</cell><cell></cell><cell>19.4</cell><cell>23.9</cell><cell>14.1</cell><cell>4.0</cell><cell>9.2</cell><cell>12.1</cell><cell>6.9</cell></row><row><cell>DistMult</cell><cell></cell><cell>12.3</cell><cell></cell><cell>23.1</cell><cell>26.9</cell><cell>16.3</cell><cell>1.9</cell><cell>7.0</cell><cell>10.1</cell><cell>4.8</cell></row><row><cell cols="2">MINERVA</cell><cell>16.0</cell><cell></cell><cell>26.1</cell><cell>29.0</cell><cell>20.6</cell><cell>21.4</cell><cell>25.6</cell><cell>26.6</cell><cell>23.3</cell></row><row><cell cols="2">GMatching</cell><cell>13.3</cell><cell></cell><cell>22.6</cell><cell>29.6</cell><cell>18.3</cell><cell>17.0</cell><cell>27.3</cell><cell>33.5</cell><cell>22.6</cell></row><row><cell>CogKR</cell><cell></cell><cell>23.4</cell><cell></cell><cell>34.1</cell><cell>36.9</cell><cell>28.4</cell><cell>23.4</cell><cell>26.5</cell><cell>27.0</cell><cell>24.7</cell></row><row><cell></cell><cell cols="4">(a) Multi-hop Analysis</cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) Convergence Analysis</cell></row><row><cell cols="3">Fig. 3. Performance analysis of CogKR.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">TABLE 6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Inference time of different methods on Wiki-One test set (sec /</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">samples).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>#Candidates CogKR</cell><cell></cell><cell cols="4">GMatching DistMult</cell><cell></cell><cell></cell><cell></cell></row><row><cell>5,000 3.0 ✓ 10</cell><cell>3</cell><cell>4.6 ✓ 10</cell><cell>2</cell><cell>1.6 ✓ 10</cell><cell>3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>4,838,243 3.0 ✓ 10</cell><cell>3</cell><cell>4.2 ✓ 10</cell><cell></cell><cell>5.1 ✓ 10</cell><cell>1</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">as the traditional baseline and GMatching as the neural</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">baseline. All the models are implemented in PyTorch and</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">evaluated on a single RTX 2080 GPU. The results with both</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">the truncated candidate set (no more than 5000) and the</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">full entity set (4,838,244) are reported in Table 6. As we</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">can see, when the candidate number is limited to 5000, the</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">running time of GMatching and DistMult is comparable to</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">that of CogKR. DistMult is even faster than CogKR due to</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">its simple scoring function. However, with the full entity set</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">as candidates, the running time of GMatching and DistMult</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">increases proportional to the number of candidates, whereas</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">the running time of CogKR remains the same. This shows</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">the efficiency of CogKR when the entity number of KGs</cell><cell></cell><cell></cell><cell></cell></row><row><cell>becomes very large.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">5.3 Quantitative Analysis</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5.3.1 Ablation Analysis</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">We conduct the ablation study to analyze the contributions</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">of different components in CogKR. The results are shown in</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>PG-KR has the same architecture with CogKR but uses the stochastic policy to select edges and policy gradient for training. We observed that basically the policy gradient does not work for CogKR, possibly because the search space</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 7</head><label>7</label><figDesc>Ablation study on FB15k-237 benchmark.</figDesc><table><row><cell>Model</cell><cell>H@1</cell><cell>H@3</cell><cell>H@10</cell><cell>MRR</cell></row><row><cell>CogKR</cell><cell>36.9</cell><cell>49.2</cell><cell>61.4</cell><cell>44.9</cell></row><row><cell>PG-KR</cell><cell>3.3</cell><cell>5.7</cell><cell>9.2</cell><cell>5.3</cell></row><row><cell>Path-KR</cell><cell>10.4</cell><cell>13.6</cell><cell>16.1</cell><cell>12.3</cell></row><row><cell>System 2</cell><cell>27.5</cell><cell>40.3</cell><cell>53.6</cell><cell>35.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 8</head><label>8</label><figDesc>Statistics of the cognitive graphs on the test set. Avg. Size indicates the average number of nodes in the cognitive graphs. Avg. Recall indicates the fraction of cognitive graphs that contain the correct answers.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Avg. Size Avg. Recall</cell></row><row><cell>FB15K-237</cell><cell>88.3</cell><cell>67.6</cell></row><row><cell>YAGO3-10</cell><cell>76.6</cell><cell>68.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Fig. 5. Case Study: different forms of graphical explanations in the experiments. Red rectangles denote query entities, green eclipses denote final answers, and purple capsules denote intermediate nodes. White eclipses and dashed lines denote irrelevant entities and edges.</figDesc><table><row><cell cols="3">Q: (Wonder Boys, genre, ?)</cell><cell cols="3">Q: (Academy Award for Best Sound Mixing, nominate, ?)</cell><cell cols="3">Q: (Michael Biehn, act in, ?)</cell></row><row><cell>Japan</cell><cell></cell><cell>English</cell><cell>Last Samurai (Film)</cell><cell></cell><cell>thriller</cell><cell>Brian Austin Green</cell><cell>profession</cell><cell>actor</cell></row><row><cell>country</cell><cell>language</cell><cell></cell><cell>nominate</cell><cell></cell><cell>genre</cell><cell>friend</cell><cell>profession</cell><cell></cell></row><row><cell>Wonder Boys (Film)</cell><cell>genre</cell><cell>comedy of manners</cell><cell>Academy Award for Best Sound Mixing</cell><cell>award</cell><cell>Terminator 2 (Film)</cell><cell>Michael Biehn</cell><cell>act in</cell><cell>The Rock (Film)</cell></row><row><cell>genre</cell><cell></cell><cell>genre_inv</cell><cell>award</cell><cell></cell><cell>nominated for</cell><cell>act in</cell><cell></cell><cell>genre</cell></row><row><cell>comedy</cell><cell>genre_ inv</cell><cell>About Schmidt (Film)</cell><cell>Aliens (Film)</cell><cell>nominated for</cell><cell>BAFTA Award for Best Sound</cell><cell>The Terminator (Film)</cell><cell>genre</cell><cell>thriller</cell></row><row><cell></cell><cell>written by</cell><cell>genre</cell><cell>nominated for</cell><cell></cell><cell>award_inv</cell><cell>director</cell><cell>prequel_inv</cell><cell>genre_ inv</cell></row><row><cell>Alexander</cell><cell></cell><cell>comedy-</cell><cell>Academy Award for</cell><cell>nominated</cell><cell>Aliens 2</cell><cell>James</cell><cell></cell><cell>Terminator 2</cell></row><row><cell>Payne</cell><cell></cell><cell>drama</cell><cell>Best Sound Editing</cell><cell>for</cell><cell>(Film)</cell><cell>Cameron</cell><cell></cell><cell>(Film)</cell></row><row><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell>(c)</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">. The code can be downloaded from https://github.com/THUDM/ CogKR.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The work is supported by the NSFC for Distinguished Young Scholar (61825602), and a research fund supported by Alibaba.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neverending learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R H</forename><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Platanios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Samadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wijaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saparov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Greaves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2302" to="2310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Label-free distant supervision for relation extraction via knowledge graph embedding</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2246" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Strong baselines for simple question answering over knowledge graphs with and without neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="291" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Augmenting end-to-end dialogue systems with commonsense knowledge</title>
		<author>
			<persName><forename type="first">T</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4970" to="4977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Explainable reasoning over knowledge graphs for recommendation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5329" to="5336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knowledge graph completion via complex tensor factorization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">É</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">38</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deeppath: A reinforcement learning method for knowledge graph reasoning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="564" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Variational knowledge graph reasoning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1823" to="1832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1811" to="1818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Random walk inference and learning in A large scale knowledge base</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2011</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="529" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">M-walk: Learning to walk over graphs using monte carlo tree search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6786" to="6797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-hop knowledge graph reasoning with reward shaping</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">V</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3243" to="3253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inductive logic programming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Muggleton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Generation Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="295" to="318" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Statistical predicate invention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Chains of reasoning over entities, relations, and text using recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Belanger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="132" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Compositional vector space models for knowledge base inference</title>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposia</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learn to explain efficiently via neural logic inductive learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dual-processing accounts of reasoning, judgment, and social cognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of psychology</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="255" to="278" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The empirical case for two systems of reasoning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Sloman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="3" to="22" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Heuristic and analytic processes in reasoning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="451" to="468" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">In two minds: dual-process accounts of reasoning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="454" to="459" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Working memory</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baddeley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">255</biblScope>
			<biblScope unit="issue">5044</biblScope>
			<biblScope unit="page" from="556" to="559" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cognitive graph for multi-hop reading comprehension at scale</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2019</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2694" to="2703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hypernetwork knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">I</forename><surname>Balazevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICANN 2019</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="553" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Tucker: Tensor factorization for knowledge graph completion</title>
		<idno type="arXiv">arXiv:1901.09590</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Traversing knowledge graphs in vector space</title>
		<author>
			<persName><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="318" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">One-shot relational learning for knowledge graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1980" to="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Few-shot knowledge graph completion</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1911">1911.11298, 2019</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Meta relational learning for few-shot link prediction in knowledge graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP 2019</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4216" to="4225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An open-world extension to knowledge graph completion models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Villmow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ulges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schwanecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3044" to="3051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Open-world knowledge graph completion</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weninger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1957" to="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Representation learning of knowledge graphs with entity descriptions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2659" to="2665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Programs with common sense</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semantic Information Processing</title>
				<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1968">1968</date>
			<biblScope unit="page" from="403" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Modeling large-scale structured relationships with shared memory for knowledge base completion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rep4NLP@ACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="57" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Differentiable learning of logical rules for knowledge base reasoning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2316" to="2325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">DIVINE: A generative adversarial imitation learning framework for knowledge graph reasoning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Generative adversarial imitation learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4565" to="4573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graph neural networks for ranking web pages</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maggini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE / WIC / ACM International Conference on Web Intelligence (WI 2005)</title>
				<meeting><address><addrLine>Compiegne, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09">September 2005. 2005</date>
			<biblScope unit="page" from="666" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng. Bull</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="52" to="74" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Revisiting semisupervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Gcc: Graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Jiezhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qibin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yuxiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hongxia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kuansan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="5171" to="5181" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Representation learning for attributed multiplex heterogeneous network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1358" to="1368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Den Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESWC</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Compositionbased multi-relational graph convolutional networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nitin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A vectorized relational graph convolutional network for multi-relational network alignment</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4135" to="4141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Inductive relation prediction on knowledge graphs</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Teru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1911">1911.06962, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Evaluating logical generalization in graph neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sodhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003.06560. 2020</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Dynamically pruned message passing networks for large-scale knowledge graph reasoning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Modeling attention flow on graphs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<idno>abs/1811.00497</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">The logical expressiveness of graph neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Barcel Ó</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Kostylev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Monet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Reutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Silva</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">¸</forename><surname>Ülc ¸ehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Representing text for joint embedding of text and knowledge bases</title>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1499" to="1509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">YAGO3: A knowledge base from multilingual wikipedias</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mahdisoltani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Biega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIDR 2015</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vrandecic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kr Ötzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
