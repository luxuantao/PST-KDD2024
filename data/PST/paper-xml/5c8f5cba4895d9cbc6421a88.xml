<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning in Physical Layer Communications</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhijin</forename><surname>Qin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Geoffrey</forename><forename type="middle">Ye</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Biing-Hwang</forename><forename type="middle">Fred</forename><surname>Juang</surname></persName>
						</author>
						<title level="a" type="main">Deep Learning in Physical Layer Communications</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received pilot signals Received pilot signals Channel Received pilot signals Received data</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep learning</term>
					<term>end-to-end communications</term>
					<term>physical layer communications</term>
					<term>signal processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning (DL) has shown great potentials to revolutionizing communication systems. This article provides an overview on the recent advancements in DL-based physical layer communications. DL can improve the performance of each individual block in communication systems or optimize the whole transmitter/receiver. Therefore, we categorize the applications of DL in physical layer communications into systems with and without block structures. For the DL-based communication systems with the block structure, we demonstrate the power of DL in signal compression and signal detection. We also discuss the recent endeavors in developing DL-based end-to-end communication systems. Finally, the potential research directions are identified to boost the intelligent physical layer communications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The idea of using neural networks (NN) to intelligentize machines can be traced to 1942 when a simple model was proposed to simulate the status of a single neuron. Deep learning (DL) adopts a deep neural network (DNN) to find data representation at each layer, which could be built by using different types of machine learning (ML) techniques, including supervised ML, unsupervised ML, and reinforcement learning. In recent years, DL has shown its overwhelming privilege in many areas, such as computer vision, robotics, and natural language processing, due to its advanced algorithms and tools in learning complicated models.</p><p>Zhijin Qin is with Queen Mary University of London, London E1 4NS, U.K., (email: z.qin@qmul.ac.uk).</p><p>Hao Ye, Geoffrey Ye Li, and Biing-Hwang Fred Juang are with Georgia Institute of Technology, Atlanta, GA 30332 USA, (email: yehao@gatech.edu; liye@ece.gatech.edu, juang@ece.gatech.edu).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:1807.11713v3 [cs.IT] 19 Feb 2019</head><p>Different from the aforementioned DL applications, where it is normally difficult to find a concrete mathematical model for feature representation, various theories and models, from information theory to channel modelling, have been well developed to describe communication systems <ref type="bibr" target="#b0">[1]</ref>. However, the gap between theory and practice motivates us to work on intelligent communications. Particularly, the following challenges have been identified in the existing physical layer communications:</p><p>• Mathematical model versus practical imperfection: The conventional communication systems rely on the mathematically expressed models for each block. While in the real-world applications, complex systems may contain unknown effects that are difficult to be expressed analytically. For example, it is hard to model underwater acoustic channels or molecular communications. Therefore, a more adaptive framework is required to handle the challenges.</p><p>• Block structures versus global optimality: The traditional communication systems consist of several processing blocks, such as channel encoding, modulation, and signal detection, which are designed and optimized within each block locally. Thus the global optimality cannot be guaranteed. Moreover, the optimal communication system structure varies with environments. As a result, optimal or robust communication systems for different scenarios are more than desired.</p><p>DL could be a pure data-driven method, where the networks/systems are optimized over a large training data set and a mathematically tractable model is unnecessary. Such a feature motivates us to exploit DL in communication systems in order to address the aforementioned challenges. In this situation, communication systems can be optimized for specific hardware configuration and channel to address the imperfection issues. On the other hand, many models in physical layer communications have been established by researchers and engineers during the past several decades. Those models can be combined with DL to design model-driven DL-based communication systems, which can take advantages of both model-based algorithms and DL <ref type="bibr" target="#b1">[2]</ref>.</p><p>There is evidence that the "learned" algorithms could be executed faster with lower power consumption than the existing manually "programmed" counterparts as NNs can be highly parallelized on the concurrent architectures and implemented with low-precision data types. Moreover, the passion on developing As shown in Fig. <ref type="figure" target="#fig_1">1</ref> (a), the inputs of the NN are {x 1 , x 2 , . . . , x n } with the corresponding weights, {w 1 , w 2 , . . . , w n }. The neuron can be represented by a non-linear activation function, σ (•), that takes the sum of the weighted inputs. The output of the neuron can be expressed as y = σ ( n i=1 w i x i + b), where b is the shift of the neuron. An NN can be established by connecting multiple neuron elements to generate multiple outputs to construct a layered architecture. In the training process, the labelled data, i.e., a set of input and output vector pairs, is used to adjust the weight set, W, by minimizing a loss function. In the NN with single neuron element, W = {b, w 1 , w 2 , . . . , w n }. The commonly-used loss functions include mean-squared error (MSE) and categorical cross-entropy. To train the model for a specific scenario, the loss function can be revised by introducing the l 1 -or l 2 -norm of W or activations. l 1or l 2 -norm of W can also introduced in the loss function as the regularizer to improve the generalization capabilities. Stochastic gradient descent (SGD) is one of the most popular algorithms to optimize W.</p><p>With the layered architecture, a DNN includes multiple fully connected hidden layers, in which each of them represents a different feature of the input data. Fig. <ref type="figure" target="#fig_1">1 (b) and (c</ref>) show two typical DNN models: feedforward neural network (FNN) and recurrent neural network (RNN). In FNNs, each neuron is connected to the adjacent layers while the neurons in the same layers are not connected to each other.</p><p>The deep convolutional network (DCN) is developed from the fully connected FNN by only keeping some of the connections between neurons and their adjacent layers. As a result, DCN can significantly reduce the number of parameters to be trained <ref type="bibr" target="#b2">[3]</ref>. Recently, DL has boosted many applications due to the powerful algorithms and tools. DCN has shown its great potential for signal compression and recovery problems, which will be demonstrated in Section III-A.</p><p>For the RNN in Fig. <ref type="figure" target="#fig_1">1 (c</ref>), the outputs of each layer are determined by both the current inputs and their hidden states in the previous time step. The critical difference between FNN and RNN is that the latter has memory and can capture the hidden layer outputs in the previous step. However, as RNN is dependent on time over a long term, non-stationary errors may show up during the training process. A special type of RNN, named long short-term memory (LSTM), has been further proposed to eliminate some unnecessary information in the network. LSTM has been widely applied in various cases, such as the joint deign of source-channel coding, which will be briefly discussed in Section III-A.</p><p>2) Generative Adversarial Net (GAN) and Conditional GAN: Training a typical DNN is heavily dependent on the large amount of labelled data, which may be difficult to obtain or even unavailable in certain circumstance. As shown in Fig. <ref type="figure" target="#fig_2">2</ref>, GAN is a type of generative method, which can produce data that follows certain target distribution. By doing so, demands for the amount of labelled data can be lowered. In Fig. <ref type="figure" target="#fig_2">2</ref>, a GAN consists of a generator, G, and a discriminator, D. D attempts to differentiate between the real data and the fake data generated by G while G tries to generate plausible data to fool D into making mistakes, which introduces min-max two player game between G and D. As result of the min-max two player game, the generator, G, will generate data with the same distribution as the real data and therefore, the discriminator, D, cannot identify the difference between the real and fake data.</p><p>Conditional GAN is an extension of GAN by providing extra conditioning information, m, where the conditioning information has been fed to both G and D as the additional input.</p><p>In communication systems, a GAN and a conditional GAN can be applied to model the distribution of the channel output. Moreover, the learned model can be utilized as a surrogate of the real channel when training the transmitter so that the gradients can pass through to the transmitter. An application example of conditional GAN will be introduced in Section IV-B. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Bayesian Optimal Estimator</head><p>Besides the standard DL models, the expert knowledge can be beneficial in modifying the structures of DL models to provide more explainable and predictable models in physical layer communications. In fact, many signal processing modules in communication systems, such as the multi-input multi-output (MIMO) detection and channel decoding, can be cast as posterior probability inference problems in probabilistic graphical models, where the dependence of observation variables (e.g. the received signals)</p><p>and the latent variables (e.g. the transmitted signals) are expressed explicitly. The posterior distribution of the latent variables can be calculated via Bayesian inference and then the Bayesian optimal estimators are obtained by minimizing the expected MSE with given posterior distributions.</p><p>Even though the exact computation of the Bayesian optimal estimators are computationally intractable in many problems, some iterative approaches, such as the approximate message passing (AMP) and expectation-propagation, can approximate the performance of Bayesian optimal efficiently. Moreover, these iterative detectors can be further improved by unfolding and representing the iterative procedures with DL models, where the parameters of the model can be updated based on the training data. A detailed example will be shown in Section III-B. The model-driven methods exploit some prior knowledge of system to reduce the number of parameters to be learned. While the data-driven methods assume a general system structure that usually has lots of unknown parameters to be trained by a huge data set. Each of them has its advantages and disadvantages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Deep Learning based Communications</head><p>In general, it involves the trade-off between variance and bias in the learning theory. With prior knowledge, the sample complexity for learning models can be largely reduced, but the models may suffer when the prior knowledge is not accurate in the real scenario. On the contrary, the data-driven model is with less presumption. The sample complexity is large but it can be more robust under variant circumstances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DEEP LEARNING BASED BLOCK-STRUCTURED COMMUNICATIONS</head><p>Even though the existing block-structured communication systems have been carefully designed from their infancy to the fifth generation (5G), more efforts are still required to break the bottleneck in wireless communication systems. In this section, we focus on the applications of DL in different communication blocks, which are categorized into intelligent signal compression and detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Intelligent Signal Compression</head><p>Most types of source data exhibit unique inner structures that can be utilized for compression. Such structured data can be modelled by different approaches. Sparse representation is a commonly-used one.</p><p>It is worth noting that the most important property of DL is that it can automatically find compact lowdimensional represeations/featues of high dimensional data <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, which can be demonstrated by the following two examples.  CSI is then recovered at the base station. However, traditional CS algorithms face challenges as realworld data is not exactly sparse and the convergence speed of the existing signal recovery algorithms is relatively slow, which has limited the practical applications of CS <ref type="bibr" target="#b6">[7]</ref>.</p><p>DCN has been applied to learn the inverse transformation from measurement vectors to signals to improve the recovery speed in CS <ref type="bibr" target="#b6">[7]</ref>. Particularly, DCN has two distinctive features that make it uniquely applicable to sparse recovery problems. One is that the neurons are sparsely connected. The other is with shared weights across the entire receptive fields of one layer. DCN can increase the learning speed comparing to a fully-connected network <ref type="bibr" target="#b7">[8]</ref>. Learned denoising-based AMP (LDAMP) <ref type="bibr" target="#b8">[9]</ref> is one of the excellent signal recovery algorithms in terms of both accuracy and speed, which has been applied to channel estimation in millimeter-wave (mmWave) communications <ref type="bibr" target="#b1">[2]</ref>. However, the achieved improvement still cannot boost the CS-based CSI estimation.</p><p>CsiNet <ref type="bibr" target="#b9">[10]</ref> has been proposed to mimic the CS processes for channel compression, feedback, and recovery speed.</p><p>2) Data-Driven Joint Source-Channel Coding: The typical source coding mainly utilizes the sparse property to remove the redundancy in source data while channel coding improves the robustness to noise by adding redundancy to the coded information when it is transmitted over channels. Shannon separation theorem guarantees that source coding and channel coding can be designed separately without loss of optimality. However, in many communication systems, source coding and channel coding are designed jointly as it is not practical to have very large blocks.</p><p>A joint source-channel coding based on DL has been proposed in <ref type="bibr" target="#b10">[11]</ref>. With text as the source data, the DL-based source-channel encoder and decoder may output different sentences but preserving their semantic information content. Specifically, the proposed model adopts a RNN encoder, a binarization layer, a channel layer, and a RNN decoder. The text is structured before it is processed by the stacked bidirectional LSTM networks. Then the binarizer is adopted to output binary values, which are taken as the inputs of the channel. At the receiver, a stack of LSTM is used for decoding. By doing so, the word-error rate is lowered compared with various traditional separate source-channel coding baselines, such as using huffman and Reed-Solomon code for source and channel coding, respectively. Even though this design is particularly for text processing, it inspires us to apply DL to where recovery of the exact transmitted data is not compulsory as long as the main information within it is conveyed. For example, in sparse support detection, we need to determine if there is a sparse support at each location while the exact amplitude of each location is not of interest.</p><p>In addition to the aforementioned two examples, DL has also been widely applied in other signal compression applications. For example, instead of performing joint source-channel coding, DL can be applied to source coding and channel coding, separately, to achieve better performance compared to typical coding techniques. Moreover, DNN has also been widely applied to facilitate the design of measurement matrix and signal recovery algorithm in CS <ref type="bibr" target="#b6">[7]</ref>, which can be used in various wireless scenarios, i.e., channel estimation and wideband spectrum sensing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Intelligent Signal Detection</head><p>The DL-based detection algorithms can significantly improve the performance of communication systems, especially when the joint optimization of the traditional processing blocks is required and when the channels are hard to be characterized by analytical models. Here, we provide two examples for DL-based detection.</p><p>1) Data-Driven Joint Channel Estimation and Signal Detection: Traditionally, channel estimation and signal detection are two separate procedures at the receiver. The CSI is first estimated by means of pilots prior to the detection of the transmit symbols. Then with the estimated CSI, the transmit symbols can be recovered at the receiver. A joint channel estimation and signal detection approach has been proposed in <ref type="bibr" target="#b11">[12]</ref>. Specifically, a five-layer fully connected DNN is embedded into an orthogonal frequency-division multiplexing (OFDM) receiver for joint channel estimation and detection by treating the channel as a 'black box'. The DNN is trained to reconstruct the transmit data by feeding the received signals corresponding to the transmit data and pilots as inputs. Therefore, the channel information can be inferred implicitly by the DNN and used to predict the transmit data directly without explicitly estimating the CSI. Fig. <ref type="figure" target="#fig_6">5</ref> demonstrates that the DNN-based channel estimation and detection method outperforms the minimum MSE-based approach when without adequate pilots or cyclic prefix, and with nonlinear distortion <ref type="foot" target="#foot_0">1</ref> . The advantage is that when these adversarial effects occur, the data-driven method can learn to deal with these effects in a supervised manner, i.e., updating the parameters to minimize the empirical loss, which improves the robustness to the undesired circumstances.</p><p>2) Model-Driven MIMO Detection: In MIMO detection, iterative methods, which are based on Bayesian optimal detectors, have shown superior performance with moderate computation complexity. However, these detectors often impose assumptions on the channel distribution, which limits the performance under many complicated environments. By incorporating learning based approaches, the adaptability of the detectors can be improved since the parameters of the model can be refined according to the specific data. In <ref type="bibr" target="#b1">[2]</ref>, the iterative procedures are unfolded to a signal flow graph. Only several critical variables are required to tune the graph in the supervised learning manner. This trainable framework has been combined</p><p>with the orthogonal AMP detector, where only two variables are set as the trainable parameters in each iteration. Since the number of trainable parameters are comparable to that of iterations, it can be easily trained within a shorter period and with less training data comparing to a regular DNN while improving the performance of the orthogonal AMP detector in Rayleigh and correlated MIMO channels. Therefore, this approach can be scaled to massive MIMO communications with great potentials to be applied to time-varying channels.</p><p>Apart from the wireless signal compression and detection, DL has been exploited for various tasks in physical layer communications. Compared with the traditional methods, it has shown higher robustness to channels. For example, DNN has been utilized in the channel decoding and is more robust to variations of the additive white Gaussian noise (AWGN) channel model <ref type="bibr" target="#b12">[13]</ref>. In addition, DL can improve the system performance by exploiting the additional contextual information. For example, in mmWave systems, DL can be used for beam prediction, where some contextual information, such as the locations of the receiver and the surrounding vehicles in vehicular networks, can be taken into consideration to improve the prediction. Moreover, DL has shown its privilege on the molecular signal detection when the channel models are optimized based on training data instead of any prior channel information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DEEP LEARNING BASED END-TO-END COMMUNICATIONS</head><p>In the previous section, we have discussed the applications of DL in each individual block of communication systems. In this section, we will present innovative learning-based communication systems by treating the entire communication system as an end-to-end reconstruction task <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Particularly, based on the data-driven methods, the transmitter learns to encode the source data into encoded symbols (or transmit signals) to be transmitted over the channel while the receiver learns to recover the source data from the received signals. The weights of the model are optimized in a supervised learning manner based on an end-to-end loss on the recovery accuracy. By doing so, the block structure in the conventional communication systems is no longer required. Moreover, the end-to-end method has great potentials to provide a universal solution for different channels.</p><p>As aforementioned, the weights of the DNN are usually learned based on the SGD with the gradients of the loss function back-propagated from the output layer to the input layer. Nevertheless, when the channel parameters are unknown in advance, the gradients cannot back-propagate through the unknown channel since the gradients for updating the transmitter is blocked by the unknown channel, which forestalls the learning of the end-to-end networks. The channel transfer function may be pre-assumed to solve the issue, but any such assumption would bias the learned models, repeating the pitfalls resulted from the likely discrepancy between the assumed channel models and the actual channels. In addition, in real communication systems, an accurate channel transfer function is difficult to obtain in advance since the end-to-end channel often embraces different types of random effects, such as channel noise and timevarying, which may be unknown or cannot be expressed analytically. As shown as in Fig. <ref type="figure">6</ref>, we will introduce two methods to address the issue in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Reinforcement Learning Based End-to-End Systems</head><p>In <ref type="bibr" target="#b14">[15]</ref>, a reinforcement learning based approach has been proposed to circumvent the problem of missing gradients from channels when optimizing the transmitter. As shown in Fig. <ref type="figure">6</ref> (a), the transmitter, converting the source data into the transmit symbols, is considered as an agent while both the channel and the receiver are regarded as the environment. The agent will learn to take actions to maximize the cumulative rewards emitted from the environment. At each time, the transmit data is regarded as the state observed by the transmitter and the transmit signals are regarded as the action taken by the transmitter.</p><p>The end-to-end loss on each sample will be calculated at the receiver and fed back to the transmitter as the reward from the environment, which guilds the training of the transmitter. By using the policy gradient algorithm, a standard reinforcement learning approach, the transmitter can learn to maximize the reward, i.e., optimize the end-to-end loss, without requiring the gradients from the channel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Conditional GAN Based End-to-End Systems</head><p>In order to solve the missing gradient problem and lower the demands for the large amount of training data, a generative approach based on conditional GAN has been proposed in <ref type="bibr" target="#b3">[4]</ref>. As in Fig. <ref type="figure">6</ref> (b), the end-Fig. <ref type="figure">6</ref>: End-to-end communication system models.</p><p>to-end learning of a communication system is enabled without requiring prior information of the channel by modelling the conditional distribution of the channel. In Fig. <ref type="figure">6</ref> (b), the end-to-end pipeline consists of DNNs for the transmitter, the channel generator, and the receiver, which are trained iteratively. Since the conditional GAN learns to mimic the channel effects, it acts as a surrogate channel for the gradients to pass through, which enables the training of the transmitter. The conditioning information for the conditional GAN is the transmit signals from the transmitter along with the received pilot information used for estimating the channel. Therefore, the generated output distribution will be specific to the instantaneous channel and the transmit signals. As a result, the conditional GAN based end-to-end communication system can be applied to more realistic time-varying channels. The simulation results in <ref type="bibr" target="#b3">[4]</ref> confirm the effectiveness of the conditional GAN based end-to-end communication system, by showing similar performance as the Hamming (7,4) code with maximum-likelihood decoding (MLD).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS AND FUTURE DIRECTIONS</head><p>We has demonstrated great potentials of DL in physical layer communications in the above. By summarizing how to apply DL in communication systems, the following research directions have been identified to bring the intelligent physical layer communications from theory to practice.</p><p>A. Can DL-based End-to-End Communications Beat the Traditional?</p><p>We have briefly introduced end-to-end communications in Section IV. From the initial research results in <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b14">[15]</ref>, the performance of DL-based end-to-end communications is comparable with the traditional ones. However, it is not clear whether the DL-based end-to-end communications eventually outperform the traditional ones in terms of performance and complexity or how much gain can be achieved. We are expecting the answers to these questions soon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Tradeoff between System Performance and Training Efficiency</head><p>The existing work has shown the power of data-driven models in physical layer communications. Even though a universal transmitter/receiver can be optimized in the end-to-end learning-based communication design, the training process takes very long as all the communication blocks are merged. In order to improve the training efficiency and achieve good system performance, part of the communication blocks can be kept and model-driven DL methods can be considered. Then we need to carefully design the system to achieve a good tradeoff between the training efficiency and system performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Communication Metric Learning</head><p>In traditional communication systems, the objective is the error-free reconstruction of the transmit data. While in real applications, the objective of sharing the information may vary from task to task and the reconstruction metrics may not be satisfactory for all tasks. For instance, bit-error rate is not a good metric for images and videos transmission since it cannot reflect the properties of human visual perception. In the end-to-end communication systems, the metric should be revised to address specific requirements for each application. The basic idea is that the transmit data will not be treated as equally important, the recovered data may contain transmission error, but the semantic information contained in the data, which is further employed for the application-specific tasks at the receiver, should remain intact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Open Access Real-World Data Sets</head><p>The bloom of various applications of learning techniques should be largely credited to the accessible real-world data sets, such as ImageNet for computer vision. These open access data sets provide an efficient way to compare the performance of different learning algorithms. However, such a type of accessible data sets for wireless communications are still under developed. The data protection and privacy regulations further limit the open access of real-world communication data. However, it is still essential to publish some data sets, i.e., channel responses under different channel conditions, to speed up the development of DL-based physical layer communications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>artificial intelligence-powered devices from manufacturers, such as Intel c Movidius TM Neural Compute Stick, has also boosted the boom of DL-based wireless communications. This article will identify the gains that DL can bring to wireless physical layer communications, including the systems with the block structure and the end-to-end structure merging those blocks. The rest of this article is organized as follows. Section II introduces the important basis of DNN and illustrates DLbased communication systems. Section III discusses how to apply DL to block-structured communication systems. Section IV demonstrates DL-based end-to-end communication systems, where individual block for a specific function, such as channel estimation or decoding, disappears. Section V concludes this article with potential research directions in the area of DL-based physical layer communications. II. DEEP NEURAL NETWORKS AND DEEP LEARNING BASED COMMUNICATIONS In this section, we will first introduce the basis of DNN, generative adversarial network (GAN), conditional GAN, and Bayesian optimal estimator, which are widely used in DL-based communication systems. Then we will discuss the intelligent communication systems with DL. A. Deep Neural Networks 1) Deep Neural Networks Basis: As aforementioned, research on NN started from the single neuron.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Development of neural networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Structure of conditional GAN [4].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3 shows the intelligent communication system structure with DL. Compared to the conventional block-based communication structure, DL-based communication systems utilize the power of DL to facilitate transmission. A data-driven DL-based communication system is usually represented by a DNN and a large amount of labelled data is used to tune the parameters of the DNN. Such a DNN can be regarded as a 'black box' and used in each processing block individually to replace the existing algorithms,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Intelligent communication system structure.</figDesc><graphic url="image-28.png" coords="7,180.00,69.11,251.99,102.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: DL-based channel compression, feedback, and recovery by CsiNet [10].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Performance comparison of DL and minimum MSE-based joint channel estimation and signal detection in OFDM systems [12].</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The data set and simulation codes can be downloaded from https://github.com/haoyye/OFDM_DNN.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An introduction to deep learning for the physical layer</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">O</forename><surname>Shea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoydis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Cogn. Commun. Netw</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="563" to="575" />
			<date type="published" when="2017-12">Dec. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Model-driven deep learning for physical layer communications</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.06059</idno>
		<imprint>
			<date type="published" when="2018-09">Sept. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks [lecture notes]</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Mag</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="2018-11">Nov. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Channel agnostic end-to-end learning based communication systems with conditional GAN</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-H</forename><forename type="middle">F</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sivanesan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Global Commun. Conf. (GLOBECOM&apos;18)</title>
				<meeting>IEEE Global Commun. Conf. (GLOBECOM&apos;18)<address><addrLine>Abu Dhabi, UAE</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12">Dec. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning: A brief survey</title>
		<author>
			<persName><forename type="first">K</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Bharath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="26" to="38" />
			<date type="published" when="2017-11">Nov. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sparse representation for wireless communications: A compressive sensing approach</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="40" to="58" />
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to invert: Signal recovery via deep convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intl. Conf. Acoustics Speech Signal Process. (ICASSP&apos;17)</title>
				<meeting>IEEE Intl. Conf. Acoustics Speech Signal ess. (ICASSP&apos;17)<address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-03">Mar. 2017</date>
			<biblScope unit="page" from="2272" to="2276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Theoretical foundations of deep learning via sparse representations: A multilayer sparse model and its connection to convolutional neural networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Papyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sulam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="72" to="89" />
			<date type="published" when="2018-07">Jul. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learned D-AMP: principled neural network based compressive image recovery</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06625</idno>
		<imprint>
			<date type="published" when="2017-11">Nov. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arxiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning for massive MIMO CSI feedback</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Wireless Commun. Lett</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="748" to="751" />
			<date type="published" when="2018-10">Oct. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep learning for joint source-channel coding of text</title>
		<author>
			<persName><forename type="first">N</forename><surname>Farsad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goldsmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intl. Conf. Acoustics Speech Signal Process. (ICASSP&apos;18)</title>
				<meeting>IEEE Intl. Conf. Acoustics Speech Signal ess. (ICASSP&apos;18)<address><addrLine>Calgary, AB</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04">Apr. 2018</date>
			<biblScope unit="page" from="2326" to="2330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Power of deep learning for channel estimation and signal detection in OFDM systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-H</forename><forename type="middle">F</forename><surname>Juang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Wireless Commun. Lett</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="114" to="117" />
			<date type="published" when="2018-02">Feb. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Communication algorithms via deep learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Viswanath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09317</idno>
		<imprint>
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep learning based communication over the air</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dörner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cammerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoydis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Brink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Signal Process</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="132" to="143" />
			<date type="published" when="2018-02">Feb. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">End-to-end learning of communications systems without a channel model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Aoudia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoydis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02276</idno>
		<imprint>
			<date type="published" when="2018-12">Dec. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
