<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-End Change Detection for High Resolution Satellite Images Using Improved UNet++</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-06-10">10 June 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daifeng</forename><surname>Peng</surname></persName>
							<email>daifeng@nuist.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Remote Sensing and Geomatics Engineering</orgName>
								<orgName type="department" key="dep2">Science and Technology</orgName>
								<orgName type="institution">Nanjing University of Information</orgName>
								<address>
									<postCode>210044</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yongjun</forename><surname>Zhang</surname></persName>
							<email>zhangyj@whu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Remote Sensing and Information Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haiyan</forename><surname>Guan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Remote Sensing and Geomatics Engineering</orgName>
								<orgName type="department" key="dep2">Science and Technology</orgName>
								<orgName type="institution">Nanjing University of Information</orgName>
								<address>
									<postCode>210044</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-End Change Detection for High Resolution Satellite Images Using Improved UNet++</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-06-10">10 June 2019</date>
						</imprint>
					</monogr>
					<idno type="MD5">1152E1380ADF66634FD75184F5579A16</idno>
					<idno type="DOI">10.3390/rs11111382</idno>
					<note type="submission">Received: 10 May 2019; Accepted: 8 June 2019;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>change detection</term>
					<term>deep learning</term>
					<term>end-to-end</term>
					<term>encoder-decoder architecture</term>
					<term>feature maps</term>
					<term>multiple side-outputs fusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Change detection (CD) is essential to the accurate understanding of land surface changes using available Earth observation data. Due to the great advantages in deep feature representation and nonlinear problem modeling, deep learning is becoming increasingly popular to solve CD tasks in remote-sensing community. However, most existing deep learning-based CD methods are implemented by either generating difference images using deep features or learning change relations between pixel patches, which leads to error accumulation problems since many intermediate processing steps are needed to obtain final change maps. To address the above-mentioned issues, a novel end-to-end CD method is proposed based on an effective encoder-decoder architecture for semantic segmentation named UNet++, where change maps could be learned from scratch using available annotated datasets. Firstly, co-registered image pairs are concatenated as an input for the improved UNet++ network, where both global and fine-grained information can be utilized to generate feature maps with high spatial accuracy. Then, the fusion strategy of multiple side outputs is adopted to combine change maps from different semantic levels, thereby generating a final change map with high accuracy. The effectiveness and reliability of our proposed CD method are verified on very-high-resolution (VHR) satellite image datasets. Extensive experimental results have shown that our proposed approach outperforms the other state-of-the-art CD methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With ever-increasing Earth observation data available from all kinds of satellite sensors, such as DeepGlobal, WorldView, QuickBird, ZY-3, GF1, GF2, Sentinel, and Landsat, it is easy to obtain multi-temporal remote sensing (RS) data using the same or different sensors. Based on multi-temporal RS images acquired at the same geographical areas, the task of change detection (CD) is the process of identifying differences in the state of an object or natural phenomena by observing it at different times <ref type="bibr" target="#b0">[1]</ref>, which is a significant issue to accurately process and understand the changes of Earth surface. Generally, CD has been widely applied in numerous fields, such as land cover and land use mapping, natural resource investigation, urban expansion monitoring, environmental assessment, and rapid response to disaster events <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>.</p><p>In the past decades, a large number of CD methods have been developed. Based on the analysis unit, traditional CD methods can be divided into two categories: pixel-based CD (PBCD) and object-based CD (OBCD) <ref type="bibr" target="#b5">[6]</ref>. In the former case, a difference image (DI) is usually generated by directly comparing pixel spectral or textual values, from which the final change map (CM) is obtained by threshold segmentation or cluster analysis. Numerous PBCD approaches have been proposed to exploit the spectral and textual features of pixels, such as image algebra-based methods <ref type="bibr" target="#b6">[7]</ref>, image transformation-based methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, image classification-based methods <ref type="bibr" target="#b9">[10]</ref>, and machine learning-based methods <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. However, contextual information neglected for pixels are treated independently, which leads to a great deal of "salt and pepper" noise. To overcome the drawbacks, spatial-contextual information has to be considered for delineating the spatial properties. To model spatial-contextual information, many methods have been introduced, such as simple neighboring windows <ref type="bibr" target="#b7">[8]</ref>, the Markov random field <ref type="bibr" target="#b13">[14]</ref>, conditional random fields <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, hypergraph models <ref type="bibr" target="#b16">[17]</ref>, and level sets <ref type="bibr" target="#b17">[18]</ref>. However, PBCD methods, which are mostly suitable for middle-and low-resolution RS images, often fail to work in very-high-resolution (VHR) images for the increased variability within image objects <ref type="bibr" target="#b18">[19]</ref>. OBCD methods are proposed for CD in VHR images particularly, where images are segmented into disjoint and homogeneous objects first, followed by comparison and analysis of bi-temporal objects. As abundant spectral, textual, structural, and geometric information can be extracted within image objects, similarity analysis of the bi-temporal objects using those features are mostly studied in OBCD <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. The post-classification comparison strategy is also utilized in OBCD for certain CD tasks, especially when "from-to" change information has to be determined <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Recently, deep learning (DL) methods have achieved dominant advantages over traditional methods in the areas of image analysis, natural language processing, and 3D scene understanding. Due to their great success, the interest of the RS community towards DL methods is growing fast, for the benefits of human-like reasoning and robust features which embody the semantics of input images <ref type="bibr" target="#b23">[24]</ref>. In the literature, a large amount of attempts have been made to solve CD problems using DL techniques. Basically, DL-based CD methods can be divided into three categories: <ref type="bibr" target="#b0">(1)</ref>  • FB-DLCD Hand-crafted features are usually designed for particular tasks, which need a great deal of expert domain knowledge and possess poor generality, while deep features are learned hierarchically from available datasets, which are more abstract and robust <ref type="bibr" target="#b24">[25]</ref>. In particular, deep features generated from the pre-trained convolutional neural network (CNN) models on natural images have been proven effective to the transferring to RS images, such as VGGNet and ResNet. Therefore, a large number of studies have been made to introduce deep features from pre-trained CNN for CD <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>. In order to exploit the statistical structure and relations of image regions at different resolutions, Amin et al. <ref type="bibr" target="#b28">[29]</ref> utilized Zoom out CNNs features from VGG-16 for optical image CD.</p><p>In some cases, more discriminative features can be learned by using designed DL models with available datasets, which is more beneficial for CD. Zhang et al. <ref type="bibr" target="#b29">[30]</ref> utilized the deep belief network (DBN) to learn abstract and invariant features directly from raw images, and then two-dimensional (2-D) polar domain and clustering methods were adopted to generate a CD map. Nevertheless, DBN, unlike CNN, has weak feature learning abilities. Thus, Siamese CNN architectures with weighted contrastive loss <ref type="bibr" target="#b30">[31]</ref> and improved triplet loss <ref type="bibr" target="#b31">[32]</ref> were exploited to learn discriminative deep features between changed and unchanged pixels, then DIs were generated based on the Euclidean distances of deep features, and finally CM could be obtained by a simple threshold.</p><p>Because transfer learning fails to work between heterogeneous images, it is impossible to obtain deep feature representation for multi-modality images using pre-trained CNN models. Hence, to extract deep features, some studies present complex DL models, such as the conditional generative adversarial network (cGAN) <ref type="bibr" target="#b32">[33]</ref> and iterative feature mapping network (IFMN) <ref type="bibr" target="#b33">[34]</ref>.</p><p>• PB-DLCD Rather than using DIs directly for obtaining CD results in FB-DLCD, pixel patches (or superpixels) are constructed from raw images or DI, which are fed into an elaborate DL model to learn the change relation of the center pixels (or superpixels). Before training, DIs are usually generated using traditional methods for obtaining proper training samples and labels, namely pseudo-training sets. In <ref type="bibr" target="#b34">[35]</ref> and <ref type="bibr" target="#b35">[36]</ref>, shallow neural networks, such as DBN and sparse denoising autoencoder (SDAE), are employed to learn semantic differences between the bi-temporal patches or superpixels. In <ref type="bibr" target="#b36">[37]</ref>, a refined DI is obtained based on the generative adversarial network (GAN), where the joint distribution of the image patches and training data are fed into the network for training. Siamese CNN architectures are widely utilized in the PB-DLCD for its effective feature fusion and representation abilities <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>. In addition, such PB-DLCD strategy was also adopted for multi-modality images or incomplete images, such as heterogeneous Synthetic Aperture Radar (SAR) images <ref type="bibr" target="#b39">[40]</ref>, laser scanning point clouds and 2-D imagery <ref type="bibr" target="#b40">[41]</ref>, and incomplete satellite images <ref type="bibr" target="#b41">[42]</ref>.</p><p>In order to overcome the effect of DI, some attempts using an end-to-end manner have been made to solve CD tasks. Gong et al. <ref type="bibr" target="#b42">[43]</ref> proposed a novel CD method for SAR images, where a CM is generated by using the trained Restricted Boltzmann Machine (RBM). Rodrigo et al. <ref type="bibr" target="#b43">[44]</ref> firstly proposed two end-to-end PB-DLCD architectures, where the CNNs could be trained from scratch using only the provided datasets. Wang et al. <ref type="bibr" target="#b44">[45]</ref> performed hyperspectral image CD by a general end-to-end 2-D CNN framework, where 2-D mixed-affinity matrices are generated and pixel change types are obtained by the CNN output. In <ref type="bibr" target="#b45">[46]</ref> and <ref type="bibr" target="#b46">[47]</ref>, dual-dense convolution network (DCN) and Spectral-Spatial Joint Learning Network (SSJLN) were proposed to implement PB-DLCD, respectively. It is noteworthy that recurrent neural networks (RNNs) were firstly exploited to model temporal change relations in <ref type="bibr" target="#b47">[48]</ref>. Mou et al. <ref type="bibr" target="#b48">[49]</ref> further proposed an end-to-end CD network by combining the CNN with the RNN, where a joint spectral-spatial-temporal feature representation is learned. However, a large number of training samples are needed to train an end-to-end CNN. To overcome the drawbacks, Gong et al. <ref type="bibr" target="#b49">[50]</ref> detected multispectral image changes by a generative discriminatory classified network (GDCN), where labeled data, unlabeled data, and new fake data generated by the GAN are used. The generator recovers the real data from input noises to provide additional training samples, which could boost the performance of the discriminatory classified network.</p><p>• IB-DLCD In the field of semantic segmentation, a fully convolutional network (FCN) is widely used due to its high efficiency and accuracy. Segmentation results are generated from images to images through end-to-end training, which reduces the effect of pixel patches as much as possible. In the literature, some attempts have been made to include an FCN. UNet-based FCN architectures were employed successfully <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref> in IB-DLCD, which were trained in an end-to-end manner from scratch using only available CD datasets. It is noteworthy that two fully convolutional Siamese architectures with skip connections were firstly proposed in <ref type="bibr" target="#b52">[53]</ref>. Lebedev et al. <ref type="bibr" target="#b54">[54]</ref> detected changes in high-resolution satellite images by an end-to-end CD method based on GANs. However, the network is sensitive to small changes, and the GAN is very time-consuming and difficult to train. Moreover, other FCN-based end-to-end CD architectures were also proposed for natural images <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b56">56]</ref> and hyperspectral images <ref type="bibr" target="#b57">[57]</ref>.</p><p>To sum up, PB-DLCD generally consists of three steps: (1) deep feature representation, which uses either pre-trained CNN models or elaborately designed DL models; (2) DI generation by using the Euclidean distance; (3) CD result retrieval by threshold segmentation or cluster analysis. However, the errors will inevitably be propagated from the early stages to later steps. In addition to that, the number of changed pixels must be proportional to that of the unchanged pixels for threshold segmentation of DI. Unfortunately, that assumption does not scale well to complex and large datasets, where images may contain no changed or unchanged pixels. Although the effect of DI could be reduced in PB-DLCD, the following limitations still exist: (1) a proper size, which has great influence on the performance of CNNs, is difficult to define for the pixel patches; (2) pixel patches are randomly split into training and testing sets, which easily leads to overfitting since neighboring pixels carry redundant information. Nevertheless, the existing IB-DLCD methods still need to be improved to capture complex and small changes. For the benefit of retrieval, a summary of the above-mentioned methods is presented in Table <ref type="table" target="#tab_0">1</ref>. To address the above-mentioned issues, we proposed a novel end-to-end method based on improved UNet++ <ref type="bibr" target="#b58">[58]</ref>, which is an effective encoder-decoder architecture for semantic segmentation. A novel loss function was designed and an effective deep supervision (DS) strategy was implemented, which are capable of capturing changes with varying sizes effectively in complex scenes. The main contributions of our article are three-fold:</p><p>(1) To the best of our knowledge, a comprehensive summary of DLCD techniques are firstly presented, which is useful for grasping the development process and tendency of DLCD. (2) An end-to-end CNN architecture was proposed for CD of VHR satellite images, where an improved UNet++ model with novel DS is presented so as to capture subtle changes in challenging scenes. (3) A comprehensive comparison of the existing FCN-based end-to-end CD methods was investigated.</p><p>The reminder of this article is organized as follows. Section 2 illustrates the related work of semantic segmentation using FCNs. The proposed CD method is described in detail in Section 3. In Section 4, the effectiveness of our proposed method is investigated and compared with some state-of-the-art (SOTA) IB-DLCD methods using real RS datasets. Discussion is presented in Section 5. Finally, Section 6 draws the concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>In classification tasks, the last few layers in standard CNN architectures are always several fully connected (FC) layers with a 1-D distribution over all classes as the output. However, a 2-D dense class prediction map is needed for semantic segmentation tasks. Based on a standard CNN, a patch-based CNN approach <ref type="bibr" target="#b59">[59]</ref> was proposed, where the label of each pixel is generated by the patch enclosing it. Nevertheless, the approach leads to great deficiency on both speed and accuracy for the correlations among patches ignored and many redundant computations on overlapped regions are introduced. Long et al. <ref type="bibr" target="#b60">[60]</ref> first proposed FCN for semantic segmentation, where FC layers are removed and replaced by convolution layers. In an FCN, input images are down-sampled into small images after several convolution and pooling operations, then the down-sampled images are up-sampled into the original size by bilinear interpolation or deconvolution. Since computations are shared across overlapping areas, FCN achieves great efficiency. In order to make finer predictions, some methods, such as atrous convolution <ref type="bibr" target="#b61">[61]</ref>, residual connections <ref type="bibr" target="#b62">[62]</ref>, and pyramid pooling modules <ref type="bibr" target="#b63">[63]</ref>, utilize intermediate layers to enhance the output feature maps, which contribute to expanding the receptive field and overcoming the vanishing gradient problems.</p><p>It is noteworthy that an encoder-decoder architecture becomes increasingly popular in semantic segmentation due to its high flexibility and superiority. The illustration of such a CNN architecture is shown in Figure <ref type="figure" target="#fig_1">1</ref>. An input image goes through the encoder part first to generate down-sampled feature maps, which consists of several convolution (Conv) layers and max-pooling layers in sequence. To obtain better convergence of deep networks, each Conv layer is followed by Batch Normalization (BN) and Rectified Linear Unit (ReLU) layers. Then, the decoder part is implemented for up-sampling the feature maps to the same size as the original image, where up-sampling layers are followed by several Conv layers to produce dense features with finer resolution. Finally, a softmax layer is added to generate a final segmentation map. The structures of the encoder and decoder parts are symmetrical with skip connections between them, which proves to be effective to produce fine-grained segmentation results.</p><p>decoder parts are symmetrical with skip connections between them, which proves to be effective to produce fine-grained segmentation results.</p><p>The mostly used encoder-decoder example is SegNet <ref type="bibr" target="#b64">[64]</ref>, where unpooling operation is included for better up-sampling. However, skip connections are ignored, leading to poor spatial accuracy. UNet, an extension of SegNet by adding skip connections between the encoder and decoder layers, has better spatial accuracy and achieves great success in semantic segmentation on both medical images <ref type="bibr" target="#b65">[65]</ref> and RS images <ref type="bibr" target="#b66">[66]</ref>. Recently, Zhou et al. <ref type="bibr" target="#b58">[58]</ref> proposed a novel medical image segmentation architecture named UNet++, which can be considered as an extension of UNet. To reduce the semantic gap between the feature maps from the encoder and decoder sub-networks, UNet++ uses a series of nested and dense skip pathways, rather than only connections between encoder and decoder networks. The UNet++ architecture possesses the advantages of capturing finegrained details, thereby generating better segmentation results than UNet. Therefore, it is promising to exploit the potential of UNet++ for semantic segmentation on RS images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, the proposed network architecture is illustrated in detail first. Then, the loss function formulation is discussed. Finally, we present detailed information on the training and prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Proposed Network Architecture</head><p>Because CD can be treated as a problem of binary image segmentation, it is natural to introduce advanced semantic segmentation architectures to solve CD tasks. To perform CD on VHR images, we, inspired by UNet++, developed an end-to-end architecture. The flowchart of the proposed method is illustrated in Figure <ref type="figure" target="#fig_4">2</ref>. Two periods of images are concatenated as the input of the network, which proves to be effective for learning bi-temporal changes through deep supervised training <ref type="bibr" target="#b52">[53]</ref>. Then, the UNet++ model with dense skip connections is adopted as the backbone to learn multiscale and different semantic levels of visual features representations. To further improve the spatial details, DS is implemented by using multiple side-output fusion (MSOF). Finally, a sigmoid layer is followed to generate the final CM. The mostly used encoder-decoder example is SegNet <ref type="bibr" target="#b64">[64]</ref>, where unpooling operation is included for better up-sampling. However, skip connections are ignored, leading to poor spatial accuracy. UNet, an extension of SegNet by adding skip connections between the encoder and decoder layers, has better spatial accuracy and achieves great success in semantic segmentation on both medical images <ref type="bibr" target="#b65">[65]</ref> and RS images <ref type="bibr" target="#b66">[66]</ref>. Recently, Zhou et al. <ref type="bibr" target="#b58">[58]</ref> proposed a novel medical image segmentation architecture named UNet++, which can be considered as an extension of UNet. To reduce the semantic gap between the feature maps from the encoder and decoder sub-networks, UNet++ uses a series of nested and dense skip pathways, rather than only connections between encoder and decoder networks. The UNet++ architecture possesses the advantages of capturing fine-grained details, thereby generating better segmentation results than UNet. Therefore, it is promising to exploit the potential of UNet++ for semantic segmentation on RS images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, the proposed network architecture is illustrated in detail first. Then, the loss function formulation is discussed. Finally, we present detailed information on the training and prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Proposed Network Architecture</head><p>Because CD can be treated as a problem of binary image segmentation, it is natural to introduce advanced semantic segmentation architectures to solve CD tasks. To perform CD on VHR images, we, inspired by UNet++, developed an end-to-end architecture. The flowchart of the proposed method is illustrated in Figure <ref type="figure" target="#fig_4">2</ref>. Two periods of images are concatenated as the input of the network, which proves to be effective for learning bi-temporal changes through deep supervised training <ref type="bibr" target="#b52">[53]</ref>. Then, the UNet++ model with dense skip connections is adopted as the backbone to learn multiscale and different semantic levels of visual features representations. To further improve the spatial details, DS is implemented by using multiple side-output fusion (MSOF). Finally, a sigmoid layer is followed to generate the final CM.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Backbone of UNet++</head><p>UNet++ with nested dense skip pathways has great benefits for extracting multi-scale feature maps from multi-level convolution pathways, which is similar to a UNet architecture, a normal UNet++ architecture consisting of convolution units, down-sampling and up-sampling modules, and skip connections between convolution units, as shown in Figure <ref type="figure" target="#fig_5">2(a)</ref>. The most significant difference between UNet++ and UNet is the re-designed skip pathways, which adopts the same dense </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Backbone of UNet++</head><p>UNet++ with nested dense skip pathways has great benefits for extracting multi-scale feature maps from multi-level convolution pathways, which is similar to a UNet architecture, a normal UNet++ architecture consisting of convolution units, down-sampling and up-sampling modules, and skip connections between convolution units, as shown in Figure <ref type="figure" target="#fig_5">2a</ref>. The most significant difference between UNet++ and UNet is the re-designed skip pathways, which adopts the same dense connection strategy as DenseNet <ref type="bibr" target="#b67">[67]</ref>. Take node X 0,4 as an example, where only one skip connection is applied from node X 0,0 in the UNet architecture, while in UNet++, node X 0,4 receives the skip connections from all previous convolution units at the same level, namely X 0,0 , X 0,1 , X 0,2 and X 0,3 . In such a way, the semantic levels of the encoder feature maps are closer to those in the corresponding decoder part, which facilitates the optimization of the optimizer. Assume x i,j represents the output of node X i,j , where i denotes the i th down-sampling layer along the encoder way, j denotes the j th convolution layer along the skip pathway. The accumulation of feature maps by x i,j can be expressed as:</p><formula xml:id="formula_0">x i,j =          H x i-1, j , j = 0 H [[x i,k ] j-1 k=0 , x i+1,j-1 ] j &gt; 0 ,<label>(1)</label></formula><p>where H (.) represents a convolution operation followed by an activation function, (.) is an up-sampling layer, and [.] denotes the concatenation operation. In general, nodes at level j of =0 receive only one input from a previous down-sampling layer, while nodes at level j of &gt;0 receive j + 1 inputs from both the skip pathways and the up-sampling layer. For example, x 1,0 = H x 0,0 denotes x 1,0 receives the input from node x 0,0 , and then a convolution operation and activation function are applied.</p><p>x 0,4 = H x 0,0 , x 0,1 , x 0,2 , x 0,3 , x 1,3 denotes nodes x 0,0 , x 0,1 , x 0,2 , x 0,3 , the up-sampling results of node x 1,3 are concatenated first, and then a convolution operation and an activation function are adopted to generate node x 0,4 . It is noteworthy that residual modules are adopted in our convolution unit, which facilitates better convergence abilities for our deep networks. As seen in Figure <ref type="figure" target="#fig_5">2b</ref>, a 2-D convolution layer (Conv2D) is implemented first, which is followed by a BN layer. Then, a further Conv2D and BN layer is applied. Finally, the output will be generated by adding the outputs from the second BN layer and the first Conv2D layer. It should be noted that scaled exponential linear units (SeLUs) is adopted as the activation function instead of ReLU, which allows for employing stronger regularization schemes and making learning highly robust <ref type="bibr" target="#b68">[68]</ref>. Another major difference is the multi-level full-resolution feature maps-generating strategy.</p><p>Only a single-level feature map is generated in the UNet architecture through the pathway X 0,0 → X 1,0 → X 2,0 → X 3,0 → X 4,0 → X 3,1 → X 2,2 → X 1,3 → X 0,4 , as illustrated in Figure <ref type="figure" target="#fig_5">2a</ref>.</p><p>While in UNet++, another three full-resolution feature maps are also obtained, through the pathways X 0,0 → X 1,0 → X 0,1 }, X 0,0 → X 1,0 → X 2,0 → X 1,1 → X 0,2 }, and</p><formula xml:id="formula_1">X 0,0 → X 1,0 → X 2,0 → X 3,0 → X 2,1 → X 1,</formula><p>2 → X 0,3 }, respectively. Thus, the strengths of the four full-resolution feature maps could be combined, which is also beneficial for later DS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Deep Supervision by Multiple Side-Outputs Fusion</head><p>DS is usually implemented by means of supervising side-output layers through auxiliary classifiers <ref type="bibr" target="#b69">[69]</ref>. On the one hand, DS improves the convergence of deep networks by overcoming the vanishing gradient problems. On the other hand, more meaningful features ranging from low to high levels could be learned. In <ref type="bibr" target="#b57">[57]</ref>, DS is implemented by averaging the outputs from all segmentation branches, which fails to work for our CD task. Instead, an MSOF strategy is utilized, which is similar to the one proposed in <ref type="bibr" target="#b70">[70]</ref>.</p><p>As shown in Figure <ref type="figure" target="#fig_5">2a</ref>, for the four output nodes X 0,1 , X 0,2 , X 0,3 , X 0,4 , a sigmoid layer is followed to obtain side output results Y 0,1 , Y 0,2 , Y 0,3 , Y 0,4 . Then, a new output node Y 0,5 could be generated by concatenating the four side output results:</p><formula xml:id="formula_2">X 0,5 = Y 0,1 ⊕ Y 0,2 ⊕ Y 0,3 ⊕ Y 0,4 ,<label>(2)</label></formula><p>where ⊕ denotes the concatenation operation. Again, X 0,5 is followed by a sigmoid layer, and the fusion output Y 0,5 could thus be generated. Therefore, five outputs are generated in our deep networks, namely Y 0,1 , Y 0,2 , Y 0,3 , Y 0,4 , Y 0,5 , where Y 0,5 is the fusion output of Y 0,1 , Y 0,2 , Y 0,3 , Y 0,4 . Through the MSOF operation, multi-level features information from all side-output layers are embedded in the final output Y 0,5 , which is capable of capturing finer spatial details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Loss Function Formulation</head><p>In our proposed FCN architecture, five output layers are generated after the classifiers of sigmoid layers. Suppose the corresponding weights are denoted as ω i (i = 1, 2, 3, 4, 5). Then, the overall loss function L could be defined as:</p><formula xml:id="formula_3">L = 5 i=1 ω i L i side ,<label>(3)</label></formula><p>where L i side (i = 1, 2, 3, 4, 5) denotes the loss from the i th side output, which is employed by combining balanced binary cross-entropy and dice coefficient loss:</p><formula xml:id="formula_4">L i side = L i bce + λL i dice ,<label>(4)</label></formula><p>where L bce denotes the balanced binary cross-entropy loss, L dice is the dice coefficient loss, and λ refers to the weight that balances the two losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Balanced Binary Cross-Entropy Loss</head><p>For CD tasks of satellite images, the distributions of changed/unchanged pixels are heavily biased. In particular, some areas are covered by only changed or unchanged pixels, which leads to serious class imbalance problems during deep neural network training. Hence, trade-off parameters have to be introduced for biased sampling. In our end-to-end training manner, the loss function is computed over all pixels in a training image pair X = x j , j = 1, 2, . . . , |X| and CM Y = y j , j = 1, 2, . . . , |Y| , y j ∈ {0, 1}. A simple automatically balancing strategy is adopted, and the class-balanced cross-entropy loss function can be defined as:</p><formula xml:id="formula_5">L bce = -β j Y + logPr y j = 1 -(1 -β) j Y - logPr y j = 0 ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_6">β = |Y -|/(|Y + | + |Y -|) and 1 -β = |Y + |/(|Y + | + |Y -|), |Y + | and |Y -|</formula><p>represent the numbers of changed and unchanged pixels in the ground truth label images, respectively, and Pr(.) is the sigmoid output at pixel j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Dice Coefficient Loss</head><p>To improve segmentation performance and weaken the effect of class imbalance problems, dice coefficient loss is usually applied in semantic segmentation tasks. In general, the similarity of two contour regions can be defined by dice coefficient. Moreover, the dice coefficient loss could be defined as:</p><formula xml:id="formula_7">L dice = 1 - 2Y Ŷ Y + Ŷ ,<label>(6)</label></formula><p>where Y and Ŷ denote the predicted probabilities and the ground truth labels of a training image pair, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training and Prediction</head><p>The proposed method is implemented by Keras with TensorFlow as the backend, which is powered by a workstation with Intel Xeon CPU W-2123 (3.6 GHz, 8 cores, and 32GB RAM) and a single NVIDIA GTX 1080 Ti GPU. During the training process, Adam optimizer with a learning rate of 1 × 10 -4 is applied. Based on the GPU memory, the batch size is set to 8 for 15 epochs, and the learning rate drops after every 5 epochs. As our proposed architecture is an FCN-based model, it is easy to train the model in an end-to-end manner for an arbitrary size of input images. After training, test images could be fed into the trained model to generate the prediction result. For a image patch with size of 256 × 256, it takes about 0.0447 s to predict the final CD map, which is of high efficiency.</p><p>To contribute to the geoscience community, the implementation of our proposed method will be released through GitHub (https://github.com/daifeng2016/End-to-end-CD-for-VHR-satellite-image).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>In this section, several experiments are carried out to verify the effectiveness of our proposed method. First, we present a description of the VHR image dataset, which is provided in <ref type="bibr" target="#b54">[54]</ref>. Evaluation metrics are also provided in detail for quantitative analysis of our method. Second, SOTA methods are presented for comparisons. Then, a sensitive analysis of the parameters and the experimental setups is described in detail. Finally, we give a comprehensive analysis of the experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaludation Metrics</head><p>It has to be mentioned that CD tasks have long been hindered for the lack of open datasets, which are crucial for fair and effective comparisons of different algorithms. As a large amount of labeled data are needed for training deep neural networks, it is impossible to use only a small size of co-registered image pairs, which are adopted in most traditional CD tasks. Fortunately, a publicly available dataset of satellite image pairs are presented by Lebedev <ref type="bibr" target="#b54">[54]</ref>. The datasets were obtained by Google Earth, covering season-varying RS images of the same region. There are 11 multi-spectral image pairs in the dataset, including 7 pairs of season-varying images with a size of 4725 × 2200 pixels for creating manual reference maps and 4 pairs of images with a size of 1900 × 1000 pixels for adding additional objects manually. It is noteworthy that the dataset consists of multi-source remotely sensed imagery with resolutions varying from 3 cm to 100 cm per pixel, where the season changes between bi-temporal images vary largely. During the generation of reference maps, only the appearance and disappearance of objects were considered as image changes while ignoring changes due to season differences, brightness, and other factors. On the one hand, the dataset is very challenging for CD using traditional methods; on the other hand, different object changes (such as cars, buildings, and tanks) could be well considered, as illustrated in Figure <ref type="figure" target="#fig_7">3</ref>. As it is impossible to train CNNs with large images due to the limitation of GPU, image patches have to be generated. We utilize the image patches generated in <ref type="bibr" target="#b54">[54]</ref>, which consist of 10,000 training sets and 3000 testing and validation sets, created by cropping a size of 256 × 256 randomly rotated fragments with at least a part of the target object.</p><p>In order to verify the validity of our proposed method, four evaluation metrics are applied based on the comparisons between the prediction CMs and the ground truth maps, namely Precision (P), Recall (R), F1-score (F 1 ), and Overall Accuracy (OA). In the CD task, a large value of P denotes a small number of false alarms, and a large value of R represents a small number of missed detections. Meanwhile, F1 and OA reveal the overall performance, where their larger values will lead to better performance. These four evaluation metrics are descried as:</p><formula xml:id="formula_8">P = TP TP + FP ,<label>(7)</label></formula><formula xml:id="formula_9">R = TP TP + FN ,<label>(8)</label></formula><formula xml:id="formula_10">F 1 = 2PR P + R ,<label>(9)</label></formula><formula xml:id="formula_11">OA = TP + TN TP + TN + FP + FN , (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>where TP, FP, TN, and FN denote the number of true positives, the number of false positives, the number of true negatives, and the number of false negatives, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison Methods</head><p>To verify the superiority and effectiveness of our proposed CD method, some SOTA IB-DLCD approaches are compared, which are described as follows:</p><p>1) Change detection network (CDNet) <ref type="bibr" target="#b56">[56]</ref> was proposed for pixel-wise CD in street view scenes, which consists of contraction blocks and expansion blocks, and the final CM is generated by a soft-max layer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison Methods</head><p>To verify the superiority and effectiveness of our proposed CD method, some SOTA IB-DLCD approaches are compared, which are described as follows:</p><p>(1) Change detection network (CDNet) <ref type="bibr" target="#b56">[56]</ref> was proposed for pixel-wise CD in street view scenes, which consists of contraction blocks and expansion blocks, and the final CM is generated by a soft-max layer. (2) Fully convolutional-early fusion (FC-EF) <ref type="bibr" target="#b52">[53]</ref> was proposed for CD of satellite images. Image pairs are stacked as the input images. Skip connections are utilized to complement the less localized information with spatial details, thereby producing CMs with precise boundaries. (3) Fully convolutional Siamese-concatenation (FC-Siam-conc) <ref type="bibr" target="#b52">[53]</ref> is a Siamese extension of FC-EF model. The encoding layers are separated into two streams of equal structure with shared weights. Then, the skip connections are concatenated in the decoder part. (4) Fully convolutional Siamese-difference (FC-Siam-diff) <ref type="bibr" target="#b52">[53]</ref> is another Siamese extension of FC-EF model. The skip connections from the encoding streams are concatenated by using the absolute value of their difference. ( <ref type="formula" target="#formula_5">5</ref>) FC-EF with residual blocks (FC-EF-Res) <ref type="bibr" target="#b50">[51]</ref> is employed for CD in high-resolution satellite images. It is an extension of FC-EF architecture, where residual blocks with skip connections are used to improve the spatial accuracy of CM. ( <ref type="formula" target="#formula_7">6</ref>) Fully convolutional network with pyramid pooling (FCN-PP) <ref type="bibr" target="#b51">[52]</ref> is applied for landslide inventory mapping. A U-shape architecture is used to construct the FCN. Additionally, pyramid pooling is utilized to capture wider receptive field and overcome the drawbacks of global pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Parameter Setting</head><p>The size of convolution kernel is set to 3 × 3 pixels for all convolutional layers, which can maintain spatial information and increase the computation speed effectively. The number of convolutional filters in the encoder part is set to {32, 64, 128, 256, 512}. As two periods of RGB images with a size of 256 × 256 pixels are stacked to feed into the network, the input is a tensor with 256 × 256 × 6 pixels, while the output is a tensor with 256 × 256 × 1 pixels. For the proposed loss function, the weight of each side output ω i (i = 1, 2, 3, 4, 5) is set to 1.0, while λ is set to 0.5 for balancing the weight of binary cross-entropy loss and dice coefficient loss. The effect of loss function, data augmentation, and MSOF will be described in the following subsections. In addition, the parameters of the other competitors are set as illustrated in the literature <ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b56">56]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Effect of Loss Function</head><p>Loss function plays an important role in the final CD results. In particular, the parameter λ, which balances the weight of binary cross-entropy loss and dice coefficient loss, is of great significance on our proposed loss function. To verify the sensitivity of parameter λ, we varied λ from 0 to 1.0, and the corresponding evaluation metrics were calculated, as illustrated in Figure <ref type="figure" target="#fig_8">4</ref>. When λ is set to 0, only binary cross-entropy loss is utilized, where the values of P, R, F1, and OA remain at a low level. Then, the four quantitative evaluation metrics increase with the increase of parameter λ, which verifies the effectiveness of combining binary cross-entropy loss and dice coefficient loss. Note that P, R, F1, and OA achieve the maximum values when λ is 0.5, which means that the influence of binary cross-entropy loss and dice coefficient loss are well balanced. However, the values of the four quantitative evaluation metrics show a shock downward trend with the further increase of parameter λ. Therefore, parameter λ is set to 0.5 for the sake of better CD performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Effect of Data Augmentation</head><p>The raw datasets consist of 10,000 training sets and 3000 testing and validation sets, leading to an overfitting problem when training a large network. Because our proposed network contains more than 9 million parameters, it is critical to implement data augmentation to avoid overfitting, as well as improving the generalization ability. To augment the training sets, each image pair is shifted and scaled, rotated by 90 °, 180 °, and 270 °, flipped in horizontal and vertical directions. Figure <ref type="figure" target="#fig_10">5</ref> illustrates the effect of data augmentation by the four quantitative evaluation metrics. We can conclude that the P, R, F1, and OA values can increase by a large margin with data augmentation, which are increased by 40.13%, 57.95%, 58.34%, and 14.01%, respectively. The main reason lies in the fact that 70,000 training sets and 21,000 validation sets are employed after data augmentation, in which case the proposed network parameters can be better learned from more training sets. In addition to this, the overfitting effect can be reduced and the generalization ability of the proposed network can be improved to a large extent. Hence, it is of great significance to implement data augmentation so as to improve the CD accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Effect of Data Augmentation</head><p>The raw datasets consist of 10,000 training sets and 3000 testing and validation sets, leading to an overfitting problem when training a large network. Because our proposed network contains more than 9 million parameters, it is critical to implement data augmentation to avoid overfitting, as well as improving the generalization ability. To augment the training sets, each image pair is shifted and scaled, rotated by 90</p><p>• , 180</p><p>• , and 270</p><p>• , flipped in horizontal and vertical directions. Figure <ref type="figure" target="#fig_10">5</ref> illustrates the effect of data augmentation by the four quantitative evaluation metrics. We can conclude that the P, R, F1, and OA values can increase by a large margin with data augmentation, which are increased by 40.13%, 57.95%, 58.34%, and 14.01%, respectively. The main reason lies in the fact that 70,000 training sets and 21,000 validation sets are employed after data augmentation, in which case the proposed network parameters can be better learned from more training sets. In addition to this, the overfitting effect can be reduced and the generalization ability of the proposed network can be improved to a large extent. Hence, it is of great significance to implement data augmentation so as to improve the CD accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Effect of MSOF</head><p>In order to improve the convergence of the proposed deep networks and learn more meaningful features ranging from low to high levels, an MSOF strategy is utilized. Figure <ref type="figure" target="#fig_11">6</ref> presents the influence of MSOF in terms of four quantitative evaluation metrics, namely P, R, F1, and OA. As can be seen, the accuracy of CD can be further improved by the usage of MSOF strategy, where the increase of P, R, F1, and OA are 1.70%, 5.29%, 3.88%, and 1.36%, respectively. This is because feature maps from multiple semantic levels are combined, where more detailed information can be captured. Therefore, MSOF is effective to improve the CD accuracy in our proposed CD method.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results Comparisons</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Effect of MSOF</head><p>In order to improve the convergence of the proposed deep networks and learn more meaningful features ranging from low to high levels, an MSOF strategy is utilized. Figure <ref type="figure" target="#fig_11">6</ref> presents the influence of MSOF in terms of four quantitative evaluation metrics, namely P, R, F1, and OA. As can be seen, the accuracy of CD can be further improved by the usage of MSOF strategy, where the increase of P, R, F1, and OA are 1.70%, 5.29%, 3.88%, and 1.36%, respectively. This is because feature maps from multiple semantic levels are combined, where more detailed information can be captured. Therefore, MSOF is effective to improve the CD accuracy in our proposed CD method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Effect of MSOF</head><p>In order to improve the convergence of the proposed deep networks and learn more meaningful features ranging from low to high levels, an MSOF strategy is utilized. Figure <ref type="figure" target="#fig_11">6</ref> presents the influence of MSOF in terms of four quantitative evaluation metrics, namely P, R, F1, and OA. As can be seen, the accuracy of CD can be further improved by the usage of MSOF strategy, where the increase of P, R, F1, and OA are 1.70%, 5.29%, 3.88%, and 1.36%, respectively. This is because feature maps from multiple semantic levels are combined, where more detailed information can be captured. Therefore, MSOF is effective to improve the CD accuracy in our proposed CD method.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results Comparisons</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results Comparisons</head><p>In order to verify the effectiveness and superiority of our proposed CD method, six typical testing areas, which consist of complex changes such as cars, roads, and buildings, are presented for visual comparisons, as illustrated in Figures <ref type="figure">7</ref><ref type="figure" target="#fig_17">8</ref><ref type="figure" target="#fig_20">9</ref><ref type="figure" target="#fig_2">10</ref><ref type="figure" target="#fig_22">11</ref><ref type="figure" target="#fig_27">12</ref>. It can be observed that the proposed CD method achieves the best CD performance (Figures <ref type="figure">7j,</ref><ref type="figure" target="#fig_19">8j</ref>, 9j, 10j, 11j and 12j), where the CMs are closely consistent with the reference CMs (Figures <ref type="figure" target="#fig_26">7c, 8c, 9c, 10c, 11c</ref> and<ref type="figure" target="#fig_27">12c</ref>). In particular, compared with other methods, our approach generates the CMs with more accurate boundaries and less missed detections. As seen in Figures <ref type="figure">7</ref> and<ref type="figure" target="#fig_17">8</ref>, the boundaries of building changes are clearer and the inner parts are more complete. Note that, compared to the other SOTA methods, our approach achieves the better detection performance for the changes of small objects, e.g., car changes in Figures <ref type="figure" target="#fig_20">9</ref> and<ref type="figure" target="#fig_2">10</ref>. In addition, our CD method is also capable of capturing complex and obscure changes, such as long and narrow roads, as seen in Figures <ref type="figure" target="#fig_26">11</ref> and<ref type="figure" target="#fig_27">12</ref>. We can see that the road changes can only be partly or coarsely detected by the comparative SOTA methods (Figures <ref type="figure" target="#fig_26">11d-i</ref> and<ref type="figure" target="#fig_27">12d-i</ref>), while the outlines and locations of the roads are better detected in Figures <ref type="figure" target="#fig_26">11j</ref> and<ref type="figure" target="#fig_27">12j</ref>.</p><p>OA-were calculated and are summarized in Table <ref type="table" target="#tab_1">2</ref>. We concluded that the CDNet method obtains the lowest F1 and OA values among all the seven SOTA methods. The reason lies in the fact that the network consists of only contraction and expansion blocks without skip connections between the encoder and decoder parts. Therefore, the detailed information from the low levels was missed in the expansion layers, as can be seen in Figures <ref type="figure" target="#fig_27">7-12d</ref>. Based on the UNet backbone, the FC-EF method (Figures <ref type="figure" target="#fig_27">7-12e</ref>) achieved better CD results than the CDNet because skip connections were implemented between the encoder blocks and the corresonding decoder blocks, with F1 and OA values increased by 12.05% and 3.88%, respectively. Rather than using traditional convolutional blocks, the FC-EF-Res method adopts residual connections, which greatly facilitates the training of very deep networks, as well as improving the spatial accuracy of the final CM. In order to overcome the drawbacks of global pooling, a pyramid pooling module was integrated into the FCN-PP, where multi-scale features from different convolutional layers were combined to obtain strong featurerepresentation ability. Therefore, the FC-EF-Res (Figures <ref type="figure" target="#fig_27">7-12h</ref>) and FCN-PP (Figures <ref type="figure" target="#fig_27">7-12i</ref>) methods achieved better CD results than the FC-EF method, with F1 values increased by 1.95% and 4.36%, respectively, and OA values increased by 0.24% and 1.31%, respectively. Rather than using concatenated image pairs as the input, the Siamese architecture was employed in FC-Siam-conc and FC-Siam-diff, which are two kinds of Siamese extensions of FC-EF model. We concluded that FC-Siam-conc (Figures <ref type="figure" target="#fig_27">7-12f</ref>) and FC-Siam-diff (Figures <ref type="figure" target="#fig_27">7-12g</ref>) obtained better CD results than FC-EF, with F1 increased by 6.99% and 8.59%, , respectively, and OA values increased by 1.69% and 1.72%, respectively. The reasons lie in the fact that the explicit comparisons between image pairs were integrated into Siamese-based skip connections. In particular, difference skip connections could act as the guidance of comparing the difference between image pairs in the architecture, which is the reason why the FC-Siam-diff method achieves even better CD performance than the FC-Siam-conc method.            Meanwhile, for quantitative comparisons, four quantitative evaluation metrics-P, R, F1, and OA-were calculated and are summarized in Table <ref type="table" target="#tab_1">2</ref>. We concluded that the CDNet method obtains the lowest F1 and OA values among all the seven SOTA methods. The reason lies in the fact that the network consists of only contraction and expansion blocks without skip connections between the encoder and decoder parts. Therefore, the detailed information from the low levels was missed in the expansion layers, as can be seen in Figures <ref type="figure">7d,</ref><ref type="figure" target="#fig_19">8d</ref>, 9d, 10d, 11d and 12d. Based on the UNet backbone, the FC-EF method (Figures 7e, 8e, 9e, 10e, 11e and 12e) achieved better CD results than the CDNet because skip connections were implemented between the encoder blocks and the corresonding decoder blocks, with F1 and OA values increased by 12.05% and 3.88%, respectively. Rather than using traditional convolutional blocks, the FC-EF-Res method adopts residual connections, which greatly facilitates the training of very deep networks, as well as improving the spatial accuracy of the final CM. In order to overcome the drawbacks of global pooling, a pyramid pooling module was integrated into the FCN-PP, where multi-scale features from different convolutional layers were combined to obtain strong feature-representation ability. Therefore, the FC-EF-Res (Figures <ref type="figure">7h,</ref><ref type="figure" target="#fig_19">8h</ref>, 9h, 10h, 11h and 12h) and FCN-PP (Figures 7i, 8i, 9i, 10i, 11i and 12i) methods achieved better CD results than the FC-EF method, with F1 values increased by 1.95% and 4.36%, respectively, and OA values increased by 0.24% and 1.31%, respectively. Rather than using concatenated image pairs as the input, the Siamese architecture was employed in FC-Siam-conc and FC-Siam-diff, which are two kinds of Siamese extensions of FC-EF model. We concluded that FC-Siam-conc (Figures <ref type="figure">7f,</ref><ref type="figure" target="#fig_19">8f</ref>, 9f, 10f, 11f and 12f) and FC-Siam-diff (Figures <ref type="figure">7g,</ref><ref type="figure" target="#fig_19">8g</ref>, 9g, 10g, 11g and 12g) obtained better CD results than FC-EF, with F1 increased by 6.99% and 8.59%, respectively, and OA values increased by 1.69% and 1.72%, respectively. The reasons lie in the fact that the explicit comparisons between image pairs were integrated into Siamese-based skip connections. In particular, difference skip connections could act as the guidance of comparing the difference between image pairs in the architecture, which is the reason why the FC-Siam-diff method achieves even better CD performance than the FC-Siam-conc method. Note that our CD method (Figures 7j, 8j, 9j, 10j, 11j and 12j) achieved the best performance among all the comparative SOTA methods, with P, R, F1, and OA values reaching 0.8954, 0.8711, 0.8756, and 0.9673, respectively. The reasons lie in the following aspects: (1) UNet++ is utilized as the backbone, where dense skip connections are adopted to learn more powerful multi-scale features from different semantic levels; (2) residual blocks are employed in the convolutional unit of the encoder-decoder architecture, which facilitates the gradient convergence in the proposed deep CNN network, as well as capturing more detailed information; (3) a novel DS method named MSOF is applied in our proposed CD method, which could effectively combine multi-scale feature maps from different semantic levels to generate the final CM; (4) a novel loss function is proposed for our CD task, which combines weighted binary cross-entropy and dice coefficient loss effectively, thereby reducing the class imbalance problem. Compared with CDNet, the proposed CD method achieves the F1 and OA values increased by 27.23% and 6.24%, respectively, which verifies its effectiveness and superiority.</p><p>Figure <ref type="figure" target="#fig_28">13</ref> illustrates the number of parameters in different comparative CD approaches. We concluded that CDNet possesses the least network parameters and the worst CD performance (as seen in Table <ref type="table" target="#tab_1">2</ref>). This is because the skip connections are ignored and the change relations between image pairs might not be modeled well using only the simple encoder-decoder architecture. Nevertheless, the skip connections were adopted in the FC-EF model, where the network parameters increase sharply from 1.13 M to 7.77 M, yielding average improvements of F1 and OA values of 0.08 and 0.03, respectively. It should be mentioned that the residual connections might further improve the CD accuracy without any increase of network parameters. That is the reason why the FC-EF-Res method achieves higher F1 and OA values while sharing the same network parameters with the FC-EF. In addition, the FCN-PP method obtains higher F1 and OA values at the cost of increasing network parameters, which are increased by 40.28% compared with the FC-EF method. Due to the additional inner skip connections in the FC-Siam-conc and FC-Siam-diff methods, both the network parameters exceed 10 M, thus increasing the GPU burdens during the training stage to a large extent. It is worth noting that our method achieves the best CD performance with only 9.06 M parameters, which strikes a better balance between CD performance and network parameters.</p><p>developed DL techniques for semantic segmentation, a novel end-to-end CD method was proposed for performing CD tasks on VHR satellite images. The effectiveness of the proposed CD method was comprehensively examined based on the VHR satellite image datasets. Additionally, the superiority of the proposed CD method was verified through the quantitative and qualitative analysis against several SOTA end-to-end CD methods. Our proposed end-to-end CD model is based on the appearance or disappearance of existing objects while ignoring seasonal changes, and there is no need to implement radiometric corrections, which is a necessary step in traditional CD methods. In addition, the inner change relations between multitemporal image pairs can be learned from manual interpretation CMs, thereby introducing human domain knowledge effectively. That is especially useful for detecting changes of interest in complex areas, when a large amount of non-interesting changes are unnecessarily detected by using traditional CD methods. It should be noted that, due to the usage of dense skip connections and MSOF strategy, the proposed CD approach is robust to object changes of different scales and sizes, ranging from small cars to large construction structures. This means that our CD method might capture multi-scale object changes, which is critical for detecting objects with sharp changes in sizes and scales on VHR satellite images. Particularly, in difficult areas, our deep neural network model is easily to be fine-tuned by introducing human domain knowledge and adding corresponding samples. In addition, as image pairs are concatenated as the input for our model, it is flexible to include temporal information from more than two periods of images, making it possible to extend the CD task from image pairs to image sequences. Note that our proposed CD model has low computational burden in terms of inference. For example, it takes less than 0.05 s to predict an image with a size of 256 × 256 pixels. Therefore, our FCN model provides a promising solution to implement real-time CD once the model is well-trained. As the change relations between multi-temporal images are learned from scratch using available training datasets, the CD results are mainly influenced by: 1) data distribution between the training sets and testing sets; and 2) the accuracy of ground truth maps. When the data distribution is consistent between training sets and testing sets, the trained model will be generalized well to the testing sets and accurate CMs can be produced, otherwise CD results will be greatly influenced. In terms of ground truth maps, only reasonable mapping functions can be learned with high-quality ground truth maps, thereby generating high-accuracy CMs. It is worth noting that although our proposed approach is verified on VHR satellite images, the improved </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Traditional CD methods generally consist of three stages: pre-processing (geometrical rectification, radiometric and atmospheric corrections, image registration, etc.), change information extraction, and accuracy assessment. However, the errors are propagated from the former stages to later stages, which inevitably reduce CD accuracy and reliability. In particular, threshold segmentation is generally utilized in traditional CD methods, which works on the assumptions that the number of changed pixels is proportional to that of the unchanged. However, this is not the real case in some complex scenes. Therefore, it is of great importance to solve CD tasks in an end-to-end manner, where the change relations are learned directly from image pairs. Inspired by the recently developed DL techniques for semantic segmentation, a novel end-to-end CD method was proposed for performing CD tasks on VHR satellite images.</p><p>The effectiveness of the proposed CD method was comprehensively examined based on the VHR satellite image datasets. Additionally, the superiority of the proposed CD method was verified through the quantitative and qualitative analysis against several SOTA end-to-end CD methods. Our proposed end-to-end CD model is based on the appearance or disappearance of existing objects while ignoring seasonal changes, and there is no need to implement radiometric corrections, which is a necessary step in traditional CD methods. In addition, the inner change relations between multi-temporal image pairs can be learned from manual interpretation CMs, thereby introducing human domain knowledge effectively. That is especially useful for detecting changes of interest in complex areas, when a large amount of non-interesting changes are unnecessarily detected by using traditional CD methods. It should be noted that, due to the usage of dense skip connections and MSOF strategy, the proposed CD approach is robust to object changes of different scales and sizes, ranging from small cars to large construction structures. This means that our CD method might capture multi-scale object changes, which is critical for detecting objects with sharp changes in sizes and scales on VHR satellite images. Particularly, in difficult areas, our deep neural network model is easily to be fine-tuned by introducing human domain knowledge and adding corresponding samples. In addition, as image pairs are concatenated as the input for our model, it is flexible to include temporal information from more than two periods of images, making it possible to extend the CD task from image pairs to image sequences. Note that our proposed CD model has low computational burden in terms of inference. For example, it takes less than 0.05 s to predict an image with a size of 256 × 256 pixels. Therefore, our FCN model provides a promising solution to implement real-time CD once the model is well-trained. As the change relations between multi-temporal images are learned from scratch using available training datasets, the CD results are mainly influenced by: 1) data distribution between the training sets and testing sets; and 2) the accuracy of ground truth maps. When the data distribution is consistent between training sets and testing sets, the trained model will be generalized well to the testing sets and accurate CMs can be produced, otherwise CD results will be greatly influenced. In terms of ground truth maps, only reasonable mapping functions can be learned with high-quality ground truth maps, thereby generating high-accuracy CMs. It is worth noting that although our proposed approach is verified on VHR satellite images, the improved UNet++ itself is independent of raster data forms. This means other forms of 2-D raster data (such as hyperspectral images or multi-channel radar images) can also be applied for CD using our approach; the only difference is the data normalization method before the images are fed into the network.</p><p>However, there exist several potential limitations to the proposed method. First, to enlarge the receptive field, the proposed UNet++ architecture utilizes the down-sampling and up-sampling strategy, where the size of feature maps will be half of the inputs after down-sampling while twice the inputs after up-sampling. As four max-pooling layers are applied in the down-sampling part, the size of the feature maps will be one-eighth of the original images size. Therefore, in order to resize the feature maps to the original size after up-sampling operations, the size of the input images should be a multiple of eight (such as 128, 256, and 512). Furthermore, because the proposed FCN architecture contains millions of parameters, a large number of training samples are needed. Due to different sizes and locations of the object changes, it is quite labor-intensive to obtain enough reference CMs with high accuracy. Thus, recently developed DL techniques, such as transfer learning, reinforcement learning, weakly supervised learning, should be exploited for our network to solve the issues of a limited number of training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, an improved UNet++ architecture was proposed for end-to-end CD of VHR satellite images. The dense skip connections within the UNet++ architecture were utilized to learn multi-scale feature maps from different semantic levels. In order to facilitate gradient convergence of the deep FCN network, we adopted a residual block strategy, which was also helpful for capturing more detailed information. In addition to this, the MSOF strategy was adopted to combine multi-scale side-output feature maps and then generate the final CM. To reduce the class imbalance effect, we combined the weighted binary cross-entropy loss and dice coefficient loss effectively. The effectiveness of the proposed method was elaborately examined through the experiments on the VHR satellite image datasets. Compared with other SOTA methods, the proposed approach obtained the best CD performance on both visual comparison and quantitative metrics evaluation. However, the proposed architecture requires a large number of true CMs, which limits the widespread use to a certain extent. In addition, our architecture focuses on only change/no-change information, which is not enough for some practical applications. In the future, we will exploit the potentials of weakly supervised learning and samples generation techniques, as well as investigating the means to build the semantic relations for the changed areas.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>feature-based deep learning change detection (FB-DLCD), (2) patch-based deep learning change detection (PB-DLCD), and (3) image-based deep learning change detection (IB-DLCD).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. An illustration of encoder-decoder-based FCN architecture for semantic segmentation.</figDesc><graphic coords="5,79.52,180.29,441.90,169.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. An illustration of encoder-decoder-based FCN architecture for semantic segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Flowchart of the proposed method: (a) illustration of the main flowchart; (b) illustration of the convolution unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Flowchart of the proposed method: (a) illustration of the main flowchart; (b) illustration of the convolution unit.</figDesc><graphic coords="6,131.39,549.24,339.84,98.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Remote Sens. 2019, 11 , x 10 of 23 Figure 3 .</head><label>11233</label><figDesc>Figure 3. An illustration of different object changes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. An illustration of different object changes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The effects of parameter 𝜆 on the accuracy of our CD method: (a) the effect of parameter 𝜆 on Precision; (b) the effect of parameter 𝜆 on Recall; (c) the effect of parameter 𝜆 on F1-score; (d) the effect of parameter 𝜆 on OA.</figDesc><graphic coords="12,82.52,74.44,442.08,355.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The effects of parameter λ on the accuracy of our CD method: (a) the effect of parameter λ on Precision; (b) the effect of parameter λ on Recall; (c) the effect of parameter λ on F1-score; (d) the effect of parameter λ on OA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The effect of data augmentation on the accuracy of our proposed CD method in terms of Precision, Recall, F1-score, and OA.</figDesc><graphic coords="13,95.08,89.04,413.28,258.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. The effect of multiple side-outputs fusion (MSOF) strategy on the accuracy of our proposed CD method in terms of Precision, Recall, F1-score, and OA.</figDesc><graphic coords="13,91.27,511.75,424.80,217.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The effect of data augmentation on the accuracy of our proposed CD method in terms of Precision, Recall, F1-score, and OA.</figDesc><graphic coords="13,97.03,83.83,413.28,258.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Remote Sens. 2019, 11 , x 13 of 23 Figure 5 .</head><label>11235</label><figDesc>Figure 5. The effect of data augmentation on the accuracy of our proposed CD method in terms of Precision, Recall, F1-score, and OA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. The effect of multiple side-outputs fusion (MSOF) strategy on the accuracy of our proposed CD method in terms of Precision, Recall, F1-score, and OA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. The effect of multiple side-outputs fusion (MSOF) strategy on the accuracy of our proposed CD method in terms of Precision, Recall, F1-score, and OA.</figDesc><graphic coords="13,89.32,516.96,424.80,217.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 7 . 23 Figure 7 .</head><label>7237</label><figDesc>Figure 7. Visual comparisons of change detection results using different approaches for area 1: (a) image T 1 , (b) image T 2 , (c) reference change map, (d) CDNet, (e) FC-EF, (f) FC-Siam-conc, (g) FC-Siam-diff, (h) FC-EF-Res, (i) FCN-PP, and (j) proposed method. The changed parts are marked in white while the unchanged parts are in black. Remote Sens. 2019, 11, x 15 of 23 Figure 7. Visual comparisons of change detection results using different approaches for area 1: (a) image 𝑇 , (b) image 𝑇 , (c) reference change map, (d) CDNet, (e) FC-EF, (f) FC-Siam-conc, (g) FC-Siam-diff, (h) FC-EF-Res, (i) FCN-PP, and (j) proposed method. The changed parts are marked in white while the unchanged parts are in black.</figDesc><graphic coords="14,98.64,298.38,397.93,158.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Visual comparisons of change detection results using different approaches for area 2: (a) image 𝑇 , (b) image 𝑇 , (c) reference change map, (d) CDNet, (e) FC-EF, (f) FC-Siam-conc, (g) FC-Siam-diff, (h) FC-EF-Res, (i) FCN-PP, and (j) proposed method. The changed parts are marked in white while the unchanged parts are in black.</figDesc><graphic coords="14,98.83,523.60,397.98,166.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Visual comparisons of change detection results using different approaches for area 2: (a) image T 1 , (b) image T 2 , (c) reference change map, (d) CDNet, (e) FC-EF, (f) FC-Siam-conc, (g) FC-Siam-diff, (h) FC-EF-Res, (i) FCN-PP, and (j) proposed method. The changed parts are marked in white while the unchanged parts are in black.</figDesc><graphic coords="14,98.83,752.83,397.87,165.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Visual comparisons of change detection results using different approaches for area 2: (a) image 𝑇 , (b) image 𝑇 , (c) reference change map, (d) CDNet, (e) FC-EF, (f) FC-Siam-conc, (g) FC-Siam-diff, (h) FC-EF-Res, (i) FCN-PP, and (j) proposed method. The changed parts are marked in white while the unchanged parts are in black.</figDesc><graphic coords="15,109.69,-124.82,375.87,156.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Visual comparisons of change detection results using different approaches for area 3: (a) image 𝑇 , (b) image 𝑇 , (c) reference change map, (d) CDNet, (e) FC-EF, (f) FC-Siam-conc, (g) FC-Siam-diff, (h) FC-EF-Res, (i) FCN-PP, and (j) proposed method. The changed parts are marked in white while the unchanged parts are in black.</figDesc><graphic coords="15,110.05,315.63,375.77,156.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 9 . 23 Figure 10 .</head><label>92310</label><figDesc>Figure 9. Visual comparisons of change detection results using different approaches for area 3: (a) image T 1 , (b) image T 2 , (c) reference change map, (d) CDNet, (e) FC-EF, (f) FC-Siam-conc, (g) FC-Siam-diff, (h) FC-EF-Res, (i) FCN-PP, and (j) proposed method. The changed parts are marked in white while the unchanged parts are in black. Remote Sens. 2019, 11, x 16 of 23</figDesc><graphic coords="15,109.69,91.67,375.77,156.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Visual comparisons of change detection results using different approaches for area 5: (a) image 𝑇 , (b) image 𝑇 , (c) reference change map, (d) CDNet, (e) FC-EF, (f) FC-Siam-conc, (g) FC-Siam-diff, (h) FC-EF-Res, (i) FCN-PP, and (j) proposed method. The changed parts are marked in white while the unchanged part are in black.</figDesc><graphic coords="15,109.52,537.91,375.87,155.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 10 . 23 Figure 10 .</head><label>102310</label><figDesc>Figure 10. Visual comparisons of change detection results using different approaches for area 4: (a) image T 1 , (b) image T 2 , (c) reference change map, (d) CDNet, (e) FC-EF, (f) FC-Siam-conc, (g) FC-Siam-diff, (h) FC-EF-Res, (i) FCN-PP, and (j) proposed method. The changed parts are marked in white while the unchanged parts are in black.</figDesc><graphic coords="15,109.52,322.34,375.77,156.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Visual comparisons of change detection results using different approaches for area 5: (a) image 𝑇 , (b) image 𝑇 , (c) reference change map, (d) CDNet, (e) FC-EF, (f) FC-Siam-conc, (g) FC-Siam-diff, (h) FC-EF-Res, (i) FCN-PP, and (j) proposed method. The changed parts are marked in white while the unchanged part are in black.</figDesc><graphic coords="15,110.05,746.43,375.77,157.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Visual comparisons of change detection results using different approaches for area 5: (a) image T 1 , (b) image T 2 , (c) reference change map, (d) CDNet, (e) FC-EF, (f) FC-Siam-conc, (g) FC-Siam-diff, (h) FC-EF-Res, (i) FCN-PP, and (j) proposed method. The changed parts are marked in white while the unchanged part are in black.</figDesc><graphic coords="15,109.52,753.13,375.77,157.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Visual comparisons of change detection results using different approaches for area 5: (a) image 𝑇 , (b) image 𝑇 , (c) reference change map, (d) CDNet, (e) FC-EF, (f) FC-Siam-conc, (g) FC-Siam-diff, (h) FC-EF-Res, (i) FCN-PP, and (j) proposed method. The changed parts are marked in white while the unchanged part are in black.</figDesc><graphic coords="16,98.64,-136.59,397.98,164.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Visual comparison of change detection results using different approaches for area 6: (a) image T 1 , (b) image T 2 , (c) reference change map, (d) CDNet, (e) FC-EF, (f) FC-Siam-conc, (g) FC-Siam-diff, (h) FC-EF-Res, (i) FCN-PP, and (j) proposed method. The changed parts are marked in white while the unchanged are in black.</figDesc><graphic coords="16,98.64,91.29,397.87,166.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. A comparison of network parameter size for different methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. A comparison of network parameter size for different methods.</figDesc><graphic coords="18,83.88,62.96,425.28,260.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Summary of contemporary CD methods. Celik<ref type="bibr" target="#b7">[8]</ref>, Deng et al.<ref type="bibr" target="#b8">[9]</ref>, Wu et al.<ref type="bibr" target="#b9">[10]</ref>, Huang et al.<ref type="bibr" target="#b10">[11]</ref>, Benedek et al.<ref type="bibr" target="#b13">[14]</ref>, and Bazi et al.<ref type="bibr" target="#b17">[18]</ref> </figDesc><table><row><cell>Methods</cell><cell>Category</cell><cell>Example Studies</cell></row><row><cell>Traditional CD methods</cell><cell cols="2">PBCD Bruzzone et al. [7], OBCD Ma et al. [20], Zhang et al. [21], Gil-Yepes et al. [22], Qin et al. [23]</cell></row><row><cell></cell><cell>FB-DLCD</cell><cell>Sakurada et al. [26], Saha et al. [27], Hou et al. [28], El Amin et al. [29], Zhan et al. [31], Zhang et al. [32], Niu et al. [33], and Zhan et al. [34]</cell></row><row><cell>Deep learning</cell><cell></cell><cell>Gong et al. [36], Arabi et al. [38], Ma et al. [40], Zhang et al. [41], Khan</cell></row><row><cell>CD methods</cell><cell>PB-DLCD</cell><cell>et al. [42], Daudt et al. [44], Wang et al. [45], Wiratama et al. [46], Zhang</cell></row><row><cell></cell><cell></cell><cell>et al. [47], Mou et al. [49], and Gong et al. [50]</cell></row><row><cell></cell><cell>IB-DLCD</cell><cell>Lei et al. [52], Daudy et al. [53], Lebedev et al. [54], and Guo et al. [55]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative evaluation results of different approaches, where the best values are in bold.</figDesc><table><row><cell>Methods</cell><cell>Precision</cell><cell>Recall</cell><cell>F1-Score</cell><cell>OA</cell></row><row><cell>CDNet</cell><cell>0.7395</cell><cell>0.6797</cell><cell>0.6882</cell><cell>0.9105</cell></row><row><cell>FC-EF</cell><cell>0.8156</cell><cell>0.7613</cell><cell>0.7711</cell><cell>0.9413</cell></row><row><cell>FC-Siam-conc</cell><cell>0.8441</cell><cell>0.8250</cell><cell>0.8250</cell><cell>0.9572</cell></row><row><cell>FC-Siam-diff</cell><cell>0.8578</cell><cell>0.8364</cell><cell>0.8373</cell><cell>0.9575</cell></row><row><cell>FC-EF-Res</cell><cell>0.8093</cell><cell>0.7881</cell><cell>0.7861</cell><cell>0.9436</cell></row><row><cell>FCN-PP</cell><cell>0.8264</cell><cell>0.8060</cell><cell>0.8047</cell><cell>0.9536</cell></row><row><cell>Proposed method</cell><cell>0.8954</cell><cell>0.8711</cell><cell>0.8756</cell><cell>0.9673</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Remote Sens. 2019, 11, 1382; doi:10.3390/rs11111382 www.mdpi.com/journal/remotesensing</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Funding: This work was supported in part by the National Natural Science Foundation of China (under grant numbers: 41801386 and 41671454), in part by the Natural Science Foundation of Jiangsu Province (under grant number: BK20180797), and in part by the Startup Project for Introducing Talent of NUIST (under grant number: 2018r029).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Author Contributions: D.P. conceived and designed the experiments, and he also wrote the main manuscript. H.G. and Y.Z. gave comments and suggestions on the manuscript, as well as proofreading the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of Interest:</head><p>The authors declare no conflicts of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abbreviations</head><p>The </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Review Article Digital change detection techniques using remotely-sensed data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.1080/01431168908903939</idno>
	</analytic>
	<monogr>
		<title level="j">Int. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="989" to="1003" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A critical synthesis of remotely sensed optical image change detection techniques</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Tewkesbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Comber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Tate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Fisher</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.rse.2015.01.006</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Environ</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Updating land-cover maps by classification of image time series: A novel change-detection-driven transfer learning approach</title>
		<author>
			<persName><forename type="first">B</forename><surname>Demir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bovolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2012.2195727</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="300" to="312" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A comprehensive change detection method for updating the National Land Cover Database to circa 2011</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Danielson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Homer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xian</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.rse.2013.01.012</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Environ</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page" from="159" to="175" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Land cover change detection at coarse spatial scales based on iterative estimation and previous state information</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Hégarat-Mascle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ottlé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guerin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.rse.2005.01.011</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Environ</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="464" to="479" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Change detection from remotely sensed images: From pixel-based to object-based approaches</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stanley</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2013.03.006</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="91" to="106" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic analysis of the difference image for unsupervised change detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Prieto</surname></persName>
		</author>
		<idno type="DOI">10.1109/36.843009</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1171" to="1182" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised change detection in satellite images using principal component analysis and k-means clustering</title>
		<author>
			<persName><forename type="first">T</forename><surname>Celik</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2009.2025059</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="772" to="776" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">PCA-based land-use change detection and analysis using mul-titemporal and multisensor satellite data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<idno type="DOI">10.1080/01431160801950162</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4823" to="4838" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A post-classification change detection method based on iterative slow feature analysis and Bayesian soft fusion</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.rse.2017.07.009</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Environ</title>
		<imprint>
			<biblScope unit="volume">199</biblScope>
			<biblScope unit="page" from="241" to="255" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Use of a dark object concept and support vector machines to automate forest cover change analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Townshend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Masek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Goward</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.rse.2007.07.023</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Environ</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="970" to="985" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic change detection in high-resolution remote-sensing images by means of level set evolution and support vector machine classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shang</surname></persName>
		</author>
		<idno type="DOI">10.1080/01431161.2014.951740</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="6255" to="6270" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Supervised change detection in VHR images using contextual information and support vector machines</title>
		<author>
			<persName><forename type="first">M</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bovolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kanevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jag.2011.10.013</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Appl. Earth Obs. Geoinform</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="77" to="85" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Change detection in optical aerial images by a multilayer conditional mixed Markov model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Benedek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Szir´anyi</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2009.2022633</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="3416" to="3430" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A new change detection method in high-resolution remote sensing images based on a conditional random field model</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1080/01431161.2016.1148284</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1173" to="1189" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised change detection based on hybrid conditional random field model for high spatial resolution remote sensing imagery</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2018.2819367</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="4002" to="4015" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A hypergraph-based context-sensitive representation technique for VHR remote-sensing image change detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1080/2150704X.2016.1163744</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1814" to="1825" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised change detection in multispectral remotely sensed imagery with level set methods</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Melgani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Al-Sharari</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2010.2045506</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="3178" to="3187" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Object-based change detection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Wulder</surname></persName>
		</author>
		<idno type="DOI">10.1080/01431161.2011.648285</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4434" to="4457" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Object-based change detection in urban areas: The effects of segmentation strategy, scale, and feature space on unsupervised methods</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blaschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tiede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs8090761</idno>
		<imprint/>
	</monogr>
	<note>Remote Sens. 2016, 8, 761. [CrossRef</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Object-based change detection for VHR images based on multiscale uncertainty analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2017.2763182</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="13" to="17" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Description and validation of a new set of object-based temporal geostatistical features for land-use/land-cover change detection</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Gil-Yepes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Recio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Á</forename><surname>Balaguer-Beser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hermosilla</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2016.08.010</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="77" to="91" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Object-based land cover change detection for cross-sensor images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ban</surname></persName>
		</author>
		<idno type="DOI">10.1080/01431161.2013.805282</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="6723" to="6737" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning in remote sensing: A comprehensive review and list of resources</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fraundorfer</surname></persName>
		</author>
		<idno type="DOI">10.1109/MGRS.2017.2762307</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Mag</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="8" to="36" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning for remote sensing data: A technical tutorial on the state of the art</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<idno type="DOI">10.1109/MGRS.2016.2540798</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Mag</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="22" to="40" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Change Detection from a Street Image Pair using CNN Features and Superpixel Segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sakurada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference BMVC</title>
		<meeting>the British Machine Vision Conference BMVC<address><addrLine>Swansea, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09">September 2015</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="61" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised Deep Change Vector Analysis for Multiple-Change De-tection in VHR Images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bovolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2018.2886643</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="3677" to="3693" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Change Detection Based on Deep Features and Low Rank</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2017.2766840</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="2418" to="2422" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Zoom out cnns features for optical remote sensing change detection</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>El Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 2nd International Conference on Image, Vision and Computing (ICIVC)</title>
		<meeting>the 2017 2nd International Conference on Image, Vision and Computing (ICIVC)<address><addrLine>Chengdu, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06">June 2017</date>
			<biblScope unit="page" from="812" to="817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Feature-level change detection using deep representation and feature change analysis for multispectral imagery</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2016.2601930</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1666" to="1670" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Change detection based on deep siamese convolutional network for optical aerial images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2017.2738149</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1845" to="1849" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Triplet-Based Semantic Relation Learning for Aerial Remote Sensing Image Change Detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2018.2869608</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="266" to="270" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Conditional Adversarial Network for Change Detection in Heterogeneous Images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2018.2868704</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="45" to="49" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Iterative feature mapping network for detecting multiple changes in multi-source remote sensing images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2018.09.002</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page" from="38" to="51" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multiscale Superpixel Segmentation with Deep Features for Change Detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2019.2902613</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="36600" to="36616" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Superpixel-based difference representation learning for change detection in multispectral remote sensing images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Miao</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2017.2650198</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="2658" to="2673" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generative adversarial networks for change detection in multi-spectral imagery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2017.2762694</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="2310" to="2314" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Optical Remote Sensing Change Detection Through Deep Siamese Network</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E A</forename><surname>Arabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Karoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Djerriri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium</title>
		<meeting>the IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07">July 2018</date>
			<biblScope unit="page" from="5041" to="5044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Local Descriptor Learning for Change Detection in Synthetic Aperture Radar Images via Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2018.2889326</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="15389" to="15403" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Change Detection in Remote Sensing Images Based on Image Mapping and a Deep Capsule Network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11060626</idno>
		<imprint/>
	</monogr>
	<note>Remote Sens. 2019, 11, 626. [CrossRef</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Change Detection between Multimodal Remote Sensing Data Using Siamese CNN</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vosselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gerke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.09562</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Forest change detection in incomplete satellite images with deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2017.2707528</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="5407" to="5423" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Change detection in synthetic aperture radar images based on deep neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2015.2435783</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="125" to="138" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Urban change detection for multispectral earth observation using convolutional neural networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Daudt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Le Saux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gousseau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.08468</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">GETNET: A General End-to-End 2-D CNN Framework for Hyper-spectral Image Change Detection</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2018.2849692</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="3" to="13" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dual-Dense Convolution Network for Change Detection of High-Resolution Panchromatic Imagery</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wiratama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sim</surname></persName>
		</author>
		<idno type="DOI">10.3390/app8101785</idno>
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="1785">2018. 1785</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The Spectral-Spatial Joint Learning for Change Detection in Multispectral Imagery</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11030240</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">240</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning a transferable change rule from a recurrent neural network for land cover change detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs8060506</idno>
		<imprint/>
	</monogr>
	<note>Remote Sens. 2016, 8, 506. [CrossRef</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning spectral-spatial-temporal features via a recurrent convolutional neural network for change detection in multispectral imagery</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2018.2863224</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="924" to="935" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Discriminatory Classified Net-work for Change Detection in Multispectral Imagery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Generative</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTARS.2018.2887108</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="321" to="333" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">High Resolution Semantic Change Detection</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Daudt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Le Saux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gousseau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.08452v1</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Landslide Inventory Mapping from Bi-temporal Images Using Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Nandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="982" to="986" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fully convolutional siamese networks for change detection</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Daudt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Le Saux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 25th IEEE International Conference on Image Processing (ICIP)</title>
		<meeting>the 2018 25th IEEE International Conference on Image Processing (ICIP)<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-10">7-10 October 2018</date>
			<biblScope unit="page" from="4063" to="4067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Remote</forename><surname>Sens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 1382 23 of 23</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Change detection in remote sensing images using conditional adversarial networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">V</forename><surname>Vizilter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vygolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Knyaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Rubis</surname></persName>
		</author>
		<idno type="DOI">10.5194/isprs-archives-XLII-2-565-2018</idno>
	</analytic>
	<monogr>
		<title level="j">Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="565" to="571" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Learning to Measure Change: Fully Convolutional Siamese Metric Networks for Scene Change Detection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09111</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Streetview change detection with deconvolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Alcantarilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gherardi</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10514-018-9734-5</idno>
	</analytic>
	<monogr>
		<title level="j">Auton. Robots</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1301" to="1322" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Unsupervised Deep Noise Modeling for Hyperspectral Image Change Detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11030258</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sens</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unet++: A nested u-net architecture for medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</title>
		<meeting><address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Classification and segmentation of satellite orthoimagery using convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>L¨angkvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kiselev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alirezaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Loutfi</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs8040329</idno>
		<imprint/>
	</monogr>
	<note>Remote Sens. 2016, 8, 329. [CrossRef</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2015.2389824</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for robust semantic pixel-wise labelling</title>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.07293</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting>the International Conference on Medical Image Computing and Computer-Assisted Intervention<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-10">October 2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Objects Segmentation from High-Resolution Aerial Images Using U-Net With Pyramid Pooling Layers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2018.2868880</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="115" to="119" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Selfnormalizing neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="971" to="980" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Deeply-supervised nets</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.5185</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12">December 2015</date>
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
