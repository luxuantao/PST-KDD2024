<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gaussian Process Dynamical Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jack</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
							<email>jmwang@dgp.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<postCode>M5S 3G4</postCode>
									<settlement>Toronto</settlement>
									<region>ON</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
							<email>fleet@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<postCode>M5S 3G4</postCode>
									<settlement>Toronto</settlement>
									<region>ON</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
							<email>hertzman¡@dgp.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<postCode>M5S 3G4</postCode>
									<settlement>Toronto</settlement>
									<region>ON</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gaussian Process Dynamical Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C687E0D739C33694F784287B2A5D3884</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces Gaussian Process Dynamical Models (GPDM) for nonlinear time series analysis. A GPDM comprises a low-dimensional latent space with associated dynamics, and a map from the latent space to an observation space. We marginalize out the model parameters in closed-form, which amounts to using Gaussian Process (GP) priors for both the dynamics and the observation mappings. This results in a nonparametric model for dynamical systems that accounts for uncertainty in the model. We demonstrate the approach on human motion capture data in which each pose is 62-dimensional. Despite the use of small data sets, the GPDM learns an effective representation of the nonlinear dynamics in these spaces.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A central difficulty in modeling time-series data is in determining a model that can capture the nonlinearities of the data without overfitting. Linear autoregressive models require relatively few parameters and allow closed-form analysis, but can only model a limited range of systems. In contrast, existing nonlinear models can model complex dynamics, but may require large training sets to learn accurate MAP models.</p><p>In this paper we investigate learning nonlinear dynamical models for high-dimensional datasets. We take a Bayesian approach to modeling dynamics, averaging over dynamics parameters rather than estimating them. Inspired by the fact that averaging over nonlinear regression models leads to a Gaussian Process (GP) model, we show that integrating over parameters in nonlinear dynamical systems can also be performed in closed-form. The resulting Gaussian Process Dynamical Model (GPDM) is fully defined by a set of lowdimensional representations of the training data, with both dynamics and observation mappings learned from GP regression. As a natural consequence of GP regression, the GPDM removes the need to select many parameters associated with function approximators while retaining the expressiveness of nonlinear dynamics and observation.</p><p>Our work is motivated by modeling human motion for video-based people tracking and data-driven animation. An individual human pose is typically parameterized with more than 60 parameters. Despite the large state space, the space of activity-specific human poses and motions has a much smaller intrinsic dimensionality; in our experiments with walking and golf swings, 3 dimensions often suffice. Bayesian people tracking requires dynamical models in the form of transition densities in order to specify prediction distributions over new poses at each time instant (e.g., <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref>); similarly, data-driven computer animation requires prior distributions over poses and motion (e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6]</ref>). Our work builds on the extensive literature in nonlinear time-series analysis, of which we mention a few examples. Two main themes are the use of switching linear models (e.g., <ref type="bibr" target="#b10">[11]</ref>), and nonlinear transition functions, such as represented by Radial Basis Functions <ref type="bibr" target="#b1">[2]</ref>. Both approaches require sufficient amounts of training data that one can learn the parameters of the switching or basis functions. Determining the appropriate number of basis functions is also difficult. In Kernel Dynamical Modeling <ref type="bibr" target="#b11">[12]</ref>, linear dynamics are kernelized to model nonlinear systems.</p><p>Supervised learning with GP regression has been used to model dynamics for a variety of applications <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13]</ref>. These methods model dynamics directly in observation space, which is impractical for the high-dimensional motion capture data. Our approach is most directly inspired by the unsupervised Gaussian Process Latent Variable Model (GPLVM) <ref type="bibr" target="#b4">[5]</ref>, which models the joint distribution of the observed data and their corresponding representation in a low dimensional latent space. This distribution can then be used as a prior for inference from new measurements. However, the GPLVM is not a dynamical model; it assumes that data are generated independently. Accordingly it does not respect temporal continuity of the data, nor does it model the dynamics in the latent space. Here we augment the GPLVM with a latent dynamical model. The result is a Bayesian generalization of subspace dynamical models to nonlinear latent mappings and dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Gaussian Process Dynamics</head><p>The Gaussian Process Dynamical Model (GPDM) comprises a mapping from a latent space to the data space, and a dynamical model in the latent space (Figure <ref type="figure" target="#fig_0">1</ref>). These mappings are typically nonlinear. The GPDM is obtained by marginalizing out the parameters of the two mappings, and optimizing the latent coordinates of training data.</p><p>More precisely, our goal is to model the probability density of a sequence of vector-valued states 1 6 3 !1 87 9 41 #" , with discrete-time index @ and 1 A7 CB ED GF . As a basic model, consider a latent-variable mapping with first-order Markov dynamics:</p><p>7 ¨H GI P 7 RQ 8 TS ¥ VU AW YX 23a 7</p><p>(1)</p><p>1 87 b¨c 8I d 87 S ¦ eU AW YX 6f a 7</p><p>(2)</p><p>Here, 7 B gD h denotes the i -dimensional latent coordinates at time @ , X pa 7 and X f a 7 are zero-mean, white Gaussian noise processes, H and c are (nonlinear) mappings parameter- ized by ¥ and ¦ , respectively. Figure <ref type="figure" target="#fig_0">1</ref>(a) depicts the graphical model.</p><p>While linear mappings have been used extensively in auto-regressive models, here we consider the nonlinear case for which H and c are linear combinations of basis functions: H GI d S ¥ VU b¨q sr ut r wv wr I d AU </p><p>for weights ¥ © t 3 4t ¡ $ and ¦ T ¡ $ , and basis functions v r and 6y</p><p>. In order to fit the parameters of this model to training data, one must select an appropriate number of basis functions, and one must ensure that there is enough data to constrain the shape of each basis function. Ensuring both of these conditions can be very difficult in practice.</p><p>However, from a Bayesian perspective, the specific forms of H and c -including the numbers of basis functions -are incidental, and should therefore be marginalized out.</p><p>With an isotropic Gaussian prior on the columns of ¦ , marginalizing over c can be done in closed form <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref> to yield</p><formula xml:id="formula_1">¢ I d( ¤£ § ¢ ¤ U ¨£ ¥ ¦£ " § I © AU " F £ £ F ! " $# &amp;% (' 0) 21 Q # ( 3¥ ¡ ( &amp; 54 6 (5)</formula><p>where ( 1 6 3 41 #" 5$ &amp; , is a kernel matrix, and ¢ ¤ 87 ¤ ¤ ¡ 9¥ A@ comprises the kernel hyperparameters. The elements of kernel matrix are defined by a kernel function, I B U r a y DC I P r 4 y U . For the latent mapping, § FE ( , we currently use the RBF kernel</p><formula xml:id="formula_2">C G %I d 4 IH 'U ¨¤ P " $# ¤ ¡ ¨£ Q£ # IH R£ S£ ¡ 6 W ¤ Q 8 T VU XW a W Y (6)</formula><p>As in the SGPLVM <ref type="bibr" target="#b3">[4]</ref>, we use a scaling matrix ¥ ba !c ed f I hg 9g F U to account for differing variances in the different data dimensions. This is equivalent to a GP with kernel function C #I d ! H U 0i pg ¡ q for dimension r . Hyperparameter ¤ represents the overall scale of the output function, while ¤ ¡ corresponds to the inverse width of the RBFs. The variance of the noise term X Gf a 7 is given by ¤ Q # T . The dynamic mapping on the latent coordinates § is conceptually similar, but more subtle. 1   As above, we form the joint probability density over the latent coordinates and the dynamics weights ¥ in (3). We then marginalize over the weights ¥ , i.e.,</p><formula xml:id="formula_3">¢ I § 8£ ¢ £ U ẗs ¢ I § ¥ 8£ ¢ £ U i ¥ üs ¢ I § 8£ ¥ p¢ £ U ¢ I ¥ 8£ ¢ £ U i ¥ ©<label>(7)</label></formula><p>Incorporating the Markov property (Eqn. ( <ref type="formula">1</ref>)) gives:</p><formula xml:id="formula_4">¢ I § 8£ ¢ £ U ¨¢ I P U s " v 7 hw ¡ ¢ I d 7 £ 7 RQ # ¥ ¢ £ U ¢ I ¥ 8£ ¢ £ U i ¥ © (8)</formula><p>where ¢ £ is a vector of kernel hyperparameters. Assuming an isotropic Gaussian prior on the columns of ¥ , it can be shown that this expression simplifies to:</p><formula xml:id="formula_5">¢ I P § 8£ 3¢ £ U ¨¢ I P U % § I xp AU y " %Q 8 9 h £ &amp; £ h X ! " # &amp;% ¨' ) 21 Q 8 § 9 § &amp; 9 4 6 (9)</formula><p>where § 9 0 ¡ 4 #" 5$ '&amp; , is the I B # % U I B # % U kernel matrix constructed from 7 6 ! #" %Q # @ , and 2 is assumed to be have an isotropic Gaussian prior.</p><p>We model dynamics using both the RBF kernel of the form of Eqn. ( <ref type="formula">6</ref>), as well as the following "linear + RBF" kernel:</p><formula xml:id="formula_6">C I d ! H U ¨£ X ! # £ ¡ ¨£ Q£ # H £ Q£ ¡ W £ T &amp; H W £ Q # U W a W Y<label>(10)</label></formula><p>The kernel corresponds to representing c as the sum of a linear term and RBF terms. The inclusion of the linear term is motivated by the fact that linear dynamical models, such as 1 Conceptually, we would like to model each pair ed (f hg hd if ej lk hm as a training pair for regression with n . However, we cannot simply substitute them directly into the GP model of Eqn. <ref type="bibr" target="#b4">(5)</ref> as this leads to the nonsensical expression o l ed lp g rq sq sq sg hd lt &amp;u xd k g rq sq sq sg hd lt v k m .</p><p>first or second-order autoregressive models, are useful for many systems. Hyperparameters £ T £ ¡ represent the output scale and the inverse width of the RBF terms, and £ T represents the output scale of the linear term. Together, they control the relative weighting between the terms, while £ Q # represents the variance of the noise term X pa 7 .</p><p>It should be noted that, due to the nonlinear dynamical mapping in (3), the joint distribution of the latent coordinates is not Gaussian. Moreover, while the density over the initial state may be Gaussian, it will not remain Gaussian once propagated through the dynamics. One can also see this in (9) since 7 terms occur inside the kernel matrix, as well as outside of it. The log likelihood is not quadratic in A7 . Finally, we also place priors on the hyperparameters ( ¢ I ¢ £ U ¡ ¢ r £ Q 8 r , and</p><formula xml:id="formula_7">¢ I ¢ ¤ U ¡ ¢ r ¤ Q # r</formula><p>) to discourage overfitting. Together, the priors, the latent mapping, and the dynamics define a generative model for time-series observations (Figure <ref type="figure" target="#fig_0">1(b)</ref>):</p><formula xml:id="formula_8">¢ I § !( ¢ £ ¢ ¤ U ¨¢ I P( £ § ¢ ¤ U ¢ I P § £ d¢ £ U ¢ I ¢ £ U ¢ I ¢ ¤ U<label>(11)</label></formula><p>Multiple sequences. This model extends naturally to multiple sequences ( 4( ¤£ . Each sequence has associated latent coordinates § 4 § £ within a shared latent space.</p><p>For the latent mapping c we can conceptually concatenate all sequences within the GP likelihood (Eqn. ( <ref type="formula">5</ref>)). A similar concatenation applies for the dynamics, but omitting the first frame of each sequence from § 9 , and omitting the final frame of each sequence from the kernel matrix . The same structure applies whether we are learning from multiple sequences, or learning from one sequence and inferring another. That is, if we learn from a sequence ( , and then infer the latent coordinates for a new sequence ( ¡ , then the joint likelihood entails full kernel matrices and formed from both sequences.</p><p>Higher-order features. The GPDM can be extended to model higher-order Markov chains, and to model velocity and acceleration in inputs and outputs. For example, a second-order dynamical model, 87 G¨H GI d 87 RQ # T 4 87 RQ ¡ S ¥ VU 6W X 3a 7 <ref type="bibr" target="#b11">(12)</ref> may be used to explicitly model the dependence of the prediction on two past frames (or on velocity). In the GPDM framework, the equivalent model entails defining the kernel function as a function of the current and previous time-step:</p><formula xml:id="formula_9">C G eI s 7 4 7 Q $ R A ¦¥ ! ¦¥ Q $ TU b¨£ X ! # £ ¡ ¨£ S£ 7 # ¦¥ (£ S£ ¡ # £ T ¨£ S£ 7 Q T # ¦¥ Q T £ Q£ ¡ W £ &amp; 7 ¦¥ W £ ¨ § &amp; 7 Q T ¦¥ Q T W £ Q 8 © U 7 a ¥<label>(13)</label></formula><p>Similarly, the dynamics can be formulated to predict velocity:</p><formula xml:id="formula_10">7 RQ 8 5üH GI d 87 RQ 8 S ¥ U #W X pa 7<label>(14)</label></formula><p>Velocity prediction may be more appropriate for modeling smoothly motion trajectories.</p><p>Using Euler integration with time-step @ , we have A7 © 87 RQ 8 W 7 RQ 8 @ . The dynamics likelihood ¢ I P § 8£ 3¢ £ U can then be written by redefining § 9 ¨ ¡ # 4 " # " %Q 8 $ &amp; i @ in Eqn. <ref type="bibr" target="#b8">(9)</ref>. In this paper, we use a fixed time-step of @ ¨% . This is analogous to using #7 RQ # as a "mean function." Higher-order features can also be fused together with position information to reduce the Gaussian process prediction variance <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b8">9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Properties of the GPDM and Algorithms</head><p>Learning the GPDM from measurements ( entails minimizing the negative log-posterior:</p><formula xml:id="formula_11">¨# ¢ I P § ¢ £ ¢ ¤ £ ( U (15) ¨i ¨ £ £ #W % l' ) 1 Q # § 9 § &amp; 9 4 W q y £ y (16) # £ ¥ ¦£ #W ¡ ¨ £ £ AW % ¨' 0) 1 Q # ( 3¥ ¡ ( &amp; 4 W q y ¤ y</formula><p>up to an additive constant. We minimize with respect to § ¢ £ and ¢ ¤ numerically.</p><p>Figure <ref type="figure" target="#fig_2">2</ref> shows a GPDM 3D latent space learned from a human motion capture data comprising three walk cycles. Each pose was defined by 56 Euler angles for joints, 3 global (torso) pose angles, and 3 global (torso) translational velocities. For learning, the data was mean-subtracted, and the latent coordinates were initialized with PCA. We used 3D latent spaces for all experiments shown here. Using 2D latent spaces leads to intersecting latent trajectories. This creates large "jumps" to appear in the model, leading to unreliable dynamics. Finally, the GPDMs are learned by minimizing in (16).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># ¨</head><p>¢ ¤£ ¦¥ W a § a %a © . This shows the confidence with which the model reconstructs a pose from a latent position . In effect, the GPDM models a high probability "tube" around the data. To illustrate the latent dynamical process, Fig. <ref type="figure" target="#fig_3">2(d)</ref> shows 25 fair samples from the latent dynamics of the GPDM. All samples are conditioned on the same initial state, , and each has a length of 60 time steps. As noted above, because we marginalize over the weights of the dynamic mapping, ¥ , the distribution over a pose sequence cannot be factored into a sequence of low-order Markov transitions (Fig. <ref type="figure" target="#fig_0">1(a)</ref>). Hence, we draw fair samples § y y © ¢ I § E © 2£ 4 § !( ¢ £ U , using hybrid Monte Carlo [8]. The resulting trajectories (Fig. <ref type="figure" target="#fig_3">2(c</ref>)) are smooth and similar to the training motions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Mean Prediction Sequences</head><p>For both 3D people tracking and computer animation, it is desirable to generate new motions efficiently. Here we consider a simple online method for generating a new motion, called mean-prediction, which avoids the relatively expensive Monte Carlo sampling used above. In mean-prediction, we consider the next timestep 67 conditioned on #7 RQ 8 from the Gaussian prediction <ref type="bibr" target="#b7">[8]</ref>:</p><formula xml:id="formula_12">87 I ! I 87 RQ 8 U S ¢ ¡ I 87 RQ 8 U #" U<label>(17)</label></formula><formula xml:id="formula_13">I d AU Cg § &amp; 9 Q 8 %$ I d AU C ¢ ¡ I d AU DC I d 4 AU # $ I P AU &amp; Q 8</formula><p>&amp;$ I P AU (18) where $ I P AU is a vector containing C eI d ! r U in the ' -th entry and r is the ' 7 !( training vector. In particular, we set the latent position at each time-step to be the most-likely (mean) point given the previous step: #7 ) I P 87 RQ 8 U . In this way we ignore the process noise that one might normally add. We find that this mean-prediction often generates motions that are more like the fair samples shown in Fig. <ref type="figure" target="#fig_3">2(d)</ref>, than if random process noise had been included at each time step. Similarly, new poses are given by 1 7 0 I d 7 U .</p><p>Depending on the dataset and the choice of kernels, long sequences generated by sampling or mean-prediction can diverge from the data. On our data sets, mean-prediction trajectories from the GPDM with an RBF or linear+RBF kernel for dynamics usually produce sequences that roughly follow the training data (e.g., see the red curves in Figure <ref type="figure" target="#fig_1">3</ref>). This usually means producing closed limit cycles with walking data. We also found that meanprediction motions are often very close to the mean obtained from the HMC sampler; by initializing HMC with mean-prediction, we find that the sampler reaches equilibrium in a small number of interations. Compared to the RBF kernels, mean-prediction motions generated from GPDMs with the linear kernel often deviate from the original data (e.g., see Figure <ref type="figure" target="#fig_5">3a</ref>), and lead to over-smoothed animation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Optimization</head><p>While mean-prediction is efficient, there is nothing in the algorithm that prevents trajectories from drifting away from the training data. Thus, it is sometimes desirable to optimize a particular motion under the GPDM, which often reduces drift of the mean-prediction mo- § is then optimized directly (holding the latent positions of the previously learned latent positions, § , and hyperparam- eters, ¢ £ , fixed). To see why optimization generates motion close to the traing data, note that the variance of pose 7 ¡ 6 is determined by ¢ ¡ I d 7 U , which will be lower when 7 is nearer the training data. Consequently, the likelihood of 67 ¡ 6 can be increased by moving 87 closer to the training data. This generalizes the preference of the SGPLVM for poses similar to the examples <ref type="bibr" target="#b3">[4]</ref>, and is a natural consequence of the Bayesian approach. As an example, Fig. <ref type="figure" target="#fig_2">2</ref>(e) shows an optimized walk sequence initialized from the mean-prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Forecasting</head><p>We performed a simple experiment to compare the predictive power of the GPDM to a linear dynamical system, implemented as a GPDM with linear kernel in the latent space and RBF latent mapping. We trained each model on the first 130 frames of the 60Hz walking sequence (corresponding to 2 cycles), and tested on the remaining 23 frames. From each test frame mean-prediction was used to predict the pose 8 frames ahead, and then the RMS pose error was computed against ground truth. The test was repeated using mean-prediction and optimization for three kernels and first-order Markov dynamics: Due to the nonlinear nature of the walking dynamics in latent space, the RBF and Lin-ear+RBF kernels outperform the linear kernel. Moreover, optimization (initialized by mean-prediction) improves the result in all cases, for reasons explained above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Missing Data</head><p>The GPDM model can also handle incomplete data (a common problem with human motion capture sequences). The GPDM is learned by minimizing (Eqn. ( <ref type="formula">16</ref>)), but with the terms corresponding to missing poses 1 7 removed. The latent coordinates for missing data are initialized by cubic spline interpolation from the 3D PCA initialization of observations. While this produces good results for short missing segments (e.g., 10-15 frames of the 157 frame walk sequence used in Fig. <ref type="figure" target="#fig_2">2</ref>), it fails on long missing segments. The problem lies with the difficulty in initializing the missing latent positions sufficiently close to the training data. To solve the problem, we first learn a model with a subsampled data sequence. Reducing sampling density effectively increases uncertainty in the reconstruction process so that the probability density over the latent space falls off more smoothly from the data.</p><p>We then restart the learning with the entire data set, but with the kernel hyperparameters fixed. In doing so, the dynamics terms in the objective function exert more influence over the latent coordinates of the training data, and a smooth model is learned.</p><p>With 50 missing frames of the 157 walk sequence, this optimization produces models (Fig. <ref type="figure" target="#fig_6">4</ref>) that are much smoother than those in Fig. <ref type="figure" target="#fig_2">2</ref>. The linear kernel is able to pull the latent coordinates onto a cylinder (Fig. <ref type="figure" target="#fig_6">4b</ref>), and thereby provides an accurate dynamical model. Both models shown in Fig. <ref type="figure" target="#fig_6">4</ref> produce estimates of the missing poses that are visually indistinguishable from the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and Extensions</head><p>One of the main strengths of the GPDM model is the ability to generalize well from small datasets. Conversely, performance is a major issue in applying GP methods to larger datasets. Previous approaches prune uninformative vectors from the training data <ref type="bibr" target="#b4">[5]</ref>. This is not straightforward when learning a GPDM, however, because each timestep is highly correlated with the steps before and after it. For example, if we hold 7 fixed during opti- mization, then it is unlikely that the optimizer will make much adjustment to 27 ¡ 6 or 87 RQ 8 .</p><p>The use of higher-order features provides a possible solution to this problem. Specifically, consider a dynamical model of the form 7 ¨H GI P 7 RQ 8 7 RQ 8 U . Since adjacent time-steps are related only by the velocity 7 ¡ I P 87 # 87 RQ # U i @ , we can handle irregularly-sampled datapoints by adjusting the timestep @ , possibly using a different @ at each step.</p><p>A number of further extensions to the GPDM model are possible. It would be straightforward to include a control signal ¢ 7 in the dynamics H GI P 7 £¢ 7 U . It would also be interesting to explore uncertainty in latent variable estimation (e.g., see <ref type="bibr" target="#b2">[3]</ref>). Our use of maximum likelihood latent coordinates is motivated by Lawrence's observation that model uncertainty and latent coordinate uncertainty are interchangeable when learning PCA <ref type="bibr" target="#b4">[5]</ref>. However, in some applications, uncertainty about latent coordinates may be highly structured (e.g., due to depth ambiguities in motion tracking).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Time-series graphical models. (a) Nonlinear latent-variable model for time series. (Hyperparameters ¢ £ and ¢ ¤ are not shown.) (b) GPDM model. Because the mapping parameters ¥ and ¦ have been marginalized over, all latent coordinates § © ! #" %$ '&amp; are jointly correlated, as are all poses ( )0 1 2 3 41 #" 5$ '&amp; .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 3 )</head><label>3</label><figDesc>c 8I d S ¦ xU b¨q y y 6y I d AU</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 (</head><label>2</label><figDesc>Figure2(a) shows a 3D SGPLVM learned from walking data. Note that the latent trajectories are not smooth; there are numerous cases where consecutive poses in the walking sequence are relatively far apart in the latent space. By contrast, Fig.2(b)shows that the GPDM produces a much smoother configuration of latent positions. Here the GPDM arranges the latent positions roughly in the shape of a saddle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 (</head><label>2</label><figDesc>Figure 2(c) shows a volume visualization of the inverse reconstruction variance, i.e.,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Models learned from a walking sequence of 2.5 gait cycles. The latent positions learned with a GPLVM (a) and a GPDM (b) are shown in blue. Vectors depict the temporal sequence. (c) -log variance for reconstruction shows regions of latent space that are reconstructed with high confidence. (d) Random trajectories drawn from the model using HMC (green), and their mean (red). (e) A GPDM of walk data learned with RBF+linear kernel dynamics. The simulation (red) was started far from the training data, and then optimized (green). The poses were reconstructed from points on the optimized trajectory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 (</head><label>3</label><figDesc>Figure 3(b) shows a 3D GPDM learned from three swings of a golf club. The learning aligns the sequences and nicely accounts for variations in speed during the club trajectory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: GPDM from walk sequence with missing data learned with (a) a RBF+linear kernel for dynamics, and (b) a linear kernel for dynamics. Blue curves depict original data. Green curves are the reconstructed, missing data.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work made use of Neil Lawrence's publicly-available GPLVM code, the CMU mocap database (mocap.cs.cmu.edu), and Joe Conti's volume visualization code from mathworks.com. This research was supported by NSERC and CIAR.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<title level="m">Style machines. Proc. SIGGRAPH</title>
		<imprint>
			<date type="published" when="2000-07">July 2000</date>
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning nonlinear dynamical systems using an EM algorithm</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS 11</title>
		<meeting>NIPS 11</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="431" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gaussian process priors with uncertain inputs -application to multiple-step ahead time series forecasting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Candela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Murray-Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Style-based inverse kinematics</title>
		<author>
			<persName><forename type="first">K</forename><surname>Grochow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Popović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="522" to="531" />
			<date type="published" when="2004-08">Aug. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gaussian process latent variable models for visualisation of high dimensional data</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS 16</title>
		<meeting>NIPS 16</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Interactive control of avatars animated with human motion data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S A</forename><surname>Reitsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Pollard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="491" to="500" />
			<date type="published" when="2002-07">July 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Direct identification of nonlinear structure using Gaussian process prior models</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Leithead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Solak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Leith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Control Conference</title>
		<meeting>European Control Conference</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Information Theory, Inference, and Learning Algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mackay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Transformations of Gaussian process priors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Murray-Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Pearlmutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Glasgow University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Bayesian Learning for Neural Networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning switching linear models of human motion</title>
		<author>
			<persName><forename type="first">V</forename><surname>Pavlović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Maccormick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS 13</title>
		<meeting>NIPS 13</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="981" to="987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamical modeling with kernels for nonlinear time series prediction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ralaivola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS 16</title>
		<meeting>NIPS 16</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gaussian processes in reinforcement learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kuss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS 16</title>
		<meeting>NIPS 16</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stochastic tracking of 3D human figures using 2D motion</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sidenbladh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="702" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Derivative observations in Gaussian process models of dynamic systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Solak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Murray-Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Leithead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Leith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1033" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
