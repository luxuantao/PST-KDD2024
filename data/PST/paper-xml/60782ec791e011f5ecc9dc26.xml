<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Search to aggregate neighborhood for graph neural network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-04-14">14 Apr 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
							<email>zhaohuan@4paradigm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">4Paradigm Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
							<email>yaoquanming@4paradigm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">4Paradigm Inc</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weiwei</forename><surname>Tu</surname></persName>
							<email>tuweiwei@4paradigm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">4Paradigm Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Search to aggregate neighborhood for graph neural network</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-04-14">14 Apr 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2104.06608v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>graph neural network</term>
					<term>neural architecture search</term>
					<term>message passing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent years have witnessed the popularity and success of graph neural networks (GNN) in various scenarios. To obtain data-specific GNN architectures, researchers turn to neural architecture search (NAS), which have made impressive success in discovering effective architectures in convolutional neural networks. However, it is non-trivial to apply NAS approaches to GNN due to challenges in search space design and expensive searching cost of existing NAS methods. In this work, to obtain the data-specific GNN architectures and address the computational challenges facing by NAS approaches, we propose a framework, which tries to Search to Aggregate NEighborhood (SANE), to automatically design data-specific GNN architectures. By designing a novel and expressive search space, we propose a differentiable search algorithm, which is more efficient than previous reinforcement learning based methods. Experimental results on four tasks and seven real-world datasets demonstrate the superiority of SANE compared to existing GNN models and NAS approaches in terms of effectiveness and efficiency. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Graph neural networks (GNNs) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> have been extensively researched in the past five years, and show promising results on various graph-based tasks, e.g., node classification <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref>, recommendation <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b9">[10]</ref>, fraud detection <ref type="bibr" target="#b10">[11]</ref>, chemistry <ref type="bibr" target="#b11">[12]</ref> and travel data analysis <ref type="bibr" target="#b12">[13]</ref>. In the literature, various GNN architectures <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b17">[18]</ref> have been designed for different tasks, and most of these approaches are relying on a neighborhood aggregation (or message passing) schema <ref type="bibr" target="#b11">[12]</ref> (see the example in Figure <ref type="figure" target="#fig_0">1</ref>(a) and (b)). Despite the success of GNN models, they are facing a major challenge. That is there is no single model that can perform the best on all tasks and no optimal model on different datasets even for the same task (see the experimental results in Table <ref type="table" target="#tab_4">VI</ref>). Thus given a new task or dataset, huge computational and expertise resources would be invested to find a good GNN architecture, which limits the application of GNN models. Moreover, existing GNN models do not make full use of the best architecture design practice. For example, existing GNN models tend to stack multiple layers with the same aggregation function to aggregate hidden features of neighbors, however, it remains to be seen whether different combinations of aggregation functions can further improve the performance. In one word, it is left to be explored whether we can obtain 1 Code is available at: https://github.com/AutoML-4Paradigm/SANE. Correspondence is to Q. <ref type="bibr">Yao.</ref> data-specific GNN architectures beyond existing ones. This problem is dubbed architecture challenge.</p><p>To address this architecture challenge, researchers turn to neural architecture search (NAS) <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, which has been a hot topic since it shows promising results in automatically designing novel and better neural architectures beyond humandesigned ones. For example, in computer vision, the searched architectures by NAS can beat the state-of-the-art humandesigned ones on CIFAR-10 and ImageNet datasets by a large margin <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Motivated by such a success, very recently, two preliminary works, GraphNAS <ref type="bibr" target="#b22">[23]</ref> and Auto-GNN <ref type="bibr" target="#b23">[24]</ref>, made the first attempt to tackle the architecture challenge in GNN with the NAS approaches. However, it is non-trivial to apply NAS to GNN. One of the key components of NAS approaches <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> is the design of search space, i.e., defining what to search, which directly affects the effectiveness and efficiency of the search algorithms. A natural search space is to include all hyper-parameters related to GNN models, e.g., the hidden embedding size, aggregation functions, and number of layers, as done in GraphNAS and Auto-GNN. However, this straightforward design of search space in GraphNAS and Auto-GNN have two problems. The first one is that various GNN architectures, e.g., GeniePath <ref type="bibr" target="#b17">[18]</ref>, or JK-Network <ref type="bibr" target="#b16">[17]</ref>, are not included, thus the best performance might not be that good. The second one is that it makes the architecture search process too expensive in GraphNAS/Auto-GNN by incorporating too many hyper-parameters into the search space. In NAS literature, it remains a challenging problem to design a proper search space, which should be expressive (large) and compact (small) enough, thus a good balance between accuracy and efficiency can be achieved.</p><p>Besides, existing NAS approaches for GNNs are facing an inherent challenge, which is that they are extremely expensive due to the trial-and-error nature, i.e., one has to train from scratch and evaluate as many as possible candidate architectures over the search space before obtaining a good one <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref>. Even in small graphs, in which most existing human-designed GNN architectures are tuned, the search cost of NAS approaches, i.e., GraphNAS and Auto-GNN, can be quite expensive. This challenge is dubbed computational challenge.</p><p>In this work, to address the architecture and computational challenges, we propose a novel NAS framework, which tries to Search to Aggregate NEighborhood (SANE) for automatic architecture search in GNN. By revisiting extensive GNN models, we define a novel and expressive search space, which can emulate more human-designed GNN architectures than existing NAS approaches, i.e., GraphNAS and Auto-GNN. To accelerate the search process, we adopt the advanced oneshot NAS paradigm <ref type="bibr" target="#b26">[27]</ref>, and design a differentiable search algorithm, which trains a supernet subsuming all candidate architectures, thus greatly reducing the computational cost. We further conduct extensive experiments on three types of tasks, including transductive, inductive, and database (DB) tasks, to demonstrate the effectiveness and efficiency of the proposed framework. To summarize, the contributions of this work are in the following:</p><p>? In this work, to address the architecture challenge in GNN models, we propose the SANE framework based on NAS. By designing a novel and expressive search space, SANE can emulate more human-designed GNN architectures than existing NAS approaches. ? To address the computational challenge, we propose a differentiable architecture search algorithm, which is efficient in nature comparing to the trial-and-error based NAS approaches. To the best of our knowledge, this is the first differentiable NAS approach for GNN. ? Extensive experiments on five real-world datasets are conducted to compare SANE with human-designed GNN models and NAS approaches. The experimental results demonstrate the superiority of SANE in terms of effectiveness and efficiency compared to all baseline methods. Notations. Formally, let G = (V, E) be a simple graph with node features X ? R N ?d , where V and E represent the node and edge sets, respectively. N represents the number of the nodes, and d is the dimension of node features, We use N (v) to represent the first-order neighbors of a node v in G, i.e., N (v) = {u ? V|(v, u) ? E}. In the literature, we further create a new set N (v), which is the neighbor set including itself, i.e., N (v) = {v} ? {u ? V|(v, u) ? E}. A new graph G is always created by adding self-loop to every v ? V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Graph Neural Network (GNN)</head><p>GNN was first proposed in <ref type="bibr" target="#b1">[2]</ref> and many of its variants <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b17">[18]</ref> have been proposed in the past five years. Generally, these GNN models can be unified by a neighborhood aggregation or message passing schema <ref type="bibr" target="#b11">[12]</ref>, where the representation of each node is learned by iteratively aggregating the embeddings ("message") of its neighbors. A typical K-layer GNN in the neighborhood aggregation schema can be written as follows (see the illustrative example in Figure <ref type="figure" target="#fig_0">1</ref>(a) and (b)): the l-th layer</p><formula xml:id="formula_0">(l = 1, ? ? ? , k) updates h v for each node v as h l v = ?(W l ? AGG node ({h l-1 u , ?u ? N (v)})),<label>(1)</label></formula><p>where h l v ? R d l represents the hidden features of a node v learned by the l-th layer, and d l is the corresponding dimension. W l is a trainable weight matrix shared by all nodes in the graph, and ? is a non-linear activation function, e.g., sigmoid or ReLU. AGG node is the key component, i.e., a predefined aggregation function, which varies across different GNN models. For example, a weighted summation function is designed as AGG node in <ref type="bibr" target="#b13">[14]</ref>, and different functions, e.g., mean and max-pooling, are proposed as the node aggregators in <ref type="bibr" target="#b2">[3]</ref>. Further, to weigh the importance of different neighbors, attention mechanism is incorporated to design the node aggregators <ref type="bibr" target="#b3">[4]</ref>. For more details of different node aggregators, we refer readers to Table XI in Appendix B.</p><p>Motivated by the success of residual network <ref type="bibr" target="#b27">[28]</ref>, residual mechanisms are incorporated to improve the performance of GNN models. In <ref type="bibr" target="#b28">[29]</ref>, two simple residual connections are designed to improve the performance of the vanilla GCN model. And in <ref type="bibr" target="#b16">[17]</ref>, skip-connections are used to propagate message from intermediate layers to the last layer, then the final representation of the node v is computed by a layer aggregator as</p><formula xml:id="formula_1">z v = AGG layer (h 1 v , ? ? ? , h K v )</formula><p>, where AGG layer can be different operations, e.g., max-pooling, concatenation. In this way, neighbors in long ranges are used together to learn the final representation of each node, and the performance gain is reported with the layer aggregator <ref type="bibr" target="#b16">[17]</ref>. With the node and layer aggregators, we point out the two key components of exiting GNN models, i.e., the neighborhood aggregation function and the range of the neighborhood, which are tuned depending on the tasks <ref type="bibr" target="#b16">[17]</ref>. In Section III-A, we will introduce the search space of SANE based on these two components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Neural Architecture Search (NAS)</head><p>Neural architecture search (NAS) <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b29">[30]</ref> aims to automatically find unseen and better architectures comparing to expert-designed ones, which have shown promising results in searching for convolutional neural networks (CNN) <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. Early NAS approaches follow a trial-and-error pipeline, which firstly samples a candidate architecture from a pre-defined search space, then trains it from scratch, and finally gets the validation accuracy. This process is repeated many times before obtaining an architecture with satisfying performance. Representative methods are reinforcement learning (RL) algorithms <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, which are inherently time-consuming to train thousands of candidate architectures during the search process. To address the efficiency problem, a series of methods adopt weight sharing strategy to reduce the computational cost. To be specific, instead of training one by one thousands of separate models from scratch, one can train a single large network (supernet) capable of emulating any architecture in the search space. Then each architecture can inherit the weights from the supernet, and the best architecture can be obtained more efficiently. This paradigm is also referred to as "one-shot NAS", and representative methods are <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b35">[36]</ref>.</p><p>Recently, to obtain data-specific GNN architectures, several works based on NAS were proposed. GraphNAS <ref type="bibr" target="#b22">[23]</ref> and Auto-GNN <ref type="bibr" target="#b23">[24]</ref> made the first attempt to introduce NAS into GNN. In <ref type="bibr" target="#b36">[37]</ref>, an evolutionary-based search algorithm is proposed to search architectures on top of the Graph Convolutional Network (GCN) <ref type="bibr" target="#b13">[14]</ref> model, and action recognition problem is considered. However, in this work, we focus on the node representation learning problem, which is an important one in GNN literature. In <ref type="bibr" target="#b37">[38]</ref>, a RL-based method is proposed to search for node-specific layer numbers given a GNN model, e.g., GCN or GAT, and in <ref type="bibr" target="#b38">[39]</ref>, propagation matrices in the message passing framework are searched. These two works can be regarded as orthogonal works of our framework. For GraphNAS and Auto-GNN, they are RL-based methods, thus very expensive in nature. Besides, the search space of the proposed SANE is more expressive than those of GraphNAS and Auto-GNN. In <ref type="bibr" target="#b39">[40]</ref>, the search space of GraphNAS is further simplified by introducing node and layer aggregators. However, the search method is still RL-based one. Further, to address the computational challenges, we design a differentiable search algorithm based on the one-shot method <ref type="bibr" target="#b26">[27]</ref>. To the best of our knowledge, this is the first differentiable NAS approach for architecture search in GNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED FRAMEWORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Search Space Design</head><p>In the literature <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, designing a good search space is very important for NAS approaches. On one hand, a good search space should be large and expressive enough to emulate various existing GNN models, thus ensures the competitive performance (see Table <ref type="table" target="#tab_4">VI</ref>). On the other hand, the search space should be small and compact enough for the sake of computational resources, i.e., searching time (see Table <ref type="table" target="#tab_6">VII</ref>). In the well-established work <ref type="bibr" target="#b15">[16]</ref>, the authors shows that the expressive capability is dependent on the properties of different aggregation functions, thus to design an expressive yet simplified search space, we focus on two key important components: node and layer aggregators, which are introduced in the following: TABLE II: Comparisons between existing GNN models and the proposed SANE. Note that the variants of attention is from <ref type="bibr" target="#b22">[23]</ref>, which represents different aggregation methods despite that they all use the attention mechanism. To show the expressive capability of the designed search space, here we further give a detailed comparison between SANE and existing GNN models in Table <ref type="table" target="#tab_0">II</ref>, from which we can see that SANE can emulate existing models. Besides, we also discuss the connections between SANE and more recent advanced GNN baselines in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Differentiable Architecture Search</head><p>In this part, we first introduce how to represent the search space of SANE as a supernet, which is a directed acyclic graph (DAG) in Figure <ref type="figure" target="#fig_0">1</ref>(c), and then how to use gradient descent for the architecture search.</p><p>1) Continuous Relaxation of the Search Space: Assume we use a K-layer GNN with JK-Network as backbone (K = 3 in Figure <ref type="figure" target="#fig_0">1(c)</ref>), and the supernet has K + 3 nodes, where each node x l is a latent representation, e.g., the input features of a node, or the embeddings in the intermediate layers. Each directed edge (i, j) is associated with an operation o ij that transforms x l , e.g., GAT aggregator. Without loss of generality, we have one input node, one output node, and one node representing the set of all skipped intermediated layers, thus we have K + 3 node for the supernet in total. Then the task is transformed to find a proper operation on each edge, leading to a discrete search space, which is difficult in nature.</p><p>Motivated by the differentiable architecture search in <ref type="bibr" target="#b26">[27]</ref>, we relax the categorical choice of a particular operation to a softmax over all possible operations:</p><formula xml:id="formula_2">?ij (x) = o?O exp(? ij o ) o ?O exp(? ij o ) o(x),<label>(2)</label></formula><p>where the operation mixing weights for a pair of nodes (i, j) are parameterized by a vector ? ij ? R |O| , and O is chosen from the three operation sets: O n , O l , O s as introduced in Section III-A. Then we have the corresponding ? n , ? l , ? s .</p><p>x represents the input hidden features for a GNN layer, e.g., {h (l-1) u , ?u ? N (v)}. Let ?n , ?s and ?l are the mixed operations from O n , O s , O l based on (2), respectively, and we remove the superscript of o ij for simplicity when there is no misunderstanding. Then given a node v in the graph, the neighborhood aggregation process by SANE is</p><formula xml:id="formula_3">h l v = ?(W l n ? ?n ({h l-1 u , ?u ? N (v)})),<label>(3)</label></formula><p>where W l n is shared by candidate architectures from the search space by each node aggregator. Then, for the last layer for the node v, the embeddings can be computed by</p><formula xml:id="formula_4">H K+1 v = ?s (h 1 v ), ? ? ? , ?s (h K v ) ,<label>(4)</label></formula><formula xml:id="formula_5">z v = ?l H K+1 v ,<label>(5)</label></formula><p>where [?] represents we stack all the embeddings from K intermediate layers for the last layer. From the above equations, we can see that the computing process is the summation of all operations, i.e., aggregator or skip, from the corresponding set, which is what a "supernet" means. After we obtain the final representation of the node z v , we can inject it to different types of loss depending on the given task. Thus, SANE is to solve a bi-level optimization problem as:</p><formula xml:id="formula_6">min ??A L val (w * (?), ?),<label>(6)</label></formula><formula xml:id="formula_7">s.t. w * (?) = arg min w L tra (w, ?),<label>(7)</label></formula><p>where L tra and L val are the training and validation loss, respectively. ? = {? n , ? s , ? l } represents a network architecture, and w * (?) the corresponding weights after training. In the experiments, we focus on the node classification task, thus cross-entropy loss is used. Thus, the task of architecture search by SANE is to learn three continuous variables ? n , ? s , ? l with each ? = {? ij }.</p><p>2) Optimization by gradient descent: We can observe that the SANE problem is a bi-level optimization problem, where the architecture parameters ? is optimized on validation data (i.e., ( <ref type="formula" target="#formula_6">6</ref>)), and the network weights w is optimized on training data (i.e., <ref type="bibr" target="#b6">(7)</ref>). With the above continuous relaxation, the advantage is that the recently developed one-shot NAS approach <ref type="bibr" target="#b26">[27]</ref> can be applied.</p><p>The optimizing detail is given in Algorithm 1. Specifically, following <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b31">[32]</ref>, we give the gradient-based approximation to update the architecture parameters, i.e.,</p><formula xml:id="formula_8">? ? L val (w * (?), ?) ? ? ? L val (w -?? w L tra (w, ?), ?), (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>where w is the current weight, and ? is the learning rate for a step of inner optimization in <ref type="bibr" target="#b6">(7)</ref> for w. Thus, instead of obtaining the optimized w * (?), we only need to approximate it by adapting w using only a single training step. After training, we retain the top-k strongest operations, i.e, the largest weights according (2), and form the complete architecture with the searched operations. Then the searched architecture is retrained from scratch and tuned on the validation data to obtain the best hyper-parameters. Note that in the experiment, we set k = 1 for simplicity, which means that a discrete architecture is obtained by replacing each mixed operation ?ij with the operation of the largest weight, i.e, o ij = arg max o?O ? ij o .</p><p>Algorithm 1 SANE -Search to Aggregate NEighborhood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Require:</head><p>The search space A, the number of top architectures k, the epochs T for search. Ensure: The k searched architectures A k .</p><p>1:</p><formula xml:id="formula_10">while t = 1, ? ? ? , T do 2:</formula><p>Compute the validation loss L val ;</p><p>3:</p><p>Update ? n , ? s and ? l by gradient descend rule <ref type="bibr" target="#b7">(8)</ref> with ( <ref type="formula" target="#formula_3">3</ref>), ( <ref type="formula" target="#formula_4">4</ref>) and ( <ref type="formula" target="#formula_5">5</ref>) respectively; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with Existing NAS Methods</head><p>In this part, we further give a comparison between SANE and GraphNAS <ref type="bibr" target="#b22">[23]</ref>, Auto-GNN <ref type="bibr" target="#b23">[24]</ref>, and Policy-GNN <ref type="bibr" target="#b37">[38]</ref>, which are the latest NAS methods for GNN in node-level representation learning. The results are shown in Table <ref type="table" target="#tab_3">III</ref>. We can see that the advantages of the expressive search space and differentiable search algorithm are evident. Besides, Policy-GNN focuses on searching for the number of layers given a GNN backbone, which can be regarded as an orthogonal work of SANE, then here we discuss more about the comparisons between GraphNAS/Auto-GNN and SANE.</p><p>The key difference is that SANE does not include parameters like hidden embedding size, number of attention heads, which tends to be called hyper-parameters of GNN models. The underlying philosophy is that the expressive capability of GNN models is mainly relying on the properties of the aggregation functions, as shown in the well-established work <ref type="bibr" target="#b15">[16]</ref>, thus we focus on including more node aggregators to guarantee as powerful as possible the searched architectures. The layer aggregators are further included to alleviate the oversmoothing problems in deeper GNN models <ref type="bibr" target="#b16">[17]</ref>. Moreover, this simplified and compact search space has a side advantage, which is that the search space is made smaller in orders, thus the cost of architecture search is reduced in orders. For example, when considering the search space for a 3-layer GNN in our experiments, the total number of architectures in the search space is 11 3 ? 2 3 ? 3 = 31, 944. While in Auto-GNN, there are (14112) 3 ? 2.8 ? 10 12 candidate architectures to be searched <ref type="bibr" target="#b23">[24]</ref>. Finally, since the hyper-parameters are tuned by retraining the derived GNN architectures by Algorithm 1, which is also a standard practice in CNN architecture search <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b33">[34]</ref>, SANE actually decouples the architecture search and hyper-parameters tuning, while GraphNAS/Auto-GNN mix them up. In Section IV-E3, we show that by running GraphNAS over the search space of SANE, the performance can be improved, which means that better architectures can be obtained given the same time budget, thus demonstrating the advantages of the decoupling process as well as the simplified and compact search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT</head><p>In this section, we conduct extensive experiments to demonstrate the superiority of the propose SANE in three tasks: transductive task, inductive task, and DB task.</p><p>A. Experimental Settings 1) Datasets and Tasks: Here, we introduce details of different tasks and corresponding datasets (Table <ref type="table" target="#tab_4">IV</ref>). Note that transductive and inductive tasks are standard ones in the literature <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>. We further add one popular database (DB) task, entity alignment in cross-lingual knowledge base (KB), to show the capability of SANE in broader domains. Transductive Task. Only a subset of nodes in one graph is allowed to access as training data, and other nodes are used as validation and test data. For this setting, we use three benchmark datasets: Cora, CiteSeer, PubMed. They are all citation networks, provided by <ref type="bibr" target="#b40">[41]</ref>. Each node represents a paper, and each edge represents the citation relation between two papers. The dataset contains bag-of-words features for each paper (node), and the task is to classify papers into different subjects based on the citation networks. We split the nodes in all graphs into 60%, 20%, 20% for training, validation, and test. Inductive Task. In this task, we use a number of graphs as training data, and other completely unseen graphs as validation/test data. For this setting, we use the PPI dataset, provided by <ref type="bibr" target="#b2">[3]</ref>, on which the task is to classify protein functions. PPI DB Task. For the DB task, we choose the cross-lingual entity alignment, which matches entities referring to the same instances in different languages in two KBs . In the literature <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, GNN methods have been incorporated to this task to make use of the structure information underlying the cross-lingual KBs. We use the DBLP15K datasets built by <ref type="bibr" target="#b43">[44]</ref>, which were generated from DBpedia, a large-scale multi-lingual KB containing rich inter-language links between different language versions. We choose the subset of Chinese and English in our experiments, and the statistics of the dataset are in Table <ref type="table" target="#tab_4">V</ref>. For the experimental setting, we follow <ref type="bibr" target="#b41">[42]</ref> and use 30% of inter-language links for training, 10% for validation and the remaining 60% for test. For the evaluation metric, we use Hits@k to evaluate the performance of SANE. For the sake of space, we refer readers to <ref type="bibr" target="#b6">[7]</ref> for more details.</p><p>2) Compared Methods: We compare SANE with two groups of state-of-the-art methods: human-designed GNN architectures and NAS approaches. Human-designed GNNs. As shown in Table <ref type="table" target="#tab_0">II</ref>, the humandesigned GNN architectures are: GCN <ref type="bibr" target="#b13">[14]</ref>, GraphSAGE <ref type="bibr" target="#b2">[3]</ref>, GAT <ref type="bibr" target="#b3">[4]</ref>, GIN <ref type="bibr" target="#b15">[16]</ref>, LGCN <ref type="bibr" target="#b14">[15]</ref>, and GeniePath <ref type="bibr" target="#b26">[27]</ref>. For models with variants, like different aggregators in GraphSAGE, we report the best performance across the variants. Besides, we add JK-Network to all models except for LGCN, and obtain 5 more baselines: GCN-JK, GraphSAGE-JK, GAT-JK, GIN-JK, GeniePath-JK. For LGCN, we use the code released by the authors 2 , and for other baselines, we use the popular  open-source library PyTorch Geometric (PyG) <ref type="bibr" target="#b44">[45]</ref>, which implements various GNN models. For all baselines, we train it from scratch with the obtained best hyper-parameters on validation datasets, and get the test performance. We repeat this process 5 times, and report the final mean accuracy with standard deviation. NAS approaches for GNN. We consider the following methods: (i). Random search (denoted as "Random") <ref type="bibr" target="#b45">[46]</ref>: a simple baseline in NAS, which uniform randomly samples architectures from the search space; (ii). Bayesian optimization<ref type="foot" target="#foot_1">3</ref> (denoted as "Bayesian") <ref type="bibr" target="#b46">[47]</ref>: a popular sequential model-based global optimization method for hyper-parameter optimization, which uses tree-structured Parzen estimator as the measurement for expected improvement; (iii). GraphNAS<ref type="foot" target="#foot_2">4</ref>  <ref type="bibr" target="#b22">[23]</ref>, a RLbased NAS approach for GNN, which has two variants based on the adoption of weight sharing mechanism. We denoted as GraphNAS-WS the one using weight sharing. Note that Auto-GNN <ref type="bibr" target="#b23">[24]</ref> is not compared for three reasons: 1) the search spaces of Auto-GNN and GraphNAS are actually the same; 2) both of these two works use the RL method; 3) the code of Auto-GNN is not publicly available. Random and Bayesian are searching on the designed search space of SANE, where a GNN architecture is sampled from the search space, and trained till convergence to obtain the validation performance. 200 models are sampled in total and the architecture with the best validation performance is trained from scratch, and do some hyper-parameters tuning on the validation dataset, and obtain the test performance. For Graph-NAS, we set the epoch of training the RL-based controller to 200, and in each epoch, a GNN architecture is sampled, and trained for enough epochs (600 ? 1000 depending on datasets), update the parameters of RL-based controller. In the end, we sample 10 architectures and collect the top 5 architectures that achieve the best validation accuracy. Then the best architecture is trained from scratch. Again, we do some hyper-parameters tuning based on the validation dataset, and report the best test performance. Note that we repeat the TABLE VI: Performance comparisons of transductive and inductive tasks. For transductive task, we use the mean classification accuracy (with standard deviation) as the evaluation metric, and for inductive task, we use Micro-F1 (with standard deviation) as evaluation metric. We categorize baselines into human-designed architectures and NAS approaches. The best results in different groups of baselines are underlined, and the best result on each dataset is in boldface. re-training of the architecture for five times, and report the final mean accuracy with standard deviation.</p><p>3) Implementation details of SANE: Our experiments are running with Pytorch (version 1.2) <ref type="bibr" target="#b47">[48]</ref> on a GPU 2080Ti (Memory: 12GB, Cuda version: 10.2). We implement SANE on top of the building code provided by DARTS <ref type="foot" target="#foot_3">5</ref> and PyG (version 1.2) <ref type="foot" target="#foot_4">6</ref> . More implementing details are given in Appendix C. Note that we set ? = 0 in (8) in our experiments, which means we are using first-order approximation as introduced in <ref type="bibr" target="#b26">[27]</ref>. It is more efficient and the performance is good enough in our experiments. For all tasks, we run the search process for 5 times with different random seeds, and retrieve top-1 architecture each time. By collecting the best architecture out of the 5 top-1 architectures on validation datasets, we repeat 5 times the process in re-training the best one, finetuning hyper-parameters on validation data, and reporting the test performance. Again, the final mean accuracy with standard deviations are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance on Transductive and Inductive Tasks</head><p>The results of transductive and inductive tasks are given in Table <ref type="table" target="#tab_4">VI</ref>, and we give detailed analyses in the following.</p><p>1) Transductive Task: Overall, we can see that SANE consistently outperforms all baselines on three datasets, which demonstrates the effectiveness of the searched architectures by SANE. When looking at the results of human-designed architectures, we can first observe that GCN and Graph-SAGE outperform other more complex models, e.g., GAT or GeniePath, which is similar to the observation in the paper of GeniePath <ref type="bibr" target="#b17">[18]</ref>. We attribute this to the fact that these three graphs are not that large, thus the complex aggregators might be easily prone to the overfitting problem. Besides, there is no absolute winner among human-designed architectures, which further verifies the need of searching for data-specific architectures. Another interesting observation is that when adding JK-Network to base GNN architectures, the performance increases consistently, which aligns with the experimental results in JK-Network <ref type="bibr" target="#b16">[17]</ref>. It demonstrates the importance of introducing the layer aggregators into the search space of SANE.</p><p>On the other hand, when looking at the performance of NAS approaches, the superiority of SANE is also clear from the gain on performance. Recall that, Random Bayesian and GraphNAS all search in a discrete search space, while SANE searches in a differentiable one enabled by Equation ( <ref type="formula" target="#formula_2">2</ref>). This shows a differentiable search space is easier for search algorithms to find a better local optimal. Such an observation is also previously made in <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b31">[32]</ref>, which search for CNN in a differentiable space.</p><p>2) Inductive Task: We can see a similar trend in the inductive task that SANE performs consistently better than all baselines. However, among human-designed architectures, the best two are GAT and GeniePath with JK-network, which is not the same as that from the transductive task. This further shows the importance to search data-specific GNN architectures.</p><p>3) Searched Architectures: We visualize the searched architectures (top-1) by SANE on different datasets in Figure <ref type="figure">2</ref>. Fig. <ref type="figure">2:</ref> The searched architectures by SANE on different datasets. Note that on CiteSeer, the skip connection is removed during the search process, thus we make it in light gray for clear presentation.</p><p>As can be seen, first, these architecture are data-dependent and new to the literature. Then, searching for skip-connections indeed make a difference, as the last GNN layer in Figure <ref type="figure">2</ref>(b) and the middle layer in Figure <ref type="figure">2</ref>(c) do not connect to the output layer. Finally, as attention-based node aggregators are more expressive than non-attentive ones, thus GAT (and its variants) are more popularly used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Search Efficiency</head><p>In this part, we compare the efficiency of SANE and NAS baselines by showing the test accuracy w.r.t the running time on transductive and inductive tasks. From Figure <ref type="figure" target="#fig_4">3</ref>, we can observe that the efficiency improvements are in orders of magnitude, which aligns with the experiments in previous one-shot NAS approaches, like DARTS <ref type="bibr" target="#b26">[27]</ref> and NASP <ref type="bibr" target="#b31">[32]</ref>. Besides, as in Section IV-B, we can see that SANE can obtain an architecture with higher test accuracy than random search, Bayesian, and GraphNAS.</p><p>To further show the efficiency improvements, we record the search time of each method to obtain an architecture, where the epochs of SANE and GraphNAS are set to 200, and the number of trial-and-error process in Random and Bayesian is set to 200 as well, i.e., explore 200 candidate architectures, and the results are given in Table <ref type="table" target="#tab_6">VII</ref>. The search time cost of SANE is two orders of magnitude smaller than those of NAS baselines, which further demonstrates the superiority of SNAE in terms of search efficiency. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. DB task</head><p>Since DB task is different from the benchmark tasks, we adjust the settings of SANE following the proposed GCN-TABLE VIII: The results of DB task. We use Hits@K as the evaluation metric, and the results of JAPE and GCN-Align are from <ref type="bibr" target="#b41">[42]</ref>. Note that JAPE is the variant using the same features as GCN-Align. Align <ref type="bibr" target="#b41">[42]</ref>, which uses two 2-layer GCN in their experiments.</p><p>To be specific, we set the number of layers to 2, which is different from that in the transductive task, and remove the layer aggregator because we observe that in our experiments the performance decreases when simply adding the layer aggregator to the GCN architecture in <ref type="bibr" target="#b41">[42]</ref>. Therefore, we use SANE to search for different combinations of node aggregators for the entity alignment task, and the results are shown in Table <ref type="table" target="#tab_6">VIII</ref>. We can see that the performance of SANE is better than GCN-Align and JAPE, which demonstrates the effectiveness of SANE on the entity alignment task. We further emphasize the following observations:</p><p>? The performance gains of SANE compared to GCN-Align is evident, which demonstrates the advantage of different combinations of node aggregators. And The searched architecture is "GAT-GeniePath", and more hyper-parameters are given in Appendix C. ? The experimental results show the capability of SANE in broader domains. A taking-away message is that SANE can further improve the performance of a task where a regular GNN model, e.g., GCN, can work. We notice that there are following works of GCN-Align, e.g., <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b48">[49]</ref>, however their modifications are orthogonal to node aggregators, thus they can be integrated with SANE to further improve the performance. We leave this for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Study</head><p>1) The influence of differentiable search: In Algorithm 1 in Section III-B, we can see that during the search process,   the ? is updated w.r.t. to the validation loss L and used to update the weights of the mixed operations in (3) to ( <ref type="formula" target="#formula_4">4</ref>).</p><p>Here we introduce a random explore parameter , which is the probability that we randomly sample a single operation in each edge of the supernet, and only update the weights corresponding to the sampled operations. Then when = 0, the algorithm is the same to Algorithm 1, and when = 1, it is equivalent to random search with weight sharing. Thus we can show the influence of the differentiable search algorithm by varying . In this part, we conduct experiments by varying ? [0, 0.2, 0.5, 0.9, 1.0], and show the performance trending in Figure <ref type="figure" target="#fig_6">4</ref>(a) on the transductive and inductive tasks.</p><p>From Figure <ref type="figure" target="#fig_6">4</ref>(a), on all four datasets, we can see that the test accuracy decreases with the increasing of , and arrives the worst when = 1.0. This means that the gradient descent method outperforms random sampling for architecture search. In other words, it demonstrates the effectiveness of the proposed differentiable search algorithm in Section III-B.  2) The influence of K: In Section IV, we choose a 3layer GNN (K=3) as the backbone in our experiments for its empirically good performance. In this work, we focus on searching for shallow GNN architectures (K ? 6). In this part, we conduct experiments by varying K ? <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, and show the performance trending in Figure <ref type="figure" target="#fig_6">4</ref>(b). We can see that with the increase of K, the test accuracy increases firstly and then decreases, which aligns with the motivation of setting K = 3 in the experiments in Section IV-B.</p><p>3) The efficacy of the designed search space: In Section III-C, we discuss the advantages of search space between SANE and GraphNAS/Auto-GNN. In this part, we conduct experiments to further show the advantages. To be specific, we run GraphNAS over its own and SANE's search space, given the same time budget (20 hours), and compare the final test accuracy of the searched architectures in Table <ref type="table" target="#tab_8">IX</ref>. From Table <ref type="table" target="#tab_8">IX</ref>, we can see that despite the simplicity of the search space, SANE can obtain better or at least close accuracy compared to GraphNAS, which means better architectures can be obtained given the same time budget, thus demonstrating the efficacy of the designed search space. 4) Failure of searching for universal approximators: In this part, we further show the performance of searching Multi-Layer Perception (MLP) as node aggregators since it is a universal function approximator: any continuous function on a compact set can be approximated with a large enough MLP <ref type="bibr" target="#b49">[50]</ref>. And in <ref type="bibr" target="#b15">[16]</ref>, Xu et al. shows that using MLP aggregators can be as powerful as the Weisfeiler-Lehman (WL) test under some conditions, which upper bounds the expressive capability of existing GNN models. However, in practice, it is very challenging to obtain satisfying performance by MLPs without prior knowledge, since it is too general to design the MLP of a suitable structure given a task. This motivates us to incorporate various existing GNN models as node aggregators in our search space (Table <ref type="table" target="#tab_0">I</ref>), thus the performance </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this work, to address the architecture and computational challenges facing existing GNN models and NAS approaches, we propose to Search to Aggregate NEighborhood (SANE) for graph neural architecture search. By reviewing various humandesigned GNN architectures, we define an expressive search space including node and layer aggregators, which can emulate more unseen GNN architectures beyond existing ones. A differentiable architecture search algorithm is further proposed, which leads to a more efficient search algorithm than existing NAS methods for GNNs. Extensive experiments are conducted on five real-world datasets in transductive, inductive, and DB tasks. The experimental results demonstrate the superiority of SANE comparing to GNN and NAS baselines in terms of effectiveness and efficiency.</p><p>For future directions, we will explore more advanced NAS approaches to further improve the performance of SANE. Besides, we can explore beyond node classification tasks and focus on more graph-based tasks, e.g., the whole graph classification <ref type="bibr" target="#b50">[51]</ref>. In these cases, different graph pooling methods can be searched for the whole graph representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ACKNOWLEDGMENTS</head><p>This work is supported by National Key R&amp;D Program of China (2019YFB1705100). We also thank Lanning Wei to implement several experiments in this work. We further thank all anonymous reviewers for their constructive comments, which help us to improve the quality of this manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Discussion about recent GNN baselines</head><p>During the period of preparing for this work, we noticed that there were some new GNN models proposed in the literature, e.g., Geom-GCN <ref type="bibr" target="#b51">[52]</ref>, GraphSaint <ref type="bibr" target="#b52">[53]</ref>, DropEdge <ref type="bibr" target="#b53">[54]</ref>, and PariNorm <ref type="bibr" target="#b54">[55]</ref>. We did not include these works as baselines, since they can be regarded as orthogonal works of SANE to the GNN literature.</p><p>To be specific, as shown in Eq. ( <ref type="formula" target="#formula_0">1</ref>), the embedding of a node v in the l-th layer of a K-layer GNN is computed as:</p><formula xml:id="formula_11">h l v = ?(W l ? AGG node ({h l-1 u , ?u ? N (v)})</formula><p>). From this computation process, we can summarize four key components of a GNN model: aggregation function (AGG node ), number of layers (l), neighbors ( N (v)), and hyper-parameters (?, dimension size, etc.), which decide the properties of a GNN model, e.g., the model capacity, expressive capability, and prediction performance.</p><p>SANE mainly focus on the aggregation functions, which affect the expressive capability of GNN models. GraphSaint mainly focuses on neighbors selection in each layer, thus the "neighbor explosion" problem can be addressed. Geom-GCN also focuses on neighbors selection, which constructs a novel neighborhood set in the continuous space, thus the structural information can be utilized. DropEdge mainly focuses on the depth of a GNN model, i.e., the number of layers, which can alleviate the over-smoothing problem with the increasing of the number of GNN layers. Besides the three works, there are more other works on the four key components, like MixHop <ref type="bibr" target="#b55">[56]</ref> integrating neighbors of different hops in a GNN layer, or PairNorm <ref type="bibr" target="#b54">[55]</ref> working on the depth of a GNN models. Therefore, all these works can be integrated as a whole to improve each other. For example, the DropEdge or Geom-GCN methods can further help SANE in constructing more powerful GNN models. This is what we mean "orthogonal" works of SANE. Since we mainly focus on the aggregation functions in this work, we only compare the GNN variants with different aggregations functions. We believe the application of NAS to GNN has unique values, and the proposed SANE can benefit the GNN community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details of Node Aggregators</head><p>As introduced in Section III-A, we have 11 types of node aggregators, which are based on well-known existing GNN models: GCN <ref type="bibr" target="#b13">[14]</ref>, GraphSAGE <ref type="bibr" target="#b2">[3]</ref>, GAT <ref type="bibr" target="#b3">[4]</ref>, GIN <ref type="bibr" target="#b15">[16]</ref>, and GeniePath <ref type="bibr" target="#b17">[18]</ref>. Here we give key explanations to these node aggregators in Table <ref type="table" target="#tab_9">XI</ref>. For more details, we refer readers to the original papers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Implementing Details</head><p>In this part, we give more implementation details of all methods including GNN baselines, NAS baselines, and SANE.</p><p>? For all GNN baselines, we use the Adam optimizer, and set learning rate lr = 0.005, dropout p = 0.5, and L 2 norm to 0.0005. For other parameters, we do some tuning, and present the best ones in Table XIII. ? For Random and Bayesian, the number of searched architectures is set to 200, and for each sampled architecture, we tune it using hyperopt for 50 iterations. ? For SANE, in the search phase, hidden embedding size is set to 32 for sake of computational resource, and lr = 0.005, dropout p = 0.6, and L 2 norm = 0.0002, and for each searched architecture, we tune it using hyperopt<ref type="foot" target="#foot_5">7</ref> for 50 iterations. The hyper-parameters are given in Table <ref type="table" target="#tab_9">XII</ref>, and we set dropout p = 0.6 for it performs empirically well.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: An illustration of the proposed framework. (Best viewed in color) (a) Upper Left: an example graph with five nodes. The gray rectangle represents the input features of each node; (b) Bottom Left: a typical 3-layer GNN model following the message passing neighborhood aggregation schema, which computes the embeddings of node "2"; (c) The supernet for a 3-layer GNN, introduced in Section III-B, and ? n , ? l , ? s represent, respectively, weight vectors for node aggregators, layer aggregators, and skip-connections in the corresponding edges. The rectangles denote the representations, out of which three green ones represent the hidden embeddings, gray (h 0 v ) and yellow (z v ) ones represent the input and output embeddings, respectively, and blue one (h 4 v ) represents the set of output embeddings of three node aggregators for the layer aggregator.</figDesc><graphic url="image-1.png" coords="2,113.22,50.54,385.55,242.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>2 https://github.com/HongyangGao/LGCN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: The test accuracy w.r.t. search time (in seconds) in log base 10.</figDesc><graphic url="image-6.png" coords="9,54.01,55.52,123.38,92.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) Random explore: . (b) Number of GNN layers: K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: The test accuracy w.r.t. different and K.</figDesc><graphic url="image-10.png" coords="9,49.73,552.81,123.02,92.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>The operations we use as node and layer aggregators for the search space of SANE.</figDesc><table><row><cell></cell><cell>Operations</cell></row><row><cell></cell><cell>SAGE-SUM, SAGE-MEAN, SAGE-MAX, GCN,</cell></row><row><cell>On</cell><cell>GAT,GAT-SYM, GAT-COS, GAT-LINEAR,</cell></row><row><cell></cell><cell>GAT-GEN-LINEAR, GIN, GeniePath</cell></row><row><cell>O l</cell><cell>CONCAT, MAX, LSTM</cell></row><row><cell>Os</cell><cell>IDENTITY, ZERO</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>: end while 7: Derive the final architecture {? * n , ? * s , ? * l } based on the trained {? n , ? s , ? l };</figDesc><table /><note><p><p>4:</p>Compute the training loss L tra ; 5: Update weights w by descending ? w L tra (w, ?) with the architecture ? = {? n , ? s , ? l }; 68: return Searched {? * n , ? * s , ? * l }.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>A detailed comparison between SANE and existing NAS methods for GNN.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Search space</cell><cell>Search algorithm</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Node aggregators</cell><cell>Layer aggregators</cell></row><row><cell></cell><cell></cell><cell cols="2">GraphNAS, Auto-GNN</cell><cell></cell><cell>?</cell><cell>?</cell><cell>RL</cell></row><row><cell></cell><cell></cell><cell cols="2">Policy-GNN</cell><cell></cell><cell>?</cell><cell>?</cell><cell>RL</cell></row><row><cell></cell><cell></cell><cell></cell><cell>SANE</cell><cell></cell><cell>?</cell><cell>?</cell><cell>Differentiable</cell></row><row><cell cols="6">TABLE IV: Dataset statistics of the datasets in the experi-</cell></row><row><cell cols="6">ments. N, E, F and C denote the number of "Nodes", "Edges",</cell></row><row><cell cols="4">"Features" and "Classes", respectively.</cell><cell></cell><cell></cell></row><row><cell>Task</cell><cell>Dataset</cell><cell>N</cell><cell>E</cell><cell>F</cell><cell>C</cell></row><row><cell></cell><cell>Cora</cell><cell>2,708</cell><cell>5,278</cell><cell>1,433</cell><cell>7</cell></row><row><cell>Transductive</cell><cell>CiteSeer</cell><cell>3,327</cell><cell>4,552</cell><cell>3,703</cell><cell>6</cell></row><row><cell></cell><cell>PubMed</cell><cell>19,717</cell><cell>44,324</cell><cell>500</cell><cell>3</cell></row><row><cell>Inductive</cell><cell>PPI</cell><cell cols="2">56,944 818,716</cell><cell>121</cell><cell>50</cell></row></table><note><p>consists of 24 graphs, each corresponds to a human tissue. Each node has positional gene sets, motif gene sets, and immunological signatures as features and gene ontology sets as labels. 20 graphs are used for training, 2 graphs are used for validation and the rest for test.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V :</head><label>V</label><figDesc>The statistics of the dataset DBP15K ZH-EN .</figDesc><table><row><cell cols="5">#Entities #Relations #Attributes #Rel.triples #Attr.triples</cell></row><row><cell>Chinese 66,469</cell><cell>2,830</cell><cell>8.113</cell><cell>153,929</cell><cell>379,684</cell></row><row><cell>English 98,125</cell><cell>2,317</cell><cell>7,173</cell><cell>237,674</cell><cell>567,755</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII :</head><label>VII</label><figDesc>The search time (clock time in seconds) of running once each mode.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Transductive task</cell><cell>Inductive task</cell></row><row><cell></cell><cell>Cora</cell><cell cols="2">CiteSeer PubMed</cell><cell>PPI</cell></row><row><cell>Random</cell><cell>1,500</cell><cell>2,694</cell><cell>3,174</cell><cell>13,934</cell></row><row><cell>Bayesian</cell><cell>1,631</cell><cell>2,895</cell><cell>4,384</cell><cell>14,543</cell></row><row><cell>GraphNAS</cell><cell>3,240</cell><cell>3,665</cell><cell>5,917</cell><cell>15,940</cell></row><row><cell>SANE</cell><cell>14</cell><cell>35</cell><cell>54</cell><cell>298</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IX :</head><label>IX</label><figDesc>Performance comparisons of two search spaces on four benchmark datasets. We show the mean classification accuracy (with STD). "-WS" represents the weight sharing variant of GraphNAS.</figDesc><table><row><cell>Methods</cell><cell>Cora</cell><cell>CiteSeer</cell><cell>PubMed</cell><cell>PPI</cell></row><row><cell>GraphNAS</cell><cell>0.8840 (0.0071)</cell><cell>0.7762 (0.0061)</cell><cell>0.8896 (0.0024)</cell><cell>0.9698 (0.0128)</cell></row><row><cell>GraphNAS-WS</cell><cell>0.8808 (0.0101)</cell><cell>0.7613 (0.0156)</cell><cell>0.8842 (0.0103)</cell><cell>0.9584 (0.0415)</cell></row><row><cell>GraphNAS(SANE search space)</cell><cell>0.8826 (0.0023)</cell><cell>0.7707 (0.0064)</cell><cell>0.8877 (0.0012)</cell><cell>0.9887 (0.0010)</cell></row><row><cell>GraphNAS-WS(SANE search space)</cell><cell>0.8895 (0.0051)</cell><cell>0.7695 (0.0069)</cell><cell>0.8942 (0.0010)</cell><cell>0.9875 (0.0006)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE X :</head><label>X</label><figDesc>The performance of searching for MLP. We list the best performance of SANE from Table VI as comparison. To be specific, we set the parameter space of the MLP as w ?<ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32,</ref> 64]  and d ? [1, 2, 3],where w and d represents the hidden embedding size (width) and the depth of the MLP, respectively. For simplicity, we adopt the Random and Bayesian as the NAS approaches, and search on the four benchmark datasets, where the settings are the same as those in Section IV-B. The performance is shown in TableX, from which we can see that the performance gap between searching for MLP and SANE are large. This observation demonstrates the difficulties of searching MLP as node aggregators despite its powerful expressive capability as WL test. On the other hand, it demonstrates the necessity of the designed search space of SANE, which includes existing human-designed GNN models, thus the performance can be guaranteed in practice.</figDesc><table><row><cell>Dataset</cell><cell>Random</cell><cell>Bayesian</cell><cell>SANE</cell></row><row><cell>Cora</cell><cell cols="2">0.8698 (0.0011) 0.8470 (0.0032)</cell><cell>0.8926 (0.0123)</cell></row><row><cell>CiteSeer</cell><cell cols="2">0.7298 (0.0078) 0.7103 (0.0057)</cell><cell>0.7859 (0.0108)</cell></row><row><cell>PubMed</cell><cell cols="2">0.8662 (0.0030) 0.8699 (0.0065)</cell><cell>0.9047 (0.0091)</cell></row><row><cell>PPI</cell><cell cols="2">0.8166 (0.0089) 0.8685 (0.0017)</cell><cell>0.9856 (0.0120)</cell></row><row><cell cols="4">of any search algorithm can be guaranteed. Then to show the</cell></row><row><cell cols="4">challenges of directly searching for MLP as node aggregators,</cell></row><row><cell cols="4">we propose to search for a specific MLP as node aggregators</cell></row><row><cell cols="2">with NAS approaches.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE XIII :</head><label>XIII</label><figDesc>More implementing details of GNN baselines.Here we give the number of layers, hidden dimension size, activation function, and the number of heads (GAT models). For JK-network, we further give the layer aggregators.</figDesc><table><row><cell></cell><cell>Cora&amp;CiteSeer</cell><cell>PubMed</cell><cell>PPI</cell></row><row><cell>GCN</cell><cell>3, 64, elu</cell><cell>3, 128, elu</cell><cell>3, 256, elu</cell></row><row><cell>GraphSAGE</cell><cell>2, 64, relu</cell><cell>2, 128, relu</cell><cell>3, 256, elu</cell></row><row><cell>GAT</cell><cell>3, 64, relu, 8</cell><cell>3, 128, relu, 8</cell><cell>3, 256, relu, 8</cell></row><row><cell>GIN</cell><cell>3, 128, relu</cell><cell>3, 128, relu</cell><cell>3, 256, relu</cell></row><row><cell>LGCN</cell><cell>3, 128, relu</cell><cell>3, 128, relu</cell><cell>3, 256, relu</cell></row><row><cell>GeniePath</cell><cell>3, 256, tanh, 4</cell><cell>3, 256, tanh, 4</cell><cell>3, 256, tanh, 4</cell></row><row><cell>JK-Network</cell><cell>CONCAT</cell><cell>CONCAT</cell><cell>LSTM</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>? Node aggregators: We choose 11 node aggregators based on popular GNN models, and they are presented in TableI. We denote the node aggregator set by O n . Note that the</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://github.com/hyperopt/hyperopt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://github.com/GraphNAS/GraphNAS</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://github.com/quark0/darts</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>https://github.com/rusty1s/pytorch geometric</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>https://github.com/hyperopt/hyperopt</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>TABLE XI: More explanations to the node aggregators in <ref type="bibr" target="#b0">(1)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GNN models</head><p>Symbol in the paper Key explanations</p><p>Apply mean, max, or sum operation to {hu|u ? N (v)}.</p><p>GAT <ref type="bibr" target="#b3">[4]</ref> GAT Compute attention score: e gat uv = Leaky ReLU a Wuhu||Wvhv .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GAT-SYM</head><p>e sys uv = e gat uv + e gat vu .</p><p>GAT-COS e cos uv = Wuhu, Wvhv .</p><p>GAT-LINEAR e lin uv = tanh Wuhu + Wvhv .</p><p>GAT-GEN-LINEAR e gen-lin uv</p><p>LGCN <ref type="bibr" target="#b14">[15]</ref> CNN Use 1-D CNN as the aggregator, equivalent to a weighted summation aggregator.</p><p>GeniePath <ref type="bibr" target="#b17">[18]</ref> GeniePath Composition of GAT and LSTM-based aggregators JK-Network <ref type="bibr" target="#b16">[17]</ref> Depending on the base above GNN </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>tech. rep.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Heterogeneous graph attention network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2022" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Billion-scale commodity embedding for e-commerce recommendation in alibaba</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="839" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Beyond personalization: Social content recommendation for creator equality and consumer satisfaction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="235" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchical bipartite graph neural networks: Towards large-scale ecommerce applications</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Engineering</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1677" to="1688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Price-aware recommendation with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Engineering</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="133" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fakedetector: Effective fake news detection with deep diffusive neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Engineering</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1826" to="1829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Did you enjoy the ride? understanding passenger experience via heterogeneous network embedding</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Engineering</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1392" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large-scale learnable graph convolutional networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1416" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>-I. Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5449" to="5458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Geniepath: Graph neural networks with adaptive receptive paths</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4424" to="4431" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Designing neural network architectures using reinforcement learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graph neural architecture search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conferences on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1403" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Auto-GNN: Neural architecture search of graph neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03184</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">tech. rep., arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural architecture search: A survey</title>
		<author>
			<persName><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3730" to="3740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Taking human out of learning applications: A survey on automated machine learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.13306</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>tech. rep.</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via proximal iterations</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-W</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4092" to="4101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SNAS: stochastic neural architecture search</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bayesnas: A bayesian approach for neural architecture search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7603" to="7613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adaptive stochastic natural gradient method for one-shot neural architecture search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Akimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shirakawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yoshinari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nishida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning graph convolutional network for skeleton-based human action recognition by neural searching</title>
		<author>
			<persName><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Policy-gnn: Aggregation optimization for graph neural networks</title>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="461" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Propagation model search for graph neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>tech. rep.</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Simplifying architecture search for graph neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>tech. rep.</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cross-lingual knowledge graph alignment via graph convolutional networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="349" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Crosslingual knowledge graph alignment via graph matching neural network</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3156" to="3161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cross-lingual entity alignment via joint attribute-preserving embedding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="628" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with pytorch geometric</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">tech. rep., arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012-02">Feb. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Algorithms for hyper-parameter optimization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>K?gl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2546" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multi-channel graph neural network for entity alignment</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1452" to="1461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks with a nonpolynomial activation function can approximate any function</title>
		<author>
			<persName><forename type="first">M</forename><surname>Leshno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schocken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NN</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="861" to="867" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>tech. rep., arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Geom-GCN: Geometric graph convolutional networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Graphsaint: Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prasanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Dropedge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pairnorm: Tackling oversmoothing in gnns</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Steeg</surname></persName>
		</author>
		<author>
			<persName><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
