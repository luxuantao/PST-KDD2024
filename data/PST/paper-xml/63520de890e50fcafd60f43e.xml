<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Late Prompt Tuning: A Late Prompt Could Be Better Than Many Prompts</title>
				<funder ref="#_bq8yNpJ">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
				<funder ref="#_6uv5EPY">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-20">20 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiangyang</forename><surname>Liu</surname></persName>
							<email>xiangyangliu20@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianxiang</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
							<email>xjhuang@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
							<email>xpqiu@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Late Prompt Tuning: A Late Prompt Could Be Better Than Many Prompts</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-20">20 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2210.11292v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prompt tuning is a parameter-efficient tuning (PETuning) method for utilizing pretrained models (PTMs) that simply prepends a soft prompt to the input and only optimizes the prompt to adapt PTMs to downstream tasks. Although it is parameter-and deployment-efficient, its performance still lags behind other state-of-the-art PETuning methods. Besides, the training cost of prompt tuning is not significantly reduced due to the back-propagation through the entire model. Through empirical analyses, we shed some light on the lagging performance of prompt tuning and recognize a trade-off between the propagation distance from label signals to the inserted prompt and the influence of the prompt on model outputs. Further, we present Late Prompt Tuning (LPT) that inserts a late prompt into an intermediate layer of the PTM instead of the input layer or all layers. The late prompt is obtained by a neural prompt generator conditioned on the hidden states before the prompt insertion layer and therefore is instance-dependent. Through extensive experimental results across various tasks and PTMs, we show that LPT can achieve competitive performance to full model tuning and other PETuning methods under both full-data and few-shot scenarios while possessing faster training speed and lower memory cost.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pre-trained models <ref type="bibr" target="#b2">(Devlin et al., 2019;</ref><ref type="bibr" target="#b24">Radford et al., 2019;</ref><ref type="bibr" target="#b42">Yang et al., 2019;</ref><ref type="bibr" target="#b25">Raffel et al., 2020;</ref><ref type="bibr" target="#b11">Lewis et al., 2020;</ref><ref type="bibr">Liu et al., 2022a;</ref><ref type="bibr" target="#b23">Qiu et al., 2020;</ref><ref type="bibr">Lin et al., 2021)</ref> have pushed most NLP tasks to state-of-the-art. Model tuning (or fine-tuning) is a popular method for utilizing PTMs on downstream tasks that needs to tune all parameters of PTMs for every task. Despite the welcome outcome, it leads to prohibitive adaptation costs, especially for supersized PTMs <ref type="bibr">(Brown et al.,</ref>  All methods are evaluated on 10 text classification tasks using RoBERTa LARGE . The radius of every circle indicates training speed (tokens per millisecond). LPT w/ NPG and LPT w/o PG represent LPT with naive prompt generator and without prompt generator, respectively. The details can be found in Section 5. <ref type="bibr">Wang et al., 2021a)</ref>. Parameter-efficient tuning (PETuning) is a new tuning paradigm that can adapt PTMs to downstream tasks by only tuning a very small number of internal or additional parameters.</p><p>Prompt tuning <ref type="bibr" target="#b10">(Lester et al., 2021</ref>) is a simple and popular PETuning method that prepends a sequence of soft prompt tokens to the input and only optimizes the prompt to adapt PTMs to downstream tasks. It has an absolute advantage in parameter efficiency and facilitates mixed-task inference, which makes the deployment of PTMs convenient. However, compared with other advanced PETuning methods, e.g., Adapter <ref type="bibr" target="#b7">(Houlsby et al., 2019;</ref><ref type="bibr" target="#b19">Mahabadi et al., 2021)</ref>, <ref type="bibr">LoRA (Hu et al., 2022), and</ref><ref type="bibr">BitFit (Zaken et al., 2022)</ref>, prompt tuning suffers from lower performance and convergence rate. Compared with full model tuning, although the number of trainable parameters in prompt tuning reduces by ?17,000? (from 355M to 21K on RoBERTa LARGE ), the training speed only increases by ?1.5?, and the memory cost only reduces by 29.8%. 1 P-tuning v2 <ref type="bibr">(Liu et al., 2022b)</ref> improves the performance of prompt tuning by inserting soft prompts into every hidden layer of PTMs, but it is difficult to optimize and needs more training steps to attain competitive performance.</p><p>In this paper, we explore why prompt tuning performs poorly and find there is a trade-off between the propagation distance from label signals to the inserted prompt and the influence of the prompt on model outputs. The key to prompt tuning is to make the soft prompt carry task-related information through downstream training. The trained prompt can interact with text inputs during the model forward pass to obtain text representations with taskrelated information. Since the prompt is inserted into the input in prompt tuning, it has a strong ability to influence the outputs of PTM through sufficient interactions with text inputs. However, there is a long propagation path from label signals to the prompt. It leads us to ask the question: Does this long propagation path cause a lot of task-related information to be lost during propagation and thus affect performance? To verify the impact of the propagation distance on performance, we conduct pilot experiments by shortening it in Section 4 and find that the performance first increases then decreases with the shortening of the length. This finding inspires us to present the late prompt (i.e., inserting the prompt into an intermediate hidden layer of PTM). The late prompt not only receives more task-related information at each update due to the shorter propagation path of task-related information but also maintains the adequate ability to influence the outputs of PTM. Despite the higher performance and faster convergence rate of late prompt than prompt tuning, the hidden states produced by PTM before the prompt insertion layer are underutilized. To further improve performance and take full advantage of these contextual hidden representations, we introduce a prompt generator to generate the soft prompt (termed as instanceaware prompt) for each instance using the corresponding hidden states.</p><p>Based on the late and instance-aware prompt, we present Late Prompt Tuning (LPT) to improve prompt tuning. Since the soft prompt is inserted into an intermediate layer of PTM, we have no need to compute gradients for model parameters be-1 Refer to Section 6.5 for details. low the prompt insertion layer, and therefore speed up the training process and reduce memory costs. Extensive experimental results show that LPT outperforms most prompt-based tuning methods and can be comparable with adapter-based tuning methods and even full model tuning. Especially in the few-shot scenario with only 100 training samples, LPT outperforms prompt tuning by 12.4 points and model tuning by 5.0 points in the average performance of ten text classification tasks. Besides, it is 2.0? faster and reduces by 56.6% than model tuning in terms of training speed and memory cost on RoBERTa LARGE , respectively. Figure <ref type="figure" target="#fig_0">1</ref> shows an overall comparison between LPT and its counterparts. To sum up, the key contributions of this paper are:</p><p>? We explore why prompt tuning performs poorly and find that it is due to the long propagation path from label signals to the input prompt and present a simple variant named late prompt tuning to address the issue.</p><p>? Combining the late and instance-aware prompts, we present LPT, which not only attains comparable performance with adapterbased tuning methods and even model tuning but also greatly reduces training costs.</p><p>? We verify the versatility of LPT in the fulldata and few-shot scenarios across 10 text classification tasks and 3 PTMs. Code is publicly available at https://github.com/ xyltt/LPT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Adapter-based tuning. One research line of PETuning is adapter-based tuning <ref type="bibr" target="#b3">(Ding et al., 2022)</ref> that inserts some adapter modules between model layers and optimizes these adapters in downstream training for model adaptation.</p><p>Adapter <ref type="bibr" target="#b7">(Houlsby et al., 2019)</ref> inserts adapter modules with bottleneck architecture between every consecutive Transformer <ref type="bibr" target="#b32">(Vaswani et al., 2017)</ref> sublayers. AdapterDrop <ref type="bibr" target="#b27">(R?ckl? et al., 2021)</ref> investigates the efficiency through removing adapters from lower layers. Compacter <ref type="bibr" target="#b19">(Mahabadi et al., 2021)</ref> uses low-rank optimization and parameterized hypercomplex multiplication <ref type="bibr" target="#b44">(Zhang et al., 2021)</ref> to compress adapters. Adapter-based tuning methods have comparable results with model tuning when training data is sufficient but don't work well in the few-shot scenario <ref type="bibr" target="#b36">(Wang et al., 2022)</ref>.</p><p>Prompt-based tuning. Another main research line of PETuning is prompt-based tuning that inserts some additional soft prompts into the hidden states instead of injecting new neural modules to PTMs. Prompt tuning <ref type="bibr" target="#b10">(Lester et al., 2021)</ref> and Ptuning <ref type="bibr" target="#b16">(Liu et al., 2021)</ref> insert a soft prompt to word embeddings only, and can achieve competitive results when applied to supersized PTMs. Prefixtuning (Li and Liang, 2021) and P-tuning v2 <ref type="bibr">(Liu et al., 2022b)</ref> insert prompts to every hidden layer of PTM. BBT <ref type="bibr">(Sun et al., 2022b)</ref> optimizes the inserted prompt with derivative-free optimization. Some prompt-based tuning methods, like prompt tuning and BBT, formulate downstream tasks as pre-training tasks (e.g., masked language modeling task) to close the gap between pre-training and downstream training <ref type="bibr">(Sun et al., 2022a)</ref>. There are also some prompt-based methods with instanceaware prompt. IDPG <ref type="bibr" target="#b41">(Wu et al., 2022)</ref> uses the prompt generator with parameterized hypercomplex multiplication <ref type="bibr" target="#b44">(Zhang et al., 2021)</ref> to generate a soft prompt for every instance. Contexttuning <ref type="bibr" target="#b31">(Tang et al., 2022)</ref> uses BERT model <ref type="bibr" target="#b2">(Devlin et al., 2019)</ref> as the prompt generator and focuses on NLG tasks. IPL <ref type="bibr" target="#b9">(Jin et al., 2022)</ref> first calculates relevance scores between prompt tokens and inputs, then uses the scores to re-weight the original prompt tokens. But it tunes all parameters of PTM. All the above methods with instanceaware prompt have the same weakness because they need to encode the inputs using an extra encoder, which slows down the training and increases inference latency. There are also some other popular PETuning methods, such as BitFit <ref type="bibr" target="#b43">(Zaken et al., 2022)</ref> which only tunes the bias terms, LoRA <ref type="bibr">(Hu et al., 2022)</ref> which optimizes low-rank decomposition matrices of the weights within self-attention layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Formulation</head><p>Given a PTM M, in the setting of model tuning, we first reformulate the inputs with single sentence as E([CLS] S 1 <ref type="bibr">[SEP]</ref>) and the inputs with sentence pair as E(</p><formula xml:id="formula_0">[CLS] S 1 [SEP] S 2 [SEP]),</formula><p>where E is the embedding layer of M. The final hidden state of [CLS] token will be used to predict label. In the setting of prompt tuning, we insert a randomly initialized soft prompt p into word embeddings, and also modify the original inputs using different manual templates with a [MASK] token for different tasks. For example, the inputs with single sentence from a sentiment analysis task will be transform into concat(p, E([CLS] S 1 It was <ref type="bibr">[MASK]</ref>. [SEP])). Then, we map the original labels Y to some words in the vocabulary V of M, which formulates downstream tasks as a language modeling task to close the gap between pre-training and downstream training. The final hidden state of [MASK] token will be used to predict label.</p><p>In the setting of our proposed method LPT, we use a prompt generator (PG) to generate an independent prompt p for every input. In addition, the layer that the prompt inserts into is an intermediate layer of PTM instead of word embeddings, and we refer to the layer as the prompt layer (PL).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Why Prompt Tuning Performs Poorly?</head><p>The workflow of prompt tuning is to make the inserted soft prompt carry task-related information through downstream training. In the inference phase, this prompt can interact with test inputs during layer-upon-layer propagation so that the hidden representations of these inputs also contain taskrelated information. There are strong interactions between the prompt and text inputs because prompt tuning inserts prompt into word embeddings. However, there is a long propagation path from label signals to the prompt. Therefore, we speculate that the poor performance of prompt tuning is due to the long propagation path of task-related information, which causes a lot of task-related information to be lost during propagation in the frozen model and thus affect performance. To verify this conjecture, we conduct some pilot experiments on TREC <ref type="bibr" target="#b33">(Voorhees and Tice, 2000)</ref> and RTE <ref type="bibr" target="#b1">(Dagan et al., 2005)</ref> datasets using RoBERTa LARGE <ref type="bibr" target="#b17">(Liu et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Does shortening the propagation distance improve performance?</head><p>We start by considering a simple experiment setting where the soft prompt is inserted into different layers of RoBERTa LARGE then we look at how performance changes as the prompt layer changes. As shown in the left plots of Figure <ref type="figure" target="#fig_1">2</ref>, we can observe that the performance first increases and then decreases with the rise of the prompt layer and obtain the highest performance when the prompting layer is in the range of 12 to 14. In addition, we also explore the convergence rates at different prompt layers. For simplification, we only consider three different prompt layers 1, 13, and 24. The middle plots in Figure <ref type="figure" target="#fig_1">2</ref> show that the model has the fastest convergence rate when the Middle: Comparison of convergence rates for different prompt layers. Right: The estimated mutual information between hidden states of each layer and label. 'PL' denotes the prompt layer. 'PL = 1' denotes the traditional prompt tuning <ref type="bibr" target="#b10">(Lester et al., 2021)</ref>. We show mean and standard deviation of performance over 3 different random seeds.</p><p>prompt layer is 13. The trend is consistent with the performance trend shown on the left plots. We can preliminarily identify that properly shortening the propagation distance can improve performance according to these results. However, the performance starts to degrade when we extremely shorten the propagation path of task-related information. We attribute this to the interaction between the prompt and inputs becomes very weak when we unduly shorten the propagation path, which leads to the slighter influence of the prompt on model outputs and the gradual decline of performance.</p><p>Task-related information in hidden states. To quantify the task-related information carried in the soft prompt, we follow <ref type="bibr">Wang et al. (2021b)</ref> and adopt the mutual information I(h, y) between the hidden states and label of each input. The estimate method of I(h, y) is provided in Appendix A. The right plots of Figure <ref type="figure" target="#fig_1">2</ref> show the I(h, y) at different layers. We note that I(h, y) gradually increases with the forward pass of prompt (i.e., the effect of the prompt on the hidden states gradually increases) when the prompt layer is 13. And its I(h, y) in the last layer is the highest among the three different prompt layer settings, which means that the soft prompt carries more task-related information. The other two prompt layer settings all collapse, especially on the RTE task, because there is no better trade-off between the propagation distance and the effect of prompt on hidden states.</p><p>The above observations suggest that our conjecture about the poor performance of prompt tuning is correct. The long propagation path of task-related information leads to poor performance and low convergence rate. And we find that properly shortening the propagation distance can improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">LPT: Late Prompt Tuning</head><p>From the experiment results in Section 4, we observe that using late prompt can greatly improve the performance of prompt tuning. Moreover, late prompt can bring two other advantages: (1) No gradient calculation for model parameters below the prompt layer; (2) The hidden states produced by the model before the prompt layer can be used to generate a great independent prompt for each instance. Based on these advantages, we propose an efficient prompt-based tuning method LPT which combines late and instance-aware prompts. An illustration of LPT is shown in Figure <ref type="figure" target="#fig_2">3</ref>. In this section, we will introduce two different prompt generators used in LPT and how to determine the prompt layer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Prompt Generators</head><p>Naive prompt generator (NPG). The prompt generator is a simple feed-forward layer with bottleneck architecture. Assume the prompt length is l, then we can generate an independent prompt for each instance as below:</p><formula xml:id="formula_1">p = W 2 (ReLU(W 1 h [CLS] + b 1 )) + b 2 , (1) p = Reshape(p),<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">W 1 ? R m?d , W 2 ? R (l?d)?m , h [CLS] ? R d and p ? R l?d .</formula><p>b 1 and b 2 are bias terms. d is the dimension of hidden states. Since m d, the prompt generator doesn't have too many parameters. However, the number of parameters within W 2 will increase with the prompt length l.</p><p>To tackle this problem, we propose the following pooling prompt generator.</p><p>Pooling prompt generator (PPG). PPG introduces a pooling operation between downprojection and up-projection operations, which directly obtains the prompt with length l through pooling on input sequences (i.e., pooling the input with length n to the prompt with length l). The generator is more lightweight to generate a prompt,</p><formula xml:id="formula_3">p = ReLU(Pooling(W 1 h + b 1 )), (3) p = W 2 p + b 2 . (4) Different from NPG, W 1 ? R m?d , W 2 ? R d?m</formula><p>and h ? R d?n here. n is the length of the original input. In this paper, we consider both Average Pooling and Max Pooling, referred to as APPG and MPPG, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">How to Determine Prompt Layer?</head><p>Generating a good prompt needs a good contextual representation for the input. In this sub-section, we will explore how to choose the prompt layer to guarantee that LPT can attain a good trade-off between performance and efficiency through some pilot experiments on TREC <ref type="bibr" target="#b33">(Voorhees and Tice, 2000)</ref> and RTE <ref type="bibr" target="#b1">(Dagan et al., 2005)</ref> datasets. As shown in Figure <ref type="figure" target="#fig_3">4</ref>, the performance of NPG has a significant decline when the prompt layer is in the range from 14 to 24. However, different from NPG, APPG and MPPG retain high performance as the prompt layer approaches the output layer, especially on TREC dataset. We believe that this is due to the hidden states from the higher layers can help generate a better prompt, while NPG only uses [CLS] token as the representation of the entire input when generating the prompt, which leads to the loss of information. According to the above observations, LPT with APPG and MPPG can achieve a better trade-off for both relatively simple (TREC) and difficult (RTE) tasks. But in this work, to ensure that all methods (NPG, APPG and MPPG) can achieve a good performance while maintaining a relatively low training costs, we simply choose the most intermediate layer of PTM as the prompt layer. That is, we choose the 13-th layer as the prompt layer for RoBERTa LARGE .</p><p>6 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Evaluation Datasets</head><p>We evaluate our method on 5 single-sentence and 5 sentence-pair classification tasks, including 6 tasks from GLUE benchmark <ref type="bibr" target="#b34">(Wang et al., 2019)</ref> and 4 other popular tasks include MPQA <ref type="bibr" target="#b38">(Wiebe et al., 2005)</ref>, MR <ref type="bibr" target="#b21">(Pang and Lee, 2005)</ref>, Subj <ref type="bibr" target="#b20">(Pang and Lee, 2004)</ref> and TREC <ref type="bibr" target="#b33">(Voorhees and Tice, 2000)</ref> tasks. All details about data statistics and splits can be found in Appendix B. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experiment Settings</head><p>We evaluate our method in both full-data and few-shot scenarios on three PTMs, including RoBERTa LARGE <ref type="bibr" target="#b17">(Liu et al., 2019)</ref>, DeBERTa LARGE <ref type="bibr" target="#b6">(He et al., 2021)</ref> and GPT2 LARGE <ref type="bibr" target="#b24">(Radford et al., 2019)</ref>. According to the conclusion from the Section 5.2, we choose the 13-th layer as the prompt layer for RoBERTa LARGE and DeBERTa LARGE , and the 19-th layer for GPT2 LARGE except special explanation. More implementation details are provided in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Baselines</head><p>We from original training sets. Besides, we randomly sample 1000 samples from the original training sets as development sets and there is no overlap with sampled training sets. For the tasks from GLUE benchmark <ref type="bibr" target="#b34">(Wang et al., 2019)</ref>, the original development sets are used as the test sets and the test sets remain unchanged for 4 other tasks.</p><p>Table <ref type="table" target="#tab_1">2</ref> and 3 show the overall comparison of all the methods in the few-shot scenario. LPT w/ NPG outperforms all the baselines in two different fewshot settings. Especially when the training set has only 100 samples, LPT w/ NPG outperforms model tuning by 5 points and Adapter by 7.1 points. This indicates that our method has better generalization performance when the training data is very scarce. However, we note that LPT w/ MPPG and LPT w/ APPG don't perform as well in the few-shot scenario as they do in the full-data scenario. We speculate that this is owing to the optimal state of the pooling layer to retain only useful information, and sufficient training data is needed to achieve this state. Nevertheless, both LPT w/ MPPG and LPT w/ APPG are also superior to all the baselines when the training set has 100 samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on other PTMs</head><p>To verify the generality of our conclusion about why prompt tuning performs poorly and the versatility of the proposed method LPT, we also conduct experiments on two other popular PTMs, DeBERTa LARGE <ref type="bibr" target="#b6">(He et al., 2021)</ref>, and GPT2 LARGE <ref type="bibr" target="#b24">(Radford et al., 2019)</ref>. The results are shown in Table <ref type="table" target="#tab_3">4</ref>. Only using the late prompt to shorten the propagation path of taskrelated information (i.e., LPT w/o PG) is also far superior to the traditional prompt tuning method on these two PTMs. This result enhances the reliability of our conclusion. Moreover, LPT with different prompt generators further improves the performance, closing the gap with model tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Efficiency Evaluation</head><p>We compare the efficiency of our method with all the baselines on RoBERTa LARGE <ref type="bibr" target="#b17">(Liu et al., 2019)</ref> and GPT2 LARGE <ref type="bibr" target="#b24">(Radford et al., 2019)</ref>  such that model tuning method can fit the fixed budget of a NVIDIA GTX 3090 GPU (24GB) and other methods use the same batch size as model tuning. We set the length of all inputs to 256 and evaluate the accuracy in the few-shot scenario that the number of training data is 100 for all methods.</p><p>In Table <ref type="table" target="#tab_5">5</ref>, we report accuracy, tuable parameters, training speed (tokens per millisecond) and memory cost (GB) of each method. Our methods not only outperform all prompt-based methods considered in terms of efficiency and memory cost, but obtain the highest performance. Compared with AdapterDrop that has similar efficiency with LPT, our method LPT w/ NPG outperforms it by 20.1 and 7 points on RoBERTa LARGE and GPT2 LARGE , respectively. In addition, we also explore the impact of the choice of prompt layer on all efficiency metrics, and the specific experiment results are in Appendix D. Overall, given a large scale PTM with millions or billions of parameters, such as RoBERTa <ref type="bibr" target="#b17">(Liu et al., 2019)</ref>, DeBERTa <ref type="bibr" target="#b6">(He et al., 2021)</ref>, and GPT2 <ref type="bibr" target="#b24">(Radford et al., 2019)</ref>, higher training speed and lower memory cost is a paramount importance for practical applications. And LPT offers a better trade-off in terms of training budget and performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Analyses</head><p>Effect of prompt layer. To enhance the reliability of the conclusion (i.e., the most intermediate layer of PTM is the optimal choice of the prompt layer) from Section 5.2, we also conduct the same experiments with Section 5.2 on the other two PTMs that include DeBERTa LARGE <ref type="bibr" target="#b6">(He et al., 2021)</ref> and GPT2 LARGE <ref type="bibr" target="#b24">(Radford et al., 2019)</ref>   layer is also the optimal choice of the prompt layer on DeBERTa LARGE and GPT2 LARGE models, especially for LPT w/ NPG. These results enhance the reliability of our conclusion that a better tradeoff between performance and efficiency can be achieved by selecting the most intermediate layer of PTM as the prompt layer.</p><p>Visualization of instance-aware prompt. We selected the subj dataset <ref type="bibr" target="#b20">(Pang and Lee, 2004)</ref> with 1000 development samples for this analysis. For the sake of simplification, we only visualize the instance-aware prompt of LPT w/ NPG method. As shown in Figure <ref type="figure" target="#fig_5">6</ref>, we use the same color to mark the samples that their representations are close. We can clearly observe that our method can generate similar prompts for the instances with relatively similar sentence representation. On the contrary, the independent prompts of instances with quite different sentence representations are also quite different. The visualization result indicates that our method learns a special prompt for each instance and can be aware of the important information of the instance to drive PTMs better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we explore why prompt tuning performs poorly and find there is a trade-off between the propagation distance from label signals to the  inserted prompt and the influence of the prompt on model outputs. With this discovery, we present a more efficient and effective prompt tuning method LPT with late and instance-aware prompts. Experiment results in full-data and few-shot scenarios demonstrate LPT can achieve comparable or even better performance than state-of-the-art PETuning methods and full model tuning while having higher training speed and lower memory cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Although we showed that our proposed method can greatly improve performance and reduce training costs for diverse NLU tasks on three different PTMs (i.e., RoBERTa LARGE , DeBERTa LARGE and GPT2 LARGE ), the larger PTMs with billions of or more parameters and NLG tasks were not considered. But our main thought of using late and instance-aware prompt is simple and can be easily transferred to other backbone architectures and different types of tasks. It would be interesting to investigate if our findings hold for other backbone models and types of tasks. And we will explore it in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Details for Mutual Information Estimation</head><p>Because the mutual information cannot be be calculated directly, we estimate it by training a new classifier using the hidden states h as inputs and the original labels of inputs as outputs. Then, we estimate I(h, y) using the performance achieved by the classifier. Since I(h, y) = H(y) -H(y|h) = H(y) -E (h,y) [-log p(y|h)] <ref type="bibr">(Wang et al., 2021b)</ref>, we can train a new classifier q ? (y|h) to approximate p(y|h), such that we have</p><formula xml:id="formula_4">I(h, y) ? max ? {H (y) -1 N [ N i=1 -log q ? (y i |h i )]}.</formula><p>Because H(y) is a constant, we are going to ignore it here. Based on the above conditions, we can use the loss of q ? (y|h) (i.e., -1 N [ N i=1 -log q ? (y i |h i )]) as the estimate of I(h, y). Further simplification, we use the performance of this new classifier to estimate mutual information I(h, y). Because RoBERTa LARGE <ref type="bibr" target="#b17">(Liu et al., 2019)</ref> has 24 layers totally except embedding layer, we can obtain 24 hidden states for each input. Hence, we need to train 24 new classifiers for each method. To speed up the training process, we use a 6-layer RoBERTa LARGE as q ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Datasets</head><p>For SST-2 <ref type="bibr" target="#b28">(Socher et al., 2013)</ref>, MNLI <ref type="bibr" target="#b39">(Williams et al., 2018)</ref>, MRPC <ref type="bibr" target="#b4">(Dolan and Brockett, 2005)</ref>, QNLI <ref type="bibr" target="#b26">(Rajpurkar et al., 2016)</ref>, QQP 3 and RTE <ref type="bibr" target="#b1">(Dagan et al., 2005)</ref> datasets which are from GLUE benchmark <ref type="bibr" target="#b34">(Wang et al., 2019)</ref>, we use their original data splits. For 4 other datasets, we select a certain number of samples from the training set as the development set, and the number of samples for each label is determined according to its proportion in the original training set. The dataset statistics after split are shown in Table 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Implementation Details</head><p>The search space of hyperparameters considered in this paper is shown in Table <ref type="table">7</ref>. As an additional note, we use the same number of training epochs or steps for all the methods. For adapterbased tuning methods, we set the down-projection size m to 16. We set the prompt length to 20 for prompt tuning <ref type="bibr" target="#b10">(Lester et al., 2021)</ref> and P-tuning v2 <ref type="bibr">(Liu et al., 2022b)</ref>, and 5 for S-IDPG-PHM <ref type="bibr" target="#b41">(Wu et al., 2022)</ref> and LPT w/ NPG. For LPT w/ MPPG and LPT w/ APPG, due to the number of tunable 3 https://www.quora.com/q/quoradata/ parameters being invariable with prompt length changes, we also search the prompt length in the range of {10, 15, 20} for them. Besides, we set the down-projection size m of S-IDPG-PHM and LPT to 256 and 128, respectively. The hyperparameter r and ? in LoRA are set to 8 and 16 on RoBERTa LARGE , 4 and 32 on GPT2 LARGE . For the batch size of GPT2 model listed in Table <ref type="table">7</ref>, it refers to the number of samples in a single forward pass. Due to the large scale of GPT2 LARGE , we use gradient accumulation technique to avoid outof-memory, and the accumulation step is 2 or 4. We use AdamW optimizer <ref type="bibr" target="#b18">(Loshchilov and Hutter, 2019)</ref> for all the methods in this work. We use Pytorch <ref type="bibr" target="#b22">(Paszke et al., 2019</ref>) and HuggingFace's Transformers <ref type="bibr" target="#b40">(Wolf et al., 2020)</ref> libraries to implement all the methods in this work. All experiments are conducted on 8 NVIDIA GTX 3090 GPUs.</p><p>We follow <ref type="bibr" target="#b5">Gao et al. (2021)</ref> and show the used manual templates and label words in Table <ref type="table">8</ref> and<ref type="table">Table 9</ref>, respectively. Note that, since the vocabulary of the GPT2 model doesn't have the [MASK] token, we justly use it to represent the positions that are needed to predict.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Efficiency Evaluation on Different</head><p>Prompt Layers.</p><p>We select the prompt layer in the range of {7, 13, 19} to explore the influence from different prompt layers for the trade-off between efficiency and performance. The experiment settings are consistent with those described in Section 6.5.</p><p>Table <ref type="table" target="#tab_7">10</ref> shows the performance, the number of tunable parameters, training speed, and memory cost for LPT with three different prompt layers.</p><p>When the prompt layer is the 13th layer, both performance and training efficiency are better than when it is the 7th layer. When the prompt layer is the 19th layer, the efficiency is further improved while the performance degrades a lot.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overall comparison between LPT and baselines of only 100 training samples for each task. All methods are evaluated on 10 text classification tasks using RoBERTa LARGE . The radius of every circle indicates training speed (tokens per millisecond). LPT w/ NPG and LPT w/o PG represent LPT with naive prompt generator and without prompt generator, respectively. The details can be found in Section 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Left: The performance achieved by inserting a soft prompt into different layers of RoBERTa LARGE .Middle: Comparison of convergence rates for different prompt layers. Right: The estimated mutual information between hidden states of each layer and label. 'PL' denotes the prompt layer. 'PL = 1' denotes the traditional prompt tuning<ref type="bibr" target="#b10">(Lester et al., 2021)</ref>. We show mean and standard deviation of performance over 3 different random seeds.</figDesc><graphic url="image-5.png" coords="4,225.07,195.74,145.14,126.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An illustration of LPT. Left: Naive (NPG) and pooling (PPG) prompt generators. Right: The forward and backward pass of LPT.</figDesc><graphic url="image-7.png" coords="5,70.87,70.87,218.27,133.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The change trend of performance with different prompt layers for three different prompt generators. The backbone model is RoBERTa LARGE . We show mean and standard deviation of performance over 3 different random seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The change trend of performance with different prompt layers on DeBERTa LARGE (upper) and GPT2 LARGE (lower). We show mean and standard deviation of performance over 3 different random seeds.</figDesc><graphic url="image-12.png" coords="9,71.68,164.06,106.95,92.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Sentence representation visualization (left) and instance-aware prompt visualization (right).</figDesc><graphic url="image-14.png" coords="9,71.68,325.73,106.95,106.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Overall comparison in full-data scenario. All the methods are evaluated on test sets except the tasks from GLUE benchmark. We report mean and standard deviation of performance over 3 different random seeds for all the methods except model tuning. The best results are highlighted in bold and the second best results are marked with underline. Prompt Tuning-256 indicates the prompt tuning method with prompt length 256. All the results are obtained using RoBERTa LARGE .</figDesc><table><row><cell>Method</cell><cell>Tunable Parameters</cell><cell>SST-2 (acc)</cell><cell>MPQA (acc)</cell><cell>MR (acc)</cell><cell>Subj (acc)</cell><cell>TREC (acc)</cell><cell>MNLI (acc)</cell><cell>MRPC (acc and F1)</cell><cell>QNLI (acc)</cell><cell>QQP (acc and F1)</cell><cell>RTE (acc)</cell><cell>Avg</cell></row><row><cell>Model Tuning</cell><cell>355M</cell><cell>95.6</cell><cell>90.2</cell><cell>91.3</cell><cell>96.8</cell><cell>97.6</cell><cell>89.3</cell><cell>91.2</cell><cell>94.6</cell><cell>90.7</cell><cell>86.2</cell><cell>92.4</cell></row><row><cell>Adapter</cell><cell>1.6M</cell><cell cols="6">96.2 (0.2) 89.2 (0.5) 91.6 (0.4) 96.8 (0.4) 97.0 (0.3) 90.5 (0.1)</cell><cell>90.3 (1.0)</cell><cell>94.7 (0.3)</cell><cell>89.4 (0.7)</cell><cell cols="2">85.5 (1.2) 92.3</cell></row><row><cell>AdapterDrop</cell><cell>811K</cell><cell cols="6">95.3 (0.3) 89.1 (0.7) 91.0 (0.5) 95.3 (0.6) 95.7 (0.5) 88.5 (0.2)</cell><cell>90.1 (1.3)</cell><cell>93.3 (0.3)</cell><cell>88.3 (0.3)</cell><cell cols="2">81.1 (2.0) 90.8</cell></row><row><cell>BitFit</cell><cell>273K</cell><cell cols="6">95.9 (0.1) 89.2 (0.9) 91.8 (0.5) 96.9 (0.1) 96.2 (0.3) 90.0 (0.1)</cell><cell>89.6 (0.9)</cell><cell>94.4 (0.2)</cell><cell>87.9 (0.4)</cell><cell cols="2">82.4 (1.1) 91.4</cell></row><row><cell>LoRA</cell><cell>788K</cell><cell cols="6">96.2 (0.3) 90.1 (0.3) 92.0 (0.1) 97.1 (0.4) 96.8 (0.6) 89.8 (0.3)</cell><cell>91.1 (0.6)</cell><cell>94.8 (0.2)</cell><cell>89.8 (0.1)</cell><cell cols="2">84.8 (2.1) 92.3</cell></row><row><cell>Prompt Tuning</cell><cell>21K</cell><cell cols="6">94.9 (0.5) 88.8 (0.8) 89.6 (0.5) 93.9 (0.6) 86.4 (0.7) 86.7 (0.9)</cell><cell>75.7 (0.7)</cell><cell>91.4 (0.1)</cell><cell>81.2 (0.8)</cell><cell cols="2">60.8 (0.5) 84.9</cell></row><row><cell>Prompt Tuning-256</cell><cell>262K</cell><cell cols="6">95.8 (0.4) 90.2 (0.2) 91.8 (0.4) 95.8 (0.5) 93.3 (0.4) 87.7 (0.5)</cell><cell>76.2 (2.4)</cell><cell>91.6 (0.8)</cell><cell>85.3 (0.3)</cell><cell cols="2">59.7 (2.4) 86.7</cell></row><row><cell>P-tuning v2</cell><cell>985K</cell><cell cols="6">95.8 (0.4) 89.9 (0.6) 91.4 (0.4) 96.5 (0.2) 95.8 (0.6) 88.2 (0.2)</cell><cell>86.5 (2.1)</cell><cell>93.7 (0.3)</cell><cell>85.3 (0.2)</cell><cell cols="2">66.9 (2.3) 89.0</cell></row><row><cell>S-IDPG-PHM</cell><cell>114K</cell><cell cols="6">94.8 (0.3) 89.5 (0.6) 90.8 (0.5) 95.9 (0.6) 89.3 (0.4) 87.4 (0.5)</cell><cell>77.3 (1.2)</cell><cell>91.2 (0.4)</cell><cell>82.3 (1.9)</cell><cell cols="2">62.7 (1.9) 86.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LPT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LPT w/o PG</cell><cell>21K</cell><cell cols="6">95.5 (0.3) 87.6 (1.7) 89.3 (0.6) 95.1 (0.2) 89.7 (0.7) 88.0 (0.4)</cell><cell>82.3 (1.3)</cell><cell>92.0 (0.1)</cell><cell>84.2 (0.5)</cell><cell cols="2">75.2 (1.8) 87.9</cell></row><row><cell>LPT w/ NPG</cell><cell>792K</cell><cell cols="6">95.5 (0.4) 89.0 (0.1) 90.9 (0.2) 95.8 (0.2) 95.9 (0.4) 87.0 (0.3)</cell><cell>88.4 (1.5)</cell><cell>91.7 (0.6)</cell><cell>86.6 (0.5)</cell><cell cols="2">79.7 (3.2) 90.1</cell></row><row><cell>LPT w/ MPPG</cell><cell>263K</cell><cell cols="6">95.4 (0.4) 89.1 (0.2) 90.7 (0.1) 96.5 (0.2) 97.4 (0.2) 87.7 (0.3)</cell><cell>90.4 (0.6)</cell><cell>91.3 (0.3)</cell><cell>88.6 (0.4)</cell><cell cols="2">78.7 (3.3) 90.6</cell></row><row><cell>LPT w/ APPG</cell><cell>263K</cell><cell cols="6">95.3 (0.2) 89.1 (0.3) 90.7 (0.1) 96.2 (0.2) 97.0 (0.2) 87.4 (0.3)</cell><cell>90.2 (1.0)</cell><cell>91.6 (0.4)</cell><cell>87.4 (0.4)</cell><cell cols="2">79.2 (3.3) 90.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results in the few-shot scenario of 100 training samples. We report mean and standard deviation of performance over 4 different data splits for all the methods. Bold and Underline indicate the best and the second best results. All the results are obtained using RoBERTa LARGE .</figDesc><table><row><cell></cell><cell>tuning</cell></row><row><cell></cell><cell>v2 which inserts prompts to each layer of PTM. (ii)</cell></row><row><cell></cell><cell>Increasing the prompt length for prompt tuning can</cell></row><row><cell></cell><cell>improve performance to some extend, but increas-</cell></row><row><cell></cell><cell>ing the training burden and inference latency no-</cell></row><row><cell></cell><cell>tably. (iii) Our method LPT with different prompt</cell></row><row><cell></cell><cell>generators (i.e., LPT w/ NPG, LPT w/ MPPG, and</cell></row><row><cell></cell><cell>LPT w/ APPG) outperforms all the prompt-based</cell></row><row><cell>consider Model Tuning, adapter-based tuning,</cell><cell>methods including S-IDPG-PHM that also claims</cell></row><row><cell>prompt-based tuning methods and two other state-</cell><cell>instance-aware prompt. (iv) The performance of</cell></row><row><cell>of-the-art PETuning methods that include (1) Bit-</cell><cell>LPT with the prompt generators is comparable with</cell></row><row><cell>Fit (Zaken et al., 2022) and (2) LoRA (Hu et al.,</cell><cell>AdapterDrop, especially for LPT w/ MPPG and</cell></row><row><cell>2022) as our baselines. For adapter-based tuning</cell><cell>LPT w/ APPG. But their number of tunable pa-</cell></row><row><cell>methods, we compare with (1) Adapter (Houlsby</cell><cell>rameters is only one-third of AdapterDrop. (v)</cell></row><row><cell>et al., 2019) and (2) AdapterDrop (R?ckl? et al.,</cell><cell>Prompt-based methods are weaker than adapter-</cell></row><row><cell>2021). For prompt-based tuning methods, we</cell><cell>based methods and model tuning on sentence-pair</cell></row><row><cell>compare with (1) Prompt Tuning (Lester et al.,</cell><cell>tasks, which is consistent with the results from</cell></row><row><cell>2021), (2) P-tuning v2 (Liu et al., 2022b) and (3)</cell><cell>Sun et al. (2022b) and Ding et al. (2022). It may</cell></row><row><cell>IDPG (Wu et al., 2022). We implement Aadpter,</cell><cell>be because sentence-pair tasks are more difficult</cell></row><row><cell>AdapterDrop, BitFit, and LoRA using OpenDelta 2</cell><cell>than single-sentence tasks and more influenced by</cell></row><row><cell>library. For IDPG which also raises instance-aware</cell><cell>manual templates and label words.</cell></row><row><cell>prompt, we only compare with the version with</cell><cell></cell></row></table><note><p><p><p><p><p><p><p><p><p><p>single-layer prompt, that is S-IDPG-PHM. And we don't use supplementary training like</p><ref type="bibr" target="#b41">Wu et al. (2022)</ref> </p>to enhance performance.</p>2 https://github.com/thunlp/OpenDelta</p>6.4 Main Results</p>Results in full-data scenario. The overall comparison of the results in full-data scenario is shown in Table</p>1</p>. We can observe that: (i) Our method with only late prompt, that is LPT w/o PG can greatly improve the performance of the traditional prompt tuning under the same number of tunable parameters and even is comparable with P-Results in few-shot scenario We further evaluate our method in few-shot scenario. Following</p><ref type="bibr" target="#b41">Wu et al. (2022)</ref></p>, we consider two settings where the number of training data is 100 and 500, respectively. We randomly sample the training samples</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results in the few-shot scenario of 500 training samples. We report mean and standard deviation of performance over 4 different data splits for all the methods. Bold and Underline indicate the best and the second best results. All the results are obtained using RoBERTa LARGE .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>models. For each backbone, we select the largest batch size Results on two single-sentence and two sentence-pair tasks using DeBERTa LARGE and GPT2 LARGE models as the backbone. Bold and Underline indicate the best and the second best results.</figDesc><table><row><cell>Method</cell><cell>Tunable Parameters</cell><cell>Subj (acc)</cell><cell>TREC (acc)</cell><cell>MRPC (acc and F1)</cell><cell>RTE (acc)</cell><cell>Avg</cell></row><row><cell></cell><cell></cell><cell cols="2">DeBERTaLARGE</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model Tuning</cell><cell>406M</cell><cell>97.4</cell><cell>97.4</cell><cell>91.2</cell><cell>87.5</cell><cell>93.4</cell></row><row><cell>Prompt Tuning</cell><cell>21K</cell><cell cols="2">94.2 (0.5) 87.7 (2.0)</cell><cell>79.8 (1.6)</cell><cell cols="2">64.6 (3.7) 81.6</cell></row><row><cell>LPT w/o PG</cell><cell>21K</cell><cell cols="2">94.9 (0.5) 94.4 (0.3)</cell><cell>81.4 (1.2)</cell><cell cols="2">75.1 (1.9) 86.5</cell></row><row><cell>LPT w/ NPG</cell><cell>792K</cell><cell cols="2">96.5 (0.2) 96.3 (0.3)</cell><cell>90.8 (0.8)</cell><cell cols="2">84.4 (0.7) 92.0</cell></row><row><cell>LPT w/ MPPG</cell><cell>263K</cell><cell cols="2">96.9 (0.2) 97.3 (0.3)</cell><cell>89.6 (1.0)</cell><cell cols="2">81.1 (1.6) 91.2</cell></row><row><cell>LPT w/ APPG</cell><cell>263K</cell><cell cols="2">96.5 (0.2) 97.0 (0.2)</cell><cell>89.7 (1.2)</cell><cell cols="2">82.6 (1.3) 91.5</cell></row><row><cell></cell><cell></cell><cell cols="2">GPT2LARGE</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model Tuning</cell><cell>774M</cell><cell>97.2</cell><cell>97.0</cell><cell>88.0</cell><cell>75.8</cell><cell>89.5</cell></row><row><cell>Prompt Tuning</cell><cell>26K</cell><cell cols="2">88.8 (1.0) 82.7 (1.1)</cell><cell>75.1 (0.5)</cell><cell cols="2">53.7 (1.3) 75.1</cell></row><row><cell>LPT w/o PG</cell><cell>26K</cell><cell cols="2">94.9 (1.2) 93.7 (2.3)</cell><cell>77.3 (1.3)</cell><cell cols="2">57.8 (2.1) 80.9</cell></row><row><cell>LPT w/ NPG</cell><cell>990K</cell><cell cols="2">96.0 (0.3) 96.1 (0.4)</cell><cell>82.9 (1.0)</cell><cell cols="2">69.9 (1.0) 86.2</cell></row><row><cell>LPT w/ MPPG</cell><cell>329K</cell><cell cols="2">95.9 (0.3) 96.3 (0.5)</cell><cell>85.6 (0.4)</cell><cell cols="2">71.6 (0.6) 87.4</cell></row><row><cell>LPT w/ APPG</cell><cell>329K</cell><cell cols="2">95.6 (0.3) 96.7 (0.3)</cell><cell>85.7 (0.2)</cell><cell cols="2">72.9 (0.8) 87.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>models. As shown in Figure5, the most intermediate</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell><cell>Tuable Parameters</cell><cell cols="2">Training Speed Memory Cost tokens/ms (?) GB (?)</cell></row><row><cell></cell><cell></cell><cell cols="2">RoBERTaLARGE</cell><cell></cell></row><row><cell cols="2">Model Tuning 52.0 (1.9)</cell><cell>355M</cell><cell>11.6</cell><cell>23.5</cell></row><row><cell>Adapter</cell><cell>50.3 (2.5)</cell><cell>1.6M</cell><cell>15.5 (1.3?)</cell><cell>16.5 (29.8%)</cell></row><row><cell>AdapterDrop</cell><cell>49.4 (3.4)</cell><cell>811K</cell><cell>21.6 (1.9?)</cell><cell>9.5 (59.6%)</cell></row><row><cell>BitFit</cell><cell>50.2 (1.8)</cell><cell>273K</cell><cell>16.5 (1.4?)</cell><cell>15.7 (33.2%)</cell></row><row><cell>LoRA</cell><cell>50.1 (2.7)</cell><cell>788K</cell><cell>16.4 (1.4?)</cell><cell>16.2 (31.1%)</cell></row><row><cell cols="2">Prompt Tuning 58.2 (1.7)</cell><cell>21K</cell><cell>16.9 (1.5?)</cell><cell>17.8 (24.3%)</cell></row><row><cell>P-tuning v2</cell><cell>53.2 (2.4)</cell><cell>985K</cell><cell>19.2 (1.7?)</cell><cell>16.8 (28.5%)</cell></row><row><cell cols="2">S-IDPG-PHM 58.8 (1.9)</cell><cell>114K</cell><cell>12.0 (1.0?)</cell><cell>16.8 (28.5%)</cell></row><row><cell>LPT w/ NPG</cell><cell>69.5 (3.1)</cell><cell>792K</cell><cell>23.2 (2.0?)</cell><cell>10.1 (56.6%)</cell></row><row><cell cols="2">LPT w/ MPPG 62.4 (3.1)</cell><cell>263K</cell><cell>23.4 (2.0?)</cell><cell>10.6 (54.9%)</cell></row><row><cell cols="2">LPT w/ APPG 63.0 (2.2)</cell><cell>263K</cell><cell>23.4 (2.0?)</cell><cell>10.6 (54.9%)</cell></row><row><cell></cell><cell></cell><cell>GPT2LARGE</cell><cell></cell><cell></cell></row><row><cell cols="2">Model Tuning 50.0 (1.9)</cell><cell>774M</cell><cell>2.6</cell><cell>22.1</cell></row><row><cell>Adapter</cell><cell>52.8 (2.9)</cell><cell>3.0M</cell><cell>3.3 (1.3?)</cell><cell>11.8 (46.6%)</cell></row><row><cell>AdapterDrop</cell><cell>49.9 (0.9)</cell><cell>1.5M</cell><cell>6.0 (2.3?)</cell><cell>8.4 (62.0%)</cell></row><row><cell>BitFit</cell><cell>51.3 (2.4)</cell><cell>511K</cell><cell>4.3 (1.7?)</cell><cell>11.5 (48.0%)</cell></row><row><cell>LoRA</cell><cell>52.6 (1.9)</cell><cell>740K</cell><cell>4.1 (1.6?)</cell><cell>11.5 (47.1%)</cell></row><row><cell cols="2">Prompt Tuning 50.3 (1.2)</cell><cell>26K</cell><cell>4.4 (1.7?)</cell><cell>13.6 (38.5%)</cell></row><row><cell>P-tuning v2</cell><cell>49.7 (1.9)</cell><cell>1.9M</cell><cell>4.5 (1.7?)</cell><cell>13.0 (41.2%)</cell></row><row><cell cols="2">S-IDPG-PHM 52.1 (2.3)</cell><cell>171K</cell><cell>3.2 (1.2?)</cell><cell>12.7 (42.5%)</cell></row><row><cell>LPT w/ NPG</cell><cell>56.9 (2.0)</cell><cell>990K</cell><cell>6.0 (2.3?)</cell><cell>9.4 (57.5%)</cell></row><row><cell cols="2">LPT w/ MPPG 54.2 (2.6)</cell><cell>329K</cell><cell>6.2 (2.4?)</cell><cell>9.6 (56.6%)</cell></row><row><cell cols="2">LPT w/ APPG 53.6 (1.7)</cell><cell>329K</cell><cell>6.2 (2.4?)</cell><cell>9.6 (56.6%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison of parameter efficiency, training efficiency and memory cost for all the methods on two different backbone models. All methods are evaluated on RTE dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 10 :</head><label>10</label><figDesc>Trade-off between performance and training efficiency. 'PL' denotes the prompt layer. Bold and Underline marks the best and the second best results, respectively. All methods are evaluated on RTE dataset using RoBERTa LARGE model.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by the <rs type="funder">National Key Research and Development Program of China</rs> (No.<rs type="grantNumber">2020AAA0106700</rs>) and <rs type="funder">National Natural Science Foundation of China</rs> (No.<rs type="grantNumber">62022027</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_bq8yNpJ">
					<idno type="grant-number">2020AAA0106700</idno>
				</org>
				<org type="funding" xml:id="_6uv5EPY">
					<idno type="grant-number">62022027</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>The finding and proposed method aims to improve prompt tuning in terms of training costs and performance. The used datasets are widely used in previous work and, to our knowledge, do not have any attached privacy or ethical issues.    </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mc-Candlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note>NeurIPS 2020, December 6-12, 2020, virtual</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The PASCAL recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<idno type="DOI">10.1007/11736790_9</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges, Evaluating Predictive Uncertainty, Visual Object Classification and Recognizing Textual Entailment, First PASCAL Machine Learning Challenges Workshop, MLCW 2005</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Southampton, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005-04-11">2005. April 11-13, 2005</date>
			<biblScope unit="volume">3944</biblScope>
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Min</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weize</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2203.06904</idno>
		<idno>CoRR, abs/2203.06904</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Paraphrasing, IWP@IJCNLP 2005</title>
		<meeting>the Third International Workshop on Paraphrasing, IWP@IJCNLP 2005<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Asian Federation of Natural Language Processing</publisher>
			<date type="published" when="2005-10">2005. October 2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.295</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08-01">2021. August 1-6, 2021</date>
			<biblScope unit="page" from="3816" to="3830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deberta: decoding-enhanced bert with disentangled attention</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2021-05-03">2021. May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for NLP</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruna</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML</title>
		<meeting>the 36th International Conference on Machine Learning, ICML<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06">2019. 2019, 9-15 June 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="2790" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">LoRA: Low-rank adaptation of large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Instance-aware prompt learning for language understanding and generation</title>
		<author>
			<persName><forename type="first">Feihu</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinliang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
		<idno>CoRR, abs/2201.07126</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.243</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-07-11">2021. 7-11 November, 2021</date>
			<biblScope unit="page" from="3045" to="3059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BART: denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05">Marjan Ghazvininejad,. 2020. July 5-10, 2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.353</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08-01">2021. August 1-6, 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4582" to="4597" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Xiangyang Liu, and Xipeng Qiu. 2021. A survey of transformers</title>
		<author>
			<persName><forename type="first">Tianyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wang</surname></persName>
		</author>
		<idno>CoRR, abs/2106.04554</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Xuanjing Huang, and Xipeng Qiu. 2022a. Towards efficient NLP: A standard evaluation and A strong baseline</title>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junliang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingling</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.240</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022<address><addrLine>Seattle, WA, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">July 10-15, 2022</date>
			<biblScope unit="page" from="3288" to="3303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">2022b. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixuan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-short.8</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="61" to="68" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno>abs/2103.10385</idno>
	</analytic>
	<monogr>
		<title level="j">GPT understands</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Compacter: Efficient low-rank hypercomplex adapter layers</title>
		<author>
			<persName><forename type="first">Rabeeh</forename><surname>Karimi Mahabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021</title>
		<imprint>
			<date type="published" when="2021-12-06">2021. December 6-14, 2021</date>
			<biblScope unit="page" from="1022" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.3115/1218955.1218990</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2004-07">2004. July, 2004</date>
			<biblScope unit="page" from="271" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.3115/1219840.1219855</idno>
	</analytic>
	<monogr>
		<title level="m">ACL 2005, 43rd Annual Meeting of the</title>
		<meeting><address><addrLine>University of Michigan, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computer Linguistics</publisher>
			<date type="published" when="2005-06-30">2005. 25-30 June 2005</date>
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
	<note>Proceedings of the Conference</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. 2019. December 8-14, 2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pre-trained models for natural language processing: A survey</title>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yige</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11431-020-1647-3</idno>
	</analytic>
	<monogr>
		<title level="j">SCIENCE CHINA Technological Sciences</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>OpenAI blog</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Squad: 100, 000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2016-11-01">2016. 2016. November 1-4, 2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adapterdrop: On the efficiency of adapters in transformers</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>R?ckl?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregor</forename><surname>Geigle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Glockner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tilman</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.626</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-07-11">2021. 7-11 November, 2021</date>
			<biblScope unit="page" from="7930" to="7946" />
		</imprint>
	</monogr>
	<note>Nils Reimers, and Iryna Gurevych</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2013-10">2013. 18-21 October 2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Xipeng Qiu, and Xuan-Jing Huang. 2022a. Paradigm shift in natural language processing</title>
		<author>
			<persName><forename type="first">Tianxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11633-022-1331-6</idno>
		<imprint>
			<publisher>Machine Intelligence Research</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Xuanjing Huang, and Xipeng Qiu. 2022b. Black-box tuning for language-model-as-a-service</title>
		<author>
			<persName><forename type="first">Tianxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Qian</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<meeting><address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-07">July 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="20841" to="20855" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Context-tuning: Learning contextualized prompts for natural language generation</title>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<idno>CoRR, abs/2201.08670</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">2017. 2017. December 4-9, 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Building a question answering test collection</title>
		<author>
			<persName><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><forename type="middle">M</forename><surname>Tice</surname></persName>
		</author>
		<idno type="DOI">10.1145/345508.345577</idno>
	</analytic>
	<monogr>
		<title level="m">SIGIR 2000: Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000-07-24">2000. July 24-28, 2000</date>
			<biblScope unit="page" from="200" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. 2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">titan: Exploring larger-scale knowledge enhanced pre-training for language understanding and generation</title>
		<author>
			<persName><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weibao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyuan</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangfan</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiuliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno>CoRR, abs/2112.12731</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">List: Lite prompted self-training makes parameter-efficient few-shot learners</title>
		<author>
			<persName><forename type="first">Yaqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhabrata</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Hassan Awadallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.findings-naacl.174</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: NAACL 2022</title>
		<meeting><address><addrLine>Seattle, WA, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-07-10">2022. July 10-15, 2022</date>
			<biblScope unit="page" from="2262" to="2281" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Revisiting locally supervised learning: an alternative to end-to-end training</title>
		<author>
			<persName><forename type="first">Yulin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zanlin</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiji</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-05-03">2021. May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>OpenReview. net</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Annotating expressions of opinions and emotions in language</title>
		<author>
			<persName><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10579-005-7880-9</idno>
	</analytic>
	<monogr>
		<title level="j">Lang. Resour. Evaluation</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="165" to="210" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-01">2018. June 1-6, 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
	<note>Bowman. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, EMNLP 2020 -Demos, Online</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, EMNLP 2020 -Demos, Online</meeting>
		<imprint>
			<date type="published" when="2020-11-16">2020. November 16-20, 2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">IDPG: an instance-dependent prompt generation method</title>
		<author>
			<persName><forename type="first">Zhuofeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G</forename><surname>Vinod Vydiswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.403</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022<address><addrLine>Seattle, WA, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-07-10">2022. July 10-15, 2022</date>
			<biblScope unit="page" from="5507" to="5521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. 2019. December 8-14, 2019</date>
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Ben Zaken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05-22">2022. May 22-27, 2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>Short Papers), ACL 2022</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Beyond fully-connected layers with quaternions: Parameterization of hypercomplex multiplications with 1/n parameters</title>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvin</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siu</forename><forename type="middle">Cheung</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-05-03">2021. May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>OpenReview. net</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
