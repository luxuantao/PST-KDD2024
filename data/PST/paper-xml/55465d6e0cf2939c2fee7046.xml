<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Group Happiness Intensity Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Abhinav</forename><surname>Dhall</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Roland</forename><surname>Goecke</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Tom</forename><surname>Gedeon</surname></persName>
						</author>
						<title level="a" type="main">Automatic Group Happiness Intensity Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CADB8FF7DC90F9EEBC5576D58FE37EA1</idno>
					<idno type="DOI">10.1109/TAFFC.2015.2397456</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2015.2397456, IEEE Transactions on Affective Computing This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2015.2397456, IEEE Transactions on Affective Computing This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2015.2397456, IEEE Transactions on Affective Computing This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2015.2397456, IEEE Transactions on Affective Computing This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2015.2397456, IEEE Transactions on Affective Computing This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2015.2397456, IEEE Transactions on Affective Computing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Facial expression recognition</term>
					<term>group mood</term>
					<term>unconstrained conditions</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recent advancement of social media has given users a platform to socially engage and interact with a larger population. Millions of images and videos are being uploaded everyday by users on the web from different events and social gatherings. There is an increasing interest in designing systems capable of understanding human manifestations of emotional attributes and affective displays. As images and videos from social events generally contain multiple subjects, it is an essential step to study these groups of people. In this paper, we study the problem of happiness intensity analysis of a group of people in an image using facial expression analysis. A user perception study is conducted to understand various attributes, which affect a person's perception of the happiness intensity of a group. We identify the challenges in developing an automatic mood analysis system and propose three models based on the attributes in the study. An 'in the wild' image-based database is collected. To validate the methods, both quantitative and qualitative experiments are performed and applied to the problem of shot selection, event summarisation and album creation. The experiments show that the global and local attributes defined in the paper provide useful information for theme expression analysis, with results close to human perception results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Automatic facial expression analysis has seen much research in recent times. However, little attention has been given to the estimation of the overall expression theme conveyed by a group of people in an image. With the growing popularity of data sharing and broadcasting websites such as YouTube and Flickr, every day users are uploading millions of images and videos of social events such as a party, wedding or a graduation ceremony. Generally, these videos and images were recorded in different conditions and may contain one or more subjects. From a view of automatic emotion analysis, these diverse scenarios have received less attention in the affective computing community.</p><p>Consider an illustrative example of inferring the mood of a group of people posing for a group photograph at a school reunion. To scale the current emotion detection algorithms to work on this type of data in the wild, there are several challenges to overcome such as emotion modelling of groups of people, labelled data, and face analysis. Expression analysis has been a long studied problem, focussing on inferring the emotional state of a single subject only. This paper discusses the problem of automatic mood analysis of a group of people. Here, we are interested in knowing an individual's intensity of happiness and its contribution to the overall mood of the scene. The contribution towards the theme expression can be affected by the social context. The context can constitute various global and local factors (such as the relative position of the person in the image, their distance from the camera and the level of face occlusion). We model this global and local information based on a group graph, embed these features in our method and pose</p><p>• A. Dhall and R. Goecke are with the University of Canberra. T. Gedeon is with the Australian National University E-mail: {abhinav.dhall, tom.gedeon}@anu.edu.au, roland.goecke@ieee.org the problem in a probabilistic graphical model framework based on a relatively weighted soft-assignment.</p><p>Analysing the theme expression conveyed by groups of people in images is an unexplored problem that has many real-world applications: image search, retrieval, representation and browsing; event summarisation and highlight creation; candid photo shot selection; expression apex detection in video; video thumbnail creation etc. A recent Forbes magazine article <ref type="bibr" target="#b0">[1]</ref> discusses the lack of ability of current image search engines to use context. Information such as the mood of a group can be used to model the context. These problems, where group mood information can be utilised, are a motivation for exploring the various group mood models. One basic approach is to average the happiness intensities of all people in a group. However, the perception of the mood of a group is defined by attributes such as where people stand, how much of their face is visible etc. These social attributes play an important role in defining the overall happiness 1 an image conveys.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">KEY CONTRIBUTIONS</head><p>1) An automatic framework for happiness intensity analysis of a group of people in images based on the social context. 2) A weighted model is presented, taking into consideration the global and local attributes that affect the perceived happiness intensity of a group. 3) A labelled 'in the wild' database containing images of groups of people is collected using a semi-automatic process and compared with existing databases. The remainder of the paper is organised as follows: Section 3 discusses prior work on various aspects of the visual analysis of a group of people. Section 4 describes the problems and challenges involved in automatic group expression analysis. The details of a 149-user survey investigating attributes affecting the perception of mood are discussed in Section 5. An 'in the wild' database collection method is detailed in Section 6. Section 7 discusses a basic model based on averaging. Global context based social features are presented in Section 8. Local context based on occlusion intensity is described in Section 9. The global and local contexts are combined and applied to the averaging approach in Section 10. The manual attributes are combined with data-driven attributes in a supervised hierarchal Bayesian framework in Section 11. Section 12 discusses the results of the proposed frameworks, including both quantitative and qualitative experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LITERATURE REVIEW</head><p>The analysis of a group of people in an image or a video has recently received much attention in computer vision. Methods can be broadly divided into two categories: a) Bottom-up methods: The subject's attributes are used to infer information at the group level <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>; b) Topdown methods: The group/sub-group information is used as a prior for inference of subject level attributes <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bottom-up Techniques</head><p>Tracking groups of people in a crowd has been of particular interest lately <ref type="bibr" target="#b1">[2]</ref>. Based on trajectories constructed from the movement of people, <ref type="bibr" target="#b1">[2]</ref> propose a hierarchical clustering algorithm which detects sub-groups in crowd video clips. In an interesting experiment, <ref type="bibr" target="#b2">[3]</ref> installed cameras at four locations on the MIT campus and tried to estimate the mood of people looking into the camera and compute a mood map for the campus using the Shore framework <ref type="bibr" target="#b6">[7]</ref> for face analysis, which detects multiple faces in a scene in real-time. The framework also generates attributes such as age, gender and pose. In <ref type="bibr" target="#b2">[3]</ref>, the scene level happiness averages the individual persons' smiles. However, in reality, group emotion is not an averaging model <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. There are attributes, which affect the perception of a group's emotion and the emotion of the group itself. The literature in social psychology suggests that group emotion can be conceptualised in different ways and is best represented by pairing the top-down and bottom-up approaches <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>.</p><p>In another interesting bottom-up method, <ref type="bibr" target="#b9">[10]</ref> proposed group classification for recognising urban tribes (a group of people part of a common activity). Low-level features, such as colour histograms, and high-level features, such as age, gender, hair and hat, were used as attributes (using the Face.com API) to learn a Bag-of-Words (BoW)-based classifier. To add the group context, a histogram describing the distance between two faces and the number of overlapping bounding boxes was computed. Fourteen classes depicting various groups, such as 'informal club', 'beach party' and 'hipsters', were used. The experiments showed that a combination of attributes can be used to describe a type of group. In 'Hippster wars' <ref type="bibr" target="#b10">[11]</ref>, a framework based on clothes related features was proposed for classifying a group of people based on their social group type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Top-down Techniques</head><p>In an interesting top-down approach, <ref type="bibr" target="#b4">[5]</ref> proposed contextual features based on the group structure for computing the age and gender of individuals. The global attributes described here are similar to <ref type="bibr" target="#b4">[5]</ref>'s contextual features of social context. However, the problem in <ref type="bibr" target="#b4">[5]</ref> is inverse to the problem of inferring the mood of a group of people in an image, which is discussed in this paper. Their experiments on images obtained from the web, show an impressive increase in performance when the group context is used. In another top-down approach, <ref type="bibr" target="#b5">[6]</ref> model the social relationship between people standing together in a group for aiding recognition. The social relationships are inferred in unseen images by learning them from weakly labelled images. A graphical model based on social relationships, such as 'father-child' and 'mother-child', and social relationship features, such as relative height, height difference and face ratio. In <ref type="bibr" target="#b11">[12]</ref> a face discovery method based on exploring social features, such as on social event images, is proposed.</p><p>In object detection and recognition work by <ref type="bibr" target="#b12">[13]</ref>, scene context information and its relationship with the objects is described. Moreover, <ref type="bibr" target="#b13">[14]</ref> acknowledges the benefit of using global spatial constraints for scene analysis. In face recognition <ref type="bibr" target="#b14">[15]</ref>, social context is employed to model the relationship between people, e.g. between friends on Facebook, using a Conditional Random Field (CRF) <ref type="bibr" target="#b15">[16]</ref>.</p><p>Recently, <ref type="bibr" target="#b16">[17]</ref> proposed a framework for selecting candid shots from a video of a single person. A physiological study was conducted, where 150 subjects were shown images of a person. They were asked to rate the attractiveness of the images and mention attributes, which influenced their decision. Professional photographers were also asked to label the images. Further, a regression model was learnt based on various attributes, such as eye blink, clarity of face and face pose. A limitation of this approach is that the samples contain a single subject only. <ref type="bibr" target="#b17">[18]</ref> proposed affect based video clip browsing by learning two regression models, predicting valence and arousal values, to describe the affect. The regression models learnt on an ensemble of audio-video features, such as motion, shot switch rate, frame brightness, pitch, bandwidth, roll off, and spectral flux. However, expression information for individuals or groups in the scenes was not used.</p><p>The literature for analysing a single subject's happiness / smile is rich. One prominent approach by <ref type="bibr" target="#b18">[19]</ref> proposed a new image-based database labelled for smiling and nonsmiling images and evaluated several state of the art methods for smile detection. However, in the existing literature, the faces are considered independent of each other. For computing the contribution of each subject, two types of factors affect group level emotion analysis: (1) Local factors (individual subject level): age, gender, face visibility, face pose, eye blink etc. (2) Global factors: where do people stand, with whom people stand etc. In this paper, the focus is on face visibility, smile intensity, relative face size and relative face distance. Labelled images containing groups of people are required, which we collect from Flickr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CHALLENGES</head><p>The following subsections discuss the challenges in creating an automatic system for happiness intensity inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Attributes</head><p>Human perception of the mood of a group of people is very subjective. <ref type="bibr" target="#b8">[9]</ref> argue that the mood of a group is composed by two broad categories of components: topdown and bottom-up. Top-down is the affective context, i.e. attributes such as group history, background, social event etc., which have an effect on the group members. For example, a group of people laughing at a party displays happiness in a different way than a group of people in an office meeting room. From an image perspective, this means that the scene/background information can be used as affective context. The bottom-up component deals with the subjects in the group in terms of attributes of individuals that affect the perception of the group's mood. It defines the contribution of individuals to the overall group mood.</p><p>From now on, the top-down component is referred to as 'global context' and the bottom-up component as 'local context'. There can be various attributes, which define these two components. For example, global context contains but is not limited to scene information, social event information, who is standing with whom, where are people standing in an image and w.r.t. the camera. Local context, i.e. subject specific attributes, cover an individual's mood/emotion, face visibility, face size w.r.t. neighbours, age, gender, head pose and eye blink. To further understand these attributes, a perception user study is performed. The study and its results are detailed in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data and Labelling</head><p>Data simulating 'in the wild' conditions is a major challenge for making emotion recognition methods work in real-world conditions. Generally, emotion analysis databases are lab-recorded and contain a single subject in an image or video. It is easy to ask people to pose in a laboratory, but acted expressions are very different from spontaneous ones. Anyone working in emotion analysis will attest to the difficulty of collecting spontaneous data in realworld conditions. For learning and testing an emotion analysis system, labelled data containing groups of people in different social scenarios is required. Once the data is available, the next task is labelling. According to <ref type="bibr" target="#b19">[20]</ref>, moods are low-intensity, diffuse affective states that usually do not have a clear antecedent. Mood can be positive/pleasant and negative/unpleasant. The type of labelling (discrete or continuous) required is problem dependent. The database proposed in this paper -HAPPEI -is labelled for neutral to pleasant mood with discrete levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Visual Analysis</head><p>Inferring the group mood involves classic computer vision tasks. As a pre-processing step, the first challenge is face and facial landmark detection. Ideally, for a facial dynamics analysis system, one will want a subject independent facial landmark detector <ref type="bibr" target="#b20">[21]</ref>. Further, <ref type="bibr" target="#b20">[21]</ref> argue that a subject dependent facial parts method, such as Active Appearance Models (AAM) <ref type="bibr" target="#b21">[22]</ref>, performs better than subject independent Constrained Local Models (CLM) <ref type="bibr" target="#b22">[23]</ref>. However, if a proper descriptor is used on top of previously aligned faces from a subject independent detector, the aligment error can be compensated for. Moreover, <ref type="bibr" target="#b23">[24]</ref> show the effectiveness of the Mixture of Pictorial Structures <ref type="bibr" target="#b23">[24]</ref> over CLM and AAM for facial landmark localisation when there is much head movement. Motivated by these arguments, the parts based model of <ref type="bibr" target="#b24">[25]</ref> <ref type="foot" target="#foot_0">2</ref> is used here (Section 12). The images in the new database HAPPEI were downloaded from Flickr, creating the challenge of real-world varied illumination scenarios and differences in face resolution. To overcome these, LPQ and PHOG are used, as LPQ is robust to varied illumination and PHOG is robust to scale <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SURVEY</head><p>To understand the attributes (Section 4.1) affecting the perception of the group mood, a user study was conducted. Two sets of surveys were developed. In the first part (Figure <ref type="figure" target="#fig_0">1</ref>), subjects were asked to compare two images for their apparent mood and rate the one with a higher positive mood. Further, they were asked various questions about the attibutes/reasons, which made them choose a specific image/group out of the two images/groups. A total of 149 subjects participated in this survey (94 males / 55 females). There are a total of three cases in the first survey. Figure <ref type="figure" target="#fig_0">1</ref> shows the questions in one of the cases. Cases 1, 2 and 3 in Figure <ref type="figure">2</ref> describe the analysis of the responses of the participants for the three cases in the survey. On the left of the figures, the two images to be compared are displayed.</p><p>The images in the survey were chosen on the basis of two criteria: (1) to validate the hypothesis that adding an occlusion attribute to the model decreased the error (user perception vs. model output), which was noticed in the earlier experiments in <ref type="bibr" target="#b26">[27]</ref>. Therefore, in one case, two images shot in succession were chosen, in which one of the subjects covered his face in the first shot (Figure <ref type="figure">2</ref> Case 1). It is interesting to note that a larger number of survey participants (69.0%) chose image B in Figure <ref type="figure">2</ref> Case 1 as having a more positive mood score on the scale of neutral towards thrilled. Out of these 69.0%, 51.1% chose 'faces being less occluded' as one of the reasons. The other dominating attribute for their decision was the larger number of people smiling (54.6%). Both attributes are correlated; it is easier to infer the expression of a person when the face is clearly visible.</p><p>(2) In the other case (Figure <ref type="figure">2</ref> Case 2), two images were randomly chosen. 76.0% of participants ranked Image A higher than Image B. 46.0% of participants chose 'larger number of people smiling' as the most dominant attribute (49.1% for participants who selected Image A and 37.1% for participants who selected Image B). 46.4% of participants who chose Image A selected 'large smiles of people in the center of the group'. Also, 56.0% of the participants chose the presence of a pleasant background as the reason for their selection. In the question: 'Any other reason for your choice?', participants pointed to the context, background, togetherness of the group, and party like scenario in Image A (Figure <ref type="figure">2</ref> Case 2), which made them consider the group in Image A being happier. Other considerable responses were body pose, people closer to each other in the group and spontaneous facial expressions (i.e. when subjects are not explicitly posing in front of the camera). For the question: 'Please define the scene in one word in Image A' and 'Please define the scene in one word in Image B', the majority of the participants defined the scene in the context of mood such as 'relaxed', 'pleasant', 'interesting', 'happening', or 'enjoyable'.</p><p>In Figure <ref type="figure">2</ref> Case 3, 41.0% of the survey participants chose 'the large size of face(s) with smiles in the image'. For the question 'What are the dominating attributes/characteristics of the leader(s) in the group that affect the group's mood?', the participants mentioned the large size of face(s) with smiles in the image, the centre location of the subject, large smiles, and people standing closer. Based on the user survey, in the following sections, the attributes which are discussed further are relative location of members of a group, relative face size to estimate if a person is in the front or back, face visibility/occlusion and mood of a member. In a recent study by Jiang et al. <ref type="bibr" target="#b27">[28]</ref>, the authors conducted an eye tracking based study to locate the objects and attributes, which are salient in group/crowd images. Their saliency related attributes are similar to our findings. Based on observations from the fixations, <ref type="bibr" target="#b27">[28]</ref> proposed high-level features related to attributes such as face size, face occlusion and pose. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DATABASE CREATION AND LABELLING</head><p>Popular facial expression databases, such as CK+ <ref type="bibr" target="#b28">[29]</ref>, FEEDTUM <ref type="bibr" target="#b29">[30]</ref> and MultiPIE <ref type="bibr" target="#b30">[31]</ref>, are all 'individual' centric databases, i.e. contain a single subject only in any particular image or video. For the problem in this work, there are various databases, which are partially relevant <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b3">[4]</ref>. In an interesting work, <ref type="bibr" target="#b18">[19]</ref> proposed the GENKI database. It was collected from Google images shot in unconstrained conditions containing a single subject per image smiling or non-smiling. However, it does not fulfill our requirements as the intensity level of happiness is not labelled at both image and face level. <ref type="bibr" target="#b3">[4]</ref> propose a dynamic facial expressions database -Acted Facial Expressions In The Wild (AFEW) -collected from movies. It contains both single and multiple subject videos. However, there are no intensity labels present and multiple subject video clips are few in number. Therefore, a new labelled database for image-based group mood analysis is required. Databases such as GENKI and AFEW have been compiled using semi-automatic approaches. <ref type="bibr" target="#b18">[19]</ref> used Google images based on a keyword search for finding relevant images. [4] used a recommender system based approach where a system suggested video clips to the labellers based on emotion related keywords in closed caption subtitles. This makes the process of database creation and labelling easier and less time consuming. Inspired by <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b18">[19]</ref>, a semi-automatic approach is followed. Web based photo sharing websites such as Flickr and Facebook contain billions of images. From a research perspective, not only are these a huge repository of images but also come with rich associated labels, which contain very useful information describing the scene in the images.</p><p>We collected a labelled 'in the wild' database -called HAPpy PEople Images (HAPPEI) -from Flickr containing 4886 images. A Matlab based program was developed to automatically search and download images, which had keywords associated with groups of people and events. A total of 40 keywords were used (e.g. 'party + people', 'group + photo', 'graduation + ceremony', 'marriage', 'bar', 'reunion', 'function', 'convocation'). After downloading the images, a Viola-Jones object detector trained on different data (frontal and pose models in OpenCV) was executed on the images. Only images containing more than one subject were kept. False detections were manually removed. Figure <ref type="figure" target="#fig_3">3</ref>(a) shows a collage of images from the database.</p><p>All images were annotated with a group level mood intensity ('neutral' to 'thrilled'). Moreover, in the 4886 images, 8500 faces were manually annotated for face level happiness intensity, occlusion intensity and pose by four human labelers, who annotated different images. The mood was represented by the happiness intensity corresponding to six stages of happiness: Neutral, Small Smile, Large Smile, Small Laugh, Large Laugh and Thrilled (Figure <ref type="figure" target="#fig_3">3(b)</ref>). As a reference during labelling, when the teeth of a member of a group were visible, the happiness intensity was labelled as a laugh. When the mouth was open wide, a Thrilled label was assigned. A face with a closed mouth was assigned the  label Smile. The LabelMe <ref type="bibr" target="#b31">[32]</ref> based Bonn annotation tool <ref type="bibr" target="#b32">[33]</ref> was used for labelling. It is interesting to note that ideally one would like to infer the mood of a group by the means of self-rating along with the perception of mood of the group. In this work, no self-rating was conducted as the data was collected from the internet. In this database, the labels are based on the perception of the labellers. One can see this work as a stepping stone to group mood analysis. The aim of the models (Sections 7, 10 and 11.1) proposed in this work is to infer the perceived group mood as closely as possible to human observers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">GROUP EXPRESSION MODEL</head><p>Given an image I containing a group of people G of size s and their happiness intensity level I H , a simple Group Expression Model (GEM) can be formulated as an average of the happiness intensities of all faces in the group</p><formula xml:id="formula_0">GEM = i I Hi s .<label>(1)</label></formula><p>In this simple formulation, both global information, e.g. the relative position of people in the image, and local information, e.g. the level of occlusion of a face, are ignored. In order to add the bottom-up and top-down components (Section 4.1), it is proposed here to add these social context features as weights to the process of determining the happiness intensity of a group image. The experiments (Section 12) on HAPPEI confirm the positive effect of adding social feature weights to GEM . In the next section, methods for computing the global context are discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">GLOBAL CONTEXT</head><p>Barsade and Gibson <ref type="bibr" target="#b7">[8]</ref> as well as Kelley and Barsade <ref type="bibr" target="#b8">[9]</ref> emphasise the contribution of the top-down component to the perception of the happiness intensity of a group.</p><p>Here, the top-down contribution represents the effect of the group on a subject. Furthermore, in the survey (Section 5), participants mentioned attributes such as location and face size of subjects in a group. In this section, we formulate the weights for describing the top-down component. The tip of the nose p i is considered as the position of a face f i in the image. To map the global structure of the group, a fully connected graph G = (V, E) is constructed. Here, V i ∈ G represents a face in the group and each edge represents the link between two faces (V i , V m ) ∈ E. The weight w(V i , V m ) is the Euclidean distance between p i and p m . Prim's minimal spanning tree algorithm <ref type="bibr" target="#b33">[34]</ref> is computed on G, which provides information about the relative position of people in the group with respect to their neighbours. In Figure <ref type="figure" target="#fig_4">4</ref>, the min-span tree of the group graph is shown.</p><p>Once the location and minimally connected neighbours of a face are known, the relative size of a face f i with respect to its neighbours is calculated. The size of a face is taken as the distance between the location of the eyes (intraocular distance), d i = ||l -r||. The relative face size θ i of f i is then given by</p><formula xml:id="formula_1">θ i = d i i d i /n<label>(2)</label></formula><p>where the term i d i /n is the mean face size in a region r around face f i , with r containing a total of n faces including f i . Generally speaking, the faces which have a larger size in a group photo are of the people who are standing closer to the camera. Here, it is assumed that the expression intensity of their faces contributes more to the perceived group mood intensity than the faces of people standing in the back. Eichner and Ferrari <ref type="bibr" target="#b34">[35]</ref> made a similar assumption to find if a person is standing in the foreground or at the back in a multiple people pose detection scenario.</p><p>Based on the centre locations p i of all faces in a group G, the centroid c g of G is computed. The relative distance δ i of each face f i is</p><formula xml:id="formula_2">δ i = ||p i -c g || . (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>δ i is further normalised based on the mean relative distance. Faces closer to the centroid are given a higher weighting than faces further away. Using Equations 2 and 3, a global weight is assigned to each face in the group</p><formula xml:id="formula_4">ψ i = ||1 -α δ i || * θ i 2 β-1<label>(4)</label></formula><p>where parameters α and β control the effect of these weight factors on the global weight. Figure <ref type="figure" target="#fig_5">5</ref> demonstrates the effect of the global context on the overall output of GEM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">LOCAL CONTEXT</head><p>In the previous section, global context features, which compute weights on the basis of two factors: (1) where do people stand in a group and (2) how far are they away from the camera, are defined. The bottom-up component of the framework is now defined in this section. The local Occlusion Intensity Estimate: Occlusion in faces, selfinduced (e.g. sunglasses) or due to interaction between people in groups (e.g. one person standing partially in front of another and occluding the face), is a common problem. Lind and Tang <ref type="bibr" target="#b35">[36]</ref> introduced an automatic occlusion detection and rectification method for faces via GraphCutbased detection and confidence sampling. They also proposed a face quality model based on global correlation and local patterns to derive occlusion detection and rectification.</p><p>The presence of occlusion on a face reduces its visibility and, therefore, hampers the clear estimation of facial expressions. It also reduces the face's contribution to the overall expression intensity of a group portrayed in an image. Because of this, the happiness intensity level I H of a face f i in a group is penalised if (at least partially) occluded. Thus, along with an automatic method for occlusion detection, an estimate of the level of occlusion is required. Unlike <ref type="bibr" target="#b35">[36]</ref>, it is proposed to learn a mapping model F : X → Y, where X are the descriptors calculated on the faces and Y is the amount of occlusion.</p><p>The mapping function F is learnt using the Kernel Partial Least Squares (KPLS) <ref type="bibr" target="#b36">[37]</ref> regression framework. The PLS set of methods has recently become very popular in computer vision <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>. In <ref type="bibr" target="#b38">[39]</ref> and <ref type="bibr" target="#b39">[40]</ref>, PLS is used for dimensionality reduction as a prior step before classification. <ref type="bibr" target="#b37">[38]</ref> use KPLS based regression for simultaneous dimensionality reduction and age estimation and show that KPLS works well for face analysis when the feature vector is high dimension. For the occlusion intensity estimation problem, the training set X is a set of input samples x i of dimension N . Y is the corresponding set of vectors y i of dimension M . Then, the PLS framework defines a decomposition of matrices X and Y as</p><formula xml:id="formula_5">X = TP T + E (5) Y = UQ T + E (<label>6</label></formula><formula xml:id="formula_6">)</formula><p>where T and U are the n × p score matrices of the p extracted latent projections. The N ×p matrix P and M ×p matrix Q denote the corresponding loading matrices and the n × N matrix E and n × M matrix F denote the residual matrices that account for the error made by the projection. The classic NIPALS method <ref type="bibr" target="#b40">[41]</ref> is used to solve the optimisation criteria</p><formula xml:id="formula_7">[cov(t, u)] 2 = [cov(Xw, Yc)] 2 = max |r|=|s|=1 [cov(Xr, Ys)] 2 (7)</formula><p>where cov(t; u) = t T u/n is the sample covariance between the score vectors t and u. The score vectors {t i } p i=1 are good predictors of Y and the inner relation between the score vectors t and u is given by U = TA + H, where A is a p × p diagonal matrix and H is the residual matrix.</p><p>To perform classification, the regression matrix B is calculated as</p><formula xml:id="formula_8">B = X T U(T T XX T U) -1 T T Y . (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>For a given test sample matrix X test , the estimated labels matrix Ŷ is given by</p><formula xml:id="formula_10">Ŷ = X test B<label>(9)</label></formula><p>For a detailed description, readers may refer to <ref type="bibr" target="#b36">[37]</ref>. Now, for a non-linear mapping, the kernel trick can be applied to the PLS method. X is substituted with Φ = [Φ(x 1 ), ..., Φ(x n )] T , which maps input data to a higherdimensional space. The kernel matrix is then defined by the Gram matrix K = ΦΦ T , in which the kernel function defines each element K i,j = k(x i , x j ). Therefore, Equations 8 and 9 can be rewritten as</p><formula xml:id="formula_11">Ŷ = K test R (10) R = U(T T K T U) -1 T T Y (<label>11</label></formula><formula xml:id="formula_12">)</formula><p>where K test = Φ test Φ T is the kernel matrix for test samples. The input sample vector x i is a normalised combination of Hue, Saturation and the PHOG <ref type="bibr" target="#b41">[42]</ref> for each face. In the training set, X contains both occluded and non-occluded faces, Y contains the labels identifying the amount of occlusion (where 0 signifies no occlusion). The labels were manually created during the database creation process (Section 6). The output label y i is used to compute the local weight λ i , which will penalise I H for a face f i in the presence of occlusion. It is defined as</p><formula xml:id="formula_13">λ i = ||1 -γy i || (<label>12</label></formula><formula xml:id="formula_14">)</formula><p>where γ is the parameter, which controls the effect of the local weight.</p><p>Happiness Intensity Computation: A regression based mapping function F is learnt using KPLS for regressing the happiness intensity of a subject's face. The input feature vector is the PHOG descriptor computed over aligned faces. As discussed earlier, the advantage of learning via KPLS is that it performs dimensionality reduction and prediction in one step. Moreover, KPLS based classification has been successfully applied to facial action units <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">WEIGHTED GROUP EXPRESSION MODEL</head><p>The global and local contexts defined in Eq. 4 and 12 are used to formulate the relative weight for each face f i as</p><formula xml:id="formula_15">π i = λ i ψ i (<label>13</label></formula><formula xml:id="formula_16">)</formula><p>This relative weight is applied to the I H of each face in the group G and based on Eq. 1 and 13, the new weighted GEM is defined as</p><formula xml:id="formula_17">GEM w = i I Hi π i s . (<label>14</label></formula><formula xml:id="formula_18">)</formula><p>This formulation takes into consideration the structure of the group and the local context of the faces in it. The contribution of each face f i 's I Hi towards the overall perception of the group mood is weighted relatively, here I Hi is the happiness intensity of f i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">SOCIAL CONTEXT AS ATTRIBUTES</head><p>The social features described above can also be viewed as manually defined attributes. From the survey (Section 5), it is evident that along with the social context features, there are many others such as age, attractiveness, and gender. The assumptions in GEM w do not hold true for some scenarios, for example: when a baby is in the lap of the mother. In this case, the system will give a higher weight to the mother and a lower weight to the baby, assuming that the mother is in the front and the baby is in the background. In order to implicitly add the effect of other attributes, a feature augmentation approach is presented next. Lately, attributes have been very popular in the computer vision community (e.g. <ref type="bibr" target="#b43">[44]</ref>). Attributes are defined as high-level semantically meaningful representations. They have been, for example, successfully applied to object recognition <ref type="bibr" target="#b43">[44]</ref>, scene analysis <ref type="bibr" target="#b44">[45]</ref> and face analysis <ref type="bibr" target="#b45">[46]</ref>.</p><p>Based on the regressed happiness intensities, the attributes are defined as Neutral, Small Smile, Large Smile, Small Laugh, Large Laugh, Thrilled, and for occlusion as Face Visible, Partial Occlusion and High Occlusion. These attributes are computed for each face in the group. Attributes based on global context are Relative Distance and Relative Size. Figure <ref type="figure" target="#fig_6">6</ref> describes the manual attributes for faces in a group.</p><p>Defining attributes manually is a subjective task, which can result in many important discriminative attributes being ignored. Inspired by <ref type="bibr" target="#b46">[47]</ref>, low-level feature based attributes are computed. They propose the use of manually defined attributes along with data-driven attributes. Their experiments show a leap in performance for human action recognition based on a combination of manual and datadriven attributes. A weighted bag of visual words based on extracting low-level features is computed.</p><p>Furthermore, a topic model is learnt using Latent Dirichlet allocation (LDA) <ref type="bibr" target="#b47">[48]</ref>. The manually defined and weighted data-driven attributes are combined to form a single feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.1">Augmented Group Expression Model</head><p>Topic models, though originally developed for document analysis, have been successfully applied to computer vision problems. One very popular topic modelling technique is LDA <ref type="bibr" target="#b47">[48]</ref>, a hierarchical Bayesian model, where topic proportions for a document are drawn from a Dirichlet distribution and words in the document are repeatedly sampled from a topic, which itself is drawn from those topic proportions.</p><p>[46] introduced people-LDA, where topics were modelled around faces in images along with titles from news. The work of <ref type="bibr" target="#b45">[46]</ref> has some similarities to the method proposed here but the single biggest difference is that <ref type="bibr" target="#b45">[46]</ref> create topics around single people rather than around a group of people. The proposed group model creates topics around happiness intensitie for a group of people. For learning the topic model, a dictionary is learnt first.</p><p>Weighted Soft Assignment: K-means is applied to the image features for defining the visual words. For creating a histogram, each word of a document is assigned to one or more visual words. If the assignment is limited to one word, it is called hard assignment and if multiple words are considered, it is called soft assignment. The cons of hard assignment are that if a patch (face in a group G) in an image is similar to more than one visual word, the multiple association information is lost. Therefore, <ref type="bibr" target="#b48">[49]</ref> defined a weighted soft assignment to weight the significance of each visual word towards a patch. For a visual vocabulary of K visual words, a K-dimensional vector T = [t 1 ...t K ] with each component t k representing the weight of a visual word k in an group G is defined as</p><formula xml:id="formula_19">t k = N i Mi j 1 2 i-1 sim(j, k),<label>(15)</label></formula><p>where M i represents the number of face f j whose i th nearest neighbour is the visual word k. The measure sim(j, k) represents the similarity between face f j and the visual word k. It is worth noting that the contribution of each word is dependent to its similarity to a visual word weighted by the factor 1 2 i-1 . Relatively weighted soft-assignment: Along with the contribution of each visual word to a group G, it is interesting to add the global attributes as weights here. It is intuitive to note that the weights affect the frequency component of words, which here represent faces in a group. Therefore, it is similar to applying weights in GEM w and looking at the contribution based on a neighbourhood analysis of a particular subject under consideration. As the final goal is to understand the contribution of each face f i towards the happiness intensity of its group G, the relative weight formulated in Eq. 13 is used to define a 'relatively weighted' soft-assignment. Eq. 15 can then be modified as</p><formula xml:id="formula_20">t k = N i Mi j ψ j 2 i-1 sim(j, k) .<label>(16)</label></formula><p>Now, along with weights for each nearest visual word for a patch, another weight term is being introduced, which represents the contribution of the patch to the group. These data-driven visual words are appended with the manual attributes. Note that the histogram computed here is influenced by the global attributes of the faces in the group.</p><p>The default LDA formulation is an unsupervised Bayesian method. In their recent work, <ref type="bibr" target="#b49">[50]</ref> proposed the Supervised LDA (SLDA) by adding a response variable for each document. It was shown to perform better for regression and classification tasks. Using a supervised topic model is a natural choice for HAPPEI, as the human annotated labels for the happiness intensities at the image level are present. The document corpus is the set of groups G. The word here represents each face in G. The Max Entropy Discriminant LDA (MedLDA) <ref type="bibr" target="#b50">[51]</ref> is computed for topic model creation and test label inference. The LDA formulation for groups is referred to as GEM LDA . In the results section, the average model GEM is compared with the weighted average model GEM w and the feature augmented topic model GEM LDA .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.1">Face Processing Pipeline</head><p>Given an image, Viola-Jones (VJ) object detector <ref type="bibr" target="#b51">[52]</ref> models trained on frontal and profile faces are applied to the images. For extracting the fiducial points, the part-based point detector of <ref type="bibr" target="#b24">[25]</ref> is applied. The resulting nine points describe the location of the left and right corners of both eyes, the centre point of the nose, left and right corners of the nostrils, and the left and right corners of the mouth. For aligning the faces, an affine transform is applied.</p><p>As the images were collected from Flickr, containing different scenarios and complex backgrounds, classic face detectors, such as the VJ object detector, result in a fairly high false positive rate (13.6%). To minimise this error, a non-linear binary SVM <ref type="bibr" target="#b52">[53]</ref> is trained. The training set contains samples of faces and non-faces. For face samples, all true positives from the output of the VJ detector applied to 1300 images from the HAPPEI database are selected. For non-faces, the samples are manually selected from the same VJ output. To create a large number of false positives from real world data, an image set containing monuments, mountains and water scenes (but no persons facing the camera) is constructed. To learn the parameters for SVM, five-fold cross validation is performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.2">Implementation Details</head><p>Given a test image I containing group G, the faces in the group are first detected and aligned, then cropped to a size of 70 × 70 pixels. For the happiness intensity detection, PHOG features are extracted from the face. Here, the pyramid level L = 3, angle range = [0 -360] and bin count = 16. The number of latent variables is chosen as 18 after empirical validation. PHOG is scale invariant. The use of PHOG is motivated by <ref type="bibr" target="#b53">[54]</ref>, where PHOG performed well for facial expression analysis.</p><p>The parameters for MedLDA are α = 0.1, k = 25, for SVM f old = 5. 1500 documents are used for training and 500 for testing. The range of labels is the group mood intensity range [0-100] with a step size of 10. For learning the dictionary, the number of words k is empirically set to 60. In Eq. 4 and 12, the parameters are set to α = 0.3, β = 1.1 and γ = 0.1, which are weights that control the effect of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.3">Human Label Comparison</head><p>The Mean Average Error (MAE) is used as performance measure. The performance of the KPLS based occlusion intensity and happiness intensity estimators is compared with Support Vector Regression (SVR) <ref type="bibr" target="#b52">[53]</ref> based occlusion intensity and happiness intensity estimators. Figure <ref type="figure">8</ref> displays the comparison based on the MAE scores. The MAE for occlusion intensity is 0.79 for KPLS and 1.03 for SVR. The MAE for happiness intensity estimation for KPLS is 0.798 and for SVR 0.965.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.4">User Study</head><p>A total of 15 subjects participated in a two-part user survey and were asked to a) rate happiness intensities in 40 images and b) rate the output of the three methods for their output of the top 5 happiest images from an event. Here, the users were asked to provide a score in the range of 0 (not good at all) to 5 (very good) for the three methods for three social events each. The users did not know, which output belonged to which method. For part a), Figure <ref type="figure" target="#fig_7">7</ref> shows the output. Note that the happiness scores computed by the GEM w are close to the mean human score and are well within the range of the standard deviation of the human labellers' scores. The group of people in images in the top row have high happiness intensity. The groups in the lower row have a lower happiness intensity. From Figure <ref type="figure" target="#fig_7">7</ref>, it is evident that the upper and lower bounds of the happiness intensity range assigned by the participants to the top row images are generally higher than the intensities assigned to the bottom row images. The average standard deviation of the happiness intensities is 1.67. It is interesting to note that for some images, there was a high variation as compared to others. This can be attributed to the difference in perception of survey participants, as for different people different objects can be more salient.</p><p>For part b), ANOVA tests were performed with the hypothesis that adding social context to group mood analysis leads to an estimate closer to human perception. For GEM and GEM w , p &lt; 0.0006, which is statistically significant in the one-way ANOVA. For GEM and GEM LDA , p &lt; 0.0002, which is also statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.5">Image Ranking from an Event</head><p>For comparison of the proposed framework, volunteers were asked to rank a set of images containing a group of people from an event in the following task: Given a social event with the same or different people present in one or more photographs, the happiest moment of the event is to be found. Therefore, all the images are ranked on the basis of their decreasing amount of (perceived) happiness intensity. Figure <ref type="figure" target="#fig_8">9</ref> is a screenshot of an event ranking experiment. In the first row, the images are arranged based on their timestamp, i.e. when they were shot. The second row shows the ranking by human labellers. The highest happiness intensity image is on the left and decreases from left to right. In comparison, the output of GEM w is in row 3, where the proposed method ranked the images in order of their decreasing happiness intensity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.6">Candid Group Shot Selection</head><p>There are situations in social gatherings when multiple photographs are taken for the same subjects in a similar scene within a short span of time. Due to the dynamic nature of groups of people, it is a challenging task to find the most favourable expression together in a group of people. Here, the group mood analysis method is applied to shot selection after a number of pictures have been taken. In Figure <ref type="figure" target="#fig_9">10</ref>, the rows show the shots taken at short intervals. The GEM w ranks the images containing the same subjects and the best image (highest happiness quotient) is displayed in the fourth column.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13">CONCLUSIONS</head><p>Social events generate many group shots. In this paper, a framework for estimating the group mood from an image, focussing on positive mood, is proposed. To the best of our knowledge, this is the first work for analysing group mood based on the structure of a group and local attributes such as occlusion. An 'in the wild' database called HAPPEI is collected from Flickr based on keyword search. It is labelled at both image and face level. From the perspective of social context, the global structure of the group is explored. Relative weights are assigned to the happiness intensities of individual faces in a group, so as to estimate their contribution to the perceived group mood. The experiments show that assigning relative weights to intensities helps to better predict the group mood. The feature augmented topic model based group mood analysis model performs better than the average and weighted group expressions models.</p><p>In this work, for inferring the group mood, the global social features are based on the relative location of a person. The aim is to find salient or important faces, which can be the leader in the group. An interesting direction is to compute image saliency and weight the confidence of subjects who fall in the highly salient area. A natural extension of the proposed work is adding negative emotion group images to the database and framework <ref type="bibr" target="#b54">[55]</ref>. Further, human body pose can be merged with the face analysis of a group of people. Based on recent work by <ref type="bibr" target="#b55">[56]</ref>, body pose can convey affect information. In the images downloaded from the internet, there can be challenges such as face blur and occlusion due to neighbours in a group. This can make the inference of the mood of a person non-trivial. Body pose information can be fused with face information for robust inference. Attributes such as clothes colours and background scene details can also give important information about the social event and, hence, aid in inferring the mood of a group. The mood value of the group can be fused with other attributes such as the one mentioned in the Kansei image retrieval systems <ref type="bibr" target="#b56">[57]</ref>. In the future, social context factors, such as age and gender, will be explored. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Screenshot of a case in the user survey (Section 5).</figDesc><graphic coords="4,112.09,63.10,334.53,443.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3 Fig. 2 .</head><label>32</label><figDesc>Fig.2. Results of the analysis of some cases in the survey (Section 5).</figDesc><graphic coords="5,111.00,521.82,340.16,165.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) A collage of sample images in HAPPEI. (b) Sample face level happiness intensity labels in HAPPEI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The HAPPEI database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Top Left: Image with mood intensity score = 70. Top Right: Min-span tree depicting connection between faces. Bottom Left: Happiness intensity heat map generated using the GEM model (mood intensity score = 81). Bottom Right: Happiness intensity heat map with social context. The contribution of the faces with respect to their neighbours (F2 and F4) towards the overall intensity of the group is weighted (mood intensity score = 71.4). Adding the global context feature reduces the error.</figDesc><graphic coords="7,90.07,57.43,382.03,245.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Top Left: Image with happiness intensity score=70. Top Right: Min-span tree showing connection between faces. Bottom Left: Happiness intensity heat map. Bottom Right: Happiness intensity heat map with social context, the contribution of the occluded faces (F2 and F4) towards the overall intensity of the group is penalised.</figDesc><graphic coords="8,105.34,57.43,351.47,235.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Manually defined attributes.</figDesc><graphic coords="9,293.22,63.10,226.76,142.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The graph describes the comparison of the group mood intensity as calculated by the proposed method with the results from the user study. The top row shows images with high intensity score and the bottom row shows images which are close to neutral. Please note that the images are from different events.</figDesc><graphic coords="11,96.83,54.59,368.51,250.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. The top row contains images from a graduation ceremony organised by timestamps. The second row contains images ranked by human annotators in order of decreasing happiness intensity (from left to right). The third row contains images ranked by decreasing happiness intensity (from left to right) by GEM w .</figDesc><graphic coords="12,111.00,57.43,340.16,142.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Candid Group Shot Selection: Each row represents a series of photographs of the same people. The fourth column is the selected shot based on the highest score from GEM w (Eq. 14).</figDesc><graphic coords="13,103.92,63.10,354.33,168.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2015.2397456, IEEE Transactions on Affective Computing JOURNAL OF L A T E X CLASS FILES, VOL. 6, NO. 1, JANUARY 2007</figDesc><table /><note><p>1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>shows the MAE comparison of GEM , GEM w and GEM LDA . As</figDesc><table><row><cell>Method</cell><cell>GEM</cell><cell>GEMw</cell><cell>GEM LDA</cell></row><row><cell>MAE</cell><cell>0.455</cell><cell>0.434</cell><cell>0.379</cell></row></table><note><p><p>(a) Happiness Intensity (b) Occlusion Intensity Fig. 8. Comparison of happiness and occlusion intensity methods.</p>hypothesised, the effect of adding social features is evident in the lower MAE in GEM w and GEM LDA .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1</head><label>1</label><figDesc>Comparison of GEM , GEM w and GEM LDA .</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p><ref type="bibr" target="#b23">[24]</ref> report that their method works better than<ref type="bibr" target="#b24">[25]</ref>; however, due to its lower computational complexity<ref type="bibr" target="#b24">[25]</ref> is used here.1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>JOURNAL OF L A T E X CLASS FILES, VOL. 6, NO.1, JANUARY 2007  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">How tumblr and pinterest are fueling the image intelligence problem</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caroll</surname></persName>
		</author>
		<idno>2012. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vision-based analysis of small groups in pedestrian crowds</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ruback</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1003" to="1016" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mood meter: counting smiles in the wild</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Hoque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Drevo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 ACM Conference on Ubiquitous Computing</title>
		<meeting>the 2012 ACM Conference on Ubiquitous Computing</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="301" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Collecting large, richly annotated facial-expression databases from movies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2006">2012. 2, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding Images of Groups of People</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Confernece on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Confernece on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="256" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Seeing people in social context: Recognizing people and social relationships</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="169" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Face detection and tracking in video sequences using the modifiedcensus transformation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Küblbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ernst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vision Computing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="564" to="572" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Group emotion: A view from top and bottom</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Barsäde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Research on Managing in Groups and Teams</title>
		<editor>
			<persName><forename type="first">Deborah</forename><surname>Gruenfeld</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Margaret</forename><surname>Neale</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Elizabeth</forename><surname>Mannix</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mood and emotions in small groups and work teams</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Barsade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organizational behavior and human decision processes</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Urban tribes: Analyzing group photos from a social perspective</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition and Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition and Workshops</meeting>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="28" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hipster wars: Discovering elements of fashion styles</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="472" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Face discovery with social context</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Statistical context priming for object detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="763" to="770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">From appearance to contextbased recognition: Dense labeling n small images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Autotagging facebook: Social network context improves photo annotation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zickler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Two faces are better than one: Face recognition in group photographs</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Manyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Biometrics (IJCB)</title>
		<meeting>the International Joint Conference on Biometrics (IJCB)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Candid portrait selection from video</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transaction on Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">128</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Utilizing affective analysis for efficient movie browsing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1853" to="1856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Toward Practical Smile Detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Whitehill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Littlewort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">R</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Affect in social judgments and decisions: A multiprocess model</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Forgas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in experimental social psychology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="227" to="275" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">In the pursuit of effective affective computing: The relationship between features and registration</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Chew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1006" to="1016" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="681" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Face alignment through subspace constrained mean-shifts</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2003">September 2009. 3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2879" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hello! My name is... Buffy&quot; -Automatic Naming of Characters in TV Video</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine and Vision Conference (BMVC)</title>
		<meeting>the British Machine and Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A ssim-based approach for finding similar facial expressions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Automatic Faces and Gesture Recognition and Workshop FERA</title>
		<meeting>the IEEE International Conference on Automatic Faces and Gesture Recognition and Workshop FERA</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="815" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Finding Happiest Moments in a Social Context</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision (ACCV)</title>
		<meeting>the Asian Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="613" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Saliency in crowd</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="17" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition and Workshops (CVPRW)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition and Workshops (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Facial expressions and emotion database</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wallhoff</surname></persName>
		</author>
		<ptr target="http://www.mmk.ei.tum.de/∼waf/fgnet/feedtum.html.6" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-PIE</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Automatic Face and Gesture Recognition (FG)</title>
		<meeting>the IEEE International Conference on Automatic Face and Gesture Recognition (FG)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Labelme: A database and web-based tool for image annotation</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">University of Bonn, Department of Photogrammetry</title>
		<author>
			<persName><forename type="first">F</forename><surname>Korc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schneider</surname></persName>
		</author>
		<idno>TR-IGG-P-2007-01</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
	<note>Annotation tool</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Shortest connection networks and some generalizations</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Prim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell system technical journal</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1389" to="1401" />
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">We Are Family: Joint Pose Estimation of Multiple Persons</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="228" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Quality-Driven Face Occlusion Detection and Recovery</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<title level="m">Chemoinformatics and Advanced Machine Learning Perspectives: Complex Computational Methods and Collaborative Techniques. ACCM, IGI Global</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note>ch. Nonlinear Partial Least Squares: An Overview</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Simultaneous dimensionality reduction and human age estimation via kernel partial least squares regression</title>
		<author>
			<persName><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="657" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Human detection using partial least squares analysis</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="24" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A Robust and Scalable Approach to Face Identification</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="476" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Quantitative Sociology: International perspectives on mathematical and statistical model building</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Blalock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Path models with latent variables: The NIPALS approach</title>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1975">1975</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Representing Shape with a Spatial Pyramid Kernel</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Munoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM international conference on Image and video retrieval (CIVR)</title>
		<meeting>the ACM international conference on Image and video retrieval (CIVR)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="401" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Facial action unit detection using kernel partial least squares</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Ekenel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Confernece on Computer Vision and Workshops (ICCW)</title>
		<meeting>the IEEE International Confernece on Computer Vision and Workshops (ICCW)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2092" to="2099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Relative attributes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="503" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A bayesian hierarchical model for learning natural scene categories</title>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="524" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">People-lda: Anchoring topics to people using face recognition</title>
		<author>
			<persName><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Real-time indoor scene understanding using bayesian filtering with motion cues</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the Annual Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="601" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Towards optimal bag-offeatures for object categorization and semantic video retrieval</title>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM international conference on Image and Video Retrieval (CIVR)</title>
		<meeting>the ACM international conference on Image and Video Retrieval (CIVR)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="494" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Supervised Topic Models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the Annual Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Medlda: maximum margin supervised topic models for regression and classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page">158</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<idno>511. 10</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">LIBSVM: a library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/cjlin/libsvm" />
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Emotion recognition using PHOG and LPQ features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference Automatic Faces &amp; Gesture Recognition workshop FERA</title>
		<meeting>the IEEE Conference Automatic Faces &amp; Gesture Recognition workshop FERA</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="878" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The More the Merrier: Analysing the Affect of a Group of People In Images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Automatic Face and Gesture Recognition (FG)</title>
		<meeting>the IEEE International Conference on Automatic Face and Gesture Recognition (FG)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Affective body expression perception and recognition: a survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kleinsmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bianchi-Berthouze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="33" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Exploring kansei in multimedia information</title>
		<author>
			<persName><forename type="first">N</forename><surname>Berthouze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Berthouze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Kansei Engineering International</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
