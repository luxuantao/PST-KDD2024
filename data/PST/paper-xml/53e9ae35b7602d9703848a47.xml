<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BloomFlash: Bloom Filter on Flash-based Storage</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Biplob</forename><surname>Debnath</surname></persName>
							<email>biplob.debnath@emc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">EMC Corporation</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sudipta</forename><surname>Sengupta</surname></persName>
							<email>sudipta@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jin</forename><surname>Li</surname></persName>
							<email>jinl@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Lilja</surname></persName>
							<email>lilja@umn.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Minnesota</orgName>
								<address>
									<settlement>Minneapolis</settlement>
									<region>MN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">H C</forename><surname>Du</surname></persName>
							<email>du@cs.umn.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Minnesota</orgName>
								<address>
									<settlement>Minneapolis</settlement>
									<region>MN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BloomFlash: Bloom Filter on Flash-based Storage</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5D84E0E8A49301DAE61AB9DC5C9A27E8</idno>
					<idno type="DOI">10.1109/ICDCS.2011.44</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The bloom filter is a probabilistic data structure that provides a compact representation of a set of elements. To keep false positive probabilities low, the size of the bloom filter must be dimensioned a priori to be linear in the maximum number of keys inserted, with the linearity constant ranging typically from one to few bytes. A bloom filter is most commonly used as an inmemory data structure, hence its size is limited by the availability of RAM space on the machine. As datasets have grown over time to Internet scale, so have the RAM space requirements of bloom filters. If sufficient RAM space is not available, we advocate that flash memory may serve as a suitable medium for storing bloom filters, since it is about one-tenth the cost of RAM per GB while still providing access times orders of magnitude faster than hard disk.</p><p>We present BLOOMFLASH, a bloom filter designed for flash memory based storage, that provides a new dimension of tradeoff with bloom filter access times to reduce RAM space usage (and hence system cost). The simple design of a single flat bloom filter on flash suffers from many performance bottlenecks, including in-place bit updates that are inefficient on flash and multiple reads and random writes spread out across many flash pages for a single lookup or insert operation. To mitigate these performance bottlenecks, BLOOMFLASH leverages two key design innovations: (i) buffering bit updates in RAM and applying them in bulk to flash that helps to reduce random writes to flash, and (ii) a hierarchical bloom filter design consisting of component bloom filters, stored one per flash page, that helps to localize reads and writes on flash. We use two real-world data traces taken from representative bloom filter applications to drive and evaluate our design. BLOOMFLASH achieves bloom filter access times in the range of few tens of sec, thus allowing up to order of tens of thousands operations per sec.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The bloom filter is a bit-vector data structure that provides a compact representation of a set of elements (keys). It supports insertion of elements and membership queries. A membership answer is probabilistically correct in the sense that it allows a small probability of a false positive (i.e., an incorrect answer for a non-member element). The bloom filter allows tradeoffs between small size (compactness) and low false positives (accuracy). To keep false positives low, the size of the bloom filter must be dimensioned a priori to be linear in the maximum number of keys inserted, with the linearity constant typically ranging from one to few bytes. A bloom filter is most commonly used as an in-memory data structure, 1 Work done when the author was a PhD student at the University of Minnesota hence its size is limited by the availability of RAM space on the machine.</p><p>An element lookup in a bloom filter involves hashing it to some number, say , of different positions in the vector and check that these bits are all 1 -if so, it is concluded that element is in the set that the bloom filter represents, otherwise not. An element insertion involves setting the corresponding bit positions to 1. It can now be easily seen how a false positive can happen during a lookup -an element may not have been inserted in the bloom filter but may have had all its bit positions set to 1 during insertions of other elements. Further notation on bloom filters and discussion of false positive probabilities is provided in Section III-B.</p><p>An excellent survey of bloom filter applications is provided in <ref type="bibr" target="#b0">[1]</ref>. A common use of bloom filters is to identify nonexistent elements so as to avoid high query latencies involved with accesses down the storage hierarchy or across the network. Many data center applications, for example, Venti <ref type="bibr" target="#b1">[2]</ref>, a data deduplication system, and Google Big Table <ref type="bibr" target="#b2">[3]</ref>, use bloom filters to detect unique data, which helps to avoid slow secondary storage accesses. However, the size of the bloom filter in some applications is too big to fit in main memory. As datasets have grown over time to Internet scale, so have the RAM space requirements of bloom filters, even after partitioned processing of data across multiple servers. In current data center applications, it is not uncommon to allocate upwards of few GB of RAM for storing bloom filters.</p><p>When sufficient RAM space is not available, it may be necessary to store the bloom filter in magnetic disk-based storage. However, as bloom filter operations are randomly spread over its length, the slow random access (seek) performance of disks, of the order of 10 msec, becomes a huge bottleneck. The use of independent hash functions destroys any locality that may be present in the element space, hence, even using a RAM based cache does not help to improve bloom filter performance. Since flash memory has faster read performance, of the order of 10-100 sec in currently available Solid State Drives (SSDs), it is a viable storage option for implementing bloom filters and striking tradeoffs between high cost of RAM and fast bloom filter access times. Flash memory, however, provides relatively slow performance for random write operations (vs. reads and sequential writes), as explained in Section III. The aim of this paper is to design a flash-based bloom filter which is aware of flash memory characteristics.</p><p>There are two types of popular flash devices, NOR and NAND flash. NAND flash architecture allows a denser layout and greater storage capacity per chip. As a result, NAND flash memory has been significantly cheaper than DRAM, with cost decreasing at faster speeds. NAND flash characteristics have lead to an explosion in its usage in consumer electronic devices, such as MP3 players, phones, caches and Solid State Disks (SSDs). In the rest of the paper, we use NAND flash based SSDs as the architectural choice and simply refer to it as flash memory. We describe SSDs in detail in Section II.</p><p>In this paper, we present the design and evaluation of BLOOMFLASH, a flash memory based bloom filter, that provides a new dimension of tradeoff with bloom filter access times to reduce RAM space usage (and hence system cost). The simple design of a single flat bloom filter on flash suffers from many performance bottlenecks, including in-place bit updates that are inefficient on flash and multiple reads and random writes spread out across many flash pages for a single lookup or insert operation. To mitigate these performance bottlenecks, we exploit two key design decisions: (i) buffering bit updates in RAM and applying them in bulk to flash using two different flushing policies that helps to reduce random writes to flash, and (ii) a hierarchical bloom filter design consisting of component bloom filters, stored one per flash page, that helps to localize reads and writes on flash. We use two real-world data traces taken from representative bloom filter applications to drive and evaluate our design. Our experimental results show that BLOOMFLASH achieves bloom filter access times in the range of few tens of sec, thus allowing up to about 28,500 ops/sec.</p><p>The rest of the paper is organized as follows. We provide an overview of flash memory in Section II. In Section III, we develop the design of BLOOMFLASH. In Section IV, we describe two real-world data center applications and used them to evaluate and justify the design choices in BLOOMFLASH. We review related work in Section V. Finally, we conclude in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. FLASH-BASED STORAGE OVERVIEW</head><p>Figure <ref type="figure" target="#fig_0">1</ref> gives a block-diagram of an NAND flash based SSD. In flash memory, data are stored in an array of flash blocks. Each block spans 32-64 pages, where a page is the smallest unit of read and write operations. A distinguishing feature of flash memory is that read operations are very fast compared to magnetic disk drive. Moreover, unlike disks, random read operations are as fast as sequential read operations as there is no mechanical head movement. A major drawback of the flash memory is that it does not allow in-place updates (i.e., overwrite). Page write operations in a flash memory must be preceded by an erase operation and within a block, pages need be to written sequentially. The in-place update problem becomes complicated as write operations are performed in the page granularity, while erase operations are performed in the block granularity. The typical access latencies for read, write, and erase operations are 25 microseconds, 200 microseconds, and 1500 microseconds, respectively <ref type="bibr" target="#b3">[4]</ref>.</p><p>The Flash Translation layer (FTL) is an intermediate software layer inside SSD, which makes linear flash memory device act like a virtual disk. The FTL receives logical read and write commands from the applications and converts them to the internal flash memory commands. To emulate disk like in-place update operation for a logical page ( ), the FTL writes data into a new physical page ( ), maintains a mapping between logical pages and physical pages, and marks the previous physical location of as invalid for future garbage collection. Although FTL allows current disk based application to use SSD without any modifications, it needs to internally deal with flash physical constraint of erasing a block before overwriting a page in that block. Besides the in-place update problem, flash memory exhibits another limitation -a flash block can only be erased for limited number of times (e.g., 10K-100K) <ref type="bibr" target="#b3">[4]</ref>. FTL uses various wear leveling techniques to even out the erase counts of different blocks in the flash memory to increase its overall longevity <ref type="bibr" target="#b4">[5]</ref>. Recent studies show that current FTL schemes are very effective for the workloads with sequential access write patterns. However, for the workloads with random access patterns, these schemes show very poor performance <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. One of the design goals of BLOOMFLASH is to use flash memory in FTL friendly manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. BLOOM FILTER ON FLASH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Coping with Flash Constraints</head><p>Our design is driven by the need to work around small random write operations that are not efficient on flash media. Small random writes effectively need to update data portions within pages. Since a (physical) flash page cannot be updated in place, a new (physical) page will need to be allocated and the unmodified portion of the data on the page needs to be relocated to the new page.</p><p>To validate the performance gap between sequential and random writes on flash, we used Iometer <ref type="bibr" target="#b12">[13]</ref>, a widely used performance evaluation tool in the storage community, on a 160GB fusionIO SSD <ref type="bibr" target="#b13">[14]</ref> attached over PCIe bus to an Intel Core 2 Duo E6850 3GHz CPU. The number of worker threads was fixed at 8 and the number of outstanding I/Os for the drive at 64. The results for IOPS (I/O operations per sec) on 4KB I/O request sizes are summarized in Figure <ref type="figure" target="#fig_1">2</ref>. Each test was run for 1 hour. The IOPS performance of sequential writes is about 3x that of random writes and worsens when the tests are run for longer durations (due to accumulating device garbage collection overheads). We also observe that the IOPS performance of (random/sequential) reads is about 6x that sequential writes. (The slight gap between IOPS performance of sequential and random reads is possibly due to prefetching inside the device.)</p><p>Given the above, in order to use flash in the most efficient way we have focused on reducing random write operations as much as possible. However, this is very challenging as bloom filter operations are inherently random in nature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. BloomFlash Design</head><p>BLOOMFLASH stores the bloom filter bits on flash and uses RAM to buffer bit updates resulting from element insertions so as to mitigate flash writing and garbage collection overheads that could result from excessive in-place bit updates to flash. The bloom filter organization on flash is divided into pages corresponding to flash page sizes that are typically 2KB or 4KB. All read and write operations on flash are performed at the page level. We begin with a single flat bloom filter design and then refine it with the objective of localizing read and write operations, corresponding to element lookups and insertions, to within a single flash page. We present and investigate two different policies for flushing buffered page updates to RAM that work with either design.</p><p>We begin with some notation. Let be the total number of bits in the bloom filter. Let be the maximum number of elements (keys) inserted. The insertion of a key into the bloom filter involves selecting bit positions, using different hash functions, and setting these bits to 1. A lookup for a key in the bloom filter needs to compute the bit positions using the hash functions and checking whether all these bit positions are 1 -if so, the key is inferred to be in the bloom filter. Clearly, there is a chance of a false positive event here, since these positions might have their bits to 1 by other inserted keys, while the current key may not have been inserted at all. After insertion of keys, the probability, ( , , ) of a false positive during lookup can be shown to be</p><formula xml:id="formula_0">( , , ) = ( 1 - ( 1 - 1 ) ) ≈ ( 1 --/ )</formula><p>Given the maximum number of elements inserted and any one of two quantities and , we can minimize the false positive probability by choosing the other quantity as per the equation = ln 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Single Flat Bloom Filter on Flash</head><p>In our first design, the bloom filter is laid out in sequential logical address space on flash as shown in Figure <ref type="figure" target="#fig_2">3(a)</ref>. With a flash physical page size of bytes (typically equal to 2KB or 4KB), each page of flash holds 8 bits, so that the number of flash pages required is = ⌈ 8 ⌉. Henceforth, we will assume that is an integral multiple of 8 . During an element lookup, the different bit positions are translated to (logical) flash page ids and offsets. Each flash page is read to check whether the corresponding bit(s) within it is 0 or 1. A lookup can terminate with a negative answer when a bit position is read as 0 (in which case the remaining bit positions, if any, need not be read). An element insertion operation needs to set (at most) bit positions to 1 (if they are not already set to 1).</p><p>Because flash page writes are inefficient, these bit update operations are deferred using a buffer in RAM. The idea is collect multiple updates for the same page and apply them at once. Since RAM buffer space is limited, we need to periodically flush some of the buffered updates to flash so as to accommodate new updates. It appears that this flushing policy is critical for designing an efficient flash-based bloom filter. We discuss different buffer flushing policies separately after we describe two designs for bloom filter layout on flash.</p><p>Due to the random nature of the bit positions selected for each element, an element lookup will almost always involve different flash page reads. This makes the element lookup time about times that of a single flash page read. Also, an element write involves updates to about flash pages (could be less if some of the bits are already set) -thus each element write leads to bit updates on about different flash pages to be buffered in RAM. Both of these are shortcomings of this first simple design, which we address in the next design that is the choice for BLOOMFLASH.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Using Hierarchical Bloom Filter Design to Localize Reads and Writes</head><p>The first design suffers from the drawback that a bloom filter lookup (almost always) involves flash page reads, since the random nature of the bit positions will place them in different flash pages with high probability. Although flash media is good for random reads (vs. hard disk), we aim to improve performance further by targeting just a single flash page read for a lookup operation on the bloom filter.</p><p>Our solution approach is to partition the single flat bloom filter into many smaller component bloom filters, each of the size of one flash page (e.g., 4KB) as shown in Figure <ref type="figure" target="#fig_2">3(b)</ref>. We call the resulting data structure a hierarchical bloom filter. With a flash page size of bytes, this gives = ⌈ 8 ⌉ component bloom filters, each fitting on one flash page. To insert or lookup an element, a first hash function is used to identify the bloom filter it should belong into. Then, bit positions are identified within this bloom filter for setting or checking the bits. Thus, this design requires only one flash page read per element lookup. Moreover, this design also localizes bit position writes associated with an element insertion to within a single flash page -in contrast, in the first design, an element insertion involves setting bits in (almost always) flash pages. Hence, insertion of ′ elements can make at most ′ flash pages dirty in the worst case, while that number is ′ in the first design. This aspect of the second design helps to reduce flash page writes.</p><p>We next analyze the false positive probability of this composite bloom filter data structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of False Positive Probability</head><p>In BLOOMFLASH, the overall bloom filter data structure is realized by multiple component bloom filters and each element is assigned to one component bloom filter (using a hash function). We analyze the false positive probability of this composite bloom filter data structure. Suppose that keys are inserted into the hierarchical bloom filter and this leads to element insertions in the -th component bloom filter for = 1, 2, . . . , . Let the size of each small bloom filter be ′ = 8 bits. As before, is the number of bit positions that is checked in each small bloom filter for an element lookup. The probability of an element hashing to the -th component bloom filter is 1/ and the false positive probability of that element in that component bloom filter is ( , ′ , ). Thus, the false positive probability of the hierarchical bloom filter is given by</p><formula xml:id="formula_2">1 ∑ =1 ( , ′ , )</formula><p>Using Jensen's equality for convex functions, it can be shown that the false positive probability of the hierarchical bloom filter is at least that of that of the (equivalent) single flat bloom filter ( , , ) (in the first design), that is</p><formula xml:id="formula_3">1 ∑ =1 ( , ′ , ) ≥ ( , , )</formula><p>with equality if and only if all the 's are equal, i.e., the hashing of elements to component bloom filters achieves exactly equal distribution of elements across the component bloom filters. In fact, as the distribution of elements across component bloom filters gets more skewed (or, unbalanced), the gap widens between the false positive probabilities of the hierarchical bloom filter and single flat bloom filter designs.</p><p>A simple way to understand this effect is as follows: as the number of elements inserted into a component bloom filter increases beyond the average of / , its false positive rate increases non-linearly above ( , , ); hence, the false positive probabilities of above-average occupancy component bloom filters is not averaged out by that of other below-average occupancy small bloom filters.</p><p>Thus, it might be necessary to enforce fairly equal load balancing of elements across component bloom filters in this design in order to keep the false positive probability comparable to that of the single flat bloom filter design. One simple way to achieve this is to use the power of two choice idea from <ref type="bibr" target="#b14">[15]</ref> that has been used to balance a distribution of balls thrown into bins. With a load balanced design for hierarchical bloom filter, each element would be hashed to two candidate component bloom filters and actually inserted into the one that has currently fewer inserted elements. During lookup, each element is looked up in both of its candidate component bloom filters (hence bloom filter lookup times would double).</p><p>With the help of our experimental setup (as described in Section IV), we have found that the single-hash based assignment of an element to a component bloom filter achieves fairly equal load balancing of elements across component bloom filters. Hence, a more intricate design for load balancing is not required. We observe in our evaluations (on the data traces used) that the ratio of elements inserted in the maximum occupancy bloom filter to that in the minimum occupancy bloom filter is about 1.1, hence the distribution of each bin is within 10% from the average. Because of this, the false positive probability of the composite bloom filter is about the same as that of the (equivalent) single flat bloom filter. An intuitive explanation for why this fairly equal load balancing is achieved is that the number of elements inserted is orders of magnitude more than the number of component bloom filters -for example, with a flash page size of = 4096 bytes and / = 8, the number of component bloom filters is /4096. Hence, by deriving analogy from the well studied balls-and-bins problem, we expect the random assignment of balls to bins to achieve a fairly well balanced distribution. (As a side note, using the idea of two choices for inserting an element into a component bloom filter brings down the ratio of elements inserted in the maximum occupancy bloom filter to that in the minimum occupancy bloom filter to about 1.001.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Buffering Updates in RAM to Reduce Flash Writes</head><p>We can buffer updates in RAM for each of the bloom filter designs above so as to reduce the number of flash writes. Since in-place updates to (logical) flash pages actually lead to new (physical) pages being allocated and written and increases garbage collection activity, this strategy can help to reduce write amplification and garbage collection overheads. Additionally, the latency of bloom filter reads could also decrease somewhat since a bit position (write) that is buffered in RAM can be read with reading the containing page on flash. Insertion Group the buffered updates by the page id.</p><p>To realize this strategy, we group and store in RAM bit position updates (changes from 0 to 1 as a result of element insertion) for the same flash page -each update records a bit position offset within the page and all updates within a page share a common page id information. These updates are flushed to flash when the turn arrives to flush the corresponding page. When multiple contiguous flash pages in logical address page are flushed together (vs. each page separately when their turn arrives), the write performance improves due to the large (sequential) nature of the writes. Hence, we apply the scheduling of pages flushed to flash in the granularity of contiguous (logical) page groups rather than individual pages. We evaluate the performance of different page group sizes in Section IV and fix a page group size of 16 in our system implementation.</p><p>We investigate two page group flushing policies as follows:</p><p>• Dirtiest group first: Under this policy, the page group that contains the largest number of bit position updates (i.e., contains the maximum dirty bits) is given priority for flushing to flash. This greedy policy minimizes the total number of write operations. Since write operations more expensive than read operations on flash media, this is the conventional buffering policy. However, this policy ignores the performance degradation that can result from writing page groups in arbitrary order (vs. their ordering in the logical address space) since this results in random write operations to flash. • Groups in sequential order: Under this policy, page groups are flushed out in sequential order in logical address space (with wraparound when the last page group is reached). This treats the bloom filter storage area on flash like an append log (with wraparound) and aims to reap the write performance benefits of using flash in a log-structured manner (the latter has been reported in earlier work <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>). Flushing of a page group is triggered when a bit position update, resulting from an element insertion process, needs to be buffered in RAM and a pre-configured space threshold in RAM for storing buffered updates is exceeded. Only one page group is flushed at a time and is chosen in accordance with either of the above flushing policies. We investigate the impact of varying buffer space thresholds on bloom filter performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EVALUATION</head><p>In this section, we evaluate the design of BLOOMFLASH in Fusion-IO <ref type="bibr" target="#b13">[14]</ref> and Samsung <ref type="bibr" target="#b18">[19]</ref> Solid State Drives (SSDs) with the help of two real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Applications Used</head><p>We describe two real-world data center applications that use bloom filters to reduce data lookup latencies in the system. Data traces obtained from real-world instances of these applications is used to drive and evaluate the design of BLOOMFLASH. The properties of the two traces in terms of associated bloom filter operations are summarized in Table <ref type="table">I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Online Multi-player Gaming</head><p>Online multi-player gaming technology allows people from geographically diverse regions around the globe to participate in the same game. The number of concurrent players in such a game could range from tens to hundreds of thousands and the number of concurrent game instances offered by a single online service could range from tens to hundreds. An important challenge in online multi-player gaming is the requirement to scale the number of users per game and the number of simultaneous game instances. At the core of this is the need to maintain server-side state so as to track player actions on each client machine and update global game states to make them visible to other players as quickly as possible. These functionalities map to set and get key operations performed by clients on server-side state. The real-time responsiveness of the game is, thus, critically dependent on the response time and throughput of these operations.</p><p>In such online multi-player gaming applications, front-end caching servers are used to scale a back-end of partitioned database servers implementing a key-value store. Specifically, we consider the Microsoft Xbox LIVE Primetime online multiplayer game <ref type="bibr" target="#b19">[20]</ref>. This application frequently uses lookups on non-existing keys to implement the game logic. Key writes arriving at a front-end server are broadcast to all other frontends so that they can insert that key into their local bloom filters. For a key read operation, if the key is not in the local bloom filter of the front-end, that frontend can return null to the client; otherwise, the read is sent to the appropriate back-end database server.</p><p>We evaluate the performance of BLOOMFLASH on a large trace of get-set key operations obtained from real-world instances of Xbox LIVE Primetime <ref type="bibr" target="#b19">[20]</ref> in which the key is a dot-separated sequence of strings with total length averaging  94 bytes. The ratio of bloom filter lookup to insert operations in the trace is about 165:1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Storage Deduplication</head><p>Deduplication is a recent trend in storage backup systems that eliminates redundancy of data across full and incremental backup data sets <ref type="bibr" target="#b20">[21]</ref>. It works by splitting files into multiple chunks using a content-aware chunking algorithm like Rabin fingerprinting and using SHA-1 hash signatures for each chunk to determine whether two chunks contain identical data <ref type="bibr" target="#b20">[21]</ref>.</p><p>In inline storage deduplication systems, the chunks (or their hashes) arrive one-at-a-time at the deduplication server from client systems. The server needs to lookup each chunk hash in an index it maintains for all chunk hashes seen so far for that backup location instance -if there is a match, the incoming chunk contains redundant data and can be deduplicated; if not, the (new) chunk hash needs to be inserted into the index.</p><p>Because storage systems currently need to scale to tens of terabytes to petabytes of data volume, the chunk hash index is too big to fit in RAM, hence it is stored on hard disk. Index operations are thus throughput limited by expensive disk seek operations. Since backups need to be completed over windows of half-a-day or so (e.g., nights and weekends), it is desirable to obtain high throughput in inline storage deduplication systems. Typical in such systems (e.g., <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b20">[21]</ref>) is the use of a bloom filter to determine if a chunk hash has been seen earlier so that disk access latencies can be avoided for chunks that are seen for the first time (which is the majority of chunks in the first full backup of a dataset).</p><p>To obtain a chunk hash trace from real-world datasets, we have built a storage deduplication analysis tool that can crawl a root directory, generate the sequence of chunk hashes for a given average chunk hash size, and compute the number of deduplicated chunks and storage bytes. We use traces generated from this tool on an enterprise backup dataset to evaluate bloom filter performance. In this application, the key is a 20-byte SHA-1 hash of the corresponding chunk. The trace contains 27,748,824 total chunks and 12,082,492 unique chunks. Using this, we obtain the ratio of bloom filter lookup to insert operations in the trace as 1.3:1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. C Implementation</head><p>We have prototyped BLOOMFLASH in approximately 3000 lines of C code. To implement the hash functions for bloom filter access, we use MurmurHash <ref type="bibr" target="#b22">[23]</ref>. We store the bloom filter data in the file system and turn off operating system buffering of file blocks so as to avoid any favorable effects due to it. The value of is chosen to be 6. Since the number of keys in each trace is known (i.e., the value of ), we use equation( <ref type="formula" target="#formula_1">1</ref>) to determine the size of the bloom filter (single or hierarchical). In the rest of this section, the value ℎ = implies that * ℎ number of update entries are buffered in the RAM cache. We set the value of ℎ to 4KB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Platforms</head><p>We use two different Solid State Drives (SSDs) to evaluate the performance of BLOOMFLASH. The first one is a Fusion-IO flash SSD <ref type="bibr" target="#b13">[14]</ref> of capacity 80GB. The second one is a Samsung flash SSD <ref type="bibr" target="#b18">[19]</ref> of capacity 100GB. The CPU is an Intel Core 2 Duo E6850 with a clock speed of 3GHz per core. We log the average number of ops/sec at a period of every 10,000 get-set operations during each run and then take the overall average over a run to obtain throughput numbers. To calculate latency, we divide total execution time by the total number of get-set operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Result Analysis</head><p>In this section, we first evaluate the effect of buffering in the single flat BF design. Next, we evaluate the improvement of the hierarchical BF design. During these set of experiments, we have used ℎ = 4 and = 16, since this combination provides better throughput. Finally, we evaluate the impact of group size and cache size on the hierarchical BF design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single Flat BF Design</head><p>Buffering helps to improve the performance of this design by 34%-42%. Figure <ref type="figure" target="#fig_3">4</ref> shows the performance trends of single flat BF design for Xbox and dedupe traces running in Fusion-IO drive.</p><p>Xbox Trace. Buffering helps to improve throughput by 39%-42%. Without any buffering, we achieve only 3,520 ops/sec. However, with buffering, we achieve higher throughput. For the dirtiest-group flushing policy, we achieve 4,890 ops/sec, while for the sequential-group-ordering, we achieve 5,005 ops/sec. The average key lookup time is 282.07 sec, 202.78 sec, and 198.29 sec for no-buffering, the dirtiest-group flushing, and the sequential-group-ordering flushing policies, respectively. As explained in Section III-E, buffering helps to reduce expensive flash write operations by applying the changes due to update operations in bulk to the underlying flash device, which consequently helps to improve throughput. In particular, buffering helps to reduce the total number of flash write operations approximately by 77% for the dirtiestgroup policy and 75% for the sequential-group-ordering policy.</p><p>Dedupe Trace. Buffering improves throughput approximately by 34%. It helps to improve throughput by reducing the total number of expensive flash write operations. It reduces write operations approximately by 87% for both the flushing policies. The dirtiest-group flushing policy achieves 3,044 ops/sec and the sequential-group-ordering achieves 3,056 ops/sec, while without any buffering, we achieve only 2,275 ops/sec. The average key lookup time is 422.63 sec, 326.55 sec, and 325.15 sec for no-buffering, the dirtiest-group flushing, and the sequential-group-ordering flushing policies, respectively.</p><p>From Figure <ref type="figure" target="#fig_3">4</ref>, it is clear that the throughput is higher for Xbox compared to dedupe case. The main reason behind this trend is that Xbox workload is read-intensive, while dedupe workload is write-intensive. Since current flash-based SSDs provide comparatively faster read performance, certainly we achieve higher throughput for read-intensive Xbox trace. On the other hand, the average key lookup time is higher for dedupe case due to frequent garbage collection operations inside SSDs incurred by excessive write operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hierarchical BF Design</head><p>Hierarchical design improves the performance compared to the Single BF design since it localizes read and write operations to a single place for each lookup and insert operation. The improvement gain is linearly related with the value chosen for (i.e., number of hash functions).</p><p>Figure <ref type="figure" target="#fig_5">5</ref> shows the performance trends of hierarchical design for Xbox and dedupe traces running in Fusion-IO and Samsung drives. The trends in Figure <ref type="figure" target="#fig_5">5</ref>(a) show that the hierarchical BF design improves throughput 5.11x-7.00x and latency 5.60x-7.38x, compared to the Single Flat BF design for the Fusion-IO drive. The main reason for this improvement is the following. The single flat BF design uses = 6 different hash functions for the bloom filter operations and these hash functions mostly perform random I/Os in the six different flash locations. In contrast, the hierarchical BF design uses one I/O per bloom filter operation, irrespective of the value of . In addition, this design inherently localizes the updates in the buffer, which consequently helps to further improve throughput by reducing random write operations. This is also evident from the result trend that on average, hierarchical BF design provides approximately six times more throughput. Table <ref type="table">II</ref> and III give summaries of the improvement in hierarchical BF design for the Fusion-IO drive. We also observe similar trends for the Samsung drive.</p><p>Xbox Trace. Compared to single flat BF design, throughput improves by 5.33x-7.00x and latency improves by 5.60x-7.38x for the Fusion-IO drive. In particular, Figure <ref type="figure" target="#fig_5">5(a)</ref> shows that with hierarchical BF design for Xbox, dirtiestgroup flushing policy achieves 26,736 ops/sec and sequentialgroup-ordering achieves 26,663 ops/sec, while without any buffering, we achieve only 24,653 ops/sec. Thus, both dirtiestgroup and sequential-group-ordering flushing policies improve throughput by 8%, compared to having no buffer. Unlike, the single flat BF case, here buffering does not significantly help to improve throughput. Buffering helps to reduce total number of flash write operations by 3%. This occurs due to impact of hierarhical BF design as it localizes the update operations from six different flash locations to single flash location, consequently reducing the total number of flash I/O operations. Compared to single flat BF in Figure <ref type="figure" target="#fig_3">4</ref>, dirtiestgroup flushing, sequential-group-ordering flushing, and no buffering policies achieve 5.45x, 5.33x, and 7.00x higher throughput, respectively. The average key look up time is 35.35 sec, 35.36 sec, and 38.20 sec, respectively.    in this case buffering does not help much to improve the performance. The main reason is that during flushing, we need to perform read operations in order to fetch old page from flash, need to merge buffered update with it, and finally need to write the updated page back to flash. Clearly, flushing has some overhead in terms of flash read opeations. For Samsung drive, this overhead compensated the gain from buffering due to writing updates in bulk to the underlying flash storage. Dedupe Trace. Compared to the single flat BF design, throughput improves by 5.11x-5.41x and latency improves by 5.73x-7.17x. Figure <ref type="figure" target="#fig_5">5</ref>(a) shows that with hierarchical BF design, the dirtiest-group flushing policy achieves 15,640 ops/sec and the sequential-group-ordering achieves 15,630 ops/sec, while without any buffering, we achieve only 12,318 ops/sec by using Fusion-IO drive. Thus, both dirtiest-group and sequential-group-ordering flushing policies improve throughput by 27% compared to having no buffer. Compared to the single flat BF in Figure <ref type="figure" target="#fig_3">4</ref>, dirtiest-group flushing, sequentialgroup-ordering flushing, and no buffering policies achieve 5.13x, 5.11x, and 5.41x higher throughput, respectively. The average key lookup time is 56.73 sec, 56.75 sec, and 58.95 sec, respectively. Figure <ref type="figure" target="#fig_5">5</ref>(a) shows that with Samsung drive, the dirtiestgroup flushing policy achieves 5,276 ops/sec and the sequential-group-ordering achieves 4,826 ops/sec, while without any buffering, we achieve only 4,042 ops/sec. Thus, dirtiest-group and sequential-group-ordering flushing policies improve throughput by 31% and 19%, respectively, compared to having no buffer.</p><p>Overall, for hierarchical BF design, Xbox achieves higher throughput compared to the dedupe case. This is due to the read intensive nature of Xbox trace and relatively faster read performance of the current generations of flash-based SSDs.</p><p>Since hierarchical BF design provides significant improvement in throughput and latency compared to the single flat BF design, in the rest of this section we focus only on the hierarchical design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group Size Effect on Flushing Policies</head><p>Now, we study the impact of flushing polices in the hierarchical BF design by varying the group size. Figure <ref type="figure">6</ref> shows the performance trends due to various flushing policies. When group size = 1, every time we flush updates for only one page. Whereas, for group size = 16, every time we flush the updates for 16 consecutive pages. During this set of experiments, we use ℎ = 4. For dedupe with Fusion-IO, with = 1, the dirtiest-group and sequential-group-ordering flushing policies achieve 9% and 3% higher throughput, respectively, compared to no buffering. While with = 16, both dirtiest-group and sequential-group-ordering flushing policies achieve 27% higher throughput compared no buffering case. By increasing group size, we can utilize the better sequential write performance of the flash memory, consequently throughput increases. In Samsung drive, compared to no buffering case, for = 1, the dirtiest-group flushing policy achieves 16% higher throughput. While, for = 16, both dirtiest-group and sequential-group-ordering flushing policies achieve 31% and 19% higher throughput, respectively, compared no buffering case. Clearly, for both SSDs, larger group size helps to achieve higher throughput.</p><p>For dedupe with Fusion-IO, compared to no buffering case, for = 1, both dirtiest-group and sequentialgroup-ordering flushing policies achieve 1% higher throughput. While for = 16, both dirtiest-group and sequential-group-ordering flushing policies achieve 8% higher throughput compared no buffering case. In Samsung drive, group size does not have much impact on throughput for the dedupe trace.</p><p>Overall, the dirtiest-group flushing policy with = 16 provides higher throughput.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cache Size Effect</head><p>Figure <ref type="figure" target="#fig_6">7</ref> shows the effect of increasing cache size on the Xbox trace performance running in Fusion-IO drive. In this set of experiments, we set = 16. As expected with the increase of cache size, throughput also increases. This trend occurs as larger cache helps to reduce the total number of expensive write operations. However, the improvement rate is slow. This is due to the read intensive nature of Xbox trace. Since update operations are not frequent, increasing cache size does not help much. For dedupe trace, increasing cache size helps to improve performance due to its write-intensive nature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RELATED WORK</head><p>Our hierarchical bloom filter design is inspired by the blocked bloom filter design in <ref type="bibr" target="#b23">[24]</ref>, which uses a set of CPU cache line sized (about 64 bytes) smaller bloom filters. The latter design goes back further in the literature and is adapted from <ref type="bibr" target="#b24">[25]</ref>. In our case, each smaller bloom filter is sized to fit into a flash page. In the blocked bloom filter case, lots of bloom filters are needed due to relative smaller size of cachelines. As a result, the probability of some smaller bloom filters getting overloaded is very high. In contrast, due to relatively larger size of flash pages, we need to use fewer number of bloom filters and the probability of overloading is quite low. Multiple bloom filters have been used to dynamically "grow" a bloom filter when the number of keys to be inserted is not known a priori. The method in <ref type="bibr" target="#b25">[26]</ref> starts with a single standard bloom filter. When insertion of an additional element will increase the false positive probability beyond a threshold, a new bloom filter is instantiated and insertions happen in that one. At any given time, only one bloom filter is active for insertions. A lookup on an element proceeds by searching in all bloom filters. In contrast, all component bloom filters in BLOOMFLASH are always active and a hash function is used to assign an element to a component bloom filter. Also, the element lookup process in BLOOMFLASH involves searching in only one component bloom filter.</p><p>Recently, several studies including BufferHash <ref type="bibr" target="#b26">[27]</ref>, FAWN <ref type="bibr" target="#b27">[28]</ref>, ChunkStash <ref type="bibr" target="#b10">[11]</ref>, FlashStore <ref type="bibr" target="#b9">[10]</ref>, SkimyStash <ref type="bibr" target="#b11">[12]</ref>, and MicroHash <ref type="bibr" target="#b17">[18]</ref> use flash memory to design key-value store using hash table based data structure. Although bloom filter also uses hash functions to insert and lookup keys, but these systems cannot be directly used to design an efficient bloom filter as it is based on different design principles compared to the hash table. Bloom filter gives probabilistic answer, while hash table gives exact answer for a lookup query.</p><p>Our work in this paper first appeared as a technical report in <ref type="bibr" target="#b28">[29]</ref>. Since then, there has been further interest in the literature on designing bloom filters for flash memory. The authors in <ref type="bibr" target="#b29">[30]</ref> also propose a hierarchical design for bloom filters on flash. However, they optimize the bloom filter for batch processing scenarios. Their design operates in two phases: build phase and probe phase. In the build phase, a bloom filter is created for a set of keys, but no lookups are performed. In the probe phase, they lookup the bloom filter for keys, but no insertions are performed. This is a common scenario in database query processing. In contrast, our design is more generic and can handle simultaneous key insert and lookup operations.</p><p>In addition, their design <ref type="bibr" target="#b29">[30]</ref> uses a dedicated buffer in RAM for each small bloom filter. During the build phase, when a buffer is full, it is flushed to flash, hence the buffer flushing policy is random, which increases random write operations to flash memory. The key lookups in the probe phase are also batched, unlike our design. During probe phase, keys to be looked up are first buffered in the dedicated buffer in RAM. After this is full, all keys are looked up simultaneously (in batch) in the respective small bloom filters. In contrast, our design uses a flash-friendly flushing policy which helps to reduce random write operations to flash. Moreover, our design supports an online model for key lookups and insertions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We designed and evaluated BLOOMFLASH, a bloom filter data structure on flash-based storage, that can tradeoff access times for a very slim RAM footprint. For server applications that need to use bloom filters for Internet scale datasets, this frees up constrained RAM space for other computation and lowers system cost. BLOOMFLASH was carefully designed to be flash aware and work with the constraints of flash storage media. BLOOMFLASH exploits two key design decisions: (i) buffering bit updates in RAM and applying them in bulk to flash (using two different flush-to-flash policies) that helps to reduce random writes to flash, and (ii) a hierarchical bloom filter design consisting of component bloom filters, stored one per flash page, that helps to localize reads and writes on flash. Evaluations on real-world data traces taken from representative bloom filter applications show that BLOOMFLASH achieves bloom filter access times in the range of few tens of sec on currently available flash SSDs, thus allowing up to approximately 28,500 operations per sec.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Flash-based Solid State Disk (SSD)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. IOPS for sequential/random reads and writes using 4KB I/O request size on a 160GB fusionIO drive.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Bloom filter designs on flash: Single flat bloom filter vs. hierarchical bloom filter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Single Flat BF Design on Fusion-IO ( ℎ = 4 and = 16)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Fig. 5. Hierarchical BF Design ( ℎ = 4 and = 16)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 (</head><label>5</label><figDesc>Figure 5(b) shows that with Samsung drive, for Xbox, the dirtiest-group flushing policy achieves 12,757 ops/sec and sequential-group-ordering achieves 12,973 ops/sec, while without any buffering, we achieve only 12,829 ops/sec. Thus,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Effect of cache size for Xbox in the hierarchical BF design (for Fusion-IO with = 16)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Thus,</figDesc><table><row><cell></cell><cell cols="2">Single-Flat-BF</cell><cell cols="2">Hierarchical-BF</cell><cell cols="2">Improvement</cell></row><row><cell></cell><cell>Xbox</cell><cell>Dedupe</cell><cell>Xbox</cell><cell>Dedupe</cell><cell cols="2">Xbox Dedupe</cell></row><row><cell></cell><cell cols="4">(ops/sec) (ops/sec) (ops/sec) (ops/sec)</cell><cell></cell><cell></cell></row><row><cell>No-Buffer</cell><cell>3,520</cell><cell>2,275</cell><cell>24,653</cell><cell>12,318</cell><cell>7.00x</cell><cell>5.41x</cell></row><row><cell>Dirtiest-Group</cell><cell>4,890</cell><cell>3,044</cell><cell>26,736</cell><cell>15,640</cell><cell>5.47x</cell><cell>5.13x</cell></row><row><cell>Sequential-Group-Order</cell><cell>5,005</cell><cell>3,056</cell><cell>26,663</cell><cell>15,630</cell><cell>5.33x</cell><cell>5.11x</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Network Applications of Bloom Filters: A Survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Broder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitzenmacher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Venti: A New Approach to Archival Data Storage</title>
		<author>
			<persName><forename type="first">S</forename><surname>Quinlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dorward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FAST</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bigtable: A Distributed Storage System for Structured Data</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burrows</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fikes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Gruber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Design Tradeoffs for SSD Performance</title>
		<author>
			<persName><forename type="first">N</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wobber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Manasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Panigrahy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Algorithms and Data Structures for Flash Memories</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Toledo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">DFTL: A Flash Translation Layer Employing Demand-Based Selective Caching of Page-Level Address Mappings</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Urgaonkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>ASPLOS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BPLRU: A Buffer Management Scheme for Improving Random Writes in Flash Storage</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FAST</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Flashing Up the Storage Layer</title>
		<author>
			<persName><forename type="first">I</forename><surname>Koltsidas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Viglas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Online Maintenance of Very Large Random Samples on Flash Storage</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gibbons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">FlashStore: High Throughput Persistent Key-Value Store</title>
		<author>
			<persName><forename type="first">B</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ChunkStash: Speeding Up Inline Deduplication Using Flash Memory</title>
	</analytic>
	<monogr>
		<title level="m">USENIX ATC</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SkimpyStash: A RAM Space Skimpy Key-Value Store on Flashbased Storage</title>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<ptr target="http://www.iometer.org/" />
		<title level="m">Iometer</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<ptr target="http://www.fusionio.com/PDFs/DataSheetioDrive2.pdf" />
		<title level="m">Fusion-IO Drive Datasheet</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Balanced Allocations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Broder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Upfal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM Journal on Computing</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">FlashLogging: Exploiting Flash Devices for Synchronous Logging Performance</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Flash-Memory Based File System</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nishioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Motoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Microhash: An Efficient Index Structure for Flash-based Sensor Devices</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zeinalipour-Yazti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kalogeraki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gunopulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Najjar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>in FAST</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ssd</forename><surname>Samsung</surname></persName>
		</author>
		<ptr target="http://www.samsung.com/global/business/semiconductor/productInfo.do?fmlyid=161&amp;partnum=MCCOE64G5MPP" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Xbox LIVE 1 vs 100 game</title>
		<ptr target="http://www.xbox.com/en-US/games/1/1v100/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Avoiding the Disk Bottleneck in the Data Domain Deduplication File System</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FAST</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sparse Indexing: Large Scale, Inline Deduplication Using Sampling and Locality</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lillibridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Eshghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bhagwat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Deolalikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Trezise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Camble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FAST</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<ptr target="http://en.wikipedia.org/wiki/MurmurHash" />
		<title level="m">MurmurHash Fuction</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cache-, Hash-, and Space-Efficient Bloom Filters</title>
		<author>
			<persName><forename type="first">F</forename><surname>Putze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Singler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Journal of Experimental Algorithmics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An algorithm for approximate membership checking with application to password security</title>
		<author>
			<persName><forename type="first">U</forename><surname>Manber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Letters</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The Dynamic Bloom Filters</title>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2010-01">January 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cheap and Large CAMs for High Performance Data-Intensive Networked Systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kappes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NSDI</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">FAWN: A Fast Array of Wimpy Nodes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">BloomFlash: Bloom Filter on Flash-based Storage</title>
		<author>
			<persName><forename type="first">B</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lilja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<idno>MSR-TR-2010-161</idno>
	</analytic>
	<monogr>
		<title level="j">Microsoft Research Technical Report</title>
		<imprint>
			<date type="published" when="2010-03">March 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Buffered Bloom Filters on Solid State Storage</title>
		<author>
			<persName><forename type="first">M</forename><surname>Canim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mihaila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First International Workshop on Accelerating Data Management Systems Using Modern Processor and Storage Architectures (ADMS)</title>
		<imprint>
			<date type="published" when="2010-09">September 2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
