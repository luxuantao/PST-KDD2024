<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-Modal Scene Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lluis</forename><surname>Castrejon</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
						</author>
						<title level="a" type="main">Cross-Modal Scene Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">67A22A7EBDA1877AC0F4A502E496AB78</idno>
					<idno type="DOI">10.1109/TPAMI.2017.2753232</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2017.2753232, IEEE Transactions on Pattern Analysis and Machine Intelligence 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>cross-modal perception</term>
					<term>domain adaptation</term>
					<term>scene understanding. * denotes equal contribution we</term>
					<term>water</term>
					<term>fishes</term>
					<term>you</term>
					<term>drink</term>
					<term>formed</term>
					<term>greek</term>
					<term>would</term>
					<term>ball</term>
					<term>have play</term>
					<term>children</term>
					<term>there</term>
					<term>equipment</term>
					<term>are</term>
					<term>for</term>
					<term>train</term>
					<term>hole</term>
					<term>games</term>
					<term>path ropes</term>
					<term>recess</term>
					<term>seats</term>
					<term>dug</term>
					<term>that</term>
					<term>square</term>
					<term>down</term>
					<term>each</term>
					<term>fight</term>
					<term>it bed</term>
					<term>nightstand</term>
					<term>window</term>
					<term>gas</term>
					<term>shampoo</term>
					<term>you</term>
					<term>tallest</term>
					<term>rock</term>
					<term>i</term>
					<term>my church</term>
					<term>priest</term>
					<term>sermon</term>
					<term>religious</term>
					<term>he</term>
					<term>impressive</term>
					<term>large</term>
					<term>stared</term>
					<term>fountain</term>
					<term>gas ice</term>
					<term>terrain</term>
					<term>plane</term>
					<term>cold</term>
					<term>i</term>
					<term>nightstand</term>
					<term>inside</term>
					<term>beds</term>
					<term>two</term>
					<term>movement Real Clip art Sketches Spatial text Descriptions</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>People can recognize scenes across many different modalities beyond natural images. In this paper, we investigate how to learn cross-modal scene representations that transfer across modalities. To study this problem, we introduce a new cross-modal scene dataset. While convolutional neural networks can categorize scenes well, they also learn an intermediate representation not aligned across modalities, which is undesirable for cross-modal transfer applications. We present methods to regularize cross-modal convolutional neural networks so that they have a shared representation that is agnostic of the modality. Our experiments suggest that our scene representation can help transfer representations across modalities for retrieval. Moreover, our visualizations suggest that units emerge in the shared representation that tend to activate on consistent concepts independently of the modality.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>C AN you recognize the scenes in Figure <ref type="figure" target="#fig_0">1</ref>, even though they are depicted in different modalities? Most people have the capability to perceive a concept in one modality, but represent it independently of the modality. This crossmodal ability enables people to perform some important abstraction tasks, such as learning in different modalities (cartoons, stories) and applying them in the real-world.</p><p>Unfortunately, visual representations do not yet have this cross-modal capability. Standard approaches typically learn a separate representation for each modality, which works well when operating within the same modality. However, the representations learned are not aligned across modalities, which makes cross-modal transfer difficult.</p><p>Two modalities are strongly aligned if, for two images from each modality, we have paired data and correspondence at the level of objects. In contrast, weak alignment is if we have only unpaired data and a coarse global label that is shared across both images. For instance, if we have a picture of a bedroom and a line drawing of a different bedroom, the only thing that we know is shared across these two images is the scene type. However, they will differ in the objects and viewpoint inside.</p><p>In this paper, our goal is to learn a representation for scenes that has strong alignment without using paired data. We seek to learn representations that will connect objects (such as bed, car) across modalities (e.g., a picture of a car, a line drawing of a car, and the word "car") without ever specifying that such a correspondence exists.</p><p>To investigate this, we assembled a new cross-modal scene dataset, which captures hundreds of natural scene types in five different modalities, and we show a few examples in Figure <ref type="figure" target="#fig_0">1</ref>. Using this dataset and only annotations of scene categories, we propose to learn an aligned crossmodal scene representation. We present two approaches to regularize cross-modal convolutional networks so that the intermediate representations are aligned across modalities, even when only weak alignment of scene categories is available during training. Figure <ref type="figure">2</ref> visualizes the representation that our full method learns. Notice that our approach learns hidden units that activate on the same object, regardless of the modality. Although the only supervision is the scene category, our approach enables alignment to emerge automatically.</p><p>Our approach builds on a foundation of domain adaptation <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b17">[18]</ref> and multi-modal learning <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b38">[39]</ref> methods in computer vision. However, our focus is learning cross-modal representations when the modalities are significantly different (e.g., text and natural images) and with minimal supervision. In our approach, the only supervision we give is the scene category, and no alignments nor correspondences are annotated. To our knowledge, the adaptation of intermediate representations across several extremely different modalities with minimal supervision has not yet been extensively explored.</p><p>We believe cross-modal representations can have a large impact on several computer vision applications. For example, data in one modality may be difficult to acquire for privacy, legal, or logistic reasons (eg, images in hospitals), but may be abundant in other modalities, allowing us to train models using accessible modalities. In search, users may wish to retrieve similar natural images given a query in a modality that is simpler for a human to produce (eg, drawing or writing). Additionally, some modalities may be more effective for human-machine communication.</p><p>Our experiments suggest our network is learning an aligned cross-modal representation without paired data. We show four main results. Firstly, we show that our method enables better representations for cross-modal retrieval than a fine-tuning approach. Secondly, we experimented with zero-shot recognition and retrieval using our representation, and results suggest our approach can perform well even when training data for that category is missing for some modalities. Thirdly, we visualize the internal activations of our network, and we demonstrate that units automatically emerge that activate on high-level concepts agnostic of the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bedroom Kindergarten classroom</head><p>There is a bed with a striped bedspread. Beside this is a nightstand with a drawer. There is also a tall dresser and a chair with a blue cushion. On the dresser is a jewelry box and a clock. I am inside a room surrounded by my favorite things. This room is filled with pillows and a comfortable bed. There are stuffed animals everywhere. I have posters on the walls. My jewelry box is on the dresser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clip art Sketches Spatial text Descriptions</head><p>There are brightly colored wooden tables with little chairs. There is a rug in one corner with ABC blocks on it. There is a bookcase with picture books, a larger teacher's desk and a chalkboard.</p><p>The young students gather in the room at their tables to color. They learn numbers and letters and play games. At nap time they all pull out mats and go to sleep. modality. Finally, we show that our learned representation enables us to reconstruct images across other modalities. The remainder of this paper describes and analyzes our cross-modal representations in detail. In section 2, we first discuss related work that our work builds upon. In section 3, we introduce our new cross-modal scene dataset. In section 4, we present two complementary approaches to regularize convolutional networks so that intermediate representations are aligned across modalities. In section 5, we present our visualizations and experiments in cross-modal transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Domain Adaptation: Domain adaptation techniques address the problem of learning models on some source data distribution that generalize to a different target distribution. <ref type="bibr" target="#b37">[38]</ref> proposes a method for domain adaptation using metric learning. In <ref type="bibr" target="#b17">[18]</ref> this approach is extended to work on unsupervised settings where one does not have access to target data labels, while <ref type="bibr" target="#b40">[41]</ref> uses deep CNNs instead. <ref type="bibr" target="#b11">[12]</ref> learns a transformation between source and target subspaces and utilizes that for domain adaptation. <ref type="bibr" target="#b39">[40]</ref> shows the biases inherent in common vision datasets and <ref type="bibr" target="#b22">[23]</ref> proposes models that remain invariant to them. <ref type="bibr" target="#b27">[28]</ref> learns an aligned representation for domain adaptation using CNNs and the MMD metric. Our method differs from these works in that it seeks to find a cross-modal representations between highly different modalities instead of modelling close domain shifts.</p><p>One-Shot/Zero-Shot Learning: One-shot learning techniques <ref type="bibr" target="#b10">[11]</ref> have been developed to learn classifiers from a single or a few examples, mostly by reusing classifier parameters <ref type="bibr" target="#b12">[13]</ref>, using contextual information <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b19">[20]</ref> or sharing part detectors <ref type="bibr" target="#b3">[4]</ref>. In a similar fashion, zero-shot learning <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b42">[43]</ref> addresses the problem of learning new classifiers without training examples in a given domain, by using additional knowledge in the form of textual descriptions or attributes. The goal of our method is to learn aligned representations across domains, which could be used for zero-shot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-modal content retrieval and multi-modal embeddings:</head><p>Large unannotated image collections are difficult to explore, and retrieving content given fine-grained queries might be a difficult task. A common solution to this issue is to use query examples from a different modality in which it is easy to express a concept (such as a clip art images, text or a sketches) and then rank the images in the collection according to their similarity to the input query. Matching can be done by establishing a similarity metric between content from different domains. <ref type="bibr" target="#b8">[9]</ref> focuses on recovering semantically related natural images to a given sketch query and <ref type="bibr" target="#b43">[44]</ref> uses query sketches to recover 3D shapes. <ref type="bibr" target="#b20">[21]</ref> uses an MRF of topic models to retrieve images using text, while <ref type="bibr" target="#b35">[36]</ref> models the correlations between visual SIFT features and text hidden topic models to retrieve media across both domains. CCA <ref type="bibr" target="#b18">[19]</ref> and variants <ref type="bibr" target="#b36">[37]</ref> are commonly employed methods in cross-modal content retrieval. Another possibility is to learn a joint embedding for images and text in which nearest neighbors are semantically related. <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b31">[32]</ref> learn a semantic embedding that joins representations from a CNN trained on ImageNet and distributed word representations. <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b45">[46]</ref> extend them to include a decoder that maps common representations to captions. <ref type="bibr" target="#b38">[39]</ref> maps visual features to a word semantic embedding. Our method learns a joint embedding for many different modalities, including different visual domains and text. Another group of works incorporate sound as another modality <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b0">[1]</ref>. Our joint representation is different from previous works in that it is initially obtained from a CNN and sentence embeddings are mapped to it. Furthermore, we do not require explicit one-to-one correspondences across modalities.</p><p>Learning from Visual Abstraction: <ref type="bibr" target="#b48">[49]</ref> introduced clipart images for visual abstraction. The idea is to learn concepts by collecting data in the abstract world rather than the natural images so that we are not affected by mistakes in mid-level recognition such as incorrect objection detections. <ref type="bibr" target="#b13">[14]</ref> learns dynamics and <ref type="bibr" target="#b49">[50]</ref> learns sentence phrases in this abstract world and transfer them to natural images. Our work can complement this effort by learning models in a representation space that is invariant to modality. Fig. <ref type="figure">2:</ref> We learn low-level representations specific for each modality (white and grays) and a high-level representation that is shared across all modalities (red). Above, we also show masks of inputs that activate specific units the most <ref type="bibr" target="#b46">[47]</ref>. Interestingly, although the network is trained without aligned data, units emerge in the shared representation that tend to fire on the same objects independently of the modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CROSS-MODAL PLACES DATASET</head><p>We assembled a new dataset 1 to train and evaluate crossmodal scene recognition models called CMPlaces. It covers five different modalities: natural images, line drawings, cartoons, text descriptions, and spatial text images. We show a few samples from these modalities in Figure <ref type="figure" target="#fig_0">1</ref>. Each example in the dataset is annotated with a scene label. We use the same list of 205 scene categories as Places <ref type="bibr" target="#b47">[48]</ref>, which is one of the largest scene datasets available today. Hence, the examples in our dataset span a large number of natural situations. Note that the examples in our dataset are not paired between modalities since our goal is to learn strong alignments from weakly aligned data. Furthermore, this design decision eased data collection.</p><p>We chose these modalities for two reasons. Firstly, since our goal is to study transfer across significantly different modalities, we seek modalities with different statistics to those of natural images (such as line drawings and text). Secondly, these modalities are easier to generate than real images, which is relevant to applications such as image retrieval. For each modality we select 10 random examples in each of the 205 categories for the validation set and the rest for the training set, except for natural images for which we employ the training and validation splits from <ref type="bibr" target="#b47">[48]</ref> containing 2.5 million images.</p><p>Natural Images: We use images from the Places 205 Database <ref type="bibr" target="#b47">[48]</ref> to form the natural images modality.</p><p>Line Drawings: We collected a new database of sketches organized into the same 205 scene categories through Amazon Mechanical Turk (AMT). The workers were presented with the WordNet description of a scene and were asked to draw it with their mouse. We instructed workers to not write text that identifies the scene (such as a sign). We collected Descriptions: We also built a database of scene descriptions through AMT. We once again presented users with the WordNet definition of a scene, but instead we asked them 1. Dataset is available at http://projects.csail.mit.edu/cmplaces/ to write a detailed description of the scene that comes to their mind after reading the definition. We specifically asked the users to avoid using trivial words that could easily give away the scene category (such as writing "this is a bedroom"), and we encouraged them to write full paragraphs. We split our dataset into 9,752 training descriptions and 2,050 validation descriptions. We believe Descriptions is a good modality to study as humans communicate easily in this modality and allows to depict scenes with great detail, making it an interesting but challenging modality to transfer between.</p><p>Clip Art: We assembled a dataset of clip art images for the 205 scene categories defined in Places205. Clip art images were collected from image search engines by using queries containing the scene category and then manually filtered. This dataset complements other cartoon datasets <ref type="bibr" target="#b48">[49]</ref>, but focuses on scenes. We believe clip art can be an interesting modality because they are readily available on the Internet and depict everyday situations. We split the dataset into 11,372 training and 1,954 validation images (some categories had less than 10 examples).</p><p>Spatial Text: Finally, we created a dataset that combines images and text. This modality consists of an image with words written on it that correspond to spatial locations of objects. We automatically construct this dataset using images from SUN <ref type="bibr" target="#b44">[45]</ref> and its annotated objects. We created 456,300 training images and 2,050 validation images. This modality has an interesting application for content retrieval. By learning a cross-modal representation with this modality, users could use a user interface to write the names of objects and place them in the image where they want them to appear. Then, this query can be used to retrieve a natural image with a similar object layout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CROSS-MODAL SCENE REPRESENTATION</head><p>In this section we describe our approach for learning crossmodal scene representations. Our goal is to learn a strongly aligned representation for the different modalities in CM-Places. Specifically, we want to learn a representation in which different scene parts or concepts are represented independently of the modality. This task is challenging partly because our training data is only annotated with scene labels instead of having one-to-one correspondences, meaning that our approach must learn a strong alignment from weakly aligned data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Cross-Modal Scene Networks</head><p>We extend single-modality classification networks <ref type="bibr" target="#b25">[26]</ref> in order to handle multiple modalities. The main modifications we introduce are that we a) have one network for each modality and b) enforce higher-level layers to be shared across all modalities. The motivation is to let early layers specialize to modality specific features (such as edges in natural images, shapes in line drawings, or phrases in text), while higher layers are meant to capture higherlevel concepts (such as objects) in a representation that is independent of the modality .</p><p>We show this network topology in Figure <ref type="figure" target="#fig_2">3</ref> with modalspecific layers (white) and shared layers (red). The modalspecific layers each produce a convolutional feature map (pool5), which is then fed into the shared layers (fc6 and fc7). For visual modalities, we use the same convolutional network architecture (Figure <ref type="figure" target="#fig_2">3a</ref>), but different weights across modalities. However, since text cannot be fed into a CNN (descriptions are not images), we instead encode each description into skip thought vectors <ref type="bibr" target="#b24">[25]</ref> and use a multiple layer perceptron to map them into a representation with the same dimensionaly as pool5 (Figure <ref type="figure" target="#fig_2">3b</ref>). Note that, in contrast to siamese networks <ref type="bibr" target="#b5">[6]</ref>, our architecture allows learning alignments without paired data.</p><p>We could train these networks jointly end-to-end to categorize the scene label while sharing weights across modalities in higher layers. Unfortunately, we empirically discovered that this method by itself does not learn a robust cross-modal representation. This approach encourages units in the later layers to emerge that are specific to a modality (e.g., fires only on cartoon cars). Instead, our goal is to have a representation that is independent the modality (e.g., fires on cars in all modalities).</p><p>In the rest of this section, we address this problem with two complementary ideas. Our first idea modifies the popular fine-tuning procedure, but applies it on modalities instead. Our second idea is to regularize the activations in the network to have common statistics. We finally discuss how these methods can be combined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Method A: Modality Tuning</head><p>Our first approach is inspired by finetuning, which is a popular method for transfer learning with deep architectures <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b47">[48]</ref>. The conventional approach for finetuning is to replace the last layer of the network with a new layer for the target task. The intuition behind fine-tuning is that the earlier layers can be shared across all vision tasks (which may be difficult to learn otherwise without large amounts of data in the target task), while the later layers can specialize to the target task.</p><p>We propose a modification to the fine-tuning procedure for cross-modal alignment. Rather than replacing the last layers of the network (which are task specific), we can   <ref type="bibr" target="#b47">[48]</ref> to produce pool5. b) When the input is a description, we use an MLP on skip-thought vectors <ref type="bibr" target="#b24">[25]</ref> to produce pool5 (as text cannot be easily fed into the same CNN). Fig. <ref type="figure">4</ref>: Statistical Regularization. We illustrate this regularization with an example. Above, the feature distribution p(x i ) learned from Places network is modeled with a GMM, and on incorporated as a prior on x i while optimizing the deep model in line drawings modality. instead replace the earlier layers of the network (which are modality specific). By freezing the later layers in the network, we transfer a high level representation to other modalities. This approach can be viewed as finetuning the network for a modality rather than a task.</p><p>To do this, we must first learn a source representation that will be utilized for all five modalities. We use the Places-CNN network as our initial representation. Places is a reasonable representation to start with because <ref type="bibr" target="#b46">[47]</ref> shows that high-level concepts (objects) emerge in the later layers. We then train each modal-specific network to categorize scenes in its modality while holding the shared higher layers fixed. Consequently, each network will be forced to produce an aligned intermediate representation so that the higher layers will categorize the correct scene.</p><p>Since the higher level layers were originally trained with only one modality (natural images), they did not have a chance to adapt to the other modalities. After we train the networks for each modality for a fixed number of iterations, we can unfreeze the later layers, and train the full network jointly, allowing the later layers to accommodate information from the other modalities without overfitting to modalspecific representations.</p><p>Our approach is a form of curriculum learning <ref type="bibr" target="#b4">[5]</ref>. If</p><p>we train this multi-modal network with the later layers unfrozen from the beginning, units tend to specialize to a particular modality, which is undesirable for cross-modal transfer. By enforcing a curriculum to learn high level concepts first, then transfer to modalities, we can obtain representations that are more modality-invariant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Method B: Statistical Regularization</head><p>Our second approach is to encourage intermediate layers to have similar statistics across modalities. Our approach builds upon <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b1">[2]</ref> who transfer statistical properties across object detection tasks. Here, we instead transfer statistical properties of the activations across modalities. Let x n and y n be a training image and the scene label respectively, which we use to learn the network parameters w. We write h i (x n ; w) to refer to the hidden activations for the ith layer given input x n , and z(x n ; w) is the output of the network. During learning, we add a regularization term over hidden activations h:</p><formula xml:id="formula_0">min w n L(z(x n ; w), y n ) + n,i λ i • R i (h i (x n ; w)) (1)</formula><p>where the first term L is the standard softmax objective and the second term R is a regularization over the activations. 2  The importance of this regularization is controlled by the hyperparameter λ i ∈ R.</p><p>The purpose of R is to encourage activations in the intermediate hidden layers to have similar statistics across modalities. Let P i (h) be a distribution over the hidden activations in layer i. We then define R to be the negative log likelihood:</p><formula xml:id="formula_1">R i (h) = -log P i (h; θ i )<label>(2)</label></formula><p>Since P i is unknown we learn it by assuming it is a parametric distribution and estimating its parameters with a large training set. To that goal, we use activations in the hidden layers of Places-CNN to estimate P i for each layer. The only constraint on P i is that its log likelihood is differentiable with respect to h i , as during learning we will optimize Eqn.1 via backpropagation. While there are a variety of types of distributions we could use, we explore two: Multivariate Gaussian (B-Single). We consider modeling P i with a normal distribution: P i (h; µ, Σ) ∼ N (µ, Σ). By taking the negative log likelihood, we obtain the regularization term R i (h) for this choice of distribution:</p><formula xml:id="formula_2">R i (h; µ i , Σ i ) = 1 2 (h -µ i ) T Σ i -1 (h -µ i )<label>(3)</label></formula><p>where we have omitted a constant term that does not affect the optimum of the objective. Notice that the derivatives δ Ri δh can be easily computed, allowing us to back-propagate this cost through the network.</p><p>Gaussian Mixture (B-GMM). We also consider using a mixture of Gaussians to parametrize P i , which is more flexible than a single Gaussian distribution. Under this model, the negative log likelihood is:</p><formula xml:id="formula_3">R i (h; α, µ, Σ) = -log K k=1 α k • P k (h; µ k , Σ k )<label>(4)</label></formula><p>2. We omitted the weight decay from the objective for clarity. In practice, we also use weight decay.</p><p>such that P k (h; µ, Σ) ∼ N (µ, Σ) and k α k = 1 for α k ≥ 0 ∀ k . Note that we have dropped the layer subscript i for clarity, however it is present on all parameters. Since δ Ri δh can be analytically computed, we can efficiently incorporate this cost into our objective during learning with backpropagation. To reduce the number of parameters, we assume the covariances Σ k are diagonal.</p><p>We fit a separate distribution for each of the regularized layers in our experiments (pool5, fc6, fc7). During learning, the optimization will favor solutions that categorize the scene but also have an internal shared representation that is likely under P i . Since P i is estimated using Places-CNN, we are enforcing each modality network to have similar higher layers statistics to those of Places-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Method C: Joint Method</head><p>The two proposed methods (A and B) operate on complementary principles and may be jointly applied while learning the networks. We combine both methods by first fixing the shared layers for a given number of iterations. Then, we unfreeze the weights of the shared layers, but now train with the regularization of method B to encourage activations to be statistically similar across modalities and avoid overfitting to a specific modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Implementation Details</head><p>We implemented our network models using Caffe <ref type="bibr" target="#b21">[22]</ref>. Both our methods build on top of the model described in <ref type="bibr" target="#b25">[26]</ref>, with the modification that the activations from layers pool5 onwards are shared across modalities, and layers before are modal-specific. Architectures for method A only use standard layer types found in the default version of the framework. In contrast, for model B we implemented a layer to perform regularization given the statistics of a GMM as explained in the previous sections. In our experiments the GMM models are composed by K = 100 different single gaussians. In method B each modality network is trained individually and same statistical properties extracted from natural images are enforced on each model.</p><p>For each model we have a separate CNN initialized using the weights of Places-CNN <ref type="bibr" target="#b47">[48]</ref>. The weights in the lower-layers can adapt independently for each modality, while we impose restrictions in the higher layer weights as explained for each method. Because CNNs start training from a good initialization, we set up the learning rate to lr = 1e -3 (higher learning rates made our models diverge). We train the models using Stochastic Gradient Descent.</p><p>To adapt textual data to our models we use the network architecture described here. First, we represent descriptions by average-pooling the Skip-thought <ref type="bibr" target="#b24">[25]</ref> representations of each sentence in a given description (a description contains multiple sentences). To adapt this input to our shared representation we employ a 2-layer MLP. The layer size is constant and equal to 4800 units, which is the same dimensionality as that of a Skip-thought vector, and we use ReLU non-linearities. The weights of these layers are initialized using a gaussian distribution with std = 0.1. This choice is important as the statistics of the Skip-thought representations are quite different to those of images and inadequate weight initializations prevent the network from  TABLE 2: Cross-Modal Retrieval PR@10: We report the precision at top 10 retrieval of images across modalities using fc7 features. Each column shows a different query-target pair. On the far right, we average over all pairs. Our methods perform better on average than the finetuning baselines with method C performing the best.  adapting textual descriptions to the shared representation. Finally, the output layer of the MLP is fully-connected to the first layer (pool5) of our shared representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head><p>Our goal in this paper is to learn a representation that is aligned across modalities. We show four main results that evaluate how well our methods address this problem. First, we perform cross-modal retrieval of semanticallyrelated content. Secondly, we analyze the network's ability to recognize scene categories which are absent from modality. Thirdly, we show visualizations of the learned representations that give a qualitative measure of how this alignment is achieved. Finally, we show we can reconstruct natural images from other modalities using the features in the aligned representation as a qualitative measure of which semantics are preserved in our cross-modal representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Cross-Modal Retrieval</head><p>In this experiment we test the performance of our models to retrieve content depicting the same scene across modalities. Our hypothesis is that, if our representation is strongly aligned, then nearest neighbors in this common representation will be semantically related and similar scenes will be retrieved.</p><p>We proceed by first extracting features for the validation set of each modality from the shared layers of our cross-modal representation. Then, for every modality, we randomly sample a query image and compute the cosine distance to the extracted feature vectors of all content in the other modalities. We rank the documents according to the distances and compute the Average Precision (AP) when using the scene labels. We repeat this procedure 1000 times and report the obtained mean APs for cross-modality retrieval in table 1. We also report mean precision at top 10 (PR@10) results in table 2. For completeness, we show examples of retrievals in Figure <ref type="figure">5</ref>. We compare our results against finetuning baselines: Fig. <ref type="figure">5</ref>: Cross-Modality Retrieval : An example of cross-modal retrieval given a query from each of the modalities. For each row, the leftmost column depicts the query example, while the rest of the columns show the top 2 ranked results in each modalitiy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Finetuning individual networks (BL-Individual):</head><p>In this baseline we finetune a separate CNN for each of the modalities. The CNNs follow the AlexNet <ref type="bibr" target="#b25">[26]</ref> architecture and are initialized with the weights of Places-CNN. We then finetune each one of them using the training set from the corresponding modality. This is the current standard approach employed in the computer vision community, but it does not enforce the representations in higher CNN layers to be aligned across modalities explicitly.</p><p>Finetuning with shared upper layers (BL-Shared-Upper): Similarly to our method A, we force networks for each modality to share layers from pool5 onwards. However, as opposed to our method, in this baseline we do not fix the weights in the shared layers and instead let them be updated by backpropagation. We also include a version of this model that is trained from scratch (no finetuning) which is referred to as BL-Shared-Upper-Scratch.</p><p>Subspace alignment methods (Subsp. Align. <ref type="bibr" target="#b11">[12]</ref>): In addition to the finetuning baselines, we also compare our results against a state of the art domain adaptation method referred to as subspace alignment (SA) <ref type="bibr" target="#b11">[12]</ref>. SA learns a transformation between two domains by aligning source and target subspaces. This transformation provides a crossmodal similarity metric that can be used for cross-modal retrieval. We used the code provided by the authors <ref type="bibr" target="#b11">[12]</ref> and adapted it for our setting. SA requires a set of features to operate over. One option is to train a deep model for each modality individually from scratch, however, the deep models overfits miserably for three of the modalities for which we have limited training data (i.e. clipart, line drawings, descriptions). Hence we applied SA on top of two deep features that we can obtain. First we picked the original PlacesCNN features (i.e. fc7) <ref type="bibr" target="#b47">[48]</ref> extracted in all visual modalities except the descriptions. The pairwise transformations between each pair of modalities are learned over the training set and then applied for the retrieval experiments on the test set. We also fine-tuned the original PlacesCNN for each modality using the training set and then extracted fc7 features for each modality. Note that this is somewhat similar to modality tuning since the upper layers are also migrated from the PlacesCNN during finetuning, however, the weights are not fixed. Also note that SA only provides pairwise alignments, that is 10 separate transformations for each pair of modalities, and it does not align all five modalities at once on the same representation.</p><p>CCA approaches are common for cross-modal retrieval, however past approaches were not directly comparable to our method. Standard CCA requires sample-level alignment, which is missing in our dataset. Cluster CCA <ref type="bibr" target="#b36">[37]</ref> works for class-level alignments, but the formulation is intended for only two modalities. On other hand, Generalized CCA <ref type="bibr" target="#b18">[19]</ref> does work for multiple modalities but still requires sample-level alignments. Concurrent work with ours extends CCA to multi-label settings <ref type="bibr" target="#b34">[35]</ref>.</p><p>As displayed in table 1 and table 2 both method A and B improve over baselines, suggesting that the proposed methods have a better semantic alignment in fc7 across all modalities. Furthermore, method C outperforms all other reported methods on average. Particularly, we can observe how method C is able to obtain a comparable performance for retrievals using descriptions to method A, while retaining the superior performance of method B for the other modalities. Note that in our experiments the baseline methods perform similarly to our method in all modalities except for descriptions, as they were not able to align the textual and visual data very well. Also note that the performance gap between our method and the baselines increases as modalities differ from each other (see DSC results). For statistical regularization, using GMM instead of a single Gaussian also notably improves the performance,  arguably because of the increased complexity of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Zero-Shot Recognition and Retrieval</head><p>One important application of cross-modal transfer is learning in one modality (e.g., natural images), but leveraging it in a different modality (e.g., sketches or text). For example, some domains may be easier to acquire training data (due to privacy or cost), but the model will eventually be tested in a different domain. Here, we experiment using our approach for scene recognition when some modalities lack training data of some categories. We train our models the same way as before, except we remove some categories from the clip art, sketches, spatial text, and textual description modalities. To do this, we randomly chose 55 scene categories to remove from these modalities' training data. Hence, only the natural image modality has access to all 205 categories Although most modalities lack any training data for some categories, our hope is the network's alignment between data-limited modalities and natural images will be robust enough that it can still recognize the removed categories at inference time. Table <ref type="table" target="#tab_7">4</ref> shows classification accuracy on the held-out categories for the data-limited modalities. For visual modalities, statistical regularization outperforms the baseline non-regularized network, suggesting this approach helps an alignment to emerge that is useful for classification. However, for textual descriptions, modality tuning provides a better alignment. We believe this is because text is a significantly different modality from images, making it harder to align. We also show performance using the pre-trained PlacesCNN, which never saw the other modalities during training. Our approach tends to outperform the PlacesCNN on the modalities that are very different to natural images.</p><p>We also compare our results against the subspace alignment <ref type="bibr" target="#b11">[12]</ref> methods. The cross-modal similarity metric provided by SA allows us to train an SVM in the source domain and successfully adapt it to the target domain.</p><p>First we experimented with PlacesCNN features extracted from four visual modalities. The pairwise transformations between natural images and each modality is learned over the training set and then applied for zero-shot recognition on the test set. As shown in the table 4, SA gives good results when adapted from natural images to clip-arts (two similar domains). However performance degrades rapidly for line drawings, and does not even work for the spatial text modality. Similar to other domain adaptation methods, SA is also designed for moderate domain changes and it fails to align domains with extreme changes as results suggest in table <ref type="bibr" target="#b3">4</ref>.</p><p>In order for SA to work better, we fine-tuned the original PlacesCNN for each modality using the training set (150 categories version) and then extracted fc7 features for each modality. Then we applied the SA on top of these better aligned features. Although the classification results are more competitive in this case, they still are not reaching the best result in each task.</p><p>Another critical point is that SA and Method B variants, including both StatReg(Gauss) and StatReg(GMM), learn pairwise alignment between natural images and the target modality as opposed to the joint training of all modalities. Although this enables them to have a better alignment between the two modalities when they are similar (see clipart in table <ref type="table" target="#tab_7">4</ref>), the alignment is not necessarily shared by all five modalities at the same time. In methods A and C, the alignment is shared in all five modalities.</p><p>We also experimented with cross-modal retrieval on the held-out categories. Table <ref type="table" target="#tab_9">5</ref> shows mean average precision for retrieval on these missing categories. While modality tuning provides a slight improvement over baselines on average, combining both of our approaches yields better retrievals in the absence of missing categories. SA baselines also achieve good performances in a couple of cases but in the overall method C yields the best performance. Table <ref type="table" target="#tab_7">4</ref> and table 5 both suggest our network is starting to learn an alignment even the absence of categories for some modalities. However, they also suggest a trade-off that depends on the task. If the task is classification, then our experiments suggest one of statistical regularization or modality tuning is better. However, if the task is retrieval, then combining both methods is better. We believe this is because the both our methods can be seen as a cross-modal regularization. Stronger regularization on the internal activations helps retrieval performance because it helps the features to be more specific to instances. Nevertheless, such regularization also adds more constraints during learning that may hurt classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Hidden Unit Visualizations</head><p>We now investigate what input data activates units in our shared representation. For visual data, we use a visualization similar to <ref type="bibr" target="#b46">[47]</ref>. For textual descriptions, we compute the paragraphs that maximally activate each filter, and then we employ tf-idf features to determine the most common relevant words in these paragraphs.</p><p>Figure <ref type="figure">6</ref> shows, for some of the 256 filters in pool5, the images in each visual modality that maximally activated the filter with their mask superimposed, as well as the most Fig. <ref type="figure">6</ref>: Visualizing Unit Activations: We visualize pool5 in our cross-modal representation above by finding masks of images/descriptions that activate a specific unit the most <ref type="bibr" target="#b46">[47]</ref>. Interestingly, the same unit learns to detect the same concept across modalities, suggesting that it may has learned to generalize across these modalities.  common words in the paragraphs that maximally activated the units. We can observe how the same concept can be detected across modalities without having explicitly aligned training data. These results suggest that our method is learning some strong alignments across modality only using weak labels coming from the scene categories.</p><p>To quantify this observation, we set up an experiment. We showed human subjects activations of 100 random units from pool5. These activations included the top five responses in each modality with their mask. The task was to select, for each unit, those images that depicted a common concept if it existed. Activations could be generated from either the baseline BL-Ind or from our method A, but this information is hidden from the subjects.</p><p>After running the experiment, we selected those results in which at least 4 images for the real modality were selected. This ensured that the results were not noisy and were produced using units with consistent activations, as we empirically found this to be a good indicator of whether a unit represented an aligned concept. We then computed the number of times subjects selected at least one image in each of the other modalities. With our method, 33% of the times this process selected at least one image from each modality, whereas for the baseline this only happened 25% of the times. Furthermore, 19% of the times we selected at least two images for each modality as opposed to only 14% for the baseline. These results suggest that, when a unit is detecting a clear concept, our method outperforms the best finetuning method and can strongly align the different modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Analyzing Modality Invariance</head><p>A representation is invariant to modality if the feature vector does not store information about the origin modality. Since modality invariant representations would be useful crossmodal transfer, we wish to analyze the degree to which modality-specific information is contained in the representation. Using examples from the validation set, Figure <ref type="figure">7</ref> shows a two-dimensional embedding of the representation from our networks using t-SNE <ref type="bibr" target="#b28">[29]</ref>. To do this, we randomly sample 1, 000 examples from each modality and compute t-SNE of the fc7 features. We then color each point by the modality. The visualization shows that the baseline network (without any cross-modal regularization) clearly separates the representation by modality, which is Fig. <ref type="figure">7</ref>: t-SNE Embedding of Cross-Modal Representation: We visualize the embedding for fc7 of representations from different networks using t-SNE <ref type="bibr" target="#b28">[29]</ref>. Colors correspond to the modality. If the representation is agnostic to the modality, then the features should not cluster by modality. These visualizations suggest that our full method does a better job at discarding modality information than baselines. undesirable. Statistical regularization offers some invariance to modality, except for text. While our representation is not completely invariant to modality, the visualization suggests the full approach tends to be better at discarding modality information than baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Feature Reconstructions</head><p>Here we investigate if we can generate images in different modalities given a query. The motivation is to gain some visual understanding of which concepts are preserved across modalities and which information is discarded <ref type="bibr" target="#b41">[42]</ref>. We use the reconstruction approach from <ref type="bibr" target="#b7">[8]</ref> out-of-the-box, but we train the network using our features. We learn an inverting network for each modality that learns a mapping from features in the shared pool5 layer to downsampled reconstructions of the original images. We refer readers to <ref type="bibr" target="#b7">[8]</ref> for full details. We employ pool5 features as opposed to fc7 features because the amount of compression of the input image in the latter produces worse reconstructions.</p><p>If concepts in our representation are correctly aligned, our hypothesis is that the reconstruction network will learn to generate images that capture the statistics of the data in the output modality and while show same concepts across modalities in similar spatial locations. Note that one limitation of these inversions is that output images are blurry, even when reconstructing images within a same modality, due to the data compression in pool5. However, our reconstructions have similar quality to those in <ref type="bibr" target="#b7">[8]</ref> when reconstructing from pool5 features within a modality.</p><p>Figure <ref type="figure" target="#fig_4">8</ref> shows some successful examples of reconstructions. We observed this is a hard, arguably because the statistics of the activations in the common representation are very different across modalities despite the alignment, which might be due to the reduced amount of information in some of the modalities (i.e. clipart and spatial text images contain much less information that natural images). However, we note that in the examples the trained model is capable of reproducing the statistics of the output modality. Moreover, the reconstructions usually depict the same concepts present in the original image, indicating that our representation is aligning and preserving scene information across modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>Humans are able to leverage knowledge and experiences independently of the modality they perceive it in, and a similar capability in machines would enable several important applications in retrieval and recognition. In this paper, we proposed an approach to learn aligned crossmodal representations without paired data. Interestingly, our experiments suggest that our approach encourages alignment to emerge in the representation automatically across modalities, even when the training data is unaligned.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Can you recognize scenes across different modalities? Above, we show a few examples of our new cross-modal scene dataset. In this paper, we investigate how to learn cross-modal scene representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Scene Networks: We use two types of networks. a)For pixel based modalities, we use a CNN based off<ref type="bibr" target="#b47">[48]</ref> to produce pool5. b) When the input is a description, we use an MLP on skip-thought vectors<ref type="bibr" target="#b24">[25]</ref> to produce pool5 (as text cannot be easily fed into the same CNN).</figDesc><graphic coords="4,312.00,243.96,251.99,134.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Inverting features across modalities: We visualize some of the generated images by our inverting network trained on real images. Top row: reconstructions from real images. These preserve most of the details of the original image but are blurry because of the low dimensionality of the pool5 representation. Second row: reconstructions from line drawings, where the network adds colors to the reconstructions while preserving the original scene composition. Third row: inversions from the spatial text modality. Reconstructions are less detailed but roughly preserve the location, shape and colors of the different parts of the input scene. Fourth row: inversions from the clip-art modality; and inversions from natural image to line drawing modality.</figDesc><graphic coords="10,48.00,213.46,516.01,223.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="7,73.80,43.70,464.38,229.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Y Aytar, C Vondrick, A Torralba are with Massachusetts Institute of Technology, 77 Massachusetts Ave, Cambridge, MA 02139 USA. L Castrejon is with the Department of Computer Science, University of Toronto, Ontario, Canada. H Pirsiavash is with the University of Maryland Baltimore County, 1000 Hilltop Cir, ITE 342, Baltimore, MD 21250 USA</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Shared Cross--Modal Representation</head><label></label><figDesc></figDesc><table><row><cell>Unit 13: Tower Roof</cell><cell></cell><cell></cell><cell></cell><cell>religious, church, plants, impressive, monks</cell></row><row><cell>Unit 241:</cell><cell></cell><cell></cell><cell></cell><cell>plants, fruits, basil,</cell></row><row><cell>Plants</cell><cell></cell><cell></cell><cell></cell><cell>land, mint</cell></row><row><cell>Modal</cell><cell></cell><cell></cell><cell>pool5</cell><cell></cell></row><row><cell>Specific CNNs</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(MLP)</cell></row><row><cell>Natural Images</cell><cell>Sketches</cell><cell>Clip  Art</cell><cell>Spatial Text</cell><cell>Descriptions</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>14,830 training examples and 2,050 validation examples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 1 :</head><label>1</label><figDesc>Cross-Modal Retrieval mAP: We report the mean average precision (mAP) on retrieving images across modalities using fc7 features. Each column shows a different query-target pair. On the far right, we average over all pairs. For comparison, chance obtains 0.73 mAP. Best performances in each column are highlighted as bold in both this table and the others. Our methods perform better on average than the finetuning baselines with method C performing the best.</figDesc><table><row><cell>Cross Modal Retrieval</cell><cell cols="15">Query Target CLP SPT LDR DSC NAT SPT LDR DSC NAT CLP LDR DSC NAT CLP SPT DSC NAT CLP SPT LDR PR@10 NAT CLP SPT LDR DSC Mean</cell></row><row><cell>BL-Individual</cell><cell></cell><cell cols="3">17.8 12.0 10.4 0.5 22.9 10.2</cell><cell cols="3">9.8 0.6 12.3 8.8</cell><cell cols="4">5.3 0.4 10.1 8.4 5.1 0.5</cell><cell cols="3">0.7 0.7 0.8</cell><cell>0.7</cell><cell>6.9</cell></row><row><cell cols="2">BL-Shared-Upper-Scratch</cell><cell>7.1 7.6</cell><cell cols="2">4.7 10.4 11.1 4.9</cell><cell cols="2">3.4 8.4</cell><cell>9.7 4.3</cell><cell cols="2">2.7 8.1</cell><cell cols="5">5.4 2.9 2.8 4.6 10.3 5.8 6.3</cell><cell>3.1</cell><cell>6.2</cell></row><row><cell>BL-Shared-Upper</cell><cell></cell><cell>11.1 12.6</cell><cell cols="2">4.9 14.2 16.8 7.0</cell><cell cols="3">4.1 9.9 12.0 6.1</cell><cell cols="2">2.9 8.1</cell><cell cols="2">5.9 3.6 3.4 3.8</cell><cell cols="3">5.9 4.9 6.4</cell><cell>3.3</cell><cell>7.4</cell></row><row><cell cols="3">Subsp. Align. [12] + PlacesNet 10.0 0.4</cell><cell>3.1</cell><cell>-13.0 0.5</cell><cell>2.3</cell><cell>-</cell><cell>0.4 0.4</cell><cell>0.5</cell><cell>-</cell><cell>3.6 3.1 0.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Subsp. Align. [12] + PlacesNet Finetune</cell><cell cols="6">16.0 9.2 10.8 0.7 24.8 8.3 11.1 0.7 12.7 8.7</cell><cell cols="4">5.0 0.6 12.8 9.6 4.6 0.7</cell><cell cols="3">0.6 0.7 0.7</cell><cell>0.5</cell><cell>6.9</cell></row><row><cell>A: Tune</cell><cell></cell><cell>14.3 10.6</cell><cell cols="2">7.8 20.7 18.1 8.2</cell><cell cols="2">6.1 14.5</cell><cell>9.6 4.8</cell><cell cols="2">3.4 10.4</cell><cell cols="5">8.8 5.1 3.7 8.4 14.8 5.5 8.6</cell><cell>3.8</cell><cell>9.4</cell></row><row><cell>A: Tune (Free)</cell><cell></cell><cell>15.0 16.4</cell><cell cols="2">8.9 19.8 16.8 8.1</cell><cell cols="3">4.9 13.8 21.1 9.0</cell><cell cols="2">5.6 17.4</cell><cell cols="5">8.4 4.6 4.3 8.1 12.2 4.5 9.8</cell><cell>3.9</cell><cell>10.6</cell></row><row><cell cols="2">B: StatReg (Gaussian)</cell><cell cols="6">16.9 11.6 10.8 0.9 22.8 9.1 10.4 0.6 12.1 8.6</cell><cell cols="2">5.0 0.7</cell><cell cols="2">9.5 7.7 5.1 0.6</cell><cell cols="3">1.4 1.3 1.3</cell><cell>1.3</cell><cell>6.9</cell></row><row><cell>B: StatReg (GMM)</cell><cell></cell><cell cols="6">18.2 10.8 11.3 0.5 23.9 9.9 10.4 0.5 11.0 7.4</cell><cell cols="4">4.7 0.5 13.0 9.1 6.2 0.5</cell><cell cols="3">0.7 0.5 0.7</cell><cell>0.6</cell><cell>7.0</cell></row><row><cell cols="2">C: Tune + StatReg (GMM)</cell><cell>14.1 16.6</cell><cell cols="2">7.9 23.2 17.8 10.0</cell><cell cols="3">6.1 15.1 18.1 8.7</cell><cell cols="2">5.2 17.7</cell><cell cols="5">8.8 5.4 5.4 7.9 33.5 17.1 20.9</cell><cell>9.2</cell><cell>13.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3 : Mean Cross-Modal Retrieval mAPs across Lay- ers:</head><label>3</label><figDesc>Note that the baseline results decrease drastically as we go lower levels (e.g. fc7 to fc6) in the deep network. However the alignment approaches are much less affected.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 4 :</head><label>4</label><figDesc>Zero-Shot Scene Classification: We hold out 55 scene categories during training for the clip art, spatial text, line drawings, and text descriptions modalities, and evaluate the network's ability to still classify them on the validation set. Since categories were not removed from the natural images, the network can still solve the scene classification task by finding a strong alignment between modalities. Our results suggest that our approach enables better scene classification with missing data, suggesting the network is learning a more robust alignment.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 5 :</head><label>5</label><figDesc>Zero-Shot Scene Retrieval: We hold out 55 scene categories during training for the clip art, spatial text, line drawings, and text descriptions modalities, and evaluate the network's ability to still retrieve these categories. Our results suggest that our approach outperforms baselines even when the retrievals are done with missing training data.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We thank TIG for managing our computer cluster. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used for this research. This work was supported by NSF grant IIS-1524817, by a Google faculty research award to A.T and by a Google Ph.D. fellowship to C.V.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Soundnet: Learning sound representations from unlabeled video</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="892" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Part level transfer regularization for enhancing exemplar svms</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Predicting deep zero-shot convolutional neural networks using textual descriptions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cross-generalization: Learning novel classes from a single example by feature replacement</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005. 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="672" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Signature verification using a siamese time delay neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="669" to="688" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.1531</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inverting convolutional networks with convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sketchbased image retrieval: Benchmark and bag-of-features descriptors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hildebrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Boubekeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TVCG</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Write a classifier: Zeroshot learning using purely textual descriptions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised visual domain adaptation using subspace alignment</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Object classification from a single example utilizing class relevance metrics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Predicting object dynamics in scenes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2027" to="2034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">What makes a good detector?-structured priors for learning from few examples</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="354" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Domain adaptation for object recognition: An unsupervised approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Canonical correlation analysis: An overview with application to learning methods</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hardoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Geometric context from a single image</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="654" to="661" />
		</imprint>
	</monogr>
	<note>Computer Vision, 2005. ICCV 2005</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning cross-modality similarity for multinomial data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Undoing the damage of dataset bias</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unifying visualsemantic embeddings with multimodal neural language models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Skip-thought vectors</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007">2012. 4, 5, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02791</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Contextual models for object detection using boosted random fields</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.08512</idno>
		<title level="m">Visually indicated sounds</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Zeroshot learning with semantic output codes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Palatucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-label cross-modal retrieval</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rasiwasia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4094" to="4102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A new approach to cross-modal multimedia retrieval</title>
		<author>
			<persName><forename type="first">N</forename><surname>Rasiwasia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Costa</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Coviello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cluster canonical correlation analysis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Rasiwasia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4068" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hoggles: Visualizing object detection features</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning visual biases from human imagination</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sketch-based 3d shape retrieval using convolutional neural networks</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03044</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Object detectors emerge in deep scene cnns</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2009">2014. 3, 4, 8, 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007">2014. 3, 4, 5, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Bringing semantics into focus using visual abstraction</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning the visual interpretation of sentences</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1681" to="1688" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
