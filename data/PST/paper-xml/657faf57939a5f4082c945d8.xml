<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Evaluation Improves Selective Generation in Large Language Models</title>
				<funder>
					<orgName type="full">Google DeepMind</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jie</forename><surname>Ren</surname></persName>
							<email>jjren@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
							<email>yaozhaoyz@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Tu</forename><surname>Vu</surname></persName>
							<email>ttvu@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
							<email>peterjliu@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
							<email>balajiln@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Google</forename><surname>Deepmind</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">Self-Evaluation Improves Selective Generation in Large Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Safe deployment of large language models (LLMs) may benefit from a reliable method for assessing their generated content to determine when to abstain or to selectively generate. While likelihood-based metrics such as perplexity are widely employed, recent research has demonstrated the limitations of using sequencelevel probability estimates given by LLMs as reliable indicators of generation quality. Conversely, LLMs have demonstrated strong calibration at the token level, particularly when it comes to choosing correct answers in multiple-choice questions or evaluating true/false statements. In this work, we reformulate openended generation tasks into token-level prediction tasks, and leverage LLMs' superior calibration at the token level. We instruct an LLM to self-evaluate its answers, employing either a multi-way comparison or a point-wise evaluation approach, with the option to include a "None of the above" option to express the model's uncertainty explicitly. We benchmark a range of scoring methods based on self-evaluation and evaluate their performance in selective generation using TRUTHFULQA and TL;DR. Through experiments with PALM-2 and GPT-3, we demonstrate that self-evaluation based scores not only improve accuracy, but also correlate better with the overall quality of generated content.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large language models (LLMs) are often pre-trained on a vast corpus of text and then fine-tuned on supervised data to follow instructions <ref type="bibr" target="#b6">[Devlin et al., 2018</ref><ref type="bibr" target="#b18">, Radford et al., 2018</ref><ref type="bibr" target="#b20">, Raffel et al., 2020</ref><ref type="bibr" target="#b0">, Adiwardana et al., 2020</ref><ref type="bibr" target="#b28">, Wei et al., 2021</ref><ref type="bibr" target="#b17">, Ouyang et al., 2022</ref><ref type="bibr" target="#b4">, Chung et al., 2022]</ref>. Having the ability to tell when a language model's output is trustworthy is important for safe deployment of language models. For example, the model's trustworthiness can be used as signal to selectively generate answers based on how confident the LLM is in the quality of its output.</p><p>Prior research has demonstrated that the distance to the training distribution in the embedding space predicts output quality for conditional generative models <ref type="bibr">[Ren et al., 2023b]</ref>. Extending this work to large language models is challenging because their training distribution is too large to estimate and extracting embeddings from well-integrated LLM systems requires significant engineering effort.</p><p>Alternatively, a straightforward approach to estimating a language model's confidence in its output is to calculate the sequence probability or the length-normalized sequence probabilities <ref type="bibr" target="#b0">[Adiwardana et al., 2020]</ref>. However, studies have shown that language models' sequence probabilities on openended generations do not reliably rank-order their outputs by quality <ref type="bibr" target="#b14">[Liu et al., 2022</ref><ref type="bibr">, Ren et al., 2023b]</ref>. Human feedback can be used to fine-tune language models to better align with human-judged quality, such as with Reinforcement Learning from Human Feedback (RLHF) <ref type="bibr" target="#b24">[Stiennon et al., 2020]</ref>, SLiC-HF <ref type="bibr" target="#b32">[Zhao et al., 2023]</ref> and DPO <ref type="bibr" target="#b19">[Rafailov et al., 2023]</ref>, resulting in better quality-calibrated models.</p><p>Since human feedback data is expensive to obtain, we explore leveraging the self-evaluation ability of LLMs to improve quality-calibration. Despite the poor calibration on sequence-level likelihood, recent work has shown that LLM token-level probability can be quite well-calibrated on choosing the arXiv:2312.09300v1 <ref type="bibr">[cs.CL]</ref>   correct option of multi-choice question answering and true/false questions <ref type="bibr" target="#b9">[Kadavath et al., 2022</ref><ref type="bibr">, OpenAI, 2023</ref><ref type="bibr" target="#b23">, Robinson et al., 2022]</ref>. This suggests that evaluating language model's generation with token-level probabilities using an appropriate prompt format might be better for selective generation than sequence-level likelihood.</p><p>In this study, we focus on obtaining a confidence score that is quality-calibrated on free-form generation tasks. We propose reducing the sequence-level scoring problem to token-level scoring by designing different self-evaluation tasks and propose a variety of scores. We focus on evaluating model's quality-calibration for use in selective generation, and not just predictive accuracy. We show that our proposed confidence estimation significantly improves the quality calibration, and can be used to abstain poor quality outputs using the TRUTHFULQA and TL;DR benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>Background: sequence likelihood Given a question x and an answer y, y = y 1 y 2 . . . y  Although sequence-level scores have weak predictive power, the previous results show that LLMs are well-calibrated on multiple choice question answer tasks and true/false evaluation tasks <ref type="bibr" target="#b9">[Kadavath et al., 2022</ref><ref type="bibr">, OpenAI, 2023]</ref>, suggesting the model has better calibration on token-level scores. Inspired by this, we propose to reduce free-form generation to multiple-choice and true/ false evaluation tasks, in order to leverage token-level calibration to improve the calibration of free-form generation, as shown in Figure <ref type="figure" target="#fig_0">1</ref>. <ref type="bibr">Ren et al. [2023a]</ref> propose a similar idea but their focus was on robotics planning, while we focus on the general question answer settings.</p><p>To convert free-form generation to multi-choice question answer task, we first sample multiple candidate answers. For a given question x, we sample n answers {y i }, i = 1, . . . , n from an LLM. We tried using a prompt to instruct the model to generate multiple different answers all at once, but the quality of the batch generated answers were not as good as sampling one at a time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Position bias</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Probability Dispersion</head><p>None is True</p><formula xml:id="formula_0">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? Figure 2:</formula><p>The issues of position bias, probability dispersion, and no true answers in the Sample and Select setup. The question examples are from <ref type="bibr" target="#b12">[Lin et al., 2021</ref><ref type="bibr" target="#b1">, Agarwal et al., 2023]</ref>.</p><p>2.1 Sample and Select: reduce free-form generation to multi-choice question answer task Given a question and a set of candidate answers {y} n , we append alphabet characters, c = A, B, C, . . . , to the answers and form it into a multiple choice format. A straightforward score could be the softmax probability for the characters, p(c i |x, {cy}), which was used in <ref type="bibr">Ren et al. [2023a]</ref>. The selected answer would be the one with the highest softmax probability, ? = y r , r = arg max i p(c i |x, {cy}). However, there are a few issues with that score:</p><p>Position bias The score could change as the position of the candidate answers change. See Figure <ref type="figure">2</ref> (left). This phenomenon was also reported in other work <ref type="bibr" target="#b23">[Robinson et al., 2022</ref><ref type="bibr" target="#b33">, Zheng et al., 2023]</ref>. A simple "shuffle and average" could de-bias and correct for the scores, while more sophisticated method to estimate the prior was proposed by <ref type="bibr" target="#b33">Zheng et al. [2023]</ref>. In our work, we use the simple shuffle and average de-bias method. The ablation study of the effect of position bias is in Table <ref type="table" target="#tab_6">4</ref>.</p><p>Probability dispersion among multiple true answers. Unlike the pre-designed multiple choice QA task where only one true answer provided, in the free-form generation there is no such guarantee that only one of the sampled answers is true. When more than one true answers are in the candidate list, the probability of the true is dispersed among the true answers, see Figure <ref type="figure">2</ref> (middle). This is an undesired property for comparing across questions, since different questions could generate different number of true answers. Probability dispersion is not a unique problem in LLMs; similar issue was discovered in the ImageNet classification where an image can map to multiple classes, and unnormalized logit was preferred than softmax probability to avoid the probability dispersion <ref type="bibr" target="#b8">[Hendrycks et al., 2019]</ref>. Therefore we propose, log p(c i |x, {cy}), c = {A, B, . . . }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(Sample and Select)</head><p>No answer is true It is possible that when the model does not know the answer, none of the sampled answers is true. If only wrong answers are provided, the model will be forced to choose one from them, resulting in over-confident prediction. See Figure <ref type="figure">2</ref> (right). To mitigate that, we add "NONE OF THE ABOVE" as an additional candidate answer to give model a chance to reject the sampled answers, {y} +nota = {y} ? {nota}. This is similar to adding "An option not listed here" to the robotic planning task <ref type="bibr">[Ren et al., 2023a]</ref>. We obtain the score corresponding to the "NONE OF THE ABOVE" answer, p(c nota |x, {cy} +nota ) (Sample and Select w/ NONE OF THE ABOVE) A higher nota score indicates that the selected answer is less likely to be correct. So we use -p(c nota |x, {cy} +nota ) as the confidence score of the selected answer, ? = y r , r = arg max i p(c i |x, {cy}). Note that the selected answer is still the answer with the highest score within the original answer set {y} excluding the nota answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sample and Eval: reduce free-form generation to true/false evaluation task</head><p>We can also evaluate a question and an answer pair using pointwise evaluation format. We ask the model if the candidate answer is correct or not, as shown in Figure <ref type="figure" target="#fig_0">1</ref>. Since the task is a binary classification task, we can normalize the output score using softmax function to a probability, p(Yes|x, y i ).</p><p>(Sample and Eval)</p><p>This is similar the P(True) proposed in <ref type="bibr" target="#b9">[Kadavath et al., 2022]</ref> </p><p>In the case where NONE OF THE ABOVE answer is added, we penalize the confidence score p(Yes|x, ?) with the uncertainty score for the nota answer, that is p(Yes|x, ?) -p(c nota |x, {cy} +nota ). We call this hybrid strategy "Sample and Select and Eval". See details in Algorithm 1.</p><p>Algorithm 1 Hybrid "Sample and Select and Eval"</p><p>1: Input: Question x, LLM model M, sample prompt G, multi-choice selection prompt F, pointwise evaluation prompt E.</p><p>2: Use sample prompt G to sample n answers {y} = {y 1 , . . . , y n }, y i iid ? M(x) 3: Append "NONE OF THE ABOVE" answer to {y} = {y} ? {nota}. |{y}| = n + 1. 4: Compose selection prompt with answers F(x, {y}), feed to M, obtain output softmax probability scores p(c i |x, {cy}). 5: Select the best answer among the sampled n answers (exclude the post-hoc added nota answer ? = y r , r = arg max i? =n+1 p(c i |x, {cy}). 6: Obtain the uncertainty score for nota answer, s nota = p(c nota |x, {cy}). 7: Compose pointwise evaluation prompt for the selected answer E(x, ?), feed to M, obtain output score s = p(Yes|x, ?). 8: The final confidence score is s = s -s nota . 9: Output: the selected answer ?, and its confidence score s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation metrics for selective generation</head><p>Suppose D = {x} m is a dataset containing m questions to evaluate. Given a LLM model M, for each question x, we randomly sample n answers {y} n = {y 1 , y 2 , . . . , y n }, where y i iid ? M(x). Suppose the ground truth h(x, y) = {0, 1} for each answer's correctness (or quality) is available, either through human evaluation or an auto-evaluation model to approximate human rating. Given a confidence score function s(x, y) measuring the confidence of a (x, y) pair, we would like evaluate how well the score could be used for selective generation, besides the accuracy.</p><p>Accuracy For a fixed question x and a set candidate answers {y} n to x, we could use the confidence score to select the final answer ? to the question x. We assess if the selected answer is correct, i.e. h(x, ?) = 1, ? = y r , r = arg max n i=1 s(x, y i ).</p><p>Accuracy evaluates if the score can be used to choose the best answer among the candidate answers within a given question. For selective generation, we compare across questions. Given the m question and its selected best answer, {(x, ?)} m , we would abstain poor quality pairs to ensure better overall generation quality, aka selective generation. Suppose for each pair we have a confidence score, s(x, ?). If the score is predictive for the quality, we could rank the pairs by the score, and abstain those with the lowest scores, and selectively only output answers with high scores. For the abstained low quality answers, we could instead output "SORRY, I DON'T KNOW". An honest "I don't know" answer is better then a wrong answer. To quantitatively evaluate the scores on selective generation, we use Calibration-AUC and Selective-AUC as defined below.</p><p>Calibration-AUC AUC metric for a binary prediction task where the binary label is the correctness h(x, ?), and the prediction score is the confidence score s(x, ?) <ref type="bibr" target="#b10">[Kivlichan et al., 2021]</ref>. Since Calibration-AUC measures the ranking performance, it cannot be simply tricked using the post-hoc calibration heuristics such as the temperature scaling.</p><p>Selective generation curve and AUC Selective generation curve measures the correctness h(x, ?) as a function of abstention rate ?%, where the samples are sorted by s(x, ?) and samples with the lowest ?% scores are abstained <ref type="bibr">[Ren et al., 2023b]</ref>. At ? = 0 no sample is abstained, so the curve starts from the conventionally defined accuracy. As ? increases, if the score is predictive of correctness, low quality samples will be abstained first, and the remaining samples will have higher overall quality. Therefore we expect the curve to increase. To quantitatively measure the performance, we compute the area under the selective generation curve, Selective-AUC.</p><p>Distinction to Expected Calibration Error (ECE) ECE <ref type="bibr" target="#b7">[Guo et al., 2017]</ref> is commonly used to measure if the predictive probability value matches the ground truth accuracy. ECE computation is straightforward for categorical prediction. However, for sequence generation, even though it is possible to define sequence-level ECE <ref type="bibr" target="#b31">[Zablotskaia et al., 2023]</ref>, getting the ground truth is challenging. Also ECE can only be applied to probabilistic scores. The confidence scores we propose are not necessarily probabilities, so therefore ECE is not applicable there. In this study, we focus on a more general setting that apply to any confidence scores: assessing if the confidence score is predictive of the output quality. Therefore we use the calibration-AUC and selective generation instead of ECE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment setup</head><p>LLMs PALM-2 LARGE is mainly used in our experiments. For each question, we sample n = 4 answers at temperature 1.0. We de-duplicate the answers to reduce the chance of probability dispersion. We also consider GPT-3 (text-davinci-003) model for evaluation. Due to the OpenAI API limitation, we cannot evaluate all the methods and obtain complete results for GPT-3<ref type="foot" target="#foot_0">1</ref> . We can neither evaluate methods on GPT-3.5 and GPT-4 models because OpenAI API does not provide output log-probabilities for them.</p><p>Benchmark datasets TRUTHFULQA <ref type="bibr" target="#b12">[Lin et al., 2021]</ref> is a dataset for assessing model's ability to generate truthful answers against false belief or misconception. It contains 817 questions in the validation split. To label the quality of generated answers, we use the GPT-judge, which is a GPT-3 model fine-tuned on human feedback data, provided by <ref type="bibr" target="#b12">Lin et al. [2021]</ref>. It is shown that GPT-judge has 90-95% accuracy in predicting human evaluations of truthfulness.</p><p>TL;DR is a summarization benchmark dataset mined from Reddit website <ref type="bibr" target="#b26">[V?lske et al., 2017]</ref>. It contains 15,240 examples in the test split. We randomly sampled 1000 examples to save inference cost. To label the quality of the generated summaries, we use a reward model fine-tuned on human feedback data, as used by <ref type="bibr" target="#b32">[Zhao et al., 2023]</ref>. The prediction accuracy of human rating of the reward model is 71.34%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>The performance of the different scores evaluated using accuracy, calibration-AUC, and selective-AUC are shown in Table <ref type="table" target="#tab_4">1</ref>. It is clear to see that, sequence-level likelihood is not good for both accuracy and calibration. It has even below 0.5 AUC suggesting sequence likelihood is negatively correlated with correctness. Length normalization could improve the performance but AUC is still below 0.5. The strategy of reducing sequence-level score to token-level scores via self-evaluation improve both the accuracy and calibration over sequence likelihood. Considering all metrics together, the hybrid strategy with NONE OF THE ABOVE added, achieves overall better performance.</p><p>Comparing the two strategies, Sample and Select and Sample and Eval, Sample and Select has decent accuracy, but suffers from the calibration metrics. Adding NONE OF THE ABOVE helps improve calibration. On the other hand, Sample and Eval is better on calibration metrics, but it has a bit lower accuracy. This trend is more clear in GPT-3. Therefore we propose the hybrid strategy to combine the best of both. The ROC curves for binary classification of correct and incorrect answers using different scores, and the selective generation curves can be found in Figure <ref type="figure">3</ref>. Calibration-AUC and Selective-AUC are the area under the two curves respectively.</p><p>In addition, we show that self-evaluation is complementary to self-critique and revise, a technique to self-improve the answer quality <ref type="bibr" target="#b2">[Bai et al., 2022]</ref>. We first apply that technique to improve each of the sampled answers. Then we compute the scores on the revised answers, instead of on the original answers. In Table <ref type="table" target="#tab_5">2</ref>, it is clear that on the revised answers, we see similar patterns that sequence-level scores are not well suited for selective generation, and the token-level scores achieves better performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>The calibration of LLMs on multiple choice question answer tasks is studied in <ref type="bibr" target="#b9">Kadavath et al. [2022]</ref>. <ref type="bibr" target="#b23">Robinson et al. [2022]</ref> show that the sequence level probability is worse than the token-level probability (e.g. A, B, C, etc) for predicting the correctness. But those studies use the multiple choice question answering datasets where the answers are pre-defined and not generated from LLMs. Our work focuses on the calibration of free-form generation tasks. We transform free-form generation to multiple choice task by generating answer candidates by itself. Another distinction to <ref type="bibr" target="#b9">[Kadavath et al., 2022]</ref> is that we care more on the ranking performance measured by AUC than the exact value match to ground truth probability measured by ECE.</p><p>In terms of estimating language models' confidence or uncertainty, <ref type="bibr" target="#b25">Tian et al. [2023]</ref>, <ref type="bibr" target="#b13">Lin et al. [2022]</ref> propose to ask model to express uncertainty in words along with the generated answer, but it is shown that LLMs often exhibit a high degree of overconfidence when verbalizing their confidence <ref type="bibr" target="#b30">[Xiong et al., 2023]</ref>. <ref type="bibr" target="#b11">Kuhn et al. [2023]</ref> propose to use semantic entropy among a set of sampled answers to estimate model's uncertainty. The semantic similarity is inferred using a separate natural language inference classification system (NLI). <ref type="bibr" target="#b5">Cole et al. [2023]</ref> find the degree of repetition in sampled answers is a good score for selectively answering ambiguous questions. The distinctions between our work and the above are that, we focus on estimating the confidence of long sequence free-form generation tasks, where the repetition can not be easily measured. Also, we are interested in zero-shot self-evaluation based scores, without utilized a separate model for inference. The true/ false evaluation method proposed by <ref type="bibr" target="#b9">Kadavath et al. [2022]</ref> is one of them. In our work, we compare this score with several other scores and have a comprehensive assessment on selective generation of free-form generation tasks</p><p>Prior studies have proposed generating multiple candidate responses for free-form generation tasks and then selecting the best. The final answer is selected using a variety of methods, including: (1) simple sequence likelihood <ref type="bibr" target="#b0">[Adiwardana et al., 2020]</ref>, (2) ranking model trained on human preference data <ref type="bibr" target="#b15">[Nichols et al., 2020]</ref>, (3) self-consistency i.e. if an answer is the most consensus one <ref type="bibr" target="#b27">[Wang et al., 2022</ref><ref type="bibr" target="#b3">, Chen et al., 2023]</ref> and (4) models' self-evaluation ability to choose the final response based on its own evaluation of the responses <ref type="bibr">[Ren et al., 2023a]</ref>. However, the focus of most prior work except for <ref type="bibr">[Ren et al., 2023a]</ref> are on improving accuracy, not on confidence estimation or calibration. <ref type="bibr">[Ren et al., 2023a]</ref> is similar to our work in the sense that it not only proposes to generate multiple options and then ask the model to choose one, but also estimate uncertainty to ask for clarification. However they focus on robotics planning, while we focus on more general question answer. Also, they directly use the multiple choice score output, while we identified the position bias and probability dispersion problems in the scores, and propose hybrid method to address them</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We show that although generic sequence-level scores are not well suited for selective generation (even negatively correlated with the the quality) for free-form generation, asking the model again to self-evaluate could reduce the sequence-level score to token-levels scores, improving quality calibration. Self-evaluation is though at the cost of increasing inference time by 1 or 2 (hybrid mode) times. Alternative to this post-hoc method, how to improve the quality calibration of the sequence-level score during training and finetuning is one of our future work.</p><p>In the following task, you will receive a question with a proposed answer. Please judge if the answer is factual, informative, unbiased, and safe.       </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Demonstration of our approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The prompt for True/False evaluation in TRUTHFULQA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: for True/False evaluation with other candidates in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The prompt for generating a critique in TRUTHFULQA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: The prompt for generating a revised answer given the critique in TRUTHFULQA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FigureFigure 12 :</head><label>12</label><figDesc>Figure The prompt sampling an answer in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: The prompt for pointwise evaluation with other candidates in TL;DR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>14 Dec 2023</figDesc><table><row><cell>question</cell><cell>LLM</cell><cell>answer answer answer</cell><cell>sequence likelihood score</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Hybrid</cell></row><row><cell>Sample and Select</cell><cell cols="2">Sample and Eval</cell><cell>What happens to you if you eat watermelon seeds?</cell></row><row><cell>What happens to you if you eat watermelon seeds?</cell><cell cols="2">What happens to you if you eat watermelon seeds?</cell><cell>Candidate answers: A) They will grow in your stomach. B) They will be digested and pass through you.</cell></row><row><cell>Candidate answers: A) They will grow in your stomach.</cell><cell cols="2">Answer: They will be digested and pass</cell><cell>C) You will get sick. D) You will die.</cell></row><row><cell>B) They will be digested and pass through you.</cell><cell>through you.</cell><cell></cell><cell>What happens to you if you eat watermelon seeds?</cell></row><row><cell>C) You will get sick.</cell><cell cols="2">Is the above answer correct?</cell><cell>Answer:</cell></row><row><cell>D) You will die.</cell><cell>A) Yes</cell><cell></cell><cell>They will be digested and pass through you.</cell></row><row><cell>E) None of the above</cell><cell>B) No</cell><cell></cell><cell>Is the above answer correct?</cell></row><row><cell>Which answer is correct? ?</cell><cell>Letter: ?</cell><cell></cell><cell>A) Yes B) No</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Letter: ?</cell></row><row><cell>P(?=A/B/C/D)</cell><cell cols="2">P(?=Yes/No)</cell><cell>P(?=Yes/No)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>. They also propose to include candidate answers in the prompt, p(Yes|x, y i , {y}).(Sample and Eval w/ other candidates)But that work focuses on the scaling law of the score's calibration, and did not compare it with sequence-level score and Sample and Select score.2.3 Combining the best of both worlds: select the answer via multi-choice evaluation and score the selected answer via pointwise evaluationSample and Select and Sample and Eval have their own pros and cons. In Sample and Select, although the un-normalized logit is better than softmax probability for calibration purpose, the logit score is still dependent on the other candidate answers. For fairly comparing across (x, y) pairs, a good score should measure the confidence to the (x, y) itself, not dependent on other candidate answers. Sample and Eval score p(Yes|y i , x) is indeed independent of other answers. On the other hand, Sample and Select provides the opportunity for comparing different answers and select the best. Therefore, we combine the best of both: We first use Sample and Select to select the best answer within a given question. The answer with the highest softmax probability score is selected, ? = y r , r = arg max i p(c i |x, {cy}). After selection, we discard the score because it is not good for cross question comparison. We score the selected answer via Sample and Eval p(Yes|x, ?).</figDesc><table /><note><p><p><p>p</p>(Yes|x, ?)</p>, where ? = y r , r = arg max i p(c i |x, {cy}).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Comparison of different scores for the accuracy and calibration metrics on TRUTHFULQA for PALM-2 LARGE and GPT-3 models. The numbers are in percentage.</figDesc><table><row><cell></cell><cell cols="3">Accuracy Calibration-AUC Selective-AUC</cell></row><row><cell>PALM-2 LARGE</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sequence likelihood</cell><cell>48.23</cell><cell>39.80</cell><cell>33.63</cell></row><row><cell>Len-norm sequence likelihood</cell><cell>52.75</cell><cell>50.09</cell><cell>42.15</cell></row><row><cell>Sample and Select</cell><cell>58.26</cell><cell>53.17</cell><cell>48.59</cell></row><row><cell>Sample and Select w/ nota</cell><cell>58.13</cell><cell>72.59</cell><cell>56.61</cell></row><row><cell>Sample and Eval</cell><cell>59.12</cell><cell>73.79</cell><cell>58.19</cell></row><row><cell>Sample and Eval w/ candidates</cell><cell>59.00</cell><cell>68.78</cell><cell>55.70</cell></row><row><cell>Hybrid</cell><cell>58.26</cell><cell>73.76</cell><cell>57.38</cell></row><row><cell>Hybrid w/ nota</cell><cell>58.14</cell><cell>75.34</cell><cell>58.10</cell></row><row><cell>GPT-3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sequence likelihood</cell><cell>67.19</cell><cell>40.50</cell><cell>49.76</cell></row><row><cell>Len-norm sequence likelihood</cell><cell>67.19</cell><cell>42.06</cell><cell>50.22</cell></row><row><cell>Sample and Select</cell><cell>72.24</cell><cell>47.97</cell><cell>56.75</cell></row><row><cell>Sample and Select w/ nota</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell>Sample and Eval</cell><cell>67.83</cell><cell>48.47</cell><cell>53.28</cell></row><row><cell>Sample and Eval w/ candidates</cell><cell>68.48</cell><cell>51.36</cell><cell>55.28</cell></row><row><cell>Hybrid</cell><cell>72.24</cell><cell>51.66</cell><cell>58.46</cell></row><row><cell>Hybrid w/ nota</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Self-critique and revise further improves the model's accuracy, calibration, and selective generation on TRUTHFULQA on PALM-2.</figDesc><table><row><cell></cell><cell cols="3">Accuracy Calibration-AUC Selective-AUC</cell></row><row><cell>Sequence likelihood</cell><cell>54.83</cell><cell>38.96</cell><cell>38.40</cell></row><row><cell>Len-norm sequence likelihood</cell><cell>59.12</cell><cell>49.64</cell><cell>47.03</cell></row><row><cell>Sample and Select</cell><cell>64.87</cell><cell>50.41</cell><cell>52.40</cell></row><row><cell>Sample and Select w/ nota</cell><cell>64.60</cell><cell>66.92</cell><cell>58.69</cell></row><row><cell>Sample and Eval</cell><cell>66.34</cell><cell>70.55</cell><cell>61.81</cell></row><row><cell>Sample and Eval w/ candidates</cell><cell>66.71</cell><cell>64.69</cell><cell>59.44</cell></row><row><cell>Hybrid</cell><cell>64.87</cell><cell>71.35</cell><cell>61.11</cell></row><row><cell>Hybrid w/ nota</cell><cell>64.50</cell><cell>72.72</cell><cell>61.44</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Effect of position bias on metrics. The results are based on PALM-2 LARGE.</figDesc><table><row><cell></cell><cell cols="3">Accuracy Calibration-AUC Selective-AUC</cell></row><row><cell>TRUTHFULQA</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sample and Select, vanilla</cell><cell>58.26</cell><cell>53.17</cell><cell>48.59</cell></row><row><cell>Sample and Select, de-biased</cell><cell>58.87</cell><cell>52.13</cell><cell>48.58</cell></row><row><cell>TL;DR</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sample and Select, vanilla</cell><cell>70.20</cell><cell>46.65</cell><cell>54.68</cell></row><row><cell>Sample and Select, de-biased</cell><cell>70.70</cell><cell>43.94</cell><cell>53.86</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>In the following task, you will receive a question and an answer. Please generate an explanation why this answer would be non-factual. Say None if you don't have any critique. Watermelon seeds can cause a watermelon to grow in your stomach.==This answer could be wrong because the stomach is an acidic environment that is not suitable for watermelon seeds to grow.</figDesc><table><row><cell>==</cell></row><row><cell>Question:</cell></row><row><cell>What happens to you if you eat watermelon seeds?</cell></row><row><cell>==</cell></row><row><cell>Answer:</cell></row><row><cell>==</cell></row><row><cell>Question:</cell></row><row><cell>{question}</cell></row><row><cell>==</cell></row><row><cell>Answer:</cell></row><row><cell>{answer}</cell></row><row><cell>==</cell></row><row><cell>This answer could be wrong because</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>For GPT-3 model, the API can only output log-probability for up to 5 most likely tokens. Because of this limitation, a few methods cannot be evaluated on GPT-3. For example, the most likely tokens in the multi-response evaluation setting are not necessarily A, B, C etc., but the most likely letter and its variants such as 'A', ' A', or 'A\n'. Therefore the maximum token prediction and its log-probability are always available, but the log-probability for a specific token such as 'E' for the "None of the above" answer is not available.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank <rs type="person">Denny Zhou</rs>, <rs type="person">Zelda Mariet</rs>, <rs type="person">Sharat Chikkerur</rs>, <rs type="person">Jasper Snoek</rs>, and <rs type="person">Alexander D'Amour</rs> from <rs type="funder">Google DeepMind</rs> for helpful discussions for insightful discussion and providing valuable feedback for this work. We would also like to express our appreciation towards <rs type="person">Lyric Doshi</rs>, <rs type="person">Xuezhi Wang</rs>, and <rs type="person">Michael W. Dusenberry</rs> from <rs type="affiliation">Google DeepMind</rs> for their technical support.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">1</ref><p>. The area under the ROC curve is calibration-AUC, and the area under the selective generation curve is selective-AUC.</p><p>4.3 Self-evaluation improves calibration on TL;DR summarization TL;DR is a summarization benchmark dataset mined from Reddit website <ref type="bibr" target="#b26">[V?lske et al., 2017]</ref>.</p><p>Evaluating the different scores on that dataset shows again that the sequence-level scores are not suitable for calibration. Self-evaluation based token-level scores improve the both accuracy and calibration performance (Table <ref type="table">3</ref>). Sample and Select has higher accuracy but lower calibration-AUC than Sample and Eval, and adding NONE OF THE ABOVE option helps to improve Calibration-AUC without sacrificing much the accuracy. Hybrid methods in general have decent performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effect of position bias</head><p>We assess the effect of position bias on the performance. We compare the vanilla setting where the answers are ordered by default, and the de-biased setting where the answer scores are averaged across all n! possible permutations. The difference on the performance is not that significant. Given</p><p>A Prompts used in the study   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Towards a human-like opendomain chatbot</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Adiwardana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Nemade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09977</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Can NLP models&apos; identify&apos;,&apos;distinguish&apos;, and&apos;justify&apos;questions that don</title>
		<author>
			<persName><forename type="first">Ayushi</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nisarg</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neeraj</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavan</forename><surname>Mallina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhavin</forename><surname>Aryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srihari</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tirth</forename><surname>Raju Sangaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nihar</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitta</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName><surname>Baral</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.04635</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandipan</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cameron</forename><surname>Mckinnon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.08073</idno>
		<title level="m">Constitutional AI: Harmlessness from AI feedback</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Universal self-consistency for large language model generation</title>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renat</forename><surname>Aksitov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kefan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sushant</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.17311</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Scaling instruction-finetuned language models</title>
		<author>
			<persName><forename type="first">Chung</forename><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Brahma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.11416</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><surname>Jeremy R Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Martin Eisenschlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><surname>Eisenstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14613</idno>
		<title level="m">Selectively answering ambiguous questions</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Scaling out-of-distribution detection for real-world settings</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammadreza</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11132</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Language models (mostly) know what they know</title>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Schiefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zac</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Tran-Johnson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.05221</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Measuring and improving modelmoderator collaboration using uncertainty estimation</title>
		<author>
			<persName><forename type="first">Zi</forename><surname>Ian D Kivlichan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremiah</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Vasserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.04212</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Lorenz</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Farquhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.09664</idno>
		<title level="m">Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Owain</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><surname>Truthfulqa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.07958</idno>
		<title level="m">Measuring how models mimic human falsehoods</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Owain</forename><surname>Evans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.14334</idno>
		<title level="m">Teaching models to express their uncertainty in words</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><surname>Brio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.16804</idno>
		<title level="m">Bringing order to abstractive summarization</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Collaborative storytelling with large-scale neural language models</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randy</forename><surname>Gomez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<idno>arXiv</idno>
		<title level="m">OpenAI. GPT-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2303" to="08774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2203.02155" />
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Direct preference optimization: Your language model is secretly a reward model</title>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Rafailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Archit</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v21/20-074.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Anushri</forename><surname>Allen Z Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Dixit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumeet</forename><surname>Bodrova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leila</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Takayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><surname>Varley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.01928</idno>
		<title level="m">Robots that ask for help: Uncertainty alignment for large language model planners</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Out-of-distribution detection and selective generation for conditional language models</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kundan</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Leveraging large language models for multiple choice question answering</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Michael Rytting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wingate</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.12353</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to summarize with human feedback</title>
		<author>
			<persName><forename type="first">Nisan</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">F</forename><surname>Christiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3008" to="3021" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback</title>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Archit</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Rafailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaxiu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14975</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tl; dr: Mining reddit to learn automatic summarization</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>V?lske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahbaz</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on New Frontiers in Summarization</title>
		<meeting>the Workshop on New Frontiers in Summarization</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="59" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Self-consistency improves chain of thought reasoning in language models</title>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.11171</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01652</idno>
		<title level="m">Finetuned language models are zero-shot learners</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Can LLMs express their uncertainty? an empirical evaluation of confidence elicitation in LLMs</title>
		<author>
			<persName><forename type="first">Miao</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.13063</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">On uncertainty calibration and selective generation in probabilistic neural summarization: A benchmark study</title>
		<author>
			<persName><forename type="first">Polina</forename><surname>Zablotskaia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Du</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremiah</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.08653</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Misha</forename><surname>Khalman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.10425</idno>
		<title level="m">Slic-hf: Sequence likelihood calibration with human feedback</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">On large language models&apos; selection bias in multi-choice questions</title>
		<author>
			<persName><forename type="first">Chujie</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.03882</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
