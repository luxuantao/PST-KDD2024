<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Zeroth-Order Stochastic Variance Reduction for Nonconvex Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sijia</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MIT-IBM Watson AI Lab</orgName>
								<address>
									<country>IBM Research</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bhavya</forename><surname>Kailkhura</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Lawrence Livermore National Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MIT-IBM Watson AI Lab</orgName>
								<address>
									<country>IBM Research</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paishun</forename><surname>Ting</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MIT-IBM Watson AI Lab</orgName>
								<address>
									<country>IBM Research</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lisa</forename><surname>Amini</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MIT-IBM Watson AI Lab</orgName>
								<address>
									<country>IBM Research</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Zeroth-Order Stochastic Variance Reduction for Nonconvex Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5A2811069A3E18FEC85ED39FF973D2AF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As application demands for zeroth-order (gradient-free) optimization accelerate, the need for variance reduced and faster converging approaches is also intensifying. This paper addresses these challenges by presenting: a) a comprehensive theoretical analysis of variance reduced zeroth-order (ZO) optimization, b) a novel variance reduced ZO algorithm, called ZO-SVRG, and c) an experimental evaluation of our approach in the context of two compelling applications, black-box chemical material classification and generation of adversarial examples from black-box deep neural network models. Our theoretical analysis uncovers an essential difficulty in the analysis of ZO-SVRG: the unbiased assumption on gradient estimates no longer holds. We prove that compared to its first-order counterpart, ZO-SVRG with a two-point random gradient estimator could suffer an additional error of order O(1/b), where b is the mini-batch size. To mitigate this error, we propose two accelerated versions of ZO-SVRG utilizing variance reduced gradient estimators, which achieve the best rate known for ZO stochastic optimization (in terms of iterations). Our extensive experimental results show that our approaches outperform other state-of-the-art ZO algorithms, and strike a balance between the convergence rate and the function query complexity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Zeroth-order (gradient-free) optimization is increasingly embraced for solving machine learning problems where explicit expressions of the gradients are difficult or infeasible to obtain. Recent examples have shown zeroth-order (ZO) based generation of prediction-evasive, black-box adversarial attacks on deep neural networks (DNNs) as effective as state-of-the-art white-box attacks, despite leveraging only the inputs and outputs of the targeted DNN <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. Additional classes of applications include network control and management with time-varying constraints and limited computation capacity <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, and parameter inference of black-box systems <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. ZO algorithms achieve gradientfree optimization by approximating the full gradient via gradient estimators based on only the function values <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>Although many ZO algorithms have recently been developed and analyzed <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>, they often suffer from the high variances of ZO gradient estimates, and in turn, hampered convergence rates. In addition, these algorithms are mainly designed for convex settings, which limits their applicability in a wide range of (non-convex) machine learning problems.</p><p>In this paper, we study the problem of design and analysis of variance reduced and faster converging nonconvex ZO optimization methods. To reduce the variance of ZO gradient estimates, one can draw motivations from similar ideas in the first-order regime. The stochastic variance reduced gradient (SVRG) is a commonly-used, effective first-order approach to reduce the variance <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref>. Due to the variance reduction, it improves the convergence rate of stochastic gradient descent (SGD) from O(1/ √ T ) <ref type="foot" target="#foot_1">1</ref> to O(1/T ), where T is the total number of iterations.</p><p>Although SVRG has shown a great promise, applying similar ideas to ZO optimization is not a trivial task. The main challenge arises due to the fact that SVRG relies upon the assumption that a stochastic gradient is an unbiased estimate of the true batch/full gradient, which unfortunately does not hold in the ZO case. Therefore, it is an open question whether the ZO stochastic variance reduced gradient could enable faster convergence of ZO algorithms. In this paper, we attempt to fill the gap between ZO optimization and SVRG.</p><p>Contributions We propose and evaluate a novel ZO algorithm for nonconvex stochastic optimization, ZO-SVRG, which integrates SVRG with ZO gradient estimators. We show that compared to SVRG, ZO-SVRG achieves a similar convergence rate that decays linearly with O(1/T ) but up to an additional error correction term of order 1/b, where b is the mini-batch size. We show that this correction term will be eliminated as the full batch of data is used, corresponding to b = n where n is the number of data samples. In this scenario, ZO-SVRG would reduce to ZO gradient descent (ZO-GD) <ref type="bibr" target="#b12">[13]</ref>. However, without a careful treatment, this correction term (e.g., when b is small) could be a critical factor affecting the optimization performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>In ZO algorithms, a full gradient is typically approximated using either a one-point or a two-point gradient estimator, where the former acquires a gradient estimate ∇f (x) by querying f (•) at a single random location close to x <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, and the latter computes a finite difference using two random function queries <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. In this paper, we focus on the two-point gradient estimator since it has a lower variance and thus improves the complexity bounds of ZO algorithms.</p><p>Despite the meteoric rise of two-point based ZO algorithms, most of the work is restricted to convex problems <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>. For example, a ZO mirror descent algorithm proposed by <ref type="bibr" target="#b13">[14]</ref> has an exact rate</p><formula xml:id="formula_0">O( √ d/ √ T )</formula><p>, where d is the number of optimization variables. The same rate is obtained by bandit convex optimization <ref type="bibr" target="#b14">[15]</ref> and ZO online alternating direction method of multipliers <ref type="bibr" target="#b4">[5]</ref>. Current studies suggested that ZO algorithms typically agree with the iteration complexity of first-order algorithms up to a small-degree polynomial of the problem size d.</p><p>In contrast to the convex setting, non-convex ZO algorithms are comparatively under-studied except a few recent attempts <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref>. Different from convex optimization, the stationary condition is used to measure the convergence of nonconvex methods. In <ref type="bibr" target="#b12">[13]</ref>, the ZO gradient descent (ZO-GD) algorithm was proposed for deterministic nonconvex programming, which yields O(d/T ) convergence rate. A stochastic version of ZO-GD (namely, ZO-SGD) studied in <ref type="bibr" target="#b23">[24]</ref> achieves the rate of O(</p><formula xml:id="formula_1">√ d/ √ T ).</formula><p>In <ref type="bibr" target="#b24">[25]</ref>, a ZO distributed algorithm was developed for multi-agent optimization, leading to O(1/T + d/q) convergence rate. Here q is the number of random directions used to construct a gradient estimate. In <ref type="bibr" target="#b6">[7]</ref>, an asynchronous ZO stochastic coordinate descent (ZO-SCD) was derived for parallel optimization and achieved the rate of O(</p><formula xml:id="formula_2">√ d/ √ T ).</formula><p>In <ref type="bibr" target="#b25">[26]</ref>, a variant of ZO-SCD, known as ZO stochastic variance reduced coordinate (ZO-SVRC) descent, improved the convergence rate from O(</p><formula xml:id="formula_3">√ d/ √ T ) to O(d/T</formula><p>) under the same parameter setting for the gradient estimation. Although the authors in <ref type="bibr" target="#b25">[26]</ref> considered the stochastic variance reduced technique, only a coordinate descent algorithm using a coordinate-wise (deterministic) gradient estimator was studied. This motivates our study on a more general framework ZO-SVRG under different gradient estimators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>Consider a nonconvex finite-sum problem of the form minimize</p><formula xml:id="formula_4">x∈R d f (x) := 1 n n i=1 fi(x),<label>(1)</label></formula><p>where {f i (x)} n i=1 are n individual nonconvex cost functions. The generic form (1) encompasses many machine learning problems, ranging from generalized linear models to neural networks. We next elaborate on assumptions of problem <ref type="bibr" target="#b0">(1)</ref>, and provide a background on ZO gradient estimators.</p><p>3.1 Assumptions A1: Functions {f i } have L-Lipschitz continuous gradients (L-smooth), i.e., ∇f i (x) -∇f i (y) 2 ≤ L xy 2 for any x and y, i ∈ [n], and some L &lt; ∞. Here • 2 denotes the Euclidean norm, and for ease of notation [n] represents the integer set {1, 2, . . . , n}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2:</head><p>The variance of stochastic gradients is bounded as</p><formula xml:id="formula_5">1 n n i=1 ∇f i (x) -∇f (x) 2 2 ≤ σ 2 .</formula><p>Here ∇f i (x) can be viewed as a stochastic gradient of ∇f (x) by randomly picking an index i ∈ [n].</p><p>Both A1 and A2 are the standard assumptions used in nonconvex optimization literature <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref>. Note that A2 is milder than the assumption of bounded gradients <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25]</ref>. For example, if ∇f i (x) 2 ≤ σ, then A2 is satisfied with σ = 2σ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ZO gradient estimation</head><p>Given an individual cost function f i (or an arbitrary function under A1 and A2), a two-point random gradient estimator ∇f i (x) is defined by <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16]</ref> </p><formula xml:id="formula_6">∇fi(x) = (d/µ) [fi(x + µui) -fi(x)] ui, for i ∈ [n],<label>(RandGradEst)</label></formula><p>where recall that d is the number of optimization variables, µ &gt; 0 is a smoothing parameter<ref type="foot" target="#foot_2">2</ref> , and {u i } are i.i.d. random directions drawn from a uniform distribution over a unit sphere <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>In general, RandGradEst is a biased approximation to the true gradient ∇f i (x), and its bias reduces as µ approaches zero. However, in a practical system, if µ is too small, then the function difference could be dominated by the system noise and fails to represent the function differential <ref type="bibr" target="#b6">[7]</ref>. For µ &gt; 0, although the ZO gradient estimate introduces bias to the true gradient, it remains unbiased to the gradient of a so-called randomized smoothing function with parameter µ; see Lemma 1 of Appendix A.1.</p><p>Remark 1 Instead of using a single sample u i in RandGradEst, the average of q i.i.d. samples {u i,j } q j=1 can also be used for gradient estimation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25]</ref>,</p><formula xml:id="formula_7">∇fi(x) = (d/(µq)) q j=1 [fi(x + µui,j) -fi(x)] ui,j, for i ∈ [n], (Avg-RandGradEst)</formula><p>which we call an average random gradient estimator.</p><p>In addition to RandGradEst and Avg-RandGradEst, the work <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> considered a coordinate-wise gradient estimator. Here every partial derivative is estimated via the two-point querying scheme under fixed direction vectors,</p><formula xml:id="formula_8">∇fi(x) = d =1 (1/(2µ )) [fi(x + µ e ) -fi(x -µ e )] e l , for i ∈ [n],<label>(CoordGradEst)</label></formula><p>where µ &gt; 0 is a coordinate-wise smoothing parameter, and e ∈ R d is a standard basis vector with 1 at its th coordinate, and 0s elsewhere. Compared to RandGradEst, CoordGradEst is deterministic and requires d times more function queries. However, as will be evident later, it yields an improved iteration complexity (i.e., convergence rate). More details on ZO gradient estimation can be found in Appendix A.1. compute gradient blending via (2):</p><formula xml:id="formula_9">v s k = ∇f I k (x s k ) -∇f I k (x s 0 ) + g s , 7: update x s k+1 = x s k -η k v s k , 8:</formula><p>end for 9:</p><p>set xs = x s m , 10: end for 11: return x chosen uniformly random from</p><formula xml:id="formula_10">{{x s k } m-1 k=0 } S s=1 .</formula><p>Algorithm 2: ZO-SVRG(T, m, {η k }, b, x0 , µ)</p><p>1: Input: In addition to parameters in SVRG, set smoothing parameter µ &gt; 0. 2: for s = 1, 2, . . . , S do compute ZO gradient blending (3):</p><formula xml:id="formula_11">vs k = ∇f I k (x s k ) -∇f I k (x s 0 ) + ĝs , 8: update x s k+1 = x s k -η k vs k , 9:</formula><p>end for 10:</p><p>set xs = x s m , 11: end for 12: return x chosen uniformly random from</p><formula xml:id="formula_12">{{x s k } m-1 k=0 } S s=1 .</formula><p>4 ZO stochastic variance reduced gradient (ZO-SVRG)</p><p>4.1 SVRG: from first-order to zeroth-order</p><p>It has been shown in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> that the first-order SVRG achieves the convergence rate O(1/T ), yielding O( √ T ) less iterations than the ordinary SGD for solving finite sum problems. The key step of SVRG <ref type="foot" target="#foot_3">3</ref> (Algorithm 1) is to generate an auxiliary sequence x at which the full gradient is used as a reference in building a modified stochastic gradient estimate</p><formula xml:id="formula_13">ĝ = ∇fI(x) -(∇fI(x) -∇f (x)), ∇fI(x) = (1/b) i∈I ∇fi(x)<label>(2)</label></formula><p>where ĝ denotes the gradient estimate at x, I ⊆ [n] is a mini-batch of size b (chosen uniformly randomly <ref type="foot" target="#foot_4">4</ref> ), and ∇f (x) = ∇f [n] (x). The key property of ( <ref type="formula" target="#formula_13">2</ref>) is that ĝ is an unbiased gradient estimate of ∇f (x). The gradient blending (2) is also motivated by a variance reduced technique known as control variate <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref>. The link between SVRG and control variate is discussed in Appendix A.2.</p><p>In the ZO setting, the gradient blending (2) is approximated using only function values,</p><formula xml:id="formula_14">ĝ = ∇fI(x) -( ∇fI(x) -∇f (x)), ∇fI(x) = (1/b) i∈I ∇fi(x),<label>(3)</label></formula><p>where ∇f (x) = ∇f [n] (x), and ∇f i is a ZO gradient estimate specified by RandGradEst, Avg-RandGradEst or CoordGradEst. Replacing (2) with (3) in SVRG (Algorithm 1) leads to a new ZO algorithm, which we call ZO-SVRG (Algorithm 2). We highlight that although ZO-SVRG is similar to SVRG except the use of ZO gradient estimators to estimate batch, mini-batch, as well as blended gradients, this seemingly minor difference yields an essential difficulty in the analysis of ZO-SVRG. That is, the unbiased assumption on gradient estimates used in SVRG no longer holds. Thus, a careful analysis of ZO-SVRG is much needed to ensure its optimization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Convergence analysis</head><p>In what follows, we focus on the analysis of ZO-SVRG using RandGradEst. Later, we will study ZO-SVRG with Avg-RandGradEst and CoordGradEst. We start by investigating the second-order moment of the blended ZO gradient estimate vs k in the form of (3); see Proposition 1. Proposition 1 Suppose A2 holds and RandGradEst is used in Algorithm 2. The blended ZO gradient estimate vs k in Step 7 of Algorithm 2 satisfies</p><formula xml:id="formula_15">E[ vs k 2 2 ]≤ 4(b + 18δn)d b E ∇f (x s k ) 2 2 + 6(4d + 1)L 2 δn b E x s k -x s 0 2 2 + (6δn + b)L 2 d 2 µ 2 b + 72dσ 2 δn b ,<label>(4)</label></formula><p>where δ n = 1 if the mini-batch contains i.i.d. samples from [n] with replacement, and δ n = I(b &lt; n) if samples are randomly selected without replacement. Here</p><formula xml:id="formula_16">I(b &lt; n) is 1 if b &lt; n, and 0 if b = n. Proof: See Appendix A.3.</formula><p>Compared to SVRG and its variants <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23]</ref>, the error bound (4) involves a new error term O(dσ 2 /b) for b &lt; n, which is induced by the second-order moment of RandGradEst (Appendix A.1). With the aid of Proposition 1, Theorem 1 provides the convergence rate of ZO-SVRG in terms of an upper bound on E[ ∇f (x) 2 ] at the solution x.</p><p>Theorem 1 Suppose A1 and A2 hold, and the random gradient estimator (RandGradEst) is used. The output x of Algorithm 2 satisfies</p><formula xml:id="formula_17">E ∇f (x) 2 2 ≤ f (x0) -f * T γ + Lµ 2 T γ + Sχm T γ ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_18">T = Sm, f * = min x f (x), γ = min k∈[m] γ k , χ m = m-1</formula><p>k=0 χ k , and</p><formula xml:id="formula_19">γ k = 1 2 1 -c k+1 β k η k -L 2 + c k+1 4db+72dδn b η 2 k (<label>6</label></formula><formula xml:id="formula_20">)</formula><formula xml:id="formula_21">χ k = 1 -c k+1 β k µ 2 d 2 L 2 4 η k + L 2 + c k+1 (6δn+b)L 2 d 2 µ 2 +72dσ 2 δn b η 2 k .<label>(7)</label></formula><p>In ( <ref type="formula" target="#formula_19">6</ref>)-( <ref type="formula" target="#formula_21">7</ref>), β k is a positive parameter ensuring γ k &gt; 0, and the coefficients {c k } are given by</p><formula xml:id="formula_22">c k = 1 + β k η k + 6(4d + 1)L 2 δnη 2 k b c k+1 + 3(4d + 1)L 3 δnη 2 k b , cm = 0.<label>(8)</label></formula><p>Proof: See Appendix A.4.</p><p>Compared to the convergence rate of SVRG as given in [20, Theorem 2], Theorem 1 exhibits two additional errors (Lµ 2 /(T γ)) and (Sχ m /(T γ)) due to the use of ZO gradient estimates. Roughly speaking, if we choose the smoothing parameter µ reasonably small, then the error (Lµ 2 /(T γ)) would reduce, leading to non-dominant effect on the convergence rate of ZO-SVRG. For the term (Sχ m /(T γ)), the quantity χ m is more involved, relying on the epoch length m, the step size η k , the smoothing parameter µ, the mini-batch size b, and the number of optimization variables d. In order to acquire explicit dependence on these parameters and to explore deeper insights of convergence, we simplify (5) for a specific parameter setting, as formalized below.</p><p>Corollary 1 Suppose we set</p><formula xml:id="formula_23">µ = 1 √ dT , η k = η = ρ Ld ,<label>(9)</label></formula><p>β k = β = L, and m = d 31ρ , where 0 &lt; ρ ≤ 1 is a universal constant that is independent of b, d, L, and T . Then Theorem 1 implies</p><formula xml:id="formula_24">f (x0)-f * T γ ≤ O d T , Lµ 2 T γ ≤ O 1 T 2 , and Sχm T γ ≤ O d T + δn b , which yields E ∇f (x) 2 2 ≤ O d T + δn b .<label>(10)</label></formula><p>Proof: See Appendix A.5.</p><p>It is worth mentioning that the condition on the value of smoothing parameter µ in Corollary 1 is less restrictive than several ZO algorithms <ref type="foot" target="#foot_5">5</ref> . For example, ZO-SGD in <ref type="bibr" target="#b23">[24]</ref> required µ ≤ O(d -1 T -1/2 ), and ZO-ADMM <ref type="bibr" target="#b4">[5]</ref> and ZO-mirror descent <ref type="bibr" target="#b13">[14]</ref> considered µ t = O(d -1.5 t -1 ). Moreover, similar to <ref type="bibr" target="#b4">[5]</ref>, we set the step size η linearly scaled with 1/d. Compared to the aforementioned ZO algorithms <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24]</ref>, the convergence performance of ZO-SVRG in <ref type="bibr" target="#b9">(10)</ref> has an improved (linear rather than sub-linear) dependence on 1/T . However, it suffers an additional error of order O(δ n /b) inherited from (Sχ m /(T γ)) in <ref type="bibr" target="#b4">(5)</ref>, which is also a consequence of the last error term in (4). We recall from the definition of δ n in Proposition 1 that if b &lt; n or samples in the mini-batch are chosen independently from [n], then δ n = 1. However, the error term is eliminated when I k = [n] (corresponding to δ n = 0). In this case, ZO-SVRG (Algorithm 2) reduces to ZO-GD in <ref type="bibr" target="#b12">[13]</ref> since Step 7 of Algorithm 2 becomes vs k = ∇f (x s k ). A recent work <ref type="bibr">[25,</ref> Theorem 1] also identified the possible side effect O(1/b) for b &lt; n in the context of ZO nonconvex multi-agent optimization using a method of multipliers. Note that a large mini-batch reduces the variance of RandGradEst and improves the convergence of ZO optimization methods. Although the tightness of the error bound <ref type="bibr" target="#b9">(10)</ref> is not proven, we conjecture that the dependence on T and b could be optimal, since the form is consistent with SVRG, and the latter does not rely on the selected parameters in <ref type="bibr" target="#b8">(9)</ref>.</p><p>Lastly, we highlight that the theoretical analysis of ZO-SVRG is different from ZO-SVRC <ref type="bibr" target="#b25">[26]</ref>.</p><p>For the latter, the coordinate-wise (deterministic) gradient estimate is used and hence maintains Lipschitz continuity, which does not hold for a random gradient estimate. As a result, it becomes nontrivial to bound the distance of two random gradient estimates; see Appendix A.3. Moreover, reference <ref type="bibr" target="#b25">[26]</ref> does not fully uncover the effect of dimension dependency on the convergence of ZO-SVRC. However, we clearly analyze this effect for ZO-SVRG in Corollary 1. Furthermore, our convergence analysis is performed under milder assumptions, while ZO-SVRC requires extra assumptions on gradients of coordinate-wise smoothing functions. In Sec. 6, we will compare the empirical performance of ZO-SVRC with our method through two real-life applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acceleration of ZO-SVRG: Towards improved iteration complexity</head><p>In this section, we improve the iteration complexity of ZO-SVRG (Algorithm 2) by using Avg-RandGradEst and CoordGradEst, respectively. We start by comparing the squared errors of different gradient estimates to the true gradient ∇f , as formalized in Proposition 2.</p><p>Proposition 2 Consider a gradient estimator ∇f (x) = ∇f (x) + ω, then the squared error</p><formula xml:id="formula_25">E[ ω 2 2 ]        E ω 2 2 ≤ O (d) ∇f (x) 2 2 + O µ 2 L 2 d 2 for RandGradEst, E ω 2 2 ≤ O q+d q ∇f (x) 2 2 + O µ 2 L 2 d 2 for Avg-RandGradEst, ω 2 2 ≤ O L 2 d d =1 µ 2 for CoordGradEst.<label>(11)</label></formula><p>Proof: See Appendix A.6.</p><p>Proposition 2 shows that compared to CoordGradEst, RandGradEst and Avg-RandGradEst involve an additional error term within a factor O(d) and O((q + d)/q) of ∇f (x) 2 2 , respectively. Such an error is introduced by the second-order moment of gradient estimators using random direction samples <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, and it decreases as the number of direction samples q increases. On the other hand, all gradient estimators have a common error bounded by O(µ 2 L 2 d 2 ), where let µ = µ for ∈ [d] in CoordGradEst. If µ is specified as in (9), then we obtain the error term O(d/T ), consistent with the convergence rate of ZO-SVRG in Corollary 1.</p><p>In Theorem 2, we show the effect of Avg-RandGradEst on the convergence rate of ZO-SVRG.</p><p>Theorem 2 Suppose A1 and A2 hold, and Avg-RandGradEst is used in Algorithm 2. Then E ∇f (x) 2  2 is bounded in the same way as given in <ref type="bibr" target="#b4">(5)</ref>, where the parameters γ k , χ k and c k for k ∈ [m] are modified by</p><formula xml:id="formula_26">γ k = 1 2 1 -c k+1 β k η k -L 2 + c k+1 (72δn+4b)(q+d) bq η 2 k , χ k = 1 -c k+1 β k µ 2 d 2 L 2 4 η k + L 2 + c k+1 (6δn+b)(q+1)L 2 d 2 µ 2 +72(q+d)σ 2 δn bq η 2 k , c k = 1 + β k η k + 6(4d+5q)L 2 δn bq η 2 k c k+1 + 3(4d+5q)L 3 δn bq η 2 k , with c m = 0.</formula><p>Given the setting in Corollary 1 and m = d 55ρ , the convergence rate simplifies to</p><formula xml:id="formula_27">E ∇f (x) 2 2 ≤ O d T + δn b min{d, q} . (<label>12</label></formula><formula xml:id="formula_28">)</formula><p>Proof: See Appendix A.7</p><p>By contrast with Corollary 1, it can be seen from ( <ref type="formula" target="#formula_27">12</ref>) that the use of Avg-RandGradEst reduces the error O(δ n /b) in <ref type="bibr" target="#b9">(10)</ref> through multiple (q) direction samples. If T db ≤ q ≤ d, then the convergence error under Ave-RandGradEst will be dominated by O(d/T ). Our empirical results show that a moderate choice of q can significantly speed up the convergence of ZO-SVRG.</p><p>We next study the effect of the coordinate-wise gradient estimator (CoordGradEst) on the convergence rate of ZO-SVRG, as formalized in Theorem 3.</p><p>Theorem 3 Suppose A1 and A2 hold, and CoordGradEst with µ = µ is used in Algorithm 2. Then</p><formula xml:id="formula_29">E ∇f (x) 2 2 ≤ f (x0) -f * T γ + Sχm T γ , (<label>13</label></formula><formula xml:id="formula_30">)</formula><p>where T , f * , γ and χ m were defined in (5), the parameters γ k , χ k and c k for k ∈ [m] are given by</p><formula xml:id="formula_31">γ k = 1 2 1 -c k+1 β k η k -4 L 2 + c k+1 η 2 k , χ k = 1 4 + c k+1 β k L 2 µ 2 d 2 2 η k + L 2 + c k+1 µ 2 L 2 d 2 η 2 k , c k = 1 + β k η k + 2dL 2 δnη 2 k b c k+1 + dL 3 δnη 2 k b</formula><p>, with c m = 0, and β k is a positive parameter ensuring γ k &gt; 0. Given the specific setting in Corollary 1 and m = d 3ρ , the convergence rate simplifies to</p><formula xml:id="formula_32">E ∇f (x) 2 2 ≤ O d T .<label>(14)</label></formula><p>Proof: See Appendix A.8.</p><p>Theorem 3 shows that the use of CoordGradEst improves the iteration complexity, where the error of order O(1/b) in Corollary 1 or O(1/(b min{d, q})) in Theorem 2 has been eliminated in <ref type="bibr" target="#b13">(14)</ref>. This improvement is benefited from the low variance of CoordGradEst shown by Proposition 2. We can also see this benefit by comparing χ k in Theorem 3 with (7): the former avoids the term (dσ 2 /b).</p><p>The disadvantage of CoordGradEst is the need of d times more function queries than RandGradEst in gradient estimation.</p><p>Recall that RandGradEst, Avg-RandGradEst and CoordGradEst require O(1), O(q) and O(d) function queries, respectively. In ZO-SVRG (Algorithm 2), the total number of gradient evaluations is given by nS + bT , where T = mS. Therefore, by fixing the number of iterations T , the function query complexity of ZO-SVRG using the studied estimators is then given by O(nS + bT ), O(q(nS + bT )) and O(d(nS + bT )), respectively. In Table <ref type="table" target="#tab_1">1</ref>, we summarize the convergence rates and the function query complexities of ZO-SVRG and its two variants, which we call ZO-SVRG-Ave and ZO-SVRG-Coord, respectively. For comparison, we also present the results of ZO-SGD <ref type="bibr" target="#b23">[24]</ref> and ZO-SVRC <ref type="bibr" target="#b25">[26]</ref>, where the later updates J coordinates per iteration within an epoch. Table <ref type="table" target="#tab_1">1</ref> shows that ZO-SGD has the lowest query complexity but has the worst convergence rate. ZO-SVRG-coord yields the best convergence rate in the cost of high query complexity. By contrast, ZO-SVRG (with an appropriate mini-batch size) and ZO-SVRG-Ave could achieve better trade-offs between the convergence rate and the query complexity. </p><formula xml:id="formula_33">&lt; n) Query complexity ZO-SVRG (RandGradEst) O 1 d O d T + 1 b O (nS + bT ) ZO-SVRG-Ave (Avg-RandGradEst) O( 1 d ) O d T + 1 b min{d,q} O (qnS + qbT ) ZO-SVRG-Coord (CoordGradEst) O( 1 d ) O( d T ) O(dnS + dbT ) ZO-SGD [24] (RandGradEst) O min{ 1 d , 1 √ dT } O √ d √ T O(bT ) ZO-SVRC [26] (CoordGradEst) O 1 n α , α ∈ (0, 1) O d T O (dnS + JbT )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Applications and experiments</head><p>We evaluate the performance of our proposed algorithms on two applications: black-box classification and generating adversarial examples from black-box DNNs. The first application is motivated by a real-world material science problem, where a material is classified to either be a conductor or an insulator from a density function theory (DFT) based black-box simulator <ref type="bibr" target="#b30">[31]</ref>. The second application arises in testing the robustness of a deployed DNN via iterative model queries <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>. Since ZO-SVRG belongs to the class of ZO counterparts of first-order algorithms using random/deterministic gradient estimation, we compare it with ZO-SGD and ZO-SVRC, the most relevant methods to ZO-SVRG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Black-box binary classification</head><p>We consider a non-linear least square problem [32, Sec. 3.2], i.e., problem (1) with f i (x) = (y i -φ(x; a i ))</p><formula xml:id="formula_34">2 for i ∈ [n].</formula><p>Here (a i , y i ) is the ith data sample containing feature vector a i ∈ R d and label y i ∈ {0, 1}, and φ(x; a i ) is a black-box function that only returns the function value given an input. The used dataset consists of N = 1000 crystalline materials/compounds extracted from Open Quantum Materials Database <ref type="bibr" target="#b32">[33]</ref>. Each compound has d = 145 chemical features, and its label (0 is conductor and 1 is insulator) is determined by a DFT simulator <ref type="bibr" target="#b33">[34]</ref>. Due to the black-box nature of DFT, the true φ is unknown <ref type="foot" target="#foot_6">6</ref> . We split the dataset into two equal parts, leading to n = 500 training samples and (N -n) testing samples. We refer readers to Appendix A.10 for more details on our dataset and the setting of experiments.   In Fig. <ref type="figure" target="#fig_3">2</ref>, we present the training loss against the number of epochs (i.e., iterations divided by the epoch length m = 50) and function queries. We compare our proposed algorithms ZO-SVRG, ZO-SVRG-Coord and ZO-SVRG-Ave with ZO-SGD <ref type="bibr" target="#b23">[24]</ref> and ZO-SVRC <ref type="bibr" target="#b25">[26]</ref>.  <ref type="table" target="#tab_1">1</ref>), leading to a non-dominant factor versus O(d/T ) in the convergence rate of ZO-SVRG-Ave. Fig. <ref type="figure" target="#fig_4">2-(b</ref>) presents the training loss against the number of function queries. For the same experiment, Table <ref type="table" target="#tab_2">2</ref> shows the number of iterations and the testing error of algorithms studied in Fig. <ref type="figure" target="#fig_3">2</ref>-(b) using 7.3 × 10 6 function queries. We observe that the performance of CoordGradEst based algorithms (i.e., ZO-SVRC and ZO-SVRG-Coord) degrade due to the need of large number of function queries to construct coordinate-wise gradient estimates. By contrast, algorithms based on random gradient estimators (i.e., ZO-SGD, ZO-SVRG and ZO-SVRG-Ave) yield better both training and testing results, while ZO-SGD consumes an extremely large number of iterations (14600 epochs). As a result, ZO-SVRG (b = 40) and ZO-SVRG-Ave achieve better tradeoffs between the iteration and the function query complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generation of adversarial examples from black-box DNNs 7</head><p>In image classification, adversarial examples refer to carefully crafted perturbations such that, when added to the natural images, are visually imperceptible but will lead the target model to misclassify. In the setting of 'zeroth order' attacks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b34">35]</ref>, the model parameters are hidden and acquiring its gradient is inadmissible. Only In Fig. <ref type="figure" target="#fig_1">3</ref>, we show the black-box attack loss (against the number of epochs) as well as the least 2 distortion of the successful (universal) adversarial perturbations. We observe that compared to ZO-SGD, ZO-SVRG-Ave offers a faster iteration convergence to a more accurate solution, and its convergence trajectory is more stable as q becomes larger (due to the reduced variance of Avg-RandGradEst). Note that the sharp drop of attack loss in each method is caused by the hinge-like loss as part of the total loss function, which turns to 0 only if the attack becomes successful. In addition, ZO-SVRG-Ave improves the 2 distortion of adversarial examples compared to ZO-SGD (e.g., 30% improvement when q = 30). We present the corresponding adversarial examples in Appendix A.11.</p><p>In contrast with the iteration complexity, ZO-SVRG-Ave requires roughly 30× (q = 10), 77× (q = 20) and 380× (q = 30) more function evaluations than ZO-SGD to reach a neighborhood of the smallest attack loss (e.g., 7 in our example). Furthermore, we present the black-box attack loss versus the number of query counts in Fig. <ref type="figure">A1</ref> (Appendix A.11). As we can see, ZO-SVRG-Ave requires more queries than ZO-SGD to achieve the first significant drop in attack loss. However, by fixing the total number of queries (10 7 ), ZO-SVRG-Ave eventually converges to a lower loss than ZO-SGD: the former reaches the average loss 4.81 with std 0.32 (computed from the last 100 attack losses), while the latter reaches 6.74 ± 0.46.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we studied ZO-SVRG, a new ZO nonconvex optimization method. We presented new convergence results beyond the existing work on ZO nonconvex optimization. We show that ZO-SVRG improves the convergence rate of ZO-SGD from O(1/ √ T ) to O(1/T ) but suffers a new correction term of order O(1/b). The is the side effect of combining a two-point random gradient estimators with SVRG. We then propose two accelerated variants of ZO-SVRG based on improved gradient estimators of reduced variances. We show an illuminating trade-off between the iteration and the function query complexity. Experimental results and theoretical analysis validate the effectiveness of our approaches compared to other state-of-the-art algorithms. In the future, we will compare ZO-SVRG with other derivative-free (non-gradient estimation based) methods for solving black-box optimization problems. It will also be interesting to study the problem of ZO distributed optimization, e.g., using CoordGradEst under a block coordinate descent framework <ref type="bibr" target="#b35">[36]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 : 1 : 4 :</head><label>114</label><figDesc>SVRG(T, m, {η k }, b, x0 ) Input: total number of iterations T , epoch length m, number of epochs S = T /m , step sizes {η k } m-1 k=0 , mini-batch b, and x0 . 2: for s = 1, 2, . . . , S do 3: set g s = ∇f (x s-1 ), x s 0 = xs-1 , for k = 0, 1, . . . , m -1 do 5: choose mini-batch I k of size b, 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3 :</head><label>3</label><figDesc>compute ZO estimate ĝs = ∇f (x s-1 ), for k = 0, 1, . . . , m -1 do 6: choose mini-batch I k of size b, 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Training loss versus iterations (b) Training loss versus function queries</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison of different ZO algorithms for the task of chemical material classification.</figDesc><graphic coords="8,119.11,179.11,182.16,136.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 -</head><label>2</label><figDesc>(a) presents the convergence trajectories of ZO algorithms as functions of the number of epochs, where ZO-SVRG is evaluated under different mini-batch sizes b ∈ {1, 10, 40}. We observe that the convergence error of ZO-SVRG decreases as b increases, and for a small mini-batch size b ≤ 10, ZO-SVRG likely converges to a neighborhood of a critical point as shown by Corollary 1. We also note that our proposed algorithms ZO-SVRG (b = 40), ZO-SVRG-Coord and ZO-SVRG-Ave have faster convergence speeds (i.e., less iteration complexity) than the existing algorithms ZO-SGD and ZO-SVRC. Particularly, the use of multiple random direction samples in Avg-RandGradEst significantly accelerates ZO-SVRG since the error of order O(1/b) is reduced to O(1/(bq)) (see Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of ZO-SGD and ZO-SVRG-Ave for generation of universal adversarial perturbations from a black-box DNN. Left: Attack loss versus epochs. Right: 2 distortion and improvement (%) with respect to ZO-SGD.</figDesc><graphic coords="9,127.85,72.00,170.28,127.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>To mitigate this error term, we propose two accelerated ZO-SVRG variants, utilizing reduced variance gradient estimators. These</figDesc><table /><note><p><p>yield a faster convergence rate towards O(d/T ), the best known iteration complexity bound for ZO stochastic optimization.</p>Our work offers a comprehensive study on how ZO gradient estimators affect SVRG on both iteration complexity (i.e., convergence rate) and function query complexity. Compared to the existing ZO algorithms, our methods can strike a balance between iteration complexity and function query complexity. To demonstrate the flexibility of our approach in managing this trade-off, we conduct an empirical evaluation of our proposed algorithms and other state-of-the-art algorithms on two diverse applications: black-box chemical material classification and generation of universal adversarial perturbations from black-box deep neural network models. Extensive experimental results and theoretical analysis validate the effectiveness of our approaches.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Summary of convergence rate and function query complexity of our proposals given T iterations.</figDesc><table><row><cell>Method</cell><cell>Grad. estimator</cell><cell>Stepsize</cell><cell>Convergence rate (worst case as b</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Testing error for chemical material classification using 7.3 × 10 6 function queries.</figDesc><table><row><cell># of epochs</cell><cell>14600</cell><cell>100</cell><cell>2920</cell><cell>50</cell><cell>365</cell></row><row><cell>Error (%)</cell><cell>12.56%</cell><cell>23.70%</cell><cell>11.18%</cell><cell>20.67%</cell><cell>15.26%</cell></row></table><note><p><p><p><p><p><p>Method</p>ZO-SGD</p><ref type="bibr" target="#b23">[24]</ref> </p>ZO-SVRC</p><ref type="bibr" target="#b25">[26]</ref> </p>ZO-SVRG ZO-SVRG-Coord ZO-SVRG-Ave</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>In the big O notation, the constant numbers are ignored, and the dominant factors are kept.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>The parameter µ can be generalized to µi for i ∈ [n]. Here we assume µi = µ for ease of representation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>Different from the standard SVRG<ref type="bibr" target="#b18">[19]</ref>, we consider its mini-batch variant in<ref type="bibr" target="#b19">[20]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>For mini-batch I, SVRG<ref type="bibr" target="#b19">[20]</ref> assumes i.i.d. samples with replacement, while a variant of SVRG (called SCSG) assumes samples without replacement<ref type="bibr" target="#b22">[23]</ref>. This paper considers both sampling strategies.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5"><p>One exception is ZO-SCD<ref type="bibr" target="#b6">[7]</ref> (and its variant ZO-SVRC<ref type="bibr" target="#b25">[26]</ref>), where µ ≤ O(1/ √ T ).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6"><p>One can mimic DFT simulator using a logistic function once the parameter x is learned from ZO algorithms.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_7"><p>Code to reproduce experiments can be found at https://github.com/IBM/ZOSVRG-BlackBox-Adv</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_8"><p>https://github.com/carlini/nn_robust_attacks</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was fully supported by the MIT-IBM Watson AI Lab. Bhavya Kailkhura was supported under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344 (LLNL-CONF-751658). The authors are also grateful to the anonymous reviewers for their helpful comments,</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Practical black-box attacks against machine learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security</title>
		<meeting>the 2017 ACM on Asia Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="506" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models</title>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security</title>
		<meeting>the 10th ACM Workshop on Artificial Intelligence and Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Bandit convex optimization for scalable and dynamic iot management</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Giannakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09060</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Zeroth-order online admm: Convergence analysis and applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Hero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Twenty-First International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2018-04">April 2018</date>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="288" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optimization for simulation: Theory vs. practice</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INFORMS Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="192" to="215" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A comprehensive linear speedup analysis for asynchronous stochastic parallel optimization from zeroth-order to first-order</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3054" to="3062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Algorithms for minimization without derivatives</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Brent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>Courier Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Introduction to stochastic search and optimization: estimation, simulation, and control</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Spall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">65</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Online convex optimization in the bandit setting: gradient descent without a gradient</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Flaxman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Kalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms</title>
		<meeting>the sixteenth annual ACM-SIAM symposium on Discrete algorithms</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the complexity of bandit and derivative-free stochastic convex optimization</title>
		<author>
			<persName><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optimal algorithms for online convex optimization with multi-point bandit feedback</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="28" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Random gradient-free minimization of convex functions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Spokoiny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational Mathematics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="527" to="566" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Optimal rates for zero-order convex optimization: The power of two function evaluations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wibisono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2788" to="2806" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An optimal algorithm for bandit and zero-order convex optimization with two-point feedback</title>
		<author>
			<persName><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">52</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the information-adaptive variants of the admm: an iteration complexity perspective</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optimization Online</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An accelerated method for derivative-free smooth stochastic convex optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dvurechensky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gasnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gorbunov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.09022</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stochastic zeroth-order optimization in high dimensions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Twenty-First International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-04">April 2018</date>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="1356" to="1365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Accelerating stochastic gradient descent using predictive variance reduction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stochastic variance reduction for nonconvex optimization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hefny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="314" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Accelerated stochastic gradient descent for minimizing finite sums</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nitanda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="195" to="203" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improved svrg for non-strongly-convex or sum-of-non-convex objectives</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1080" to="1089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Non-convex finite-sum optimization via scsg methods</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2345" to="2355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stochastic first-and zeroth-order methods for nonconvex stochastic programming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghadimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2341" to="2368" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Zeroth order nonconvex multi-agent optimization over networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hajinezhad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garcia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09997</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Zeroth-order asynchronous doubly stochastic algorithm with variance reduction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01425</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On blackbox backpropagation and jacobian sensing</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6524" to="6532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rebar: Low-variance, unbiased gradient estimates for discrete latent variable models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lawson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2624" to="2633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Backpropagation through the void: Optimizing control variates for black-box gradient estimation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Roeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00123</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">On the theory of variance reduction for stochastic gradient monte carlo</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Chatterji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Flammarion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-A</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05431</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Density-functional theory</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Ayers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Medicinal Chemistry for Drug Discovery</title>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="103" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Second-order optimization for non-convex machine learning: An empirical study</title>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roosta-Khorasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07827</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The open quantum materials database (oqmd): assessing the accuracy of dft formation energies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kirklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Saal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Meredig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Doak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aykol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wolverton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Materials</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">15010</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficiency of ab-initio total energy calculations for metals and semiconductors using a plane-wave basis set</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kresse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Furthmüller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational materials science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="50" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Randomized block coordinate descent for online and stochastic optimization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1407.0107</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Online learning and online convex optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="194" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A general-purpose machine learning framework for predicting properties of inorganic materials</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wolverton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Materials</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16028</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
