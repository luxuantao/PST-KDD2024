<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring PIM Architecture for High-Performance Graph Pattern Mining</title>
				<funder ref="#_QpQKPsa #_pPYazd4">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiya</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Iowa</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Linfeng</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Iowa</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Iowa</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rujia</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Iowa</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring PIM Architecture for High-Performance Graph Pattern Mining</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/LCA.2021.3103665</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph mining applications, such as subgraph pattern matching and mining, are widely used in real-world domains such as bioinformatics, social network analysis, and computer vision. Such applications are considered as a new class of data-intensive applications that generate massive irregular computation workloads and memory accesses, which are different from many well-studied graph applications such as BFS and page rank. In this letter, we use the emerging process-in-memory architecture to accelerate data-intensive operations in graph mining tasks. We first identify the code blocks that are best suitable for PIM execution. Then, we observe a significant load imbalance on PIM architecture and analyze the root cause for such imbalance in graph mining applications. Lastly, we evaluate several scheduling schemes that help reduce the load imbalance and discuss potential optimizations to enhance performance further.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>PROCESS-IN-MEMORY architecture (PIM) <ref type="bibr" target="#b0">[1]</ref> is considered as a promising solution to enhance the performance of memory-bounded data-intensive applications. With PIM architecture, it is possible to integrate general-purpose or specialized computation units in or near the memory module. When the application and data are appropriately placed and scheduled on the PIM and host CPU, we can reduce massive data movement between the CPU and memory module to achieve high-performance and energy-efficient computation. For example, classical graph processing applications, such as BFS and page rank, have been implemented on emerging PIM architectures with software and hardware co-designs <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b13">[14]</ref>.</p><p>Recently, graph pattern mining (GPMI) algorithms emerge as a new class of data-intensive applications that has attracted extensive attention from system <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> and architecture <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b20">[21]</ref> domain. GPMI has many real-world use cases, such as motif extraction from gene networks <ref type="bibr" target="#b14">[15]</ref> and pattern search over semantic data <ref type="bibr" target="#b4">[5]</ref>. GPMI is fundamentally different from the general graph processing applications in several ways: 1) the computation involves more complex iterations which may cause load imbalance; 2) the computation involves enormous data accesses (more details in Section 2.2). Therefore, it is challenging to use conventional hardware, such as CPU or GPU, to accelerate the computation.</p><p>Therefore, we are motivated to examine the new class of GPMI applications and study the challenges of applying PIM architecture to such applications. We first explore the memory access characteristics of the graph matching algorithm and find the intersection and subtraction (I/S) operations are memory access intensive, which are suitable for PIM architecture. Then we compare the performance of I/S operations on CPU host and PIM to evaluate the potential performance gain. We identify that the workload distribution to PIM cores could cause significant imbalance and hurt the performance improvement from PIM architecture. We evaluate several scheduling schemes which reduce the load imbalance in selected workloads. In addition, we identify the root cause of such load imbalance regarding the input graph and patterns and propose potential schemes that can overcome such challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND MOTIVATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Process-in-Memory Architecture</head><p>Processing-in-Memory (PIM) integrates processing units inside the memory to reduce the overhead of frequent data movement. PIM can be implemented using a variety of technologies. 3D-stacking with TSVs technology is a commonly used technology for PIM due to its large bandwidth and energy advantages. Two of the most prominent 3D-stacked memory technologies today are Hybrid Memory Cube (HMC) <ref type="bibr" target="#b2">[3]</ref> and High Bandwidth Memory (HBM) specification <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, both of which consist of one logic die stacked with several DRAM dies. The PIM cores could be either implemented on the logic die <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b13">[14]</ref> or in the DRAM banks <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b10">[11]</ref>.</p><p>In this work, we assume that the PIM cores are integrated into the HMC architecture, and they can process the same ISA as the host. The host access the HMC with an external link, while the PIM cores access the HMC via internal TSVs. Note that the latest PIM module from Samsung <ref type="bibr" target="#b10">[11]</ref> uses the HBM architecture with 128 programmable computing units. Due to the limited documentation on the new PIM interface, we follow prior research work and use the HMC interface for evaluation purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Mining Applications and Algorithms</head><p>Applications. In this work, we focus on the motif counting (MC) GPMI application. A motif is any connected, unlabeled graph pattern. The goal is to identify all motifs (patterns) with k vertices and count the embeddings of each of the patterns. This kernel is widely used in bioinformatics. The evaluated patterns are shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>Representative Algorithms. There are two available algorithms that can support GPMI applications. 1) Exhaustive-check method, used in Arabesque <ref type="bibr" target="#b18">[19]</ref>, RStream <ref type="bibr" target="#b19">[20]</ref>, Gramer <ref type="bibr" target="#b20">[21]</ref>, which explores the subgraphs to a certain size, and performs isomorphism checks to aggregate the explored subgraphs; and 2) Pattern-enumeration method, used in Automine <ref type="bibr" target="#b12">[13]</ref>, GraphZero <ref type="bibr" target="#b11">[12]</ref>, GraphPi <ref type="bibr" target="#b17">[18]</ref>, directly finds the subgraphs that are isomorphic to the pattern. Compared with the exhaustive-check method, the pattern-enumeration method can achieve higher performance since it eliminates the computation-intensive isomorphism tests with lots of edgedimension random accesses and avoids checking the subgraphs not matching the pattern. AutoMine <ref type="bibr" target="#b12">[13]</ref> outperforms RStream <ref type="bibr" target="#b19">[20]</ref> and Arabesque <ref type="bibr" target="#b18">[19]</ref> by several orders of magnitude on real-world graphs of different scales. Therefore, we focus on the pattern-enumeration algorithm for GPMI applications.</p><p>Pattern Enumeration Steps. Fig. <ref type="figure" target="#fig_1">2</ref> shows the steps of pattern enumeration with AutoMine algorithm. First, it generates all patterns according to the requirements of the application (Step 1). Then, for each pattern, it first constructs a colored complete pattern graph to encode all the neighborhood relations of the vertices in the pattern (Step 2). Specifically, it paints all present edges black and adds red edges for the absent ones. Next, it assigns an order to the vertices of the pattern, and specifies the direction for the edges from small id vertices to large id vertices (Step 3). Finally, according to the vertex ids and the directed edges, we can construct a multi-layer nest_-for_loop (pseudocode in Fig. <ref type="figure" target="#fig_1">2</ref>) to find all embeddings (also called subgraphs) that match the pattern. Each vertex in the pattern is associated with a for loop. The loops start from the smallest vertex id v 0 . If the incoming edge ?i; j? is black, which means there is an edge between vertices i and j, then vertex j belongs to the Jiya Su and Rujia Wang are with the Computer Science Department, Illinois Institute of Technology, Chicago, IL 60616 USA. E-mail: jsu18@hawk.iit.edu, rwang67@iit.edu. Linfeng He and Peng Jiang are with the Computer Science Department, University of Iowa, Iowa City, IA 52242 USA. E-mail: {linfeng-he, peng-jiang}@uiowa.edu. intersection of the neighbor sets of vertex i; if the edge is red, which means there is no edge between vertices i and j, then vertex j belongs to the subtraction of the neighbor set of the vertex i. Take v 2 in the 3-size pattern 1 as an example, since the incoming edge ?0; 2? is black and edge ?1;</p><formula xml:id="formula_0">2? is red, v 2 2 N?v 0 ? ? N?v 1 ?.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Motivation</head><p>Identify Suitable PIM Workload From GPMI Applications. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, accessing the neighbor vertices and operating on the neighbor vertex list (e.g., N?v 0 ?, N?v 1 ?) with intersect(\) and subtraction (?) set operations (I/S operations in short) are critical and memory-intensive. This makes I/S operations include most of the memory accesses of the nest_for_loop function. Table <ref type="table" target="#tab_1">1</ref> summarizes the execution time and memory access ratios of the I/S operations in the nest_for_loop function on 5 different graphs (The details of the graphs are shown in Table <ref type="table">2</ref>).</p><p>The data is collected by running the applications on a 16-core CPU simulator (details in Section 4.1). We find that I/S operations account for 20$33 percent of the entire nest_for_loop execution time. Moreover, the number of memory accesses coming from I/S operations accounts for 85$96 percent, except for CiteSeer. CiteSeer graph is relatively small (84 KB) and can fit into the cache (Table <ref type="table" target="#tab_2">3</ref>), so the I/S operations generate relatively small proportion of memory accesses. In other larger graphs, I/S memory access accounts for more than 85 percent, which can be considered as a memory-intensive application. Therefore, it is reasonable to offload I/S operations to PIM. Note that the I/S operations do not dominant the execution time, other code blocks which involve complex computation but few memory accesses (e.g., select the execution order of the I/S operations, and remove the duplication of the result in each for loop) also need to be optimized with other software or hardware approaches.</p><p>Load Imbalance of I/S Operations. Additionally, from Fig. <ref type="figure" target="#fig_1">2</ref>, we know that finding n-size patterns requires n layers of for loops. When executing the code on multiple cores, the most straightforward way is to assign the I/S operations (the second for loop to the last for loop) to the same core base on the root vertex (v 0 in the firstlevel loop). Such a method (root vertex-based assignment) can guarantee the data dependency of following I/S operations.</p><p>However, the number of loops at each layer is determined by the results of I/S operations (e.g., N?v 0 ? ? N?v 1 ? in Fig. <ref type="figure" target="#fig_1">2</ref>), which varies a lot based on patterns or graphs, and cannot be determined by offline profiling. In comparison, for general graph processing applications such as BFS and PR, the workload of each vertex is small and easy to obtain from the vertex degree. Therefore, compared with general graph processing applications, the workloads of GPMI applications on different cores could differ significantly. As the matching size increases, the number of layers of the for loop increases, resulting in a more significant load imbalance. Also, PIM usually has much more cores (128 in this paper) than the host, making the load imbalance problem even worse. To fully utilize the parallelism brought by the PIM architecture, we should address the load imbalance issue properly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">I/S OPERATION SCHEDULING</head><p>We now describe the following three root vertex-based scheduling methods. Note that, round-robin and balanced queue are two common scheduling schemes to address load imbalance issue. We also show the results of No Dependency on PIM as a reference, representing the ideal case where we do not consider the execution dependencies between I/S operations. Our system settings can be found in Section 4.1.</p><p>I/S on CPU (CPU). This scheme only uses CPU cores. The nest_for_loop function is assigned to different CPU cores according to their root vertices (v 0 ) IDs. Round-robin on PIM (RR). I/S operations are all executed on PIM cores. Based on the root vertex ids, the nest_for_loop functions (tasks) on CPU core 0 are assigned from PIM core 0 to PIM core 7 in turn, and then from PIM core 7 to PIM core 0 in reverse. Balanced queue on PIM (BQ). In the PIM core, for each I/S operation, we sum the lengths of the two arrays (N?v i ? and ?N?v j ?) to estimate the workload of this I/S operation and store the estimated workload in a queue. Then for an incoming task, the host assigns the task to a corresponding PIM core with the least workload in the queue. No Dependency on PIM (Ideal). This scheduling method ignores the dependencies among all I/S operations with the same root vertex and treats each single I/S operation as an independent task. I/S operations are assigned to different PIM cores through the round robin strategy.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Graph Datasets. Table <ref type="table">2</ref> shows the five real-world graphs used in our experiments. These graphs are used in most graph mining papers, such as Arabesque <ref type="bibr" target="#b18">[19]</ref>, RStream <ref type="bibr" target="#b19">[20]</ref>, and AutoMine <ref type="bibr" target="#b12">[13]</ref>.</p><p>Before execution, we sort the vertices based on their degree from largest to smallest (the id of the vertex with the highest degree is 0, the id with the second highest degree is 1, and so on).</p><p>Applications. We run 3-size and 4-size motif counting (MC) in the experiments. For 3-size MC, there are 2 different patterns; and for 4-size MC, there are 6 different patterns (Fig. <ref type="figure" target="#fig_0">1</ref>). We evaluate the performance of these 8 patterns separately.</p><p>System Configurations. We use ZSim <ref type="bibr" target="#b16">[17]</ref> with Ramulator <ref type="bibr" target="#b9">[10]</ref> to simulate the host CPU and PIM system. We modify Zsim to generate traces for the CPU and PIM when executing the nest_for_loop function. We also modify Ramulator to support 16 CPU cores with 3-level caches and 128 PIM cores (following Samsung PIM core number <ref type="bibr" target="#b10">[11]</ref>) with L1i and L1d caches. All caches use LRU policy. The host CPU frequency is 4 times of the PIM core frequency, which is also adopted from the Function-In-Memory DRAM <ref type="bibr" target="#b10">[11]</ref>. We allow one CPU core to assign tasks to the corresponding 8 PIM cores (CPU core 0 offloads tasks to PIM cores 0-7, etc). Table <ref type="table" target="#tab_2">3</ref> shows the detailed configurations of our simulated system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results and Analysis</head><p>Fig. <ref type="figure" target="#fig_2">3</ref> shows the I/S operation execution time with different scheduling schemes. We present two values for each experiment: the longest time (in light color) all cores finish the workload; the average time (in shaded color) spent on each core to complete the workload. The closer the longest time is to the average time, the more balanced the workload at each core.</p><p>Average Time. For all graphs and patterns, the average I/S time on PIM is around half the average time on the CPU. This indicates that the selected I/S operations are indeed suitable for PIM execution. While the number of PIM cores is 8 times the CPU cores, the frequency of the PIM is 4 times slower than that of the CPU. Taken together, PIM is twice as fast as CPU. Second, for the three scheduling methods on PIM, although the task scheduling methods are different, the total workload is the same, so the average time of the three methods is very close.</p><p>Load Imbalance on CPU and PIM. We observe moderate load imbalance on CPU for most patterns and graphs, except for Youtube 3-size pattern 1 and CiteSeer 4-size pattern 1. Meanwhile, since PIM has more cores, we observe a much more severe load imbalance. Compared to the CPU cores, while the average time with PIM cores is reduced, the longest time may not(e.g., with RR scheduling). In CiteSeer, 3-size Youtube, and 4-size Mico pattern 1&amp;3, imbalanced workload makes I/S operations completion time on PIM longer than on CPU.</p><p>Effectiveness and Limitations of PIM-Side Scheduling. With BQ scheduling, we use static information (length of two neighboring lists) to estimate each core's workload. BQ can effectively mitigate the load imbalance in various input graphs. However, compared to the Ideal case, we can further reduce a performance gap on 3size pattern 1, 4-size patterns 1-3. The results show that, while the length of two neighboring lists can estimate the heaviness of workload per core, using the root vertex to partition the I/S operations could lead to a large task on a core, and no matter how the task is scheduled, it will always be the bottleneck.</p><p>Load Imbalance Regarding Pattern Size. As the pattern size increases, the number of for loop layers of the nest_for_loop function increases, and the workload of a single task scheduled by root vertex increases, resulting in more obvious workload differences of each task. For CiteSeer, Mico and Patents graphs, all the scheduling methods of 4-size pattern 1 are more unbalanced than the corresponding 3-size pattern 1.</p><p>Load Imbalance Regarding Pattern Shape. As discussed in Section 2.3, due to the root vertex-based task assignment, the task per core can vary a lot based on the results of the I/S operations. Additionally, the characteristics of patterns can also determine the load. The patterns in Fig. <ref type="figure" target="#fig_0">1</ref> can be divided into three categories: a) 3-size pattern 1, 2; b) 4-size pattern 1, 3, 5, 6; c) 4-size pattern 2, 4. In each category, a following pattern has one more edge than the previous pattern. In the nest_for_loop, adding an edge to the pattern means that a subtraction(?) operation in the for loop will be replaced by a intersection(\) operation. Since A ? B ? A \ B, according to the data in Table <ref type="table">2</ref>, the edges of all graphs are sparse, which means that jN?v?j &gt; &gt; jN?v?j. Therefore, jN?v i ? ? N?v j ?j ? jN?v i ? \ N?v j ?j &gt; &gt; jN?v i ? \ N?v j ?j. As a result, the execution time and load imbalance decreases when we extend the edge in each category: a) 3-size: pattern 1 &gt; 2; b) 4-size: pattern 1 &gt; 3 &gt; 5 &gt; 6; c) 4-size pattern 2 &gt; 4. Our experimental results in Fig. <ref type="figure" target="#fig_2">3</ref> also show the same behavior. This is also reason that for the 3-size pattern 2 and 4-size patterns 4, 5, and 6, the root vertex BQ scheduling  method can almost mitigate the load imbalance, but the scheduling method needs to be optimized for other patterns with more subtraction operations. Potential Solutions to Address Load Imbalance With PIM Architecture. As such, to fully utilize the PIM architecture to accelerate the I/S operations, there is a need to address the load imbalance issue, which cannot be mitigated with conventional scheduling schemes (RR or BQ) for graph processing. Two potential solutions are: divide the workload based on non-root vertex id so that the scheduling can be done at a finer grain; implement work stealing mechanisms among PIM cores so that the workload distribution to PIM cores can be determined at runtime.</p><p>Impact of Parallelism and Bandwidth. We have also evaluated the execution time of the BQ scheduling method on 128 CPU cores. Due to the limited space, we discuss the average execution time of running 3-size pattern 1 on the CiteSeer graph as an example. On 16 CPU cores at 4 GHz is 31.3 ms, on 128 CPU cores at 4 GHz is 11.9 ms. By reducing the CPU core frequency to 1 GHz, on 128 cores, the time is 21.3 ms. In comparison, on 128 OoO PIM cores, the time is 15.9 ms, and on 128 in-order PIM cores, the time is 21.6 ms. We observe that both parallelism and bandwidth of PIM architecture could impact the performance. The parallelism gained from 16 CPU cores to 128 CPU cores improves the I/S operation performance by 2.6x. When all are running at 1 GHz, 128 OoO PIM cores can be 1.34x faster than 128 OoO CPU cores, which means that the high memory bandwidth can also help with the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Several recent works proposed specialized hardware for GPMI applications. GRAMER <ref type="bibr" target="#b20">[21]</ref> adds a specialized memory hierarchy, where the valuable data permanently resides in the static memory while others are dynamically maintained in a cache-like memory with a lightweight replacement strategy to improve the performance. IntersectX <ref type="bibr" target="#b15">[16]</ref> accelerates graph mining with the help of the extension of stream instructions set and the architectural improvement based on conventional processors. SISA <ref type="bibr" target="#b1">[2]</ref> uses specialized set-centric ISA and in-memory logic to alleviate the bandwidth requirements of the set operations. Compared with the concurrent work listed above, we focus on leveraging general-purpose PIM cores to accelerate GPMI applications with balanced data and task allocation. Moreover, we identify that, to efficiently exploit the computation power of host and PIM, we have to carefully schedule the operations while maintaining the correct dependencies between loops. The observations and challenges are not discussed in any other work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>To conclude, in this work, we identify the PIM-suitable I/S operations in GPMI applications and evaluate them on general purpuse PIM architecture. However, it is challenging to maintain good load balancing when assigning tasks from the host to the PIM. We find that load imbalance significance depends on many factors, including the pattern itself. We evaluate two classical scheduling schemes and find out that static scheduling schemes for improving load balance cannot fully solve the problem. The root cause is from the unique GPMI algorithm: using the root vertex to partition the I/S operations could lead to a large task on a core, and no matter how the task is scheduled, the large task will always be the bottleneck. Based on the observations, we plan to explore fine-grained scheduling schemes with runtime work-stealing to further release the power of PIM architecture for this new class of applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Patterns with 3 and 4 vertices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Pattern enumeration with AutoMine [13] method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The I/S operation performance of 3-size and 4-size matching.</figDesc><graphic url="image-1.png" coords="3,317.69,47.53,210.74,170.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 The</head><label>1</label><figDesc>Time and Memory Access Ratios of I/S Operations in the nest_for_loop Function</figDesc><table><row><cell cols="2">Matching Size Graph</cell><cell></cell><cell cols="4">Execution time ratio Memory access ratio</cell></row><row><cell>3-size</cell><cell>CiteSeer</cell><cell></cell><cell>19.12%</cell><cell></cell><cell cols="2">38.97%</cell></row><row><cell></cell><cell>MiCo</cell><cell></cell><cell>29.67%</cell><cell></cell><cell cols="2">94.16%</cell></row><row><cell></cell><cell>Patents</cell><cell></cell><cell>23.65%</cell><cell></cell><cell cols="2">85.46%</cell></row><row><cell></cell><cell>Youtube</cell><cell></cell><cell>16.67%</cell><cell></cell><cell cols="2">92.82%</cell></row><row><cell></cell><cell cols="2">LiveJournal-1</cell><cell>24.70%</cell><cell></cell><cell cols="2">96.04%</cell></row><row><cell>4-size</cell><cell>CiteSeer</cell><cell></cell><cell>26.16%</cell><cell></cell><cell cols="2">36.29%</cell></row><row><cell></cell><cell>MiCo</cell><cell></cell><cell>32.87%</cell><cell></cell><cell cols="2">96.06%</cell></row><row><cell></cell><cell>Patents</cell><cell></cell><cell>25.00%</cell><cell></cell><cell cols="2">89.42%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TABLE 2</cell><cell></cell><cell></cell></row><row><cell cols="7">Graph Datasets [13] 5 Percent Deg. = The Top 5 Percent Node Degrees</cell></row><row><cell></cell><cell></cell><cell cols="3">/ All Node Degrees</cell><cell></cell></row><row><cell>Graphs</cell><cell cols="6">Vertices Edges Size Avg.Deg. Max.Deg. 5% Deg.</cell></row><row><cell>CiteSeer</cell><cell>3264</cell><cell cols="2">4536 84 KB</cell><cell>2.78</cell><cell>99</cell><cell>23.2%</cell></row><row><cell>MiCo</cell><cell cols="3">100 K 1.08 M 18 MB</cell><cell>21.60</cell><cell>1359</cell><cell>29.9%</cell></row><row><cell>cit-Patents</cell><cell cols="3">3.77 M 16.52 M 332 MB</cell><cell>8.75</cell><cell>793</cell><cell>22.9%</cell></row><row><cell>com-Youtube</cell><cell cols="3">1.13 M 2.99 M 57 MB</cell><cell>5.27</cell><cell cols="2">28,754 56.7%</cell></row><row><cell cols="4">soc-LiveJournal1 4.85 M 43.11 M 1.2 G</cell><cell>17.79</cell><cell cols="2">20,334 42.4%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3</head><label>3</label><figDesc>CK ? 1 ns, t RAS ? 27 ns, t RCD ? 14 ns, t CL ? 14 ns, t WR ? 15 ns, t RP ? 14 ns<ref type="bibr" target="#b13">[14]</ref> </figDesc><table><row><cell></cell><cell>System Configurations</cell></row><row><cell cols="2">Host Processer [1], [6]</cell></row><row><cell>Cores</cell><cell>16 OoO cores, 4GHz, 4-issue</cell></row><row><cell>L1I Cache</cell><cell>private, 32 KB, 4-way, 4-cycle , 64 B, 16 MSHRs</cell></row><row><cell>L1D Cache</cell><cell>private, 32 KB, 8-way, 4-cycle , 64 B, 16 MSHRs</cell></row><row><cell>L2 Cache</cell><cell>private, 256 KB, 8-way, 12-cycle, 64 B, 16 MSHRs</cell></row><row><cell>L3 Cache</cell><cell>shared, 16 MB, 8 banks, 16-way, 28-cycle, 64 B, 16</cell></row><row><cell></cell><cell>MSHRs per bank</cell></row><row><cell>PIM Cores</cell><cell></cell></row><row><cell>Cores</cell><cell>128 in-order cores, 1 GHz, 4-issue [1], [7], [11]</cell></row><row><cell>L1I Cache</cell><cell>private, 32 KB, 4-way, 4-cycle, 64 B, 16 MSHRs [6]</cell></row><row><cell>L1D Cache</cell><cell>private, 32 KB, 8-way, 4-cycle, 64 B, 16 MSHRs [6]</cell></row><row><cell cols="2">3D Memory Stack</cell></row><row><cell>Organization</cell><cell>4 GB, 4 layers ? 32 vaults ? 1 stack [3]</cell></row><row><cell cols="2">Timing Parameters t Serial links 4 links, 16 bits link width, 30 Gb/s lane speed, total</cell></row><row><cell></cell><cell>240 GB/s bandwidth [3]</cell></row><row><cell>Internal links</cell><cell>32 links, 12 Bytes/cycle, 15 GB/s per link, total</cell></row><row><cell></cell><cell>480 GB/s bandwidth [14]</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Authorized licensed use limited to: The University of Iowa. Downloaded on July 27,2022 at 22:16:19 UTC from IEEE Xplore. Restrictions apply.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work was supported by the <rs type="funder">National Science Foundation</rs> under awards <rs type="grantNumber">CCF-2029014</rs> and <rs type="grantNumber">CCF-2028825</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_QpQKPsa">
					<idno type="grant-number">CCF-2029014</idno>
				</org>
				<org type="funding" xml:id="_pPYazd4">
					<idno type="grant-number">CCF-2028825</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pim-enabled instructions: A lowoverhead, locality-aware processing-in-memory architecture</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Int. Symp. Comput. Archit</title>
		<meeting>Annu. Int. Symp. Comput. Archit</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="336" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">SISA: Set-centric instruction set architecture for graph mining on processing-in-memory systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Besta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07582</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Hybrid memory cube specification 2.1</title>
		<author>
			<persName><forename type="first">H</forename><surname>Consortium</surname></persName>
		</author>
		<ptr target="https://www.nuvation.com/sites/default/files/Nuvation-Engineering-Images/Articles/FPGAs-and-HMC/HMC-30G-VSR/HMCC/Specification.pdf" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards near data processing of convolutional neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 31st Int. Conf. VLSI Des. 17th Int. Conf. Embedded Syst</title>
		<meeting>31st Int. Conf. VLSI Des. 17th Int. Conf. Embedded Syst</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="380" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Keyword search over RDF graphs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Elbassuoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Blanco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th ACM Int. Conf. Inf. Knowl. Manage</title>
		<meeting>20th ACM Int. Conf. Inf. Knowl. Manage</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="237" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Practical near-data processing for inmemory analytics frameworks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Parallel Archit. Compilation</title>
		<meeting>Int. Conf. Parallel Archit. Compilation</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">iPIM: Programmable in-memory image processing accelerator using near-bank architecture</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Int. Symp. Comput. Archit</title>
		<meeting>Annu. Int. Symp. Comput. Archit</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="804" to="817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Medal: Scalable DIMM based near data processing accelerator for DNA seeding algorithm</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huangfu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu</title>
		<meeting>Annu</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="587" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">HBM (high bandwidth memory) DRAM technology and architecture</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Memory Workshop</title>
		<imprint>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ramulator: A fast and extensible DRAM simulator</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Archit. Lett</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="49" />
			<date type="published" when="2016-06">Jan.-Jun. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A 20 nm 6 GB function-in-memory DRAM, based on HBM2 with a 1.2 TFLOPS programmable computing unit using bank-level parallelism, for machine learning applications</title>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Solid-State Circuits Conf</title>
		<meeting>IEEE Int. Solid-State Circuits Conf</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="350" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">GraphZero: Breaking symmetry for efficient graph mining</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mawhirter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12877</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">AutoMine: Harmonizing high-level abstraction and high performance for graph mining</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mawhirter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 27th ACM Symp</title>
		<meeting>27th ACM Symp</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="509" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">GraphPIM: Enabling instruction-level PIM offloading in graph computing frameworks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Nai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. High Perform. Comput. Archit</title>
		<meeting>IEEE Int. Symp. High Perform. Comput. Archit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="457" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A survey of graph mining techniques for biological datasets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tatikonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ucar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Managing and Mining Graph Data</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="547" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">IntersectX: An accelerator for graph mining</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.10848</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ZSim: Fast and accurate microarchitectural simulation of thousand-core systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Annu. Int. Symp. Comput. Archit</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="475" to="486" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">GraphPi: High performance graph pattern matching through effective redundancy elimination</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. High Perform. Comput., Netw., Storage Anal</title>
		<meeting>Int. Conf. High Perform. Comput., Netw., Storage Anal</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Arabesque: A system for distributed graph mining</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Teixeira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th Symp. Operating Syst. Princ</title>
		<meeting>25th Symp. Operating Syst. Princ</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="425" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">RStream: Marrying relational algebra with streaming for efficient graph mining on a single machine</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th USENIX Conf</title>
		<meeting>13th USENIX Conf</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="763" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A locality-aware energy-efficient accelerator for graph mining applications</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu</title>
		<meeting>Annu</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="895" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">For more information on this or any other computing topic, please visit our Digital Library at</title>
		<ptr target="www.computer.org/csdl" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
