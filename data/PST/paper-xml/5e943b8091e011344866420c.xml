<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Longformer: The Long-Document Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
							<email>beltagy@allenai.org</email>
							<affiliation key="aff1">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
							<email>matthewp@allenai.org</email>
							<affiliation key="aff1">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
							<email>armanc@allenai.org</email>
							<affiliation key="aff1">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Allen Institute for Artificial Intelligence</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Longformer: The Long-Document Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on Wiki-Hop and TriviaQA. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformers <ref type="bibr" target="#b47">(Vaswani et al., 2017)</ref> have achieved state-of-the-art results in a wide range of natural language tasks including generative language modeling <ref type="bibr" target="#b18">(Dai et al., 2019;</ref><ref type="bibr" target="#b40">Radford et al., 2019)</ref> and discriminative language understanding <ref type="bibr" target="#b19">(Devlin et al., 2019)</ref>. This success is partly due to the self-attention component which enables the network to capture contextual information from the entire sequence. While powerful, the memory and computational requirements of self-attention grow quadratically with sequence length, making it infeasible (or very expensive) to process long sequences on current hardware.</p><p>To address this limitation, we present Longformer, a modified Transformer architecture with a self-attention operation that scales linearly with the sequence length, making it versatile for processing long documents (Fig. <ref type="figure" target="#fig_0">1</ref>). This is an advantage for natural language tasks such as long document classification, question answering (QA), and coreference resolution, where existing approaches partition or shorten the long context into smaller sequences that fall within the typical 512 token limit of BERT-style pretrained models. Such partitioning could potentially result in loss of important cross-partition information, and to mitigate this problem, existing methods often rely on complex architectures to address such interactions. On the other hand, our proposed Longformer is able to build contextual representations of the entire context using multiple layers of attention, reducing the need for task-specific architectures.</p><p>Recent work has addressed the computational inefficiency of Transformers on long sequences (see Tab. 1). However, they primarily focus on autoregressive language modeling, while the application of long document transformers to document-level NLP tasks in the transfer learning setting <ref type="bibr" target="#b17">(Dai and Le, 2015;</ref><ref type="bibr" target="#b37">Peters et al., 2018;</ref><ref type="bibr">Howard and Ruder, 2018;</ref><ref type="bibr" target="#b19">Devlin et al., 2019)</ref> has remained largely unexplored. We address this gap and show that Longformer's attention mechanism can act as a arXiv:2004.05150v1 [cs.CL] 10 Apr 2020 drop-in replacement for the self-attention mechanism in pretrained Transformers, and leads to gains across a suite of document NLP tasks.</p><p>Longformer's attention mechanism is a combination of a windowed local-context self-attention and an end task motivated global attention that encodes inductive bias about the task. Through ablations and controlled trials we show both attention types are essential -the local attention is primarily used to build contextual representations, while the global attention allows Longformer to build full sequence representations for prediction.</p><p>We first evaluate Longformer on autoregressive character-level language modeling using a combination of windowed and a new dilated attention pattern, allowing the model to process sequences of up to 32K characters on modern GPUs. We achieve state-of-the-art results on text8 and enwik8 benchmark datasets, demonstrating the effectiveness of Longformer in long document modeling.</p><p>Then, to evaluate Longformer's ability to replace the full self-attention operation of existing pretrained models, we pretrain it with the masked language modeling (MLM) objective, continuing from the RoBERTa <ref type="bibr" target="#b32">(Liu et al., 2019)</ref> released checkpoint. After pretraining, we apply it to downstream language tasks through finetuning and demonstrate that Longformer consistently outperforms RoBERTa on a wide range of document-level natural language tasks including text classification, QA, and coreference resolution, achieving state-ofthe-art results on two of these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Long-Document Transformers Tab. 1 summarizes recent prior work on long documents. Two types of self-attention approaches have been explored. The first is a left-to-right (ltr) approach that processes the document in chunks moving from left-to-right. While such models have been successful in autoregressive language modeling, they are unsuitable for transfer learning approaches with tasks that benefit from bidirectional context.</p><p>Our work falls within the other general approach that defines some form of sparse attention pattern and avoids computing the full quadratic attention matrix multiplication. The model with the most similar attention pattern to ours is Sparse Transformer <ref type="bibr" target="#b14">(Child et al., 2019)</ref>, which uses a form of dilated sliding window of blocks of size 8x8 provided by BlockSparse <ref type="bibr" target="#b23">(Gray et al., 2017)</ref>. While both models use custom CUDA kernels, ours is implemented in TVM <ref type="bibr" target="#b12">(Chen et al., 2018</ref>) ( ยง3.2) making it more customizable and maintainable than BlockSparse which is implemented in C++, and designed for a specific versions of TensorFlow. We also introduce additional task motivated global attention patterns suitable for common NLP tasks ( ยง3) and show they are essential for good performance in the transfer learning setting.</p><p>A few models tried tasks other than autoregressive language modeling, which is a step forward because arguably focusing on language modeling as the primary evaluation has led to the development of models with limited applicability. BP-Transformer <ref type="bibr" target="#b52">(Ye et al., 2019)</ref> evaluated on machine translation (MT), but didn't explore the pretrainfinetune setting. Blockwise attention <ref type="bibr" target="#b39">(Qiu et al., 2019)</ref> pretrained their models and evaluated on question answering (QA). However, the evaluation is limited as it doesn't include language modeling, and the QA datasets are of relatively short documents,<ref type="foot" target="#foot_0">2</ref> therefore the effectiveness of this model on long document tasks remains unexplored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task-specific Models for Long Documents</head><p>Many task-specific approaches have been developed to workaround the 512 limit of pretrained transformer models like BERT. The simplest approach just truncates the document, commonly used for classification <ref type="bibr">(Xie et al., 2019</ref>). Another approach chunks the document into chunks of length 512 (could be overlapping), processes each chunk separately, then combines the activations with a task specific model <ref type="bibr" target="#b26">(Joshi et al., 2019)</ref>. A third approach popular for multihop and open domain QA tasks uses a two-stage model where the first stage retrieves relevant documents that are passed onto the second stage for answer extrac- tion <ref type="bibr" target="#b15">(Clark and Gardner, 2017;</ref><ref type="bibr" target="#b10">Chen et al., 2017)</ref>.</p><p>All of these approaches suffer from information loss due to truncation or cascading errors from the two stage approach. In contrast, Longformer can process long sequences without truncating or chunking, allowing us to adopt a much simpler approach that concatenates the available context and processes it in a single pass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>The original Transformer model has a self-attention component with O(n 2 ) time and memory complexity where n is the input sequence length and thus, is not efficient to scale to long inputs. To address this challenge, we sparsify the full self-attention matrix according to an "attention pattern" specifying pairs of input locations attending to one another. Unlike the full self-attention, our proposed attention pattern scales linearly with the input sequence, making it efficient for longer sequences. In the following, we discuss the design and implementation of this attention pattern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Attention Pattern</head><p>Fig. <ref type="figure" target="#fig_1">2</ref> summarizes the configurations of our proposed attention pattern.</p><p>Sliding Window Given the importance of local context <ref type="bibr" target="#b30">(Kovaleva et al., 2019)</ref>, our attention pattern employs a fixed-size window attention surrounding each token. Using multiple stacked layers of such windowed attention results in a large receptive field, where top layers have access to all input locations and have the capacity to build representations that incorporate information across the entire input. More formally, in this attention pattern, given a fixed window size w, each token attends to 1 2 w tokens on each side (Fig. <ref type="figure" target="#fig_1">2b</ref>). The computation complexity of this pattern is O(nรw), which scales linearly with input sequence length n. To make this attention pattern efficient, w should be small compared with n. However, as mentioned above, a model with typical multiple stacked transformer will have a large receptive field. This is analogues to CNNs where stacking layers of small kernels leads to high level features that are built from a large portion of the input (receptive field) <ref type="bibr" target="#b49">(Wu et al., 2019)</ref>. In our case, with a transformer of layers, the receptive field size is ร w (assuming w is fixed for all layers). Depending on the application, it might be helpful to use different values of w for each layer to balance between efficiency and model representation capacity ( ยง4.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dilated Sliding Window</head><p>To further increase the receptive field without increasing computation, the sliding window can be "dilated". This is analogues to dilated CNNs (van den <ref type="bibr" target="#b35">Oord et al., 2016)</ref> where the window has gaps of size dilation d (Fig. <ref type="figure" target="#fig_1">2c</ref>). Assuming a fixed d and w for all layers, the receptive field is ร d ร w, which can reach tens of thousands of tokens even for small values of d.</p><p>In multi-headed attention, each attention head computes a different attention score. We found settings with different dilation configurations per head improves performance by allowing some heads without dilation to focus on local context, while others with dilation focus on longer context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global Attention</head><p>In state-of-the-art BERT-style models for natural language tasks, the optimal input representation differs from language modeling and varies by task. For masked language modeling (MLM), the model uses local context to predict the masked word, while for classification, the model aggregates the representation of the whole sequence into a special token ([CLS] in case of BERT). For QA, the question and document are concatenated, allowing the model to compare the question with the document through self-attention.</p><p>In our case, the windowed and dilated attention are not flexible enough to learn task-specific representations. Accordingly, we add "global attention" on few pre-selected input locations. Importantly, we make this attention operation symmetric: that is, a token with a global attention attends to all tokens across the sequence, and all tokens in the sequence attend to it. Fig. <ref type="figure" target="#fig_1">2d</ref> shows an example of a sliding window attention with global attention at a few tokens at custom locations. For example for classification, global attention is used for the [CLS] token while in QA global attention is provided on all question tokens. Note that, since the number of such tokens is small relative and independent of n (the total number of tokens in the input sequence) the complexity of the combined local and global attention is still O(n).</p><p>While specifying global attention is task specific, it is much simpler than existing task specific approaches that chunk/shorten the input into smaller sequences and often use complex architecture to combine information across these chunks. Further, it increases the representational power of the model as it allows building contextual representations across the entire sequence.</p><p>Linear Projections for Global Attention Recall that given the linear projections Q, K, V , the Transformer model <ref type="bibr" target="#b47">(Vaswani et al., 2017)</ref> computes attention scores as follows:</p><formula xml:id="formula_0">Attention(Q, K, V ) = softmax QK T โ d k V (1)</formula><p>We use two sets of projections, Q s , K s , V s to compute attention scores of sliding window attention, and Q g , K g , V g to compute attention scores for the global attention. The additional projections provide flexibility to model the different types of attention, which we show is critical for best performance on downstream tasks. Q g , K g , V g are all initialized with values that match Q s , K s , V s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CUDA Kernels for our Attention Pattern</head><p>In regular transformers, attention scores are computed as in Eqn. 1. The expensive operation is the matrix multiplication QK T because both Q and K have n (sequence length) projections.</p><p>Our dilated sliding window attention pattern is not straightforward to implement using modern deep learning libraries. Implementing it requires a form of banded matrix multiplication (matrix multiplication where the output is all zero except certain diagonals) that is not supported in existing deep learning libraries like PyTorch/Tensorflow. In addition, naive implementations with loops is unusably slow. To address this challenge, we provide a highly efficient custom CUDA kernel that implements these attention operations and allows parallelizing the operation on GPU threads.</p><p>Tensor Virtual Machine (TVM) We build our custom CUDA kernel using TVM <ref type="bibr" target="#b12">(Chen et al., 2018)</ref>, a deep learning compiler stack that compiles high level description of a function into optimized device-specific code. Using TVM, we describe our form of banded matrix multiplication in high-level python constructs, then TVM generates the corresponding CUDA code and compiles it for GPUs.</p><p>Fig. <ref type="figure" target="#fig_0">1</ref> compares the runtime and memory of the full self-attention, our TVM implementation of the dilated sliding window, and a naive implementation of this attention. It is clear the full attention implementation is fast but it takes significant amount of memory because it needs to store all n 2 values. The naive implementation with loops is not memory consuming because it only stores the non-zero values, however it is significantly slow and impractical to use. Finally, our implementation is both fast and memory efficient because it only computes and stores the non-zero values.<ref type="foot" target="#foot_1">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Autoregressive Language Modeling</head><p>Autoregressive or left-to-right language modeling is loosely defined as estimating the probability distribution of an existing token/character given its previous tokens/characters in an input sequence. This task is considered one of the fundamental tasks in natural language and recent prior work on modeling long sequences using transformers has relied on this task as their primary evaluation <ref type="bibr" target="#b18">(Dai et al., 2019;</ref><ref type="bibr" target="#b41">Rae et al., 2020;</ref><ref type="bibr" target="#b43">Sukhbaatar et al., 2019)</ref>. Similarly, we develop and evaluate our model on autoregressive language modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Attention Pattern</head><p>For autoregressive language modeling we use our dilated sliding window attention. Following <ref type="bibr" target="#b43">Sukhbaatar et al. (2019)</ref> we use differing window sizes across the layers. In particular, we use small window sizes for the lower layers and increase window sizes as we move to higher layers. This allows the top layers to learn higher-level representation of the entire sequence while having the  <ref type="bibr" target="#b18">(Dai et al., 2019)</ref> 41M -1.06 Reformer <ref type="bibr" target="#b29">(Kitaev et al., 2020)</ref> --1.05 Adaptive <ref type="bibr" target="#b43">(Sukhbaatar et al., 2019)</ref> 39M 1.04 1.02 BP-Transformer <ref type="bibr" target="#b52">(Ye et al., 2019)</ref> 38M -1.02 Our Longformer 41M 1.02 1.00 We do not use dilated sliding windows for lower layers to maximize their capacity to learn and utilize the immediate local context. For the higher layers, we use a small amount of increasing dilation only on 2 heads. This gives the model the ability to directly attend to distant tokens without sacrificing local context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment Setup</head><p>Task and Datasets We focus on character-level autoregressive language modeling because the sequences are naturally longer than those of wordlevel language modeling, making them suitable for evaluating our model. We used text8 and enwik8 <ref type="bibr" target="#b34">(Mahoney, 2009)</ref> for evaluation, two standard and widely used character-level language modeling datasets.</p><p>Training Ideally, we would like to train our model on the largest window size and sequence length we can fit in a modern GPU memory. However, we found that the model needs a large number of gradient updates to learn the local context first; before learning to utilize longer context. To accom-modate this, we adopt a staged training procedure where we increase the attention window size and sequence length across multiple training phrases. In particular, in the first phase we start with a short sequence length and window size, then on each subsequent phase, we double the window size and the sequence length, and halve the learning rate. This makes training fast, while keeping the slow part (longest sequences and window sizes) to the end. We train the model over 5 total phases with starting sequence length of 2,048 and ending sequence length of 23,040 on the last phase (see Appendix A for detailed configurations of each phase).</p><p>Evaluation At evaluation time, we are able to run our model on sequence of length 32,256. Following prior work <ref type="bibr" target="#b18">(Dai et al., 2019;</ref><ref type="bibr" target="#b43">Sukhbaatar et al., 2019)</ref>, during evaluation we split the dataset into overlapping sequences of size 32,256 with a step of size 512, and report the performance on the last 512 tokens on the sequence.</p><p>Hyperparameters Our model only specifies how the self-attention component works, and it is agnostic to the other design choices for the transformer model. We use relative position embeddings with sinusoidal weights as in <ref type="bibr" target="#b18">Dai et al. (2019)</ref>. We use two different model sizes, a small (12 layers, 512 hidden size) model as in <ref type="bibr" target="#b18">Dai et al. (2019)</ref>, and a large (30 layers, 512 hidden size) model as in <ref type="bibr" target="#b14">Child et al. (2019)</ref>. We employed mixed precision training (floating points 16 and 32) using apex<ref type="foot" target="#foot_2">4</ref> to reduce memory consumption and speed-up training. However, we kept the attention computation in fp32 to avoid numerical instability issues. <ref type="foot" target="#foot_3">5</ref> We used gradient checkpointing <ref type="bibr" target="#b13">(Chen et al., 2016)</ref> to reduce memory usage, and ran our experiments on 48GB RTX8000 GPUs. Refer to Appendix A for a more detailed list of hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Results</head><p>Tab. 2 and 3 summarize evaluation results on text8 and enwik8 datasets. We achieve a new state-of-the-art on both text8 and enwik8 using the small models with BPC of 1.10 and 1.00 on text8 and enwik8 respectively, demonstrating the effectiveness of our model.</p><p>For large models, given how expensive these experiments are, and following recent work <ref type="bibr" target="#b29">(Kitaev et al., 2020;</ref><ref type="bibr" target="#b41">Rae et al., 2020)</ref>   <ref type="bibr">et al., 2020)</ref> are not good fit for the pretrainingfinetuning paradigm as discussed in ยง2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Ablation Study</head><p>To show the importance of the design choices of our attention patterns, we tried different variants and report their controlled experiment results. To make the ablation study more manageable, we train each configuration for 150K steps<ref type="foot" target="#foot_4">6</ref> with phase 1 configuration on a small model on text8, then report the BPC performance on the dev set. The top of Tab. 4 demonstrates the impact of different ways of configuring the window sizes per layer. We observe that increasing the window size from the bottom to the top layer leads to the best performance, arranging them in the reverse way leads to worse performance, and using a fixed window size (the average of window sizes of the other configuration) leads to a performance that it is in between. The bottom of Tab. 4 shows the impact of adding dilation. Adding some dilation to two heads leads to some improvement compared with no dilation at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Pretraining and Finetuning</head><p>Current state-of-the-art systems for many NLP tasks finetune a pretrained model with task supervision (e.g. BERT). One of our main motivations is to develop such a model suitable for long document tasks. To do so, we pretrained Longformer  We pretrain Longformer with masked language modeling (MLM), where the goal is to recover randomly masked tokens in a sequence. Since MLM pretraining is expensive, we continue pretraining from the RoBERTa <ref type="bibr" target="#b32">(Liu et al., 2019)</ref> released checkpoint, while only making the minimal changes necessary to support Longformer's attention mechanism. Note that our attention pattern can be plugged into any pretrained transformer model without the need to change the model architecture.</p><p>Attention Pattern We use the sliding window attention with window size of 512 on all layers. This matches RoBERTa's sequence length, and therefore uses the same amount of computation as RoBERTa. <ref type="foot" target="#foot_6">8</ref>Position Embeddings RoBERTa uses learned absolute position embeddings with the maximum position being 512. To support longer documents, we add extra position embeddings to support up to position 4,096. To leverage RoBERTa's pretrained weights, instead of randomly initializing the new position embeddings, we initialize them by copying the 512 position embeddings from RoBERTa multiple times as analysis of BERT's attention heads shows a strong learned bias to attending to local context, including the previous or next token <ref type="bibr" target="#b16">(Clark et al., 2019)</ref>. Using the copy initialization preserves this local structure everywhere except at the partition boundaries. Despite its simplicity, we found this to be a very effective (see Tab. 6), allowing Longformer pretraining to rapidly converge with a small number of gradient updates.</p><p>Pretraining Data In order to allow the model to learn long dependencies in pretraining, we compiled a corpus of long documents. Some of these data sources were also included in the original RoBERTa pretraining including the Books corpus <ref type="bibr" target="#b54">(Zhu et al., 2015)</ref> plus English Wikipedia. We additionally included one third of a subset of the Realnews dataset <ref type="bibr" target="#b53">(Zellers et al., 2019)</ref> with documents longer than 1,200 tokens as well as one third of the Stories (Trinh and Le, 2018) corpus. Our goal was to include a mix of long and short documents to both allow the model to learn longer dependencies while not to forget information from the original RoBERTa pretraining. The statistics of the pretraining data is shown in Tab. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Continued MLM Pretraining</head><p>We train 9 two sizes of Longformer, a base model and a large model. Both models are trained for 65K gradient updates with sequences length 4,096, batch size 64 (2 18 tokens), maximum learning rate of 3e-5, linear warmup of 500 steps, followed by a power 3 polynomial decay. The rest of the hyperparameters are the same as RoBERTa.</p><p>Tab. 6 shows the BPC on the development set of our training corpus. The first row shows a 1.846 BPC using RoBERTa-base, which is comparable to the 1.880 BPC reported on the RoBERTa paper on their corpus. This indicates our training corpus is from a distribution close to that used to train RoBERTa. The following two rows show the performance of Longformer before pretraining with randomly initialized position embeddings and with copied position embeddings. The significant difference indicates the importance of the copy initialization, and the relative small difference between the RoBERTa BPC and the initialized BPC indicates that our sliding window attention is working well with the RoBERTa weights. The following two rows show the impact of continuing pretraining. Traininig for 2K steps improves BPC from 1.957 to 1.753, which further decreases to 1.705 after 65K steps, demonstrating the model is learning to better utilize the sliding window attention and longer context. Similar patterns are observed with RoBERTa-large and Longformer-large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Tasks</head><p>We apply Longformer to multiple long document tasks, including QA, coreference resolution and classification. Tab. 7 shows the evaluation datasets 9 using fairseq <ref type="bibr">(Ott et al.,</ref>   have contexts significantly longer than 512 wordpieces. Our primary goal is to evaluate whether our attention mechanism can act as a replacement for the standard self-attention mechanism in BERT style models, and to perform controlled trials against a strong baseline. We are also interested in evaluating whether we can replace complicated task specific models necessitated by BERT's limited context with simpler models that just concatenate all available context into a single sequence.</p><p>Our baseline is a RoBERTa based model that breaks the context into the longest possible segment, passes each individually through RoBERTa, and concatenates the activations for further processing. For QA tasks, we also concatenate the question to each segment so that RoBERTa can condition it's contextual representations of the context on the question. The Longformer variant replaces the RoBERTa self-attention mechanism with our windowed attention used during pretraining, plus a task motivated global attention. The global attention uses additional linear projections ( ยง3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question answering</head><p>We consider three datasets with long contexts: WikiHop <ref type="bibr" target="#b48">(Welbl et al., 2018)</ref>, Wikipedia setting of TriviaQA <ref type="bibr" target="#b25">(Joshi et al., 2017)</ref>, and HotpotQA, distractor dev setting <ref type="bibr" target="#b51">(Yang et al., 2018)</ref>. <ref type="foot" target="#foot_7">10</ref>For WikiHop and TriviaQA we follow the simple QA model of BERT <ref type="bibr" target="#b19">(Devlin et al., 2019)</ref> dataset-specific prediction layer. WikiHop uses a classification layer for the candidate while Trivi-aQA uses the loss function of <ref type="bibr" target="#b15">Clark and Gardner (2017)</ref> to predict answer span. We include global attention to question tokens and answer candidates for WikiHop and to question tokens for TriviaQA.</p><p>For HotpotQA, we experiment with two different models, a single stage multitask model that jointly predicts evidence and answer spans, and a twostage model that first extracts evidence paragraphs and passes them to a second stage for answer extraction. The single stage model combines a span extraction loss with a question type (yes/no/span) classification head over the first token and an evidence extraction loss predicted from special tokens at the end of sentences and paragraphs. We also include global attention to sentence and paragraph tokens. Note that both approaches are significantly simpler than recent SOTA models that include pipeline approaches of multiple stages and customized architectures (e.g., <ref type="bibr" target="#b46">(Tu et al., 2019;</ref><ref type="bibr" target="#b11">Chen et al., 2019;</ref><ref type="bibr" target="#b45">Tu et al., 2020;</ref><ref type="bibr" target="#b24">Groeneveld et al., 2020)</ref>). See Appendix B for further details about the models and hyperparameters.</p><p>Coreference Resolution We use OntoNotes <ref type="bibr" target="#b38">(Pradhan et al., 2012)</ref>, and the model from <ref type="bibr" target="#b26">Joshi et al. (2019)</ref>, a modification of the system from <ref type="bibr" target="#b31">Lee et al. (2018)</ref> to replace ELMo with BERT. The Longformer system is a straightforward adaption of the baseline model by replacing RoBERTa with Longformer and extending the sequence length. We didn't use global attention for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document Classification</head><p>We evaluate on IMDB <ref type="bibr" target="#b33">(Maas et al., 2011)</ref> and Hyperpartisan news detection <ref type="bibr" target="#b27">(Kiesel et al., 2019)</ref> datasets. 11 IMDB is a standard sentiment classification datasets consisting of movie reviews. While most documents in this dataset are short, about 13.6% of them are 11 For Hyperpartisan we split the training data into train/dev/test sets using standard 90/10/10 splits and performed each experiment five times with different seeds to control variability associated with the small dataset. larger than 512 wordpieces (Tab. 7). Documents in Hyperpartisan are relatively long, and it is small with only 645 documents making it a good test for Longformer's ability to adapt to limited data. We use global attention on the [CLS] token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>Main Result Tab. 8 summarizes the results of all our finetuning experiments. We observe that Longformer consistently outperforms the RoBERTa baseline. Its performance gain is especially obvious for tasks that require long context such as WikiHop and Hyperpartisan. For TriviaQA, the improvement is more modest as the local context is often sufficient to answer the question. In the case of HotpotQA, the supporting fact auxiliary supervision allows models to easily find relevant contexts and then focus on local context, leading to smaller gains. This is contrasted with WikiHop that only includes distant supervision of intermediate reasoning chains, where our approach excels by reasoning over the entire context. On the IMDB and OntoNotes datasets the performance gains are smaller. For IMDB, the majority of the dataset consists of short documents and thus it is expected to see smaller improvements. For OntoNotes, we found that the distance between any two mentions is typically quite small so that a baseline that processes smaller chunks separately is able to stitch together mentions into coreference chains without considering cross chunk interactions.</p><p>Longformer-large for QA We also evaluate the performance of Longformer-large on long context QA tasks. Tab. 9 shows that our Longformer-large achieves new state-of-the-art results on WikiHop and TriviaQA by large margins (3.6 and 4 points respectively). Tab. 10 summarizes results of Hot-potQA, and, as expected, using Longformer-large improves the result compared to Longformer-base. The two-stage model improves the results even further, likely because of the increased capacity that allows each stage to specialize on one task.  <ref type="bibr" target="#b22">(Glaร et al., 2019)</ref> (not GNN) 79.4 86.2 -HGN <ref type="bibr" target="#b20">(Fang et al., 2019)</ref> 81.0 87.9 73.0 C2F Reader <ref type="bibr" target="#b42">(Shao et al., 2020)</ref> --73.9</p><p>Table <ref type="table" target="#tab_0">10</ref>: HotpotQA results, distractor setting on the dev set. All numbers are F1 scores. <ref type="foot" target="#foot_9">13</ref>This model matches performance of TAP2 <ref type="bibr" target="#b22">(Glaร et al., 2019)</ref>, which is the the best performing single model that doesn't use a form of graph neural networks (GNN; Kipf and Welling, 2017). However, our model is simpler than TAP2 where they use a three-stage approach consisting of extracting paragraphs, evidence sentences and finally answer spans with an additional specialized span selection pretraining. All the better performing models for HotpotQA <ref type="bibr" target="#b42">(Shao et al., 2020;</ref><ref type="bibr" target="#b20">Fang et al., 2019)</ref> use multi-stage approaches plus GNNs or graph network of entities, which seem to encode an important inductive bias for the task.<ref type="foot" target="#foot_8">12</ref> Furthermore, the 10 paragraphs come from 10 different documents and the concatenated context is therefore not coherent. In this case we suspect that Longformer has less chance to use information from pretraining due to difference between the pretraining and finetuning input structure. Additional pretraining tasks could help further improve results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablations on WikiHop</head><p>Tab. 11 presents an ablation study for WikiHop. All results use Longformer-base, trained for 5 epochs with identical hyperparameters except where noted.</p><p>Longformer benefits from longer sequences, global attention, separate projection matrices for global attention, MLM pretraining, and longer training.</p><p>In addition, when configured as in RoBERTa-base (seqlen: 512, and n 2 attention) Longformer performs slightly worse then RoBERTa-base, confirm- ing that performance gains are not due to additional pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>We present Longformer, a transformer-based model that is scalable for processing long documents and that makes it easy to perform a wide range of document-level NLP tasks without chunking/shortening the long input and without complex architecture to combine information across these chunks. Longformer employs an attention pattern that combines local and global information while also scaling linearly with the sequence length.</p><p>Longformer achieves state-of-the-art results on the character-level language modeling tasks of text8 and enwik8. When pretrained, Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on Wiki-Hop and TriviaQA. For future work, we would like to explore other attention patterns that are more efficient by dynamically adapting to the input. We also would like to apply our model to other relevant long document tasks such as summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Character LM Hyperparameters</head><p>We evaluate on text8 and enwik8, both contain 100M characters from Wikipedia split into 90M, 5M, 5M for train, dev, test. Our implementation is based on the Transformer-XL <ref type="bibr" target="#b18">(Dai et al., 2019</ref>) code<ref type="foot" target="#foot_10">14</ref> with the memory mechanism disabled. Our hyperparameters and stage configurations are listed in Tab. 12. Our CUDA kernel supports the autoregressive mode where each token attends to a window of previous tokens only. Our implementation also includes a version of the relative position embedding that is compatible with our dilated sliding window attention.</p><p>We ran the small model experiments on 4 RTX8000 GPUs for 16 days. For the large model, we ran experiments on 8 RTX8000 GPUs for 13 days. Most of our hyperparameter search is similar to the ablation in Tab. 4 where we run the configuration for 150K steps on text8. We experimented with absolute position embeddings and learned position embeddings, dropout values of [0.1, 0.2] (small model) and [0.1, 0.4] (large model), prelayernorm and post-layernorm <ref type="bibr" target="#b50">(Xiong et al., 2020)</ref>, learning rate (LR) of phase1 of values [2.5e-5, 5e-4, 1e-4] constant and cosine LR schedules, and different configurations for dilation (on all heads, on 2 heads, no dilation). Number of gradient updates/phase reported in Tab. 12 is determined by running each phase until the validation BPC stops getting better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Task specific model details</head><p>All the QA and classification models are implemented using PyTorch-Lightning<ref type="foot" target="#foot_11">15</ref> .</p><p>WikiHop Instances in WikiHop consist of: a question, answer candidates (ranging from two candidates to 79 candidates), supporting contexts (ranging from three paragraphs to 63 paragraphs), and the correct answer. The dataset does not provide any intermediate annotation for the multihop reasoning chains, requiring models to instead infer them from the indirect answer supervision.</p><p>To prepare the data for input to Longformer and RoBERTa, we first tokenize the question, answer candidates, and support contexts using RoBERTa's wordpiece tokenizer.</p><p>Then we concatenate the question and answer candidates with special tokens as</p><formula xml:id="formula_1">[q] question [/q] [ent] candidate1 [/ent] ... [ent] candidateN [/ent].</formula><p>The contexts are also concatenated using RoBERTa's document delimiter tokens as separators: &lt;/s&gt; context1 &lt;/s&gt; ... &lt;/s&gt; contextM &lt;/s&gt;. The special tokens [q], [/q], [ent], [/ent] were added to the RoBERTa vocabulary and randomly initialized before task finetuning.</p><p>After preparing the input data, we compute activations from the top layer of each model as follows. We take the question and answer candidates and concatenate them to as much context as possible up to the model sequence length (512 for RoBERTa, 4,096 for Longformer), run the sequence through the model, collect the output activations, and repeat until all of the context is exhausted (for all models except Longformer-large, where we just include the first 4,096 length sequence due to memory requirements). Then all activations for all chunks are concatenated into one long sequence. In the case of Longformer, we use global attention to the entire question and answer candidate sequence.</p><p>For prediction, we attach a linear layer to each [ent] that outputs a single logit, average over all logits for each candidate across the chunks, apply a softmax and use the cross entropy loss with the correct answer candidate.</p><p>Training used the Adam optimizer with linear warmup over 200 gradient updates to a maximum LR, and linear decay over the remainder of training. We used gradient accumulation to effective batch size of 32 instances, checking the development accuracy every 250 gradient updates and reported the maximum development accuracy. Other hyperparameters (dropout, weight decay) were identical to RoBERTa pretraining.</p><p>In general, we ran minimal hyperparameter trials, but for fair comparison between Longformer and RoBERTa ran an identical hyperparameter search with Longformer-base and RoBERTa-base. This consisted of a grid search of LR in [2e-5, 3e-5, 5e-5] and number epochs in <ref type="bibr">[5,</ref><ref type="bibr">10,</ref><ref type="bibr">15]</ref>. The best Longformer-base configuration used lr=3e-5, 15 epochs. We ran two hyperparameter trials for Longformer-large, lr=3e-5 and number epochs in <ref type="bibr">[5,</ref><ref type="bibr">15]</ref> (the 5 epoch model had higher dev accuracy of 77.6, and was the single model submitted to the public leaderboard for test set evaluation). All mod-  els were trained on a single RTX8000 GPU, with Longformer-base taking about a day for 5 epochs.</p><p>TriviaQA TriviaQA has more than 100K question, answer, document triplets for training. Documents are Wikipedia articles, and answers are named entities mentioned in the article. The span that answers the question is not annotated, but it is found using simple text matching. Similar to WikiHop, we tokenize the question and the document using RoBERTa's tokenizer, then form the input as [s] question [/s] document [/s]. We truncate the document at 4,096 wordpiece to avoid it being very slow. Afterwards, we get the activations from RoBERTa and Longformer similar to WikiHop (discussed above). We use global attention on all question tokens.</p><p>For prediction, we add one layer that predicts the beginning and end of the answer span. Because of the distant supervision nature of the training data (no gold answer spans), we use the loss function of <ref type="bibr" target="#b15">Clark and Gardner (2017)</ref> which works like an OR that the model only needs to get one answer span right, not all of them.</p><p>Hyperparameters of the best configuration are listed in Tab. 13. All other hyperparameters are similar to RoBERTa's. For hyperparameter search, we only tuned LR for the RoBERTa baseline and tried rates [3e-5, 5e-5, 1e-4], then used the best, which is 3e-5, for all subsequent experiments with no further tuning. We trained the Longformer-large with the best configuration once and submitted its output to the leaderboard. We ran our experiments on 32GB V100 GPUs. Small model takes 1 day to train on 4 GPUs, while large model takes 1 day on 8 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HotpotQA</head><p>HotpotQA dataset involves answering questions from a set of 10 paragraphs from 10 different Wikipedia articles where 2 paragraphs are relevant to the question and the rest are distractors. It includes 2 tasks of answer span extraction and evidence sentence identification. Our model for HotpotQA combines both answer span extraction and evidence extraction in one joint model. We also experimented with a two-stage model with similar setup that uses longformer first for evidence extraction and then extract answers using a BERT-based QA model. The second stage of our two-stage model is based on <ref type="bibr" target="#b24">Groeneveld et al. (2020)</ref>. Similar to Wikihop and TriviaQA, to prepare the data for input to Longformer and RoBERTa, we concatenate question and then all the 10 paragraphs in one long context. We particularly use the following input format with special tokens: "</p><formula xml:id="formula_2">[CLS] [q] question [/q] [p] sent 1,1 [s] sent 1,2 [s] ... [p] sent 2,1 [s] sent 2,2 [s] ..."</formula><p>where [s] and [p] are special tokens representing sentences and paragraphs. The special tokens were added to the RoBERTa vocabulary and randomly initialized before task finetuning. For Longformer, we use global attention to input tokens as well as sentence and paragraph tokens. For answer span extraction we use BERT's QA model <ref type="bibr" target="#b19">(Devlin et al., 2019)</ref> with addition of a question type (yes/no/span) classification head over the first special token ([CLS]). For evidence extraction we apply 2 layer feedforward networks on top of the representations corresponding to sentence and paragraph tokens to get the corresponding evidence prediction scores and use binary cross entropy loss to train the model. We combine span, question classification, sentence, and paragraphs losses and train the model in a multitask way using linear combination of losses. Our experiments are done on RTX8000 GPUs and training each epoch takes approximately half a day on 4 GPUs. We trained the model using Adam optimizer with linear warmup (1000 steps) and linear decay. We used minimal hyperpareter tuning using LRs of 3e-5 and 5e-5 and epochs of 3 to 7 and found the model with LR of 3e-5 and 6 epochs to work best. We conduct the same hyperparameter search for the RoBERTa baseline as well. The rest of hyperparameters are reported in Tab 13. Coreference model details The coreference model is a straightforward adaptation of the coarseto-fine BERT based model from <ref type="bibr" target="#b26">Joshi et al. (2019)</ref>. After preprocessing each document with the RoBERTa wordpiece tokenizer, it splits each document into non-overlapping segments up to the maximum sequence length, then concatenates the activations for the coarse-to-fine clustering stage that forms coreference clusters. The maximum sequence length was 384 for RoBERTa-base, chosen after three trials from <ref type="bibr">[256,</ref><ref type="bibr">384,</ref><ref type="bibr">512]</ref> using the default hyperparameters in the original implementation. <ref type="foot" target="#foot_12">16</ref> For Longformer-base the sequence length was 4,096. Similar to the original implementation, different learning rates were used for the pretrained RoBERTa parameters and the randomly initialized task parameters. Using a larger learning rate in the task parameters allows the optimizer to adjust them farther from their randomly initialized values without destroying the information in the pretrained RoBERTa parameters. Hyperparameter searches were minimal and consisted of grid searches of  for both RoBERTa and Longformer for a fair comparison. The best configuration for Longformer-base was RoBERTa lr=1e-5, task lr=1e-4. All other hyperparameters were the same as in the original implementation. Training takes about 10 hours on a single GPU.</p><p>Our implementation is a superhack that involves PyTorch and Tensorflow sharing a single process and GPU. To avoid re-implementing the complicated coarse-to-fine logic from Tensorflow in PyTorch (that involves a highly optimized custom GPU kernel originally released by <ref type="bibr" target="#b31">Lee et al. (2018)</ref>), we devised a system where the lower transformer portion of the model passes activations and gradients back and forth between PyTorch and Tensorflow. The input tensors are first run through the transformer in PyTorch, the activations are collected from the top layer, transferred from GPU to CPU then from CPU to Tensorflow and back to GPU to run the coarse-to-fine clustering and compute the loss. Then gradients are back propogated in Tensorflow to the top of the transformer and the process reversed to transfer them to PyTorch for back propogation through the remainder of the model. Separate optimizers are maintained with identical LR schedules for parameter updates. The overhead in this approach is minimal compared to the overall cost of running the model.</p><p>Text classification For classification, following BERT, we used a simple binary cross entropy loss on top of a first [CLS] token with addition of global attention to <ref type="bibr">[CLS]</ref>. We used Adam optimizer with batch sizes of 32 and linear warmup and decay with warmup steps equal to 0.1 of the total training steps. For both IMDB and Hyperpartisan news we did grid search of LRs [3e-5, 5e-5] and epochs <ref type="bibr">[10,</ref><ref type="bibr">15,</ref><ref type="bibr">20]</ref> and found the model with [3e-5] and epochs 15 to work best. Experiments were done on a single RTX8000 GPU.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Longformer's memory usage scales linearly with the sequence length, unlike the full self-attention mechanism that runs out of memory for long sequences on current GPUs. Longformer's GPU-kernel is nearly as fast as the highly optimized full self-attention operation, and nearly 6X faster than naive Pytorch.</figDesc><graphic url="image-1.png" coords="1,329.10,222.54,174.61,75.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparing the full self-attention pattern and the configuration of attention patterns in our Longformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of prior work on adapting Transformers for long documents. ltr: left-to-right.</figDesc><table><row><cell>Model</cell><cell cols="4">attention char-lm other pretrain</cell></row><row><cell></cell><cell>matrix</cell><cell></cell><cell>tasks</cell><cell></cell></row><row><cell>Transformer-XL (2019)</cell><cell>ltr</cell><cell>yes</cell><cell>no</cell><cell>no</cell></row><row><cell>Adaptive Span (2019)</cell><cell>ltr</cell><cell>yes</cell><cell>no</cell><cell>no</cell></row><row><cell>Compressive (2020)</cell><cell>ltr</cell><cell>yes</cell><cell>no</cell><cell>no</cell></row><row><cell>Reformer (2020)</cell><cell>sparse</cell><cell>yes</cell><cell>no</cell><cell>no</cell></row><row><cell>Sparse (2019)</cell><cell>sparse</cell><cell>yes</cell><cell>no</cell><cell>no</cell></row><row><cell cols="2">BP-Transformer (2019) sparse</cell><cell>yes</cell><cell>MT</cell><cell>no</cell></row><row><cell>Blockwise (2019)</cell><cell>sparse</cell><cell>no</cell><cell>QA</cell><cell>yes</cell></row><row><cell>Our Longformer</cell><cell>sparse</cell><cell cols="3">yes multiple yes</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Small model BPC on text8 &amp; enwik8</figDesc><table><row><cell>Model</cell><cell cols="2">#Param Test BPC</cell></row><row><cell>Transformer-XL (18 layers)</cell><cell>88M</cell><cell>1.03</cell></row><row><cell>Sparse (Child et al., 2019)</cell><cell>โ100M</cell><cell>0.99</cell></row><row><cell>Transformer-XL (24 layers)</cell><cell>277M</cell><cell>0.99</cell></row><row><cell>Adaptive (Sukhbaatar et al., 2019)</cell><cell>209M</cell><cell>0.98</cell></row><row><cell>Compressive (Rae et al., 2020)</cell><cell>277M</cell><cell>0.97</cell></row><row><cell>Our Longformer</cell><cell>102M</cell><cell>0.99</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance of large models on enwik8</figDesc><table /><note>lower layers capture local information. In addition, it provides balance between efficiency (smaller window sizes are less computationally expensive due to fewer nonzero values) and performance (larger window sizes have richer representation power and often result in performance improvements).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>, we are only</figDesc><table><row><cell>Model</cell><cell>Dev BPC</cell></row><row><cell>Decreasing w (from 512 to 32)</cell><cell>1.24</cell></row><row><cell>Fixed w (= 230)</cell><cell>1.23</cell></row><row><cell>Increasing w (from 32 to 512)</cell><cell>1.21</cell></row><row><cell>No Dilation</cell><cell>1.21</cell></row><row><cell>Dilation on 2 heads</cell><cell>1.20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Top: changing window size across layers. Bot-</cell></row><row><cell>tom: with/without dilation (@ 150K steps on phase1)</cell></row><row><cell>evaluating on enwik8. Tab. 3 shows that Long-</cell></row><row><cell>former outperforms the comparable Transformer-</cell></row><row><cell>XL model, matches the performance of the compa-</cell></row><row><cell>rable Sparse Transformer (Child et al., 2019), and</cell></row><row><cell>matches or slightly underperforms recent models</cell></row><row><cell>that have more than twice the number of parameters.</cell></row><row><cell>It is worth noting that Adaptive Span (Sukhbaatar</cell></row><row><cell>et al., 2019) and Compressive Transformer (Rae</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Pretraining data on a document corpus and finetune it for six tasks, including classification, QA and coreference resolution. The resulting model can process sequences up to 4,096 tokens long (8 times longer than BERT) 7 .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>: Average and 95th percentile of context length</cell></row><row><cell>of datasets in wordpieces. WH: WikiHop, TQA: Triv-</cell></row><row><cell>iaQA, HQA: HotpotQA, ON: OntoNotes, HY: Hyper-</cell></row><row><cell>partisan news</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Summary of finetuning results on QA, coreference resolution, and document classification. Results are on the development sets comparing our Longformer-base with RoBERTa-base. TriviaQA, Hyperpartisan metrics are F1, WikiHop and IMDB use accuracy, HotpotQa is joint F1, OntoNotes is average F1.</figDesc><table><row><cell>, and</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Leaderboard results of Longformer-large</figDesc><table><row><cell>Model</cell><cell cols="2">WikiHop TriviaQA</cell></row><row><cell>Current SOTA</cell><cell>78.3</cell><cell>73.3</cell></row><row><cell>Longformer-large</cell><cell>81.9</cell><cell>77.3</cell></row><row><cell>Model</cell><cell></cell><cell>ans. supp. joint</cell></row><row><cell>RoBERTa-base</cell><cell></cell><cell>73.5 83.4 63.5</cell></row><row><cell>Longformer-base</cell><cell></cell><cell>74.3 84.4 64.4</cell></row><row><cell>Longformer-large</cell><cell></cell><cell>78.8 86.0 69.5</cell></row><row><cell>Longformer-large (2 stage)</cell><cell></cell><cell>81.0 85.8 71.4</cell></row><row><cell>TAP2</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>WikiHop development set ablations</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>Hyperparameters for the best performing model for character-level language modeling</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 13 :</head><label>13</label><figDesc>Hyperparameters of the QA models. All models use a similar scheduler with linear warmup and decay.</figDesc><table><row><cell>Param</cell><cell cols="3">WikiHop TriviaQA HotpotQA</cell></row><row><cell>Epochs</cell><cell>15</cell><cell>5</cell><cell>6</cell></row><row><cell>LR</cell><cell>3e-5</cell><cell>3e-5</cell><cell>5e-5</cell></row><row><cell>Warmup steps</cell><cell>200</cell><cell>1000</cell><cell>1000</cell></row><row><cell>Batch size</cell><cell>32</cell><cell>32</cell><cell>32</cell></row><row><cell>Optimizer</cell><cell>Adam</cell><cell>Adam</cell><cell>Adam</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">SQuAD contexts typically fit within the 512 limit, and MRQA is constructed by dropping long-document examples.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">It is worth noting that theoretically, a perfectly optimized sliding window attention operation should be faster than the n 2 computation. However, achieving this level of performance requires special knowledge of low-level GPU programming, similar to implementing a highly optimized matrix multiplication. Our current implementation is sufficiently fast and practical to use.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">https://github.com/NVIDIA/apex</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">We found that using fp16 in attention operation results in floating point overflow and NaNs in later stages of training.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4">An obvious caveat is that there is a chance the end performance will not agree with the performance at step 150K. However, this is a reasonable approximation that saves the huge cost of running all these configurations to completion.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5">Sequences up to 16K are possible on current GPUs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6">We tried adding the additional dilation pattern on a few heads as in ยง4.1 but found it to hurt performance, likely because it is not compatible with the pretrained RoBERTa weights. We suspect that retraining such model from scratch might be needed to get improved performance</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_7">We use the full version of TriviaQA and HotpotQA, not the simplified versions in MRQA<ref type="bibr" target="#b21">(Fisch et al., 2019)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_8">We can encode this inductive bias using global attention over entities, but we leave this for future work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_9">We report numbers from dev set scores of published papers. Missing values are not publicly available. Leaderboard test results will be included in the future.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_10">https://github.com/kimiyoung/ transformer-xl</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_11">https://github.com/PyTorchLightning/ pytorch-lightning</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_12">https://github.com/mandarjoshi90/coref</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We would like to thank Noah Smith, Dan Weld, Dirk Groeneveld, Kyle Lo, Daniel King and Doug Downey for helpful discussions and feedback, and the AI2 infrastructure team for technical support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>Longformer</surname></persName>
		</author>
		<idno>seqlen: 4,096) 73.8</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Roberta-Base</forename></persName>
		</author>
		<idno>seqlen: 512) 72.4 / -1.4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>Longformer</surname></persName>
		</author>
		<idno>75.0 / +1.2</idno>
		<imprint/>
	</monogr>
	<note>seqlen: 4,096, 15 epochs</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><surname>Longformer</surname></persName>
		</author>
		<idno>71.7 / -2.1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Longformer (seqlen: 512, attention: window)</title>
		<idno>68.8 / -5.0</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Longformer (seqlen: 2,048)</title>
		<idno>73.1 / -0.7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><surname>Longformer</surname></persName>
		</author>
		<idno>73.2 / -0.6</idno>
		<title level="m">MLM pretraining)</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Longformer (no linear proj</title>
		<idno>72.2 / -1.6</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Longformer (no linear proj. no global atten</title>
		<idno>65.5 / -8.3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Character-level language modeling with deeper self-attention</title>
		<author>
			<persName><forename type="first">References</forename><surname>Rami Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dokook</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandy</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reading wikipedia to answer opendomain questions</title>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multi-hop question answering via reasoning chains</title>
		<author>
			<persName><forename type="first">Jifan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Ting</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<idno>ArXiv, abs/1910.02610</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TVM: An automated end-to-end optimizing compiler for deep learning</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meghan</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="578" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Training deep nets with sublinear memory cost</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno>ArXiv, abs/1604.06174</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>ArXiv, abs/1904.10509</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simple and effective multi-paragraph reading comprehension</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">What does bert look at? an analysis of bert&apos;s attention</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno>ArXiv, abs/1906.04341</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Hierarchical graph network for multi-hop question answering</title>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<idno>ArXiv, abs/1911.03631</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">MRQA 2019 shared task: Evaluating generalization in reading comprehension</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2nd Machine Reading for Reading Comprehension (MRQA) Workshop at EMNLP</title>
				<meeting>2nd Machine Reading for Reading Comprehension (MRQA) Workshop at EMNLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Span selection pre-training for question answering</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Glaร</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfio</forename><surname>Massimiliano Gliozzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishav</forename><surname>Chakravarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Ferritto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaudani</forename><surname>Bhargav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avirup</forename><surname>Sil</surname></persName>
		</author>
		<idno>ArXiv, abs/1909.04120</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<title level="m">Gpu kernels for block-sparse weights</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A simple yet strong pipeline for HotpotQA. arXiv preprint. Jeremy Howard and Sebastian Ruder</title>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName><surname>Sabhwaral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2018">2020. 2018</date>
		</imprint>
	</monogr>
	<note>Universal language model fine-tuning for text classification</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">BERT for coreference resolution: Baselines and analysis</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SemEval-2019 task 4: Hyperpartisan news detection</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Payam</forename><surname>Adineh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Corney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S19-2145</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Workshop on Semantic Evaluation</title>
				<meeting>the 13th International Workshop on Semantic Evaluation<address><addrLine>Minneapolis, Minnesota, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="829" to="839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Revealing the dark secrets of bert</title>
		<author>
			<persName><forename type="first">Olga</forename><forename type="middle">V</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/IJCNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Higher-order coreference resolution with coarse-tofine inference</title>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized bert pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>ArXiv, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Large text compression benchmark</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Mahoney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName><forename type="first">Aรคron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SSW</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2019</title>
				<meeting>NAACL-HLT 2019</meeting>
		<imprint>
			<publisher>Demonstrations</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<title level="m">Deep contextualized word representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">CoNLL-2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLL -Shared Task</title>
				<meeting><address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Blockwise selfattention for long document understanding</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno>ArXiv, abs/1911.02972</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><surname>Lillicrap</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Is graph structure necessary for multi-hop reasoning?</title>
		<author>
			<persName><forename type="first">Nan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno>ArXiv, abs/2004.03096</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Adaptive attention span in transformers</title>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">A simple method for commonsense reasoning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno>ArXiv, abs/1806.02847</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Graph sequential network for reasoning over sequences</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinke</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Select, answer and explain: Interpretable multi-hop reading comprehension over multiple documents</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bufang</forename><surname>Zhou</surname></persName>
		</author>
		<idno>ArXiv, abs/1911.00484</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Constructing datasets for multi-hop reading comprehension across documents</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="287" to="302" />
		</imprint>
	</monogr>
	<note>Transactions of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno>ArXiv, abs/1901.10430</idno>
	</analytic>
	<monogr>
		<title level="m">Qizhe Xie, Zihang Dai, Eduard H. Hovy, Minh-Thang Luong, and Quoc V</title>
				<imprint>
			<publisher>Unsupervised data augmentation for consistency training</publisher>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Shu Xin Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huishuai</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Wei</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<idno>ArXiv, abs/2002.04745</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Hotpotqa: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Bp-transformer: Modelling long-range context via binary partitioning</title>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno>ArXiv, abs/1911.04070</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Defending against neural fake news</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franziska</forename><surname>Roesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
