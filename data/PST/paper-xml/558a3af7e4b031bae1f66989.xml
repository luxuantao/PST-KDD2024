<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sky/Ground Modeling for Autonomous MAV Flight</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Florida</orgName>
								<address>
									<postCode>32611-6200</postCode>
									<settlement>Gainesville</settlement>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Michael</forename><forename type="middle">C</forename><surname>Nechyba</surname></persName>
							<email>nechyba@mil.ufl.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Florida</orgName>
								<address>
									<postCode>32611-6200</postCode>
									<settlement>Gainesville</settlement>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><forename type="middle">G</forename><surname>Ifju</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Mechanical and Aerospace Engineering</orgName>
								<orgName type="institution">University of Florida</orgName>
								<address>
									<postCode>32611-6250</postCode>
									<settlement>Gainesville</settlement>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sky/Ground Modeling for Autonomous MAV Flight</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">030D9A18ADF7A781BE225FBBE403B303</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, we have implemented a computer-vision based horizon-tracking algorithm for flight stability and autonomy in Micro Air Vehicles (MAVs) <ref type="bibr" target="#b0">[1]</ref>. Occasionally, this algorithm fails in scenarios where the underlying Gaussian assumption for the sky and ground appearances is not appropriate. Therefore, in this paper, we present a general statistical image modeling framework which we use to build prior models of the sky and ground. Once trained, these models can be incorporated into our existing horizon-tracking algorithm. Since the appearances of the sky and ground vary enormously, no single feature is sufficient for accurate modeling; as such, we rely both on color and texture as critical features in our modeling framework. Specifically, we choose hue and intensity for our color representation, and the complex wavelet transform (CWT) for our texture representation. We then use Hidden Markov Tree (HMT) models, which are particularly well suited for the CWT's inherent tree structure, as our underlying statistical models over our feature space. With this approach, we have achieved reliable and robust image segmentation of flight images from on-board our MAVs as well as on more difficult-to-classify sky/ground images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In this paper, we seek to build statistical appearance models that will allow us to segment sky from ground in images and flight video. This goal was inspired by our previous work in horizon tracking for Micro Air Vehicles (MAVs) <ref type="bibr" target="#b0">[1]</ref>. In that work, we developed a real-time, vision-based horizon detection and tracking algorithm for MAVs equipped with on-board video cameras. With this system, we were able to achieve self-stabilized and autonomous flights of MAVs, without any additional inertial or rate sensors. We resorted to vision-based control, since such inertial and rate sensors typically do not yet have the requisite accuracy at the miniature scale required for MAVs, where weight of sensors and other components is of paramount importance.</p><p>Overall, the horizon tracking algorithm works well, especially when the sky and ground distributions are relatively coherent. Occasionally, however, horizon detection fails in scenarios where the underlying Gaussian assumption for the sky and ground appearances is not appropriate. Moreover, the horizon detection algorithm is bootstrapped by assuming that initially the sky occupies the upper part of the image. For complex mission scenarios, this may be an incorrect assumption with potentially fatal consequences to the flight vehicle. For example, we are currently working on deploying MAVs on munitions for post-impact bomb damage assess-ment. In this case, the MAV would separate from the munition prior to impact, and an upright attitude with respect to the ground cannot be guaranteed. Correct identification of sky and ground, therefore, becomes imperative.</p><p>While modeling the appearance of sky and ground regions in images may seem intuitively easy, it is, in fact, a very challenging task. Depending on lighting, weather, landscape, etc., the appearance of the sky and ground can vary enormously. Given the complex variations in our two image classes (i.e. sky and ground), careful consideration must be given to selecting sufficiently discriminating features and a sufficiently expressive modeling framework. Having experimented with color and texture features separately, we conclude that only the feature set that includes both color and texture clues enables accurate statistical modeling for our application <ref type="bibr" target="#b1">[2]</ref>. Previous experiments also suggest that it is important to represent both local as well as regional interdependencies in the feature space. As such, we resort to wavelet-based multi-resolution analysis in the form of the Complex Wavelet Transform (CWT).</p><p>Given our feature selection, we then choose the Hidden Markov Tree (HMT) model <ref type="bibr" target="#b2">[3]</ref> as our statistical model, since it is particularly well suited to the CWT's inherent tree structure. This choice of model imposes Markov dependencies on the states of both color values and wavelet coefficients at adjacent scales of the pyramidal multi-resolution structure. We train the HMTs with the EM algorithm <ref type="bibr" target="#b3">[4]</ref> to obtain a small set of parameters that fully characterize the likelihoods of the two image classes at different scales. Finally, we fuse the posterior likelihoods at each scale, analogous to Choi's <ref type="bibr" target="#b4">[5]</ref> interscale fusion approach, and perform Bayesian segmentation.</p><p>Our approach is distinguished from others, which use wavelets exclusively, by the inclusion of color in the HMT model structure. Incorporating color introduces a number of differences between our models and those in the literature. Moreover, the design of our statistical models was guided by real-time requirements of our MAV flight system, leading to certain design choices that may be sub-optimal if real-time processing constraints had not been an issue. Although it may appear that our vision algorithm is computationally complex, we have come very close to meeting real-time requirements for our MAVs. Reading, subsampling and segmentation of a ¼¢ ¼ image takes only 0.12s on an Athlon 1.8GHz PC. Below, we give an overview of this paper. In Section II, we explain our choice of feature space, reviewing the most important aspects of the HSI color space and properties of the CWT. Next, in Section III, we describe the HMT model and Bayesian multiscale segmentation. Then, in Section IV, we present several examples of sky/ground segmentation. Finally, we conclude with a discussion of our experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. FEATURE SPACE</head><p>For our statistical models, we seek to identify features that lead to improved segmentation performance without unnecessarily increasing computational complexity. As we have already mentioned, color or texture clues by themselves yield poor segmentation results <ref type="bibr" target="#b1">[2]</ref>; therefore, below we consider a feature space that spans both color and texture domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Color</head><p>The color information in a video signal is usually encoded in the RGB color space. Unfortunately, the R, G and B color channels are highly correlated; therefore, we choose the HSI space as a more appropriate color representation for statistical modeling <ref type="bibr" target="#b5">[6]</ref>. In order to simplify our feature space, we examine the Mahalanobis distances for the hue (H), saturation (S) and intensity (I) values in sky and ground training images. Denoting as the sample mean and ¾ as the sample variance, we compute:</p><formula xml:id="formula_0">¾ À ´ × Ý À ÖÓÙÒ À µ ¾ ´ × Ý À µ ¾ • ´ ÖÓÙÒ À µ ¾ ¾ Ë ´ × Ý Ë ÖÓÙÒ Ë µ ¾ ´ × Ý Ë µ ¾ • ´ ÖÓÙÒ Ë µ ¾ ¾ Á ´ × Ý Á ÖÓÙÒ Á µ ¾ ´ × Ý Á µ ¾ • ´ ÖÓÙÒ Á µ ¾<label>(1)</label></formula><p>and observe that for various training data sets ¾ À and ¾ Á are consistently greater than ¾ Ë [2]. Thus, to reduce computational complexity, we choose only the features H and I for our statistical model. Next, we consider the representation of frequency, orientation and location of energy content in an image; in short, we want to define texture-based features. As such, we employ the wavelet transform, due to its inherent representation of texture at different scales and locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Complex Wavelet Transform</head><p>The 2-D Complex Wavelet Transform (CWT) essentially filters rows and columns of an image with a bank of complex bandpass filters, similar to the conventional Discrete Wavelet Transform (DWT) <ref type="bibr" target="#b6">[7]</ref>. Since, each coefficient contains a real and imaginary part, a redundancy of 2:1 is introduced for onedimensional signals. For images, the redundancy increases to 4:1, since two adjacent quadrants of the spectrum are required to fully represent a real 2-D signal. This is achieved by additional filtering with complex conjugates of either row or column filters <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>.</p><p>Despite its higher computational cost, we prefer the CWT over the DWT because of the CWT's following attractive properties. Kingsbury <ref type="bibr" target="#b7">[8]</ref> has shown that the Dual-Tree CWT possesses near shift invariance, unlike the DWT, where small shifts in the input signal induces major changes in coefficient values. Also, the CWT's directional selectivity is greater, producing six bandpass subimages of complex coefficients at each level. The coefficients are strongly oriented at angles ¦½ AE ¦ AE ¦ AE , as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>While it is known that the phase of CWT coefficients is less susceptible to noise corruption than the coefficient magnitudes <ref type="bibr" target="#b8">[9]</ref>, experimental results have shown that phase is not a good feature choice for sky and ground modeling <ref type="bibr" target="#b1">[2]</ref>. Computing the phase of the CWT for orientation angles ¦½ AE ¦ AE ¦ AE , yields virtually indiscernible subimages for sky and ground. Therefore, we consider only the magnitude of CWT coefficients in our representation of texture.</p><p>The magnitudes of CWT coefficients share the following properties of the DWT <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>:</p><p>1) Multi-resolution: CWT represents an image at different scales of resolution in space. 2) Clustering: if the magnitude of a wavelet coefficient is large/small, then the magnitudes of the adjacent coefficients are very likely to also be large/small. 3) Persistence: large/small values of wavelet coefficients tend to propagate through scales.</p><p>These properties naturally give rise to the HMT statistical model, which helps us compute the distribution of pixels belonging to different image classes (as described in the next section).</p><p>To see which sets of orientations tend to be the most discriminating between sky and ground, we once again experiment with the Mahalanobis distances between sky and ground coefficient magnitudes belonging to subimages at different orientation. Computing ¾ ½ AE , ¾ ½ AE , ¾ AE , etc., similarly to the expressions in (1), we observe that for the available sky and ground training images ¾ AE and ¾ AE are consistently the least significant. Therefore, our complete feature space is defined by the H and I color features and the subimages with orientation ¦½ AE and ¦ AE . To benefit from the multiscale presentation of the CWT, we replace the missing ¦ AE</p><p>subimages with H and I images instead, as shown in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>The H and I values at coarser scales are computed as the mean of the corresponding four values at the next higher-resolution scale. Hence, the H and I features also exhibit the clustering and persistence properties to some extent.</p><p>Next we describe the HMT model as an appropriate statistical framework for modeling our chosen feature set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. HIDDEN MARKOV TREE MODEL</head><p>The Hidden Markov Tree (HMT) structure can model both the clustering and persistence properties of the CWT coefficient magnitudes. It consists of a tree structure Ì that assigns a node to each coefficient 1 and connects mutually dependent nodes. Thus, every parent node is vertically connected with its four 2 children at the finer scale, as depicted in Figure <ref type="figure" target="#fig_3">3</ref>. For instance, it is obvious from the figure that</p><formula xml:id="formula_1">Ì ½ AE W ¼ ½ AE W ½ ½ AE ¡ ¡ ¡ W ´Ä ½µ ½ AE (2)</formula><p>Also, note from the figure that we assume that different features are mutually independent. In other words, connecting coefficients that belong only to the same feature, we obtain six mutually independent probability trees: Ì ½ AE , Ì AE , Ì ½¼ AE , Ì ½ AE , Ì À , and Ì Á .</p><p>It is worth noting that we tried to implement the Mixture Memory Markov Model, as proposed in <ref type="bibr" target="#b9">[10]</ref>, to account for the dependencies between features (i.e. probability trees). However, the slightly improved performance in image segmentation did not justify the substantial increase in processing time <ref type="bibr" target="#b1">[2]</ref>. Also, we experimented with the HMT-2 model, developed in <ref type="bibr" target="#b10">[11]</ref>, where a coefficient depends on its two twin parents. Since the context-based fusion method used for Bayesian classification incorporates nine parents (not only 1 Here, coefficient refers to the magnitude of CWT coefficients and/or the H and I color values. 2 Throughout the paper the CWT is assumed to be dyadic.  two), segmentation performance did not improve with the HMT-2. Finally, we note that while we do not consider horizontal dependencies among nodes at the same scale, the clustering property is still well modeled, since adjacent coefficients at one scale have a unique parent. In order to discuss HMT properties, we first need to introduce the following notation. A coefficient of a probability tree Ì Ø at a scale Â is denoted with w Â ÌØ . A node i has one parent node ´ µ and four children nodes ´ µ, such that Â´ ´ µµ Â´ µ ½ and Â´ ´ µµ Â´ µ • ½ .</p><formula xml:id="formula_2">w i LEVEL J LEVEL J -1 LEVEL J + 1 ρ(i) ρ(i)<label>(i) (i)</label></formula><p>As is customary for HMTs <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b11">[12]</ref>, we assign to each observable random variable (RV) w Â ÌØ a hidden RV, state S Â ÌØ , which determines the marginal distribution of the ob- servable coefficient value. The HMT imposes that w Â ÌØ is conditionally independent of all other RVs given its associated state S Â ÌØ . Furthermore, w Â ÌØ is conditionally independent of the entire tree, given its parent state S Â•½</p><p>´ µ ÌØ . Note that the Markov structure is related to state RVs between scales and not to coefficient values. If we assume an M-state Gaussian mixture density for the marginal distribution of w Â ÌØ , the tree Ì Ø is fully characterized by the following parameters:</p><p>1) The probability measure function of the root node:</p><formula xml:id="formula_3">È ´S¼ ÌØ Ñµ Ñ ¾ ¼ Å ½ 2) The transition probability that S Â ÌØ is in a state Ñ, given that S Â•½ ´ µ ÌØ is in a state Ò: Ò Ñ ´Â•½µ Â ÌØ È ´SÂ ÌØ Ñ S Â•½ ´ µ ÌØ Òµ Ñ Ò ¾ ¼ Å ½</formula><p>3) The mean and variance of w Â ÌØ , given S Â ÌØ Ñ:</p><formula xml:id="formula_4">Â ÌØ Ñ ¾ Â ÌØ Ñ Ñ ¾ ¼ Å ½</formula><p>In order to simplify computations and to avoid the risk of overfitting the HMT model, we assume that the statistical parameters at the same scale are equal for all coefficients.</p><p>Therefore, the model parameters are indexed by Â , denoting that they are equal for all nodes i at the scale Â. Finally, we group the parameters for all probability trees into a vector ¢.</p><p>Unlike in <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, we do not assume zero mean values, since such an assumption would lead to substantial model error, especially for the H feature that takes on values in the interval ¼ ¿ ¼ . Also, much better image segmentation is obtained if the number of possible states Å is greater than 2; since this introduces only a negligible increase in computation time, we let Å ¾ (unlike in <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. EM algorithm</head><p>Due to the Markov property and the assumption of probability tree independence, the most likely value for Ô´w Â ¢µ, can be computed by maximizing the joint likelihood:</p><formula xml:id="formula_5">Ô´w Â S Â ¢µ ÌØ Ô´w Â ÌØ S Â ÌØ ¢µ ÌØ Ô´w Â ÌØ S Â ÌØ ¢µÈ´S Â ÌØ ¢µ ÌØ Ô´w Â ÌØ S Â ÌØ ¢µ ¡ ¡È´S ¼ ÌØ Ñ ¢µ Â Ò Ñ Â•½ Â ÌØ (3) where Ø ¾ ¦ ½ AE ¦ AE À Á .</formula><p>The last expression shows all the HMT parameters which must be learned from observations. For training the HMT model, we implement the iterative Expectation-Maximization (EM) algorithm, as proposed in <ref type="bibr" target="#b3">[4]</ref>. In the E step, the state information is propagated throughout the tree by means of the upward-downward algorithm. Here, at step l of the algorithm, the expectation value of the log-likelihood from (3) is computed as follows:</p><formula xml:id="formula_6">É´¢ ¢ Ð µ S Â ÐÒÔ´w Â S Â ¢µ w Â ¢ Ð Ñ ÐÒÔ´w Â S Â ¢µÈ S Â Ñ w Â ¢ Ð (4)</formula><p>Then, in the M step, we compute</p><formula xml:id="formula_7">¢ Ð•½ Ö Ñ Ü ¢ É ¢ ¢ Ð ¡ (5)</formula><p>It has been proved that increasing the É-function is sufficient to increase the likelihood Ô´w Â ¢µ <ref type="bibr" target="#b12">[13]</ref>. We are not concerned with the convergence rate in the training process, because our data base contains long sequences of similar sky and ground images. Hence, ¢ Ð , computed for one image, is used as the input to compute ¢ Ð•½ for the next image of the training data base. Finally, after processing all sky training images, we obtain ¢ × Ý , and similarly, for ground, ¢ ÖÓÙÒ . Thus, the EM algorithm gives us the likelihoods of all coefficients at all scales for a given class, say sky, as follows:</p><formula xml:id="formula_8">Ô´w Â ¢ × Ý µ Ñ Ô´w Â S Â Ñ ¢ × Ý µ (6)</formula><p>Consequently, we are able to perform Bayesian classification at all scales, without significant computational overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multiscale Bayesian Segmentation</head><p>Most segmentation algorithms employ a classification window of some size, which provides statistical information to a classifier. A large classification window produces accurate segmentation of large, homogeneous regions, but poor results along their boundaries. On the other hand, a small window yields unreliable classification. In our case, we require not only recognition of the sky and ground regions, but also the detection of the horizon with as much accuracy as possible. Therefore, both large and small scale neighborhoods should be analyzed. Naturally, to benefit from our already trained HMT model, we again resort to its multiscale structure to perform segmentation. Thus, we implement a multiscale segmentation algorithm, similar to the one developed in <ref type="bibr" target="#b4">[5]</ref>.</p><p>Denoting with ª Â the collection of all class labels at the scale Â, ª Â Â , where Â ¾ × Ý ÖÓÙÒ , the classification is performed according to the MAP rule, as follows:</p><formula xml:id="formula_9">ª Â Ö Ñ Ü ª Â ¨È´ª Â w Â µ © (7) Ö Ñ Ü ª Â ¨Ô´ w Â ª Â µÈ ´ªÂ µ © (8) Ö Ñ Ü ª Â ´ ¾Â Ô´w Â Â µÈ ´ªÂ µ µ<label>(9)</label></formula><p>where the expression ( <ref type="formula" target="#formula_9">9</ref>) is derived assuming that coefficients w Â are mutually independent, given their class labels ª Â .</p><p>To compute the joint probability È ´ªÂ µ, we assume that the distribution of ª Â is completely determined by ª Â•½ at the coarser Â • ½scale. The conditional probability È ´ªÂ ª Â•½ µ, being unknown in general, must be estimated using a prohibitive amount of data. In order to overcome this problem, we introduce contexts <ref type="bibr" target="#b4">[5]</ref>. To each coefficient w Â , with the hidden class label Â , we assign the context c Â , which represents the information on ª Â•½ . In Figure <ref type="figure" target="#fig_4">4</ref>, we illustrate our choice for ´ µ at the coarser scale Â • ½ . contexts c Â . We assume that a set c Â represents a reliable source of information on the distribution of all class labels at Â•½ level and that w Â are mutually independent, given their corresponding contexts c Â . Moving upward to the highest tree level Ä, we apply the Markov chain rule to obtain the expression for È ´ªÂ µ</p><formula xml:id="formula_10">È ´ªÂ µ È ´ªÄ µ ¾Â Ä ½ Â È ´ c µ (10) ¾Ä È ´ Ä µ ¾Â Ä ½ Â È ´ c µ<label>(11)</label></formula><p>where, in the last step, we assume that the class labels Ä of a dyadic square with the coarsest resolution are mutually independent. Finally, from ( <ref type="formula" target="#formula_9">9</ref>) and ( <ref type="formula" target="#formula_10">11</ref>), we derive a more convenient expression for the MAP rule:</p><formula xml:id="formula_11">ª Â Ö Ñ Ü ª Â ¾Ä È ´ Ä µ ¾Â Ô´w Â Â µ Ä ½ Â È ´ c µ<label>(12)</label></formula><p>From the expression <ref type="bibr" target="#b11">(12)</ref>, it follows that the new Markov tree, introduced to perform the multiscale image segmentation, is completely characterized by prior probabilities È ´ Ä µ, transition probabilities È ´ c µ, and likelihoods Ô´w Â Â µ.</p><p>These values must be learned from the given training images. To estimate the prior and transition probabilities, we once again implement the EM algorithm, using the already known values for likelihoods. Once learned, these values are then used in a Bayes classifier to obtain the desired image segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS</head><p>For training the HMT model, we recorded two sets of 500 sky and ground images. One set presented only the sky and the other set contained air images of the ground. We carefully chose the training sets to account for great variability within the classes.</p><p>After experimenting with different image resolutions, we found that the best trade off between processing time and per-formance was achieved when ¼¢ ¼ original images were subsampled to ½¾ ¢½¾ pixels. At that resolution, the training time on an Athlon 1.8GHz PC for ½¼¼¼ training images was less than 2 minutes. Once trained, the HMT models of the sky and ground were used for image segmentation. We noted that for reading, subsampling and segmentation of a testing image it took only ¼ ½¾ seconds on an Athlon 1.8GHz PC.</p><p>Figures <ref type="figure" target="#fig_5">5</ref><ref type="figure" target="#fig_6">6</ref><ref type="figure" target="#fig_7">7</ref>we present segmentation results for three diverse sky/ground images; these results incorporated the Q-shift Dual-Tree CWT introduced in <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>Segmentation of complex image classes, such as sky and ground, demands an elaborate consideration of class properties. Clearly, in some cases, color provides sufficient information for sky and ground detection. However, due to video noise and/or unfavorable class patterns, both color and texture clues are necessary for successful recognition.</p><p>In this paper, we first presented our choice of features, consisting of H and I values from the HSI color space, and CWT coefficients. Then, we showed the implementation of the HMT model and the training steps for obtaining its parameters. We further described how the learned parameter set could be used for computing likelihoods of all nodes at all scales of our HMT. We then developed multiscale Bayesian classification for our application. We incorporated in our design results from the available literature, modifying the original algorithms for our purposes where appropriate. Most importantly, we incorporated color features into the HMT framework and designed the consequent classifier with real-time constraints in mind. Finally, we show sample classification results on diverse sky/ground images.</p><p>V. REFERENCES   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The CWT is strongly oriented at angles ¦½ AE ¦ AE ¦ AE . The original image (left) and the magnitude of the CWT coefficients: ½ AE AE AE (center) and ½¼ AE ½¿ AE ½ AE (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The arrangement of the features: the original image (left), magnitudes of the ½ AE and AE CWT and H values (center), magnitudes of the ½ AE and AE CWT and I values (right).</figDesc><graphic coords="3,50.40,76.12,151.22,151.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The three-level CWT with H values: four adjacent coefficients at one scale have a unique parent belonging to the upper coarser scale. States S Â are depicted as white balls and coefficient values w Â as black balls.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The Markov tree structure: a context c Â is determined from the class labels of nine parents Â•½</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. A MAV's flight: the MAV's propeller blades are recognized as ground.</figDesc><graphic coords="6,50.40,256.72,165.62,165.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Water surface with ice patches similar to clouds.</figDesc><graphic coords="6,227.76,257.44,165.62,165.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. A mountain view with predominant blue color and fuzzy horizon line.</figDesc><graphic coords="6,405.12,256.72,165.62,165.62" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visionguided flight stability and control for Micro Air Vehicles</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Nechyba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Ifju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Waszak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Intelligent Robots and Systems</title>
		<meeting>IEEE Int. Conf. on Intelligent Robots and Systems<address><addrLine>Lausane, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Statistical modeling and segmentation of sky/ground images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<pubPlace>UF, Gainesville, FL</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hidden Markov Tree modeling of Complex Wavelet Transforms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Romberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baraniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Acoustics, Speech, and Sig. Processing</title>
		<meeting>IEEE Int. Conf. on Acoustics, Speech, and Sig. essing<address><addrLine>Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wavelet-based statistical signal processing using Hidden Markov Models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Crouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multiscale image segmentation using wavelet-domain Hidden Markov Models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Color image segmentation: advances and prospects</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<title level="m">A Wavelet Tour of Signal Processing</title>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image processing with complex wavelets</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions Royal Society London</title>
		<imprint>
			<biblScope unit="volume">357</biblScope>
			<biblScope unit="issue">1760</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Statistical modeling with complex dyadic wavelets. Personal Web Page</title>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Lina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Clonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Goulard</surname></persName>
		</author>
		<ptr target="http://www.crm.umontreal.ca/˜clonda/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mixed memory model for image processing and modeling with complex Daubechies wavelets</title>
		<author>
			<persName><forename type="first">D</forename><surname>Clonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Lina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Goulard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE&apos;s Int. Symp. on Optical Science and Technology</title>
		<meeting>SPIE&apos;s Int. Symp. on Optical Science and Technology<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improved Hidden Markov Models in the Wavelet-Domain</title>
		<author>
			<persName><forename type="first">G</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-G</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A tutorial on Hidden Markov Models and selected applications in speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The EM algorithm and extensions</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Mclachlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Thriyambakam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Complex wavelets for shift invariant analysis and filtering of signals</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
