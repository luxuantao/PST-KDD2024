<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">THE SNOWFLAKE HYPOTHESIS: TRAINING DEEP GNN WITH ONE NODE ONE RECEPTIVE FIELD</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-08-19">19 Aug 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kun</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China (USTC</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">King Abdullah University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shilong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China (USTC</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guibin</forename><surname>Zhang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Tongji University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>You</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Shenzhen Technology University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuxuan</forename><surname>Liang</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">INTR &amp; DSA Thrust</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology (Guangzhou</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China (USTC</orgName>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">School of Software Engineering</orgName>
								<orgName type="institution">USTC</orgName>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">USTC</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">THE SNOWFLAKE HYPOTHESIS: TRAINING DEEP GNN WITH ONE NODE ONE RECEPTIVE FIELD</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-08-19">19 Aug 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2308.10051v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>63</term>
					<term>91 Blue: 65</term>
					<term>67 Red: 71</term>
					<term>14 Blue: 70</term>
					<term>58 Red: 70</term>
					<term>49 Blue: 70</term>
					<term>92 Red: 70</term>
					<term>80 Blue: 70</term>
					<term>72 Red: 70</term>
					<term>87 Blue: 70</term>
					<term>18 Red: 70</term>
					<term>63 Blue: 70</term>
					<term>37 Red: 70</term>
					<term>18 Blue: 70</term>
					<term>20</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite Graph Neural Networks (GNNs) demonstrating considerable promise in graph representation learning tasks, GNNs predominantly face significant issues with over-fitting and over-smoothing as they go deeper as models of computer vision (CV) realm. Given that the potency of numerous CV and language models is attributable to that support reliably training very deep architectures, we conduct a systematic study of deeper GNN research trajectories. Our findings indicate that the current success of deep GNNs primarily stems from (I) the adoption of innovations from CNNs, such as residual/skip connections, or (II) the tailor-made aggregation algorithms like DropEdge. However, these algorithms often lack intrinsic interpretability and indiscriminately treat all nodes within a given layer in a similar manner, thereby failing to capture the nuanced differences among various nodes. In this paper, we introduce the Snowflake Hypothesis -a novel paradigm underpinning the concept of "one node, one receptive field". The hypothesis draws inspiration from the unique and individualistic patterns of each snowflake, proposing a corresponding uniqueness in the receptive fields of nodes in the GNNs. We employ the simplest gradient and node-level cosine distance as guiding principles to regulate the aggregation depth for each node, and conduct comprehensive experiments including: (1) different training scheme; (2) various shallow and deep GNN backbones, especially on deep frameworks such as JKNet, ResGCN, PairNorm etc. (3) various numbers of layers (8, 16, 32, 64)  on multiple benchmarks (six graphs including dense graphs with millions of nodes); (4) compare with different aggregation strategies. The observational results demonstrate that our framework can serve as a universal operator for a range of tasks, and it displays tremendous potential on deep GNNs. It can be applied to various GNN frameworks, enhancing its effectiveness when operating in-depth, and guiding the selection of the optimal network depth in an explainable and generalizable way. * Yang Wang and Yuxuan Liang are the corresponding authors, ? denotes equal contributions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph Neural Networks (GNNs) <ref type="bibr" target="#b24">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b20">Hamilton et al., 2017)</ref> have emerged as the leading models for various graph representation learning tasks, including node classification <ref type="bibr" target="#b42">(Velickovic et al., 2017;</ref><ref type="bibr" target="#b0">Abu-El-Haija et al., 2020)</ref>, link prediction <ref type="bibr" target="#b53">(Zhang &amp; Chen, 2018;</ref><ref type="bibr">2019)</ref>, and graph classification <ref type="bibr" target="#b51">(Ying et al., 2018;</ref><ref type="bibr" target="#b17">Gao &amp; Ji, 2019)</ref>. The prominent performance of GNNs mainly stems from their message passing mechanism <ref type="bibr" target="#b47">(Wu et al., 2020)</ref>. Specifically, they iteratively acquire highly-informative representations by aggregating the knowledge from neighbors in the topology <ref type="bibr" target="#b47">(Wu et al., 2020)</ref>.</p><p>Though promising, over-fitting <ref type="bibr" target="#b39">(Rong et al., 2019)</ref>, over-smoothing <ref type="bibr" target="#b30">(Li et al., 2018;</ref><ref type="bibr">Chen et al., 2020a)</ref> and vanishing gradients <ref type="bibr" target="#b27">(Li et al., 2019;</ref><ref type="bibr" target="#b56">Zhao &amp; Akoglu, 2019)</ref> are three long-standing problems in the GNN area, especially when GNNs go deeper as convolutional neural networks (CNNs) <ref type="bibr" target="#b21">(He et al., 2016)</ref>. Consequently, when training an over-parameterized GNN on a small graph or utilizing a deep GNN for graph modeling, we often end up with collapsed weights or indistinguishable node representations <ref type="bibr">(Chen et al., 2020a)</ref>. Therefore, training 2-to-4-layer GNNs is not a foreign phenomenon in the graph realm and most state-of-the-art GNNs are no deeper than 4 layers <ref type="bibr" target="#b41">(Sun et al., 2019)</ref>. Nonetheless, scrutinizing the brilliant achievements on many computer vision tasks, which can be primarily attributed to the consistent and effective training of deep networks. <ref type="bibr" target="#b21">(He et al., 2016;</ref><ref type="bibr" target="#b23">Huang et al., 2017)</ref>. Graph representation learning eagerly calls for the utilization of deeper GNNs, particularly when dealing with large-scale graphs characterized by dense connections.</p><p>Most recently, several works have shown the feasibility of training GNNs with increasing depth. We can summarize the existing approaches into two categories: The first category involves prudently inheriting innovations from CNNs, such as residual/dense connections <ref type="bibr" target="#b27">(Li et al., 2019;</ref><ref type="bibr" target="#b41">Sun et al., 2019;</ref><ref type="bibr" target="#b48">Xu et al., 2018;</ref><ref type="bibr" target="#b29">Li et al., 2021;</ref><ref type="bibr" target="#b48">Xu et al., 2018;</ref><ref type="bibr">Chen et al., 2020b;</ref><ref type="bibr" target="#b50">Xu et al., 2021)</ref>, which have proven to be universally applicable and practical. For instance, JKNet <ref type="bibr" target="#b48">(Xu et al., 2018)</ref> adopts skip connections to fuse the output of each layer to maintain the discrepancies among different nodes. GCNII <ref type="bibr">(Chen et al., 2020b)</ref> and ResGCN <ref type="bibr" target="#b27">(Li et al., 2019)</ref> employ residual connections to carry the information from the previous layer to avoid the aforementioned issues. Another category is to combine various deep aggregation strategies with shallow neural networks <ref type="bibr" target="#b45">(Wu et al., 2019;</ref><ref type="bibr" target="#b10">Chien et al., 2020;</ref><ref type="bibr" target="#b31">Liu et al., 2020;</ref><ref type="bibr" target="#b57">Zou et al., 2019;</ref><ref type="bibr" target="#b39">Rong et al., 2019;</ref><ref type="bibr" target="#b18">Gasteiger et al., 2019)</ref>. For example, GDC <ref type="bibr" target="#b18">(Gasteiger et al., 2019)</ref> generalizes Personalized PageRank into a graph diffusion process. DropEdge <ref type="bibr" target="#b39">(Rong et al., 2019)</ref> resorts to a random edge-dropping strategy to implicitly increase graph diversity and reduce message passing.</p><p>However, although CNN inheritances such as residual/skip connections can partially alleviate the over-smoothing problem, these modifications fail to effectively explore the relationship between aggregation strategies and network depth. Incorporating residuals into layers with suboptimal output may inadvertently propagate detrimental information to subsequent aggregation layers. Within the second category, the majority of existing deep aggregation strategies attempt to sample a subset of neighboring nodes around the central node to implicitly enhance data diversity and prevent oversmoothing. Unfortunately, the cumbersome and particular designs make GNN models neither simple nor practical, lacking the ability to scale on other training strategies and specific datasets.</p><p>In this paper, we hypothesize that each node within a graph should possess its unique receptive field. This can be actualized through the process of element-level adjacency matrix pruning. Such a procedure enables an "early stopping" feature in node aggregation in terms of depth, which not only amplifies interpretability but also aids in mitigating the over-smoothing issue. Drawing from the extensive experimental observations, we first introduce the Snowflake Hypothesis.</p><p>The Snowflake Hypothesis (SnoH). When fitting an abstracted graph G = (V, E) with vertices set V and edges E from the natural world using GNN models (GNNs aim to learn a representation vector of a node or an entire graph based on the adjacency matrix A ? R |V|?|V| and node features X ? R |V|?F ), by utilizing a prune-specific adjacency matrix at the element level, we can uncover the distinctive receptive fields that each node ought to aggregate, akin to the uniqueness and unique patterns observed in each snowflake. This enables us to overcome the over-smoothing problem and train deeper GNNs.</p><p>We utilize the simplest gradient (version 1) and node-level cosine distance (version 2) as the guiding principles to control the aggregation depth for each node, and we adopt extensive experiments, including: 1) Different training algorithms such as a pre-training scheme <ref type="bibr" target="#b32">(Liu et al., 2023)</ref>, iterative pruning <ref type="bibr" target="#b15">(Frankle &amp; Carbin, 2018;</ref><ref type="bibr" target="#b7">Chen et al., 2021)</ref>, and re-initialization. We found that compared to popular pruning algorithms, our approach not only allowed us to leverage the benefits of pruning but also facilitated better network convergence. 2) Integration with mainstream deep GNN models, such as ResGCN <ref type="bibr" target="#b29">(Li et al., 2021)</ref>, JKNet <ref type="bibr" target="#b48">(Xu et al., 2018)</ref>, and GCNII <ref type="bibr">(Chen et al., 2020b)</ref>. This integration enabled us to better assist the network in improving performance and increasing depth.</p><p>3) Comparing our algorithm with the DropEdge or graph lottery ticket <ref type="bibr">(Chen et al., 2020c</ref>) methods, we observe that our algorithm possesses better interpretability and generalizability. Our algorithm can also be conceptualized as a data augmenter and a message passing reducer, providing a new paradigm to benefit many graph pruning applications.</p><p>Identifying unique snowflake. We identify a unique snowflake by employing layer-wise adjacency matrix pruning and node-level cosine distance discrimination. Taking a 2-layer vanilla GCN <ref type="bibr" target="#b42">(Velickovic et al., 2017)</ref> as example, assuming that the trainable parameters ? = {? (0) , ? (1) } for node classification as:</p><formula xml:id="formula_0">Z = Softmax ?? ?X? (0) ? (1) , loss function: L (G, ?) = - vi?V l y i log (z i )</formula><p>where Z is the model predictions, ? (?) denotes an activation function, ? = S-1 2 (A + I) S-1 2 is the normalized adjacency matrix with self-loops and S is the degree matrix of A + I. We minimize the cross-entropy loss L (G, ?) over all labelled nodes V l ? V, where y i and z i represents the label and prediction of node v i , respectively. We present the first versions (v1) of SnoH as follows:</p><p>1 Randomly initialize a graph neural network (GNN) f (G, ? 0 ) for training graph G.</p><p>2 Train the GNN (total D layer) for k iterations, compute the absolute gradient of each elements in the outermost adjacency matrix (note A (D) ). Remove the p% elements with the smallest gradients in the adjacency matrix.</p><p>3 Repeat step 2 by computing A (D-1) and assigning the index of zero elements in A (D-1) to A (D) , thereby setting corresponding positions of the next layer's adjacency matrix to zero.</p><p>4 Repeat steps 2-3 iteratively, removing the corresponding elements from the adjacency matrix and assigning their zero element positions to all deeper layers.</p><p>By utilizing the simplest gradient guidance which can indicate potentially promising elements <ref type="bibr" target="#b25">(Le et al., 2020;</ref><ref type="bibr" target="#b2">Blalock et al., 2020)</ref>, we can realize the Snowflake Hypothesis, enabling each node to have its unique aggregation depth and receptive field size. For ease of understanding, we showcase the detailed training implications in Appendix A. However, the task of calculating gradients for the adjacency matrix presents a substantial challenge. Specifically, for larger graphs with millions of nodes such as Ogbn-product, which possesses 61,859,140 edges <ref type="bibr" target="#b22">(Hu et al., 2020)</ref>. Blindly calculating the gradient for each edge is both difficult and imprudent. The computation of a massive number of parameters makes it extremely challenging to incorporate this algorithm into the deeper GNN framework. To address this, we present SnoHv2, where we make a simple modification to focus on node representations:</p><p>1 Randomly initialize a graph neural network (GNN) f (G, ? 0 ) for training graph G.</p><p>2 Train k iterations, calculate the cosine distance D(Z (l) , T (Z (l) )) = 1 -</p><formula xml:id="formula_1">Z (l) ?T (Z (l) ) ||Z (l) ||2?||T (Z (l)</formula><p>)||2 between the representation of each node in the GNN and its aggregated representation of the surrounding nodes at each layer, Z (l) denotes the representation of the i-th node at layer l before aggregation by the adjacency matrix, while T symbolizes the representation after aggregation. In the context of GCN, Z (l) is defined as H (l) ? W (l) , and T (Z (l) ) is represented as A (l) H (l) W (l) , where H (l) is the feature embedding at l layer.</p><p>3 Compute the nodes whose cosine distance <ref type="bibr" target="#b25">(Le et al., 2020;</ref><ref type="bibr" target="#b2">Blalock et al., 2020)</ref> is below the p% of the distance at the initial layer, and remove the element for node aggregation corresponding to these nodes. As an example, for the i-th node, this equates to pruning all elements in the i-th row of the adjacency matrix (excluding self-loops, which are not pruned).</p><p>4 Once the cosine distance of the i-th nodes at the r-th layer falls below p%, all elements in the i-th row of the adjacency matrices at all deeper layers are pruned.</p><p>The motivation behind SnoHv2 is quite straightforward: As the depth of the GNN (Graph Neural Network) increases, the issue of over-smoothing becomes more severe. Representations of neighboring nodes tend to converge, which in turn leads to the network losing its discriminative capacity. Implementing early stopping in terms of depth can aid in restoring the expressiveness of the nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">IMPLEMENTATIONS &amp; CONTRIBUTIONS</head><p>In our paper, we aim to test various training schemes, backbones, and datasets to explore the effectiveness of SnoH. We also integrate our approach with state-of-the-art deep GNN frameworks and compare it with similar popular algorithms to further illustrate the scalability and generality of our algorithm and hypothesis.</p><p>Training schemes. We select three training methods to explore the performance of our algorithm and the benefits of combining our algorithm with mainstream training approaches: (1) SnoHv1/v2(O): we adopt the original hierarchical one-shot adjacency pruning approach. (2)</p><p>SnoHv1/v2(IP): As our work can be regarded as a graph pruning method, we have opted for the widely recognized iterative pruning (IP) strategy <ref type="bibr" target="#b15">(Frankle &amp; Carbin, 2018)</ref> within the framework of unified graph sparsification (UGS) <ref type="bibr">(Chen et al., 2020c)</ref>.</p><p>(3) SnoHv1/v2(ReI): Due to the challenge of determining whether the model has converged during pruning, after pruning an adjacency matrix, we fix it and reinitialize the GNN for the next training phase. We place the details in Appendix B.</p><p>Datasets &amp; Backbones. In this paper, we utilize six graph benchmarks to evaluate the performance of our hypothesis. Specifically, we select three widely-used small graphs, namely Core, citepseer, and PubMed <ref type="bibr" target="#b24">(Kipf &amp; Welling, 2017)</ref>, for node classification. Additionally, to assess the scalability of our proposal, we incorporate three large-scale graphs known as Ogbn-arxiv, Ogbn-proteins and Ogbn-products <ref type="bibr" target="#b22">(Hu et al., 2020)</ref>. For all selected datasets, we compare our framework with different baseline settings under the same network configurations. We adopt GCN <ref type="bibr" target="#b24">(Kipf &amp; Welling, 2017)</ref>, GIN <ref type="bibr" target="#b49">(Xu et al., 2019)</ref> and GAT <ref type="bibr" target="#b43">(Veli?kovi? et al., 2017)</ref> as shallow GNNs backbones. Further, we take deep ResGCNs <ref type="bibr" target="#b28">(Li et al., 2020)</ref>, JkNet <ref type="bibr" target="#b48">(Xu et al., 2018)</ref> and PairNorm <ref type="bibr" target="#b56">(Zhao &amp; Akoglu, 2019)</ref> as deep backbones. To evaluate our framework on the graph classification task, DropEdge <ref type="bibr" target="#b39">(Rong et al., 2019)</ref> and UGS <ref type="bibr">(Chen et al., 2020c)</ref> are leveraged as the comparison algorithms. More details about experimental settings can be found in Appendix C.</p><p>Contributions. We summarize our contributions as three folds:</p><p>? We propose a node "early stopping" technique based on edge pruning to help better combat the issue of over-smoothing and over-fitting. Based on extensive observational results, we put forth "The Snowflake Hypothesis" -"one node one receptive field", which is inspired by the notion that each snowflake is unique and possesses its own pattern. Likewise, each node in GNNs should have its own receptive field, reflecting its unique characteristics. ? Our algorithm inherently possesses explainability and, while inheriting the advantages of the pruning algorithms (accelerating inference time and reducing storage overhead), it can also benefit the current graph pruning algorithms. More importantly, our algorithm is simple and convenient. Compared to the design of complex aggregation strategies, our framework does not introduce any additional information (e.g., learnable parameters), which can be easily scaled up to deep GNNs. ? We conduct comprehensive experiments, spanning an array of training algorithms, integration with various backbone architectures, and comparisons with DropEdge/UGS frameworks, across multiple graph benchmarks. Our findings indicate that SnoHv1/v2 consistently delivers standout performance, even in instances where the adjacency matrix is notably sparse. These results underscore our initial hypothesis: certain nodes necessitate early termination in their depth progression.</p><p>Prior work. Our work contributes to the domain of graph pruning algorithms, aligning with the research trajectories of graph sampling <ref type="bibr" target="#b4">(Chen et al., 2018;</ref><ref type="bibr" target="#b12">Eden et al., 2018;</ref><ref type="bibr" target="#b7">Chen et al., 2021)</ref> and graph pooling <ref type="bibr" target="#b38">(Ranjan et al., 2020;</ref><ref type="bibr" target="#b55">Zhang et al., 2021;</ref><ref type="bibr" target="#b51">Ying et al., 2018)</ref>. In particular, our algorithm shares significant parallels with the prevalent graph lottery ticket pruning algorithm <ref type="bibr">(Chen et al., 2020c)</ref>, striving to replicate the performance of an original, unpruned graph by means of iterative pruning. Moreover, we aim to address the over-smoothing and over-fitting problems that may surface during the training of deep GNNs <ref type="bibr" target="#b30">(Li et al., 2018;</ref><ref type="bibr">Chen et al., 2020a)</ref>. Current methodologies in training deep GNNs principally concentrate on two areas: (1) incorporating components such as residual/skip connections from the architecture of CNNs <ref type="bibr" target="#b27">(Li et al., 2019;</ref><ref type="bibr" target="#b41">Sun et al., 2019;</ref><ref type="bibr" target="#b48">Xu et al., 2018)</ref> , and (2) crafting a diverse array of aggregation strategies <ref type="bibr" target="#b45">(Wu et al., 2019;</ref><ref type="bibr" target="#b10">Chien et al., 2020;</ref><ref type="bibr" target="#b31">Liu et al., 2020;</ref><ref type="bibr" target="#b57">Zou et al., 2019)</ref>. These focal areas also form the bedrock of our proposed research framework. We refer detailed discussions in the appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">IDENTIFYING THE UNIQUE SNOWFLAKES IN SMALL GRAPHS</head><p>In this section, we meticulously examine and conduct numerous experiments to validate our hypothesis on several small graphs, namely Cora, citepseer, and PubMed. We choose SnoHv1/v2(O) as the benchmark training scheme. The experimental settings are placed in Appendix E.</p><p>As depicted in Tab 1, under conditions of high sparsity (especially with deep adjacency matrices where sparsity is considerable), SnoHv1 can achieve results approximating those of the original baseline. This observation indicates that implementing early stopping for certain nodes in terms of depth does not compromise the overall performance of the model. Upon transitioning to the more robust SnoHv2 version, we notice a performance enhancement in our model. This further suggests that early stopping in depth may help overcome the over-smoothing phenomenon. As frameworks  like ResGCN and JKNet are specifically designed for deep GNNs, we have not presented results for shallow layers. Here, we independently document the results of SnoHv2 for shallow layers. In the case of a 2-layer GCN on Cora, we observe a score of 86.08% (baseline 85.79%), on citepseer, it's 73.81% (baseline 73.58%), and on PubMed, it's 88.54% (baseline 88.65%). We find that, even in shallow GCN, implementing "early stopping" for certain nodes in depth could enhance performance (0.29 on Cora and 0.23 on citepseer). With regard to PubMed, we argue that due to the relative largeness, even after two layers of aggregation, better representations may not have been learned. All nodes may require a deeper receptive field, which aligns with the phenomenon observed in the table where extending the depth to between 8-32 layers leads to a performance boost after pruning.</p><p>Interestingly, when the depth of the GCN reaches 32/64 layers, SnoHv2 shows a stronger performance boost. Specifically, under the experimental setup of a 64-layer GCN + SnoHv2, improvements of 6.77%, 1.66%, and 1.12% were achieved on the Cora, citepseer, and PubMed, respectively. These astonishing results clearly illustrate the effectiveness of our algorithm. In Tab 7 in Appendix E, we present the sparsity under different datasets. We found that as the network deepens, both node sparsity and edge sparsity are decreasing. At the lower level with high sparsity (approximately 17% ? 32%), some nodes and edges were pruned, which in fact improved the model's performance. This validates the contribution of reducing the receptive field to performance enhancement. In Table <ref type="table">8</ref>, we can observe the sparsity of the 64-layer on Cora, which can reach 6.57% node sparsity and 15.26% edge sparsity in deeper layer. This further corroborates the notion that many nodes in deep networks do not require such a large receptive field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How does SnoH help deep GNNs?</head><p>We have found that when our framework is integrated, deep GNNs such as ResGCN, JKNet and PairNorm can benefit from our design. We summarize our observations as follows. Obs 1. In deep architectures (32, 64 layers), SnoH brings more significant improvements to ResGCN than in shallow architectures. For instance, we recorded the ResGCN performance as 85.21%, 71.65%, 87.01% on Cora, citepseer, and PubMed at 64 layers. When combined with SnoHv2, the performance is 85.90%, 72.94%, 88.11% (see Compare with pruning algorithm. Interestingly, our algorithm can be understood as an adjacency matrix pruning algorithm. We chose the current mainstream graph pruning algorithm UGS <ref type="bibr">(Chen et al., 2020c)</ref> and random pruning for comparison with the performance of the universal pruning algorithm. To keep the experimental settings consistent, we removed the part of UGS that targets weight pruning. And we controlled the iterative pruning rates at 5%, 10%, and 20% and pruning 5 times. In order to make a better comparison, we observed the pruning rates when discovering tickets in the lottery ticket scenario, and recorded the pruning rates of SnoHv2 when it get best performance for comparison. We showcase the comprehensive results in Tab 12 (Appendix E) and we list observations: Obs 1. Our model can achieve better performance than random pruning and UGS, even under higher sparsity levels. This further validates our performance in deeper networks, providing assurance for our algorithm's scalability on large datasets. Obs 2. Compared to the UGS, our method has better interpretability. UGS maintains the same sparsity level for each layer's adjacency matrix, which may lead to the loss of many nodes in shallow training. This is unreasonable as early nodes should aggregate essential information, ensuring they can learn better representations. Our experiments further confirm that the receptive field should gradually increase. We also migrated potential training strategies in pruning to SnoHv1 (since SnoHv2 fundamentally determines the early stopping depth of each node by similarity, different training strategies don't have much significance in SnoHv2, while the training process of SnoHv1 and the pruning process are very similar). We display the results in Appendix E. As can be easily seen, different training strategies do not significantly improve SnoHv1's results. However, in practice, iterative pruning and re-initialization strategies can bring about severe efficiency problems (Even D? on the training burden of re-initialization). Therefore, we adopt a one-shot pruning strategy as the preferred strategy for SnoHv1. Unless specified otherwise, the performance we present is that of SnoHv1(O).</p><p>Compare with other edge drop strategy (DropEdge). Another related approach can be understood as the edge drop strategy, where DropEdge <ref type="bibr" target="#b39">(Rong et al., 2019)</ref> shares similarities with SnoH.</p><p>Although DropEdge can improve performance through implicit data augmentation, it lacks interpretability in its aggregation strategy. In fact, it should not continue aggregation after halting the information aggregation for a certain node at a higher layer. It is worth noting that since DropEdge involves temporarily increasing data samples during training, it can be easily combined with SnoHv2. We present the results in Tab 3 and list observations.</p><p>Generalizability on different backbones. To validate the generalization capability of our algorithm across different backbones, we further selected popular GNN frameworks GIN <ref type="bibr" target="#b49">(Xu et al., 2019)</ref> and GAT <ref type="bibr" target="#b43">(Veli?kovi? et al., 2017)</ref> as the backbones for the generalization evaluation. We controlled the parameter ? at values of 0.2, 0.1, and 0.05 under experiments with network depths of 8, 16, and 32 layers, respectively, while recording the results.   We observe that our hypothesis remains viable when applied to GIN and GAT. When combined with SnoHv2, these backbones still demonstrate improved performance in deeper layers. Specifically, on a 16-layer GIN, we achieve a gain of 2.46%, and on a 64-layer GAT, we achieve a gain of 0.73%. These results further support the generalization capability of our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SNOHV1/V2 ON LARGE-SCALE GRAPHS</head><p>In this section, we thoroughly assess our hypothesis on large graphs, employing three benchmark datasets: Ogbn-Arxiv, Ogbn-Proteins, and Ogbn-Product. The dataset splits adhere to the guidelines provided by <ref type="bibr" target="#b22">(Hu et al., 2020)</ref>  under relatively high sparsity conditions as compared to the original network. Obs 2. SnoHv1 can achieve a slightly higher improvement (Tab 4) compare with SnoHv2 of citation network. Through heterogeneity analysis <ref type="bibr" target="#b37">(Pei et al., 2020)</ref>, we find that the citation network possesses relatively more severe heterogenous networks, and the comparison at different levels might be of relatively low significance for this type of graph; we should rather avoid the aggregation of heterogenous information from the initial layers. We have placed specific analysis and conjecture in Appendix G. Similarly, we follow UGS with a 28-layer ResGCN+Arxiv as the benchmark setting, and remove the weight pruning part. We iteratively prune at a rate of 0.05 for 20 times, observing the sparsity at which it finds the winning ticket, and compare it to the optimal sparsity of SnoHv1/v2. We control the pruning rate of each layer in SnoHv1 to be 0.3, while in SnoHv2, we set ? to 0.2. All network training for 1000 epochs with learning rate 0.001 with Adam optimizer. As shown in Fig <ref type="figure" target="#fig_4">5</ref>, we find that our pruning rate on the ResGCN is higher than that of the graph lottery ticket, yet we can achieve relatively comparable performance. This corroborates the possibility that deep layer aggregation may indeed no longer contribute to the model, allowing for stopping at shallower layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SNOHV2 ON OGBN-PROTEIN AND OGBN-PRODUCT</head><p>To verify the scalability of our model on large datasets, we further expanded the dataset size and tested its performance on datasets with tens of millions of edges. We used the Ogbn-Protein and Ogbn-Product datasets as benchmarks. Due to the high complexity of whole-graph training, we adopted the common subgraph approach <ref type="bibr" target="#b9">(Chiang et al., 2019)</ref>. Since our pruning mask is static, we split the above two datasets into a fixed number of subgraphs for training (In this work, we set values as 30, 30 for two graphs). Based on the aforementioned algorithms, we integrated GCN, ResGCN, and ResGCN+, referring to them as Cluster-GCN, Cluster-Res and Cluster-Res+ respectively. Due to the need to measure an excessive number of edge element gradients, the implementation efficiency of SnoHv1 on these two datasets is relatively low. Therefore, we only use SnoHv2 as our method for hypothesis verification. As shown in Tab 5, in comparison to backbones, the utilization of SnoHv2 leads to a significant enhancement in performance. These findings align with the observed behavior in smaller datasets. Specifically, under the configuration of 16 and 32 layers with the combination of protein and Cluster-Res, we managed to surpass the baseline by approximately 1.0%. On the GCN architecture, a notable enhancement of almost 3.0% was achieved at 32 layers. This further clarifies the validity of our hypothesis. Intriguingly, we uncovered that denser graphs, such as Ogbn-Proteins, demonstrate greater resilience to sparsification. Upon contrasting the node classification outcomes on Ogbn-ArXiv (average degree?13.77) and Ogbn-Proteins (average degree?597.00), it becomes evident that Ogbn-Proteins maintains only a minimal performance discrepancy with SnoHv2, even when applied to heavily pruned graphs (? 34.77%, sparsity of SnoHv2+Arxix ? 36.81%), this finding also aligns with the conclusions drawn in <ref type="bibr">(Chen et al., 2020c)</ref>. Although the experiments provided in Sec 3 and 4 are detailed, we only explore the overview performance of the SnoH. In this part, we turn to qualitatively analyze the effect of the receptive fields on some certain nodes in citepseer graph via some case studies shown in Fig <ref type="figure" target="#fig_5">6</ref>, where all the accuracy scores belonging to the same settings and GCN baseline. Based on the information conveyed in Fig <ref type="figure" target="#fig_5">6</ref>, the following observations can be made: Obs 1 (top line). Generally, we argue that nodes with a greater number of neighbors should be assigned a relatively higher pruning rate, while those with fewer neighboring information should be inclined to be retained. However, UGS does not adequately preserve the adjacency matrices of nodes with fewer neighbors within the receptive field, which can be detrimental to the prediction of certain nodes. The SnoH, in comparison to UGS, might exhibit better selectivity in this aspect. This ensures predictive capability for certain nodes and overcomes over-smoothing issues. Obs 2 (bottom line). Taking a microscopic look on a specific node. We observe that by blocking certain channels transmitting information to the central node, it encourages the node to pay more attention to its significant neighbors, leading to a more accurate final prediction. By reducing aggregation channels for nodes with neighbors and blocking entire aggregation channels for certain non-essential neighbors, the SnoH effectively addresses the issues of overfitting and oversmoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CASE STUDY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION &amp; FUTURE WORK</head><p>In this paper, we have proposed the Snowflake Hypothesis for the first time to discover the unique receptive field of each node, carefully suggesting the depth of early stopping for each node through the prevalent techniques of adjacency matrix pruning and cosine distance judgment. Our experiments on multiple graph datasets have demonstrated that early stopping of node aggregation at different depths can effectively enhance inference efficiency (pruning benefits), overcome the over-smoothing problem (early stopping benefits), and simultaneously offer better interpretability. Our framework is both general and succinct, compatible with many mainstream deep networks, such as ResGCN, JKNet, etc., to boost performance and can also be integrated with different training strategies. Our empirical study of the existence of snowflakes invites a number of future work and research questions. We have listed the potential research points and future work as follows:</p><p>Future work. (1) Introducing the block concept from CV. A relatively simple and faster way to accelerate training is to introduce the block concept from CV, combining multiple layers of adjacency matrices into one block. Within the same block, the pruning elements of all adjacency matrices are the same, and the shallow blocks align the reduced edges to the deeper layers. (2) Designating improved early stopping strategies, in this paper we have utilized the simplest pruning strategy to determine whether a node should stop early. We anticipate that in the future, more adaptive early stopping strategies can be discovered to assist in better supporting the Snowflake Hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DETAILS OF SNOHV1</head><p>In this section, we start by providing a simple illustration of our second version model, SnoHv1, through examples. We then delve into the detailed description of our algorithmic workflow. As shown in the Fig <ref type="figure" target="#fig_6">7</ref>, we execute our SnoHv1 on a three-layer GCN. The adjacency matrix undergoes an assigning process to align the pruning elements from inner layers with the outer layer's adjacency matrix, propagating their influence layer by layer. Additionally, the outer layer also adds pruning edges during each individual pruning, ensuring that the same node has different aggregation depths on different neighbors. Compared to SnoHv2, this type of pruning allows for more refined handling of the node's receptive field issue.</p><p>(0)</p><p>(1)</p><p>(2)</p><p>Assigning Process</p><p>Pruing edge in (0) Pruing edge in (1) Pruing edge in (2)</p><p>Pruning Process 3-layer GCN </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B TRAINING SCHEMES</head><p>In this part, we present the detailed processes of the three training methods in the Fig 8 . For SnoHv1/v2(O), we employ the one-shot pruning training approach, where a complete pruning of the adjacency matrix is performed every k iterations, removing p% of the rows. This process is repeated until all adjacency matrices are traversed. For SnoHv1/v2(IP), we refine the pruning of an adjacency matrix into an iterative pruning process, where a portion of rows (v1) or elements (v2) are pruned at each regular iterations, followed by continued training and multiple pruning iterations. It is worth noting that our approach resembles the UGS algorithm <ref type="bibr">(Chen et al., 2020c)</ref>, however, the key difference lies in our pruning being based on the notion of receptive fields. The inner layer's adjacency matrix influences the size of the receptive field in the outer layer, providing better interpretability and algorithmic rationality in an intuitive sense. As for SnoHv1/v2(ReI), during the training process, it is possible that while adjusting the receptive fields of the outer layer's adjacency matrix, the network parameters have already converged to a relatively good local optima. At this point, pruning the inner layer's adjacency matrix may have minimal impact on the model's performance. To address this, we employ a re-initialization (ReI) strategy. After each adjacency matrix pruning is completed, we re-initialize the entire model while keeping the pruned adjacency matrix fixed. Subsequently, we proceed to train and optimize the inner layer's adjacency matrix. Although the training processes of SnoHv1/v2(ReI) and SnoHv1/v2(O) involve k iterations, they may not necessarily be the same in practice. For ease of representation, we use k to denote the number of training iterations. However, in the actual implementation, we will provide specific values for the hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C DETAILS OF DATASETS AND BACKBONES</head><p>In this section, we provide detailed descriptions of the datasets used in this paper. The statistical characteristics of the datasets are presented in Table <ref type="table" target="#tab_6">6</ref>.</p><formula xml:id="formula_2">Pruned adjacent matrix () () (-) () .... () () (-) () .... () () (-) () .... .... ....<label>() () (-) ()</label></formula><p>....</p><formula xml:id="formula_3">SnoHv1/v2(O) 2 ( -1) ? Iterations ? () () (-) () .... .... () () (-) ()</formula><p>....</p><formula xml:id="formula_4">SnoHv1/v2(IP) Iterations 2 (-) (-)</formula><p>.... </p><p>.... .... ....</p><p>....  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D RELATED WORK</head><p>Graph Neural Networks. The Graph Neural Networks (GNNs) family encompasses a diverse array of graph message-passing architectures, each capable of integrating topological structures and node features to create more expressive representations of the entire graph. The efficacy of GNNs, as we illustrate, primarily originates from their inherent "message-passing" function, represented as</p><formula xml:id="formula_7">H (k) = M A, H (k-1) ; ? (k) .</formula><p>Here, H (k) ? R |V|?F corresponds to the node embedding after k iterations of GNN aggregation, M denotes the message propagation function, and ? (k) signifies the trainable parameters at the k-th layer of the GNN (H (0) = X). In light of the growing popularity of graph neural networks, a myriad of propagation functions M <ref type="bibr" target="#b19">(Gilmer et al., 2017;</ref><ref type="bibr" target="#b20">Hamilton et al., 2017)</ref> and <ref type="bibr">GNN variants (Estrach et al., 2014;</ref><ref type="bibr" target="#b42">Velickovic et al., 2017;</ref><ref type="bibr" target="#b28">Li et al., 2020;</ref><ref type="bibr" target="#b35">Mavromatis &amp; Karypis, 2020)</ref> have emerged.</p><p>Deep Graph Neural Networks. Despite the promising results obtained by GNNs, they encounter notorious over-smoothing and over-fitting issues when scaling up to deep structure. To this end, many streams of work have been dedicated to solving these issues and help GNNs have a deep structure. A prominent approach is to inherit the depth modules of CNNs to the graph realm, such as skip and residual connections <ref type="bibr" target="#b27">(Li et al., 2019;</ref><ref type="bibr" target="#b41">Sun et al., 2019;</ref><ref type="bibr" target="#b48">Xu et al., 2018;</ref><ref type="bibr" target="#b29">Li et al., 2021;</ref><ref type="bibr" target="#b48">Xu et al., 2018;</ref><ref type="bibr">Chen et al., 2020b;</ref><ref type="bibr" target="#b50">Xu et al., 2021)</ref>. However, these works do not involve customized operations for the receptive field of each node and lack a specific understanding of graphs. Another representative is combine deep aggregation strategies with shallow GNNs <ref type="bibr" target="#b45">(Wu et al., 2019;</ref><ref type="bibr" target="#b10">Chien et al., 2020;</ref><ref type="bibr" target="#b31">Liu et al., 2020;</ref><ref type="bibr" target="#b57">Zou et al., 2019;</ref><ref type="bibr" target="#b39">Rong et al., 2019;</ref><ref type="bibr" target="#b18">Gasteiger et al., 2019)</ref>. Similarly, these works prevent the over-smoothing issue by replacing the aggregation strategy of the entire network, lacking an understanding of node-specific differentiations. There are also some works that make efforts to theoretically propose methods for training deep GNNs <ref type="bibr" target="#b50">(Xu et al., 2021;</ref><ref type="bibr" target="#b36">Min et al., 2020)</ref>. However, these works are limited to specific types of GNNs, lacking generalizability and practical significance.</p><p>Graph Pooling &amp; Sampling. Graph pooling and sampling devote to reducing the computational burden of GNNs by selectively sampling sub-graphs or applying pruning methods <ref type="bibr" target="#b4">(Chen et al., 2018;</ref><ref type="bibr" target="#b12">Eden et al., 2018;</ref><ref type="bibr" target="#b7">Chen et al., 2021;</ref><ref type="bibr" target="#b12">Eden et al., 2018;</ref><ref type="bibr" target="#b7">Chen et al., 2021;</ref><ref type="bibr" target="#b17">Gao &amp; Ji, 2019;</ref><ref type="bibr" target="#b26">Lee et al., 2019)</ref>. We divide current graph pooling or sampling techniques into two categories. (1) Sampling-based methods aims at selecting the most expressive nodes or edges (i.e., dropping the rest) from the original graph to construct a new subgraph <ref type="bibr" target="#b17">(Gao &amp; Ji, 2019;</ref><ref type="bibr" target="#b26">Lee et al., 2019;</ref><ref type="bibr" target="#b38">Ranjan et al., 2020;</ref><ref type="bibr" target="#b55">Zhang et al., 2021)</ref>. Though efficient, the dropping of nodes/edges sometimes results in severe information loss and isolated subgraphs, which may cripple the performance of GNNs <ref type="bibr" target="#b46">(Wu et al., 2022)</ref>. ( <ref type="formula">2</ref>) Clustering-based methods learns how to cluster together nodes in the original graph, and produces a reduced graph where the clusters are set as nodes <ref type="bibr" target="#b51">(Ying et al., 2018;</ref><ref type="bibr" target="#b46">Wu et al., 2022;</ref><ref type="bibr" target="#b40">Roy et al., 2021)</ref>, which can remedy the aforementioned information loss problem.</p><p>Lottery Ticket Hypothesis (LTH). LTH articulates that a sparse and admirable subnetwork can be identified from a dense network by iterative pruning <ref type="bibr" target="#b15">(Frankle &amp; Carbin, 2018)</ref>. LTH is initially observed in dense networks and is broadly found in many fields <ref type="bibr" target="#b14">(Evci et al., 2020;</ref><ref type="bibr" target="#b16">Frankle et al., 2020;</ref><ref type="bibr" target="#b34">Malach et al., 2020;</ref><ref type="bibr" target="#b11">Ding et al., 2021;</ref><ref type="bibr">Chen et al., 2020c;</ref><ref type="bibr">2021)</ref>. Derivative theories <ref type="bibr">(Chen et al., 2020d;</ref><ref type="bibr" target="#b52">You et al., 2021;</ref><ref type="bibr" target="#b33">Ma et al., 2021)</ref> are proposed to optimize the procedure of network sparsification and pruning. In addition to them, Dual Lottery Ticket Hypothesis (DLTH) considers a more general case to uncover the relationship between a dense network and its sparse counterparts <ref type="bibr" target="#b1">(Bai et al., 2022;</ref><ref type="bibr" target="#b44">Wang et al., 2022)</ref>. Recenlty, graph lottery ticket <ref type="bibr">(Chen et al., 2020c)</ref> proposes to use iterative pruning methods on adjacency matrix and weights (called UGS approach) can obtain a graph lottery ticket during the trianing phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E EXPERIMENTAL SETTINGS AND RESULTS ON SMALL GRAPHS</head><p>Experimental settings. As for three small-scale graphs, we adopt the supervised node classification setting. In our implementation, we choose 60%, 20%, 20% split ratio as our train-val-test splitting of datasets. During the training phase, we choose Adam as optimizer and set learning rate as 0.01, and hidden layer deimension as 64. Tab 9 illustrates the experimental details of three compact datasets: Cora, citepseer, and PubMed. In this table, the term "SnoHv1(O) PE" signifies the pruning epoch within the one-shot pruning strategy under SnoHv1, detailing the number of epochs completed prior to pruning the adjacency matrix for each layer. "SnoHv1(ReI) PE" signifies the epoch count for reinitialization under the reinitialization scenario. The symbol ? is indicative of the depth at which the model halts aggregation under SnoHv2, which can be interpreted as an early termination when the cosine distance between the consolidated output and the initial layer output falls beneath ?. As a rule of thumb, a larger ? induces earlier termination at lesser depths. In our experiment, different depths may correspond to different values of ?. We will later discuss in detail how the settings of ? values affect the performance of the model.   We found that under different training strategies, there was no significant difference in the model's performance. All three training strategies achieved similar performance levels. However, the iterative pruning process, which involves repeatedly determining important parameters, was executed on the CPU and proved challenging to accelerate using a GPU. Additionally, when applied to large graphs, this iterative parameter evaluation process consumed a substantial amount of time. Similarly, the re-initialization method, with its repeated training and reinitialization to assess important parameters in each layer, resulted in significant time wastage. In some cases, it even took more than D times the original training time (D is the network depth for GNNs), which hampers its scalability to large graphs. Based on the above observations, we conducted tests on our framework using large graphs, specifically employing the SnoHv2 version. We believe that our findings can provide valuable insights for future research in evaluating and testing various new designs.</p><p>The effect of ? on SnoHv2. Interestingly, we observed varying sensitivities of the parameter ? across different datasets. The extreme sparsity of the deep adjacency matrix depends on the properties of graphs and backbones. Specifically, on sparse graphs like Cora and citepseer, as the depth of the GCN increases, the stop rate ? gradually decreases. For example, with a 16-layer Cora+SnoHv2 configuration, the optimal value for ? is 0.4, while for 32 and 64 layers, the optimal values are 0.2 and 0.05, respectively. However, on moderately large datasets such as PubMed, sometimes a larger value of ? can lead to performance improvement.</p><p>This phenomenon also shows slight variations with different backbone architectures. When introducing residual structures, the sparsity of the deep adjacency matrix becomes even higher. This might be because residual structures preserve shallow layer information, reducing the need for deep layer information to assist in predictions.</p><p>Compare with graph lottery tickets (UGS algorithm). In our experiment, we compared the model performance of GCN+SnoHv2 and UGS, as well as random pruning under configurations of 8, 16, and 32 layers. We were pleasantly surprised to find that our results considerably outperformed those of random pruning and UGS, particularly under these deep-layer conditions. Our model demonstrated excellent performance. For instance, under a 16-layer setup for the citepseer dataset, our results were 6.57% better than the best UGS configuration (five iterations of pruning, with each iteration pruning 5%). This trend was consistent across all datasets and under various depth settings, further corroborating the superior capabilities of our algorithm in deep scenarios.</p><p>Upon further analysis, we believe that our enhanced performance stems from the early stopping of the receptive field for some nodes in graphs. In fact, our network can be understood as a GCN in the shallow layers and approximates an MLP in the deeper layers. While it ceases to aggregate information for nodes in the deeper layers, it successfully circumvents the issue of gradient vanishing that often plagues deep MLPs.  G HOMOPHILY RATIO Definition. Homophily indicates that adjacent nodes in the graph are likely to have similar attributes or labels. In a social network, for example, people with similar interests or beliefs tend to connect with each other. This pattern holds true in various kinds of networks, and its presence can significantly affect the way GNNs process and learn from the graph.</p><p>Impact on GNN Learning. In GNNs, information is often propagated between neighboring nodes, and node embeddings are updated based on the features of adjacent nodes. If the graph exhibits homophily, this propagation of information is likely to reinforce consistent features among neighboring nodes, which can make learning tasks like node classification more tractable.</p><p>Challenges. Conversely, if a graph does not exhibit homophily (i.e., similar nodes are not more likely to be connected), this can present challenges for learning. GNN models might have difficulty making accurate predictions or inferences in such cases, as neighboring nodes may provide conflicting or less relevant information.</p><p>Measuring Homophily. In some scenarios, quantifying the level of homophily can be beneficial for understanding the graph's structure and for selecting or designing appropriate models or algorithms.</p><p>Various metrics and analyses might be used to gauge the extent of homophily within a given graph.</p><p>Heterophily. The opposite of homophily is heterophily, where neighboring nodes are more likely to be dissimilar. Recognizing whether a graph is more homophilous or heterophilous can be essential in choosing the correct approach and model for graph-based learning tasks.</p><p>In summary, homophily within GNNs signifies the inclination of connected nodes to exhibit similar attributes. This phenomenon is fundamental to the way GNNs interpret and learn from graphs, guiding not only the design but also the interpretation of various graph learning tasks. Its understanding leads to more effective model development and nuanced analysis. Through heterogeneity analysis <ref type="bibr" target="#b37">(Pei et al., 2020)</ref>, We use Eq 1 to calculate the degree of isomorphism in Arxiv, and we find that the homophily degree in Arxiv is relatively low (0.635). This might cause our SnoHv2 to be deeper in the early stopping networks when judging the cosine distance at the hierarchical layer, without overcoming the problem of early aggregation. As a result, this may lead to an insignificant improvement in our SnoHv2. As shown in Figure <ref type="figure" target="#fig_4">5</ref>, we find that our pruning rate on the 28-layer resgcn is higher than that of the lottery ticket, yet we can achieve relatively comparable performance. This corroborates the possibility that low-level aggregation may indeed no longer contribute to the model, allowing for early stopping at shallower layers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Performance comparisons on 32, 64 layer settings using SnoHv2 across three small graphs.</figDesc><graphic url="image-1.png" coords="5,116.66,304.01,376.19,204.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) Cosine distances on ResGCN+SnoHv2 of Cora across various layers settings. (b) ResGCN and JKNet structure overviews.</figDesc><graphic url="image-11.png" coords="6,121.97,348.67,229.00,98.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Experiment results on different graph backbones (GIN, GAT) across Cora dataset. The additional results on citepseer and PubMed can be found in Appendix F.</figDesc><graphic url="image-15.png" coords="7,268.84,348.52,115.96,53.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance comparisons on 16, 32, 64 layer settings using SnoHv2 across GCN, ResGCN and ResGCN+ on Ogbn-Arxiv.Table 4: Comparison between different backbones with SnoHv1 on Ogbn-Arxiv, where L denotes layers. GCN, ? = 0.08, 0.05 ResGCN, ? = 0.1, 0.07, 0.02 ResGCN+, ? = 0.1, 0.07, 0.02 16-L 32-L 64-L 16-L 32-L 64-L 16-L 32-L 64-L +SnoHv1 69.89 65.88 Collapse 71.78 71.17 70.96 70.01 70.93 70.44 w/o SnoHv1 69.47 63.84 Collapse 70.53 70.43 70.61 70.79 70.58 70.17</figDesc><graphic url="image-28.png" coords="8,369.75,221.29,122.21,62.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: (a) (b) (c) denote the edge sparsity under different backbones. It is worth noting that sparsity is represented as the ratio of the remaining edges to the total number of edges. (d) represents the sparsity of each layer under ResGCN+Arxiv setting of SnoHv1/v2 and UGS.</figDesc><graphic url="image-31.png" coords="8,403.64,469.97,89.36,70.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Top. Illustration of the two-hop receptive fields for two nodes (blue and red) along with the results showcased by different algorithms. Bottom. Prediction results for the central node C (The label is 2) using different algorithms.</figDesc><graphic url="image-35.png" coords="9,279.08,475.70,219.53,120.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: An example of our SnoHv1 in 3-layer GCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: An illustration of three training schemes depicted in our paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure10: The experimental settings of ResGCN+SnoHv2 on the citepseer and PubMed datasets are demonstrated using cosine distance. It can be clearly observed that a gradual decrease in cosine distance occurs across results obtained with 8 to 64 layers, indicating that as the depth of the GNN increases, the model exhibits oversmoothing phenomenon. Our approach effectively demonstrates this process.</figDesc><graphic url="image-65.png" coords="19,409.66,530.56,82.82,65.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: The experimental settings of GIN+SnoHv2 and GAT+SnoHv2 on the citepseer and PubMed datasets are demonstrated using cosine distance. It can be readily observed that our algorithm significantly improves the performance of various GNN backbones.</figDesc><graphic url="image-75.png" coords="22,304.43,518.25,188.21,75.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>v) : w ? N (v) ? yv = yw}| |N (v)|(1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance comparisons on 8, 16, 32 layer settings using SnoHv1/v2 across three small graphs, all experimental results are the average of five runs and the red font indicates the optimal value in a set of results. .17/85.68 83.75 83.87/84.19 80.33 81.10/83.09 66.11 68.45 /72.88  ResGCN 85.31 85.37/86.11  85.75 85.99/86.52 86.27 86.33/86.64 85.21 85.24/85.90 JKNet 86.33 87.01/86.53 86.28 86.17/87.08 87.20 87.31/88.86 84.84 85.19/85.96 PairNorm 83.66 82.11/85.90 80.29 80.44/83.25 78.66 79.34/83.16 74.12 74.19/78.60 Train scheme: SnoHv1/v2(O), Dataset: citepseer, 2-layer performance: GCN without BN = 72.44 GCN 72.39 72.41/73.24 71.28 72.10/72.33 68.99 69.21/69.89 44.37 45.12/46.65 ResGCN 72.11 72.07/72.23 72.40 71.91/71.91 72.43 72.44/73.53 71.65 72.10/72.94 JKNet 71.77 71.50/71.47 70.72 70.60/71.47 70.12 70.01/72.67 69.92 70.09/71.55 PairNorm 72.88 72.34/73.84 73.91 73.95/74.58 73.36 73.05/73.92 70.88 70.85/72.99 Train scheme: SnoHv1/v2(O), Dataset: PubMed, 2-layer performance: GCN without BN = 86.50 GCN 86.41 86.50/86.56 84.77 84.74/85.79 83.76 83.77/84.06 77.29 78.15/78.99 ResGCN 87.45 87.50/87.84 87.73 87.47/88.33 87.66 87.33/88.49 87.01 86.03/88.11 JKNet 88.20 88.31/88.51 87.32 87.62/87.95 88.81 88.75/88.99 87.25 86.98/87.93 PairNorm 87.63 87.50/88.68 87.92 87.74/88.60 87.07 87.24/87.69 85.41 85.48/87.06</figDesc><table><row><cell>Backbone</cell><cell cols="2">8 layers</cell><cell cols="2">16 layers</cell><cell cols="2">32 layers</cell><cell cols="2">64 layers</cell></row><row><cell></cell><cell>Original</cell><cell>SnoHv1/v2</cell><cell>Original</cell><cell>SnoHv1/v2</cell><cell>Original</cell><cell>SnoHv1/v2</cell><cell>Original</cell><cell>SnoHv1/v2</cell></row><row><cell cols="7">Train scheme: SnoHv1/v2(O), Dataset: Cora, 2-layer performance: GCN without BN = 85.37</cell><cell></cell></row><row><cell>GCN</cell><cell cols="2">85.11 85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison performances of SnoHv2 with UGS and random pruning (RP). Here IPR denotes iterative pruning rate and we set number of layers as 8. We use GCN backbone and set early stopping threshold of cosine distance as ? (Detailed descriptions in Appendix E).</figDesc><table><row><cell>Dataset</cell><cell cols="5">RP UGS(IPR=5%) UGS(IPR=10%) UGS(IPR=20%) SnoHv2 GCN</cell></row><row><cell>Cora (L=8)</cell><cell>69.60</cell><cell>73.64</cell><cell>66.01</cell><cell>53.29</cell><cell>85.68 85.11</cell></row><row><cell cols="2">citepseer (L=8) 45.50</cell><cell>65.80</cell><cell>51.50</cell><cell>43.10</cell><cell>73.24 72.39</cell></row><row><cell cols="2">PubMed (L=8) 77.82</cell><cell>84.33</cell><cell>80.91</cell><cell>71.05</cell><cell>86.56 86.41</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison between DropEdge and DropEdge+SnoHv2.</figDesc><table><row><cell>Accuracy</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>. For Ogbn-ArXiv, our training data comprises papers published until 2017, validation data encompasses papers published in 2018, and testing data includes papers published since 2019. Regarding Ogbn-Proteins, we partition proteins into training, validation, and test sets based on their species. For the Product dataset, we adopt sales ranking as the criterion to divide nodes into training, validation, and test sets. Specifically, we assign the top 8% of products to the training set, the next top 2% to the validation set, and the remainder to the test set.4.1 SNOHV1/V2 ON CITATION NETWORK Here, we investigate the Snowflake Hypothesis on Ogbn-Arxiv, which is representative of realworld graph scenarios. Specifically, we consider GCN, ResGCN and ResGCN+<ref type="bibr" target="#b28">(Li et al., 2020)</ref> as backbones for evaluation. On these three backbones, we prune the adjacency matrix each layer</figDesc><table /><note><p>separately guided by the cosine distance. Due to the superior ability of ResGCN and ResGCN+ to mitigate over-smoothing compared to GCN, we adopt larger values of ? on ResGCN and ResGCN+ networks. Specifically, for ResGCN and ResGCN+, we use threshold values of 0.1, 0.07, and 0.02 at layers 16, 32, and 64, respectively. For GCN, we use thresholds of 0.08, 0.05 for 16 and 32 layers. The experimental results are shown in Fig 4. We list observations as follows: Obs 1. SnoHv2, when combined with three types of backbones, can achieve the same or even better performance</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison between different backbones with SnoHv2 on Ogbn-Proteins/Products, where L denotes layers. w/o denotes without SnoHv2.</figDesc><table><row><cell></cell><cell cols="4">Cluster-GCN, ? = 0.15 Cluster-Res, ? = 0.15 Cluster-Res+, ? = 0.15</cell></row><row><cell></cell><cell>16-L 32-L</cell><cell>64-L</cell><cell>16-L 32-L 64-L 16-L 32-L</cell><cell>64-L</cell></row><row><cell>Ogbn-Proteins</cell><cell>71.88 71.32</cell><cell>71.08</cell><cell cols="2">79.80 78.87 OOM 80.04 79.32 OOM</cell></row><row><cell cols="2">Ogbn-Proteins (w/o) 71.32 68.44</cell><cell>70.55</cell><cell cols="2">78.40 77.71 OOM 79.90 79.05 OOM</cell></row><row><cell>Ogbn-Product</cell><cell cols="4">68.46 69.44 OOM 79.68 78.99 OOM 78.89 77.64 OOM</cell></row><row><cell cols="5">Ogbn-Product (w/o) 68.40 69.18 OOM 79.50 78.92 OOM 78.77 77.21 OOM</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Statistical characteristics of the dataset used in our paper.</figDesc><table><row><cell>Dataset</cell><cell>Task</cell><cell>#Nodes</cell><cell>#Edges</cell><cell cols="2">#Classes Evaluation Metric</cell></row><row><cell>Cora</cell><cell>Node classification</cell><cell>2,708</cell><cell>5,278</cell><cell>Multi-class</cell><cell>Accuracy</cell></row><row><cell>citepseer</cell><cell>Node classification</cell><cell>3,327</cell><cell>4,732</cell><cell>Multi-class</cell><cell>Accuracy</cell></row><row><cell>PubMed</cell><cell>Node classification</cell><cell>19,717</cell><cell>88,338</cell><cell>Multi-class</cell><cell>Accuracy</cell></row><row><cell cols="4">Ogbn-Proteins Node classification 132,534 39,561,252</cell><cell>Binary</cell><cell>ROC-AUC</cell></row><row><cell cols="5">Ogbn-Products Node classification 2,449,029 61,859,140 Multi-class</cell><cell>Accuracy</cell></row><row><cell cols="3">Ogbn-Arxiv Node classification 169,343</cell><cell cols="2">1,166,243 Multi-class</cell><cell>Accuracy</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Training scheme on SnoHv1. In this section, we further test different training strategies for SnoHv1, including one-shot pruning, iterative pruning, and re-initialization pruning strategies. (1) The commonly used strategy is one-shot pruning, where we prune the adjacency matrix of each layer during each training process. (2) The iterative pruning method involves splitting each training process and pruning some elements of the adjacency matrix in each layer during each epoch. The training continues iteratively by removing elements from the adjacency matrix. (3) The reinitialization strategy prunes one layer of the adjacency matrix at a time. We set the training epoch to 200, meaning that every 200 epochs, we determine which elements in each layer's adjacency matrix should be pruned.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 12 :</head><label>12</label><figDesc>Comparison performances of SnoHv2 with UGS and random pruning (RP). Here IPR denotes iterative pruning rate and we set number of layers as 8. We use GCN backbone and set early stopping threshold of cosine distance as ? (Detailed descriptions in Appendix E).</figDesc><table><row><cell>Dataset</cell><cell cols="5">RP UGS(IPR=5%) UGS(IPR=10%) UGS(IPR=20%) SnoHv2 GCN</cell></row><row><cell>Cora (L=8)</cell><cell>69.60</cell><cell>73.64</cell><cell>66.01</cell><cell>53.29</cell><cell>85.68 85.11</cell></row><row><cell cols="2">citepseer (L=8) 45.50</cell><cell>65.80</cell><cell>51.50</cell><cell>43.10</cell><cell>73.24 72.39</cell></row><row><cell cols="2">PubMed (L=8) 77.82</cell><cell>84.33</cell><cell>80.91</cell><cell>71.05</cell><cell>86.56 86.41</cell></row><row><cell cols="2">Cora (L=16) 51.98</cell><cell>60.32</cell><cell>55.53</cell><cell>47.24</cell><cell>84.19 83.75</cell></row><row><cell cols="2">citepseer (L=16) 60.36</cell><cell>66.31</cell><cell>58.12</cell><cell>30.13</cell><cell>72.33 71.28</cell></row><row><cell cols="2">PubMed (L=16) 53.22</cell><cell>79.22</cell><cell>72.52</cell><cell>58.39</cell><cell>85.79 84.77</cell></row><row><cell cols="2">Cora (L=32) 58.25</cell><cell>69.25</cell><cell>53.64</cell><cell>39.20</cell><cell>83.09 80.33</cell></row><row><cell cols="2">citepseer (L=32) 51.95</cell><cell>57.37</cell><cell>50.31</cell><cell>51.24</cell><cell>69.89 68.99</cell></row><row><cell cols="2">PubMed (L=32) 58.32</cell><cell>77.42</cell><cell>64.26</cell><cell>60.77</cell><cell>84.06 83.76</cell></row><row><cell></cell><cell>Citeseer</cell><cell></cell><cell></cell><cell>Citeseer</cell><cell></cell></row><row><cell></cell><cell>PubMed</cell><cell></cell><cell></cell><cell>PubMed</cell><cell></cell></row></table><note><p>F GENERALIZATION VALIDATION EXPERIMENTS ON GIN AND GAT</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">N-gcn: Multi-scale graph convolution for semi-supervised node classification</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joonseok</forename><surname>Lee</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">uncertainty in artificial intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="841" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Yue</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.04248</idno>
		<title level="m">Dual lottery ticket hypothesis</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">What is the state of neural network pruning? Proceedings of machine learning and systems</title>
		<author>
			<persName><forename type="first">Davis</forename><surname>Blalock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><forename type="middle">Gonzalez</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Guttag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="129" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Measuring and relieving the oversmoothing problem for graph neural networks from the topological view</title>
		<author>
			<persName><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3438" to="3445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10247</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The lottery ticket hypothesis for pre-trained bert networks</title>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="15834" to="15846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A unified lottery ticket hypothesis for graph neural networks</title>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuxi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1695" to="1706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Earlybert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00063</idno>
		<title level="m">Efficient bert training via early-bird lottery tickets</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Clustergcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno>CoRR, abs/1905.07953</idno>
		<ptr target="http://arxiv.org/abs/1905.07953" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adaptive universal generalized pagerank graph neural network</title>
		<author>
			<persName><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07988</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Audio lottery: Speech recognition made ultra-lightweight, noise-robust, and transferable</title>
		<author>
			<persName><forename type="first">Shaojin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Provable and practical approximations for the degree distribution using sublinear graph samples</title>
		<author>
			<persName><forename type="first">Talya</forename><surname>Eden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shweta</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Pinar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>Ron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Seshadhri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference</title>
		<meeting>the 2018 World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="449" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spectral networks and deep locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna Estrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International conference on learning representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2014</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rigging the lottery: Making all tickets winners</title>
		<author>
			<persName><forename type="first">Utku</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Samuel Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2943" to="2952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03635</idno>
		<title level="m">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Linear mode connectivity and the lottery ticket hypothesis</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karolina</forename><surname>Gintare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><surname>Carbin</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3259" to="3269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph u-nets</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PMLR</title>
		<imprint>
			<biblScope unit="page" from="2083" to="2092" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Diffusion improves graph learning</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Gasteiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Wei?enberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22118" to="22133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
		<meeting>the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Going deeper with lean point networks</title>
		<author>
			<persName><forename type="first">Eric-Tuan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="9500" to="9509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Self-attention graph pooling</title>
		<author>
			<persName><forename type="first">Junhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3734" to="3743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deepergcn: All you need to train deeper gcns</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deepgcns: Making gcns go as deep as cnns</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guocheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolina</forename><forename type="middle">Delgadillo</forename><surname>Itzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdulellah</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><forename type="middle">Kassem</forename><surname>Abualshour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards deeper graph neural networks</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="338" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sanity checks for lottery tickets: Does your winning ticket really win the jackpot?</title>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuxi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghai</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12749" to="12760" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Proving the lottery ticket hypothesis: Pruning is all you need</title>
		<author>
			<persName><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilad</forename><surname>Yehudai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shalev-Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6682" to="6691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Graph infoclust: Leveraging cluster-level node information for unsupervised graph representation learning</title>
		<author>
			<persName><forename type="first">Costas</forename><surname>Mavromatis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06946</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scattering gcn: Overcoming oversmoothness in graph convolutional networks</title>
		<author>
			<persName><forename type="first">Yimeng</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederik</forename><surname>Wenkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="14498" to="14508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Geom-gcn: Geometric graph convolutional networks</title>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05287</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Asap: Adaptive structure aware pooling for learning hierarchical graph representations</title>
		<author>
			<persName><forename type="first">Ekagra</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5470" to="5477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Dropedge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10903</idno>
		<title level="m">Towards deep graph convolutional networks on node classification</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Structure-aware hierarchical graph pooling using information bottleneck</title>
		<author>
			<persName><forename type="first">Kashob</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Akm Mahbubur Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Ashraful Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Ahsan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><surname>Adagcn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05081</idno>
		<title level="m">Adaboosting graph convolutional networks into deep models</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">stat</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Searching lottery tickets in graph neural networks: A dual perspective</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengkun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junfeng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Structural entropy guided graph hierarchical pooling</title>
		<author>
			<persName><forename type="first">Junran</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangzhe</forename><surname>Li</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="24017" to="24030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018-07">Jul 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Optimization of graph neural networks: Implicit acceleration by skip connections and more depth</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11592" to="11602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Gebt: Drawing early-bird tickets in graph convolutional network training</title>
		<author>
			<persName><forename type="first">Haoran</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyan</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00794</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Inductive matrix completion based on graph neural networks</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12058</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Hierarchical multi-view graph pooling with structure learning</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengwei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dai</forename><surname>Huifen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12223</idno>
		<title level="m">Pairnorm: Tackling oversmoothing in gnns</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Layer-dependent importance sampling for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">Difan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yewen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Node sparsity (NS) and edge sparsity (ES) of each layer when GCN+SnoHv2 achieves optimal performance under three small datasets</title>
		<idno>GCN ?=0.001</idno>
	</analytic>
	<monogr>
		<title level="m">Li denotes L-th layer</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">% Table 11: Performance comparisons on 8, 16, 32 layer settings using SnoHv1(O), SnoHv1(IP), and SnoHv1(ReI) across three small graphs</title>
		<idno>L31 NS: 0.48% L31 NS: 2.89% L31 NS: 27.80% L31 ES: 1.99% L31 ES: 2.76% L31 ES: 17.68</idno>
		<imprint/>
	</monogr>
	<note>all experimental results are the average of three runs. Backbone 8 layers 16 layers 32 layers</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Dataset: Cora, 2-layer performance: GCN without BN = 85</title>
		<idno>37 GCN 84.37</idno>
	</analytic>
	<monogr>
		<title level="j">SnoHv1(O) SnoHv1(IP) SnoHv1(ReI) SnoHv1(O) SnoHv1(IP) SnoHv1(ReI) SnoHv1(O) SnoHv1(IP) SnoHv</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>ReI) Train scheme: SnoHv</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Dataset: citepseer, 2-layer performance: GCN without BN = 72</title>
		<idno>44 GCN 73.39</idno>
	</analytic>
	<monogr>
		<title level="j">SnoHv</title>
		<imprint/>
	</monogr>
	<note>Train scheme</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Dataset: PubMed, 2-layer performance: GCN without BN = 86</title>
		<idno>50 GCN 86.15</idno>
	</analytic>
	<monogr>
		<title level="j">SnoHv</title>
		<imprint/>
	</monogr>
	<note>Train scheme</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
