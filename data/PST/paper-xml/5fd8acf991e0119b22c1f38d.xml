<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haoyi</forename><surname>Zhou</surname></persName>
							<email>zhouhy@act.buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
							<email>zhangs@act.buaa.edu.cn</email>
							<affiliation key="aff1">
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jieqi</forename><surname>Peng</surname></persName>
							<email>pengjq@act.buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianxin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
							<email>xionghui@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wancai</forename><surname>Zhang</surname></persName>
							<email>zhangwancai@sdee.sgcc.com.cn</email>
							<affiliation key="aff3">
								<orgName type="institution">Beijing Guowang Fuda Science &amp; Technology Development Company</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, such as quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a ProbSparse Self-attention mechanism, which achieves O(L log L) in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling highlights dominating attention by halving cascading layer input, and efficiently handles extreme long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences at one forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on four large-scale datasets demonstrate that Informer significantly outperforms existing methods and provides a new solution to the LSTF problem.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Time-series forecasting is a critical ingredient across many domains, such as sensor network monitoring <ref type="bibr" target="#b19">(Papadimitriou and Yu 2006)</ref>, energy and smart grid management, economics and finance <ref type="bibr" target="#b33">(Zhu and Shasha 2002)</ref>, and disease propagation analysis <ref type="bibr" target="#b18">(Matsubara et al. 2014)</ref>. In these scenarios, we can leverage a substantial amount of time-series data on past behavior to make a forecast in the long run, namely long sequence time-series forecasting (LSTF). However, existing methods are designed under limited problem setting, like predicting 48 points or less <ref type="bibr" target="#b12">(Hochreiter and Schmidhuber 1997;</ref><ref type="bibr" target="#b16">Li et al. 2018;</ref><ref type="bibr">Yu et al. 2017;</ref><ref type="bibr" target="#b17">Liu et al. 2019;</ref><ref type="bibr" target="#b20">Qin et al. 2017;</ref><ref type="bibr" target="#b31">Wen et al. 2017)</ref>. The increasingly long sequences strain the models' prediction capacity to the point where some argue that this trend is holding the Copyright c 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.   <ref type="formula">1</ref>) shows the forecasting results on a real dataset, where the LSTM network predicts the hourly temperature of an electrical transformer station from the short-term period (12 points, 0.5 days) to the long-term period (480 points, 20 days). The overall performance gap is substantial when the prediction length is greater than 48 points (the solid star in Fig. <ref type="figure" target="#fig_1">(1(c)</ref>). The MSE score rises to unsatisfactory performance, the inference speed gets sharp drop, and the LSTM model fails.</p><p>The major challenge for LSTF is enhancing the prediction capacity to meet the increasingly long sequences demand, which requires (a) extraordinary long-range alignment ability and (b) efficient operations on long sequence inputs and outputs. Recently, Transformer models show superior performance in capturing long-range dependency than RNN models. The self-attention mechanism can reduce the maximum length of network signals traveling paths into the theoretical shortest O(1) and avoids the recurrent structure, whereby Transformer shows great potential for LSTF problem. But on the other hand, the self-attention mechanism violates requirement (b) due to its L-quadratic computation and memory consumption on L length inputs/outputs. Some large-scale Transformer models pour resources and yield impressive results on NLP tasks <ref type="bibr" target="#b4">(Brown et al. 2020</ref>), but the training on dozens of GPUs and expensive deploying cost make theses models unaffordable on real-world LSTF problem. The efficiency of the self-attention mechanism and Transformer framework becomes the bottleneck of applying them to LSTF problem. Thus, in this paper, we seek to answer the question: can Transformer models be improved to be computation, memory, and architecture efficient, as well as maintain higher prediction capacity?</p><p>Vanilla Transformer <ref type="bibr" target="#b29">(Vaswani et al. 2017</ref>) has three significant limitations when solving LSTF:</p><p>1. The quadratic computation of self-attention. The atom operation of self-attention mechanism, namely canonical dot-product, causes the time complexity and memory usage per layer to be O(L 2 ). 2. The memory bottleneck in stacking layers for long inputs. The stack of J encoder/decoder layer makes total memory usage to be O(J • L 2 ), which limits the model scalability on receiving long sequence inputs. 3. The speed plunge in predicting long outputs. The dynamic decoding of vanilla Transformer makes the step-by-step inference as slow as RNN-based model, suggested in Fig. <ref type="figure" target="#fig_1">(1c</ref>). There are some prior works on improving the efficiency of self-attention. The Sparse Transformer <ref type="bibr" target="#b5">(Child et al. 2019</ref><ref type="bibr">), LogSparse Transformer (Li et al. 2019)</ref>, and Longformer <ref type="bibr" target="#b2">(Beltagy, Peters, and Cohan 2020)</ref> all use a heuristic method to tackle limitation 1 and reduce the complexity of selfattention mechanism to O(L log L), where their efficiency gain is limited <ref type="bibr" target="#b21">(Qiu et al. 2019)</ref>. Reformer <ref type="bibr" target="#b13">(Kitaev, Kaiser, and Levskaya 2019</ref>) also achieves O(L log L) by locallysensitive hashing self-attention, but it only works on extremely long sequences. More recently, Linformer <ref type="bibr" target="#b30">(Wang et al. 2020</ref>) claims a linear complexity O(L), but the project matrix can not be fixed for real-world long sequence input, which may have the risk of degradation to O(L 2 ). Transformer-XL <ref type="bibr" target="#b8">(Dai et al. 2019)</ref> and Compressive Transformer <ref type="bibr" target="#b22">(Rae et al. 2019</ref>) use auxiliary hidden states to capture long-range dependency, which could amplify limitation 1 and be adverse to break the efficiency bottleneck. All the works mainly focus on limitation 1, and the limitation 2&amp;3 remains in the LSTF problem. To enhance the prediction capacity, we will tackle all of them and achieve improvement beyond efficiency in the proposed Informer.</p><p>To this end, our work delves explicitly into these three issues. We investigate the sparsity in the self-attention mechanism, make improvements of network components, and conduct extensive experiments. The contributions of this paper are summarized as follows:</p><p>• For the right part, the decoder receives long sequence inputs, pads the target elements into zero, measures the weighted attention composition of the feature map, and instantly predicts output elements (orange series) in a generative style.</p><p>sequence output with only one forward step needed, simultaneously avoiding cumulative error spreading during the inference phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminary</head><p>We first provide the problem definition. Under the rolling forecasting setting with a fixed size window, we have the input X t = {x t 1 , . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>Existing methods for time-series forecasting can be roughly grouped into two categories 1 . Classical time-series mod-els serve as a reliable workhorse for time-series forecasting <ref type="bibr" target="#b3">(Box et al. 2015;</ref><ref type="bibr" target="#b23">Ray 1990;</ref><ref type="bibr" target="#b24">Seeger et al. 2017;</ref><ref type="bibr" target="#b25">Seeger, Salinas, and Flunkert 2016)</ref>, and deep learning techniques mainly develop an encoder-decoder prediction paradigm by using RNN and their variants <ref type="bibr" target="#b12">(Hochreiter and Schmidhuber 1997;</ref><ref type="bibr" target="#b16">Li et al. 2018;</ref><ref type="bibr">Yu et al. 2017)</ref>. Our proposed Informer holds the encoder-decoder architecture while targeting the LSTF problem. Please refer to Fig.</p><p>(2) for an overview and the following sections for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficient Self-attention Mechanism</head><p>The canonical self-attention in <ref type="bibr" target="#b29">(Vaswani et al. 2017</ref>) is defined on receiving the tuple input (query, key, value) and performs the scaled dot-product as</p><formula xml:id="formula_0">A(Q, K, V) = Softmax( QK √ d )V, where Q ∈ R L Q ×d , K ∈ R L K ×d , V ∈ R L V ×d</formula><p>and d is the input dimension. To further discuss the self-attention mechanism, let q i , k i , v i stand for the i-th row in Q, K, V respectively. Following the formulation in <ref type="bibr" target="#b28">(Tsai et al. 2019)</ref>, the i-th query's attention is defined as a kernel smoother in a probability form:</p><formula xml:id="formula_1">A(q i , K, V) = j k(q i , k j ) l k(q i , k l ) v j = E p(kj |qi) [v j ] , (1)</formula><p>where p(k j |q i ) = k(qi,kj ) l k(qi,k l ) and k(q i , k j ) selects the asymmetric exponential kernel exp(</p><formula xml:id="formula_2">qik j √ d ).</formula><p>The selfattention combines the values and acquires outputs based on computing the probability p(k j |q i ). It requires the quadratic times dot-product computation and O(L Q L K ) memory usage, which is the major drawback in enhancing prediction capacity.</p><p>Some previous attempts have revealed that the distribution of self-attention probability has potential sparsity, and they have designed some "selective" counting strategies on all p(k j |q i ) without significantly affecting performance. The Sparse Transformer <ref type="bibr" target="#b5">(Child et al. 2019</ref>) incorporates both the row outputs and column inputs, in which the sparsity arises from the separated spatial correlation. The LogSparse Transformer <ref type="bibr" target="#b15">(Li et al. 2019</ref>) notices the cyclical pattern in self-attention and forces each cell to attend to its previous one by an exponential step size. The Longformer <ref type="bibr" target="#b2">(Beltagy, Peters, and Cohan 2020)</ref> extend previous two works to more complicated sparse configuration. However, they are limited to theoretical analysis from following heuristic methods and tackle each multi-head self-attention with the same strategy, which narrows its further improvement.</p><p>To motivate our approach, we first perform a qualitative assessment on the learned attention patterns of the canonical self-attention. The "sparsity" self-attention score forms a long tail distribution (see Appendix C for details), i.e., a few dot-product pairs contribute to the major attention, and others can be ignored. Then, the next question is how to distinguish them?</p><p>Query Sparsity Measurement From Eq.( <ref type="formula">1</ref>), the i-th query's attention on all the keys are defined as a probability p(k j |q i ) and the output is its composition with values v. The dominant dot-product pairs encourage the corresponding query's attention probability distribution away from the</p><formula xml:id="formula_3">uniform distribution. If p(k j |q i ) is close to a uniform dis- tribution q(k j |q i ) = 1</formula><p>L K , the self-attention becomes a trivial sum of values V and is redundant to the residential input. Naturally, the "likeness" between distribution p and q can be used to distinguish the "important" queries. We measure the "likeness" through Kullback-Leibler divergence KL(q||p) = ln</p><formula xml:id="formula_4">L K l=1 e qik l / √ d − 1 L K L K j=1 q i k j / √ d − ln L K .</formula><p>Dropping the constant, we define the i-th query's sparsity measurement as</p><formula xml:id="formula_5">M (q i , K) = ln L K j=1 e q i k j √ d − 1 L K L K j=1 q i k j √ d ,<label>(2)</label></formula><p>where the first term is the Log-Sum-Exp (LSE) of q i on all the keys, and the second term is the arithmetic mean on them. If the i-th query gains a larger M (q i , K), its attention probability p is more "diverse" and has a high chance to contain the dominate dot-product pairs in the header field of the long tail self-attention distribution.</p><p>ProbSparse Self-attention Based on the proposed measurement, we have the ProbSparse Self-attention by allowing each key only to attend to the u dominant queries:</p><formula xml:id="formula_6">A(Q, K, V) = Softmax( QK √ d )V , (<label>3</label></formula><formula xml:id="formula_7">)</formula><p>where Q is a sparse matrix of the same size of q and it only contains the Top-u queries under the sparsity measurement M (q, K). Controlled by a constant sampling factor c, we set u = c • ln L Q , which makes the ProbSparse selfattention only need to calculate O(ln L Q ) dot-product for each query-key lookup and the layer memory usage maintains O(L K ln L Q ).</p><p>However, the traversing of all queries for the measurement M (q i , K) requires calculating each dot-product pairs, i.e. quadratically O(L Q L K ), and the LSE operation has the potential numerical stability issue. Motivated by this, we proposed an approximation to the query sparsity measurement. Lemma 1. For each query q i ∈ R d and k j ∈ R d in the keys set K, we have the bounds as ln</p><formula xml:id="formula_8">L K ≤ M (q i , K) ≤ max j { qik j √ d } − 1 L K L K j=1 { qik j √ d } + ln L K . When q i ∈ K, it also holds.</formula><p>From the Lemma 1 (proof is given in Appendix D.1), we propose the max-mean measurement as</p><formula xml:id="formula_9">M (q i , K) = max j { q i k j √ d } − 1 L K L K j=1 q i k j √ d . (4)</formula><p>The order of Top-u holds in the boundary relaxation with Proposition 1 (refers proof in Appendix D.2). Under the long tail distribution, we only need randomly sample U = L Q ln L K dot-product pairs to calculate the M (q i , K), i.e. filling other pairs with zero. We select sparse Top-u from them as Q. The max-operator in M (q i , K) is less sensitive to zero values and is numerical stable. In practice, the input length of queries and keys are typically equivalent, i.e L Q = L K = L such that the total ProbSparse self-attention time complexity and space complexity are O(L ln L). Proposition 1. Assuming that k j ∼ N(μ, Σ) and we let</p><formula xml:id="formula_10">qk i denote set {(q i k j )/ √ d | j = 1, . . . , L K }, then ∀M m = max i M (q i , K) there exist κ &gt; 0 such that: in the interval ∀q 1 , q 2 ∈ {q|M (q, K) ∈ [M m , M m − κ)}, if M (q 1 , K) &gt; M (q 2 ,</formula><p>K) and Var(qk 1 ) &gt; Var(qk 2 ), we have high probability that M (q 1 , K) &gt; M(q 2 , K). To be simplify, an estimation of the probability is given in the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder: Allowing for processing longer sequential inputs under the memory usage limitation</head><p>The encoder is designed to extract the robust long-range dependency of long sequential inputs. After the input representation, the t-th sequence input X t has been shaped into a matrix X t feed en ∈ R Lx×dmodel . We give a sketch of the encoder in Fig. <ref type="figure">(</ref>3) for clarity.</p><p>Self-attention Distilling As the natural consequence of the ProbSparse Self-attention mechanism, the encoder's feature map have redundant combinations of value V. We use the distilling operation to privilege the superior ones with dominating features and make a focused self-attention feature map in the next layer. It trims the input's time dimension sharply, seeing the n-heads weights matrix (overlapping red squares) of Attention blocks in Fig. <ref type="figure" target="#fig_3">(3</ref>). Inspired by the dilated convolution <ref type="bibr" target="#b32">(Yu, Koltun, and Funkhouser 2017;</ref><ref type="bibr" target="#b11">Gupta and Rush 2017)</ref>, our "distilling" procedure forwards from j-th layer into (j + 1)-th layer as</p><formula xml:id="formula_11">X t j+1 = MaxPool ELU( Conv1d( [X t j ] AB ) ) ,<label>(5)</label></formula><p>where [•] AB contains the Multi-head ProbSparse selfattention and the essential operations in attention block, and Conv1d(•) performs an 1-D convolutional filters (kernel width=3) on time dimension with the ELU(•) activation function <ref type="bibr" target="#b7">(Clevert, Unterthiner, and Hochreiter 2016)</ref>.</p><p>We add a max-pooling layer with stride 2 and down-sample X t into its half slice after stacking a layer, which reduces the whole memory usage to be O((2 − )L log L), where is a small number. To enhance the robustness of the distilling operation, we build halving replicas of the main stack and progressively decrease the number of self-attention distilling layers by dropping one layer at a time, like a pyramid in Fig. <ref type="figure">(</ref>3), such that their output dimension is aligned. Thus, we concatenate all the stacks' outputs and have the final hidden representation of encoder.</p><p>Decoder: Generating long sequential outputs through one forward procedure</p><p>We use a standard decoder structure <ref type="bibr" target="#b29">(Vaswani et al. 2017)</ref> in Fig.</p><p>(2), and it is composed of a stack of two identical multihead attention layers. However, the generative inference is employed to alleviate the speed plunge in long prediction.</p><p>We feed the decoder with following vectors as</p><formula xml:id="formula_12">X t feed de = Concat(X t token , X t 0 ) ∈ R (Ltoken+Ly)×dmodel , (<label>6</label></formula><p>) where X t token ∈ R Ltoken×dmodel is the start token, X t 0 ∈ R Ly×dmodel is a placeholder for the target sequence (set scalar as 0). Masked multi-head attention is applied in the ProbSparse self-attention computing by setting masked dotproducts to −∞. It prevents each position from attending to coming positions, which avoids auto-regressive. A fully connected layer acquires the final output, and its outsize d y depends on whether we are performing a univariate forecasting or a multivariate one.</p><p>Generative Inference Start token is an efficient technique in NLP's "dynamic decoding" <ref type="bibr" target="#b9">(Devlin et al. 2018)</ref>, and we extend it into a generative way. Instead of choosing a specific flag as the token, we sample a L token long sequence in the input sequence, which is an earlier slice before the output sequence. Take predicting 168 points as an example (7-day temperature prediction) in Fig. <ref type="figure">(2(b</ref>)), we will take the known 5 days before the target sequence as "starttoken", and feed the generative-style inference decoder with X feed de = {X 5d , X 0 }. The X 0 contains target sequence's time stamp, i.e. the context at the target week. Note that our proposed decoder predicts all the outputs by one forward procedure and is free from the time consuming "dynamic decoding" transaction in the trivial encoder-decoder architecture. A detailed performance comparison is given in the computation efficiency section.</p><p>Loss function We choose the MSE loss function on prediction w.r.t the target sequences, and the loss is propagated back from the decoder's outputs across the entire model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Datasets</head><p>We empirically perform experiments on four datasets, including 2 collected real-world datasets for LSTF and 2 public benchmark datasets.</p><p>ETT (Electricity Transformer Temperature)<ref type="foot" target="#foot_1">2</ref> : The ETT is a crucial indicator in the electric power long-term deployment. We collected 2 years data from two separated counties in China. To explorer the granularity on the LSTF problem, we create separate datasets as {ETTh 1 , ETTh 2 } for 1-hourlevel and ETTm 1 for 15-minutes-level. Each data point consists of the target value "oil temperature" and 6 power load features. The train/val/test is 12/4/4 months.</p><p>ECL (Electricity Consuming Load)<ref type="foot" target="#foot_2">3</ref> : It collects the electricity consumption (Kwh) of 321 clients. Due to the missing data <ref type="bibr" target="#b15">(Li et al. 2019)</ref>, we convert the dataset into hourly consumption of 2 years and set 'MT 320' as the target value. The train/val/test is 15/3/4 months.</p><p>Weather<ref type="foot" target="#foot_3">4</ref> : This dataset contains local climatological data for nearly 1,600 U.S. locations, 4 years from 2010 to 2013, where data points are collected every 1 hour. Each data point consists of the target value "wet bulb" and 11 climate features. The train/val/test is 28/10/10 months.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Details</head><p>We briefly summarize basics, and more information on network components and setups are given in Appendix E.</p><p>Baselines: The details of network components are given in Appendix E.1. We have selected 5 time-series forecasting methods as comparison, including ARIMA (Ariyo, Adewumi, and Ayo 2014), Prophet <ref type="bibr" target="#b27">(Taylor and Letham 2018)</ref>, LSTMa <ref type="bibr" target="#b1">(Bahdanau, Cho, and Bengio 2015)</ref> and LSTnet <ref type="bibr" target="#b14">(Lai et al. 2018</ref>) and DeepAR <ref type="bibr" target="#b10">(Flunkert, Salinas, and Gasthaus 2017)</ref>. To better explore the ProbSparse selfattention's performance in our proposed Informer, we incorporate the canonical self-attention variant (Informer † ), the efficient variant Reformer <ref type="bibr" target="#b13">(Kitaev, Kaiser, and Levskaya 2019)</ref> and the most related work LogSparse self-attention <ref type="bibr" target="#b15">(Li et al. 2019)</ref> in the experiments.</p><p>Hyper-parameter tuning: We conduct grid search over the hyper-parameters and detail ranges are given in Appendix E.3. Informer contains a 3-layer stack and a 2-layer stack (1/4 input) in encoder, 2-layer decoder. Our proposed methods are optimized with Adam optimizer and its learning rate starts from 1e −4 , decaying 10 times smaller every 2 epochs and total epochs is 10. We set comparison methods as recommended and the batch size is 32. Setup: The input of each dataset is zero-mean normalized. Under the LSTF settings, we prolong the prediction windows size L y progressively, i.e. {1d, 2d, 7d, 14d, 30d, 40d} in {ETTh, ECL, Weather}, {6h, 12h, 24h, 72h, 168h} in ETTm. Metrics: We used two evaluation metrics, including MSE = </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Analysis</head><p>Table <ref type="table" target="#tab_1">1</ref> and Table <ref type="table" target="#tab_2">2</ref> summarize the univariate/multivariate evaluation results of all the methods on 4 datasets. We gradually prolong the prediction horizon as a higher requirement of prediction capacity. To claim a fair comparison, we have precisely controlled the problem setting to make LSTF is tractable on one single GPU for every method. The best results are highlighted in boldface.</p><p>Univariate Time-series Forecasting Under this setting, each method attains predictions in a single variable over time. From Table <ref type="table" target="#tab_1">1</ref>, we observe that: (1) The proposed model Informer greatly improves the inference performance (wining-counts in the last column) across all datasets, and their predict error rises smoothly and slowly within the growing prediction horizon. That demonstrates the success of Informer in enhancing the prediction capacity in the LSTF problem.</p><p>(2) The Informer beats its canonical degradation Informer † mostly in wining-counts, i.e., 28&gt;14, which supports the query sparsity assumption in providing a comparable attention feature map. Our proposed method also outperforms the most related work LogTrans and Reformer. We note that the Reformer keeps dynamic decoding and performs poorly in LSTF, while other methods benefit from the generative style decoder as nonautoregressive predictors.</p><p>(3) The Informer model shows significantly better results than recurrent neural networks LSTMa. Our method has a MSE decrease of 41.5% (at 168), 60.7% (at 336) and 60.7% (at 720). This reveals a shorter network path in the self-attention mechanism acquires better prediction capacity than the RNN-based models. (4) Our proposed method achieves better results than DeepAR, ARIMA and Prophet on MSE by decreasing 20.9% (at 168), 61.2% (at 336), and 51.3% (at 720) in average. On the ECL dataset, DeepAR performs better on shorter horizons (≤ 336), and our method surpasses on longer horizons. We attribute this to a specific example, in which the effectiveness of prediction capacity is reflected with the problem scalability. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Sensitivity</head><p>We perform the sensitivity analysis of the proposed Informer model on ETTh1 under the univariate setting. Input Length: In  Ablation Study: How Informer works?</p><p>We also conducted additional experiments on ETTh 1 with ablation consideration.</p><p>The performance of ProbSparse self-attention mechanism In the overall results Table <ref type="table" target="#tab_1">1</ref> &amp; 2, we limited the problem setting to make the memory usage feasible for the canonical self-attention. In this study, we compare our methods with LogTrans and Reformer, and thoroughly explore their extreme performance. To isolate the memory efficient problem, we first reduce settings as {batch size=8, heads=8, dim=64}, and maintain other setups in the univariate case. In Table <ref type="table">3</ref>, the ProbSparse self-attention shows better performance than the counterparts. The LogTrans gets OOM in extreme cases for its public implementation is the mask of the full-attention, which still has O(L<ref type="foot" target="#foot_5">2</ref> ) memory usage. Our proposed ProbSparse self-attention avoids this from the simplicity brought by the query sparsity assumption in Eq.( <ref type="formula">4</ref>), referring to the pseudo-code in Appendix E.2, and reaches smaller memory usage.</p><p>The performance of self-attention distilling In this study, we use Informer † as the benchmark to eliminate additional effects of ProbSparse self-attention. The other experimental setup is aligned with the settings of univariate Time-series. From the Table <ref type="table">4</ref>, Informer † has fulfilled all experiments and achieves better performance after taking advantage of long sequence inputs. The comparison method Informer ‡ removes the distilling operation and reaches OOM with longer inputs (&gt; 720). Regarding the benefits of long sequence inputs in the LSTF problem,  </p><formula xml:id="formula_13">(L log L) O(L log L) 1 Transformer O(L 2 ) O(L 2 ) L LogTrans O(L log L) O(L 2 ) 1 Reformer O(L log L) O(L log L) L LSTM O(L) O(L) L</formula><p><ref type="foot" target="#foot_4">1</ref> The LSTnet is hard to have a closed form.</p><p>conclude that the self-attention distilling is worth adopting, especially when a longer prediction is required.</p><p>The performance of generative style decoder In this study, we testify the potential value of our decoder in acquiring a "generative" results. Unlike the existing methods, the labels and outputs are forced to be aligned in the training and inference, our proposed decoder's predicting relies solely on the time stamp, which can predict with offsets. From Table 5, we can see that the general prediction performance of Informer ‡ resists with the offset increasing, while the counterpart fails for the dynamic decoding. It proves the decoder's ability to capture individual long-range dependency between arbitrary outputs and avoids error accumulation in the inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computation Efficiency</head><p>With the multivariate setting and each method's current finest implement, we perform a rigorous runtime comparison in Fig.  <ref type="table" target="#tab_3">6</ref>, the performance of Informer is aligned with runtime experiments. Note that the LogTrans focus on the self-attention mechanism, and we apply our proposed decoder in LogTrans for a fair comparison (the in Table <ref type="table" target="#tab_3">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we studied the long-sequence time-series forecasting problem and proposed Informer to predict long sequences. Specifically, we designed the ProbSparse selfattention mechanism and distilling operation to handle the challenges of quadratic time complexity and quadratic memory usage in vanilla Transformer. Also, the carefully designed generative decoder alleviates the limitation of traditional encoder-decoder architecture. The experiments on real-world data demonstrated the effectiveness of Informer for enhancing the prediction capacity in LSTF problem.</p><p>The proposed Informer can process long inputs and make efficient long sequence inference, which can be applied to solve the challenging long sequence times series forecasting (LSTF) problem. Some significant real-world applications are sensor network monitoring <ref type="bibr" target="#b19">(Papadimitriou and Yu 2006)</ref>, energy and smart grid management, disease propagation analysis <ref type="bibr" target="#b18">(Matsubara et al. 2014)</ref>, economics and finance forecasting (Zhu and Shasha 2002), evolution of agriecosystems, climate change forecasting, and variations in air pollution. As a specific example, online sellers can predict the monthly product supply, which helps to optimize longterm inventory management. Our contributions are not limited to the LSTF problem. In addition to acquiring long sequences, our method can bring substantial benefits to other domains, such as long sequence generation of text, music, image, and video.</p><p>Under the ethical considerations, any time-series forecasting application that learns from the history data runs the risk of producing biased predictions. It may cause irreparable losses to the real owners of the property/asset. Domain experts should guide the usage of our methods, and they have potential benefits from the long sequence forecasting. In applying our methods on electrical transformer temperature prediction as an example, the manager will examine the results and decide the future power deployment. If a long enough prediction is available, it will be helpful for the manager to prevent irreversible failure in the early stage. In addition to identifying the bias data, one promising method is adopting transfer learning. We have donated the collected data (the ETT dataset) for further research on related topics (like water supply management, 5G network deployment). Another drawback is our method requires high-performance GPU, which limits its application in underdevelopment regions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) Short sequence predictions only reveal the near future. (b) Long sequence time-series forecasting can cover an extended period for better policy-planning and investment-protecting. (c) The prediction capacity of existing methods limits the long sequence's performance, i.e., starting from length=48, the MSE rises unacceptably high, and the inference speed drops rapidly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 2: An overall graph of the Informer model. The left part is Encoder, and it receives massive long sequence inputs (the green series). We have replaced the canonical self-attention with the proposed ProbSparse self-attention. The blue trapezoid is the self-attention distilling operation to extract dominating attention, reducing the network size sharply. The layer stacking replicas improve the robustness.For the right part, the decoder receives long sequence inputs, pads the target elements into zero, measures the weighted attention composition of the feature map, and instantly predicts output elements (orange series) in a generative style.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The architecture of Informer's encoder. (1) Each horizontal stack stands for an individual one of the encoder replicas in Fig.(2). (2) The upper stack is the main stack, which receives the whole input sequence, while the second stacks take half slices of the input. (3) The red layers are dot product matrixes of self-attention mechanism, and it gets cascade decrease by applying self-attention distilling on each layer. (4) Concate the two stack's feature map as the encoder's output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>ŷ| on each prediction window (averaging for multivariate prediction), and rolling the whole set with stride = 1. Platform: All models were training/testing on a single Nvidia V100 32GB GPU. The source code is available at https://github.com/ zhouhaoyi/Informer2020.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Fig.(4(a)), when predicting short sequences (like 48), initially increasing input length of encoder/decoder degrades performance, but further increasing causes the MSE to drop because it brings repeat short-term patterns. How-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The parameters' sensitivity of Informer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The total runtime of training/testing phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(5). During the training phase, the Informer (red line) achieves the best training efficiency among Transformer-based methods. During the testing phase, our methods are much faster than others with the generative style decoding. The comparisons of theoretical time complexity and memory usage are summarized in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Univariate long sequence time-series forecasting results on four datasets (five cases) .108 0.146 0.208 0.193 0.079 0.103 0.143 0.171 0.184 0.051 0.092 0.119 0.181 0.204 0.107 0.164 0.226 0.241 0.259 0.335 0.408 0.451 0.466 0.470 28 MAE 0.178 0.245 0.294 0.363 0.365 0.206 0.240 0.296 0.327 0.339 0.153 0.217 0.249 0.320 0.345 0.223 0.282 0.338 0.352 0.367 0.423 0.466 0.488 0.499 0.520 Informer † MSE 0.046 0.129 0.183 0.189 0.201 0.083 0.111 0.154 0.166 0.181 0.054 0.087 0.115 0.182 0.207 0.107 0.167 0.237 0.252 0.263 0.304 0.416 0.479 0.482 0.538 14 MAE 0.152 0.274 0.337 0.346 0.357 0.213 0.249 0.306 0.323 0.338 0.160 0.210 0.248 0.323 0.353 0.220 0.284 0.352 0.366 0.374 0.404 0.478 0.508 0.515 0.560 LogTrans MSE 0.059 0.111 0.155 0.196 0.217 0.080 0.107 0.176 0.175 0.185 0.061 0.156 0.229 0.362 0.450 0.120 0.182 0.267 0.299 0.274 0.360 0.410 0.482 0.522 0.546 0 MAE 0.191 0.263 0.309 0.370 0.379 0.221 0.262 0.344 0.345 0.349 0.192 0.322 0.397 0.512 0.582 0.247 0.312 0.387 0.416 0.387 0.455 0.481 0.521 0.551 0.563</figDesc><table><row><cell cols="2">Methods Metric</cell><cell>24</cell><cell>48</cell><cell>ETTh1 168 336 720</cell><cell>24</cell><cell>48</cell><cell>ETTh2 168 336 720</cell><cell>24</cell><cell>48</cell><cell>ETTm1 96</cell><cell>288 672</cell><cell>24</cell><cell>48</cell><cell>Weather 168 336 720</cell><cell>48</cell><cell>ECL 168 336 720</cell><cell>960</cell><cell>count</cell></row><row><cell cols="18">Informer MSE 0.062 0Reformer MSE 0.172 0.228 1.460 1.728 1.948 0.235 0.434 0.961 1.532 1.862 0.055 0.229 0.854 0.962 1.605 0.197 0.268 0.590 1.692 1.887 0.917 1.635 3.448 4.745 6.841 MAE 0.319 0.395 1.089 0.978 1.226 0.369 0.505 0.797 1.060 1.543 0.170 0.340 0.675 1.107 1.312 0.329 0.381 0.552 0.945 1.352 0.840 1.515 2.088 3.913 4.913</cell><cell>0</cell></row><row><cell>LSTMa</cell><cell cols="17">MSE 0.094 0.175 0.210 0.556 0.635 0.135 0.172 0.359 0.516 0.562 0.099 0.289 0.255 0.480 0.988 0.107 0.166 0.305 0.404 0.784 0.475 0.703 1.186 1.473 1.493 MAE 0.232 0.322 0.352 0.644 0.704 0.275 0.318 0.470 0.548 0.613 0.201 0.371 0.370 0.528 0.805 0.222 0.298 0.404 0.476 0.709 0.509 0.617 0.854 0.910 0.9260</cell><cell>1</cell></row><row><cell>DeepAR</cell><cell cols="17">MSE 0.089 0.126 0.213 0.403 0.614 0.080 0.125 0.179 0.568 0.367 0.075 0.197 0.336 0.908 2.371 0.108 0.177 0.259 0.535 0.407 0.188 0.295 0.388 0.471 0.583 MAE 0.242 0.291 0.382 0.496 0.643 0.229 0.283 0.346 0.555 0.488 0.205 0.332 0.450 0.739 1.256 0.242 0.313 0.397 0.580 0.506 0.317 0.398 0.471 0.507 0.583</cell><cell>6</cell></row><row><cell>ARIMA</cell><cell cols="17">MSE 0.086 0.133 0.364 0.428 0.613 3.538 3.168 2.768 2.717 2.822 0.074 0.157 0.242 0.424 0.565 0.199 0.247 0.471 0.678 0.996 0.861 1.014 1.102 1.213 1.322 MAE 0.190 0.242 0.456 0.537 0.684 0.407 0.440 0.555 0.680 0.952 0.168 0.274 0.357 0.500 0.605 0.321 0.375 0.541 0.666 0.853 0.726 0.797 0.834 0.883 0.908</cell><cell>1</cell></row><row><cell>Prophet</cell><cell cols="17">MSE 0.093 0.150 1.194 1.509 2.685 0.179 0.284 2.113 2.052 3.287 0.102 0.117 0.146 0.414 2.671 0.280 0.421 2.409 1.931 3.759 0.506 2.711 2.220 4.201 6.827 MAE 0.241 0.300 0.721 1.766 3.155 0.345 0.428 1.018 2.487 4.592 0.256 0.273 0.304 0.482 1.112 0.403 0.492 1.092 2.406 1.030 0.557 1.239 3.029 1.363 4.184</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Multivariate long sequence time-series forecasting results on four datasets (five cases)Multivariate Time-series Forecasting Within this setting, some univariate methods are inappropriate, and LSTnet is the state-of-art baseline. On the contrary, our proposed Informer is easy to change from univariate prediction to multivariate one by adjusting the final FCN layer. From Table2, we observe that: (1) The proposed model Informer greatly outperforms other methods and the findings 1 &amp; 2 in the univariate settings still hold for the multivariate time-series. (2) The Informer model shows better results than RNN-based LSTMa and CNN-based LSTnet, and the MSE decreases 9.5% (at 168), 2.1% (at 336), 13.8% (at 720) in average. Compared with the univariate results, the overwhelming performance is reduced, and such phenomena can be caused by the anisotropy in feature dimensions' prediction capacity. It beyonds this paper's scope, and we explore it in future work.LSTF with Granularity Consideration We perform an additional comparison trying to explore the performance with various granularities. The sequences {96, 288, 672} of ETTm 1 (minutes-level) are aligned with {24, 48, 168} of ETTh 1 (hour-level). The Informer outperforms other baselines even if the sequences are at different granularity levels.</figDesc><table><row><cell cols="2">Methods Metric</cell><cell>24</cell><cell>48</cell><cell>ETTh1 168 336 720</cell><cell>24</cell><cell>48</cell><cell>ETTh2 168 336 720</cell><cell>24</cell><cell>48</cell><cell>ETTm1 96</cell><cell>288 672</cell><cell>24</cell><cell>48</cell><cell>Weather 168 336 720</cell><cell>48</cell><cell>ECL 168 336 720 960</cell><cell>count</cell></row><row><cell>Informer</cell><cell cols="17">MSE 0.509 0.551 0.878 0.884 0.941 0.446 0.934 1.512 1.665 2.340 0.325 0.472 0.642 1.219 1.651 0.353 0.464 0.592 0.623 0.685 0.269 0.300 0.311 0.308 0.328 32 MAE 0.523 0.563 0.722 0.753 0.768 0.523 0.733 0.996 1.035 1.209 0.440 0.537 0.626 0.871 1.002 0.381 0.455 0.531 0.546 0.575 0.351 0.376 0.385 0.385 0.406</cell></row><row><cell cols="18">Informer  † MSE 0.550 0.602 0.893 0.836 0.981 0.683 0.977 1.873 1.374 2.493 0.324 0.446 0.651 1.342 1.661 0.355 0.471 0.613 0.626 0.680 0.269 0.281 0.309 0.314 0.356 12 MAE 0.551 0.581 0.733 0.729 0.779 0.637 0.793 1.094 0.935 1.253 0.440 0.508 0.616 0.927 1.001 0.383 0.456 0.544 0.548 0.569 0.351 0.366 0.383 0.388 0.394</cell></row><row><cell>LogTrans</cell><cell cols="17">MSE 0.656 0.670 0.888 0.942 1.109 0.726 1.728 3.944 3.711 2.817 0.341 0.495 0.674 1.728 1.865 0.365 0.496 0.649 0.666 0.741 0.267 0.290 0.305 0.311 0.333 2 MAE 0.600 0.611 0.766 0.766 0.843 0.638 0.944 1.573 1.587 1.356 0.495 0.527 0.674 1.656 1.721 0.405 0.485 0.573 0.584 0.611 0.366 0.382 0.395 0.397 0.413</cell></row><row><cell>Reformer</cell><cell cols="17">MSE 0.887 1.159 1.686 1.919 2.177 1.381 1.715 4.484 3.798 5.111 0.598 0.952 1.267 1.632 1.943 0.583 0.633 1.228 1.770 2.548 1.312 1.453 1.507 1.883 1.973 0 MAE 0.630 0.750 0.996 1.090 1.218 1.475 1.585 1.650 1.508 1.793 0.489 0.645 0.795 0.886 1.006 0.497 0.556 0.763 0.997 1.407 0.911 0.975 0.978 1.002 1.185</cell></row><row><cell>LSTMa</cell><cell cols="17">MSE 0.536 0.616 1.058 1.152 1.682 1.049 1.331 3.987 3.276 3.711 0.511 1.280 1.195 1.598 2.530 0.476 0.763 0.948 1.497 1.314 0.388 0.492 0.778 1.528 1.343 0 MAE 0.528 0.577 0.725 0.794 1.018 0.689 0.805 1.560 1.375 1.520 0.517 0.819 0.785 0.952 1.259 0.464 0.589 0.713 0.889 0.875 0.444 0.498 0.629 0.945 0.886</cell></row><row><cell>LSTnet</cell><cell cols="17">MSE 1.175 1.344 1.865 2.477 1.925 2.632 3.487 1.442 1.372 2.403 1.856 1.909 2.654 1.009 1.681 0.575 0.622 0.676 0.714 0.773 0.279 0.318 0.357 0.442 0.473 4 MAE 0.793 0.864 1.092 1.193 1.084 1.337 1.577 2.389 2.429 3.403 1.058 1.085 1.378 1.902 2.701 0.507 0.553 0.585 0.607 0.643 0.337 0.368 0.391 0.433 0.443</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 :</head><label>6</label><figDesc>L-related computation statics of each layer</figDesc><table><row><cell>Methods</cell><cell>Training Time Complexity Memory Usage</cell><cell>Testing Steps</cell></row><row><cell>Informer</cell><cell>O</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Due to the space limitation, a complete related work survey is provided in Appendix A.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">We collected the ETT dataset and published it at https:// github.com/zhouhaoyi/ETDataset.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">ECL dataset was acquired at https://archive.ics.uci.edu/ml/ datasets/ElectricityLoadDiagrams20112014</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">Weather dataset was acquired at https://www.ncdc.noaa.gov/ orders/qclcd/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_4">Informer § replaces our decoder with dynamic decoding one in Informer ‡ .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_5">The '-' indicates failure for the unacceptable metric results.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by grants from the Natural Science Foundation of China (U20B2053, 61872022 and 61421003) and State Key Laboratory of Software Development Environment (SKLSDE-2020ZX-12). Thanks for computing infrastructure provided by Beijing Advanced Innovation Center for Big Data and Brain Computing. The corresponding author is Jianxin Li.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stock price prediction using the ARIMA model</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Ariyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Adewumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Ayo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 16th International Conference on Computer Modelling and Simulation</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="106" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Longformer: The Long-Document Transformer</title>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno>CoRR abs/2004.05150</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Reinsel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Ljung</surname></persName>
		</author>
		<title level="m">Time series analysis: forecasting and control</title>
				<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2020. 2005.14165</date>
		</imprint>
	</monogr>
	<note>Language Models are Few-Shot Learners. CoRR abs/</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<title level="m">Generating Long Sequences with Sparse Transformers</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the Properties of Neural Machine Translation: Encoder-Decoder Approaches</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST@EMNLP</title>
				<meeting>SSST@EMNLP</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)</title>
		<author>
			<persName><forename type="first">D</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
				<meeting><address><addrLine>Bert</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">DeepAR: Probabilistic forecasting with autoregressive recurrent networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Flunkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Salinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gasthaus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04110</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Dilated convolutions for modeling long-distance genomic dependencies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.01278</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Reformer: The Efficient Transformer</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling long-and short-term temporal patterns with deep neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR 2018</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.00235</idno>
		<title level="m">Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">DSTP-RNN: a dual-stage two-phase attention-based recurrent neural networks for long-term and multivariate time series prediction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>CoRR abs/1904.07464</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">FUNNEL: automatic mining of spatially coevolving epidemics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sakurai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Van Panhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Optimal multi-scale patterns in time series streams</title>
		<author>
			<persName><forename type="first">S</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD 2006</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="647" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="2627" to="2633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><surname>.-T</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02972</idno>
		<title level="m">Blockwise Self-Attention for Long Document Understanding</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05507</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Time series: theory and methods</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series A (Statistics in Society)</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="400" to="400" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Approximate bayesian inference in linear state space models for intermittent demand forecasting at scale</title>
		<author>
			<persName><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rangapuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Salinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gasthaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Januschowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Flunkert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.07638</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bayesian intermittent demand forecasting for large inventories</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Seeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Salinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Flunkert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4646" to="4654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Forecasting at scale</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Letham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="45" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Transformer Dissection: An Unified Understanding for Transformer&apos;s Attention via the Lens of Kernel</title>
		<author>
			<persName><forename type="first">Y.-H</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2019</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4335" to="4344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<title level="m">Linformer: Self-Attention with Linear Complexity</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A multi-horizon quantile recurrent forecaster</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Torkkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Madeka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11053</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="472" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">StatStream: Statistical Monitoring of Thousands of Data Streams in Real Time</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Shasha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00073</idno>
	</analytic>
	<monogr>
		<title level="m">Longterm forecasting using tensor-train rnns</title>
				<imprint>
			<date type="published" when="2002">2017. 2002. 2002</date>
			<biblScope unit="page" from="358" to="369" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
