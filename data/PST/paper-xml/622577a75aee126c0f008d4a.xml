<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data-Efficient and Interpretable Tabular Anomaly Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-03">3 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chun-Hao</forename><surname>Chang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jinsung</forename><surname>Yoon</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sercan</forename><surname>Arik</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Madeleine</forename><surname>Udell</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
						</author>
						<title level="a" type="main">Data-Efficient and Interpretable Tabular Anomaly Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-03">3 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.02034v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Anomaly detection (AD) plays an important role in numerous applications. We focus on two understudied aspects of AD that are critical for integration into real-world applications. First, most AD methods cannot incorporate labeled data that are often available in practice in small quantities and can be crucial to achieve high AD accuracy. Second, most AD methods are not interpretable, a bottleneck that prevents stakeholders from understanding the reason behind the anomalies. In this paper, we propose a novel AD framework that adapts a white-box model class, Generalized Additive Models, to detect anomalies using a partial identification objective which naturally handles noisy or heterogeneous features. In addition, the proposed framework, DIAD, can incorporate a small amount of labeled data to further boost anomaly detection performances in semisupervised settings. We demonstrate the superiority of our framework compared to previous work in both unsupervised and semi-supervised settings using diverse tabular datasets. For example, under 5 labeled anomalies DIAD improves from 86.2% to 89.4% AUC by learning AD from unlabeled data. We also present insightful interpretations that explain why DIAD deems certain samples as anomalies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Anomaly detection (AD) has numerous real-world applications, especially for tabular data, including detection of fraudulent transactions, intrusions for cybersecurity, and adverse outcomes in healthcare. We propose a novel method that targets the following challenges in tabular AD:</p><p>? Noisy and irrelevant features: Tabular data (such as Electronic Health Records) often contain noisy or irrele-Work done in an internship. 1 Google Cloud AI 2 Cornell University. Correspondence to: Chun-Hao Chang &lt;chkchang21@gmail.com&gt;.</p><p>vant features caused by measurement noise, outliers and inconsistent units. Yet, even a change in a small subset of features may be considered anomalous.</p><p>? Heterogeneous features: Unlike image or text data, tabular data features can have values with significantly different types (numerical, boolean, categorical, and ordinal), ranges and distributions. ? Some labeled data: In many applications, often a small set of labeled data is available. AD accuracy can be substantially boosted by using labeled information to identify some representative anomalies and ignore irrelevant ones. ? Interpretability: Without interpretable outputs, humans cannot understand the rationale behind anomalies, take actions to improve the AD model performance and build trust. Verification of model accuracy is particularly challenging for tabular data as they are not easy to visualize for humans. An interpretable AD model should identify important features used to classify anomalies to enable verification and build trust.</p><p>Most AD methods for tabular data fail to address the above challenges -their performance often deteriorates with noisy features (see Sec. 4), they cannot incorporate labeled data, and they cannot provide interpretability (see Sec. 2).</p><p>In this paper, we aim to address these challenges by proposing a Data-efficient Interpretable AD approach, which we call DIAD. Our model architecture is inspired by Generalized Additive Models (GAMs) that have been shown to obtain high accuracy and interpretability for tabular data <ref type="bibr" target="#b3">(Caruana et al., 2015;</ref><ref type="bibr">Chang et al., 2021b;</ref><ref type="bibr" target="#b13">Liu et al., 2021)</ref> and are widely used in many applications such as finding outlier patterns and auditing fairness <ref type="bibr" target="#b29">(Tan et al., 2018)</ref>. We propose to employ intuitive notions of Partial Identification (PID) as AD objective and propose to learn them with differentiable GAMs. PID scales to high-dimensional features and handles heterogeneous features, while the differentiable GAM architecture allows fine-tuning with labeled data and retain interpretability. Furthermore, by fine-tuning using a differentiable AUC loss with a small amount of labeled samples, DIAD outperforms other semi-supervised learning AD methods. For example, DIAD improves from 86.2% to 89.4% AUC with 5 labeled anomalies by unsupervised AD. DIAD provides rationale on why an example is classified as anomalous using the GAM graphs and also gives insights on the impact of labeled data on decision boundary, which In the training stage, we first learn an unsupervised anomaly detection model by interpretable GA 2 M models and PID loss with unlabeled data. Then, we fine-tune the trained unsupervised model with a small amount of labeled data using a differentiable AUC loss. At inference time, we provide both the anomaly score and explanations of the model's decisions with GA 2 M graphs of top contributing features. For example, the example data shown here has high anomaly scores due to its large cell size.</p><p>can be used to build global understanding about the task and to help improve the AD performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>AD methods only with normal data are widely studied <ref type="bibr">(Pang &amp; Aggarwal, 2021)</ref>. The methods closest to the proposed are the tree-based AD. Isolation Forest (IF) <ref type="bibr" target="#b14">(Liu et al., 2008)</ref> grows decision trees randomly and the shallower the tree depth for the data is, the more anomalous the data is. However, this approach shows performance degradation when feature dimensionality increases. Robust Random Cut Forest (RRCF, <ref type="bibr" target="#b10">Guha et al. (2016)</ref>) further improves IF by choosing features to split based on the size of its range, but it is more sensitive to scale. PIDForest <ref type="bibr" target="#b8">(Gopalan et al., 2019)</ref> zooms on the features which have large variances which makes it robust to noisy or irrelevant features.</p><p>Another family of AD methods are based on generative approaches that learn to reconstruct input features, and use the error of reconstructions or density to identify anomalies. <ref type="bibr" target="#b1">Bergmann et al. (2018)</ref> employs auto-encoders for image data. DAGMM <ref type="bibr" target="#b34">(Zong et al., 2018)</ref> first learns an autoencoder and uses a Gaussian Mixture Model to estimate density in the low-dimensional latent space. Since these generative approaches are based on reconstructing input features, they may not easily fit in high-dimensional tabular data with noisy and heterogeneous features and small amount of labeled examples.</p><p>Recently, methods with pseudo-tasks have been proposed for AD. One major line of work is to predict geometric transformations <ref type="bibr" target="#b7">(Golan &amp; El-Yaniv, 2018;</ref><ref type="bibr" target="#b0">Bergman &amp; Hoshen, 2020)</ref>, such as rotation or random transformations, and use the prediction errors to detect anomalies. <ref type="bibr" target="#b21">Qiu et al. (2021)</ref> instead learns a set of diverse transformations and show improvements for tabular and text data. CutPaste <ref type="bibr" target="#b11">(Li et al., 2021)</ref> learns to classify the images with patches replaced by another image, combined with density estimation in the latent space for image data. Several recent works focus on contrastive learning. <ref type="bibr" target="#b28">Tack et al. (2020)</ref> learns to distinguish synthetic samples from the original distribution and achieves success on image data. <ref type="bibr" target="#b27">Sohn et al. (2021)</ref> first learns a distribution-augmented contrastive representation and then uses a one-class classifier to identify anomalies. Internal Contrastive Learning (ICL, <ref type="bibr" target="#b25">Shenkar &amp; Wolf (2022)</ref>) tries to distinguish between inwindow and out-of-window features by a sliding window over features and utilizes the error to identify anomalies, achieving state-of-the-art performance for tabular data.</p><p>A few AD works focus on explainability. <ref type="bibr" target="#b30">Vinh et al. (2016)</ref>; <ref type="bibr" target="#b15">Liu et al. (2020)</ref> explain anomalies from other off-the-shelf detectors; thus, their explanations may not reflect the rationales of the detectors. <ref type="bibr" target="#b16">Liznerski et al. (2021)</ref> proposes to identify anomalies with a one-class classifier; it uses a CNN without fully connected layers so each output unit corresponds to a receptive field in the input image. This allows visualizations of which part of the input images leads to high error in the output with accurate localization; however, this approach is limited to image data.</p><p>Several works have been proposed for semi-supervised AD. Deep SAD <ref type="bibr" target="#b23">(Ruff et al., 2019)</ref> extends the deep one-class classification method DSVDD <ref type="bibr" target="#b22">(Ruff et al., 2018)</ref> into the semi-supervised setting. However, their approach is noninterpretable and performs even worse than the unsuper-vised One-Class SVM for tabular data, while the DIAD outperforms it. DevNet <ref type="bibr">(Pang et al., 2019b)</ref> formulates the AD problem into a regression problem and achieves better sample complexity under limited labeled data. <ref type="bibr">Yoon et al. (2020b)</ref> trains an embedding similar to BERT <ref type="bibr" target="#b6">(Devlin et al., 2018)</ref> combined with consistency loss <ref type="bibr" target="#b26">(Sohn et al., 2020)</ref> and achieves the state-of-the-art semi-supervised performance in tabular data. See Table <ref type="table" target="#tab_0">1</ref> for a comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>Model Architecture To render AD interpretable and allow back-propagation to learn from labeled data, our work is inspired from the NodeGAM <ref type="bibr">(Chang et al., 2021a)</ref>, an interpretable and differentiable tree-based GAM model.</p><p>What are GAM and GA 2 M? GAMs and GA 2 Ms are considered as white-box models since they allow humans to visualize their functional forms in 1-D or 2-D plots by not allowing any 3-way or more feature interactions. Specifically, given an input x ? R D , a label y, a link function g (e.g. g is log(p/1 -p) in binary classification), main effects f j for each feature j, and 2-way feature interactions f jj , GAM and GA 2 M are expressed as:</p><formula xml:id="formula_0">GAM: g(y) = f 0 + D j=1 f j (x j ), GA 2 M: g(y) = f 0 + D j=1 f j (x j ) + D j=1 j &gt;j f jj (x j , x j ).</formula><p>Unlike commonly-used deep learning architectures that use all the feature interactions, GAMs and GA 2 M are restricted to lower-order interaction (1 or 2-way), so the impact of each f j or interaction f jj (x j , x j ) can be visualized independently as a graph. That means, for f j , we may plot x j on the x-axis and f j (x j ) on the y-axis. To plot f jj , we show a scatter plot with x j on the x-axis and x j on the y-axis, where color indicates the value f jj . Note that a linear model is a special case of GAM. Humans can easily simulate the decision making of a GAM by reading f j and f jj from the graph for each feature j and j and taking the sum.</p><p>Here we review NodeGA 2 M relevant to our changes. NodeGA 2 M is based on GA 2 M but uses the neural trees to learn feature functions f j and f jj . Specifically, NodeGA 2 M consists of L layers where each layer has m differentiable soft oblivious decision trees (ODT) of equal depth C where each tree only interacts with at most 2 features. Next, we describe ODTs.</p><p>Differentiable ODTs: An ODT works like a traditional decision tree except for all nodes in the same depth share the same input features and thresholds, which allows parallel computation and makes it suitable for deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 A soft decision tree</head><p>Input: Mini-batch X ? R B?D , Temperature T (T -? 0) Symbols: Tree Depth C, Entmoid ? Trainable Parameters:</p><p>Feature selection logits</p><formula xml:id="formula_1">F 1 , F 2 ? R D , split Thresholds b ? R C , split slope S ? R C , F = [F 1 , F 2 , F 1 , ...] T ? R D?C {Alternating F 1 , F 2 } G = X ? EntMax(F /T, dim=0) ? R B?C for c = 1 to C do H c = ?( (G c -b c ) S c ?T ) {Soft binary split} end for e = H 1 (1 -H 1 ) ? ? ? ? ? (H C ) (1 -H C ) ? R B?2 C E = sum(e, dim=0) ? R 2 C {Sum across batch} Return: E count</formula><p>Specifically, an ODT of depth C compares chosen C input features to C thresholds, and returns one of the 2 C possible options. Mathematically, for feature functions F c which choose what features to split, splitting thresholds b c , and a response vector R ? R 2 C , the tree output h(x) is given as:</p><formula xml:id="formula_2">h(x) = R? I(F 1 (x) -b 1 ) I(b 1 -F 1 (x)) ? . . . ? I(F C (x) -b C ) I(b C -F C (x))</formula><p>, .</p><p>(1) where I is the step function, ? is the outer product and ? is the inner product. Both feature functions F c and I would prevent differentiability. To address this, F c (x) is replaced with a weighted sum of features with a temperature annealing that makes it gradually one-hot:</p><formula xml:id="formula_3">F c (x) = D j=1 x j entmax ? (F c /T ) j , T ? 0. (2)</formula><p>where F c ? R D are the logits for which features to choose, and entmax ? <ref type="bibr" target="#b20">(Peters et al., 2019)</ref> is the entmax normalization function as the sparse version of softmax such that the sum equals to 1. As T ? 0, the output of entmax will gradually become one-hot. Also, we replace I with entmoid which works like a sparse sigmoid that has output values between 0 and 1. Unlike NodeGAM, we also introduce a temperature annealing in the entmoid to make it closer to I since it performs better under our AD objective. That is:</p><formula xml:id="formula_4">entmoid( x T ) ? I, as T ? 0.<label>(3)</label></formula><p>Differentiability of all operations (entmax, entmoid, outer and inner products), render ODT differentiable.</p><p>Stacking trees into deep layers: Similar to residual connections, all tree outputs h(x) from previous layers become the inputs to the next layer. For input features x, the inputs Algorithm 2 DIAD Update Input: Mini-batch X, Tree model M, Smoothing ?, Leaf weights w tl for each tree t and leaf l in M</p><formula xml:id="formula_5">X = MinMaxTransform(X, min=-1, max=1) X U ? U [-1, 1] {Data uniformly drawn from [-1, 1]} E tl = M(X) {counts -See Alg. 1} E tl U = M(X U ) E tl = E tl + ?, E tl U = E tl U + ? {Smooth the counts} V tl = E tl n E tl {Volume ratio} D tl = E tl U n E tl U {Data ratio} M tl = V 2 tl P tl {Second moments} m t ? Bernoulli(p) {Sample masks per tree t} M tl = mt p * M tl + (1 -m t ) * M tl {Per-tree dropout} L M = -t,l M tl {Maximize the second moments} ?tl = V tl /P tl {Sparsity} s tl = ( 2? tl (max ?tl -min ?tl ) -1) {Normalize to [-1, 1] - Eq. 9} w tl = (1 -?)w tl + ?s tl {Update weights -Eq. 8} Optimize L M by Adam optimizer</formula><p>x l to each layer l become:</p><formula xml:id="formula_6">x 1 = x, x l = [x, h 1 (x 1 ), ..., h (l-1) (x (l-1) )] for l &gt; 1.</formula><p>(4) The final output of the model ?(x) is the average of all the tree outputs h 1 , ..., h L of all L layers:</p><formula xml:id="formula_7">?(x) = L l=1 m i=1 h li (x l )/(L ? m).</formula><p>(5)</p><p>GA 2 M design To allow only maximum two-way interactions, for each tree, at most two logits F 1 and F 2 are allowed, and let the rest of the depth the same as either F 1 or F 2 : F c = F c/2 for c &gt; 2 -this allows at most 2 features to interact within each tree. Also, we avoid the connection between two trees that focus on different feature combinations, since it may create higher feature interactions. See Alg. 1 for pseudo code.</p><p>AD Objectives Here we introduce the AD objective for our tree-based model: Partial Identification (PID) <ref type="bibr" target="#b8">(Gopalan et al., 2019)</ref>. Consider all patients admitted into an ICU. We might consider patients with blood pressure (BP) as 300 as anomalous, since it deviates from most others. In this example, BP of 300 is in a "sparse" feature space since very few people are more than 300.</p><p>To formalize this intuition, we need to introduce the concept of volume. We first define the max and min value of each feature value. Then, we define the volume of a tree leaf as the product of the proportion of the splits within the min and max value. For example, assuming the max value of BP is 400 and min value is 0, the tree split of "BP ? 300" has a volume 0.25.</p><p>We define the sparsity s l of a tree leaf l as the ratio between the volume of the leaf V l and the % of data in the leaf D l :</p><formula xml:id="formula_8">s l = V l /D l ,</formula><p>and we treat the higher sparsity as more anomalous. Let's assume only less than 0.1% have values more than 300 and the volume of "BP ? 300" is 0.25, this patient is quite anomalous in the data by having a large sparsity 0.25 0.1% . To learn the effective splitting of regions with high vs. low sparsity, we optimize the tree structures to maximize the variance of sparsity across leafs, as it splits the space into a high (anomalous) and a low (normal) sparsity region.</p><p>Note that the expected sparsity weighted by the number of data in each leaf is a constant 1. Given each tree leaf l, the % of data in the leaf is D l , sparsity s l :</p><formula xml:id="formula_9">E[s] = l [D l s l ] = l [D l V l D l ] = l [V l ] = 1.<label>(6)</label></formula><p>Therefore, maximizing variance equals to maximizing the second moment of sparsity since the first moment is 1:</p><formula xml:id="formula_10">max Var[s] = max l D l s 2 l = max l V 2 l /D l . (7)</formula><p>Estimating volume and the data ratio The above objectives require estimating % of volume V l and % of data D l for each leaf l. However, calculating volume exactly is not trivial in an oblivious decision tree since it involves complicated rules extractions. Instead, we sample random points uniformly in the input space, and count the number of the random points that end up in each tree leaf. And more points in a leaf indicate higher volume. To avoid the zero count in the denominator, we use Laplacian smoothing, which adds a constant ? to each count. We find it's crucial to set a large ?, around 50-100, to encourage models to ignore the tree leaves with fewer counts. Similarly, we estimate D l by counting the data ratio in each mini-batch. We add ? for both V l and D l .</p><p>Regularization To encourage diverse trees, we introduce a per-tree dropout noise on the estimated momentum to make each tree operate on a different subset of samples in a mini-batch. We also restrict each tree to only split on ?% of features randomly to promote diverse trees (Supp. D).</p><p>Updating leafs' weight We set the leafs' response as the sparsity to reflect the degree of the anomaly. However, since sparsity estimation involves randomness, we set the response as the damped value of sparsity to stabilize the </p><formula xml:id="formula_11">w i l = (1 -?)w (i-1) l + ?s i l . (<label>8</label></formula><formula xml:id="formula_12">)</formula><p>Normalizing sparsity Because of the residual connections, the output of each tree adds with the input features and the summation becomes the input to the next tree. But this creates a very large magnitude difference -as the output of trees could have sparsity values up to 10 4 but the input feature is normalized to -1 and 1, naive optimization tends to ignore the input features. Also, the large outputs of trees make fine-tuning hard and lead to inferior performance in the semi-supervised setting (Sec. 5). To circumvent this magnitude difference, we linearly scale the min and max value of the estimated sparsity to -1 and 1:</p><formula xml:id="formula_13">?l = V l /D l , s l = 2? l /(max l ?l -min l ?l ) -1. (9)</formula><p>Algorithm 2 overviews all training update steps.</p><p>Incorporating labeled data To optimize the labeled data in the imbalanced setting, we optimize the differentiable AUC loss <ref type="bibr" target="#b31">(Yan et al., 2003)</ref> which has been shown effective in the imbalanced setting. Specifically, given a mini-batch of labeled positive data X P and labeled negative data X N , model M, it minimizes</p><formula xml:id="formula_14">L P N = 1 |X P ||X N | xp?X P ,xn?X N max(M(x n )-M(x p ), 0).</formula><p>(10) We compare this AUC loss to commonly-used Binary Cross Entropy (BCE) loss (Sec. 5).</p><p>Data Loader Similar to Devnet <ref type="bibr">(Pang et al., 2019a)</ref>, we upsample the positive samples to make each mini-batch have the same number of positive and negative samples. We find it improves over uniform sampling (Sec. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate DIAD on various tabular datasets, in both unsupervised and semi-supervised settings. Detailed experimental settings and additional results are in Supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Unsupervised Anomaly Detection</head><p>We compare methods on 20 tabular datasets, including 14 datasets used in <ref type="bibr" target="#b8">Gopalan et al. (2019)</ref> and 6 larger datasets from <ref type="bibr">Pang et al. (2019a)</ref>. Since it's hard to tune hyperparameters in the unsupervised setting, for fair comparisons we compare all baselines using default hyperparameters. We run experiments 8 times with different random seeds and take average if methods involve randomness.   <ref type="bibr" target="#b10">(Guha et al., 2016)</ref>, LOF <ref type="bibr" target="#b2">(Breunig et al., 2000)</ref> and OC-SVM <ref type="bibr" target="#b24">(Sch?lkopf et al., 2001)</ref>.</p><p>We use 2 aggregate metrics to summarize the performances across datasets: (1) Average: we take the average of AUC across datasets, (2) Rank: to avoid dominant impact of a few datasets, we calculate the rank of each method in each dataset and average across datasets (lower rank is better).</p><p>We demonstrate overall AUC performances in Table <ref type="table" target="#tab_1">2</ref>. On average, our method performs the best in both Average and Rank. DIAD, using up to 2nd order interactions, performs better or on par with other models for most datasets. Compared to PIDForest, DIAD often underperforms on smaller datasets such as Musk and Thyroid, but outperforms on larger datasets like Backdoor, Celeba, Census and Donors.</p><p>In Table . 3, we evaluate the robustness of AD methods with the additional noisy features. More specifically, we follow the experiment settings in <ref type="bibr" target="#b8">Gopalan et al. (2019)</ref> to include 50 additional noisy features which are randomly sampled from [-1, 1] to Thyroid and Mammography datasets, and create Thyroid (noise) and Mammography (noise) respectively. In Table . 3, we show that the performance of DIAD is robust with additional noisy features (76.1?71.1, 85.0?83.1), while others show significant performance degradation. For example, on Thyroid (noise), ICL decreases from 75.9?49.5, KNN from 75.1?49.5, and CO-POD from 77.6?60.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Semi-supervised Anomaly Detection</head><p>Next, we focus on the semi-supervised setting. We show how much DIAD can improve the performance with small  labeled data in comparison to alternatives. We first divide the data into 64%-16%-20% train-val-test splits, and within the training set only a small part of data is labeled. Specifically, we assume the existence of labels for a small subset of the training set (5, 15, 30, 60 or 120 positives and the corresponding negatives to have the same anomaly ratio).</p><p>The validation set is used for model selection and we report the average performances evaluated on the disjoint 10 data splits. We compare with 3 baselines: (1) DIAD w/o PT: we directly optimize our model under the small labeled data without the first AD pre-training stage.</p><p>(2) CST: we compare with the Consistency Loss proposed in <ref type="bibr">Yoon et al. (2020a)</ref> which regularizes the model to make similar predictions between unlabeled data under dropout noise injection.</p><p>(3) DevNet <ref type="bibr">(Pang et al., 2019a)</ref>: the state-of-the-art semisupervised AD methods. Hyperparameters are in Supp. D.2.</p><p>Fig. <ref type="figure">2</ref> shows the AUC across 8 of 15 datasets (the rest can be found in Supp. C). The proposed version of DIAD (blue) outperforms DIAD without pre-training (orange) consistently in 14 of 15 datasets (except Census dataset), which demonstrates that learning the PID objectives from unlabeled data help improve the performance. Second, both the consistency loss (green) and DevNet (red) do not always improve the performance in comparison to the supervised setting. To conclude, DIAD outperforms all baselines and improve from the unlabeled setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Qualitative analyses on GAM explanations</head><p>Explaining anomalous data To let domain experts understand and debug why a sample is considered anomalous, we demonstrate explaining the most anomalous sample considered by DIAD on Mammography dataset. The task is to detect breast cancer from radiological scans, specifically the presence of clusters of microcalcifications that appear bright on a mammogram. The 11k images are segmented and preprocessed by vision pipelines and extracted 6 image-related features including the area of the cell, constrast, and noise etc. In Fig. <ref type="figure" target="#fig_2">3</ref>, we show the most anomalous data and see which feature contributes the most for sparsity (i.e. anomalous). We illustrate why the model predicts this sample as anomalous; the unusually-high 'Contrast' (Fig. <ref type="figure" target="#fig_2">3</ref>(a)) of the image differs from other samples. Also, the unusually high noise (Fig. <ref type="figure" target="#fig_2">3(b</ref>)) and 'Large area' (Fig. <ref type="figure" target="#fig_2">3(c</ref>)) also makes it anomalous. Finally, it is also considered quite anomalous in a 2-way interaction (Fig. <ref type="figure" target="#fig_2">3(d)</ref>). Since the sample has 'middle area' and 'middle gray level' which constitute a rare combination for the dataset.</p><p>Qualitative analyses on the impact of fine-tuning with labeled data In Fig. <ref type="figure" target="#fig_3">4</ref>, we visualize how predictions change before and after fine-tuning with labeled samples on Donors dataset. Donors dataset consists of 620k educational proposals for K12 level with 10 features, and the anomalies are defined as the top 5% ranked proposals as outstanding.</p><p>Here, we show 4 GAM plots before and after fine-tuning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation and sensitivity analysis</head><p>To analyze the source of gains, we perform ablation studies with some variants of DIAD. The results are presented in Table <ref type="table" target="#tab_4">5</ref>. First, we find fine-tuning with AUC is better than BCE. Sparsity normalization plays an important role in fine-tuning, since sparsity could have values up to 10 4 which negatively affect fine-tuning. Upsampling the positive samples also contributes to performance improvements. We also perform a sensitivity analysis in Supp. B that varies hyperparameters in the unsupervised AD benchmarks. Our method performs quite stable in less than 2% differences across a variety of hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussions and Conclusions</head><p>As all unsupervised AD methods rely on approximate objectives to discover anomalies such as reconstruction loss, predicting geometric transformations, or contrastive learning. The objectives inevitably would not align with labels on some datasets, as inferred from the performance ranking fluctuations across datasets. This motivates for abilities to incorporate labeled data to boost performances and incorporate interpretability to find out whether the model could be trusted and whether the approximate objective aligns with the human-defined anomalies.</p><p>Beyond the inspirations from NodeGAM for the model architecture and PID loss for the objective, we introduce novel contributions that are key for highly accurate and interpretable AD: we modify the architecture by temperature annealing, introduce a novel way to estimate and normalize sparsity, propose a novel regularization for improved generalization, and introduce semi-supervised AD via supervised fine-tuning of the unsupervised learnt representations.</p><p>Our contributions play a crucial role to push both unsuper-vised and semi-supervised AD benchmarks. Furthermore, our method provides interpretability which is crucial in high-stakes applications with needs of explanations such as finance or healthcare.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Semi-supervised AD results with smaller validation set</head><p>When we have a small set of labeled data, how should we split it between the train and validation datasets when optimizing semi-supervised methods? In Sec. 4.2 we use 64%-16%-20% for train-val-test splits, and 16% of validation set could be too large for some real-world settings. Does our method still outperform others under a smaller validation set?</p><p>To answer it, we experiment with a much smaller validation set with only 50% and 25% of original validation set (i.e. 8% and 4% of total datasets). In Table <ref type="table" target="#tab_5">7</ref>, we show the average AD performance across 15 datasets with varying size of validation data. With decreasing validation size all methods decrease the performance slightly, our method still consistently outperforms others. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sensitivity Analysis</head><p>Here we perform the sensitivity analysis from our default hyperparameter in the unsupervised AD benchmarks. We exclude Census, NYC taxi, SMTP, and HTTP datasets since some hyperparameters can not be run, resulting in total 14 datasets each with 4 different random seeds. In Fig. <ref type="figure" target="#fig_4">5</ref>, besides showing the average of all datasets (blue), we also group datasets by their sample sizes into 3 groups: (1) N &gt; 10 5 (Orange, 3 datasets), (2) 10 5 &gt; N &gt; 10 4 (green, 5 datasets), and (4) N &lt; 10 4 (red, 6 datasets). Overall DIAD performs quite stable between 82-84 except when (c) No. trees= 50 and (h) smoothing ? 10. We also find that 3 hyperparameters: (a) batch size, (b) No. Layers, and (d) Tree depth that the large group (orange) has an opposite trend than the small group (red). Large datasets prefer smaller batch size, larger layers, and shallower tree depth. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Semi-supervised AD figures</head><p>We experiment with 15 datasets and measure the performance of our methods under a different number of anomalies. We split the dataset into 64-16-20 train-val-test splits and run 10 times to report the mean and standard error. We show the performance in Fig. <ref type="figure" target="#fig_6">6</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Hyperparameters</head><p>Here we list the hyperparameters we use for both unsupervised and semi-supervised experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Unsupervised AD</head><p>Here we list the default hyperparameter for DIAD in Table <ref type="table">8</ref>. Here we explain each specific hyperparameter:</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure1. Overview of the proposed DIAD. In the training stage, we first learn an unsupervised anomaly detection model by interpretable GA 2 M models and PID loss with unlabeled data. Then, we fine-tune the trained unsupervised model with a small amount of labeled data using a differentiable AUC loss. At inference time, we provide both the anomaly score and explanations of the model's decisions with GA 2 M graphs of top contributing features. For example, the example data shown here has high anomaly scores due to its large cell size.</figDesc><graphic url="image-1.png" coords="2,127.10,67.07,340.17,106.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Explanations of the most anomalous sample in the Mammography dataset. We show the top 4 contributing GAM plots with 3 features (a-c) and 1 two-way interaction (d). (a-c) x-axis is the feature value, and y-axis is the model's predicted sparsity (higher sparsity represents more anomalous). Model's predicted sparsity is shown in the blue line. The red backgrounds indicate data density and the green line indicates the value of the most anomalous sample with Sp as its sparsity. The model finds it anomalous because it has high Contrast, Noise and Area differing from values where majority of other samples have. (d) x-axis is the Area and y-axis is the Gray Level with color indicating the sparsity (blue/red indicates anomalous/normal). The green dot is the value of the data that has 0.05 sparsity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure4. 4 GAM plots on the Donors dataset before (orange) and after (blue) fine-tuning on the labeled samples. In (a, b) we show two features that the labeled information agrees with the notion of sparsity; thus, after fine-tuning the magnitude increases. In (c, d) the label information disagrees with the notion of sparsity; thus, the magnitude changes or decreases after the fine-tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure5. Sensitivity analysis. Y-axis shows the average AUC across 14 datasets, and X-axis shows the varying hyperparameters. The dashed line is the default hyperparameter. We show 4 groups: (1) All datasets (Blue), (2) N &gt; 10 5 (Orange), (3) 10 5 &gt; N &gt; 10 4 (green), and (4) N &lt; 10 4 (red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure6. Semi-supervised AD performance in 8 tabular datasets (out of 15) with varying number of anomalies. Our method 'DIAD' (blue) outperforms other semi-supervised baselines. Summarized results can be found in Table.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Figure6. Semi-supervised AD performance in 8 tabular datasets (out of 15) with varying number of anomalies. Our method 'DIAD' (blue) outperforms other semi-supervised baselines. Summarized results can be found in Table.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of related works.</figDesc><table><row><cell>Utilizing unlabeled data</cell><cell>Handling noisy features</cell><cell>Handling heterogen-ous features</cell><cell>Utilizing labeled data</cell><cell>Interpret-ability</cell></row><row><cell>PIDForest</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DAGMM</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GOAD</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Deep SAD</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ICL</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DevNet</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DIAD</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(Ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Unsupervised AD performance (% of AUC) on 18 tabular datasets for DIAD and 9 baselines. Metrics with standard error overlapped with the best number are bolded. Methods not involving randomness do not have standard error. We show the number of samples (N) and the number of features (P). Datasets are ordered by N.</figDesc><table><row><cell></cell><cell cols="2">DIAD PIDForest</cell><cell>IF</cell><cell cols="2">COPOD PCA</cell><cell cols="6">ICL kNN RRCF LOF OC-SVM N</cell><cell>P</cell></row><row><cell>Vowels</cell><cell cols="3">78.3 ? 0.9 74.0 ? 1.0 74.9 ? 2.5</cell><cell>49.6</cell><cell cols="5">60.6 90.8 ? 2.1 97.5 80.8 ? 0.3 5.7</cell><cell>77.8</cell><cell cols="2">1K 12</cell></row><row><cell>Siesmic</cell><cell cols="3">72.2 ? 0.4 73.0 ? 0.3 70.7 ? 0.2</cell><cell>72.7</cell><cell cols="5">68.2 65.3 ? 1.6 74.0 69.7 ? 1.0 44.7</cell><cell>60.1</cell><cell cols="2">3K 15</cell></row><row><cell>Musk</cell><cell cols="9">90.8 ? 0.9 100.0 ? 0.0 100.0 ? 0.0 94.6 100.0 93.3 ? 0.7 37.3 99.8 ? 0.1 58.4</cell><cell>57.3</cell><cell cols="2">3K 166</cell></row><row><cell>Satimage</cell><cell cols="3">99.7 ? 0.0 98.2 ? 0.3 99.3 ? 0.1</cell><cell>97.4</cell><cell cols="5">97.7 98.0 ? 1.3 93.6 99.2 ? 0.2 46.0</cell><cell>42.1</cell><cell cols="2">6K 36</cell></row><row><cell>Thyroid</cell><cell cols="3">76.1 ? 2.5 88.2 ? 0.8 81.4 ? 0.9</cell><cell>77.6</cell><cell cols="5">67.3 75.9 ? 2.2 75.1 74.0 ? 0.5 26.3</cell><cell>54.7</cell><cell cols="2">7K 6</cell></row><row><cell>A. T.</cell><cell cols="3">78.3 ? 0.6 81.4 ? 0.6 78.6 ? 0.6</cell><cell>78.0</cell><cell cols="5">79.2 79.3 ? 0.7 63.4 69.9 ? 0.4 43.7</cell><cell>67.0</cell><cell cols="2">7K 10</cell></row><row><cell>NYC</cell><cell cols="3">57.3 ? 0.9 57.2 ? 0.6 55.3 ? 1.0</cell><cell>56.4</cell><cell cols="5">51.1 64.5 ? 0.9 69.7 54.4 ? 0.5 32.9</cell><cell>50.0</cell><cell cols="2">10K 10</cell></row><row><cell cols="4">Mammography 85.0 ? 0.3 84.8 ? 0.4 85.7 ? 0.5</cell><cell>90.5</cell><cell cols="5">88.6 69.8 ? 2.7 83.9 83.2 ? 0.2 28.0</cell><cell>87.2</cell><cell cols="2">11K 6</cell></row><row><cell>CPU</cell><cell cols="3">91.9 ? 0.2 93.2 ? 0.1 91.6 ? 0.2</cell><cell>93.9</cell><cell cols="5">85.8 87.5 ? 0.3 72.4 78.6 ? 0.3 44.0</cell><cell>79.4</cell><cell cols="2">18K 10</cell></row><row><cell>M. T.</cell><cell cols="3">81.2 ? 0.2 81.6 ? 0.3 82.7 ? 0.5</cell><cell>80.9</cell><cell cols="5">83.4 81.8 ? 0.4 75.9 74.7 ? 0.4 49.9</cell><cell>79.6</cell><cell cols="2">23K 10</cell></row><row><cell cols="4">Campaign 71.0 ? 0.8 78.6 ? 0.8 70.4 ? 1.9</cell><cell>78.3</cell><cell cols="5">73.4 72.0 ? 0.5 72.0 65.5 ? 0.3 46.3</cell><cell>66.7</cell><cell cols="2">41K 62</cell></row><row><cell>smtp</cell><cell cols="3">86.8 ? 0.5 91.9 ? 0.2 90.5 ? 0.7</cell><cell>91.2</cell><cell cols="5">82.3 82.2 ? 2.0 89.5 88.9 ? 2.3 9.5</cell><cell>84.1</cell><cell cols="2">95K 3</cell></row><row><cell>Backdoor</cell><cell cols="3">91.1 ? 2.5 74.2 ? 2.6 74.8 ? 4.1</cell><cell>78.9</cell><cell cols="5">88.7 91.8 ? 0.6 66.8 75.4 ? 0.7 28.6</cell><cell>86.1</cell><cell cols="2">95K 196</cell></row><row><cell>Celeba</cell><cell cols="3">77.2 ? 1.9 67.1 ? 4.8 70.3 ? 0.8</cell><cell>75.1</cell><cell cols="5">78.6 75.4 ? 2.6 56.7 61.7 ? 0.3 56.3</cell><cell>68.5</cell><cell cols="2">203K 39</cell></row><row><cell>Fraud</cell><cell cols="3">95.7 ? 0.2 94.7 ? 0.3 94.8 ? 0.1</cell><cell>94.7</cell><cell cols="5">95.2 95.5 ? 0.2 93.4 87.5 ? 0.4 52.5</cell><cell>94.8</cell><cell cols="2">285K 29</cell></row><row><cell>Census</cell><cell cols="3">65.6 ? 2.1 53.4 ? 8.1 61.9 ? 1.9</cell><cell>67.4</cell><cell cols="5">66.1 58.4 ? 0.9 64.6 55.7 ? 0.1 45.0</cell><cell>53.4</cell><cell cols="2">299K 500</cell></row><row><cell>http</cell><cell cols="4">99.3 ? 0.1 99.2 ? 0.2 100.0 ? 0.0 99.2</cell><cell cols="5">99.6 99.3 ? 0.1 23.1 98.4 ? 0.2 64.7</cell><cell>99.4</cell><cell cols="2">567K 3</cell></row><row><cell>Donors</cell><cell cols="3">87.7 ? 6.2 61.1 ? 1.3 78.3 ? 0.7</cell><cell>81.5</cell><cell cols="5">82.9 65.5 ? 11.8 61.2 64.1 ? 0.0 40.2</cell><cell>70.2</cell><cell cols="2">619K 10</cell></row><row><cell>Average</cell><cell>82.5</cell><cell>80.7</cell><cell>81.2</cell><cell>81.0</cell><cell>80.5</cell><cell>80.3</cell><cell cols="3">70.6 76.8 40.2</cell><cell>71.0</cell><cell>-</cell><cell>-</cell></row><row><cell>Rank</cell><cell>3.6</cell><cell>4.4</cell><cell>4.0</cell><cell>4.2</cell><cell>4.2</cell><cell>4.7</cell><cell>6.6</cell><cell>6.7</cell><cell>9.8</cell><cell>6.8</cell><cell>-</cell><cell>-</cell></row></table><note><p>performance. Specifically, given the training step i, sparsity s i l for each leaf l, and the update rate ?:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Summary of semi-supervised AD performances. We show the average % of AUC across 15 datasets with varying number of anomalies.</figDesc><table><row><cell cols="2">No. Anomalies 0</cell><cell>5</cell><cell>15</cell><cell>30</cell><cell>60 120</cell></row><row><cell>DIAD</cell><cell cols="5">87.1 89.4 90.0 90.4 89.4 91.0</cell></row><row><cell>DIAD w/o PT</cell><cell>-</cell><cell cols="4">86.2 87.6 88.3 87.2 88.8</cell></row><row><cell>CST</cell><cell>-</cell><cell cols="4">85.3 86.5 87.1 86.6 88.8</cell></row><row><cell>DevNet</cell><cell>-</cell><cell cols="4">83.0 84.8 85.4 83.9 85.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Ablation study for semi-supervised AD. We test our method with fine-tuning only AUC vs. BCE loss. The performance benefits from more labels. Removing sparsity normalization substantially decreases the performance.</figDesc><table><row><cell>No. Anomalies</cell><cell>5</cell><cell>15</cell><cell>30</cell><cell>60</cell><cell>120</cell></row><row><cell>DIAD</cell><cell cols="5">89.4 90.0 90.4 89.4 91.0</cell></row><row><cell>Only AUC</cell><cell cols="5">88.9 89.4 90.0 89.1 90.7</cell></row><row><cell>Only BCE</cell><cell cols="5">88.8 89.3 89.4 88.3 89.2</cell></row><row><cell>Unnormalized sparsity</cell><cell cols="5">84.1 85.6 85.7 84.2 85.6</cell></row><row><cell cols="6">No upsampling 88.6 89.1 89.4 88.5 90.1</cell></row><row><cell cols="6">In practice we might not have a large (e.g. 16% of the</cell></row><row><cell cols="6">labeled dat) validation dataset, as in Sec. 4.2, thus, it would</cell></row><row><cell cols="6">be valuable to evaluate the performances of DIAD with</cell></row><row><cell cols="6">a smaller validation dataset. In Table 6, we reduce the</cell></row><row><cell cols="6">validation dataset size to only 4% of the labeled data and</cell></row><row><cell cols="6">find DIAD still consistently outperforms others. Additional</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Summary of Semi-supervised AD performances with varying size of validation set (4%, 8% and 16% of total datasets). We show the average % of AUC across 15 datasets with varying number of anomalies. Our method DIAD still outperforms others consistently.</figDesc><table><row><cell></cell><cell cols="13">25% val data (4% of total data) 50% val data (8% of total data) 100% val data (16% of total data)</cell></row><row><cell>No. Anomalies</cell><cell>5</cell><cell>15</cell><cell>30</cell><cell>60 120</cell><cell>5</cell><cell>15</cell><cell>30</cell><cell>60 120</cell><cell>5</cell><cell>15</cell><cell>30</cell><cell>60</cell><cell>120</cell></row><row><cell cols="14">DIAD w/o PT 85.4 87.1 86.9 86.4 87.9 85.7 86.9 88.0 86.9 87.5 86.2 87.6 88.3 87.2 88.8</cell></row><row><cell>DIAD</cell><cell cols="13">89.0 89.3 89.7 89.1 90.4 89.2 89.7 90.0 89.2 90.6 89.4 90.0 90.4 89.4 91.0</cell></row><row><cell>CST</cell><cell cols="13">83.9 84.9 85.7 85.6 88.2 84.2 85.7 85.8 86.2 87.9 85.3 86.5 87.1 86.6 88.8</cell></row><row><cell>DevNet</cell><cell cols="13">82.0 83.4 84.4 82.0 84.6 83.0 85.0 85.5 83.6 85.5 83.0 84.8 85.4 83.9 85.4</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>? Batch size: the sample size of mini-batch.</p><p>? LR: learning rate.</p><p>? ?: the hyperparameter used to update the sparsity in each leaf (Eq. 8).</p><p>? Steps: the total number of training steps. We find 2000 works well across our datasets.</p><p>? LR warmup steps: we do the learning rate warmup <ref type="bibr" target="#b9">(Goyal et al., 2017)</ref> that linearly increases the learning rate from 0 to 1e-3 in the first 1000 steps.</p><p>? Smoothing ?: the smoothing count for our volume and data ratio estimation.</p><p>? Per tree dropout: the dropout noise we use for the update of each tree.</p><p>? Arch: we adopt the GAMAtt architecture form the NodeGAM <ref type="bibr">(Chang et al., 2021a</ref>).</p><p>? No. layer: the number of layers of trees.</p><p>? No. trees: the number of trees per layer.</p><p>? Addi tree dim: the dimension of the tree's output. If more than 0, it appends an additional dimension in the output of each tree.</p><p>? Tree depth: the depth of tree.</p><p>? Dim Attention: since we use the GAMAtt architecture, this determines the size of the attention embedding. We find tuning more than 32 will lead to insufficient memory in our GPU, and in general 8-16 works well.</p><p>? Column Subsample (?): this controls how many proportion of features a tree can operate on.</p><p>? Temp annealing steps (K), Min Temp: these control how fast the temperature linearly decays from 1 to the mininum temperature (0.1) in K steps. After K training steps, the entmax or entmoid become max or step functions in the model. We adopt 2-stage training. In the 1st stage, we optimize the AD objective and select a best model by the validation set performance under the random search. Then in the 2nd stage, we search the following hyperparameters with No. anomalies=120 to choose the best hyperparamter, and later run through the rest of 5, 15, 30, and 60 anomalies to report the performances.</p><p>? Learning Rate: [5e-3, 2e-3, 1e-3, 5e-4]</p><p>? Loss: ['AUC', 'BCE'].</p><p>Then for each baseline we use the same architecture but tune the hyperparameters for each baseline:</p><p>? CST: the overall loss is calcualted as follows (Eq. 7, 8, 9 in <ref type="bibr">Yoon et al. (2020a)</ref>):</p><p>The supervised loss L s is:</p><p>The consistency loss L u is:</p><p>where the g m (x, m) is to use dropout mask m to remove features and impute it with the marginal feature distribution, and the masks are sampled K times. Since the accuracy is quite stable across different ? and when K ? 20 (Fig. <ref type="figure">10</ref> ? DevNet: they first randomly sample 5000 Gaussian samples with 0 mean and 1 standard deviation and calculate the mean u R and standard deviation ? R :</p><p>r i where r i ? N (0, 1), ? R = standard deviation of {r 1 , r 2 ..., r 5000 }.</p><p>Then they calculate the loss (Eq. 6, 7 in <ref type="bibr">Pang et al. (2019a)</ref>):</p><p>The ? is the deep neural network and the a is set to 5. In short, they try to increase the output of anomalies (y = 1) to be bigger than a and let the output of normal data (y = 0) to be close to 0. We search the DevNet for different learning rates [2e-3, 1e-3, 5e-4].</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Classification-based anomaly detection for general data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.02359</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Improving unsupervised defect segmentation by applying structural similarity to autoencoders</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>L?we</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.02011</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lof: identifying density-based local outliers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Breunig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2000 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2000 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 21th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1721" to="1730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Node-gam: Neural generalized additive model for interpretable deep learning</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goldenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.01613</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">How interpretable and trustworthy are gams?</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lengerich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goldenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="95" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep anomaly detection using geometric transformations</title>
		<author>
			<persName><forename type="first">I</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10917</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pidforest: Anomaly detection via partial identification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Wieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="15809" to="15819" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Accurate</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust random cut forest based anomaly detection on streams</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Schrijvers</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2712" to="2721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cutpaste: Self-supervised learning for anomaly detection and localization</title>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9664" to="9674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Copod: copula-based outlier detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Botta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1118" to="1123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feature selection by sparse forests</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Udell</surname></persName>
		</author>
		<author>
			<persName><surname>Controlburn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1045" to="1054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<title level="m">2008 eighth ieee international conference on data mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="413" to="422" />
		</imprint>
	</monogr>
	<note>Isolation forest</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lpexplain: Local pictorial explanation for outliers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="372" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Explainable deep one-class classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liznerski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Franks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Muller</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=A5VV3UyIQz" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Toward Explainable Deep Anomaly Detection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<idno type="DOI">10.1145/3447548.3470794</idno>
		<ptr target="https://doi.org/10.1145/3447548.3470794" />
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page">2021</biblScope>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep anomaly detection with deviation networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep anomaly detection with deviation networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Sparse sequenceto-sequence models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Niculae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Martins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05702</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Neural transformation learning for deep anomaly detection beyond images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfrommer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rudolph</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16440</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goernitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4393" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep semi-supervised anomaly detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>G?rnitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02694</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Estimating the support of a highdimensional distribution</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1443" to="1471" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Anomaly detection for tabular data with internal contrastive learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shenkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=_hszZbt46bT" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07685</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning and evaluating representations for deep one-class classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HCSgyPUfeDj" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Tack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><surname>Csi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08176</idno>
		<title level="m">Novelty detection via contrastive learning on distributionally shifted instances</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distill-andcompare: Auditing black-box models using transparent model distillation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hooker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AIES</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Discovering outlying aspects in large datasets</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">X</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leckie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ramamohanarao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data mining and knowledge discovery</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1520" to="1555" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Optimizing classifier performance via an approximation to the wilcoxon-mann-whitney statistic</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Dodier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mozer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Wolniewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on machine learning (icml-03)</title>
		<meeting>the 20th international conference on machine learning (icml-03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="848" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Extending the success of self-and semi-supervised learning to tabular domain</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
		<author>
			<persName><surname>Vime</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Extending the success of self-and semi-supervised learning to tabular domain</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
		<author>
			<persName><surname>Vime</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep autoencoding gaussian mixture model for unsupervised anomaly detection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lumezanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
