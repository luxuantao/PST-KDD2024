<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multiple Instance Learning for Labeling Faces in Broadcasting News Video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jun</forename><surname>Yang</surname></persName>
							<email>juny@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rong</forename><surname>Yan</surname></persName>
							<email>yanrong@cs.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multiple Instance Learning for Labeling Faces in Broadcasting News Video</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C05B4CC359DE364F1AFD79A6373CDB19</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing-indexing methods Algorithm</term>
					<term>Performance</term>
					<term>Experimentations Face Labeling</term>
					<term>News Video</term>
					<term>Machine Learning</term>
					<term>Multiple Instance Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Labeling faces in news video with their names is an interesting research problem which was previously solved using supervised methods that demand significant user efforts on labeling training data. In this paper, we investigate a more challenging setting of the problem where there is no complete information on data labels. Specifically, by exploiting the uniqueness of a face's name, we formulate the problem as a special multi-instance learning (MIL) problem, namely exclusive MIL or eMIL problem, so that it can be tackled by a model trained with partial labeling information as the anonymity judgment of faces, which requires less user effort to collect. We propose two discriminative probabilistic learning methods named Exclusive Density (ED) and Iterative ED for eMIL problems. Experiments on the face labeling problem shows that the performance of the proposed approaches are superior to the traditional MIL algorithms and close to the performance achieved by supervised methods trained with complete data labels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Labeling faces appearing in video with their names is of great significance to the better indexing and retrieval of broadcasting news video archive, which records the activities of a large number of important people. In previous works <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b20">20]</ref>, face labeling is achieved by associating faces detected from video frames with people names extracted from video transcript (typically the closed-caption text) in the vicinity of the face. Specifically, in <ref type="bibr" target="#b21">[21]</ref> we formulate this problem as a classification problem as to determine whether the association between a face and a candidate name is correct or incorrect. In this approach, each face-name association is treated as an data instance with a binary "correct/incorrect" label, and a classifier is trained from labeled data using a supervised method to predict the probability that a name is correctly associated with a face.</p><p>The supervised approach requires sufficient labeled training data in order to achieve high accuracy. This demands labeling the correct and incorrect names for a large number of sample faces, which is labor-intensive and time-consuming due to the huge volume of the video data and the lack of effective annotation tools. In this paper, we focus on a more challenging setting of the problem, where there is no complete knowledge on data labels available to the learning methods. Instead, we exploit the partial label information in the form of constraints on data labels which can be derived from the distinct properties of the problem. Specifically, since each face may have only one name, among all the face-name associations between a face and its candidate names, only one can be correct while all the others must be incorrect. It is also possible that none of the associations is correct, which means the face's name is not among the candidate names (i.e., anonymous face). From the machine learning perspective, this corresponds to a setting where unlabeled instances are assigned into a number of groups, and within each group, at most one instance is positive while all the others are negative.</p><p>Multiple Instance Learning (MIL) is a class of learning algorithms for handling problems with only partial label information expressed as the labels on bags of instances. In the MIL setting, unlabeled instances are grouped into a set of bags, and each bag is assigned a binary label which has a logical-or relationship with the instance labels. That is, if a bag is labeled positive, at least one instance in it is positive, and if a bag is labeled negative, all the instances in it are negative. Given the analogy between them, we can formulate face labeling as a MIL problem if <ref type="bibr" target="#b1">(1)</ref> we treat each face-name association as an instance, and group the associations between a specific face and all its candidate names as a bag of instances; <ref type="bibr" target="#b2">(2)</ref> we assign a label on each bag. As we will find out, here labeling a bag is equivalent to judging whether a specific face is anonymous or not, which requires much less user efforts than labeling all the instances in the bag. Therefore, it is appealing to tackle the face labeling problem using MIL methods especially when minimum manual effort is a preference.</p><p>Furthermore, since a face's name is unique, there is exactly one positive instance in each positive bag, denoted as exclusive constraint. This contrasts to the less informative constraint in the general MIL setting, where there is at least one positive instance in each positive bag. Therefore, face labeling belongs to a special type of MIL problem, named as exclusive MIL or eMIL problem. Consequently, the traditional MIL methods are not appropriate solutions to this eMIL problem since they do not take into account the exclusive constraints. In view of this, we propose two discriminative learning algorithms, Exclusive Density (ED) and Iterative ED, to solve eMIL problems and particularly our face labeling problem.</p><p>In this paper, we first review the related work in the area of face detection/identification in news video and multiple instance learning (Section 2). After that, we present an overview of the face labeling problem and discuss its formulation as an eMIL problem (Section 3). Two discriminative learning methods, ED and Iterative ED, are proposed for the eMIL problem (Section 4). Finally, we apply the proposed methods to the face labeling problem and compare their performance with both supervised learning methods and traditional MIL methods (Section 5). The experiments show that the performance of our approaches trained with only partial label information are not only superior to that of traditional MIL methods and but also close to that achieved by supervised methods trained with complete data labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>In this section we review previous work in two areas, face detection/recognition in news video, which is the target application of this paper, as well as multi-instance learning, which inspires the formulation and solution to our problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Faces in News Video</head><p>As a critical step towards semantic video retrieval, video annotation with various semantic concepts has been an active research area in recent years. A number of methods <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr">8,</ref><ref type="bibr" target="#b17">17]</ref> were proposed to address a wide range of concepts such as those related to people (face, anchor), acoustic (speech, music, pause), object (buildings, graphics), location (outdoors, city, studio), genre (weather, financial, sports), and so on. Among them, the detection and recognition of faces is of great significance especially for broadcasting news video, which contains a large number of people as well as their activities. For example, Schneiderman et al. <ref type="bibr" target="#b12">[12]</ref> has proposed a face detector that detects the faces appearing on video frames; Chen et al. <ref type="bibr" target="#b5">[5]</ref> discussed robust face recognition in news video; Yang et al. <ref type="bibr" target="#b20">[20]</ref> addressed the problem of finding named people in news video by combining clues from closed-captions, face similarity, and other semantic concepts. Recently there has been interest on associating faces with names in the news video <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b7">7]</ref>, which is the main focus of this paper.</p><p>Name-It <ref type="bibr" target="#b11">[11]</ref> is the first proposal on associating names with faces in news video, which is done by exploring the cooccurrence between faces in video frames and people names in video closed-captions. The underlying idea is that the (similar) faces that frequently co-occur with a certain name are likely to match the name, and vice versa. Though the-oretically sound, the robustness of this method can be affected by the unreliable face similarity estimation in the lowquality news video, and no serious performance evaluation has been reported on this work. Named Faces system <ref type="bibr" target="#b7">[7]</ref> built a database of named faces by recognizing the people names overlaid on video frames (usually below the faces) using video Optical Character Recognition (OCR). This approach is subject to the quality of video OCR, and more seriously, it cannot handle all the faces whose names are not overlaid on the screen.</p><p>In our previous work <ref type="bibr" target="#b21">[21]</ref>, a supervised approach has been adopted to identify correct associations between faces and names in news video based on multi-modal features. It builds a model from labeled data to predict the probability that a name in the closed-captions matches a face on the video frame, from which we are able to predict the name of each face. Although experimentally effective, this approach requires sufficient training data as a large number of sample faces manually labeled with names to achieve high accuracy. In this paper, we will investigate an alterative approach to the same problem which requires much less manual effort in collecting labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multiple Instance Learning</head><p>Multiple instance learning (MIL) is proposed for problems with incomplete knowledge on data labels. Instead of receiving labeled instances as a conventional supervised method does, a MIL method receives a set of labeled bags, where each bag contains a number of unlabeled instances. If a bag is labeled positive, at least one of its instances must be positive, and if the bag is labeled negative, all of its instances must be negative. Thus, a bag label can be regarded as a constraint in the form of logical-or relationship with the labels of the instances in the bag. The goal of MIL is to derive a hypothesis from the bag labels to classify unseen instances and/or bags.</p><p>MIL was first introduced by Dietterich et al. <ref type="bibr" target="#b6">[6]</ref> to solve the drug activity prediction problem, for which they proposed a class of methods for learning axis-parallel rectangles (APRs) as the target hypothesis. Besides, Diverse Density (DD) is a widely used method for MIL problems proposed by Maron et al. <ref type="bibr" target="#b9">[9]</ref>, and EM-DD was proposed by Zhang et al. <ref type="bibr" target="#b22">[22]</ref> as an iterative variant of DD. Supervised learning algorithms have been adapted to MIL as well, such as the two SVM variants for MIL proposed by Andrews et al. <ref type="bibr" target="#b1">[1]</ref>, and the k-Nearest Neighbor method for MIL proposed by Wang et al. <ref type="bibr" target="#b16">[16]</ref>.</p><p>Besides the drug activity prediction problem <ref type="bibr" target="#b6">[6]</ref>, MIL has been used in a variety of applications. For example, contentbased image retrieval and classification has been a popular application of MIL <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b19">19]</ref>, where each image is modeled as a bag of regions and MIL methods are used to find the regions containing the target object. Similarly, MIL has been applied to text categorization by modeling a document as a bag of paragraphs <ref type="bibr" target="#b1">[1]</ref>. Other applications include person recognition from images <ref type="bibr" target="#b15">[15]</ref> and stock market analysis <ref type="bibr" target="#b9">[9]</ref>. An interesting work closely related to this paper was done by Song et al. <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b15">15]</ref>, who used extended multi-instance learning methods to find faces or other visual objects from images returned by a search engine <ref type="bibr" target="#b14">[14]</ref> or from video snippets returned by a news video retrieval system <ref type="bibr" target="#b15">[15]</ref>. Here, the role of MIL is to distinguish the target face from the other faces that appear in the same image or video segment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">FACE LABELING AS A MULTIPLE INSTANCE LEARNING PROBLEM</head><p>The goal of face labeling in news video is to label important faces appearing in the video with their names. Since news video usually comes with closed-caption text, which contains the names of the people involved in the news, the problem of face labeling boils down to finding correct associations between faces appearing in the video frames and the names found in the closed-captions. Moreover, a news video is temporally partitioned into a series of news stories, where each story consists of a sequence of video shots on the same event. Since each news story is an independent and semantically coherent unit, we assume that the name of a face, if mentioned in closed-captions, will only appear within the boundary of the same story that contains the face. Therefore, labeling a face in a story is to choose the most likely name from a set of candidate names in the closed-captions of that story.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> illustrates the face labeling process in a news story. Faces are detected from video frames using a face detector <ref type="bibr" target="#b12">[12]</ref>, and meanwhile people names are extracted from closed-captions using a named-entity detector <ref type="bibr" target="#b3">[3]</ref>. However, news video usually contains a large number of faces, some of which are anonymous and of little interest to users. For simplicity, we only label the faces of the people who are giving a monologue-style speech in news video. This not only reduces the number of faces, but also ensures that the remaining faces are worthwhile to label, because usually only the important people are given the chance to talk individually in the broadcasting news. The people in monologue speech can be identified either manually or automatically using the approach described in <ref type="bibr" target="#b13">[13]</ref>.</p><p>With the faces and names extracted, face labeling can be formulated as a machine learning problem as to estimating the probability that a given face is correctly associated with a candidate name, expressed as:</p><formula xml:id="formula_0">P (Y = 1|F, N, h) (1)</formula><p>where F and N represents face and name respectively, and Y ∈ {0, 1} is a binary label indicating whether the face is associated with the name. Here h is the hypothesis which consists of a set of model parameters to be determined. In practice, each pair of F and N is described by a feature vector denoted as X, and therefore the probability of a name being associated with a face can be rewritten as:</p><formula xml:id="formula_1">P (Y = 1|F, N, h) = P (Y = 1|X, h) (2)</formula><p>Suppose f i is a face to label, {n i1 , ..., n im i } is the set of candidate names for fi, and xij is the feature vector describing the association between fi and nij. (Note that the set of candidate names is different for different faces, because for each face we only consider the names in the same story as the face.) If the hypothesis h is known, we can compute P (Y = 1|xij, h) as the probability of nij being associated with fi. Therefore, we can predict the name of the face fi to be the one with the highest probability of being associated with f i , expressed as:</p><formula xml:id="formula_2">n * i = arg max n ij log P (Y = 1|x ij , h)<label>(3)</label></formula><p>The feature vector xij for a face-name pair covers a variety of features derived from multiple video modalities, such as speaker identification, overlaid text, temporal video structure, and so on. Instead of presenting the details on how each feature is computed, we summarize all the features in Table <ref type="table" target="#tab_0">1</ref>. As we can see, each feature provides a clue as to how strongly a candidate name is associated with a face from a specific perspective. For example, the temporal distance between the position where the face appears and the position where the name is mentioned is informative since usually short distance indicates a likely match; the gender of the face derived from his/her voice and the gender of the name shows whether their gender matches; the type of face (as anchor, reporter, or news subject) helps match with the type of name to get ride of unlikely candidate names. Interested users may refer to <ref type="bibr" target="#b21">[21]</ref> for a detailed description on the extraction of these features. At the core of the above formulation is the learning of h. A natural solution is to apply a supervised learning model and estimate h from labeled training data in the form of {xij, yij}, where yij ∈ {0, 1} is the manually assigned label indicating whether name n ij is associated with face f i or not. This supervised approach is exactly what we adopted </p><formula xml:id="formula_3">¡ ¢ ¡¡ ¢ ¡£ ¢ ¡¤ ¥ ¡¡ ¥ ¡£ ¦¦¦ ¥ ¡¤ £ ¢ £¡ ¢ ££ ¢ £ § ¥ £¡ ¥ ££ ¦¦¦ ¥ £ § ¢¨¡ ¢ ¨£ ¢ ¨© ¥ ¨¡ ¥ ¨£ ¦¦¦ ¥ ¨© ¢ ¡ ¢ £ ¢ ¥ ¡ ¥ £ ¦¦¦ ¥ ¦¦¦ ¦¦¦ ¦¦¦ ¦¦¦ Figure 2:</formula><p>The formulation of face labeling as a multiple instance learning problem in our previous work <ref type="bibr" target="#b21">[21]</ref>, which achieved reasonable performance in practice. To collect training data for this approach, however, we need to manually label the correct/incorrect names for a large number of sample faces. This means going through a portion of the video in a shot-by-shot manner to judge whether each name found in closed-captions matches each of the faces on the video frame, which is a tedious process. Therefore, it makes sense to explore a more challenging setting where there is no complete information on the data labels. Particularly, we seek for a formulation of face labeling as a special multiple instance learning (MIL) problem.</p><p>As shown in Figure <ref type="figure">2</ref>, each face forms several possible associations with a set of candidate names, and each association maps to an data instance x ij , whose label y ij indicates whether the association is correct (positive) or incorrect (negative). Therefore, each face fi corresponds to a set of instances as {x i1 , ..., x im i }, which can be naturally grouped into a bag of instances, shown by icons of the same shape. Each face belongs to either of the following two types:</p><p>(1) a "non-anonymous" face, such as f1, f2 and f3, for which one of its candidate names is the correct name, indicated by the solid-line link between them in Figure <ref type="figure">2;</ref><ref type="figure"></ref> (2) a "anonymous" face, such as f 4 , for which none of its candidate names is correct. In the supervised setting, we know what the correct (and incorrect) names for each face are, and thus all the instance labels are available. Here, suppose we only know whether a face is anonymous or not, the information known as bag labels. That is, if a face is non-anonymous, there is exactly one positive instance in the corresponding bag (but we do not know which one), and in this case we say the bag is positive; if a face is anonymous, all instances in the bag are negative, and in this case we say the bag is negative.</p><p>In Figure <ref type="figure">2</ref>, instances from positive bags are shown as gray icons, while those from negative bags are shown as white icons. So the question is: can we drive the instance labels from the bag labels?</p><p>We formally define a bag x i and its bag label y i as:</p><formula xml:id="formula_4">x i = {x i1 , ..., x im i } yi = yi1 ∨ ... ∨ yim i (4)</formula><p>where x ij is an instance corresponding to the association between face f i and name n ij , and y ij is its (unknown) instance label. mi is the number of instances in the bag, which is equal to the number of candidate names for face fi. Our goal is to learn a hypothesis h from the given bag labels {y i } that can predict the unknown instance labels {y ij }. This exactly fits into the setting of multiple instance learning (MIL) described in Section 2.2. Therefore, a variety of existing MIL approaches <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b9">9]</ref> can be readily applied to find such a hypothesis and solve the face labeling problem.</p><p>Using MIL methods for face labeling is appealing only if the bag labels as the anonymity of sample faces are less expensive to collect than the instance labels as the correct/incorrect names for the sample faces. This is exactly the case in our problem, since there exist many clues to easily determine the anonymity of a face in monologue speech. In news video, the name of a monologue face usually appears either in the closed-captions or in the overlaid screen text, but very rarely in neither of the places. The faces with names as overlaid text can be easily distinguished using the video OCR techniques. Therefore, most (if not all) of the remaining faces have their names in the closed-captions, so they are "non-anonymous faces" and correspond to positive bags in our formulation. If we only focus on these faces, the bag labels are readily available without manual effort. Note that not labeling the faces with overlaid names does not hurt the generality of our approach much, because their names can be easily labeled using video OCR techniques. Besides this clue, there are other clues that help determine the anonymity of a monologue face, e.g., faces appearing for a long duration or for many times are rarely anonymous, and so on. Therefore, obtaining bag labels in this problem demands significantly less human efforts and time than obtaining the complete instance labels.</p><p>A careful comparison reveals an important difference between face labeling and the standard MIL problem. In the face labeling problem, there is exactly one positive instance in each positive bag, since each face may have only one name. We call this "exclusive constraint", which is more informative than the logical-or constraint in the standard MIL setting, where there are at least one instance in a positive bag. Thus, we consider face labeling as a special MIL problem that contains extra information in the form of exclusive constraints. We call such MIL problems exclusive MIL or eMIL problems. Apparently, traditional MIL methods <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b9">9]</ref> are not appropriate solutions to an eMIL problem, since they overlook the additional information in exclusive constraints. (However, this is not the drawbacks of these MIL methods because they are for general MIL problems which do not necessarily satisfy exclusive constraints.) Actually, using MIL methods for an eMIL problem may pose mistakes in the search for optimal hypothesis. For example, Figure <ref type="figure" target="#fig_1">3</ref> shows a projection of the data instances from Figure <ref type="figure">2</ref> in a 2-D feature space, with two possible hypotheses A and B shown as rectangles. The instances inside each rectangle are classified as "genuinely" positive instances and those outside are classified as negative instances. According to the definition, both A and B are valid hypotheses for a MIL problem because each of them include at least one instance from each positive bag and meanwhile exclude all instances from negative bags. For an eMIL problem, however, B is only longer a valid hypothesis since it violates the exclusive constraints by classifying more than one instance in a positive bag as positive, whereas A is still valid since exactly one instance in each positive bag is classified positive. Thus, if applying MIL methods to this data-set, there is a high chance of concluding to the wrong hypothesis B, which may result in poor performance of the face labeling problem. In view of this, we discuss two discriminative learning methods, Exclusive Density (ED) and Iterative ED, which take advantage of the exclusive constraints in eMIL problems and therefore avoid mistakes as those discussed above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">LEARNING METHODS</head><p>We start this section by reviewing a widely-used general MIL approach Diverse Density (DD), and then propose Exclusive Density(ED) as a discriminative learning method for eMIL problems. Finally, an iterative variant of ED is presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Notations</head><p>The data D includes {x 1 , ..., x n } as a set of n bags and {y1, ..., yn} as their bag labels, where yi ∈ {0, 1}. Each bag xi consists of a set of instances as xi = {xi1, ..., xin i }, where x ij denotes the j th instance in bag x i . Each instance is described by a feature vector, and x ijd denotes the value of the d th feature component of instance xij. The labels of instances in bag xi are {yi1, ..., yin i }. In the standard MIL setting, instance labels are bounded by the bag label through a logical-or relationship, i.e., y i = y i1 ∨...∨y in i . In eMIL problems, there is a stronger constraint as y i = j y ij ∈ {0, 1} due to the exclusive constraints. For simplicity, we denote the probability of bag label P (Y = yi|yi, h) as P r(+|yi, h) if y i = 1, or as P (-|x i , h) if y i = 0. Similarly, we denote the probability of instance label P (Y = y ij |x ij , h) as P r(+|x ij , h) if y ij = 1, or as P (-|x ij , h) if y ij = 0. In our approach, a hypothesis h = (µ, σ) consists of two parts: the coordinates of a concept point as µ, and a scale vector σ defining the weights on different feature dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Diverse Density</head><p>Diverse Density (DD) <ref type="bibr" target="#b9">[9]</ref> is a widely used MIL method. The underlying idea of DD is very intuitive: it tries to find a concept point in the feature space so that at least one instance from each positive bag is close to it, while all the instances from negative bags are far away from it. Thus, the diverse density as the "goodness" metric of a concept point is measured by how many positive bag has at least one instance close to it, and how far the instances in negative bags are away from it. Formally, the diverse density of a hypothesis h is defined as the data likelihood DD(h) = P (h|x + 1 , ..., x + n , x - 1 , ..., x - m ) and the optimal hypothesis can be found by maximizing DD(h) over the hypothesis space. Note that here we follow the original notation in <ref type="bibr" target="#b9">[9]</ref>, where x + i and x - i denotes a positive or a negative bag, and x + ij or x - ij denotes the j th instance from a positive or a negative bag. With the assumption of uniform prior on the hypothesis space and the independence between bags given the hypothesis, this is equivalent to maximize i P (x + i |h) i P (x - i |h). By applying the Bayes' rule, we have the following:</p><formula xml:id="formula_5">h DD = arg max h∈H i P (h|x + i ) i P (h|x - i )<label>(5)</label></formula><p>where P (h|x + i ) and P (h|x - i ) are defined by P (h|x + ij ) and P (h|x - ij ) using noise-or model:</p><formula xml:id="formula_6">P (h|x + i ) = 1 - j (1 -P (h|x + ij ))<label>(6)</label></formula><formula xml:id="formula_7">P (h|x - i ) = j (1 -P (h|x - ij ))<label>(7)</label></formula><p>The causal probability P (h|x ij ) is estimated by Gaussianlike distribution exp(-</p><formula xml:id="formula_8">d (1/σ d ) 2 (x ijd -µ d ) 2</formula><p>), which is inversely related to the Euclidean distance between x ijd and µ. The optimal h is found by gradient search using Eq(5) as the objective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Exclusive Density (ED)</head><p>Inspired by DD, we propose Exclusive Density (ED) for eMIL problems where exclusive constraints exist. Similar to the intuition of DD, ED tries to find a concept point in the feature space so that exactly one instance from each positive bag is close to it, while all the instances from negative bags are far away from it. Here, the "goodness" of a concept point is measured by how many positive bag has exactly one instance close to it, and how far the instances in negative bags are away from it. One can see that the key difference between DD and ED is on whether the optimal concept point should be close to "at least one" or "exactly one" positive instance in each positive bag. This echoes the difference between MIL and eMIL problem.</p><p>Formally, we define the exclusive density<ref type="foot" target="#foot_0">1</ref> of a hypothesis as the conditional likelihood of the bag labels given the data and the hypothesis, expressed as ED(h) = L(h; D) = P (y1, ..., yn|x1, ..., xn, h) The optimal hypothesis hED can be found by maximizing this conditional likelihood, i.e., arg max h L(h; D). Under the assumption of the independence between bag labels given the data and the hypothesis, this is equivalent to:</p><formula xml:id="formula_9">hED =arg max h∈H i P (yi|xi, h) =arg max h∈H ∀i:y i =1 P (+|x i , h) ∀i:y i =0 P (-|x i , h)<label>(8)</label></formula><p>To represent Eq(8) as a function of h, the first step is to transform the probability on bag labels into probability on instance labels. This requires different transformations for positive and negative bags. With the exclusive constraints, a positive bag has only one positive instance and the rest are all negative instances. Therefore, the probability of a bag's label being positive depends on how likely one instance in the bag generates a positive label and meanwhile how likely the other instances generate negative labels. This leads to an intuitive definition as:</p><formula xml:id="formula_10">P (+|x i , h) = 1 Z i max j {P (+|x ij , h) k =j P (-|x ik , h)} (9)</formula><p>where Zi is a normalization factor. Since we do not know which is the only positive instance in the bag, the max() function intends to test all possible configurations and find the largest separation between any single instance and the other instances in the bag. However, since max() is not differentiable, no optimization methods can directly work on Eq <ref type="bibr" target="#b9">(9)</ref>. Therefore, we adopt the same noise-or model used in DD as a differentiable, soft-max function, which is expressed as max(x 1 , ..,</p><formula xml:id="formula_11">x n ) ≈ 1 -i (1 -x i )</formula><p>. Thus, we approximate Eq(9) by the following equations:</p><formula xml:id="formula_12">P (+|xi, h) = 1 Z i {1 - j (1 -P (+|x ij , h) k =j P (-|x ik , h))} (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>Since all the instances in a negative bag must be negative, the probability that a bag generates a negative label is high if every instance in the bag has a high probability of being negative, which leads to:</p><formula xml:id="formula_14">P (-|xi, h) = 1 Zi j P (-|xij, h)<label>(11)</label></formula><p>The normalization factor Z i in Eq <ref type="bibr" target="#b10">(10)</ref> and Eq <ref type="bibr" target="#b11">(11)</ref> is added to ensure well-defined probabilities, i.e., y i P (yi|xi, h) = 1. Therefore, we have:</p><formula xml:id="formula_15">Z i ={1 - j (1 -P (+|x ij , h) k =j P (-|x ik , h))} + j P (-|xij, h)</formula><p>According to Eq <ref type="bibr" target="#b10">(10)</ref> and Eq <ref type="bibr" target="#b11">(11)</ref>, for a concept point to receive a high exclusive density ED(h), it must be close to exactly one instance in each positive bag and meanwhile far away from the other, and also far away from the instances in negative bags.</p><p>To define P (+|x ij , h), we conceive each instance label y ij as a random variable drawn from a Bernoulli distribution parameterized by the data point xij and the hypothesis h = (µ, σ). Specifically, we define</p><formula xml:id="formula_16">P (+|x ij , h) = f ( x ij -µ p ) P (-|x ij , h) = 1 -P (+|x ij , h) (<label>12</label></formula><formula xml:id="formula_17">)</formula><p>where xij -µ p is the p-norm distance between xij and the concept point µ, and f (•) is a function transforming the distance into a probability. Apparently, f (•) ∈ [0, 1] and it should be inversely related to the distance so that an instance closer to the concept point has a higher probability of generating a positive label. This formulation implies that, there is a Bernoulli distribution at each point of the feature space, and the label of an instance is drawn from the distribution at the corresponding point.</p><p>Although f can be any function satisfying the above requirements, we define f (x) = exp(-x) and use the generalized L-2 distance metric, which leads to a Gaussian-like from:</p><formula xml:id="formula_18">P (+|x ij , h) = exp(- d (x ijd -µ d ) 2 2σ 2 d )<label>(13)</label></formula><p>The final optimization function is given by plugging Eq(10) and Eq <ref type="bibr" target="#b11">(11)</ref> into Eq <ref type="bibr">(8)</ref>. Since it is differentiable, we use gradient search method with multiple starting points to find h that maximizes ED(h). Intuitively, the instances from every positive bag are good starting points, since some of them (actually, one in each bag) should be close to the true concept.</p><p>After h has been optimized, we can use it to predict the instance labels in the data. Under the exclusive constraints, only one instance in a positive bag can be positive, so it has to the one with the highest probability of generating a positive label:</p><formula xml:id="formula_19">yij = 1 if j = arg max k P (+|x ik , h) 0 otherwise (14)</formula><p>According to the definition of negative bags, all the instances in a negative bag are set to negative.</p><p>In practice, we made two modifications on the ED algorithm. First, since the normalization factors Zi significantly increases the model complexity and the running time, we removes it from the optimization function of ED. We found experimentally this does not cause any non-trivial changes of the performance. The second modification is based on our observation that ED has a bias towards the instances that are likely to be negative more strongly than to the positive ones. One major reason is that the probability of a positive bag label as defined in Eq( <ref type="formula">9</ref>) is dominated by negative instances. As a result, the scale factors may become extremely large in order to fit the negative instances well, which sometimes causes numerical difficulties in the optimization process. To alleviate this problem, we modify the probability of a positive bag label in Eq <ref type="bibr" target="#b9">(9)</ref> into P (+|xi, h) = 1 Z i maxj P (+|xij, h) k =j P (-|x ik , h) γ i , where γ i is introduced to balance off the contribution of positive and negative instances. Similar modification is made to Eq <ref type="bibr" target="#b10">(10)</ref>. Intuitively, in our experiments we set γi = 1</p><formula xml:id="formula_20">n i -1</formula><p>with n i being the number of instance in x + i , so that the contribution from the two sides are made equal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Iterative ED</head><p>The ED algorithm uses gradient search to optimize a complex objective function which leads to a very inefficient implementation. In this section, we present an efficient iterative method called Iterative ED which maximizes a simplified objective function that is related to ED. The idea was inspired by the notion that the difficulty of an eMIL problem mainly comes from the ambiguity of not knowing which instance is the only positive instance in a positive bag. In the ED algorithm, this ambiguity is modeled by the max() function in Eq <ref type="bibr" target="#b9">(9)</ref>, which is then replaced by a very complicated noise-or model in Eq <ref type="bibr" target="#b10">(10)</ref> as its differentiable approximation. The same problem exists in the standard MIL setting, for which Zhang et al. <ref type="bibr" target="#b22">[22]</ref> have proposed a EM-style variant of DD called EM-DD to tackle the ambiguity by modeling the knowledge on which instance is positive in a bag using a set Algorithm 1 Iterative ED: An iterative variant of ED Input: a set of bags and the labels as { X 1 , y 1 , ..., Xn, yn } Output: a hypothesis h(µ, σ).</p><p>1: Choose an initial hypothesis h (0) . 2: Randomly initialize the instance labels {y (0) ij }.</p><p>3: Choose as the minimum increase of the objective function for each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Dif f = M axInteger 5: t = 0 6: while Dif f &gt; do 7: for each positive bag x i do 8:</p><formula xml:id="formula_21">y (t+1) ij = 1 if j = arg max k P (+|x ik , h (t) ) 0 otherwise</formula><p>9: end for 10:</p><formula xml:id="formula_22">h (t+1) = arg max h∈H CED({y (t+1) ij }, h) 11: Dif f = CED (t+1) -CED (t) 12: t = t + 1 13: end while 14: Return h (t)</formula><p>of missing variables.</p><p>Iterative ED uses an idea similar to EM-DD. Suppose we know which instance is the only positive instance in each positive bag under the current hypothesis h (t) , and thus all the other instances are all negative. This means all the instance labels {y (t) ij } are available. Therefore, we directly optimize our hypothesis by maximizing the data likelihood defined on instance labels instead of on bag labels. We call this simplified likelihood function as Complete-data Exclusive Density (CED), since it supposes the unknown instance labels {y ij } are available. It is expressed as:</p><formula xml:id="formula_23">CED (t) = CED({y (t) ij }, h (t) ) = L(h (t) ; D (t) ) = ∀i,j:y (t) ij =1 P (+|x ij , h (t) ) ∀i,j:y (t) ij =0 P (-|x ij , h (t) ) (15)</formula><p>By maximizing this function over the hypothesis space, we will get a new hypothesis, which in turn can be used to update instance labels. Iterative ED takes the advantage of the cyclic nature of this process. Starting from an initial hypothesis, it repeatedly performs the following two steps: In first step, it computes the conditional probability P (+|xij, h (t) ) of the instances in each positive bag, and set their labels {y (t) ij } such that the one with the highest probability is labeled positive and the rest are labeled negative; In the second step, it updates the hypothesis h (t+1) by maximizing CED (t) using gradient search. Then we replace h (t) by h (t+1) and repeat the two steps until the algorithm converges. The pseudo-code of this process is shown in Algorithm 1. Its convergence is proved below. Proposition 1. Iterative ED algorithm converges.</p><p>Proof. The convergence of the algorithm is proved by showing (1) the objective function CED (t) is monotonically increasing, and (2) it has an upper bound. ( <ref type="formula">2</ref>) is obvious since CED (t) is defined as a product of probabilities in Eq <ref type="bibr" target="#b15">(15)</ref> and thus CED (t) ≤ 1. To prove (1), note that CED (t) is completely defined by a hypothesis h (t) and the instance labels as {y (t) ij }. Hence, in each iteration, CED (t)  may change only at Step 8 where {y (t) ij } is updated or at Step 10 where h (t) is updated. In Step 10, the increase of CED (t)   is guaranteed by the gradient ascent algorithm. In Step 8, CED (t) changes when the instance labels in at least one positive bag are different from the last step. Without loss of generality, we assume that only positive instance in bag x i changes from x ij 1 in step t to x ij 2 in step t + 1, and that is the only change happened in Step 8. This means y (t) ij 1 = 1 and y (t) ij 2 = 0 in step t, while y (t) ij 1 = 0 and y (t) ij 2 = 1 in step t + 1. This change also implies P (+|xij 2 , h) &gt; P (+|xij 1 , h), based on which we derive:</p><formula xml:id="formula_24">CED (t+1) CED (t) = P (+|x ij 2 , h)P (-|x ij 1 , h) P (+|xij 1 , h)P (-|xij 2 , h) = P (+|X ij 2 , h)(1 -P (+|X ij 1 , h)) P (+|x ij 1 , h)(1 -P (-|x ij 2 , h)) &gt; 1</formula><p>which shows CED (t+1) &gt; CED (t) . Thus, the monotonic increase of CED (t) is proved.</p><p>Although not a strict EM algorithm, Iterative ED works in an EM-style procedure that estimates the instance labels as missing variables and updates the hypothesis to maximize the likelihood in an interleaved manner. As pointed out by <ref type="bibr" target="#b22">[22]</ref>, such an iterative algorithm may help avoid being trapped in local minima since the algorithm makes sharp changes in the hypothesis when its guess of the instance labels changes. Further, Iterative ED is more efficient because it has a much simpler objective function than that of ED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head><p>In this section, we first describe the test data set and the experiment set-up for the face labeling problem. Then we present the performance of the proposed learning methods and compare it with other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data Set</head><p>The test data of the face labeling problem are collected from the news video archive of ABC World News Tonight in 1998. Specifically, we use a collection of the news video in 20 days with 30 minutes per day. In the preprocessing stage, faces are detected from video frames using a face detector <ref type="bibr" target="#b12">[12]</ref>, and people names are extracted from closedcaptions using a named-entity detector <ref type="bibr" target="#b3">[3]</ref>. As mentioned in Section 3, we only label the faces in monologue-style speech, which can be automatically identified by a monologue detector <ref type="bibr" target="#b21">[21]</ref>. Faces of anchors and reporters are further removed since they are not interesting to users. Finally, there are 476 monologue faces to label, among which 234 faces have their names found in the closed-captions (i.e., non-anonymous), and the other 242 faces are simply anonymous. In average there is 4.7 candidate names for every face to label. As discussed in Section 3, we treat each face-name association as an instance, and group the associations (instances) related to the same face into a bag. This results in 476 bags (234 positive and 242 negative) consisting of totally 2236 instances. Each instance as a face-name association is described by a 8-dimensional feature vector described in Table <ref type="table" target="#tab_0">1</ref>. The work described in this paper has been also incorporated into a news video browsing and retrieval system. As shown in Figure <ref type="figure" target="#fig_2">4</ref>, the interface of the system displays bounding boxes around the detected faces in each video shot. When the cursor is moved over a face, it shows two most likely names of that face predicted using the methods proposed here. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiment Set-up</head><p>We conduct the experiments in two settings. In the first setting, PosOnly, only the 1284 instances from 234 the positive bags are used, which means we try to label only the non-anonymous faces. For comparison purpose, we evaluate the performance of 6 algorithms: ED and IterED are the exclusive density and iterative ED method proposed for eMIL problem; DD <ref type="bibr" target="#b9">[9]</ref> and EM-ED <ref type="bibr" target="#b22">[22]</ref> are widely used traditional MIL algorithms; LDA and SVM are two supervised learning algorithms, namely linear discriminant analysis and support vector machine (with RBF kernel). The first 4 algorithms work only with the bag labels (which are all positive in this setting) and thus it is legitimate to use the same data-set for both training and testing; LDA and SVM work in supervised setting with all instance labels available, and 10-fold cross-validation is used to evaluate their performance. The two supervised methods are included to give an upper bound of the performance, so that we can see how close (to this upper bound) our methods can achieve without knowing the instance labels. We label a face using the name which receives the highest probability of being associating with the face. The performance is evaluated by the accuracy of predicted names, which is the ratio of the correctly labeled faces against all the faces. Note that this is different from the classification accuracy of instance labels, since a bag of correctly labeled instances (names) only translates into one correctly labeled face. In the second setting, AllData, we use all the 2236 instances from both positive and negative bags. This includes both the anonymous faces and the non-anonymous faces. In this case, since the instances in negative bags are all known to be negative, we cannot use the same data-set for testing and training. Therefore, we use 10-fold cross-validation to evaluate the performance of all the 6 algorithms.</p><p>We ran ED, IterED, EM, and EM-DD all with 50 starting points, where each starting point is the feature vector of an instance randomly chosen from positive bags. This brings up the issue of combining the hypotheses resulted from the multiple runs. We use two strategies, max, which uses the single hypothesis that achieves the maximum value of the data likelihood function and discards the other hypotheses, and avg, which keeps all the hypotheses and computes the probability of an instance's label as the average of its probabilities predicted under all the hypotheses.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>Table <ref type="table" target="#tab_1">2</ref> summarizes the accuracies of face labels predicted by the 6 algorithms under the PosOnly and the AllData setting. Several interesting observations can be made. First, in the PosOnly setting, ED and IterED outperforms DD and EM-DD by a large margin. This indicates the importance of exploiting the exclusive constraints in the eMIL problem. Second, in the AllData setting, due to the availability of additional instances with known negative labels, the performance of almost all the algorithms improves. However, the improvement of ED/IterED is not very significant, and their advantage over DD/EM-DD shrinks compared with that in the PosOnly setting. This implies that our ED/IterED algorithm does a better job than DD/EM-DD in terms of making use of the unlabeled data so that the additional (labeled) data do not help much. Third, the performance of ED, which is trained with only the bag labels, approaches the performance of LDA and SVM, which are trained with fully labeled instances in supervised setting. This is a significant observation which means the proposed methods can solve the face labeling problem almost as well as the supervised methods while requiring less user efforts in collecting training data labels. Fourth, despite the efficiency issue, the two iterative variants, i.e., IterDD and EM-DD, work slightly worse than their original versions. Fifth, there is no strong evidence preferring avg or max, since neither of them is consistently better than the other.</p><p>We also compare the efficiency of the proposed algorithms with the traditional MIL methods. Table <ref type="table" target="#tab_2">3</ref> summarizes the average time needed for running each algorithm in the AllData setting using a single starting point. As expected, the ED algorithm is about two times slower than DD, because its optimization function is more complicated than the latter one, although both of them use gradient descent as the optimization method. A bit surprisingly, between the iterative variants of these two algorithms, IterED does not enhance the efficiency (from ED) as much as EM-DD does. A possible explanation is that, even after removing the sof tmax function in the optimization function, IterED's objective function still involves the probabilities of all the instances and therefore is rather complex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>We have investigated the problem of face labeling with only partial information on training data labels. Specifically, we have presented a formulation of the problem as an exclusive MIL or eMIL problem, and proposed two discriminative learning methods to address such problems. The effectiveness of the proposed methods has been demonstrated by the experiments on the face labeling problem, where their performance is superior to the traditional MIL algorithms and close to that achieved by supervised methods trained with complete data labels.</p><p>Although the proposed methods are applied only to the face labeling data-set, they are applicable to any other problems that can be formulated as eMIL problems. Such problems can be frequently observed in many video/image applications, in which the exclusive constraints are available because of the unique identity of real-world objects. We give some examples below:</p><p>• In the surveillance video of a hospital, there is often a need to recognize the identity of the patients captured in the video to monitor their behaviors. Since each person may correspond to only one patient-ID, this problem can be formulated as an eMIL problem if we treat the associations between an observed person and the possible patient-IDs as a bag of instances.</p><p>• To automatically generate a meeting minute from the video recording of a meeting, one needs to identify the speaker behind each voice. Since one voice has only one speaker-ID, this is also an eMIL problem if the association between a voice and the possible speaker-IDs are considered as a bag of instances.</p><p>• A user of an image retrieval system may want to find a unique object, say, "Statue of Liberty", from a set of images, where each image is partitioned into several regions. If we treat regions of an image as a bag of instances, this is also an eMIL problem since "Statute of Liberty" can appear in only one of the regions.</p><p>Besides exploring the above problems, we can also improve the proposed methods which are very limited in terms of representation power since they are unable to model complicated class boundaries. Therefore, we plan to adapt more sophisticated supervised learning methods such as SVM to eMIL problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of face labeling in one story of the news video. Faces detected in monologue-speech shots are associated with people names found in the closed-captions.</figDesc><graphic coords="3,140.51,54.62,328.42,168.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example data set in 2-d feature space, where icons of the same shape represent instances from the same bag. Gray icons are instances from positive bags and white ones are instances from negative bags. Two hypotheses A and B are shown as rectangles, and the instances inside each rectangle are classified as (hypothesized) positive instances while those outside are classified as negative.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Interface showing the predicted names of a face</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>A feature set describing the association between a face and a name Modality Feature DescriptionFace Type type-match whether the predicted type of shot as anchor, reporter, or news-subject matches with the type of name derived by text analysis Temporal Relationship face-name-dist the temporal distance (seconds) between face and the nearest occurrence of name name-rank the rank of name among the all the names ranked by their distance to face face-name-order whether name is mentioned before, within, or after face Overlaid Text vocr-similarity the similarity between name and the video OCR output of the shot containing face vocr-present whether there is overlaid text on the shot containing face</figDesc><table><row><cell>Speaker</cell><cell>utter-name</cell><cell>whether name is uttered by the person of that face</cell></row><row><cell>Identity</cell><cell>gender-match</cell><cell>whether the gender of name matches the gender of the person of face</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison on accuracies of face labels</figDesc><table><row><cell>Algorithm</cell><cell>Hypotheses Selection</cell><cell cols="2">Accuracy of face labels PosOnly AllData</cell></row><row><cell>ED</cell><cell>avg max</cell><cell>0.590 0.590</cell><cell>0.597 0.593</cell></row><row><cell>IterED</cell><cell>avg max</cell><cell>0.543 0.577</cell><cell>0.573 0.592</cell></row><row><cell>DD</cell><cell>avg max</cell><cell>0.491 0.449</cell><cell>0.548 0.568</cell></row><row><cell>EM-DD</cell><cell>avg max</cell><cell>0.470 0.478</cell><cell>0.440 0.502</cell></row><row><cell cols="2">LDA (supervised)</cell><cell>0.606</cell><cell>0.621</cell></row><row><cell cols="2">SVM (supervised)</cell><cell>0.616</cell><cell>0.631</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of average running time</figDesc><table><row><cell>Algorithm</cell><cell>Average running time (sec)</cell></row><row><cell>ED</cell><cell>222.4</cell></row><row><cell>IterED</cell><cell>127.1</cell></row><row><cell>DD</cell><cell>105.0</cell></row><row><cell>EMDD</cell><cell>4.7</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We name our method as exclusive density due to its close connections with Diverse Density. It does not mean a density function.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGEMENTS</head><p>This work is supported in part by the Advanced Research and Development Activity (ARDA) under contract numbers NBCHC040037 and H98230-04-C-0406.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Support vector machines for multiple-instance learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="561" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Names and faces in news</title>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="848" to="854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Nymble: a high-performance learning name-finder</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Combining text and audio-visual features in video indexing</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICASSP 2005</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Toward robust face recognition from multiple views</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int&apos;l Conference on Multimedia and Expo</title>
		<meeting>of Int&apos;l Conference on Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Solving the multiple instance problem with axis-parallel rectangles</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lozano-Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="31" to="71" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Named faces: Putting names to faces</title>
		<author>
			<persName><forename type="first">R</forename><surname>Houghton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="45" to="50" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic image annotation and retrieval using cross-media relevance models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 26th Int&apos;l ACM SIGIR Conference on Research and Development in Informaion Retrieval</title>
		<meeting>of the 26th Int&apos;l ACM SIGIR Conference on Research and Development in Informaion Retrieval</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="119" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A framework for multiple-instance learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lozano-Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="570" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiple-instance learning for natural scene classification</title>
		<author>
			<persName><forename type="first">O</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Ratan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Int&apos;l Conf. on Machine Learning</title>
		<meeting>15th Int&apos;l Conf. on Machine Learning</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Name-it: Association of face and name in video</title>
		<author>
			<persName><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of the Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="368" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Object detection using the statistics of parts</title>
		<author>
			<persName><forename type="first">H</forename><surname>Schneiderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="151" to="177" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Detection of TV news monologues by style analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int&apos;l Conference on Multimedia &amp; Expo</title>
		<meeting>of the IEEE Int&apos;l Conference on Multimedia &amp; Expo</meeting>
		<imprint>
			<date type="published" when="2004-06">June 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Autonomous visual model building based on image crawling through internet search engines</title>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-T</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Workshop on Multimedia Information Retrieval</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="315" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cross-modality automatic face model training from large video databases</title>
		<author>
			<persName><forename type="first">M.-T</forename><forename type="middle">S</forename><surname>Song Xiaodan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ching-Yung</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Face Processing in Video</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Solving the multiple instance problem: A lazy learning approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Zucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th Int&apos;l Conf. on Machine Learning</title>
		<meeting>17th Int&apos;l Conf. on Machine Learning</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1119" to="1125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Optimal multimodal fusion for multimedia data analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 12th annual ACM Int&apos;l Conf. on Multimedia</title>
		<meeting>of the 12th annual ACM Int&apos;l Conf. on Multimedia</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="572" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised cross feature learning for semantic concept detection in video</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Naphade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image database retrieval with multiple-instance learning techniques</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lozano-Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Internatinal Conf. on Data Engineering</title>
		<meeting>of Internatinal Conf. on Data Engineering</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="233" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Finding person X: Correlating names with visual appearances</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 3rd Int&apos;l Conf. on Image and Video Retrieval</title>
		<meeting>of 3rd Int&apos;l Conf. on Image and Video Retrieval</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="270" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Naming every individual in news video monologues</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 12th annual ACM Int&apos;l Conf. on Multimedia</title>
		<meeting>of the 12th annual ACM Int&apos;l Conf. on Multimedia</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Em-dd: An improved multiple-instance learning technique</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="1073" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Content-based image retrieval using multiple-instance learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fritts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 19th Int&apos;l Conf. on Machine Learning</title>
		<meeting>19th Int&apos;l Conf. on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="682" to="689" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
