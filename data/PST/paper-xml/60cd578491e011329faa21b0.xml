<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Topology Distillation for Recommender System</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-06-16">16 Jun 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Seongku</forename><surname>Kang</surname></persName>
							<email>seongku@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junyoung</forename><surname>Hwang</surname></persName>
							<email>jyhwang@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wonbin</forename><surname>Kweon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hwanjo</forename><surname>Yu</surname></persName>
							<email>hwanjoyu@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Topology Distillation for Recommender System</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-06-16">16 Jun 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3447548.3467319</idno>
					<idno type="arXiv">arXiv:2106.08700v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Information systems → Learning to rank</term>
					<term>Collaborative filtering</term>
					<term>Retrieval efficiency Recommender System</term>
					<term>Knowledge Distillation</term>
					<term>Relational Knowledge</term>
					<term>Model Compression</term>
					<term>Retrieval efficiency</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recommender Systems (RS) have employed knowledge distillation which is a model compression technique training a compact student model with the knowledge transferred from a pre-trained large teacher model. Recent work has shown that transferring knowledge from the teacher's intermediate layer significantly improves the recommendation quality of the student. However, they transfer the knowledge of individual representation point-wise and thus have a limitation in that primary information of RS lies in the relations in the representation space. This paper proposes a new topology distillation approach that guides the student by transferring the topological structure built upon the relations in the teacher space. We first observe that simply making the student learn the whole topological structure is not always effective and even degrades the student's performance. We demonstrate that because the capacity of the student is highly limited compared to that of the teacher, learning the whole topological structure is daunting for the student. To address this issue, we propose a novel method named Hierarchical Topology Distillation (HTD) which distills the topology hierarchically to cope with the large capacity gap. Our extensive experiments on real-world datasets show that the proposed method significantly outperforms the state-of-the-art competitors. We also provide in-depth analyses to ascertain the benefit of distilling the topology for RS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The size of recommender systems (RS) has kept increasing, as they have employed deep and sophisticated model architectures to better understand the complex nature of user-item interactions <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26]</ref>. A large model with many learning parameters has a high capacity and therefore generally achieves higher recommendation accuracy. However, it also requires high computational costs, which results in high inference latency. For this reason, it is challenging to adopt such a large model to the real-time platform <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>To tackle this problem, Knowledge Distillation (KD) has been adopted to RS <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31]</ref>. KD is a model-independent strategy to improve the performance of a compact model (i.e., student) by transferring the knowledge from a pre-trained large model (i.e., teacher). The distillation is conducted in two steps. The teacher is first trained with the training set, and the student is trained with help from the teacher along with the training set. The student model, which is a compact model, is used in the inference time. During the distillation, the teacher can provide additional supervision that is not explicitly revealed from the training set. As a result, the student trained with KD shows better prediction performance than the student trained only with the training set. Also, it has low inference latency due to its small size.</p><p>Most existing KD methods for RS transfer the knowledge from the teacher's predictions <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26]</ref> (Figure <ref type="figure" target="#fig_0">1a</ref>). They basically enforce the student to imitate the teacher's recommendation results, providing guidance to the predictions of the student. There is another recent approach that transfers the latent knowledge from the teacher's intermediate layer <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31]</ref>, pointing out that the predictions incompletely reveal the teacher's knowledge and the intermediate representation can additionally provide a detailed explanation on the final prediction of the teacher. They adopt hint regression <ref type="bibr" target="#b19">[20]</ref> that makes the student's representation approximate the teacher's representation via a few regression layers. This enables the student to get compressed information on each entity (e.g., user and item) (Figure <ref type="figure" target="#fig_0">1b</ref>) that can restore more detailed preference information in the teacher <ref type="bibr" target="#b7">[8]</ref>.</p><p>However, the existing hint regression-based methods focus on distilling the individual representation of each entity, disregarding the relations of the representations. In RS, each entity is better understood by its relations to the other entities rather than by its individual representation. For instance, a user's preference is represented in relation to (and in contrast with) other users and items. Also, the student can take advantage of the space, where the relations found by the teacher are well preserved, in finding more accurate ranking orders among the entities and thereby improving the recommendation performance.</p><p>This paper proposes a new distillation approach that effectively transfers the relational knowledge existing in the teacher's representation space. A natural question is how to define the relational knowledge and distill it to the student. We build a topological structure that represents the relations in the teacher space based on the similarity information, then utilize it to guide the learning of the student via distillation. Specifically, we train the student with the distillation loss that preserves the teacher's topological structure in its representation space along with the original loss function.</p><p>Trained with the topology distillation, the student can better preserve the relations in the teacher space, which not only improves the recommendation performance but also better captures the semantic of entities (reported in Section 4.3). However, we observe that simply making the student learn all the topology information (Figure <ref type="figure" target="#fig_0">1c</ref>) is not always effective and sometimes even degrades the student's recommendation performance (reported in Section 4.2). This phenomenon is explained by the huge capacity gap between the student and the teacher; the capacity of the student is highly limited compared to that of the teacher, and learning all the topological structure in the teacher space is often daunting for the student. To address this issue, we propose a method named Hierarchical Topology Distillation (HTD) which effectively transfers the vast teacher's knowledge to the student with limited capacity. HTD represents the topology hierarchically and transfers the knowledge in multi-levels using the hierarchy (Figure <ref type="figure" target="#fig_0">1d</ref>). Specifically, HTD adaptively finds preference groups of entities such that the entities within each group share similar preferences. Then, the topology is hierarchically structured in group-level and entity-level. The group-level topology represents the summarized relations across the groups, providing an overview of the whole topology. The entity-level topology represents the relations of entities belonging to the same group. This provides a fine-grained view on important relations among the entities having similar preferences, which directly affects the top-𝑁 recommendation performance. By compressing the complex individual relations across the groups, HTD relaxes the daunting supervision and enables the student to better focus on the important relations. In summary, the key contributions of our work are as follows:</p><p>• We address the necessity of transferring the relational knowledge from the teacher representation space and develop a general topology distillation approach for RS. • We develop a new topology distillation method, named FTD, designed to guide the student by transferring the full topological structure built upon the relations in the teacher space.</p><p>• We propose a novel topology distillation method, named HTD, designed to effectively transfer the vast relational knowledge to the student considering the huge capacity gap. • We validate the superiority of the proposed approach by extensive experiments. We also provide in-depth analyses to verify the benefit of distilling the topological structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Knowledge Distillation. Knowledge distillation (KD) is a modelindependent strategy that accelerates the training of a student model with the knowledge transferred from a pre-trained teacher model. Most KD methods have mainly focused on the image classification task. An early work <ref type="bibr" target="#b4">[5]</ref> matches the class distributions (i.e., the softmax output) of the teacher and the student. The class distribution has richer information (e.g., inter-class correlation) than the one-hot class label, which improves learning of the student model. Pointing out that utilizing the predictions alone is insufficient because meaningful intermediate information is ignored, subsequent methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> have distilled knowledge from the teacher's intermediate layer along with the predictions. <ref type="bibr" target="#b19">[20]</ref> proposes "hint regression" that matches the intermediate representations. Subsequently, <ref type="bibr" target="#b27">[28]</ref> matches the gram matrices of the representations, <ref type="bibr" target="#b28">[29]</ref> matches the attention maps from the networks, and <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24]</ref> match the similarities on activation maps of the convolutional layer.</p><p>Reducing inference latency of RS. As the size of RS is continuously increasing, various approaches have been proposed for reducing the model size and inference latency. Several methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b29">30]</ref> have utilized the binary representations of users and items. With the discretized representations, the search costs can be considerably reduced via the hash technique. However, due to their restricted capability, the loss of recommendation accuracy is inevitable <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">26]</ref>. Also, various computational acceleration techniques have been successfully adopted to reduce the search costs. In specific, order-preserving transformations <ref type="bibr" target="#b0">[1]</ref>, pruning and compression techniques <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25]</ref>, tree-based data structures <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, and approximated nearest-neighbor search <ref type="bibr" target="#b20">[21]</ref> have been employed to reduce the inference latency. However, they have limitations in that the techniques are only applicable to specific models or easy to fall into a local optimum because of the local search <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Knowledge Distillation for RS. KD, which is the model-agnostic strategy, has been widely adopted in RS. Similar to the progress on computer vision, the existing methods are categorized into two groups (Figure <ref type="figure" target="#fig_0">1a</ref>): (1) the methods distilling knowledge from the predictions, (2) the methods distilling the latent knowledge from the intermediate layer. Note that the two groups of methods can be utilized together to fully improve the student <ref type="bibr" target="#b7">[8]</ref>.</p><p>(1) KD by the predictions. Motivated by <ref type="bibr" target="#b4">[5]</ref> that matches the class distributions, most existing methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26]</ref> have focused on matching the predictions (i.e., recommendation results) from the teacher and the student. The teacher's predictions convey additional information about the subtle difference among the items, helping the student generalize better than directly learning from binary labels <ref type="bibr" target="#b11">[12]</ref>. This research direction focuses on designing a method effectively utilizing the teacher's predictions. First, <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22]</ref> distill the knowledge of the items with high scores in the teacher's predictions. Since a user is interested in only a few items, distilling knowledge of a few top-ranked items is effective to discover the user's preferable items <ref type="bibr" target="#b21">[22]</ref>. Most recently, <ref type="bibr" target="#b9">[10]</ref> utilizes rankdiscrepancy information between the predictions from the teacher and the student. Specifically, <ref type="bibr" target="#b9">[10]</ref> focuses on distilling the knowledge of the items ranked highly by the teacher but ranked lowly by the student. On the one hand, <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26]</ref> focus on distilling ranking order information from the teacher's predictions. Concretely, they adopt listwise learning <ref type="bibr" target="#b26">[27]</ref> and train the student to follow the items' ranking orders predicted by the teacher.</p><p>(2) KD by the latent knowledge. Pointing out that the predictions incompletely reveal the teacher's knowledge, a few methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31]</ref> <ref type="foot" target="#foot_0">1</ref> have focused on distilling latent knowledge from the teacher's intermediate layer. The existing methods are based on hint regression <ref type="bibr" target="#b19">[20]</ref> proposed in computer vision. Let ℎ 𝑡 : X → R 𝑑 𝑡 denote a mapping function from the input feature space to the representation space of the teacher (i.e., the teacher nested function up to the intermediate layer). Similarly, let ℎ 𝑠 : X → R 𝑑 𝑠 denote a mapping function to the representation space of the student. Also, let e 𝑡 𝑖 = ℎ 𝑡 (x 𝑖 ) and e 𝑠 𝑖 = ℎ 𝑠 (x 𝑖 ) denote the representations of entity 𝑖 from the two spaces <ref type="foot" target="#foot_1">2</ref> , where x 𝑖 is entity 𝑖's input feature. The hint regression makes e 𝑠 𝑖 approximate e 𝑡 𝑖 as follows:</p><formula xml:id="formula_0">L 𝐻𝑖𝑛𝑡 = ∥e 𝑡 𝑖 − 𝑓 (e 𝑠 𝑖 )∥ 2 2 (1)</formula><p>where 𝑓 : R 𝑑 𝑠 → R 𝑑 𝑡 is a small network to bridge the different dimensions (𝑑 𝑠 &lt;&lt; 𝑑 𝑡 ). By minimizing L 𝐻𝑖𝑛𝑡 , parameters in the student (i.e., ℎ 𝑠 ) and 𝑓 are updated. Also, it is jointly minimized with the base model (i.e., L 𝐵𝑎𝑠𝑒 + 𝜆L 𝐻𝑖𝑛𝑡 ) which can be any existing recommender. The hint regression enables e 𝑠 to capture compressed information that can restore detailed information in e 𝑡 <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31]</ref>. <ref type="bibr" target="#b30">[31]</ref> adopts this original hint regression to improve the student.</p><p>The most recent work DE <ref type="bibr" target="#b7">[8]</ref> further elaborates this approach for RS. DE argues that using a single network (𝑓 ) makes the knowledge of entities having dissimilar preferences get mixed, and this degrades the quality of distillation. Its main idea is that the knowledge of entities having similar preferences should be distilled without being mixed with that of entities having dissimilar preferences. To this end, DE clusters the representations into 𝐾 groups based on the teacher's knowledge and distills the representations in each group via a separate network 𝑓 𝑘 . Let z 𝑖 be a 𝐾-dimensional one-hot vector whose element 𝑧 𝑖𝑘 = 1 if entity 𝑖 belongs to the corresponding 𝑘-th group. For each entity 𝑖, DE loss is defined as follows:</p><formula xml:id="formula_1">L 𝐷𝐸 = ∥e 𝑡 𝑖 − 𝐾 ∑︁ 𝑘=1 𝑧 𝑖𝑘 𝑓 𝑘 (e 𝑠 𝑖 )∥ 2 2</formula><p>(2)</p><p>The one-hot vector is sampled from a categorical distribution with class probabilities 𝜶 𝑖 = 𝑣 (e 𝑡 𝑖 ), i.e., z 𝑖 ∼ Categorical 𝐾 (𝜶 𝑖 ), where 𝑣 : R 𝑑 𝑡 → R 𝐾 is a small network with Softmax output. The sampling process is approximated by Gumbel-Softmax <ref type="bibr" target="#b5">[6]</ref> and trained via backpropagation in an end-to-end manner. In sum, the representations belonging to the same group share similar preferences and are distilled via the same network without being mixed with the representations belonging to the different groups <ref type="bibr" target="#b7">[8]</ref>.</p><p>The existing methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31]</ref> based on the hint regression distill the knowledge of individual entity without consideration of how the entities are related in the representation space. Considering a user's preference is represented in relation to (and in contrast with) other users and items, each entity is better understood by its relations to the other entities rather than by its individual representation. Also, the student can take advantage of the space, where the relations found by the teacher are well preserved, in finding more accurate ranking orders among the entities and thereby improving the recommendation performance. In this work, we propose a new distillation approach for RS that directly distills the relational knowledge from the teacher's representation space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGIES</head><p>We first provide an overview of the proposed approach (Section 3.1). Before we describe the final solution, we explain a naive method for incorporating the relational knowledge in the distillation process (Section 3.2). Then, we shed light on the drawbacks of the method when applying it for KD. Motivated by the analysis, we present a new method, named HTD, which distills the relational knowledge in multi-levels to effectively cope with a large capacity gap between the teacher and the student (Section 3.3). The pseudocodes of the proposed methods are provided in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview of Topology Distillation</head><p>The proposed topology distillation approach guides the learning of the student by the topological structure built upon the relational knowledge in the teacher representation space. The relational knowledge refers to all the information on how the representations are correlated in the space; those sharing similar preferences are strongly correlated, whereas those with different preferences are weakly correlated. We build a (weighted) topology of a graph where the nodes are the representations and the edges encode the relatedness of the representations. Then, we distill the relational knowledge by making the student preserve the teacher's topological structure in its representation space. With the proposed approach, the student is trained by minimizing the following loss:</p><formula xml:id="formula_2">L = L 𝐵𝑎𝑠𝑒 + 𝜆 𝑇 𝐷 L 𝑇 𝐷<label>(3)</label></formula><p>where 𝜆 𝑇 𝐷 is a hyperparameter controlling the effects of topology distillation. The base model can be any existing recommender, and L 𝐵𝑎𝑠𝑒 corresponds to its loss function. L 𝑇 𝐷 is defined on the topology of the representations in the same batch used for the base model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Full Topology Distillation (FTD)</head><p>As a straightforward method, we distill the knowledge of the entire relations in the teacher space. Given a batch, we first generate a fully connected graph in the teacher representation space. The graph is characterized by the adjacency matrix A 𝑡 ∈ R 𝑏×𝑏 where 𝑏 is the number of representations in the batch. Each element 𝑎 𝑡 𝑖 𝑗 is the weight of an edge between entities 𝑖 and 𝑗 representing their similarity and is parameterized as follows:</p><formula xml:id="formula_3">𝑎 𝑡 𝑖 𝑗 = 𝜌 (e 𝑡 𝑖 , e 𝑡 𝑗 )<label>(4)</label></formula><p>where 𝜌 (•, •) is a similarity score such as the cosine similarity or negative Euclidean distance, in this work we use the former. Analogously, we generate a graph, characterized by the adjacency matrix A 𝑠 ∈ R 𝑏×𝑏 , in the student representation space, i.e., 𝑎 𝑠 𝑖 𝑗 = 𝜌 (e 𝑠 𝑖 , e 𝑠 𝑗 ). After obtaining the topological structures A 𝑡 and A 𝑠 from the representation space of the teacher and student, respectively, we train the student to preserve the topology discovered by the teacher by the topology-preserving distillation loss as follows:</p><formula xml:id="formula_4">L 𝐹𝑇 𝐷 = Dist(A 𝑡 , A 𝑠 ) = ∥A 𝑡 − A 𝑠 ∥ 2 𝐹 ,<label>(5)</label></formula><p>where Dist(•, •) is the distance between the topological structures, in this work, we compute it with the Frobenius norm. By minimizing L 𝐹𝑇 𝐷 , parameters in the student are updated. As this method utilizes the full topology as supervision to guide the student, we call it Full Topology Distillation (FTD). Substituting the distillation loss L 𝑇 𝐷 in Equation 3 derives the final loss for training the student.</p><p>Issues: Although FTD directly transfers the relational knowledge which is ignored in the previous work, it still has some clear drawbacks. Because the student has a very limited capacity compared to the teacher, it is often daunting for the student to learn all the relational knowledge in the teacher. Indeed, we observe that sometimes FTD even hinders the learning of the student and degrades the recommendation performance (reported in Section 4.2). Therefore, the relational knowledge should be distilled with consideration of the huge capacity gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hierarchical Topology Distillation (HTD)</head><p>Our key idea to tackle the issues is to decompose the whole topology hierarchically so as to be effectively transferred to the student. We argue that the student should focus on learning the relations among the strongly correlated entities that share similar preferences and accordingly have a direct impact on top-𝑁 recommendation performance. To this end, we summarize the numerous relations among the weakly correlated entities, enabling the student to better focus on the important relations.</p><p>During the training, HTD adaptively finds preference groups of strongly correlated entities. Then, the topology is hierarchically structured in group-level and entity-level: 1) group-level topology includes the summarized relations across the groups, providing the overview of the entire topology. 2) entity-level topology includes the relations of entities belonging to the same group. This provides fine-grained supervision on important relations of the entities having similar preferences. By compressing the complex individual relations across the groups, HTD relaxes the daunting supervision, effectively distills the relational knowledge to the student.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.1</head><p>Preference Group Assignment. To find the groups of entities having a similar preference in an end-to-end manner considering both the teacher and the student, we borrow the idea of DE <ref type="bibr" target="#b7">[8]</ref>. Formally, let there exist 𝐾 preference groups in the teacher space. We use a small network 𝑣 : R 𝑑 𝑡 → R 𝐾 with Softmax output to compute the assignment probability vector 𝜶 𝑖 ∈ R 𝐾 for each entity 𝑖 as follows:</p><formula xml:id="formula_5">𝜶 𝑖 = 𝑣 (e 𝑡 𝑖 ),<label>(6)</label></formula><p>where each element 𝛼 𝑖𝑘 encodes the probability of the entity 𝑖 to be assigned to 𝑘-th preference group. Let z 𝑖 be a 𝐾-dimensional one-hot assignment vector whose element 𝑧 𝑖𝑘 = 1 if entity 𝑖 belongs to the corresponding 𝑘-th group. We assign a group for each entity by sampling the assignment vector from a categorical distribution parameterized by {𝛼 𝑖𝑘 } i.e., 𝑝 (𝑧 𝑖𝑘 = 1 | 𝑣, e 𝑡 𝑖 ) = 𝛼 𝑖𝑘 . To make the sampling process differentiable, we adopt Gumbel-Softmax <ref type="bibr" target="#b5">[6]</ref> which is a continuous distribution on the simplex that can approximate samples from a categorical distribution.</p><formula xml:id="formula_6">𝑧 𝑖𝑘 = exp ((𝛼 𝑖𝑘 + 𝑔 𝑘 ) /𝜏) 𝐾 𝑗=1 exp 𝛼 𝑖 𝑗 + 𝑔 𝑗 /𝜏 for 𝑘 = 1, ..., 𝐾,<label>(7)</label></formula><p>where 𝑔 𝑗 is the gumbel noise drawn from Gumbel(0, 1) distribution <ref type="bibr" target="#b5">[6]</ref> and 𝜏 is the temperature parameter. We set a small value on 𝜏 so that samples from the Gumbel-Softmax distribution become onehot vector <ref type="bibr" target="#b5">[6]</ref>. This group assignment is evolved during the training via backpropagation <ref type="bibr" target="#b7">[8]</ref>. This will be explained in Section 3.3.4.</p><p>With the assignment process, for a given batch, HTD obtains the grouping information summarized by a 𝑏 × 𝐾 assignment matrix Z where each row corresponds to the one-hot assignment vector for each entity. We also denote the set of entities belonging to each group by 𝐺 𝑘 = {𝑖 |𝑧 𝑖𝑘 = 1}. Note that the group assignment is based on the teacher's knowledge. Based on the assignment, we decompose the topology hierarchically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.2</head><p>Group-level topology. HTD introduces a prototype representing the entities in each preference group, then use it to summarize the relations across the groups. Let E 𝑡 ∈ R 𝑏×𝑑 𝑡 and E 𝑠 ∈ R 𝑏×𝑑 𝑠 denote the representation matrix in the teacher space and the student space, respectively. The prototypes P 𝑡 ∈ R 𝐾×𝑑 𝑡 and P 𝑠 ∈ R 𝐾×𝑑 𝑠 are defined as follows:</p><formula xml:id="formula_7">P 𝑡 = Z⊤ E 𝑡 and P 𝑠 = Z⊤ E 𝑠 , (<label>8</label></formula><formula xml:id="formula_8">)</formula><p>where Z is normalized assignment matrix by the number of entities in each group (i.e., Z[:,𝑖 ] = Z [:,𝑖 ] / 𝑖 Z [:,𝑖 ] ). For P, each row P [𝑘,:] corresponds to the average representation for the entities belonging to each group 𝑘, and we use it as a prototype representing the group.</p><p>With the prototypes, we consider two design choices with different degrees of relaxation. In the first choice, we distill the relations between the prototypes. We build the topology characterized by the 𝐾 × 𝐾 matrix H 𝑡 which contains the relations as:</p><formula xml:id="formula_9">ℎ 𝑡 𝑘𝑚 = 𝜌 (P 𝑡 [𝑘,:] , P 𝑡 [𝑚,:] ),<label>(9)</label></formula><p>where 𝑘, 𝑚 ∈ {1, ..., 𝐾 }.</p><p>In the second choice, we distill the relations between each prototype and entities belonging to the other groups. We build the topology characterized by the 𝐾 × 𝑏 matrix H 𝑡 which contains the relations as:</p><formula xml:id="formula_10">ℎ 𝑡 𝑘 𝑗 = 𝜌 (P 𝑡 [𝑘,:] , e 𝑡 𝑗 ),<label>(10)</label></formula><p>where 𝑘 ∈ {1, ..., 𝐾 }, 𝑗 ∈ {1, ..., 𝑏}. It is worth noting that we only distill the relations across the groups (i.e., 𝑗 ∉ 𝐺 𝑘 ). Using one of the choices, we build the group-level topological structure H 𝑡 in the teacher space, and analogously, we build H 𝑠 in the student space.</p><p>The first choice puts a much higher degree of relaxation compared to the second choice. For instance, assume that there are two groups of entities (i.e., 𝐺 1 and 𝐺 2 ). Without the hierarchical approach (as done in FTD), there exists |𝐺 1 | × |𝐺 2 | relations across the groups. With the first choice, they are summarized to a single relation between two prototypes, and with the second choice, they are summarized to |𝐺 1 | + |𝐺 2 | relations. We call the first choice as Group(P,P) and the second choice as Group(P,e) and provide results with each choice in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.3</head><p>Entity-level topology. HTD distills the full relations among the strongly correlated entities in the same group. In the teacher space, the entity-level topology contains the following relations:</p><formula xml:id="formula_11">{𝜌 (e 𝑡 𝑖 , e 𝑡 𝑗 ) | (𝑖, 𝑗) ∈ 𝐺 𝑘 × 𝐺 𝑘 }, for 𝑘 ∈ {1, ..., 𝐾 },<label>(11)</label></formula><p>and analogously, we build the entity-level topology in the student space. For an efficient computation on matrix form, we introduce the 𝑏 × 𝑏 binary indicator matrix M = ZZ ⊤ indicating whether each relation is contained in the topology or not. Intuitively, each element 𝑚 𝑖 𝑗 = 1 if entity 𝑖 and 𝑗 are assigned to the same group, otherwise 𝑚 𝑖 𝑗 = 0. Then, the entity-level topology is defined by A 𝑡 with M in the teacher space and also defined by A 𝑠 with M in the student space. The distance between two topological structures is simply computed by ∥M ⊙ (A 𝑡 − A 𝑠 )∥ 2 𝐹 where ⊙ is the Hadamard product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.4</head><p>Optimization. HTD guides the student with the decomposed topological structures. The loss function is defined as follows:</p><formula xml:id="formula_12">L 𝐻𝑇 𝐷 = 𝛾 ∥H 𝑡 − H 𝑠 ∥ 2 𝐹 + ∥M ⊙ (A 𝑡 − A 𝑠 )∥ 2 𝐹 + (1 − 𝛾) 𝑏 ∑︁ 𝑖=1 ∥e 𝑡 𝑖 − 𝐾 ∑︁ 𝑘=1 𝑧 𝑖𝑘 𝑓 𝑘 (e 𝑠 𝑖 )∥ 2 2 ,<label>(12)</label></formula><p>where the first term corresponds to the topology-preserving loss, the second term corresponds to the hint regression loss adopted in DE <ref type="bibr" target="#b7">[8]</ref> that makes the group assignment process differentiable. We put a network 𝑓 𝑘 for each 𝑘-th group, then train each network to reconstruct the representations belonging to the corresponding group, which makes the entities having strong correlations get distilled by the same network <ref type="bibr" target="#b7">[8]</ref>. 𝛾 is a hyperparameter balancing the two terms. In this work, we set 0.5 to 𝛾. By minimizing L 𝐻𝑇 𝐷 , parameters in the student, 𝑣 and 𝑓 * are updated. Note that 𝑣 and 𝑓 * are not used in the inference phase, they are only utilized for the distillation in the offline training phase. Substituting the distillation loss L 𝑇 𝐷 in Equation 3 derives the final loss for training the student.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of HTD.</head><p>For more intuitive understanding, we provide a visualization of the relational knowledge distilled to the student in Figure <ref type="figure" target="#fig_1">2</ref>. We randomly choose a preference group and visualize the relations w.r.t. the entities belonging to the group. Note that there exist the same number of intra-group relations (red) in both figures. Without the hierarchical approach (as done in FTD), the student is forced to learn a huge number of relations with the entities belonging to the other groups (blue). On the other hand, HTD summarizes the numerous relations, enabling the student to better focus on learning the detailed intra-group relations, which directly affects in improving the recommendation performance.</p><p>Discussions on Group Assignment. Note that HTD is not limited to a specific group assignment method, i.e., DE. Any method that clusters the teacher representation space or prior knowledge of user/item groups (e.g., item category, user demographic features) can be utilized for more sophisticated topology decomposition, which further improves the effectiveness of HTD. The comparison with another assignment method is provided in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We validate the proposed approach on 18 experiment settings: 2 real-world datasets × 3 base models × 3 different student model sizes (Section 4.1). We first present comparison results with the state-of-the-art competitor and a detailed ablation study (Section 4.2). We also provide in-depth analyses to verify the benefit of distilling the topological structure (Section 4.3). Lastly, we provide a hyperparameter study (Section 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>We closely follow the experiment setup of DE <ref type="bibr" target="#b7">[8]</ref>. However, for a thorough evaluation, we make two changes in the setup. 1) we add LightGCN <ref type="bibr" target="#b2">[3]</ref>, which is the state-of-the-art top-𝑁 recommendation method, as a base model. 2) unlike <ref type="bibr" target="#b7">[8]</ref> that samples negative items for evaluation, we adopt the full-ranking evaluation which enables more rigorous evaluation. Refer to the appendix for more detail.</p><p>4.1.1 Datasets. We use two real-world datasets: CiteULike and Foursquare. CiteULike contains tag information for each item, and Foursquare contains GPS coordinates for each item. We use the side information to evaluate the quality of representations induced by each KD method. More details of the datasets are provided in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Base Models.</head><p>We evaluate the proposed approach on base models having different architectures and learning strategies, which are widely used for top-𝑁 recommendation task.</p><p>• BPR <ref type="bibr" target="#b18">[19]</ref>: A learning-to-rank model that models user-item interaction with Matrix Factorization (MF). • FitNet <ref type="bibr" target="#b19">[20]</ref>: A KD method utilizing the original hint regression.</p><p>• Distillation Experts (DE) <ref type="bibr" target="#b7">[8]</ref>: The state-of-the-art KD method distilling the latent knowledge. DE elaborates the hint regression. • Full Topology Distillation (FTD): A proposed method that distills the full topology (Section 3.2). • Hierarchical Topology Distillation (HTD): A proposed method that distills the hierarchical topology (Section 3.3). Note that we do not include the methods distilling the predictions (e.g., RD <ref type="bibr" target="#b21">[22]</ref>, CD <ref type="bibr" target="#b11">[12]</ref>, and RRD <ref type="bibr" target="#b7">[8]</ref>) in the competitors, because they are not competing with the methods distilling the latent knowledge <ref type="bibr" target="#b7">[8]</ref>. Instead, we provide experiment results when they are combined with the proposed approach in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Analysis</head><p>Table <ref type="table">1</ref> presents top-𝑁 recommendation performance of the methods compared (𝜙 = 0.1), and Figure <ref type="figure" target="#fig_4">4</ref> presents results with three different students sizes. For the group-level topology of HTD, we choose Group(P,e), since it consistently shows better results than Group(P,P). The detailed comparisons along with other various ablations are reported in Table <ref type="table" target="#tab_0">2</ref>. Lastly, results with prediction-based KD method are presented in Figure <ref type="figure" target="#fig_5">5</ref> and Figure <ref type="figure" target="#fig_6">6</ref>.</p><p>Overall Evaluation. In Table <ref type="table">1</ref>, we observe that HTD achieves significant performance gains compared to the main competitor, i.e., DE. This result shows that distilling the relational knowledge provides better guidance than only distilling the knowledge of individual representation. We also observe that FTD is not always effective and sometimes even degrades the student's recommendation performance (e.g., LightGCN on CiteULike). As the student's size is highly limited compared to the teacher in the KD scenario, learning all the relational knowledge is daunting for the student, which leads to degrade the effects of distillation. This result supports our claim that the relational knowledge should be distilled considering the huge capacity gap. Also, the results show that HTD  successfully copes with the issue, the student to effectively learn the relational knowledge.</p><p>In Figure <ref type="figure" target="#fig_4">4</ref>, we observe that as the model size increases, the performance gap between FTD and HTD decreases. FTD achieves comparable and even higher performance than HTD when the student has enough capacity (e.g., LightGCN on Foursquare with 𝜙 = 1.0). This result again verifies that the effectiveness of the proposed topology distillation approach. It also shows that FTD can be applied to maximize the performance of recommender in the scenario where there is no constraint on the model size by self-distillation.</p><p>Comparison with ablations. In Table <ref type="table" target="#tab_0">2</ref>, we provide the comparison with diverse ablations. For FTD, we report the results when it is used with the state-of-the-art hint regression method (denoted as FTD+DE). As HTD includes DE for the group assignment, comparison with FTD+DE shows the direct impacts of topology relaxation. We observe that FTD+DE is not as effective as HTD and sometimes even achieves worse performance than DE. This shows that the power of HTD comes from the distillation strategy transferring the relational knowledge in multi-levels.</p><p>For HTD, we compare three ablations: 1) Group only that considers only group-level topology, 2) Entity only that considers Table <ref type="table">1</ref>: Performance comparison (𝜙 = 0.1). Gain.DE denotes the improvement of HTD over DE, Gain.S denotes the improvement of HTD over Student. HTD achieves statistically significant improvements over the best baseline. We use the paired t-test with significance level at 0.05 on Recall@50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Base Model Method Recall@10 NDCG@10 Recall@20 NDCG@20 Recall@50 NDCG@50 Teacher 0. only entity-level topology, and 3) Group (P,P) that considers the relations of the prototypes only for the group-level topology (Section 3.3.2). We first observe that both group-level and entity-level topology are indeed necessary. Without either of them, the performance considerably drops. The group-level topology includes the summarized relations across the groups, providing the overview of the entire topology. On the other hand, the entity-level topology includes the full relations in each group, providing fine-grained supervision of how the entities should be correlated. Based on both two-level topology, HTD effectively transfers the relational knowledge. Lastly, we observe that Group (P,P) is not as effective as Group(P,e) adopted in HTD. We conjecture that summarizing numerous relations across the groups into a single relation may lose too much information and cannot effectively boost the student.  With prediction-based KD method. We report the results with the state-of-the-art prediction KD method (i.e., RRD <ref type="bibr" target="#b7">[8]</ref>) on CiteU-Like with 𝜙 = 0.1 in Figure <ref type="figure" target="#fig_5">5</ref>. Also, we provide the training curves of BPR with 𝜙 = 0.1 in Figure <ref type="figure" target="#fig_6">6</ref> 3 . First, we observe that the effectiveness of RRD is considerably improved when it is applied with the KD method distilling the latent knowledge (i.e., DE and HTD). This result aligns with the results reported in <ref type="bibr" target="#b7">[8]</ref> and shows the importance of distilling the latent knowledge. Second, we observe that the student recommender achieves the best performance with HTD. Unlike RRD, which makes the student imitate the ranking order of items in each user's recommendation list, HTD distills relations existing in the representation space via the topology matching. The topology includes much rich supervision including not only user-item relations but also user-user relations and item-item relations. By obtaining this additional information in a proper manner considering the capacity gap, the student can be fully improved with HTD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Benefit of Topology Distillation</head><p>To further ascertain the benefit of the topology distillation, we provide in-depth analysis on representations obtained by each KD method (𝜙 = 0.1). First, we evaluate whether the topology distillation indeed makes the student better preserve the relations in the teacher representation space than the existing method. For quantitative evaluation, we conduct the following steps: 1) In the teacher space, for each representation, we compute the similarity distributions with 100 most similar representations and 100 randomly selected representations, respectively. 2) In the student space, for each representation, we compute the similarity distributions with the representations chosen in the teacher space. 3) We compute KL divergence for each distribution and report the average value in Figure <ref type="figure" target="#fig_7">7</ref>. KL divergence of 'Most similar' indicates how well the detailed relations among the strongly correlated representations are preserved, and that of 'Random' indicates how well the overall relations in the space are preserved. We observe that HTD achieves the lowest KL divergence for both Most similar and Random, which shows that 3 After the early stopping on the validation set, we plot the final performance.   HTD indeed enables the student to better preserve the relations in the teacher space. Second, we compare the performance of two downstream tasks that evaluate how well each method encodes the items' characteristics (or semantics) into the representations. We perform the tag retrieval task for CiteULike and the region classification task for Foursquare. We train a linear and a non-linear model to predict the tag/region of each item by using the fixed item representation as the input. The detailed setup is provided in the appendix. In Table <ref type="table" target="#tab_1">3</ref>, we observe that HTD achieves consistently higher performance than DE on both of two downstream tasks. This strongly indicates that the representation space induced by HTD more accurately captures the item's semantics compared to the space induced by DE.</p><p>In sum, with the topology distillation approach, the student can indeed better preserve the relations in the teacher space. This not only improves the recommendation performance but also allows it to better capture the semantic of entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Hyperparameter Analysis</head><p>We provide analyses to guide the hyperparameter selection of the topology distillation approach. For the sake of the space, we report the results of BPR and LightGCN on CiteULike dataset with 𝜙 = 0.1 (Figure <ref type="figure">8</ref>). 1) The batch size is an important factor affecting the performance of the topology distillation. When the batch size is too small, the topology cannot include the overall relational knowledge in the representation space, leading to limited performance. For CiteULike, we observe that HTD achieves the stable performance around 2 8 -2 11 . In this work, we set the batch size to 2 10 . 2) 𝛾 is a hyperparameter for balancing the topology-preserving loss and the regression loss. We observe the stable performance with a value larger than 0.1. In this work, we set 𝛾 to 0.5. Note that 𝛾 = 0 equals DE.</p><p>3) The number of preference groups (𝐾) is an important hyperparameter of HTD. It needs to be determined considering the dataset, the capacity gap, and the selected layer for the distillation. For this setup, the best performance is achieved when 𝐾 is around 30-40 in both base models. 4) 𝜆 𝑇 𝐷 is a hyperparameter for controlling the effects of topology distillation. For this setup, the best performance is achieved when 𝜆 𝑇 𝐷 is around 10 −3 in both base models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORK</head><p>We develop a general topology distillation approach for RS, which guides the learning of the student by the topological structure built upon the relational knowledge in the teacher representation space. Concretely, we propose two topology distillation methods: 1) FTD that transfers the full topology. FTD is used in the scenario where the student has enough capacity to learn all the teacher's knowledge. 2) HTD that transfers the decomposed topology hierarchically. HTD is adopted in the conventional KD scenario where the student has a very limited capacity compared to the teacher. We conduct extensive experiments on real-world datasets and show that the proposed approach consistently outperforms the state-of-the-art competitor. We also provide in-depth analyses to ascertain the benefit of distilling the topology.</p><p>We believe the topology distillation approach can be advanced and extended in several directions. First, layer selection and simultaneous distillation from multiple layers are not investigated in this work. We especially expect that this can further improve the limited improvements by the topology distillation in the deep model (i.e., NeuMF). Second, topology distillation across different base models (e.g., from LightGCN to BPR) can be also considered to further improve the performance. Lastly, prior knowledge of user/item groups (e.g., item category, user demographic features) can be utilized for more sophisticated topology decomposition. We expect that this can further improve the effectiveness of the proposed method by providing better supervision on relational knowledge.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) The overview of KD in RS. (b-d) The conceptual illustrations of the latent knowledge that each method transfers from the teacher's representation space. Each point corresponds to a representation of each entity. (b) transfers the information of each entity to the student point-wise. However, our approach (c-d) transfers the relations among the entities. FTD/HTD refers to Full/Hierarchical topology distillation, and the solid/dotted line denotes the entity/group-level topology, respectively.</figDesc><graphic url="image-1.png" coords="2,92.25,83.68,166.45,96.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The relational knowledge distilled from teacher to student by FTD and HTD (with Group(P,e)). Red/Blue corresponds to relations of the entities belonging to the same/different preference group(s) (BPR on CiteULike).</figDesc><graphic url="image-6.png" coords="5,323.96,148.45,228.22,64.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>•</head><label></label><figDesc>NeuMF<ref type="bibr" target="#b3">[4]</ref>: A deep model that combines MF and Multi-Layer Perceptron (MLP) to learn the user-item interaction.• LightGCN<ref type="bibr" target="#b2">[3]</ref>: The state-of-the-art model which adopts simplified Graph Convolution Network (GCN) to capture the information of multi-hop neighbors. 4.1.3 Teacher/Student. For each setting, we increase the model size until the recommendation performance is no longer improved and adopt the model with the best performance as Teacher model. Then, we build three student models by limiting the model size, i.e., 𝜙 ∈ {0.1, 0.5, 1.0}. We call the student model trained without distillation as Student in this section. Figure 3 summarizes the model size and inference time. The inferences are made using PyTorch with CUDA from TITAN Xp GPU and Xeon on Gold 6130 CPU. It shows that the smaller model has lower inference latency. 4.1.4 Compared Methods. We compare the following KD methods distilling the latent knowledge from the intermediate layer of the teacher recommender.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Inference time (s) and model size (𝜙). Inference time denotes the wall time used for generating recommendation list for every user.</figDesc><graphic url="image-8.png" coords="6,323.91,107.46,108.11,79.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Recall@50 across three different student sizes (Dotted line: Teacher)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance comparison with RRD.</figDesc><graphic url="image-17.png" coords="8,71.82,83.69,204.21,75.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Training curves with RRD.</figDesc><graphic url="image-19.png" coords="8,175.04,175.73,118.92,89.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The average KL divergence from similarity distributions obtained in the teacher representation spaces.</figDesc><graphic url="image-21.png" coords="8,318.55,100.20,120.12,77.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(a) Effects of batch size and 𝛾 (b) Effects of 𝜆 𝑇 𝐷 and 𝐾 Figure 8: Effects of the hyperparameters (Recall@50).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison with ablations (𝜙 = 0.1).</figDesc><table><row><cell></cell><cell></cell><cell>BPR</cell><cell>Student FitNet DE FTD HTD</cell><cell cols="2">1533 0.1014 0.1097 0.1165 0.1131 0.1247</cell><cell>0.0883 0.0560 0.0595 0.0645 0.0630 0.0691</cell><cell>0.2196 0.1506 0.1610 0.1696 0.1660 0.1820</cell><cell>0.1058 0.0684 0.0738 0.0778 0.0763 0.0836</cell><cell>0.3253 0.2347 0.2521 0.2615 0.2624 0.2803</cell><cell>0.1247 0.0864 0.0924 0.0960 0.0953 0.1031</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Gain.DE Gain.S</cell><cell>7.0% 23.0%</cell><cell></cell><cell>7.3% 23.5%</cell><cell>7.3% 20.9%</cell><cell>7.5% 22.3%</cell><cell>7.2% 19.4%</cell><cell>7.4% 19.3%</cell></row><row><cell cols="2">CiteULike</cell><cell>NeuMF</cell><cell>Teacher Student FitNet DE FTD HTD</cell><cell cols="2">0.1487 0.0856 0.0856 0.0882 0.0875 0.0914</cell><cell>0.0844 0.0449 0.0469 0.0475 0.0474 0.0504</cell><cell>0.2048 0.1249 0.1275 0.1306 0.1291 0.1416</cell><cell>0.0986 0.0553 0.0576 0.0581 0.0579 0.0618</cell><cell>0.2993 0.1970 0.2020 0.2090 0.2069 0.2154</cell><cell>0.1155 0.0697 0.0723 0.0736 0.0733 0.0772</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Gain.DE Gain.S</cell><cell>3.6% 6.8%</cell><cell></cell><cell>6.2% 12.2%</cell><cell>8.4% 13.4%</cell><cell>6.4% 11.8%</cell><cell>3.1% 9.0%</cell><cell>4.8% 10.8%</cell></row><row><cell></cell><cell></cell><cell>LightGCN</cell><cell>Teacher Student FitNet DE FTD HTD</cell><cell cols="2">0.1610 0.1125 0.1151 0.1189 0.1112 0.1322</cell><cell>0.0934 0.0618 0.0642 0.0664 0.0615 0.0742</cell><cell>0.2274 0.1642 0.1710 0.1733 0.1635 0.1902</cell><cell>0.1091 0.0748 0.0783 0.0801 0.0747 0.0888</cell><cell>0.3326 0.2512 0.2653 0.2680 0.2542 0.2847</cell><cell>0.1299 0.0944 0.0969 0.0988 0.0926 0.1075</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Gain.DE Gain.S</cell><cell>11.2% 17.5%</cell><cell></cell><cell>11.8% 20.1%</cell><cell>9.7% 15.8%</cell><cell>10.9% 18.7%</cell><cell>6.2% 13.3%</cell><cell>8.8% 13.9%</cell></row><row><cell></cell><cell></cell><cell>BPR</cell><cell>Teacher Student FitNet DE FTD HTD</cell><cell cols="2">0.1187 0.0911 0.0957 0.0979 0.0987 0.1037</cell><cell>0.0695 0.0544 0.0564 0.0567 0.0582 0.0622</cell><cell>0.1700 0.1333 0.1386 0.1434 0.1417 0.1505</cell><cell>0.0825 0.0648 0.0672 0.0681 0.0690 0.0740</cell><cell>0.2732 0.2164 0.2258 0.2322 0.2262 0.2438</cell><cell>0.1028 0.0809 0.0845 0.0856 0.0857 0.0921</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Gain.DE Gain.S</cell><cell>5.9% 13.8%</cell><cell></cell><cell>9.7% 14.3%</cell><cell>5.0% 12.9%</cell><cell>8.7% 14.2%</cell><cell>5.0% 12.7%</cell><cell>7.6% 13.8%</cell></row><row><cell cols="2">Foursquare</cell><cell>NeuMF</cell><cell>Teacher Student FitNet DE FTD HTD</cell><cell cols="2">0.1060 0.0737 0.0829 0.0855 0.0823 0.0891</cell><cell>0.0590 0.0393 0.0462 0.0476 0.0451 0.0501</cell><cell>0.1546 0.1125 0.1243 0.1255 0.1233 0.1294</cell><cell>0.0716 0.0490 0.0564 0.0576 0.0554 0.0601</cell><cell>0.2529 0.1950 0.2062 0.2089 0.2068 0.2152</cell><cell>0.0910 0.0653 0.0729 0.0741 0.0719 0.0770</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Gain.DE Gain.S</cell><cell>4.3% 20.9%</cell><cell></cell><cell>5.3% 27.2%</cell><cell>3.1% 15.0%</cell><cell>4.3% 22.7%</cell><cell>3.0% 10.0%</cell><cell>3.9% 17.9%</cell></row><row><cell></cell><cell></cell><cell>LightGCN</cell><cell>Teacher Student FitNet DE FTD HTD</cell><cell cols="2">0.1259 0.0951 0.0993 0.1051 0.1018 0.1119</cell><cell>0.0730 0.0564 0.0587 0.0617 0.0602 0.0652</cell><cell>0.1779 0.1372 0.1431 0.1503 0.1466 0.1597</cell><cell>0.0865 0.0670 0.0697 0.0731 0.0714 0.0772</cell><cell>0.2806 0.2202 0.2315 0.2410 0.2327 0.2531</cell><cell>0.1067 0.0834 0.0872 0.0910 0.0884 0.0956</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Gain.DE Gain.S</cell><cell>6.4% 17.7%</cell><cell></cell><cell>5.6% 15.6%</cell><cell>6.3% 16.4%</cell><cell>5.6% 15.2%</cell><cell>5.0% 14.9%</cell><cell>5.1% 14.6%</cell></row><row><cell>Base</cell><cell>Method</cell><cell cols="2">CiteULike</cell><cell cols="2">Foursquare</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell cols="4">Recall@50 NDCG@50 Recall@50 NDCG@50</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>FTD</cell><cell>0.2624</cell><cell>0.0953</cell><cell>0.2262</cell><cell>0.0857</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>FTD+DE</cell><cell>0.2650</cell><cell>0.0969</cell><cell>0.2339</cell><cell>0.0867</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BPR</cell><cell cols="2">HTD Group only 0.2619 0.2803</cell><cell>0.1031 0.0948</cell><cell>0.2438 0.2349</cell><cell>0.0921 0.0874</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Entity only 0.2608</cell><cell>0.0968</cell><cell>0.2361</cell><cell>0.0871</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Group (P,P) 0.2648</cell><cell>0.0982</cell><cell>0.2399</cell><cell>0.0887</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>FTD</cell><cell>0.2542</cell><cell>0.0926</cell><cell>0.2327</cell><cell>0.0884</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>FTD+DE</cell><cell>0.2572</cell><cell>0.0931</cell><cell>0.2335</cell><cell>0.0872</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">0.2847 Group only 0.2596 LightGCN HTD</cell><cell>0.1075 0.0976</cell><cell>0.2531 0.2432</cell><cell>0.0956 0.0910</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Entity only 0.2709</cell><cell>0.1010</cell><cell>0.2453</cell><cell>0.0910</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Group (P,P) 0.2683</cell><cell>0.0987</cell><cell>0.2459</cell><cell>0.0909</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison on downstream tasks.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><ref type="bibr" target="#b7">[8]</ref> proposes two KD methods: one by prediction and the other by latent knowledge.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">We use the term 'entity' to denote the subject of each representation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment: This work was supported by the NRF grant funded by the MSIT (No. 2020R1A2B5B03097210), and the IITP grant funded by the MSIT (No. 2018-0-00584, 2019-0-01906).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 Pseudocode of the proposed methods</head><p>The pseudocode of FTD and HTD are provided in Algorithm 1 and Algorithm 2, respectively. The base model can be any existing recommender and L 𝐵𝑎𝑠𝑒 is its loss function. Note that the distillation is conducted in the offline training phase. At the online inference phase, the student model is used only.</p><p>All the computations of the topology distillation are efficiently computed on matrix form by parallel execution through GPU processor. In Algorithm 1 (FTD), the topological structures (line 5) are computed as follows:</p><p>where Cos is the operation computing the cosine similarities by Cos(B, D) = B D⊤ , B[𝑖,:] = B [𝑖,:] /∥B [𝑖,:] ∥ 2 . In Algorithm 2 (HTD with Group(P,e)), the two-level topological structures (line 5-8) are computed as follows:</p><p>The power of topology distillation comes from distilling the additional supervision in a proper manner considering the capacity gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Group Assignment.</head><p>Instead of DE, any clustering method or prior knowledge of user/item groups can be utilized for more sophisticated topology decomposition. The simplest method is 𝐾-means clustering. Specifically, we first conduct the clustering in the teacher space, then use the results for the group assignment. The results are summarized in Table <ref type="table">4</ref>. For both DE and 𝐾-means, the number of preference group (𝐾) is set to 30, and the teacher space dimensions (𝑑 𝑡 ) is 200.</p><p>We get consistently better results with the adaptive assignment by DE. We conjecture the possible reasons as follows: 1) Accurate clustering in high-dimensional space is very challenging, 2) With the adaptive approach, the assignment process gets gradually sophisticated along with the student, and thereby it provides guidance considering the student's learning. For these reasons, we use the adaptive assignment in HTD. The performance of HTD can be further improved by adopting a more advanced assignment method and prior knowledge. We leave exploring better ways of the assignment for future study. We use two public real-world datasets: CiteULike and Foursquare. We remove users having fewer than 5 (CiteULike) and 20 interactions (FourSquare) and remove items having fewer than 10 interactions (FourSquare) as done in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11]</ref>. Table <ref type="table">5</ref> summarizes the statistics of the datasets. In the case of CiteULike, each item corresponds to an article, and each article has multiple tags. In the case of Foursquare, each item corresponds to a POI (points-of-interest) such as museums and restaurants, and each POI has GPS coordinates (i.e., the latitude and longitude). We use this side information in Section 4.3. Table <ref type="table">6</ref> shows the URLs from which the datasets can be downloaded.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Evaluation Protocol and Metrics</head><p>We adopt the widely used leave-one-out evaluation protocol, whereby two interacted items for each user are held out for testing/validation, and the rest are used for training. However, unlike <ref type="bibr" target="#b7">[8]</ref> that samples a predefined number (e.g., 499) of unobserved items for evaluation, we adopt the full-ranking evaluation scheme that evaluates how well each method can rank the test item higher than all the unobserved items. Although it is time-consuming, it enables a more thorough evaluation compared to the sampling-based evaluation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>. We evaluate all methods by two widely used ranking metrics: Recall@𝑁 <ref type="bibr" target="#b13">[14]</ref> and Normalized Discounted Cumulative Gain (NDCG@𝑁 ) <ref type="bibr" target="#b6">[7]</ref>. Recall@𝑁 measures whether the test item is included in the top-𝑁 list and NDCG@𝑁 assigns higher scores on the upper ranked test items. We compute the metrics for each user, then compute the average score. Lastly, we report the average value of five independent runs for all methods.</p><p>A.5 Implementation Details.</p><p>We use PyTorch to implement the proposed methods and all the competing methods. We optimize all methods with Adam optimizer.</p><p>For DE and RRD, we use the public implementation provided by the authors. For each setting, hyperparameters are tuned by using grid searches on the validation set. The learning rate is searched in the range of {0.01, 0.005, 0.001, 0.0005, 0.0001}. The model regularizer is searched in the range of {10 −2 , 10 −3 , 10 −4 , 10 −5 , 10 −6 }. We set the total number of epochs to 500 and adopt the early stopping strategy; it terminates when Recall@50 on the validation set does not increase for 20 successive epochs. For all base models, the number of negative samples is set to 1. For NeuMF and LightGCN, the number of layers is searched in the range of {1, 2, 3, 4}. For all the distillation methods, weight for the distillation loss (𝜆) searched in the range of {1, 10 −1 , 10 −2 , 10 −3 , 10 −4 , 10 −5 }.</p><p>For the hint regression-related setup, we closely follow the setup reported in DE paper <ref type="bibr" target="#b7">[8]</ref>. Specifically, two-layer MLP with [𝑑 𝑠 → (𝑑 𝑠 + 𝑑 𝑡 )/2 → 𝑑 𝑡 ] is employed for 𝑓 in FitNet, DE and HTD. Also, one-layer perceptron with [𝑑 𝑡 → 𝐾] is employed for assigning group (𝑣) in DE and HTD. For DE and HTD, the number of preference groups (𝐾) is chosen from {5, 10, 20, 30, 40, 50}. We provide an analysis of these hyperparameters in Section 4.4.</p><p>A.6 Experiment Setup for Downstream Tasks.</p><p>We evaluate how well each method encodes the items' characteristics (or semantics) into the representations. We train a small network to predict the side information of items by using the fixed item representations as the input. Specifically, we use a linear and a non-linear model (i.e., a single-layer perceptron and three-layer perceptron, respectively) with Softmax output. The linear model has the shape of [𝑑 𝑠 → 𝐶], and the non-linear model has the shape of [𝑑 𝑠 → (𝑑 𝑠 + 𝐶)/2 → (𝑑 𝑠 + 𝐶)/2 → 𝐶] with relu, where 𝐶 is the number of tags/classes. Let q denote the output of the model whose element 𝑞 𝑖 is a prediction score for each tag/class. Also, let p denote the ground-truth vector whose element 𝑝 𝑖 = 1 if 𝑖-th tag/class is the answer, otherwise 𝑝 𝑖 = 0. We train the model by minimizing the negative log-likelihood: − 𝑖 𝑝 𝑖 log 𝑞 𝑖 . Note that the side-information is not utilized for training of the base model.</p><p>For CiteULike dataset, we perform item-tag retrieval task; by using each item representation as a query, we find a ranking list of tags that are relevant to the item. We first remove tags used less than 10 times. Then, there exist 4,153 tags and an item has 6.4 tags on average. After training, we make a ranking list of tags by sorting the prediction scores. We evaluate how many the ground-truth tags are included in the top-10 list by Recall@10. For Foursquare dataset, we perform item-region classification task; given each item representation, we predict the region class to which the item belongs. We first perform 𝑘-means clustering on the coordinates with 𝑘 = 200 and use the clustering results as the class labels. After training, we evaluate the performance by Accuracy. Finally, we perform 5-fold cross-validation and report the average result and standard deviation in Table <ref type="table">3</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Speeding up the xbox recommender system using a euclidean transformation for inner-product spaces</title>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Gilad-Bachrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liran</forename><surname>Katzir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Koenigstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Nice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulrich</forename><surname>Paquet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>In RecSys</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multidimensional binary search trees used for associative searching</title>
		<author>
			<persName><forename type="first">Jon</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bentley</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<date type="published" when="1975">1975. 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with gumbel-softmax</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cumulated gain-based evaluation of IR techniques</title>
		<author>
			<persName><forename type="first">Kalervo</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaana</forename><surname>Kekäläinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOIS</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DE-RRD: A Knowledge Distillation Framework for Recommender System</title>
		<author>
			<persName><forename type="first">Seongku</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonbin</forename><surname>Kweon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwanjo</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On sampled metrics for item recommendation</title>
		<author>
			<persName><forename type="first">Walid</forename><surname>Krichene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Bidirectional Distillation for Top-K Recommender System</title>
		<author>
			<persName><forename type="first">Wonbin</forename><surname>Kweon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seongku</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwanjo</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bootstrapping User and Item Representations for One-Class Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Dongha</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seongku</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunjun</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chanyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwanjo</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Jaewoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongwuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunjung</forename><surname>Shim</surname></persName>
		</author>
		<title level="m">Collaborative Distillation for Top-N Recommendation. ICDM</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">FEXIPRO: fast and exact inner product retrieval in recommender systems</title>
		<author>
			<persName><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nam</forename><surname>Tsz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Man</forename><forename type="middle">Lung</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Yiu</surname></persName>
		</author>
		<author>
			<persName><surname>Mamoulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Relaxed Ranking-Based Factor Model for Recommender System from Implicit Feedback</title>
		<author>
			<persName><forename type="first">Huayu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Local correlation consistency for knowledge distillation</title>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Xing Xie, and Longbing Cao. 2017. Discrete Content-Aware Matrix Factorization</title>
		<author>
			<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Discrete factorization machines for fast feature-based recommendation</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02232</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Structured knowledge distillation for semantic segmentation</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengchang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenbo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BPR: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Asymmetric LSH (ALSH) for sublinear time maximum inner product search (MIPS)</title>
		<author>
			<persName><forename type="first">Anshumali</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ranking distillation: Learning compact ranking models with high performance for recommender system</title>
		<author>
			<persName><forename type="first">Jiaxi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Lemp: Fast retrieval of large entries in a matrix product</title>
		<author>
			<persName><forename type="first">Christina</forename><surname>Teflioudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Mykytiuk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In SIGMOD</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Similarity-preserving knowledge distillation</title>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Analysing compression techniques for in-memory collaborative filtering</title>
		<author>
			<persName><forename type="first">Saúl</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Binarized collaborative filtering with distilling graph convolutional networks</title>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Listwise approach to learning to rank: theory and algorithm</title>
		<author>
			<persName><forename type="first">Fen</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wensheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donggyu</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihoon</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Discrete Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fumin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ensembled CTR Prediction via Knowledge Distillation</title>
		<author>
			<persName><forename type="first">Jieming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jincai</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zibin</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
