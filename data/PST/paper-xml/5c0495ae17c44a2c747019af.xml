<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Boosting Adversarial Attacks with Momentum</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Tsinghua Lab of Brain and Intelligence</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country>BNRist Lab</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fangzhou</forename><surname>Liao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Tsinghua Lab of Brain and Intelligence</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country>BNRist Lab</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Tsinghua Lab of Brain and Intelligence</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country>BNRist Lab</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Tsinghua Lab of Brain and Intelligence</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country>BNRist Lab</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Tsinghua Lab of Brain and Intelligence</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country>BNRist Lab</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
							<email>xlhu@mail.tsinghua.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jianguo</forename><surname>Li</surname></persName>
							<email>jianguo.li@intel.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Tsinghua Lab of Brain and Intelligence</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country>BNRist Lab</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Intel Labs China</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Boosting Adversarial Attacks with Momentum</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks are vulnerable to adversarial examples, which poses security concerns on these algorithms due to the potentially severe consequences. Adversarial attacks serve as an important surrogate to evaluate the robustness of deep learning models before they are deployed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>However, most of existing adversarial attacks can only fool a black-box model with a low success rate. To address this issue, we propose a broad class of momentum-based iterative algorithms to boost adversarial attacks. By integrating the momentum term into the iterative process for attacks, our methods can stabilize update directions and escape from poor local maxima during the iterations, resulting in more transferable adversarial examples.</head><p>To further improve the success rates for black-box attacks, we apply momentum iterative algorithms to an ensemble of models, and show that the adversarially trained models with a strong defense ability are also vulnerable to our black-box attacks. We hope that the proposed methods will serve as a benchmark for evaluating the robustness of various deep models and defense methods. With this method, we won the first places in NIPS 2017 Non-targeted Adversarial Attack and Targeted Adversarial Attack competitions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks (DNNs) are challenged by their vulnerability to adversarial examples <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b4">5]</ref>, which are crafted by adding small, human-imperceptible noises to legitimate examples, but make a model output attackerdesired inaccurate predictions. It has garnered an increasing attention to generating adversarial examples since it helps to identify the vulnerability of the models before they are launched. Besides, adversarial samples also facilitate various DNN algorithms to assess the robustness by providing Figure <ref type="figure">1</ref>. We show two adversarial examples generated by the proposed momentum iterative fast gradient sign method (MI-FGSM) for the Inception v3 <ref type="bibr" target="#b21">[22]</ref> model. Left column: the original images. Middle column: the adversarial noises by applying MI-FGSM for 10 iterations. Right column: the generated adversarial images. We also show the predicted labels and probabilities of these images given by the Inception v3. more varied training data <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>With the knowledge of the structure and parameters of a given model, many methods can successfully generate adversarial examples in the white-box manner, including optimization-based methods such as box-constrained L-BFGS <ref type="bibr" target="#b22">[23]</ref>, one-step gradient-based methods such as fast gradient sign <ref type="bibr" target="#b4">[5]</ref> and iterative variants of gradient-based methods <ref type="bibr" target="#b8">[9]</ref>. In general, a more severe issue of adversarial examples is their good transferability <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14]</ref>, i.e., the adversarial examples crafted for one model remain adversarial for others, thus making black-box attacks practical in real-world applications and posing real security issues. The phenomenon of transferability is due to the fact that different machine learning models learn similar decision boundaries around a data point, making the adversarial examples crafted for one model also effective for others. However, existing attack methods exhibit low efficacy when attacking black-box models, especially for those with a defense mechanism. For example, ensemble adversarial training <ref type="bibr" target="#b23">[24]</ref> significantly improves the robustness of deep neural networks and most of existing methods cannot successfully attack them in the black-box manner. This fact largely attributes to the trade-off between the attack ability and the transferability. In particular, the adversarial examples generated by optimization-based and iterative methods have poor transferability <ref type="bibr" target="#b9">[10]</ref>, and thus make black-box attacks less effective. On the other hand, one-step gradientbased methods generate more transferable adversarial examples, however they usually have a low success rate for the white-box model <ref type="bibr" target="#b9">[10]</ref>, making it ineffective for blackbox attacks. Given the difficulties of practical black-box attacks, Papernot et al. <ref type="bibr" target="#b15">[16]</ref> use adaptive queries to train a surrogate model to fully characterize the behavior of the target model and therefore turn the black-box attacks to whitebox attacks. However, it requires the full prediction confidences given by the target model and tremendous number of queries, especially for large scale datasets such as Ima-geNet <ref type="bibr" target="#b18">[19]</ref>. Such requirements are impractical in real-world applications. Therefore, we consider how to effectively attack a black-box model without knowing its architecture and parameters, and further, without querying.</p><p>In this paper, we propose a broad class of momentum iterative gradient-based methods to boost the success rates of the generated adversarial examples. Beyond iterative gradient-based methods that iteratively perturb the input with the gradients to maximize the loss function <ref type="bibr" target="#b4">[5]</ref>, momentum-based methods accumulate a velocity vector in the gradient direction of the loss function across iterations, for the purpose of stabilizing update directions and escaping from poor local maxima. We show that the adversarial examples generated by momentum iterative methods have higher success rates in both white-box and black-box attacks. The proposed methods alleviate the trade-off between the white-box attacks and the transferability, and act as a stronger attack algorithm than one-step methods <ref type="bibr" target="#b4">[5]</ref> and vanilla iterative methods <ref type="bibr" target="#b8">[9]</ref>.</p><p>To further improve the transferability of adversarial examples, we study several approaches for attacking an ensemble of models, because if an adversarial example fools multiple models, it is more likely to remain adversarial for other black-box models <ref type="bibr" target="#b11">[12]</ref>. We show that the adversarial examples generated by the momentum iterative methods for multiple models, can successfully fool robust models obtained by ensemble adversarial training <ref type="bibr" target="#b23">[24]</ref> in the blackbox manner. The findings in this paper raise new security issues for developing more robust deep learning models, with a hope that our attacks will be used as a benchmark to evaluate the robustness of various deep learning models and defense methods. In summary, we make the following contributions:</p><p>• We introduce a class of attack algorithms called momentum iterative gradient-based methods, in which we accumulate gradients of the loss function at each iteration to stabilize optimization and escape from poor local maxima.</p><p>• We study several ensemble approaches to attack multiple models simultaneously, which demonstrates a powerful capability of transferability by preserving a high success rate of attacks.</p><p>• We are the first to show that the models obtained by ensemble adversarial training with a powerful defense ability are also vulnerable to the black-box attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Backgrounds</head><p>In this section, we provide the background knowledge as well as review the related works about adversarial attack and defense methods. Given a classifier f (x) : x ∈ X → y ∈ Y that outputs a label y as the prediction for an input x, the goal of adversarial attacks is to seek an example x * in the vicinity of x but is misclassified by the classifier. Specifically, there are two classes of adversarial examplesnon-targeted and targeted ones. For a correctly classified input x with ground-truth label y such that f (x) = y, a non-targeted adversarial example x * is crafted by adding small noise to x without changing the label, but misleads the classifier as f (x * ) = y; and a targeted adversarial example aims to fool the classifier by outputting a specific label as f (x * ) = y * , where y * is the target label specified by the adversary, and y * = y. In most cases, the L p norm of the adversarial noise is required to be less than an allowed value ǫ as x * − x p ≤ ǫ, where p could be 0, 1, 2, ∞.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Attack methods</head><p>Existing approaches for generating adversarial examples can be categorized into three groups. We introduce their non-targeted version of attacks here, and the targeted version can be simply derived.</p><p>One-step gradient-based approaches, such as the fast gradient sign method (FGSM) <ref type="bibr" target="#b4">[5]</ref>, find an adversarial example x * by maximizing the loss function J(x * , y), where J is often the cross-entropy loss. FGSM generates adversarial examples to meet the L ∞ norm bound x * − x ∞ ≤ ǫ as</p><formula xml:id="formula_0">x * = x + ǫ • sign(∇ x J(x, y)),<label>(1)</label></formula><p>where ∇ x J(x, y) is the gradient of the loss function w.r.t.</p><p>x. The fast gradient method (FGM) is a generalization of FGSM to meet the L 2 norm bound x * − x 2 ≤ ǫ as</p><formula xml:id="formula_1">x * = x + ǫ • ∇ x J(x, y) ∇ x J(x, y) 2 . (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>Iterative methods <ref type="bibr" target="#b8">[9]</ref> iteratively apply fast gradient multiple times with a small step size α. The iterative version of FGSM (I-FGSM) can be expressed as:</p><formula xml:id="formula_3">x * 0 = x, x * t+1 = x * t + α • sign(∇ x J(x * t , y)).<label>(3)</label></formula><p>To make the generated adversarial examples satisfy the L ∞ (or L 2 ) bound, one can clip x * t into the ǫ vicinity of x or simply set α = ǫ /T with T being the number of iterations. It has been shown that iterative methods are stronger whitebox adversaries than one-step methods at the cost of worse transferability <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Optimization-based methods <ref type="bibr" target="#b22">[23]</ref> directly optimize the distance between the real and adversarial examples subject to the misclassification of adversarial examples. Boxconstrained L-BFGS can be used to solve such a problem. A more sophisticated way <ref type="bibr" target="#b0">[1]</ref> is solving:</p><formula xml:id="formula_4">arg min x * λ • x * − x p − J(x * , y).<label>(4)</label></formula><p>Since it directly optimizes the distance between an adversarial example and the corresponding real example, there is no guarantee that the L ∞ (L 2 ) distance is less than the required value. Optimization-based methods also lack the efficacy in black-box attacks just like iterative methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Defense methods</head><p>Among many attempts <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b10">11]</ref>, adversarial training is the most extensively investigated way to increase the robustness of DNNs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24]</ref>. By injecting adversarial examples into the training procedure, the adversarially trained models learn to resist the perturbations in the gradient direction of the loss function. However, they do not confer robustness to black-box attacks due to the coupling of the generation of adversarial examples and the parameters being trained. Ensemble adversarial training <ref type="bibr" target="#b23">[24]</ref> augments the training data with the adversarial samples produced not only from the model being trained, but also from other hold-out models. Therefore, the ensemble adversarially trained models are robust against one-step attacks and black-box attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this paper, we propose a broad class of momentum iterative gradient-based methods to generate adversarial examples, which can fool white-box models as well as black-box models. In this section, we elaborate the proposed algorithms. We first illustrate how to integrate momentum into iterative FGSM, which induces a momentum iterative fast gradient sign method (MI-FGSM) to generate adversarial examples satisfying the L ∞ norm restriction in the non-targeted attack fashion. We then present several methods on how to efficiently attack an ensemble of models. Finally, we extend MI-FGSM to L 2 norm bound and targeted attacks, yielding a broad class of attack methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 MI-FGSM</head><p>Input: A classifier f with loss function J; a real example x and ground-truth label y; Input: The size of perturbation ǫ; iterations T and decay factor µ. Output: An adversarial example x * with x * − x ∞ ≤ ǫ.</p><p>1: α = ǫ /T ; 2: g0 = 0; x * 0 = x; 3: for t = 0 to T − 1 do  Update gt+1 by accumulating the velocity vector in the gradient direction as</p><formula xml:id="formula_5">gt+1 = µ • gt + ∇xJ(x * t , y) ∇xJ(x * t , y) 1 ;<label>(6) 6:</label></formula><p>Update x * t+1 by applying the sign gradient as</p><formula xml:id="formula_6">x * t+1 = x * t + α • sign(gt+1);<label>(7)</label></formula><p>7: end for 8: return x * = x * T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Momentum iterative fast gradient sign method</head><p>The momentum method <ref type="bibr" target="#b17">[18]</ref> is a technique for accelerating gradient descent algorithms by accumulating a velocity vector in the gradient direction of the loss function across iterations. The memorization of previous gradients helps to barrel through narrow valleys, small humps and poor local minima or maxima <ref type="bibr" target="#b3">[4]</ref>. The momentum method also shows its effectiveness in stochastic gradient descent to stabilize the updates <ref type="bibr" target="#b19">[20]</ref>. We apply the idea of momentum to generate adversarial examples and obtain tremendous benefits.</p><p>To generate a non-targeted adversarial example x * from a real example x, which satisfies the L ∞ norm bound, gradient-based approaches seek the adversarial example by solving the constrained optimization problem arg max</p><formula xml:id="formula_7">x * J(x * , y), s.t. x * − x ∞ ≤ ǫ,<label>(5)</label></formula><p>where ǫ is the size of adversarial perturbation. FGSM generates an adversarial example by applying the sign of the gradient to a real example only once (in Eq. ( <ref type="formula" target="#formula_0">1</ref>)) by the assumption of linearity of the decision boundary around the data point. However in practice, the linear assumption may not hold when the distortion is large <ref type="bibr" target="#b11">[12]</ref>, which makes the adversarial example generated by FGSM "underfits" the model, limiting its attack ability. In contrast, iterative FGSM greedily moves the adversarial example in the direction of the sign of the gradient in each iteration (in Eq. ( <ref type="formula" target="#formula_3">3</ref>)). Therefore, the adversarial example can easily drop into poor local maxima and "overfit" the model, which is not likely to transfer across models.</p><p>In order to break such a dilemma, we integrate momentum into the iterative FGSM for the purpose of stabilizing update directions and escaping from poor local maxima. Therefore, the momentum-based method remains the transferability of adversarial examples when increasing it-erations, and at the same time acts as a strong adversary for the white-box models like iterative FGSM. It alleviates the trade-off between the attack ability and the transferability, demonstrating strong black-box attacks.</p><p>The momentum iterative fast gradient sign method (MI-FGSM) is summarized in Algorithm 1. Specifically, g t gathers the gradients of the first t iterations with a decay factor µ, defined in Eq. ( <ref type="formula" target="#formula_5">6</ref>). Then the adversarial example x * t until the t-th iteration is perturbed in the direction of the sign of g t with a step size α in Eq. <ref type="bibr" target="#b6">(7)</ref>. If µ equals to 0, MI-FGSM degenerates to the iterative FGSM. In each iteration, the current gradient ∇ x J(x * t , y) is normalized by the L 1 distance (any distance measure is feasible) of itself, because we notice that the scale of the gradients in different iterations varies in magnitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Attacking ensemble of models</head><p>In this section, we study how to attack an ensemble of models efficiently. Ensemble methods have been broadly adopted in researches and competitions for enhancing the performance and improving the robustness <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b1">2]</ref>. The idea of ensemble can also be applied to adversarial attacks, due to the fact that if an example remains adversarial for multiple models, it may capture an intrinsic direction that always fools these models and is more likely to transfer to other models at the same time <ref type="bibr" target="#b11">[12]</ref>, thus enabling powerful black-box attacks.</p><p>We propose to attack multiple models whose logit activations 1 are fused together, and we call this method ensemble in logits. Because the logits capture the logarithm relationships between the probability predictions, an ensemble of models fused by logits aggregates the fine detailed outputs of all models, whose vulnerability can be easily discovered. Specifically, to attack an ensemble of K models, we fuse the logits as</p><formula xml:id="formula_8">l(x) = K k=1 w k l k (x),<label>(8)</label></formula><p>where l k (x) are the logits of the k-th model, w k is the ensemble weight with w k ≥ 0 and K k=1 w k = 1. The loss function J(x, y) is defined as the softmax cross-entropy loss given the ground-truth label y and the logits l(x)</p><formula xml:id="formula_9">J(x, y) = −1 y • log(softmax(l(x))),<label>(9)</label></formula><p>where 1 y is the one-hot encoding of y. We summarize the MI-FGSM algorithm for attacking multiple models whose logits are averaged in Algorithm 2.</p><p>For comparison, we also introduce two alternative ensemble schemes, one of which is already studied <ref type="bibr" target="#b11">[12]</ref>. Specifically, K models can be averaged in predictions <ref type="bibr" target="#b11">[12]</ref> as p(x) = K k=1 w k p k (x), where p k (x) is the predicted probability of the k-th model given input x. K models can also be averaged in loss as J(x, y) = K k=1 w k J k (x, y). 1 Logits are the input values to softmax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 MI-FGSM for an ensemble of models</head><p>Input: The logits of K classifiers l1, l2, ..., lK ; ensemble weights w1, w2, ..., wK ; a real example x and ground-truth label y; Input: The size of perturbation ǫ; iterations T and decay factor µ. Output: An adversarial example x * with x * − x ∞ ≤ ǫ.</p><p>1: α = ǫ /T ; 2: g0 = 0; x * 0 = x; 3: for t = 0 to T − 1 do Fuse the logits as l(x * t ) = K k=1 w k l k (x * t );</p><p>6:</p><p>Get softmax cross-entropy loss J(x * t , y) based on l(x * t ) and Eq. ( <ref type="formula" target="#formula_9">9</ref>); 7:</p><p>Obtain the gradient ∇xJ(x * t , y);</p><formula xml:id="formula_10">8:</formula><p>Update gt+1 by Eq. ( <ref type="formula" target="#formula_5">6</ref>);</p><p>9:</p><p>Update x * t+1 by Eq. ( <ref type="formula" target="#formula_6">7</ref>); 10: end for 11: return x * = x * T .</p><p>In these three methods, the only difference is where to combine the outputs of multiple models, but they result in different attack abilities. We empirically find that the ensemble in logits performs better than the ensemble in predictions and the ensemble in loss, among various attack methods and various models in the ensemble, which will be demonstrated in Sec. 4.3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Extensions</head><p>The momentum iterative methods can be easily generalized to other attack settings. By replacing the current gradient with the accumulated gradient of all previous steps, any iterative method can be extended to its momentum variant.</p><p>Here we introduce the methods for generating adversarial examples in terms of the L 2 norm bound attacks and the targeted attacks.</p><p>To find an adversarial examples within the ǫ vicinity of a real example measured by L 2 distance as x * −x 2 ≤ ǫ, the momentum variant of iterative fast gradient method (MI-FGM) can be written as</p><formula xml:id="formula_11">x * t+1 = x * t + α • g t+1 g t+1 2 , (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>where g t+1 is defined in Eq. ( <ref type="formula" target="#formula_5">6</ref>) and α = ǫ /T with T standing for the total number of iterations.</p><p>For targeted attacks, the objective for finding an adversarial example misclassified as a target class y * is to minimize the loss function J(x * , y * ). The accumulated gradient is derived as</p><formula xml:id="formula_13">g t+1 = µ • g t + J(x * t , y * ) ∇ x J(x * t , y * ) 1 .<label>(11)</label></formula><p>The targeted MI-FGSM with an L ∞ norm bound is and the targeted MI-FGM with an L 2 norm bound is</p><formula xml:id="formula_14">x * t+1 = x * t − α • sign(g t+1 ),<label>(12)</label></formula><formula xml:id="formula_15">x * t+1 = x * t − α • g t+1 g t+1 2 . (<label>13</label></formula><formula xml:id="formula_16">)</formula><p>Therefore, we introduce a broad class of momentum iterative methods for attacks in various settings, whose effectiveness is demonstrated in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we conduct extensive experiments on the ImageNet dataset <ref type="bibr" target="#b18">[19]</ref> to validate the effectiveness of the proposed methods. We first specify the experimental settings in Sec. 4.1. Then we report the results for attacking a single model in Sec. 4.2 and an ensemble of models in Sec. 4.3. Our methods won both the NIPS 2017 Non-targeted and Targeted Adversarial Attack competitions, with the configurations introduced in Sec. 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>We study seven models, four of which are normally trained models-Inception v3 (Inc-v3) <ref type="bibr" target="#b21">[22]</ref>, Inception v4 (Inc-v4), Inception Resnet v2 (IncRes-v2) <ref type="bibr" target="#b20">[21]</ref>, Resnet v2-152 (Res-152) <ref type="bibr" target="#b6">[7]</ref> and the other three of which are trained by ensemble adversarial training-Inc-v3 ens3 , Inc-v3 ens4 , IncRes-v2 ens <ref type="bibr" target="#b23">[24]</ref>. We will simply call the last three models as "adversarially trained models" without ambiguity.</p><p>It is less meaningful to study the success rates of attacks if the models cannot classify the original image correctly. Therefore, we randomly choose 1000 images belonging to the 1000 categories from the ILSVRC 2012 validation set, which are all correctly classified by them.</p><p>In our experiments, we compare our methods to onestep gradient-based methods and iterative methods. Since optimization-based methods cannot explicitly control the distance between the adversarial examples and the corresponding real examples, they are not directly comparable to ours, but they have similar properties with iterative methods as discussed in Sec. 2.1. For clarity, we only report the results based on L ∞ norm bound for non-targeted attacks, and leave the results based on L 2 norm bound and targeted attacks in the supplementary material. The findings in this paper are general across different attack settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Attacking a single model</head><p>We report in Table <ref type="table" target="#tab_0">1</ref> the success rates of attacks against the models we consider. The adversarial examples are generated for Inc-v3, Inc-v4, InvRes-v2 and Res-152 respectively using FGSM, iterative FGSM (I-FGSM) and MI-FGSM attack methods. The success rates are the misclassification rates of the corresponding models with adversarial images as inputs. The maximum perturbation ǫ is set to 16 among all experiments, with pixel value in [0, 255]. The number of iterations is 10 for I-FGSM and MI-FGSM, and the decay factor µ is 1.0, which will be studied in Sec. 4.2.1.</p><p>From the table, we can observe that MI-FGSM remains as a strong white-box adversary like I-FGSM since it can attack a white-box model with a near 100% success rate. On the other hand, it can be seen that I-FGSM reduces the success rates for black-box attacks than one-step FGSM. But by integrating momentum, our MI-FGSM outperforms both FGSM and I-FGSM in black-box attacks significantly. It obtains more than 2 times of the success rates than I-FGSM in most black-box attack cases, demonstrating the effectiveness of the proposed algorithm. We show two adversarial images in Fig. <ref type="figure">1 generated for Inc-v3</ref>.</p><p>It should be noted that although our method greatly improves the success rates for black-box attacks, it is still ineffective for attacking adversarially trained models (e.g., less than 16% for IncRes-v2 ens ) in the black-box manner. Later we show that ensemble-based approaches greatly improve the results in Sec. 4.3. Next, we study several aspects of MI-FGSM that are different from vanilla iterative methods, to further explain why it performs well in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Decay factor µ</head><p>The decay factor µ plays a key role for improving the success rates of attacks. If µ = 0, momentum-based iterative methods trivially turn to vanilla iterative methods. Therefore, we study the appropriate value of the decay factor. We attack Inc-v3 model by MI-FGSM with the perturbation ǫ = 16, the number of iterations 10, and the decay factor ranging from 0.0 to 2.0 with a granularity 0.1.</p><p>We show the success rates of the generated adversarial examples against Inc-v3, Inc-v4, IncRes-v2 and Res-152 in Fig. <ref type="figure">2</ref>. The curve of the success rate against a black-box model is unimodal whose maximum value is obtained at around µ = 1.0. When µ = 1.0, another interpretation of g t defined in Eq. ( <ref type="formula" target="#formula_5">6</ref>) is that it simply adds up all previous gradients to perform the current update.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">The number of iterations</head><p>We then study the effect of the number of iterations on the success rates when using I-FGSM and MI-FGSM. We adopt the same hyper-parameters (i.e., ǫ = 16, µ = 1.0) for attacking Inc-v3 model with the number of iterations ranging from 1 to 10, and then evaluate the success rates of adversarial examples against Inc-v3, Inc-v4, IncRes-v2 and Res-152 models, with the results shown in Fig. <ref type="figure">3</ref>. It can be observed that when increasing the number of iterations, the success rate of I-FGSM against a black-box model gradually decreases, while that of MI-FGSM maintains at a high value. The results prove our argument that the adversarial examples generated by iterative methods easily overfit a white-box model and are not likely to transfer across models. But momentum-based iterative methods help to alleviate the trade-off between the white-box attacks and the transferability, thus demonstrating a strong attack ability for white-box and black-box models simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Update directions</head><p>To interpret why MI-FGSM demonstrates better transferability, we further examine the update directions given by I-FGSM and MI-FGSM along the iterations. We calculate the cosine similarity of two successive perturbations and show the results in Fig. <ref type="figure" target="#fig_0">4</ref> when attacking Inc-v3. The update direction of MI-FGSM is more stable than that of I-FGSM due to the larger value of cosine similarity in MI-FGSM.</p><p>Recall that the transferability comes from the fact that models learn similar decision boundaries around a data point <ref type="bibr" target="#b11">[12]</ref>. Although the decision boundaries are similar, they are unlikely the same due to the highly non-linear structure of DNNs. So there may exist some exceptional decision regions around a data point for a model (holes as shown in Fig. <ref type="figure" target="#fig_5">4&amp;5</ref> in <ref type="bibr" target="#b11">[12]</ref>), which are hard to transfer to other models. These regions correspond to poor local maxima in the optimization process and the iterative methods can easily trap into such regions, resulting in less transferable adversarial examples. On the other hand, the stabilized update directions obtained by the momentum methods as observed in Fig. <ref type="figure" target="#fig_0">4</ref> can help to escape from these exceptional regions, resulting in better transferability for adversarial attacks. Another interpretation is that the stabilized updated directions make the L 2 norm of the perturbations larger, which may be helpful for the transferability.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">The size of perturbation</head><p>We finally study the influence of the size of adversarial perturbation on the success rates. We attack Inc-v3 model by FGSM, I-FGSM and MI-FGSM with ǫ ranging from 1 to 40 with the image intensity [0, 255], and evaluate the performance on the white-box model Inc-v3 and a black-box model Res-152. In our experiments, we set the step size α in I-FGSM and MI-FGSM to 1, so the number of iterations grows linearly with the size of perturbation ǫ. The results are shown in Fig. <ref type="figure" target="#fig_5">5</ref>.</p><p>For the white-box attack, iterative methods reach the 100% success rate soon, but the success rate of one-step FGSM decreases when the perturbation is large. The phenomenon largely attributes to the inappropriate assumption of the linearity of the decision boundary when the perturbation is large <ref type="bibr" target="#b11">[12]</ref>. For the black-box attacks, although the success rates of these three methods grow linearly with the size of perturbation, MI-FGSM's success rate grows faster. In other words, to attack a black-box model with a required success rate, MI-FGSM can use a smaller perturbation, which is more visually indistinguishable for humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Attacking an ensemble of models</head><p>In this section, we show the experimental results of attacking an ensemble of models. We first compare the three ensemble methods introduced in Sec. 3.2, and then demonstrate that the adversarially trained models are vulnerable to our black-box attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Comparison of ensemble methods</head><p>We compare the ensemble methods for attacks in this section. We include four models in our study, which are Inc-v3, Inc-v4, IncRes-v2 and Res-152. In our experiments, we keep one model as the hold-out black-box model and attack an ensemble of the other three models by FGSM, I-FGSM and MI-FGSM respectively, to fully compare the results of the three ensemble methods, i.e., ensemble in logits, ensemble in predictions and ensemble in loss. We set ǫ to 16, the number of iterations in I-FGSM and MI-FGSM to 10, µ in MI-FGSM to 1.0, and the ensemble weights equally. The results are shown in Table <ref type="table" target="#tab_1">2</ref>.</p><p>It can be observed that the ensemble in logits outperforms the ensemble in predictions and the ensemble in loss consistently among all the attack methods and different models in the ensemble for both the white-box and blackbox attacks. Therefore, the ensemble in logits scheme is more suitable for adversarial attacks.</p><p>Another observation from Table <ref type="table" target="#tab_1">2</ref> is that the adversarial examples generated by MI-FGSM transfer at a high rate, enabling strong black-box attacks. For example, by attacking an ensemble of Inc-v4, IncRes-v2 and Res-152 fused in logits without Inc-v3, the generated adversarial examples can fool Inc-v3 with a 87.9% success rate. Normally trained models show their great vulnerability against such an attack.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Attacking adversarially trained models</head><p>To attack the adversarially trained models in the black-box manner, we include all seven models introduced in Sec. 4.1.</p><p>Similarly, we keep one adversarially trained model as the hold-out target model to evaluate the performance in the black-box manner, and attack the rest six model in an ensemble, whose logits are fused together with equal ensemble weights. The perturbation ǫ is 16 and the decay factor µ is 1.0. We compare the results of FGSM, I-FGSM and MI-FGSM with 20 iterations. The results are shown in Table <ref type="table" target="#tab_3">3</ref>.</p><p>It can be seen that the adversarially trained models also cannot defend our attacks effectively, which can fool Inc-v3 ens4 by more than 40% of the adversarial examples. Therefore, the models obtained by ensemble adversarial training, the most robust models trained on the ImageNet as far as we are concerned, are vulnerable to our attacks in the black-box manner, thus causing new security issues for developing algorithms to learn robust deep learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Competitions</head><p>There are three sub-competitions in the NIPS 2017 Adversarial Attacks and Defenses Competition, which are the Non-targeted Adversarial Attack, Targeted Adversarial Attack and Defense Against Adversarial Attack. The organizers provide 5000 ImageNet-compatible images for evaluating the attack and defense submissions. For each attack, one adversarial example is generated for each image with the size of perturbation ranging from 4 to 16 (specified by the organizers), and all adversarial examples run through all defense submissions to get the final score. We won the first places in both the non-targeted attack and targeted attack by the method introduced in this paper. We will specify the configurations in our submissions.</p><p>For the non-targeted attack<ref type="foot" target="#foot_0">2</ref> , we implement the MI-FGSM for attacking an ensemble of Inc-v3, Inc-v4, IncRes-v2, Res-152, Inc-v3 ens3 , Inc-v3 ens4 , IncRes-v2 ens and Inc-v3 adv <ref type="bibr" target="#b9">[10]</ref>. We adopt the ensemble in logits scheme. The ensemble weights are set as 1 /7.25 equally for the first seven models and 0.25 /7.25 for Inc-v3 adv . The number of iterations is 10 and the decay factor µ is 1.0.</p><p>For the targeted attack<ref type="foot" target="#foot_1">3</ref> , we build two graphs for attacks. If the size of perturbation is smaller than 8, we attack Inc-v3 and IncRes-v2 ens with ensemble weights 1 /3 and 2 /3; otherwise we attack an ensemble of Inc-v3, Inc-v3 ens3 , Inc-v3 ens4 , IncRes-v2 ens and Inc-v3 adv with ensemble weights 4 /11, 1 /11, 1 /11, 4 /11 and 1 /11. The number of iterations is 40 and 20 respectively, and the decay factor µ is also 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Taking a different perspective, we think that finding an adversarial example is an analogue to training a model and the transferability of the adversarial example is also an analogue to the generalizability of the model. By taking a meta view, we actually "train" an adversarial example given a set of models as training data. In this way, the improved transferability obtained by the momentum and ensemble methods is reasonable because the generalizability of a model is usually improved by adopting the momentum optimizer or training on more data. And we think that other tricks (e.g., SGD) for enhancing the generalizability of a model could also be incorporated into adversarial attacks for better transferability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a broad class of momentumbased iterative methods to boost adversarial attacks, which can effectively fool white-box models as well as blackbox models. Our methods consistently outperform one-step gradient-based methods and vanilla iterative methods in the black-box manner. We conduct extensive experiments to validate the effectiveness of the proposed methods and explain why they work in practice. To further improve the transferability of the generated adversarial examples, we propose to attack an ensemble of models whose logits are fused together. We show that the models obtained by ensemble adversarial training are vulnerable to our black-box attacks, which raises new security issues for the development of more robust deep learning models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>4 :</head><label>4</label><figDesc>Input x *t to f and obtain the gradient ∇xJ(x * t , y);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4 :</head><label>4</label><figDesc>Input x *t and output l k (x * t ) for k = 1, 2, ..., K;5:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Figure 2. The success rates (%) of the adversarial examples generated for Inc-v3 against Inc-v3 (white-box), Inc-v4, IncRes-v2 and Res-152 (black-box), with µ ranging from 0.0 to 2.0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The cosine similarity of two successive perturbations in I-FGSM and MI-FGSM when attacking Inc-v3 model. The results are averaged over 1000 images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The success rates (%) of the adversarial examples generated for Inc-v3 against Inc-v3 (white-box) and Res-152 (blackbox). We compare the results of FGSM, I-FGSM and MI-FGSM with different size of perturbation. The curves of Inc-v3 vs. MI-FGSM and Inc-v3 vs. I-FGSM overlap together.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The success rates (%) of non-targeted adversarial attacks against seven models we study. The adversarial examples are crafted for Inc-v3, Inc-v4, IncRes-v2 and Res-152 respectively using FGSM, I-FGSM and MI-FGSM.</figDesc><table><row><cell></cell><cell>Attack</cell><cell>Inc-v3</cell><cell>Inc-v4</cell><cell>IncRes-v2</cell><cell>Res-152</cell><cell>Inc-v3 ens3</cell><cell>Inc-v3 ens4</cell><cell>IncRes-v2ens</cell></row><row><cell></cell><cell>FGSM</cell><cell>72.3  *</cell><cell>28.2</cell><cell>26.2</cell><cell>25.3</cell><cell>11.3</cell><cell>10.9</cell><cell>4.8</cell></row><row><cell>Inc-v3</cell><cell>I-FGSM</cell><cell>100.0  *</cell><cell>22.8</cell><cell>19.9</cell><cell>16.2</cell><cell>7.5</cell><cell>6.4</cell><cell>4.1</cell></row><row><cell></cell><cell>MI-FGSM</cell><cell>100.0  *</cell><cell>48.8</cell><cell>48.0</cell><cell>35.6</cell><cell>15.1</cell><cell>15.2</cell><cell>7.8</cell></row><row><cell></cell><cell>FGSM</cell><cell>32.7</cell><cell>61.0  *</cell><cell>26.6</cell><cell>27.2</cell><cell>13.7</cell><cell>11.9</cell><cell>6.2</cell></row><row><cell>Inc-v4</cell><cell>I-FGSM</cell><cell>35.8</cell><cell>99.9  *</cell><cell>24.7</cell><cell>19.3</cell><cell>7.8</cell><cell>6.8</cell><cell>4.9</cell></row><row><cell></cell><cell>MI-FGSM</cell><cell>65.6</cell><cell>99.9  *</cell><cell>54.9</cell><cell>46.3</cell><cell>19.8</cell><cell>17.4</cell><cell>9.6</cell></row><row><cell></cell><cell>FGSM</cell><cell>32.6</cell><cell>28.1</cell><cell>55.3  *</cell><cell>25.8</cell><cell>13.1</cell><cell>12.1</cell><cell>7.5</cell></row><row><cell>IncRes-v2</cell><cell>I-FGSM</cell><cell>37.8</cell><cell>20.8</cell><cell>99.6  *</cell><cell>22.8</cell><cell>8.9</cell><cell>7.8</cell><cell>5.8</cell></row><row><cell></cell><cell>MI-FGSM</cell><cell>69.8</cell><cell>62.1</cell><cell>99.5  *</cell><cell>50.6</cell><cell>26.1</cell><cell>20.9</cell><cell>15.7</cell></row><row><cell></cell><cell>FGSM</cell><cell>35.0</cell><cell>28.2</cell><cell>27.5</cell><cell>72.9  *</cell><cell>14.6</cell><cell>13.2</cell><cell>7.5</cell></row><row><cell>Res-152</cell><cell>I-FGSM</cell><cell>26.7</cell><cell>22.7</cell><cell>21.2</cell><cell>98.6  *</cell><cell>9.3</cell><cell>8.9</cell><cell>6.2</cell></row><row><cell></cell><cell>MI-FGSM</cell><cell>53.6</cell><cell>48.9</cell><cell>44.7</cell><cell>98.5  *</cell><cell>22.1</cell><cell>21.7</cell><cell>12.9</cell></row></table><note>*  indicates the white-box attacks.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The success rates (%) of non-targeted adversarial attacks of three ensemble methods. We report the results for an ensemble of white-box models and a hold-out black-box target model. We study four models-Inc-v3, Inc-v4, IncRes-v2 and Res-152. In each row, "-" indicates the name of the hold-out model and the adversarial examples are generated for the ensemble of the other three models by FGSM, I-FGSM and MI-FGSM respectively. Ensemble in logits consistently outperform other methods.</figDesc><table><row><cell></cell><cell>Ensemble method</cell><cell cols="2">FGSM Ensemble Hold-out</cell><cell cols="2">I-FGSM Ensemble Hold-out</cell><cell cols="2">MI-FGSM Ensemble Hold-out</cell></row><row><cell></cell><cell>Logits</cell><cell>55.7</cell><cell>45.7</cell><cell>99.7</cell><cell>72.1</cell><cell>99.6</cell><cell>87.9</cell></row><row><cell>-Inc-v3</cell><cell>Predictions</cell><cell>52.3</cell><cell>42.7</cell><cell>95.1</cell><cell>62.7</cell><cell>97.1</cell><cell>83.3</cell></row><row><cell></cell><cell>Loss</cell><cell>50.5</cell><cell>42.2</cell><cell>93.8</cell><cell>63.1</cell><cell>97.0</cell><cell>81.9</cell></row><row><cell></cell><cell>Logits</cell><cell>56.1</cell><cell>39.9</cell><cell>99.8</cell><cell>61.0</cell><cell>99.5</cell><cell>81.2</cell></row><row><cell>-Inc-v4</cell><cell>Predictions</cell><cell>50.9</cell><cell>36.5</cell><cell>95.5</cell><cell>52.4</cell><cell>97.1</cell><cell>77.4</cell></row><row><cell></cell><cell>Loss</cell><cell>49.3</cell><cell>36.2</cell><cell>93.9</cell><cell>50.2</cell><cell>96.1</cell><cell>72.5</cell></row><row><cell></cell><cell>Logits</cell><cell>57.2</cell><cell>38.8</cell><cell>99.5</cell><cell>54.4</cell><cell>99.5</cell><cell>76.5</cell></row><row><cell>-IncRes-v2</cell><cell>Predictions</cell><cell>52.1</cell><cell>35.8</cell><cell>97.1</cell><cell>46.9</cell><cell>98.0</cell><cell>73.9</cell></row><row><cell></cell><cell>Loss</cell><cell>50.7</cell><cell>35.2</cell><cell>96.2</cell><cell>45.9</cell><cell>97.4</cell><cell>70.8</cell></row><row><cell></cell><cell>Logits</cell><cell>53.5</cell><cell>35.9</cell><cell>99.6</cell><cell>43.5</cell><cell>99.6</cell><cell>69.6</cell></row><row><cell>-Res-152</cell><cell>Predictions</cell><cell>51.9</cell><cell>34.6</cell><cell>99.9</cell><cell>41.0</cell><cell>99.8</cell><cell>67.0</cell></row><row><cell></cell><cell>Loss</cell><cell>50.4</cell><cell>34.1</cell><cell>98.2</cell><cell>40.1</cell><cell>98.8</cell><cell>65.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>The success rates (%) of non-targeted adversarial attacks against an ensemble of white-box models and a hold-out black-box target model. We include seven models-Inc-v3, Inc-v4, IncRes-v2, Res-152, Inc-v3ens3, Inc-v3ens4 and IncRes-v2ens. In each row, "-" indicates the name of the hold-out model and the adversarial examples are generated for the ensemble of the other six models.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">Source code is available at https://github.com/dongyp13/ Non-Targeted-Adversarial-Attacks.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">Source code is available at https://github.com/dongyp13/ Targeted-Adversarial-Attacks.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The work is supported by the National NSF of China (Nos. 61620106010, 61621136008, 61332007, 61571261 and U1611461), Beijing Natural Science Foundation (No. L172037), Tsinghua Tiangong Institute for Intelligent Computing and the NVIDIA NVAIL Program, and partially funded by Microsoft Research Asia and Tsinghua-Intel Joint Research Institute.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ensemble selection from libraries of models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Crew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ksikes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05493</idno>
		<title level="m">Towards interpretable deep neural networks by leveraging adversarial examples</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Optimization and global minimization methods suitable for neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Duch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Korczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computing surveys</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="163" to="212" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Salamon</surname></persName>
		</author>
		<title level="m">Neural network ensembles. IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="993" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural network ensembles, cross validation and active learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vedelsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02533</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2008">2017. 1, 2, 3, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dropout inference in bayesian neural networks with alpha-divergences</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Delving into transferable adversarial examples and black-box attacks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2007">2017. 1, 2, 3, 4, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On detecting adversarial perturbations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Genewein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bischoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Universal adversarial perturbations</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00633</idno>
		<title level="m">Robust deep learning via reverse cross-entropy training and thresholding test</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Practical black-box attacks against machine learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security</title>
				<meeting>the 2017 ACM on Asia Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Some methods of speeding up the convergence of iteration methods</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">USSR Computational Mathematics and Mathematical Physics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mc-Daniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
