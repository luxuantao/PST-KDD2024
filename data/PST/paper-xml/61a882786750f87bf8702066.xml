<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Junhan</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zheng</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shitao</forename><surname>Xiao</surname></persName>
							<email>stxiao@bupt.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chaozhuo</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
							<email>liandefu@ustc.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Sanjay</forename><surname>Agrawal</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft India Development Center</orgName>
								<address>
									<settlement>Bengaluru</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amit</forename><surname>Singh</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft India Development Center</orgName>
								<address>
									<settlement>Bengaluru</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guangzhong</forename><surname>Sun</surname></persName>
							<email>gzsun@ustc.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
							<email>xingx@microsoft.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The representation learning on textual graph is to generate low-dimensional embeddings for the nodes based on the individual textual features and the neighbourhood information. Recent breakthroughs on pretrained language models and graph neural networks push forward the development of corresponding techniques. The existing works mainly rely on the cascaded model architecture: the textual features of nodes are independently encoded by language models at first; the textual embeddings are aggregated by graph neural networks afterwards. However, the above architecture is limited due to the independent modeling of textual features. In this work, we propose GraphFormers, where layerwise GNN components are nested alongside the transformer blocks of language models. With the proposed architecture, the text encoding and the graph aggregation are fused into an iterative workflow, making each node's semantic accurately comprehended from the global perspective. In addition, a progressive learning strategy is introduced, where the model is successively trained on manipulated data and original data to reinforce its capability of integrating information on graph. Extensive evaluations are conducted on three large-scale benchmark datasets, where GraphFormers outperform the SOTA baselines with comparable running efficiency. The source code is released at https://github.com/microsoft/GraphFormers .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The textual graph is a widely existed data format, where each node is annotated with its textual feature. The representation learning on textual graph is to generate low-dimensional node embeddings based on the individual textual features and the information from the neighbourhood. In recent years, the breakthroughs in pretrained language models and graph neural networks contribute to the development of corresponding techniques. Particularly, with pretrained language models, such as BERT <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref> and RoBERTa <ref type="bibr" target="#b11">(Liu et al., 2019a)</ref>, the underlying semantics of texts can be captured more precisely; at the same time, with graph neural networks, like GraphSage <ref type="bibr" target="#b5">(Hamilton et al., 2017a)</ref> and GAT <ref type="bibr" target="#b23">(Veličković et al., 2018)</ref>, neighbours can be effectively aggregated for more informative node embeddings. It is necessary to combine both techniques for better textual graph representation. As suggested by GraphSage <ref type="bibr" target="#b5">(Hamilton et al., 2017a)</ref> and PinSage <ref type="bibr" target="#b35">(Ying et al., 2018)</ref>, the textual feature can be independently modeled by text encoders and further aggregated by rear-mounted GNNs for the final node embeddings. Such a representation paradigm has been widely adopted by subsequent works on various scenarios <ref type="bibr" target="#b38">(Zhu et al., 2021;</ref><ref type="bibr" target="#b10">Li et al., 2021;</ref><ref type="bibr" target="#b7">Hu et al., 2020;</ref><ref type="bibr" target="#b12">Liu et al., 2019b;</ref><ref type="bibr" target="#b37">Zhou et al., 2019)</ref>, where GNNs are combined with powerful PLM-based text encoders.</p><p>The above way of combination is called the "Cascaded Transformers-GNN" architecture (Figure <ref type="figure" target="#fig_0">1 A</ref>), as the language models (built upon Transformers) are deployed ahead of the GNN component. With the above architecture, the text encoding and the graph aggregation are performed in two consecutive steps, where there is no information exchange between the nodes when text embeddings are generated. However, the above workflow is defective considering that the linked nodes are correlated, whose underlying semantics can be mutually enhanced. For example, given a node "notes on transformers" and its neighbour "tutorials on machine translation"; by making reference to the whole context, the "transformers" here can be interpreted as a machine learning model, rather than an electric device.</p><p>Our Work. We propose "GNN-nested Transformers" (GraphFormers), which are highlighted for the fusion of GNNs and language models (Figure <ref type="figure" target="#fig_0">1 B</ref>). In GraphFormers, the GNN components are nested alongside the transformer layers (TRM) of language models, where the text encoding and graph aggregation are fused as an iteratively workflow. In each iteration, the linked nodes will exchange information with each other in the layerwise GNN component; thus, each node will be augmented by its neighbourhood information. The transformer component will work on the augmented node features, where increasingly informative node representations can be generated for the next iteration. Compared with the cascaded architecture, GraphFormers achieve more sufficient utilization of the cross-node information on graph, which significantly benefit the representation quality. Given that the layerwise GNN components merely involve simple and effective multi-head attention, GraphFormers preserve comparable running costs as the existing cascaded Transformers-GNN models.</p><p>On top of the proposed model architecture, we further improve GraphFormers' representation quality and practicability as follows. Firstly, the training of GraphFormers is likely to be shortcut: in many cases, the center node itself can be "sufficiently informative", where the training tasks can be accomplished without leveraging the neighbourhood information. As such, GraphFormers may end up with insufficiently trained GNNs. Inspired by recent success of curriculum learning <ref type="bibr" target="#b1">(Bengio et al., 2009)</ref>, we propose to train the model progressively: the first round of training is performed with manipulated data, where the nodes are randomly polluted; thus, it becomes harder to make prediction merely rely on the center nodes, and the model will be forced to leverage the whole input nodes. The second round of training gets back to the unpolluted data, where the model will be fit into the targeted distribution. Another concern about GraphFormers is that all the linked nodes are mutually dependent in the representation process: once a new node is presented, all the neighbours, regardless of whether they have been processed before, need to be encoded from scratch. As a result, a great deal of unnecessary computations will be incurred. We introduce unidirectional graph attention to alleviate this problem: only the center node is required to make reference to the neighbours, while the neighbour nodes remain independently encoded. By this means, the existing neighbours' encoding results can be cached and reused, which significantly saves the computation cost.</p><p>Extensive evaluations are conducted with three million-scale textual graph datasets: DBLP, Wiki and Product, where the representation quality is measured by the link prediction accuracy. According to our experiment results, GraphFormers significantly outperform the SOTA cascaded Transformers-GNN baselines with comparable running efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The textual graph representation is an important research topic in multiple areas, such as natural language processing, information retrieval and graph learning <ref type="bibr" target="#b32">(Yang et al., 2015;</ref><ref type="bibr">Wang et al., 2016b,a;</ref><ref type="bibr" target="#b34">Yasunaga et al., 2017;</ref><ref type="bibr" target="#b27">Wang et al., 2019a;</ref><ref type="bibr" target="#b31">Xu et al., 2019)</ref>. To learn high-quality representation for textual graph, techniques on natural language understanding and graph representation need to be jointly leveraged. In recent years, breakthroughs on pretrained language models (PLM) and graph neural networks (GNN) significantly advance the development of corresponding techniques.</p><p>PLM. The PLMs are proposed to learn universal language models with neural networks trained on large-scale corpus. The early works were based on shallow networks, e.g, word embeddings learned by Skip-Gram <ref type="bibr" target="#b14">(Mikolov et al., 2013)</ref> and GloVe <ref type="bibr" target="#b15">(Pennington et al., 2014)</ref>. In recent years, the backbone networks are being quickly scaled up: from <ref type="bibr">EMLo (Peters et al., 2018)</ref>, GPT <ref type="bibr" target="#b17">(Radford et al., 2018)</ref>, to BERT <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref>, XLNet <ref type="bibr" target="#b33">(Yang et al., 2019)</ref>, T5 <ref type="bibr" target="#b18">(Raffel et al., 2019)</ref>, <ref type="bibr">GPT-3 (Brown et al., 2020)</ref>. The large-scale models, which get fully trained with massive data, demonstrate superior performances on general NLP tasks. One of the most critical usages of PLMs is text representation, where the underlying semantics of texts are captured by low-dimensional embeddings. Such embeddings achieve competitive results on downstream tasks, like text retrieval and classification <ref type="bibr" target="#b20">(Reimers and Gurevych, 2019;</ref><ref type="bibr" target="#b13">Luan et al., 2020;</ref><ref type="bibr" target="#b4">Gao et al., 2021;</ref><ref type="bibr" target="#b21">Su et al., 2021)</ref>.</p><p>GNN. Graph neural networks are recognized as powerful tools of modeling graph data <ref type="bibr" target="#b6">(Hamilton et al., 2017b;</ref><ref type="bibr" target="#b36">Zhou et al., 2020)</ref>. Such methods (e.g., <ref type="bibr">GCN (Kipf and Welling, 2016)</ref>, GAT <ref type="bibr" target="#b23">(Veličković et al., 2018)</ref>, GraphSage <ref type="bibr" target="#b5">(Hamilton et al., 2017a)</ref>) learn effective message passing mechanisms such that information between the nodes can get aggregated for expressive graph representations.</p><p>Graph neural networks may also incorporate node attributes, like texts; and it's quite straightforward to leverage GNNs and PLMs for textual graph representation following the "cascaded architecture" suggested by GraphSage <ref type="bibr" target="#b5">(Hamilton et al., 2017a)</ref>: the node features are independently encoded at first; then, the node embeddings are aggregated via GNNs to generate the final representations. Such a representation paradigm is widely adopted by subsequent works <ref type="bibr" target="#b38">(Zhu et al., 2021;</ref><ref type="bibr" target="#b10">Li et al., 2021;</ref><ref type="bibr" target="#b7">Hu et al., 2020;</ref><ref type="bibr" target="#b12">Liu et al., 2019b;</ref><ref type="bibr" target="#b37">Zhou et al., 2019)</ref>. However, the above approaches treat the text encoding and graph aggregation as two consecutive steps, where the node-level features are independently processed. Our work is different from these approaches as the text encoding and graph aggregation are fused as an iterative workflow based on the "GNN-nested Transformers".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GraphFormers</head><p>In this work, we deal with textual graph data, where each node x is a text. The node x together with its neighbours N x are denoted as G x . Our model learns the embedding for node x based on its own textual feature and the information of its neighbourhood N x . The generated embeddings are expected to capture the relationship between the nodes, i.e., to accurately predict whether two nodes x q and x k are connected based on the embedding similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">GNN-nested Transformers</head><p>The encoding process of GraphFormers is indicated as follows. The input nodes (the center node and its neighbours) are tokenized into sequences of tokens, with special tokens [CLS] padded in the front, whose states are used for node representation. The input sequences are mapped into the initial embedding sequences {H 0 g } G based on the summation of word embeddings and position embeddings. The embedding sequences are encoded by multiple layers of GNN-nested Transformers (shown as Figure <ref type="figure" target="#fig_1">2</ref>), where the graph aggregation and text encoding are iteratively performed.</p><p>• Graph Aggregation in GNN. Each node is enhanced by its neighbourhood information based on the layerwise graph aggregation. For each node in the l-th layer, the first token-level embedding (corresponding to <ref type="bibr">[CLS]</ref>) is taken as the node-level embedding:</p><formula xml:id="formula_0">z l g ← H l g [0]</formula><p>. The node-level embeddings are gathered from all the nodes and passed to the layerwise GNN for graph aggregation. We leverage Multi-Head Attention (MHA) to encode the node-level embeddings Z l G ({z l g } G ), similar as GAT <ref type="bibr" target="#b23">(Veličković et al., 2018)</ref>. For each attention head, the scaled dot-product attention is performed as:</p><formula xml:id="formula_1">Ẑl G = MHA(Z l G ); MHA(Z l G ) = Concat(head 1 , ..., head h ); head j = softmax( QK T √ d + B)V; Q = Z l G W Q j ; K = Z l G W K j ; V = Z l G W V j ;<label>(1)</label></formula><p>In the above equations, W Q j , W K j , and W V j are the projection matrices of MHA, corresponding to the j-th attention head. A learnable position bias B is added to the dot-product result; the positions differentiate the relationship between the nodes; i.e., "center-to-center" (x to x), "center-to-neighbour" (x to N x ), and "neighbour-to-neighbour" (N x to N x ), respectively.</p><p>Each of the embeddings ẑl g (ẑ l g ∈ Ẑl G ) is dispatched to its original node and concatenated (⊕) with the token-level embeddings, which gives rise to the graph-augmented token-level embeddings:</p><formula xml:id="formula_2">H l g ← Concat(ẑ l g , H l g ).<label>(2)</label></formula><p>In this place, the GNN-processed node-level embeddings Ẑl G can be interpreted as "messagers", with which the neighbourhood information can be introduced to each of the nodes.</p><p>• Text Encoding in Transformer. The graph-augmented token-level embeddings H l g are processed by the transformer component <ref type="bibr" target="#b22">(Vaswani et al., 2017)</ref>, where the following computations are performed:</p><formula xml:id="formula_3">H l g = LN(H l g + MHA asy ( H l g )); H l+1 g = LN( H l g + MLP( H l g )).</formula><p>(3)</p><p>In the above equations, MLP is the Multi-Layer Projection unit, and LN is the Layer-Norm unit. We use asymmetric Multi-Head Attention (MHA asy ), where Q, K, V are computed as:</p><formula xml:id="formula_4">Q = H l g W Q j ; K = H l g W K j ; V = H l g W V j .<label>(4)</label></formula><p>Therefore, the output sequence H l+1 g will be of the same length as the input sequence H l g . The encoding result will be used as the input token-level embeddings for the next layer. The node-level embedding at the last layer z L x (i.e., H L g [0]) will be used as the final node representation. • Workflow. We summarize GraphFormers' encoding workflow as Algorithm 1. The initial tokenlevel embeddings {H 0 g } G are independently encoded by the first Transformer layer TRM 0 . For a L-layer GraphFormers, the graph aggregation and text encoding are iteratively performed for the subsequent L-1 steps (from 1 to L − 1). In each step, the node-level embeddings Z l G are gathered and Algorithm 1: GraphFormers' Workflow Input: The input graphs G (consist of the center node x and its neighbours).</p><p>Output: The embedding for the center node h x . begin</p><p>for each text g ∈ G do H 1 g ← TRM 0 (H 0 g ); // Get the initial token-level embeddings</p><formula xml:id="formula_5">for l = 1, ..., L − 1 do Z l G ← {z l g |g∈G}; // Gather node-level embeddings to GNN Ẑl G ← GNN(Z l G ); // Graph aggregation in GNN for each text g ∈ G do 8 H l g ← Concat(ẑ l g , H l g ); // Get graph-augmented token-level embeddings 9 H l+1 g ← TRM l ( H l g ); // Text encoding in Transformer Return h x ← z L x ;</formula><p>processed by the layerwise GNN component. The output node-level embeddings Ẑl G are dispatched to their original nodes, which generates the graph-augmented token-level embeddings H l g . The graphaugmented token-level embedding are further processed by the Transformer component. Finally, The node-level embedding (for the center node x) in the last layer z L</p><p>x is taken as our representation result. • Encoding Complexity. Given an input of M nodes, each one has P tokens; the time complexity of each layer's encoding operation is O(M<ref type="foot" target="#foot_0">2</ref> + M P 2 ): the graph aggregation takes O(M 2 ), because M node-level embeddings are gathered for multi-head attention; the text encoding takes O(M P 2 ), as each of the M node calls for the multi-head attention of P tokens. Compared with Transformers, the GNN's computation cost is much smaller, mainly because of two reasons: 1) M 2 M P 2 in general, 2) operations like MLP are not needed in graph aggregation. Therefore, the working efficiency of GraphFormers is close to the cascaded GNN-Transformers as the extra computation cost of layerwise graph aggregation is relatively small. Such a property is also empirically verified in our experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Simplification: Unidirectional Graph Aggregation</head><p>One concern about GraphFormers is that the input nodes are mutually dependent on each other during the encoding process. As a result, to generate the embedding for a node, all the related nodes in its neighbourhood need to be encoded from scratch, regardless of whether they have been processed before. Such a property is unfavorable in practice as a great deal of unnecessary computation cost might be incurred (i.e., a node will be repetitively encoded every time it serves as a neighbour node). We leverage a simple but effective simplification, the unidirectional graph aggregation, to address this problem. Particularly, only the center node x is required to make reference to the neighbourhood; while the rest of nodes N x remain independently encoded all by their own textual features:</p><formula xml:id="formula_6">H l+1 g = TRM l ( H l x ), g = x; TRM l (H l g ), ∀g ∈ N x .</formula><p>(5)</p><p>Because the encoding of the neighbour nodes is independent of the center node, the intermediate encoding results {z 1...L g } Nx can be cached in storage 2 and reused in subsequent computations when they are needed. As a result, the nodes can be prevented from being encoded repetitively, which saves a great deal of unnecessary computation cost. We empirically verify that GraphFormers maintain similar performances when the above simplification is introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Training: Two-Stage Progressive Learning</head><p>• Training Objective. We take advantage of link prediction as our training task. Given a pair of nodes q and k, the model is learned to predict whether they are connected based on their embedding similarity. Particularly, the following classification loss is minimized for a positive pair of q and k:<ref type="foot" target="#foot_1">3</ref> </p><formula xml:id="formula_7">L = − log exp( h q , h k ) exp( h q , h k ) + r∈R exp( h q , h r ) .<label>(6)</label></formula><p>In the above equation, h q and h k are the node embeddings; • denotes the computation of inner product; R stands for the negative samples. In our implementation, we leverage "in-batch negative samples" <ref type="bibr" target="#b8">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b13">Luan et al., 2020)</ref> for the reduction of encoding cost: a positive sample in one training instance will be used as a negative sample in the rest of the training instances within the same mini-batch.</p><p>• Two-stage Training. In GraphFormers, the information from the center node and neighbour nodes are not treated equally, which may undermine the model's training effect. Particularly, the center node's information can be directly utilized, while the neighbourhood information needs to be introduced via three steps: 1) encoded as node-level embeddings, 2) making graph aggregation with the center node, and 3) introduced to center node's graph augmented token-level embeddings. The message passing pathway can shortcut when the center nodes are "sufficiently informative", i.e., two nodes are sufficiently similar with each other in terms of their own textual features, such that their connection can be predicted without considering the neighbours. Given the existence of such cases, GraphFormers may end up with well-trained Transformers but insufficiently trained GNNs.</p><p>To alleviate the above problem, we introduce a warm-up training task, where the link prediction is made based on the polluted input nodes. Particularly, for each input node g, a subset of its tokens g m will be randomly masked<ref type="foot" target="#foot_2">4</ref> . As a result, the classification loss becomes:</p><formula xml:id="formula_8">L = − log exp( h q , h k ) exp( h q , h k ) + r∈R exp( h q , h r ) ,<label>(7)</label></formula><p>where h q , h k, h r are the embeddings generated from the polluted nodes. The masked tokens reduce the informativeness of each individual node; therefore, the model is forced to leverage the whole input nodes to make the right prediction.</p><p>Finally, the model training is organized as a two-stage progressive learning process. In the first stage, the model is trained to minimize L based on the polluted nodes until its convergence, which reinforce the model's capability of integrating information on graph. In the second stage, the model is continually trained to minimize L based on the original data until the convergence, which makes the model fit into the target distribution.</p><p>4 Experimental Studies</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data and Settings</head><p>We make use of the following three real-world textual graph datasets for our experimental studies.</p><p>• DBLP<ref type="foot" target="#foot_3">5</ref> , which contains the paper citation graph from DBLP up to 2020-04-09. Two papers are linked if one is cited by the other one. The paper's title is used as the textual feature.</p><p>• Wikidata5M<ref type="foot" target="#foot_4">6</ref> (Wiki) <ref type="bibr" target="#b28">(Wang et al., 2019b)</ref>, which contains the entity graph from Wikipedia. The first sentence in each entity's introduction is taken as its textual feature. • Product Graph (Product), an even larger dataset of online products collected by a world-wide search engine. In this dataset, the users' web browsing behaviors are tracked for the targeted product webpages (e.g., Amazon webpages of Nike shoes). The user's continuously browsed webpages within a short period of time (e.g., 30 minutes) is called a "session". The products within a common session are connected in the graph (which is a common way of graph construction in e-commerce scenarios <ref type="bibr" target="#b35">(Ying et al., 2018;</ref><ref type="bibr" target="#b25">Wang et al., 2018)</ref>). Each product has its unique textual description, which specifies information like the product name, brand, and saler, etc.</p><p>The textual features of all the datasets are in English. We make use of uncased WordPiece <ref type="bibr" target="#b29">(Wu et al., 2016)</ref> to tokenize the input text. In our experiment, each text is associated with 5 uniformly sampled neighbours (without replacement); for texts with neighbourhood smaller than 5, all the neighbours will be utilized. We summarized the specifications of all the datasets with Table <ref type="table" target="#tab_0">1</ref>. The experiment results are evaluated in terms of link prediction accuracy, i.e., to predict whether a query node and key node are connected given the textual features of themselves and their neighbours. In each testing instance, one query is provided with 300 keys: 1 positive plus 299 randomly sampled negative cases. We leverage three common metrics to measure the prediction accuracy: Precision@1, NDCG, and MRR.</p><p>Without specifications, we will take the unidirectional-simplified GraphFormers trained with the two-stage progressive learning as our default model. More details about the implementations and the training/testing configurations are summarized in an Appendix file. It is submitted together with our source code within the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We focus on the comparison between GNN-nested Transformers and Cascaded Transformers-GNN.</p><p>To make sure the difference between both architectures can be truthfully reflected from the evaluation results, GraphFormers and the Cascaded Transformers-GNN baselines are equipped with text encoders and graph aggregators of the same capacities. Particularly, we use the BERT-like PLM as our text encoder, where UniLM-base<ref type="foot" target="#foot_5">7</ref>  <ref type="bibr" target="#b0">(Bao et al., 2020)</ref> is chosen as the network backbone for all related methods; the final layer's [CLS] token embedding is used for the text embedding.</p><p>We enumerate the following representative graph aggregators as used in GAT <ref type="bibr" target="#b23">(Veličković et al., 2018)</ref>, GIN <ref type="bibr" target="#b30">(Xu et al., 2018)</ref>, GraphSage <ref type="bibr" target="#b5">(Hamilton et al., 2017a)</ref>. The GAT aggregator, where the node embedding is generated as the weighted sum of all the text embeddings. Each text embedding's relative importance is calculated as the attention score with the center node. The Pooling-and-Concat aggregators, where the center node's text embedding is concatenated with the neighbours' pooling result and linearly transformed for the final representation. Depending on the form of pooling function, we have the following options: Max and Mean, where neighbours are aggregated by max-pooling and mean-pooling, respectively; Att, where the neighbours are summed up based on the attention weights with the center node. By comparison, the neighbourhood information may get more emphasized with GAT; while the center node itself tends to be highlighted with Pooling-and-Concat.</p><p>We consider two more baselines which make use of simplified text encoders (such as CNN) and network embeddings: TNVE <ref type="bibr" target="#b27">(Wang et al., 2019a)</ref> and IFTN <ref type="bibr" target="#b31">(Xu et al., 2019)</ref>. We also include the PLM only baseline, which merely leverages the textual feature of the center node. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Overall Evaluation</head><p>The overall evaluation results are reported in Table <ref type="table" target="#tab_1">2</ref>. It's observed that GraphFormers consistently outperform all the baselines, especially the ones based on the cascaded Transformers-GNN, with notable advantages. Particularly, it achieves 2.9%, 4.8%, 6.5% relative improvements over the most competitive baselines (underlined) on each of the experimental datasets. Such an observation indicates that the relationship between the nodes can be captured more accurately based on the node embeddings generated by GraphFormers, which verifies the effectiveness of our proposed method.</p><p>We also observe the following underlying factors that may influence the representation quality.</p><p>Firstly, the effective utilization of neighbourhood information is critical. With the joint consideration of the center node and neighbour nodes, the PLM+GNNs methods, including GraphFormers and the cascaded Transformers-GNN baselines, significantly outperform the PLM only baseline in most of the time. We further analyze the impact of neighbourhood size as Table <ref type="table" target="#tab_2">3</ref>, with a fraction of neighbour nodes randomly sampled for each center node (using DBLP for illustration). It can be observed that both GraphFormers and PLM+Max (the most competitive baseline) achieve higher prediction accuracy than the PLM only method (P@1:0.5673, NDCG:0.7484, MRR:0.6777, as reported in Table <ref type="table" target="#tab_1">2</ref>), even with fewer neighbour nodes included. With the increasing number of neighbour nodes, the advantages become gradually enlarged. However, the marginal gain is vanishing, as the relative improvement becomes smaller when more neighbours are included. In all the testing cases, GraphFormers maintain consistent advantages over PLM+Max, which reaffirms the effectiveness of our proposed methods.</p><p>Secondly, the capacity of the text encoder is crucial for textual graph representation. All the pretrained language model based methods (GraphFormers, Cascaded Transformers-GNN baselines, PLM-only baseline) significantly outperform the baselines with simplified text encoders (TNVE, IFTN).</p><p>Thirdly, the representation quality is also sensitive to the form of graph aggregator. In Product, the cascaded Transformers-GNN baselines' performances are quite close to each other. In DBLP, PLM+(Max, Mean, Att) outperforms PLM+GAT. In Wiki, not only PLM+(Max, Mean, Att) but also PLM-only baseline outperform PLM+GAT. Such phenomenons could be attributed to the type of graph: whether it is homogeneous or heterogeneous. Particularly, both Product and DBLP can be regarded as homogeneous graphs as the nodes are connected based on the same relationships; i.e., co-view relationship in Product, and citation relationship in DBLP. In both homogeneous graphs, the connected nodes may have quite similar semantics (the co-viewed products usually serve similar user intents, and the citation relationships usually indicate similar research topics); thus, the incorporated neighbour nodes will probably provide complementary information for the link prediction between the center nodes. However, Wiki is a heterogeneous graph, where the connections between entities may have highly different semantics. As a result, the incorporation of neighbour nodes may not contribute to the link prediction task, especially when the incorporated neighbours and the prediction target are connected to the center nodes with totally different relationships. Considering that GAT tends to focus more on the neighbourhood, its performance can be vulnerable in such unfavorable situations. These findings suggest that the neighbourhood information should be properly handled in case that the information of the center node is wiped out.</p><p>Finally, we may conclude different methods' utility in textual graph representation: simplified text encoders ≺ PLMs ≺ Cascaded Transformers-GNN ≺ GNNs-nested Transformers. Such findings are consistent with our expectation that the precise modeling of individual textual feature and the effective integration of neighbourhood information will jointly contribute to high-quality textual graph representation. GraphFormers enjoy the high expressiveness of PLMs and leverage layerwise nested-GNNs to facilitate graph aggregation, which contributes to both of the above perspectives.</p><p>Table <ref type="table">4</ref>: Ablation Studies (The top ablated methods are marked in bold; "↑"/"↓": the performance is increased/decreased compared with the default setting). "-Progressive": two-stage progressive learning disabled; "-Simplified": unidirectional simplification disabled; "-Shared GNNs": GNNs parameters are not shared across the layers; "-Position": GNNs learnable position bias disabled. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>The ablation studies (as Table <ref type="table">4</ref>) are performed to clarify the following issues: 1) the impact of two-stage progressive learning, and 2) the impact of unidirectional-simplified GraphFormers.</p><p>Firstly, the two-stage progressive learning substantially improves GraphFormers' representation quality. Without such a training strategy ("-Progressive": training directly on the original data), the model's performance is decreased by 0.98%, 1.71%, and 1.18% in each of the datasets, respectively.</p><p>Secondly, the performances between simplified and non-simplified ("-Simplified") GraphFormers are comparable. In fact, the necessity of graph aggregation is not equivalent for the center node and the neighbour nodes: since the center node is the one for representation, it is much more important to ensure that the center node may extract complementary information from its neighbours. The unidirectional-simplified GraphFormers maintain such a property; thus, there is little impact on the final performances. Such a finding affirms that we may safely leverage the simplified model to save the cost of repetitively encoding the existing neighbours.</p><p>We make two additional ablation studies. "-Shared GNNs": the GNNs parameters sharing is disabled, where each layer maintains its own graph aggregator (by default, the layerwise GNN components in GraphFormers share the same set of parameters). "-Position": the learnable position bias (b in Eq. 1) is disabled in GNNs. We find that model's performance is little affected from the above changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Efficiency Analysis</head><p>We compare the time efficiency between GNN-nested Transformers (GraphFormers) and Cascaded Transformers+GNN (using PLM+Max for comparison). The evaluation is made with a Nvidia P100 GPU. Each mini-batch contains 32 encoding instances; each instance contains one center and #N neighbour nodes; the token length of each node is 16. We report the average time and memory (GPU RAM) costs per mini-batch as Table <ref type="table" target="#tab_3">5</ref>.</p><p>Firstly, the time and memory costs of both methods grow linearly with the increment of neighbours. (There are overheads of time and memory costs. The time cost overhead may come from CPU processing; while the memory cost overhead is mainly due to the model parameters <ref type="bibr" target="#b19">(Rajbhandari et al., 2020)</ref>). We may approximately remove the overheads by deducting the time and memory costs where #N=3). Such a finding is consistent with our theoretical analysis in Section 3.1.</p><p>Secondly, the overall time and memory costs of GraphFormers are quite close to PLM+Max. When the number of neighbour nodes is small, the differences between both methods are almost ignorable. The differences become slightly larger when more neighbour nodes are included, because the layerwise graph aggregations in GraphFormers get increasingly time consuming. However, the differences are still relatively small: merely around 3.5% of the overall running costs when #N is increased to 200 ("#N=200" is already more than enough for most of the real world scenarios).</p><p>Based on the above observations, we may conclude that GraphFormers are more accurate, meanwhile equally efficient and scalable as the conventional cascaded Transformer+GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Online A/B Test on Bing Search</head><p>GraphFormers has been deployed as one of the major ads retrieval algorithms on Bing Search, and it achieves highly competitive performance against the previous production system (the combination of a wide spectrum of semantic representation algorithms, including large-scale PLMs and cascaded PLMs-GNNs). Particularly, the primary objective of Ads service is to maximize the revenue meanwhile increasing the user clicks. Therefore, the following three metrics are taken as the major performance indicators: RPM<ref type="foot" target="#foot_6">8</ref> (revenue per thousand impressions), CY (click yield), and CPC<ref type="foot" target="#foot_7">9</ref> (cost per click) .</p><p>During our large-scale online A/B test, GraphFormers significantly improves the overall RPM, CY, CPC by 1.87%, 0.96% and 0.91%, respectively. A 11-day performance snapshot is demonstrated as Figure <ref type="figure" target="#fig_2">3</ref>; it can be observed that in most of the time, all three metrics are significantly improved thanks to the utilization of GraphFormers (the daily performance are measured based on millions of impressions, thus having strong statistic significance).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel model architecture GraphFormers for textual graph representation. By having GNNs nested alongside each transformer layer of the pretrained language model, the underlying semantic of each textual node can be precisely captured and effectively integrated for high-quality textual graph representation. On top of the fundamental architecture, we introduce the two-stage progressive training strategy to further strengthen GraphFormers' representation quality; we also simplify the model with the unidirectional graph aggregation, which eliminates the unnecessary computation cost. The experimental studies on three large-scale textual graph datasets verify the effectiveness of our proposed methods, where GraphFormers notably outperform the existing cascaded Transformer-GNNs methods with comparable running efficiency and scalability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgement</head><p>We are grateful to anonymous reviewers for their constructive comments on this work. The work was supported by grants from the National Natural Science Foundation of China (No. 62022077).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Model architecture comparison (a center node C is connected with two neighbours N1, N2). (A) Cascaded Transformers-GNN: text embeddings are independently generated by language models and aggregated by rear-mounted GNNs. (B) GNN-nested Transformers: the text encoding and graph aggregation are iteratively performed with the layerwise GNNs and Transformers (TRM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: GNN-nested Transformers (using the l-th layer for illustration). The graph aggregation is performed in the first place: the node-level embeddings {z l g } G are gathered from all the nodes and processed by the GNN component (the leftmost rectangle). The GNN processed node-level embeddings {ẑ l g } G are dispatched to their original nodes, which forms the graph-augmented tokenlevel embeddings. The graph-augmented token-level embeddings are further encoded by Transformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Online A/B Test: the relative improvements of RPM, CY and CPC against the last version of production system in Bing Search (green: positive; blue: negative). In most of the time, all three performance indicators are significantly improved thanks to the utilization of GraphFormers.</figDesc><graphic url="image-4.png" coords="10,103.34,63.23,412.72,91.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Specifications of the experimental datasets: the number of items, the number of neighbour nodes on average, and the number of training, validation, testing cases.</figDesc><table><row><cell></cell><cell>Product</cell><cell>DBLP</cell><cell>Wiki</cell></row><row><cell>#Item</cell><cell>5,643,688</cell><cell>4,894,081</cell><cell>4,818,679</cell></row><row><cell>#N</cell><cell>4.71</cell><cell>9.31</cell><cell>8.86</cell></row><row><cell>#Train</cell><cell>22,146,934</cell><cell>3,009,506</cell><cell>7,145,834</cell></row><row><cell>#Valid</cell><cell>30,000</cell><cell>60,000</cell><cell>66,167</cell></row><row><cell>#Test</cell><cell>306,742</cell><cell>100,000</cell><cell>100,000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Overall evaluation (GraphFormers marked in bold, the best baseline underlined). Graph-Formers outperforms all baselines, especially the ones based on cascaded Transformers-GNN.</figDesc><table><row><cell></cell><cell></cell><cell>Product</cell><cell></cell><cell></cell><cell>DBLP</cell><cell></cell><cell></cell><cell>Wiki</cell><cell></cell></row><row><cell>Methods</cell><cell>P@1</cell><cell>NDCG</cell><cell>MRR</cell><cell>P@1</cell><cell>NDCG</cell><cell>MRR</cell><cell>P@1</cell><cell>NDCG</cell><cell>MRR</cell></row><row><cell>PLM</cell><cell>0.6563</cell><cell>0.7911</cell><cell>0.7344</cell><cell>0.5673</cell><cell>0.7484</cell><cell>0.6777</cell><cell>0.3466</cell><cell>0.5799</cell><cell>0.4712</cell></row><row><cell>TNVE</cell><cell>0.4618</cell><cell>0.6204</cell><cell>0.5364</cell><cell>0.2978</cell><cell>0.5295</cell><cell>0.4163</cell><cell>0.1786</cell><cell>0.4274</cell><cell>0.2933</cell></row><row><cell>IFTN</cell><cell>0.5233</cell><cell>0.6740</cell><cell>0.5982</cell><cell>0.3691</cell><cell>0.5798</cell><cell>0.4773</cell><cell>0.1838</cell><cell>0.4276</cell><cell>0.2945</cell></row><row><cell>PLM+GAT</cell><cell>0.7540</cell><cell>0.8637</cell><cell>0.8232</cell><cell>0.6633</cell><cell>0.8204</cell><cell>0.7667</cell><cell>0.3006</cell><cell>0.5430</cell><cell>0.4270</cell></row><row><cell>PLM+Max</cell><cell>0.7570</cell><cell>0.8678</cell><cell>0.8280</cell><cell>0.6934</cell><cell>0.8386</cell><cell>0.7900</cell><cell>0.3712</cell><cell>0.6071</cell><cell>0.5022</cell></row><row><cell>PLM+Mean</cell><cell>0.7550</cell><cell>0.8671</cell><cell>0.8271</cell><cell>0.6896</cell><cell>0.8359</cell><cell>0.7866</cell><cell>0.3664</cell><cell>0.6037</cell><cell>0.4980</cell></row><row><cell>PLM+Att</cell><cell>0.7513</cell><cell>0.8652</cell><cell>0.8246</cell><cell>0.6910</cell><cell>0.8366</cell><cell>0.7875</cell><cell>0.3709</cell><cell>0.6067</cell><cell>0.5018</cell></row><row><cell>GraphFormers</cell><cell>0.7786</cell><cell>0.8793</cell><cell>0.8430</cell><cell>0.7267</cell><cell>0.8565</cell><cell>0.8133</cell><cell>0.3952</cell><cell>0.6230</cell><cell>0.5220</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Impact of neighbour size (#N).</figDesc><table><row><cell></cell><cell cols="3">GraphFormers</cell><cell></cell><cell>PLM+Max</cell><cell></cell></row><row><cell>#N</cell><cell>P@1</cell><cell>NDCG</cell><cell>MRR</cell><cell>P@1</cell><cell>NDCG</cell><cell>MRR</cell></row><row><cell>1</cell><cell>0.6485</cell><cell>0.8087</cell><cell>0.7522</cell><cell>0.6249</cell><cell>0.7946</cell><cell>0.7342</cell></row><row><cell>2</cell><cell>0.6841</cell><cell>0.8308</cell><cell>0.7804</cell><cell>0.6538</cell><cell>0.8137</cell><cell>0.7583</cell></row><row><cell>3</cell><cell>0.6980</cell><cell>0.8396</cell><cell>0.7916</cell><cell>0.6728</cell><cell>0.8256</cell><cell>0.7734</cell></row><row><cell>4</cell><cell>0.7126</cell><cell>0.8485</cell><cell>0.8029</cell><cell>0.6823</cell><cell>0.8319</cell><cell>0.7814</cell></row><row><cell>5</cell><cell>0.7267</cell><cell>0.8565</cell><cell>0.8133</cell><cell>0.6934</cell><cell>0.8386</cell><cell>0.7900</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Time and memory costs per mini-batch for PLM+Max and GraphFormers, with neighbour size increased from 3 to 200. GraphFormers achieve similar efficiency and scalability as PLM+Max.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Product</cell><cell></cell><cell></cell><cell>DBLP</cell><cell></cell><cell></cell><cell></cell><cell>Wiki</cell></row><row><cell>Methods</cell><cell></cell><cell>P@1</cell><cell>NDCG</cell><cell>MRR</cell><cell>P@1</cell><cell>NDCG</cell><cell>MRR</cell><cell>P@1</cell><cell></cell><cell>NDCG</cell><cell>MRR</cell></row><row><cell>GraphFormers</cell><cell></cell><cell>0.7786</cell><cell>0.8793</cell><cell>0.8430</cell><cell>0.7267</cell><cell>0.8565</cell><cell>0.8133</cell><cell>0.3952</cell><cell></cell><cell>0.6230</cell><cell>0.5220</cell></row><row><cell>PLM+Max</cell><cell></cell><cell>0.7570</cell><cell>0.8678</cell><cell>0.8280</cell><cell>0.6934</cell><cell>0.8386</cell><cell>0.7900</cell><cell>0.3712</cell><cell></cell><cell>0.6071</cell><cell>0.5022</cell></row><row><cell>-Progressive</cell><cell></cell><cell>0.7688</cell><cell>0.8751</cell><cell>0.8373</cell><cell>0.7096</cell><cell>0.8468</cell><cell>0.8007</cell><cell>0.3834</cell><cell></cell><cell>0.6155</cell><cell>0.5127</cell></row><row><cell>-Simplified</cell><cell cols="2">0.7795 ↑</cell><cell>0.8798 ↑</cell><cell>0.8436 ↑</cell><cell>0.7225</cell><cell>0.8542</cell><cell>0.8102</cell><cell>0.3923</cell><cell></cell><cell>0.6209</cell><cell>0.5195</cell></row><row><cell>-Shared GNNs</cell><cell></cell><cell>0.7788</cell><cell>0.8795</cell><cell>0.8433</cell><cell>0.7256</cell><cell>0.8558</cell><cell>0.8123</cell><cell cols="2">0.3945 ↓</cell><cell>0.6221 ↓</cell><cell>0.5211 ↓</cell></row><row><cell>-Position</cell><cell></cell><cell>0.7788</cell><cell>0.8795</cell><cell>0.8434</cell><cell>0.7276 ↑</cell><cell>0.8570 ↑</cell><cell>0.8139 ↑</cell><cell>0.3942</cell><cell></cell><cell>0.6222</cell><cell>0.5211</cell></row><row><cell>#N</cell><cell></cell><cell>3</cell><cell></cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>50</cell><cell></cell><cell>100</cell><cell>200</cell></row><row><cell>Time: PLM+Max</cell><cell></cell><cell cols="2">60.29 ms</cell><cell>93.41 ms</cell><cell>161.40 ms</cell><cell>295.92 ms</cell><cell cols="2">684.16 ms</cell><cell cols="2">1357.93 ms</cell><cell>2706.35 ms</cell></row><row><cell cols="2">Time: GraphFormers</cell><cell cols="2">63.95 ms</cell><cell>97.19 ms</cell><cell>170.16 ms</cell><cell>306.12 ms</cell><cell cols="2">714.32 ms</cell><cell cols="2">1411.09 ms</cell><cell>2801.67 ms</cell></row><row><cell>Mem: PLM+Max</cell><cell></cell><cell cols="2">1.33 GiB</cell><cell>1.39 GiB</cell><cell>1.55 GiB</cell><cell>1.82 GiB</cell><cell cols="2">2.67 GiB</cell><cell cols="2">4.09 GiB</cell><cell>6.92 GiB</cell></row><row><cell cols="2">Mem: GraphFormers</cell><cell cols="2">1.33 GiB</cell><cell>1.39 GiB</cell><cell>1.55 GiB</cell><cell>1.83 GiB</cell><cell cols="2">2.70 GiB</cell><cell cols="2">4.28 GiB</cell><cell>7.33 GiB</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">The encoding results can be kept in low-cost devices, whose storage capacity can be regarded as infinite.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">We remove the naive cases where q and k are included by each other's neighbour set, Nq and N k .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">We use the common MLM strategy, where 15% of the input tokens are masked: 80% of them are replaced by[MASK], the rest ones are replaced randomly or kept as the original tokens with the same probabilities.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">https://originalstatic.aminer.cn/misc/dblp.v12.7z</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4">https://deepgraphlearning.github.io/project/wikidata5m</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5">An enhanced BERT-like PLM showing more competitive performances than peers like RoBERTa, XLNet.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6">https://support.google.com/adsense/answer/190515?hl=en</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7">https://support.google.com/google-ads/answer/116495?hl=en</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unilmv2: Pseudo-masked language models for unified language model pre-training</title>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songhao</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="642" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérôme</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
				<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
				<meeting><address><addrLine>Bert</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingcheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08821</idno>
		<title level="m">Simcse: Simple contrastive learning of sentence embeddings</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017a</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<title level="m">Representation learning on graphs: Methods and applications</title>
				<imprint>
			<date type="published" when="2017">2017b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gpt-gnn: Generative pre-training of graph neural networks</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1857" to="1867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04906</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adsgnn: Behavior-graph augmented relevance modeling in sponsored search</title>
		<author>
			<persName><forename type="first">Chaozhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bochen</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanling</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.12080</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fine-grained fact verification with kernel graph attention network</title>
		<author>
			<persName><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09796</idno>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00181</idno>
		<title level="m">Sparse, dense, and attentional representations for text retrieval</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.4546</idno>
		<title level="m">Distributed representations of words and phrases and their compositionality</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="1532" to="1543" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Zero: Memory optimizations toward training trillion parameter models</title>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC20: International Conference for High Performance Computing, Networking, Storage and Analysis</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
	<note>Olatunji Ruwase, and Yuxiong He</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<title level="m">Sentence-bert: Sentence embeddings using siamese bertnetworks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Whitening sentence representations for better semantics and faster retrieval</title>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiarun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyiwen</forename><surname>Ou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15316</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph attention networks. International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Text classification with heterogeneous information network kernels</title>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016a</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Billionscale commodity embedding for e-commerce recommendation in alibaba</title>
		<author>
			<persName><forename type="first">Jizhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pipei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dik</forename><surname>Lun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="839" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Linked document embedding for classification</title>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charu</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international on conference on information and knowledge management</title>
				<meeting>the 25th ACM international on conference on information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2016">2016b</date>
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13456</idno>
		<title level="m">Improving textual network learning with variational homophilic embeddings</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Kepler: A unified model for knowledge embedding and pre-trained language representation</title>
		<author>
			<persName><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.06136</idno>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks?</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Zenan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinliang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.11057</idno>
		<title level="m">A deep neural information fusion architecture for textual network embeddings</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Network representation learning with rich text information</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page" from="2111" to="2117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Graph-based neural multi-document summarization</title>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kshitijh</forename><surname>Meelu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Pareek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnan</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06681</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Graph neural networks: A review of methods and applications. AI Open</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="57" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Gear: Graph-based evidence aggregating and reasoning for fact verification</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01843</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanling</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Pelger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huasha</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06323</idno>
		<title level="m">Textgnn: Improving text encoder via graph neural network in sponsored search</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
