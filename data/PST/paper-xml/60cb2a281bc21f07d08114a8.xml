<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Flow-guided One-shot Talking Face Generation with a High-resolution Audio-visual Dataset</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhimeng</forename><surname>Zhang</surname></persName>
							<email>zhangzhimeng@corp.netease.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Changjie Fan Virtual Human Group</orgName>
								<orgName type="institution">Netease Fuxi AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lincheng</forename><surname>Li</surname></persName>
							<email>lilincheng@corp.netease.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Changjie Fan Virtual Human Group</orgName>
								<orgName type="institution">Netease Fuxi AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Ding</surname></persName>
							<email>dingyu01@corp.netease.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Changjie Fan Virtual Human Group</orgName>
								<orgName type="institution">Netease Fuxi AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Flow-guided One-shot Talking Face Generation with a High-resolution Audio-visual Dataset</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>One-shot talking face generation should synthesize high visual quality facial videos with reasonable animations of expression and head pose, and just utilize arbitrary driving audio and arbitrary single face image as the source. Current works fail to generate over 256×256 resolution realistic-looking videos due to the lack of an appropriate high-resolution audio-visual dataset, and the limitation of the sparse facial landmarks in providing poor expression details. To synthesize high-definition videos, we build a large in-the-wild high-resolution audio-visual dataset and propose a novel flow-guided talking face generation framework. The new dataset is collected from youtube and consists of about 16 hours 720P or 1080P videos. We leverage the facial 3D morphable model (3DMM) to split the framework into two cascaded modules instead of learning a direct mapping from audio to video. In the first module, we</head><p>propose a novel animation generator to produce the movements of mouth, eyebrow and head pose simultaneously. In the second module, we transform animation into dense flow to provide more expression details and carefully design a novel flow-guided video generator to synthesize videos. Our method is able to produce high-definition videos and outperforms state-of-the-art works in objective and subjective comparisons * .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Given one reference facial image and one driving audio, one-shot talking face generation aims at synthesizing a talking avatar video with reasonable facial animations corresponding to the driving audio. Talking face generation is of importance for many applications, including virtual assistants, mixed realities, animation movies, and so forth. Due to its wide applications, talking face generation draws con- siderable attention for a long time. While many works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b3">4]</ref> make great efforts to synthesize realistic-looking videos, the generation of high-resolution videos is still a challenge. Current best work <ref type="bibr" target="#b48">[49]</ref> just generate videos with 256×256 resolution(see Figure10(e) for example), however, directly employing their model on 512×512 image will get blurry results (see Figure10(f) for example). Several factors result in this challenge.</p><p>The first reason is that there are no appropriate datasets for high-resolution talking face generation. Table <ref type="table" target="#tab_0">1</ref> illustrates some common audio-visual datasets (all available datasets are listed in <ref type="bibr" target="#b2">[3]</ref>). As shown in Table <ref type="table" target="#tab_0">1</ref>, current audio-visual datasets consist of in-the-wild datasets and in-the-lab datasets. In-the-wild datasets contain larger scale and more subjects, but they all lack video resolution. There are two main reasons: On one hand, their videos are collected from the internet published in the past 2∼5 years, and at that time the internet videos generally have low resolution. On the other hand, most in-the-wild datasets do not focus on the task of talking face generation, e.g., Voxceleb <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b7">8]</ref> is built for speaker identification and LRW <ref type="bibr" target="#b8">[9]</ref> is built for word recognition, so they do not pay attention to the video resolution. For in-the-lab datasets, while they record high resolution face videos, the number of subjects and sentences is limited because of the expensive labor costs. The largest MEAD <ref type="bibr" target="#b42">[43]</ref> only records 159 sentences with 60 actors.</p><p>The second reason is that previous works are not de- signed reasonably to handle high-resolution videos and are limited by the input of sparse facial landmarks. Initial works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b41">42]</ref> directly utilize an end-to-end framework to synthesize the video from audio. Their synthetic results even have a low definition on 128×128 videos. Other recent advances <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b3">4]</ref> leverage facial landmarks to split the pipeline into two cascaded modules. They produce sparse facial landmarks in the first module, and further generate videos from synthetic landmarks in the second module. Two modules are trained separately to alleviate the pressure of the network, thus lead to high visual quality results. However, in the second module, their methods are still hard to generate high resolution videos. We carefully discuss the reasons in Section 7. On one hand, some works directly utilize the network to learn the sophisticated mapping from landmark to image. This mapping become too complex to handle on high-resolution videos, e.g., <ref type="bibr" target="#b48">[49]</ref> synthesize blurry results on 512×512 resolution(see Fig- <ref type="figure" target="#fig_11">ure10</ref>(g) and Figure11(a) for example). On the other hand, although some works carefully design their network to explicitly model the process of image synthesis, the sparse landmark is too coarse and lose many facial expression details, e.g., <ref type="bibr" target="#b33">[34]</ref> synthesize facial image with inaccuracy mouth shape and poor wrinkles(see Figure11(c) for example).</p><p>In order to achieve above challenge and promote the development of high-resolution talking face generation, we first build a large in-the-wild high-resolution audiovisual dataset, named High-definition Talking Face Dataset (HDTF). The HDTF dataset is collected from youtube website published in recent two years and consists of about 16 hours 720P∼1080P videos. There are over 300 subjects and 10k different sentences in HDTF dataset. Our HDTF dataset has higher video resolution than previous in-thewild datasets and more subjects/sentences than in-the-lab datasets.</p><p>Next, we propose a novel flow-guided framework to synthesize high visual quality videos. Figure <ref type="figure" target="#fig_1">2</ref> illustrates the pipeline of our method. Our work first leverages 3DMM <ref type="bibr" target="#b0">[1]</ref> to split the framework into two cascaded modules, named audio-to-animation module and animation-to-video module. Compared with the facial landmarks, 3DMM is insensitive to noise due to the prior knowledge of the face. In audio-to-animation module, 3DMM is used to decouple the face into facial animation parameters (mouth, eyebrow and head pose) and we propose a novel style-specific animation generator to produce the full animation parameters with multi-task learning strategy. Our generator considers the difference of speaking style between different identity <ref type="bibr" target="#b49">[50]</ref>, and has capacity to synthesize subject-dependent animations. In animation-to-video module, we propose a flowguided framework to synthesize high visual quality videos. Our method utilizes 3DMM to transform animation parameters to dense flow. Dense flow has benefits of providing richer facial details than sparse landmarks. Then, a novel video generator is proposed to synthesize talking face videos from dense flow. Our generator is carefully designed to explicitly control the process of frame generation, so it is easy to generate more realistic results.</p><p>Our contributions are summarized as follows:</p><p>• We build a large in-the-wild audio-visual dataset, with higher video resolution than previous in-the-wild datasets and more subjects/sentences than in-the-lab datasets. • We propose a novel style-specific animation generator to produce specific style animation parameters depending on the reference identity. • To the best of our knowledge, we are the first to utilize one animation generator with multi-task learning to produce the animation parameters of mouth, eyebrow and head pose simultaneously in one-shot talking face generation. • We propose a novel carefully-designed flow-guided framework to synthesize higher visual quality videos than previous landmark-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Talking Face Generation</head><p>One-shot talking face generation. One-shot talking face generation is identity-independent. In the inference stage, the reference identity and driving audio are not restricted to appear in training data. Early works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b41">42]</ref> always take two sub-encoders to extract identity features and spoken features from the reference image and driving audio. Then, they fuse two features as input into a decoder to synthesize talking face videos in an endto-end fashion. For more accurate lip-sync results, some works use the audio-mouth mutual information loss <ref type="bibr" target="#b50">[51]</ref>, audio-mouth correlation loss <ref type="bibr" target="#b4">[5]</ref> and audio-visual disentangle learning <ref type="bibr" target="#b47">[48]</ref>. In order to improve the visual quality, some works add an extra deblurring module <ref type="bibr" target="#b6">[7]</ref> or just repair the mouth region <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>Recent advances <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b48">49]</ref> utilize facial landmarks to split the framework into two cascaded modules. In the first module, PCA component <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b3">4]</ref> or spatial displacement <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b48">49]</ref> of the landmark are used to represent facial animation parameters. <ref type="bibr" target="#b3">[4]</ref> take two networks to synthesize the facial expression and head motion. <ref type="bibr" target="#b48">[49]</ref> utilize two branches to synthesize mouth displacement and head pose/eyebrow displacement. However, both above two works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b48">49]</ref> separately train the two animation generators. In our work, we use one animation generator to synthesize mouth, eyebrow and head pose simultaneously with multi-task learning. In the second module, they employ various landmark-to-video generators to synthesize talking head videos. Different from them, our method takes dense flow as input to generate more realistic videos.</p><p>Person-specific talking face generation.</p><p>Personspecific talking face generation has benefits of synthesizing high-resolution talking face videos because the identity is in training data. <ref type="bibr" target="#b35">[36]</ref> carefully design a framework to synthesize Obama videos with about 17 hours footage. <ref type="bibr" target="#b15">[16]</ref> utilize dynamic programming algorithm to reduce the training data to 1 hour. <ref type="bibr" target="#b34">[35]</ref> train a shared generator for all identities and they only require 15 minutes footage. <ref type="bibr" target="#b38">[39]</ref> leverage a pre-trained audio-to-mouth model to reduce the required footage to 2∼3 minutes. <ref type="bibr" target="#b22">[23]</ref> use a motion capture dataset to synthesize videos with emotion and rhythmic head pose. In our work, we synthesize videos with competitive resolution and only need one reference image for a new subject.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Animation Synthesis</head><p>Animation synthesis aims at generating animation trajectories to drive a pre-defined 3D talking avatar. In mouth animation generation, the mouth shape is related to spoken co-articulation <ref type="bibr" target="#b37">[38]</ref>. Several works use CNNbased <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b11">12]</ref> or LSTM-based <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b29">30]</ref> framework to capture co-articulation effects. Some works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b29">30]</ref> focus on expressive animation generation. Other works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b11">12]</ref> focus on improving the generalization of input speech. In head pose/eyebrow animation generation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14]</ref>, there is a one-to-many mapping between speech and head pose/eyebrow <ref type="bibr" target="#b30">[31]</ref>, so <ref type="bibr" target="#b30">[31]</ref> utilize Generative Adversarial Network(GAN) <ref type="bibr" target="#b16">[17]</ref> to retain the diversity of head pose. Besides, head pose/eyebrow animation is related to speech prosody and syntactic structure <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14]</ref>, so <ref type="bibr" target="#b48">[49]</ref> take selfattention module <ref type="bibr" target="#b40">[41]</ref> to capture this long-time dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset</head><p>A large in-the-wild high resolution audio-visual dataset, named High-definition Talking Face Dataset (HDTF), is built for talking face generation. Some snapshots of HDTF are shown in Figure <ref type="figure" target="#fig_2">3</ref>. In order to collect high quality videos, we only collect online videos published in recent two years. HDTF dataset consists of about 362 different videos for 15.8 hours. The resolution of origin video is 720P or 1080P. In our work, a landmark detector is first leveraged to crop the face region. The crop window is fixed during each video. Then, each cropped video is resized into 512 × 512 (the second row in Figure3). Due to the high quality of origin videos, our final cropped videos also have high visual quality.</p><p>Then, the 3DMM <ref type="bibr" target="#b0">[1]</ref> is employed to decouple the cropped face into facial shape parameters and facial animation parameters(mouth, eyebrow and head pose). The 3DMM is a bilinear morphable model. It is represented as</p><formula xml:id="formula_0">M (c s , c e ) = M 0 + 60 i=1 c s i • V s i + 33 j=1 c e j • V e j<label>(1)</label></formula><p>where M (c s , c e ) represent the 3D facial mesh point. M 0 is the average facial mesh. {V s i } 60 i=1 and {V e j } 33 j=1 are the linear basis of facial shape and facial expression. c s and c e represent the coefficient of the basis. {V e j } 33 j=1 is combined with 28 mouth basis and 5 eyebrow basis.</p><p>We take scaled orthogonal projection <ref type="bibr" target="#b18">[19]</ref> to reconstruct 3D face according to facial landmark points, e.g., dlib. The objective is arg min</p><formula xml:id="formula_1">c s ,c e ,s,R,t E(c s , c e , s, R, t) = arg min c s ,c e ,s,R,t K k=1 δ k [p k − (s 1 0 0 0 1 0 RM (c s , c e ) (k) + t)] 2<label>(2)</label></formula><p>where p k is the k th landmark point and δ k represent its weight. K is the number of landmark points. R ∈ SO(3) is rotation matrix and t ∈ R 2 represents translation vector. s is the scale value. In our paper, we solve above objective with weighted least squares method.</p><p>After the 3D face restruction, in each video, we extract In our method, we do not directly synthesize head pose but synthesize the difference. The main reason is that the initial head pose in different videos are different. We also extract audio feature sequence</p><formula xml:id="formula_2">f audio = {f audio t ∈ R 15 } T t=1 .</formula><p>The audio feature consists of 13-dim MFCC feature and 2-dim pitch feature. The video frames is denoted as</p><formula xml:id="formula_3">I = {I t } T t=1 .</formula><p>T is the length of frames in the video. Finally, our training data is represented as {I, p mou , p ebro , p hed , f audio , p s } in each video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Method</head><p>Based on our HDTF dataset, as shown in Figure2, we propose a novel high-quality one-shot talking face generation framework. The framework consists of one audio-toanimation module and one animation-to-video module. In the first module, a novel style-specific audio-to-animation generator G ani is designed to translate reference image and driving audio to full animation parameters. In the second module, animation parameters are first transformed to approximate dense flow F app by the 3DMM. Then, F app and reference image are input into a careful-designed flowguided video generator G vid to synthesize the talking face videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Audio-to-animation</head><p>Style-specific audio-to-animation generator G ani . The structure of G ani is illustrated in In the first step, a CNN-based audio feature extractor is first employed to extract audio representation f audio from f audio . Then, considering that different identity has dif-  ferent speaking style <ref type="bibr" target="#b49">[50]</ref>, AdaIN <ref type="bibr" target="#b19">[20]</ref> operation is taken to transform f audio into the f audio ref .</p><p>In specifically, a pretrained VGG-face model <ref type="bibr" target="#b1">[2]</ref> is first used to extract identity embedding vector from the reference image I ref . Then, the identity embedding is input into fully-connected layers to generate the scale and shift parameters of the AdaIN.</p><p>Furthermore, in the second step, with three branches of the decoder, mouth, eyebrow and head pose are generated simultaneously according to f audio ref .</p><p>In mouth branch, a CNN-based mouth decoder is employed to decode f audio ref to pmou . In eyebrow and head pose branch, a long-time temporal decoder is first employed to capture the long-time dependencies. Different from <ref type="bibr" target="#b48">[49]</ref>, our long-time decoder is based on an encoder-decoder network, which has benefits of faster forward speed. Then, a CNN-based eyebrow decoder and a CNN-based head pose decoder are taken to synthesize pebro and phed .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss function.</head><p>In training stage, G ani is trained with multi-task learning strategy. In mouth synthesis, we use L1 loss and LSGAN loss <ref type="bibr" target="#b24">[25]</ref>.L1 loss is written as</p><formula xml:id="formula_4">L mou 1 = 1 T T t=1 p mou t − pmou t 1 ,<label>(3)</label></formula><p>where p mou t and pmou t are the real and synthetic mouth parameters. LSGAN loss is denoted as</p><formula xml:id="formula_5">L mou GAN = min G ani max D mou L GAN (G ani , D mou ).<label>(4)</label></formula><p>In eyebrow and head pose generation, we utilize Structural Similarity (SSIM) loss <ref type="bibr" target="#b46">[47]</ref> and LSGAN loss. SSIM simulates the human visual perception and has benefits of extracting structural information. In our work, SSIM extends to evaluate the eyebrow and head pose on each parameter. SSIM loss in eyebrow generation is written as</p><formula xml:id="formula_6">L ebro ssim = 1 − 1 5 5 i=1 (2µi μi + δ1)(2covi + δ2)) (µ 2 i + μ2 i + δ1)(σ 2 i + σ2 i + δ2)) ,<label>(5)</label></formula><p>where µ i / μi and σ i / σi are the mean and standard deviation of the i th dimension of p ebro /p ebro . cov i is the covariance. δ 1 and δ 2 are two small constants. LSGAN loss in eyebrow generation is denoted as</p><formula xml:id="formula_7">L ebro GAN = min G ani max D ebro L GAN (G ani , D ebro ).<label>(6)</label></formula><p>The loss in head pose generation has the same form (SSIM &amp; GAN) as in eyebrow generation except for the parameter dimension. The final objective function is written as</p><formula xml:id="formula_8">L(G ani ) = L mou GAN + L ebro GAN + L hed GAN + λ mou L mou 1 + λ ebro L ebro ssim + λ hed L hed ssim ,<label>(7)</label></formula><p>λ mou , λ ebro and λ hed represent the loss weights. All the GAN structures are conditional GAN, i.e., D mou/ebro/hed takes {f audio , pmou/ebro/hed } as input. The structure details are in supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Animation-to-video</head><p>In animation-to-video module, the animation parameters are first transformed to approximate dense motion flow F app by 3DMM. However, limited by the ability of 3DMM, F app is not accurate enough. Then, to solve above problem, a novel flow-guided video generator G vid is proposed. G vid is carefully designed to revise F app and synthesize high visual quality videos.</p><p>Approximate dense motion flow F app . F app describes the approximate motion direction of each pixel between two frames. Figure <ref type="figure" target="#fig_6">5</ref> (a) visualizes the F app in pseudo color. In the generation of F app , given a pair of facial animation parameters, 3DMM is able to generate accurate dense motion flow in the inner face (the green part in Figure <ref type="figure" target="#fig_6">5 (b)</ref>). However, 3DMM is incapable of describing the motion out of the face region(the blue and orange part in Figure <ref type="figure" target="#fig_6">5 (b)</ref>). In order to solve this problem, we estimate the approximate motion flow out the facial region.</p><p>As shown in Figure <ref type="figure" target="#fig_6">5</ref> (b), we crop the facial image into three parts: the inner face part (green), the upper torso part (orange) and the head-related part (blue). In the inner face part, the dense motion flow is computed from 3DMM. In the upper torso part, we assume the upper torso moves with the head, so we take the average movements of inner face as the motion value in upper torso part. In the head-related part, we focus on the hair, ear and other ornaments, and assume they move rigidly follow the nearest facial edge. The flow of each pixel in head-related part is as same as its nearest facial edge pixel. Combining the flow of three parts, we obtain the final F app . However, the background is ignored in the construction of F app , so the flow value in background is absolutely incorrect. This incorrectness will be revised in G vid .</p><p>Flow-guided video generator G vid .The structure of G vid is shown in Figure <ref type="figure" target="#fig_7">6</ref>. G vid is designed to revise F app and further synthesize high-resolution talking face videos. In order to realize above purpose, G vid also contains two steps inside the network. In the first step (purple part in Figure <ref type="figure" target="#fig_7">6</ref>), the network revises the F app , and produces an accurate dense motion flow F , an intermediate matting image g and a matting mask M m . In the second step (orange part in Figure <ref type="figure" target="#fig_7">6</ref>), F , g and M m are used to synthesize high quality videos.</p><p>In the first step(purple part in Figure <ref type="figure" target="#fig_7">6</ref>), to revise the incorrect flow in background of F app , we assume that the background is static, which is also used in many recent works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>. Upon this assumption, a foreground mask M f is generated to transform F app to accurate dense motion flow F . M f is a soft mask with a range 0 ∼ 1. The transformation is written as F = F app * M f (8) M f revises the background to static. In order to generate M f , inspired from <ref type="bibr" target="#b33">[34]</ref>, we first warp reference image I ref with F app to get warped image I ref warp . Then, I ref , F app and I ref warp are concatenated into a Hourglass network <ref type="bibr" target="#b27">[28]</ref> to generate M f . Besides, Hourglass network also outputs g and M m for the second step.</p><p>In the second step(orange part in Figure <ref type="figure" target="#fig_7">6</ref>), inspired from <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b43">44]</ref>, we synthesize the image by combining the warped version of I ref and g. The combination is balanced by a matting mask M m . To reduce the parameters of the network, inspired from <ref type="bibr" target="#b33">[34]</ref>, above combination is done in feature map space, and is written as LSGAN loss, perceptual loss <ref type="bibr" target="#b20">[21]</ref> and feature matching loss <ref type="bibr" target="#b45">[46]</ref>. The GAN loss is represented as</p><formula xml:id="formula_9">f ref = F (f ref ) * M m + g * (1 − M m )<label>(</label></formula><formula xml:id="formula_10">L vid GAN = min G vid max D vid L GAN (G vid , D vid ). (<label>10</label></formula><formula xml:id="formula_11">)</formula><p>The perceptual loss is written as</p><formula xml:id="formula_12">L vid perc = n i=1 1 W i H i C i N i (I t ) − N i ( Ît ) 1 ,<label>(11)</label></formula><p>where N i (.) denotes the i th layer with W i * H i * C i elements of a specific VGG-19 network. The feature matching loss is written as</p><formula xml:id="formula_13">L vid F M = m j=1 1 W j H j C j D vid j (I t ) − D vid j ( Ît ) 1 , (<label>12</label></formula><formula xml:id="formula_14">)</formula><p>where D vid j (.) is the j th layer in D vid . The final loss function of G vid is written as</p><formula xml:id="formula_15">L(G vid ) = L vid GAN + λ perc L vid perc + λ F M L vid F M . (<label>13</label></formula><p>) λ perc and λ F M are the weights of loss. The structure details of G vid and D vid are in supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and Results</head><p>In this section, we first display some synthetic results of our method. Then, we compare our method with stateof-the-art talking face generation works. Next, to validate the effectiveness of each sub-module, we also do quantitative and qualitative comparisons with other related works. Next, we do ablation study to evaluate the components in two sub-modules. Finally, an online user study is conducted to validate our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Synthetic results</head><p>Figure <ref type="figure" target="#fig_8">7</ref> shows some high-resolution synthetic frames driven by the same audio. Our method synthesizes high visual quality results. We further draw the curve of animation parameters of three different identities driven by the same audio in Figure <ref type="figure" target="#fig_9">8</ref>. Figure <ref type="figure" target="#fig_9">8</ref>(a) draws the sequence of mouth parameter controlling the opening of mouth. While there exist slight temporal shift and slight scale variance on mouth parameters, the tendency of the sequence is still similar on different subjects. It implies that the mouth shape mainly depends on the speech content. Figure <ref type="figure" target="#fig_9">8 (b-d</ref>) draw the eyebrow parameter (eyebrow down), head rotation (roll) and head translation (horizontal) respectively. Obviously, there has more variance in these parameters. It demonstrates that our G ani has ability to synthesize identitydependent speaking styles for different reference subjects. We also visualize the intermediate results, including F app , I ref warp , M f , F , M m and synthetic frame, of the animation-to-video module in Figure9. The M f focuses on separating the moving foreground and static background. The M m leads to the matting operation impacts on the foreground. With the joint action of M f and M m , our G vid synthesizes high visual quality videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with State-of-the-art</head><p>We compare our method with state-of-the-art one-shot talking face generation works <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b28">29]</ref> in Figure <ref type="figure" target="#fig_11">10</ref>. Vougioukas et al. <ref type="bibr" target="#b41">[42]</ref>(Figure <ref type="figure" target="#fig_11">10</ref>   Compared with previous works, our method synthesizes higher visual quality results. We also carry out quantitative comparisons with state-of-the-art works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b28">29]</ref> to evaluate the accuracy of lip synchronization. The experiments are conducted on HDTF dataset with the metric of audio-visual synchronization [10]  † . Table <ref type="table" target="#tab_1">2</ref> illustrate the experimental results. Our method synthesizes competitive synchronous lip compared with previous works. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Evaluation of Submodules</head><p>To validate our audio-to-animation module, we reproduce state-of-the-art animation generation works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b30">31]</ref>. For fair comparison, we keep the input and structure setting of their model unchanged and synthesize our animation parameters. To evaluate p mou , we measure MSE on mouth parameters and compute lips landmark distance (LMD 3D ) on 3D facial mesh. LMD 3D has benefits of handling the variance of head posture. In the evaluation of p ebro and p hed , we employ SSIM and Canonical Correlation Analysis(CCA) <ref type="bibr" target="#b39">[40]</ref> as metrics respectively. Quantitative results are shown in Table <ref type="table" target="#tab_2">3</ref>. Our method performs better than the above works.</p><p>To validate the superiority of our animation-to-video module on high-resolution one-shot talking video generation. We reproduce previous landmark based works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b48">49]</ref> on our HDTF dataset, and do quantitative and qualitative comparisons with them. In <ref type="bibr" target="#b48">[49]</ref>, all setting is as same as original paper. In <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b32">33]</ref> we replace key points with facial landmarks and ignore the affine transformation in their work. However, considering that above frameworks are designed for 256×256 resolution videos, to make the experiments more convincing, we also conduct extra experiments that make their framework easy to handle 512×512 videos. We add one extra convolutional layer with stride=2 before their network to downsample the input image to 256×256. Figure <ref type="figure" target="#fig_12">11(a-d,g</ref>) illustrate the qualitative results. Our approach synthesizes frames with higher visual quality. Table <ref type="table" target="#tab_3">4</ref> also show the quantitative compared results. PSNR, SSIM and CPBD <ref type="bibr" target="#b26">[27]</ref> are utilized as metrics to measure the visual quality. Our approach also acquires the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Experiments</head><p>Ablation Experiments are conducted to evaluate each component in two sub-modules. In audio-to-animation module, we set two conditions: (1) removing the style-  (2) synthesizing animation parameters separately (w/o multi-task). Table <ref type="table" target="#tab_2">3</ref> illustrates the results of two conditions. Both style-specific operation and multi-task training strategy are beneficial to animation generation and our full model synthesizes the best animation results. The style-specific operation significantly improves the synthetic animation. It implies that it is important to consider the speaking style of different identities in animation generation.</p><p>In animation-to-video module, we also set two conditions: (1) removing F app and generating F from dense flow in inner face with one network (w/o F app ). This condition discard the assumption of motion flow out of the face; (2) removing the matting operation (w/o matting), thus lead to equation 9 as</p><formula xml:id="formula_16">f ref = F (f ref ).<label>(14)</label></formula><p>Table <ref type="table" target="#tab_3">4</ref> shows the quantitative results of two conditions. Our full model presents the best results. Figure <ref type="figure" target="#fig_12">11</ref> (e)(f) also illustrate the synthetic results of two conditions. Without F app , the network is possible to generate inaccurate dense motion flow out of facial region, thus leads to blurry results, e.g., the hair region in figure <ref type="figure" target="#fig_12">11(e</ref>). Without matting operation, as shown in figure <ref type="figure" target="#fig_12">11</ref>(f), the facial region lose some texture details. This indicates that the matting operation is capability to refine the foreground.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">User Study</head><p>An online user study is also conducted to validate our proposed approach. We compare our method with previous state-of-the-art one-shot talking face generation works <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b28">29]</ref>. For fair comparison, 5 reference images are download from internet to obtain 4×5 = 20 videos with 5 different driving audio. 25 volunteers are invited to rate the realism of each video between 1(pretty fake)-5(pretty real). Table <ref type="table" target="#tab_4">5</ref> illustrates the results of user study. Our method achieves the highest scores and lowest standard deviation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Limitations</head><p>Our work has many limitations. In the generation of F app , the cropped region is very coarse. The synthetic videos largely depend on the M f Inaccurate M f causes the failure results. Our method does not consider the temporally coherent, so if given one reference image with mouth close, the generated face may has flicker tooth. The stylespecific operation in animation generator is still hard to synthesize the speaking style as same as real value. We only utilize rule-based method to generate the eye blink movements. The head pose is not extreme enough.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion and Conclusion</head><p>Discussion. We utilize control variate method to exploring the reason that our flow-guided animation-to-video module performs better than previous landmark-to-video module <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b48">49]</ref> on high-resolution video generation. We set two conditions: (1)whether to carefully design the network to explicitly model the process of image synthesis; (2) whether to take dense flow as the network input. Figure12 shows the experimental results. Compared with Figure12 (a) and (c), both <ref type="bibr" target="#b48">[49]</ref> and <ref type="bibr" target="#b33">[34]</ref> take facial landmark as input, <ref type="bibr" target="#b33">[34]</ref> utilize explicitly modeling in their network to synthesize more realistic results. Compared with Figure12 (a) and (b), we just replace the landmark input with F app in <ref type="bibr" target="#b48">[49]</ref>, and generate more realistic frames, especially the richer texture and accurate expression in inner face. Compared with Figure12 (c) and (d), fixing the explicitly modeling in network, our method takes dense flow as input and also generate facial image with richer wrinkles. The experiments indicate that both two conditions are beneficial to improve the visual quality of synthetic videos. Table <ref type="table" target="#tab_2">3</ref> also illustrate the quantitative results with consistent conclusion. Our framework consists of above two conditions, so the results are more realistic.</p><p>Conclusion. In this paper, we build a large in-the-wild high-resolution audio-visual dataset, named HDTF dataset, with higher resolution than previous in-the-wild datasets and more subjects/sentences than in-the-lab datasets. We also propose a novel flow-guided framework, including one style-specific animation generator and one careful-designed flow-guided video generator, to synthesize high visual quality videos. Our method outperforms the state-of-the-art works in high-resolution talking face generation. In the future, we will make great efforts to solve above limitations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Our method synthesizes high-resolution talking face videos with one driving audio and one reference facial image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The pipeline of our method. Our method has two cascaded modules: audio-to-animation module (purple part) and animation-tovideo module (orange part).</figDesc><graphic url="image-47.png" coords="3,286.13,105.53,56.99,59.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The snapshots of HDTF dataset. the face shape parameter p s ∈ R 60 , mouth parameter sequence p mou = {p mou t ∈ R 28 } T t=1 , eyebrow parameter sequence p ebro = {p ebro t ∈ R 5 } T t=1 and head pose parameter difference sequence p hed = {p hed t ∈ R 5 } T t=1 .In our method, we do not directly synthesize head pose but synthesize the difference. The main reason is that the initial head pose in different videos are different. We also extract audio feature sequence f audio = {f audio</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>G ani aims at translating reference image I ref and driving audio f audio into the style-specific animation parameters corresponding to reference face. The parameters consist of mouth parameter pmou , eyebrow parameter pebro and head pose parameter phed . G ani utilizes two steps to realize this purpose. In the first step (the purple part in Figure4), the stylespecific audio representation f audio ref is computed from I ref and f audio . f audio ref encodes the speaking content of f audio and the speaking style of I ref . In the second step (orange part in Figure 4), f audio ref is used to synthesize animation parameters with specific speaking style.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Structure of audio-to-animation generator G ani .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. (a) Visualization of approximate dense motion flow F app in pseudo color. (b) Different cropped parts in F app , including inner face part (green), head-related part(blue) and upper torso part (orange).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Structure of flow-guided video generator G vid .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Our synthetic results driving by the same audio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Animation parameters of three different subjects driven by the same audio. Different color represent different identity. The ordinate represents the value of parameter in (a)(b), the degree of head rotation in (c) and the pixel of head translation in (d).</figDesc><graphic url="image-147.png" coords="6,322.68,334.33,87.81,55.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Visualization of intermediate results in animation-tovideo module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. comparison with state-of-the-art works. 6] (Figure 10(b),(c)) synthesize low-resolution(128×128) talking face videos. The visual quality gap is obvious. Prajwal et al.[29](Figure 10(d)) has ability to synthesize videos with 512×512 resolution, but they just focus on repairing the mouth region. The eyebrow and head pose keep static when just given one reference image. Zhou et al.<ref type="bibr" target="#b48">[49]</ref> is able to synthesize 256×256 resolution videos (in Figure10(e)). However, they fail to generate 512×512 videos. We try to directly test their model on 512×512 reference image (shown in Figure10(f)) or reproduce the model train/test on HDTF dataset (shown in Figure10(g)), but still synthesize blurry results. The reason is carefully discussed in section 7. Compared with previous works, our method synthesizes higher visual quality results.We also carry out quantitative comparisons with state-of-the-art works<ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b28">29]</ref> to evaluate the accuracy of lip synchronization. The experiments are conducted on HDTF dataset with the metric of audio-visual synchronization[10] † . Table2illustrate the experimental results. Our method synthesizes competitive synchronous lip compared with previous works.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Qualitative results of animation-to-video module.</figDesc><graphic url="image-223.png" coords="8,169.74,134.20,53.31,53.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Results of control variate experiments.</figDesc><graphic url="image-222.png" coords="8,228.76,134.12,53.31,53.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Statistics of current common audio-visual datasets.</figDesc><table><row><cell>Dataset name</cell><cell cols="2">Environment Year</cell><cell>Resolution</cell><cell cols="3">Subject Hours sentence</cell></row><row><cell>LRW [9]</cell><cell>Wild</cell><cell>2016</cell><cell>360P∼480P</cell><cell>1k+</cell><cell>173</cell><cell>1k</cell></row><row><cell>Voxceleb1[26]</cell><cell>Wild</cell><cell>2017</cell><cell>360P∼720P</cell><cell>1251</cell><cell>352</cell><cell>100k</cell></row><row><cell>Voxceleb2[8]</cell><cell>Wild</cell><cell>2018</cell><cell>360P∼720P</cell><cell>6112</cell><cell>2442</cell><cell>1128k</cell></row><row><cell>GRID[11]</cell><cell>Lab</cell><cell>2006</cell><cell>720×576</cell><cell>34</cell><cell>27.5</cell><cell>51</cell></row><row><cell>RAVDESS[24]</cell><cell>Lab</cell><cell>2018</cell><cell>1280×1024</cell><cell>24</cell><cell>7</cell><cell>8</cell></row><row><cell>MEAD[43]</cell><cell>Lab</cell><cell>2020</cell><cell>1920×1080</cell><cell>60</cell><cell>40</cell><cell>159</cell></row><row><cell>Our HDTF</cell><cell>Wild</cell><cell cols="2">2020 720P∼1080P</cell><cell>300+</cell><cell>15.8</cell><cell>10k+</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparison of lip synchronization. Lower AV offset and higher AV confidence represents better lip synchronization.</figDesc><table><row><cell>AVOff↓</cell><cell>-1</cell><cell>-2</cell><cell>-2</cell><cell>-2</cell><cell>-2</cell></row><row><cell>AVConf↑</cell><cell>9.627</cell><cell>4.122</cell><cell>5.227</cell><cell>2.770</cell><cell>5.166</cell></row><row><cell>Reference Image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>MethodReal video Chen et al.[6] Prajwal et al.[29] Zhou et al.[49] Ours</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Quantitative evaluation of audio-to-animation module. MSE(mouth)↓ LMD 3D (mouth)↓ SSIM(eyebrow)↑ CCA(head pose)↑</figDesc><table><row><cell>Taylor et al.[37]</cell><cell>0.1237</cell><cell>0.2355</cell><cell>-</cell><cell>-</cell></row><row><cell>Cudeiro et al.[12]</cell><cell>0.1235</cell><cell>0.2350</cell><cell>-</cell><cell>-</cell></row><row><cell>Karras et al.[22]</cell><cell>0.1251</cell><cell>0.2365</cell><cell>0.0801</cell><cell>-</cell></row><row><cell>Sadoughi et al.[30]</cell><cell>0.1347</cell><cell>0.2470</cell><cell>0.0372</cell><cell>-</cell></row><row><cell>Sadoughi et al.[31]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.7615</cell></row><row><cell>Ours (w/o style)</cell><cell>0.1153</cell><cell>0.2308</cell><cell>0.0747</cell><cell>0.7609</cell></row><row><cell>Ours (w/o multi-task)</cell><cell>0.0912</cell><cell>0.1922</cell><cell>0.0978</cell><cell>0.7779</cell></row><row><cell>Ours</cell><cell>0.0875</cell><cell>0.1899</cell><cell>0.1023</cell><cell>0.7860</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Quantitative evaluation of animation-to-video module.</figDesc><table><row><cell></cell><cell cols="2">PSNR↑ SSIM↑ CPBD↑</cell></row><row><cell>Zhou et al.[49]</cell><cell>23.2454 0.8020</cell><cell>0.1226</cell></row><row><cell cols="2">Zhou et al.[49](interpolate to 512) 23.3482 0.8128</cell><cell>0.0936</cell></row><row><cell>Zhou et al.[49](add layer)</cell><cell>22.8777 0.7995</cell><cell>0.1112</cell></row><row><cell>Zhou et al.[49](dense)</cell><cell>24.1604 0.8102</cell><cell>0.1273</cell></row><row><cell cols="2">Zhou et al.[49](dense &amp; add layer) 23.7314 0.8045</cell><cell>0.1209</cell></row><row><cell>Siarohin et al.[34, 33]</cell><cell>23.4079 0.8167</cell><cell>0.1345</cell></row><row><cell cols="2">Siarohin et al.[34, 33] (add layer) 23.1355 0.8062</cell><cell>0.1204</cell></row><row><cell>Ours w/o F app</cell><cell>23.9650 0.8220</cell><cell>0.1399</cell></row><row><cell>Ours w/o matting</cell><cell>24.3691 0.8384</cell><cell>0.1500</cell></row><row><cell>Ours</cell><cell>24.4174 0.8400</cell><cell>0.1530</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>The results of user study.</figDesc><table><row><cell>Mean</cell><cell>2.96</cell><cell>2.88</cell><cell>3.12</cell><cell>3.60</cell></row><row><cell>Std</cell><cell>0.95</cell><cell>1.03</cell><cell>0.95</cell><cell>0.74</cell></row></table><note>Method Chen et al.<ref type="bibr" target="#b5">[6]</ref> Prajwal et al.<ref type="bibr" target="#b28">[29]</ref> Zhou et al.<ref type="bibr" target="#b48">[49]</ref> Ours specific operation (w/o style), i.e., delete the transformation from f audio to f audio ref ;</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">† https://github.com/joonson/syncnet_python</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large scale 3d morphable models</title>
		<author>
			<persName><forename type="first">James</forename><surname>Booth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasios</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><surname>Ponniah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dunaway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vggface2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName><forename type="first">Qiong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omkar</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2018)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What comprises a good talking-head video generation</title>
		<author>
			<persName><forename type="first">Lele</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guofeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03201</idno>
	</analytic>
	<monogr>
		<title level="m">A survey and benchmark</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Talking-head generation with rhythmic head motion</title>
		<author>
			<persName><forename type="first">Lele</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guofeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Celong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08547</idno>
		<imprint>
			<date type="published" when="2007">2020. 1, 2, 3, 6, 7</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lip movements generation at a glance</title>
		<author>
			<persName><forename type="first">Lele</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">K</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyao</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="520" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical cross-modal talking face generation with dynamic pixel-wise loss</title>
		<author>
			<persName><forename type="first">Lele</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">K</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyao</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2019. 1, 2, 6, 7, 8</date>
			<biblScope unit="page" from="7832" to="7841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Son</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Jamaludin</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02966</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Voxceleb2: Deep speaker recognition</title>
		<author>
			<persName><forename type="first">Son</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arsha</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05622</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName><forename type="first">Joon</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="87" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Out of time: automated lip sync in the wild</title>
		<author>
			<persName><forename type="first">Joon</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="251" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An audio-visual corpus for speech perception and auspeech recognition</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2421" to="2424" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Capture, learning, and synthesis of 3d speaking styles</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cudeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cassidy</forename><surname>Laidlaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Speech-driven facial animation using cascaded gans for learning of motion and texture</title>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandika</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjana</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brojeshwar</forename><surname>Bhowmick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="408" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling multimodal behaviors from speech prosody</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Pelachaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Artières</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Virtual Agents</title>
				<imprint>
			<publisher>Springer Berlin Heidelberg</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="217" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Speech-driven eyebrow motion synthesis with contextual markovian models</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Mathieu Radenen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Artières</surname></persName>
		</author>
		<author>
			<persName><surname>Pelachaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3756" to="3760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Text-based editing of talking-head video</title>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Flnet: Landmark driven fetching and learning network for faithful talking facial animation synthesis</title>
		<author>
			<persName><forename type="first">Kuangxiao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10861" to="10868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1501" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Audio-driven facial animation by joint endto-end learning of pose and emotion</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antti</forename><surname>Herva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Write-a-speaker: Textbased emotional and rhythmic talking-head generation</title>
		<author>
			<persName><forename type="first">Lincheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixing</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english</title>
		<author>
			<persName><forename type="first">R</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">A</forename><surname>Livingstone</surname></persName>
		</author>
		<author>
			<persName><surname>Russo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">e0196391</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">Yk</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Voxceleb: a large-scale speaker identification dataset</title>
		<author>
			<persName><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joon</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08612</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A no-reference image blur metric based on the cumulative probability of blur detection (cpbd)</title>
		<author>
			<persName><forename type="first">D</forename><surname>Niranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><forename type="middle">J</forename><surname>Narvekar</surname></persName>
		</author>
		<author>
			<persName><surname>Karam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2678" to="2683" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A lip sync expert is all you need for speech to lip generation in the wild</title>
		<author>
			<persName><forename type="first">Rudrabha</forename><surname>Kr Prajwal</surname></persName>
		</author>
		<author>
			<persName><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName><surname>Vinay P Namboodiri</surname></persName>
		</author>
		<author>
			<persName><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
				<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2008">2020. 1, 2, 6, 7, 8</date>
			<biblScope unit="page" from="484" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Expressive speechdriven lip movements with multitask learning</title>
		<author>
			<persName><forename type="first">Najmeh</forename><surname>Sadoughi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2018)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Novel realizations of speech-driven head movements with generative adversarial networks</title>
		<author>
			<persName><forename type="first">Najmeh</forename><surname>Sadoughi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Speech-driven expressive talking lips with conditional sequential generative adversarial networks</title>
		<author>
			<persName><forename type="first">Najmeh</forename><surname>Sadoughi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Animating arbitrary objects via deep motion transfer</title>
		<author>
			<persName><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stéphane</forename><surname>Lathuilière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2377" to="2386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">First order motion model for image animation</title>
		<author>
			<persName><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stéphane</forename><surname>Lathuilière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2008">2019. 2, 5, 7, 8</date>
			<biblScope unit="page" from="7137" to="7147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Ran He, and Chen Change Loy. Everybody&apos;s talkin&apos;: Let me talk as you want</title>
		<author>
			<persName><forename type="first">Linsen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05201</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Synthesizing obama: learning lip sync from audio</title>
		<author>
			<persName><forename type="first">Supasorn</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A deep learning approach for generalized speech animation</title>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taehwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moshe</forename><surname>Mahler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Krahe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasio</forename><surname>Garcia Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dynamic units of visual speech</title>
		<author>
			<persName><forename type="first">Moshe</forename><surname>Sarah L Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry-John</forename><surname>Mahler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th ACM SIGGRAPH/Eurographics conference on Computer Animation</title>
				<meeting>the 11th ACM SIGGRAPH/Eurographics conference on Computer Animation</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="275" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural voice puppetry: Audio-driven facial reenactment</title>
		<author>
			<persName><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="716" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Canonical correlation analysis. Encyclopedia of statistics in behavioral science</title>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Thompson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Realistic speech-driven facial animation with gans. International Journal of Computer Vision</title>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Vougioukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A large-scale audio-visual dataset for emotional talking-face generation</title>
		<author>
			<persName><forename type="first">Kaisiyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linsen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoqian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<meeting><address><addrLine>Mead</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Few-shot video-to-video synthesis</title>
		<author>
			<persName><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5013" to="5024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06601</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Video-tovideo synthesis. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8798" to="8807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Talking face generation by adversarially disentangled audio-visual representation</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9299" to="9306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Makeittalk: Speaker-aware talking head animation</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingzeyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Echevarria</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12992</idno>
		<imprint>
			<date type="published" when="2008">2020. 1, 2, 3, 4, 6, 7, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Visemenet: Audiodriven animator-centric speech animation</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Landreth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Highresolution talking face generation via mutual information approximation</title>
		<author>
			<persName><forename type="first">Aihua</forename><surname>Hao Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaibo</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.06589</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
