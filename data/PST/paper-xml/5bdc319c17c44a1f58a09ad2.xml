<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EAC-Net: Deep Nets with Enhancing and Cropping for Facial Action Unit Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical Engineering</orgName>
								<orgName type="department" key="dep2">the Grove School of Engineering</orgName>
								<orgName type="institution">The CUNY City College</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The CUNY Graduate Center</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Farnaz</forename><surname>Abtahi</surname></persName>
							<email>fab-tahi@gradcenter.cuny.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical Engineering</orgName>
								<orgName type="department" key="dep2">the Grove School of Engineering</orgName>
								<orgName type="institution">The CUNY City College</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">the Grove School of Engineering</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">The CUNY City College</orgName>
								<orgName type="institution" key="instit2">The CUNY Graduate Center</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhigang</forename><surname>Zhu</surname></persName>
							<email>zhu@cs.ccny.cuny.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical Engineering</orgName>
								<orgName type="department" key="dep2">the Grove School of Engineering</orgName>
								<orgName type="institution">The CUNY City College</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Thomas J. Watson School of Engineering and Applied Science</orgName>
								<orgName type="laboratory">Lijun Yin is with</orgName>
								<orgName type="institution">SUNY Binghamton Univer-sity</orgName>
								<address>
									<settlement>Binghamton</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lijun</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical Engineering</orgName>
								<orgName type="department" key="dep2">the Grove School of Engineering</orgName>
								<orgName type="institution">The CUNY City College</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EAC-Net: Deep Nets with Enhancing and Cropping for Facial Action Unit Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7DCE1D99AA735A72826E4AE64CDF26D8</idno>
					<idno type="DOI">10.1109/TPAMI.2018.2791608</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2018.2791608, IEEE Transactions on Pattern Analysis and Machine Intelligence IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 received XXXX; revised August XXXX.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convolutional neural network</term>
					<term>Facial analysis</term>
					<term>Attention coding</term>
					<term>Regions of interest</term>
					<term>Facial occlusion</term>
					<term>Head poses</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a deep learning based approach for facial action unit (AU) detection by enhancing and cropping regions of interest of face images. The approach is implemented by adding two novel nets (a.k.a. layers): the enhancing layers and the cropping layers, to a pretrained convolutional neural network (CNN) model. For the enhancing layers (noted as E-Net), we have designed an attention map based on facial landmark features and apply it to a pretrained neural network to conduct enhanced learning. For the cropping layers (noted as C-Net), we crop facial regions around the detected landmarks and design individual convolutional layers to learn deeper features for each facial region. We then combine the E-Net and the C-Net to construct a so-called Enhancing and Cropping Net (EAC-Net), which can learn both features enhancing and region cropping functions effectively. The EAC-Net integrates three important elements, i.e., learning transfer, attention coding, and regions of interest processing, making our AU detection approach more efficient and more robust to facial position and orientation changes. Our approach shows a significant performance improvement over the state-of-the-art methods when tested on the BP4D and DISFA AU datasets. The EAC-Net with a slight modification also shows its potentials in estimating accurate AU intensities. We have also studied the performance of the proposed EAC-Net under two very challenging conditions: (1) faces with partial occlusion and (2) faces with large head pose variations. Experimental results show that (1) the EAC-Net learns facial AUs correlation effectively and predicts AUs reliably even with only half of a face being visible, especially for the lower half; (2) Our EAC-Net model also works well under very large head poses, which outperforms significantly a compared baseline approach. It further shows that the EAC-Net works much better without a face frontalization than with face frontalization through image warping as pre-processing, in terms of computational efficiency and AU detection accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Facial Action Unit (AU) detection is an essential step in the facial analysis. With a robust AU detector, facial expression and facial related action problems can be solved more effectively. AU detection is the process to find some basic facial actions defined by FACS, the Facial Action Coding System <ref type="bibr" target="#b0">[1]</ref>. Each AU represents a basic facial movement or expression change. Table <ref type="table" target="#tab_0">1</ref> listed 12 basic AUs labeled in the BP4D dataset <ref type="bibr" target="#b25">[27]</ref>, and Figure <ref type="figure">1</ref> shows four basic AUs, namely eyebrows lower, cheek raiser, chin raiser and lip tighter. The AUs are elements for more complicated facial actions. For instance, sadness might be the combination of AU1 (inner brow raiser), AU4 (brow lower), and AU15 (lip corner depressor).</p><p>Most of current AU detection approaches either need the processed faces with frontal views or the texture features are interest processing -which makes our AU detection approach more robust to face position and orientation changes. Although face landmarks are used in our C-Net and EAC-Net, they do not need to be very accurately located, i.e. the approach is robust to landmark detection errors. 2) No facial preprocessing such as face frontalization via image warping is required to apply to input images in our approach, which not only saves a significant amount of preprocessing time but also maintains the original facial expression representation unaltered. We have shown that our EAC approach has resulted in an excellent performance under large head poses, without a face frontalization preprocessing step using image warping.</p><p>3) The EAC-Net with a slight modification also shows its potentials in estimating accurate AU intensities, outperforming some state-of-the-art approaches. 4) The proposed EAC-Net is able to detect AUs even when faces in the input images are partially occluded with an impressive F1 score, close to the best performance of the state-of-the-art approaches on the BP4D dataset.</p><p>This paper is organized as follows. Section 2 provides an overview of our EAC-Net approach. Related work is reviewed in Section 3. Section 4 details the design and analysis of the EAC-Net. The experimental results are summarized in Section 5. Finally conclusions are provided in Section 6.</p><p>Fig. <ref type="figure">1</ref>: Action unit images for AU 4, 6, 7, 17</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">OVERVIEW OF OUR APPROACH</head><p>The idea of our approach is inspired by recent breakthroughs in deep learning research. The first inspiration is learning transfer. Pretrained ImageNet models and transfer learning have found significant applications in many areas <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. The main reason is that the low-level convolutional layers extract similar features across different recognition tasks, which means the learned filters can be transferred.</p><p>The second inspiration is attention coding and cross-layer connections. Yang et al <ref type="bibr" target="#b3">[4]</ref> proposed to use an attention layer for finding interesting areas to provide better answers in a visual question-answering task. In salient object detection <ref type="bibr" target="#b4">[5]</ref>, a salient map is used to describe the important subareas of an image. This salient map (or attention map) is a 2D matrix with its elements ranging from 0 to 1, depicting the importance of corresponding pixels on the entire image. We believe that by applying an attention map to an existing network for facial computing, richer and more robust information can be obtained. A skipping layer structure is used in the Residual Net <ref type="bibr" target="#b24">[26]</ref>. Since both lower and higher levels of CNN layers correspond to more spatial and semantic information, the skipping connection of layers feeds both spatial and semantic features into CNNs, which can improve the model performance. The third inspiration is the region of interest (ROI) learning. The SPP Net <ref type="bibr" target="#b5">[6]</ref> proposed an ROI pooling idea, which can turn different areas of interest into fixed length features. Faster RCNN <ref type="bibr" target="#b6">[7]</ref> generated region proposals for objects to be detected and used the proposals to determine the "objectiveness". These findings made us believe that it might be possible to apply a similar operation to facial interest regions and learn individual filters for specific regions.</p><p>The design of our EAC-Net has integrated the three ideas: learning transfer, attention coding and cross-layer connections, and ROI learning. Figure <ref type="figure">2</ref> shows the structure of our EAC-Net. The EAC-Net consists of three key parts: a pretrained model, enhancing layers, and cropping layers. The first part is a fine-tuned pretrained VGG 19-layer network <ref type="bibr" target="#b7">[8]</ref>. The low-level convolutional layers (Group 1 and 2) of the pretrained network and their parameters are frozen for low-level visual feature extraction. The parameters of the higher-level convolutional layers (Group 3 and 4) are updated for the AU detection task. The use and fine-tuning of VGG pretrained net are with the spirits of learning transfer, to ensure that the network has a deep understanding of the input images. The second part of the EAC-Net consists of the enhancing layers that are on top of Group 3 and 4 convolutional layers, thus integrating attention coding and cross-layer connecting. The purpose of adding these layers is to give more attention to individual AU areas of interest, meanwhile fusing features cross different layers. The features extracted after the enhancing layers are supposed to contain more valuable information by integrating lower level spatial features and higher level semantic features for AU detection. The third part of the framework includes the cropping layers for regions of interest learning. Subfeatures are cropped from ten selected interest areas of the feature map, followed by upscale layers and individual convolutional layers for each sub-area for further regions of interest learning. The purpose of the cropping layers is to assure that only corresponding regions are compared by the individual cropping networks. Adding cropping layers as the higher convolutional layers would also help the region to obtain deeper contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head><p>AU detection has been studied for decades and many approaches have been proposed. Facial key points play an important role in AU detection. Many conventional approaches <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, [14], <ref type="bibr" target="#b14">[16]</ref>, <ref type="bibr" target="#b15">[17]</ref>, <ref type="bibr" target="#b16">[18]</ref>, <ref type="bibr" target="#b22">[24]</ref>, <ref type="bibr" target="#b28">[30]</ref> were Fig. <ref type="figure">2:</ref> The structures of FVGG, E-Net and EAC-Net for AU detection designed by employing texture features near the facial key points. Valstar et al <ref type="bibr" target="#b8">[9]</ref> analyzed Gabor wavelet features near 20 facial landmark points. The features were then selected and classified by Adaboost and SVM classifiers. Since landmark-based geometry changing is robust in many AU detection methods, Fabian et al <ref type="bibr" target="#b11">[12]</ref> proposed an approach for fusing the geometry and local texture information. The geometry information is obtained by measuring the normalized facial landmark distances and the angles of the Delaunay mask formed by the landmark points. The texture features were obtained by applying multiple orientation Gabor filters to the original images. Zhao et al <ref type="bibr" target="#b12">[13]</ref> proposed the Joint Patch and Multi-label Learning (JPML) for AU detection. Similarly, landmark-based regions were selected and SIFT features were used to represent the local patch. Overall, the conventional approaches focused on designing more representative features and fine-tuning more robust classifiers. In addition to facial AU detection, some studies also have focused on other facial related problems. Song et al <ref type="bibr" target="#b21">[23]</ref> investigated the sparsity and co-occurrence of action units. Wu <ref type="bibr" target="#b17">[19]</ref> exploited the joint of action unit detection and facial landmark localization and showed that the constraints can improve both AU and landmark detection. Girard et al <ref type="bibr" target="#b18">[20]</ref> analyzed the influence of different sizes of training datasets on appearance and shape-based AU detection. Note that AU detection is only estimating the occurrence of AUs, but sometimes the occurrence of an AU in one image is more obvious than others , thus AU intensity estimation is also a very interesting problem. Some works have been done to investigate into this problem. Gehrig et al <ref type="bibr" target="#b19">[21]</ref> tried to estimate action unit intensities by employing linear partial least squares to regress intensities in AU related regions. Nicolle, et al. <ref type="bibr" target="#b37">[39]</ref> proposed customized Multi-Task extensions of MLKR (Metric Learning for Kernel Regression), targeting on AU intensity estimation as a kernel regression problem. Baltrusaitis <ref type="bibr" target="#b38">[40]</ref> proposed an approach with cross-dataset learning and person-specific normalization for AU detection and AU intensity estimation, since more dataset can provide more identity variance for learning and personal customization can help better inference.</p><p>Over the last few years, we have witnessed that CNNs boost the performance in many computer vision tasks. Compared to most conventional artificially designed features, CNNs can learn and reveal deeper information from the training images which in turn contribute to better performance. Zhao et al <ref type="bibr" target="#b20">[22]</ref> proposed a region CNN-based approach for AU detection. Instead of directly applying a regular CNN to the entire input image, the network divides the input image into 8x8 blocks and then trains over these regional areas independently. All sub-regions are then merged back into one net, followed by regular convolutional and fully connected layers. The proposed approach outperforms both the regular and Fully Convolutional Net (FCN) <ref type="bibr" target="#b20">[22]</ref>. Jaiswal et al <ref type="bibr" target="#b13">[15]</ref> proposed using a CNN with a shallow region and shape mask a is employed to learn static CNN features while Long and Short Term Memory (LSTM) is used to extract dynamic features from the trained CNN mode <ref type="bibr" target="#b40">[42]</ref>. Gudi <ref type="bibr" target="#b39">[41]</ref> also tried deep learning in AU intensity estimation with a shallow network of 3 convolution layers and 1 fully connected layer.</p><p>Our proposed approach is also CNN-based and we share similar ideas with Zhao et al <ref type="bibr" target="#b20">[22]</ref> in training subregions independently. The distinction of our approach is that instead of directly dividing the image into blocks, we use a "smarter" way to find the important areas and crop the regions of interest for individual training. In their approach, the facial landmarks play an important role for normalizing the faces. Errors will accumulate in both landmark detection and face normalization processes, and facial normalization may neutralize expressions. However, in our approach, the network directly works on the interest areas of original face images, and even though landmarks are also used for building the attention map, our approach has a large tolerance for landmark shifting since we use a relatively large local region to cover the AU target areas. This will reduce errors from misalignment along images and in the meantime focus more on interest regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE EAC-NET</head><p>The EAC-Net is composed of three parts: the fine-tuned VGG network, enhancing layers, and cropping layers. For comparison purposes, we implement three networks using VGG net as their base: FVGG, the fine-tuned VGG network; E-Net, the Enhancing Net based on the fine-tuned VGG; and finally EAC, the integration of Enhancing and Cropping Nets based on the trained E-Net model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">FVGG: Fine-tuned VGG Model</head><p>Fine-tuning pretrained models for image classification are proved to be efficient in many areas <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. To make sure that we can have a deep understanding of the images for AU detection, we employed the VGG 19-layer model <ref type="bibr" target="#b7">[8]</ref>. The VGG model follows a conventional CNN structure, comprising 16 convolutional layers and 3 fully connected layers. It also includes 5 pooling layers downsampling input images from 224×224 to eventually 7×7 feature maps. In our implementation, we divide the convolutional layers into 5 groups as shown in Figure <ref type="figure">2</ref>, which are separated by the 5 pooling layers. There are 2, 2, 4, 4, 4 convolutional layers from Groups 1 to Group 5, respectively. The fine-tuned VGG (FVGG) structure reflects the spirit of learning transfer as VGG has been modified in many related tasks such as object detection or emotion recognition. Before designing our special purpose networks (E-Net and EAC-Net) for AU detection, we first modify and fine-tune the VGG net as our baseline approach. We keep the parameters of the first three groups of convolutional layers unchanged and update the rest of the layers during training. In order to match with our AU detection tasks, the numbers of nodes for the last 2 fully connected layers are changed to 2048 (by reducing parameters) and 12 (by matching the 12 AU targets), respectively. Dropout is also applied on both new layers to prevent overfitting during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">E-Net: The Enhancing Net</head><p>In a regular CNN, when an image is fed into the network, CNN filters will slide through the whole image, and different image regions are processed equally. This is fine for object classification task since we have no idea of how the input image looks like. But for structured images, such as faces, we would like to give more attention to more interesting regions.</p><p>As one of our inspirations, Yang et al <ref type="bibr" target="#b3">[4]</ref> applied an attentional layer to CNN middle features when tried to find a more effective feature for a question answering task. However, directly applying attention layer will discard a lot of information that is not included in the attention layer, therefore we need a method to combine both attention enhanced features and the rest of the less important areas. Another interesting approach for combining CNN multilayer features is the skipping layer structure in the Residual Net <ref type="bibr" target="#b24">[26]</ref>. We illustrate the skipping layer structure in Figure <ref type="figure" target="#fig_0">3</ref> (left). The skipping layer connection of the residual net makes the CNN incorporating both lower level spatial features and higher level semantic features as input, together providing a better representation of the input image. Integrating these two ideas, we designed our E-Net unit as the combination of the regions enhanced feature and the regular filtered feature as shown in 3 (right), which is an effective integration of the attention layer and skipping layer structures. Figure <ref type="figure">2</ref> includes the E-Net structure and Figure <ref type="figure" target="#fig_0">3</ref> demonstrates how the E-Net works by using our enhancing layers. The feature map output from Group 2 is multiplied by the designed attention map -the first enhancing layer (details will be discussed below), in parallel with the convolutional layers in Group 3. The two feature maps -one from the enhancing layer and the other from the Group 3 convolutional layers -are then fused by element-wise summation. The same operation is performed jointly by the second enhancing layer with the convolutional layers in Group 4. The reason why we designed the enhancing layer is that not all the areas of a facial image are equally important for individual AU detection. We can see that different AUs focus on corresponding sub-areas of the face. For example, in Figure <ref type="figure" target="#fig_1">4</ref>, the eyebrow raiser AU is close to the texture in the area near the middle of the eyebrows and the lip corner AUs are determined by the texture information around lip corners. These areas are more important than the nose or most of the other parts of the face. For this reason, we build the attention map for AUs based on key facial landmarks, as shown in Figure <ref type="figure" target="#fig_1">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AU center definitions.</head><p>We have noticed that many previous works provide robust facial landmark positions <ref type="bibr" target="#b23">[25]</ref>. Furthermore, our approach does not require the localization of landmarks to be very accurate in pixels since we are trying to find the areas for AUs. We work on 12 AUs as listed in Table <ref type="table" target="#tab_0">1</ref> since these are the ones labeled in the datasets we use. After obtaining the facial landmarks as shown in Figure <ref type="figure" target="#fig_1">4</ref>, we can define the centers for AUs and then build a bounding box around the center. Observing the AU figure, we manually define the center of AUs (the green spots) based on the muscles of a human face. Note that many AU centers are not directly on the same spots of the detected landmarks. We define a scaled distance as a reference for facial pixel shifting by calculating the distance of the outer corners of the two eyes, as shown in Figure <ref type="figure" target="#fig_1">4</ref>. The centers for 12 listed AUs in Table <ref type="table" target="#tab_0">1</ref> are illustrated in Figure <ref type="figure" target="#fig_1">4</ref> (the green spots). Since most AUs are symmetric on a human face, we define one pair of points for each AU. We should note that some AUs share the same centers, such as the lip corner puller and the lip corner depressor. So finally we defined 20 AU centers on the face for the 12 AUs listed in Table <ref type="table" target="#tab_0">1</ref>. The rules for defining the centers of the 12 AUs are also summarized in Table <ref type="table" target="#tab_0">1</ref>. After obtaining the AU centers, we can build the attention map based on center positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AU center localization and AU area extraction.</head><p>Given an image like the one shown in Figure <ref type="figure" target="#fig_1">4</ref> (left), we first obtain the landmarks for the key points on the face, which are shown with blue points. Having the facial key points, we can obtain the AU centers by shifting a distance or directly using existing facial landmarks. The AU centers are illustrated with green points in Figure <ref type="figure" target="#fig_1">4</ref>. The AU centers are in pairs due to the symmetry of the human face, with each AU center corresponding to one or more AUs. To make the shifting distance more adaptable to all face images, we define a measurement reference for the shifting distance. Inner corner distance is used as the scaled-distance, as shown in Figure <ref type="figure" target="#fig_1">4</ref>. This scaled-distance (listed in in Table <ref type="table" target="#tab_0">1</ref> for each AU) is used to help locate the AU centers. We first resize the images to 100x100 to make sure the same scales are shared among all images. Then, for each AU center, we define the nearby 5 pixels belonging to the same area, therefore the size of each AU area is 11x11. Higher weight is assigned to the closer points to the AU center. The relationship follows the following equation: </p><formula xml:id="formula_0">w = 1 -0.095 • d m (1)</formula><p>where d m is the Manhattan distance to the AU center.</p><p>Attention map generation and application. An attention map obtained for a face image is also demonstrated in Figure <ref type="figure" target="#fig_1">4</ref>. The areas in the attention map with higher values correspond to the AU active area in the face image and can enhance deep learning at these areas in our enhancing layers. We then apply the obtained attention map to the VGG feature maps. Figure <ref type="figure">2</ref> has shown the E-Net structure in the EAC-Net framework. For Group 1 and Group 2, we keep the layers unchanged for detecting low-level features. For Group 5 (size 14×14), the feature maps are too small to use any attention map, so eventually, we apply the attention map only to Group 3 and Group 4, thus we add two enhancing layers. Adding attention layers directly to the feature maps by replacing the original ones will lose all the contextual information. So, we add the attention maps to the feature maps in parallel with the convolution operations, in Group 3 and Group 4, respectively, as shown in Figure <ref type="figure">2</ref>. The element-wise summation is then conducted to obtain enhanced feature maps. We call our enhancing net based on the fine-tuned VGG model the E-Net. The E-Net structure is also similar to Residual Net <ref type="bibr" target="#b24">[26]</ref> but is designed not only for integrating features of different levels but also for generating enhanced features by applying an attention map. Of course, the E-Net also includes the VGG fine-tuning network. After training this model, we observed that the E-Net can lead to 5% increase in average F1 score and 19% increase in average accuracy on the BP4D AU dataset <ref type="bibr" target="#b25">[27]</ref>.</p><p>The detailed experimental results are reported in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">EAC-Net: The Integrated Model with Enhancing and Cropping</head><p>The E-Net can generate the features with more emphasis on the AU related regions, but it does not change the fact that the same AU areas are not normalized across different images, and different facial regions still share the same set of filters to extract features. For a general object detection task, sharing the same filters for the whole feature map is a norm, but for understanding a face, which is a very structured target, this is not an effective approach. Naturally different facial regions should use different sets of filters, which would be more efficient since we can train separate filters for specific AU regions, around nose, eye or mouth areas, respectively.</p><p>In the state of the art work, a simple region layer structure <ref type="bibr" target="#b20">[22]</ref> has been proposed to provide certain local convolutional training. The idea is to divide a feature map into 8×8 subregions, and each region uses a separate CNN model. Then if faces are aligned for all the input images, we can obtain those locally trained CNN models for individual sub-blocks. The disadvantage though is that during face normalization and image division, some context information might be lost, and since face normalization is trying to align the faces to a neutral expression, the facial expression might be weakened.</p><p>C-Net construction. The goal of our cropping net (C-Net) is to obtain each individual AU related area without changing the textural information of the original images. Then, for each AU sub-area, independent convolutional layers are applied to learn more features. The reason for us to crop the feature maps at such a "'late" layer (after Group 4 of the VGG) is: We hope the model can learn more general information from the input images via VGG (and the attention map), as well as the correlation of multiple parts of the AU regions. Then when we want to have a better focus of AU status at local regions, we treat each region individually at a high level in order to have a more robust prediction.</p><p>Figure <ref type="figure">2</ref> also includes the structure of the C-Net. Cropping layers are added to the end of Group 4, right after the enhancing feature maps are obtained. After we obtain the feature map from the output of convolutional Group 4, we crop out 20 regional feature maps by rescaling the 20 AU centers, which have been used in the E-Net. The cropped feature maps corresponding to different AUs. In order to learn them individually, we construct 20 local convolutional layers using them as inputs. The output size for the feature map from Group 4 is 512×28×28. With the same ratio as an AU area of 11×11 pixels in the attention map versus the face image of 100×100 pixels, the cropped areas should have the size of 3×3. For each of the 20 AU centers, we obtain a feature sub-map with size 512×3×3; in total, we have 20 such feature sub-maps for the 20 AU centers. When adding convolutional layers after the cropped feature map, the newly obtained feature map size will be 512×1×1. We feel that this is less representative for the AUs. So, before adding new convolutional layers, we apply an upscaling layer to the feature maps, upscaling the feature maps to 512×6×6. Actually, our experiments show that this upscaling layer by itself leads to approximately 1% increase in AUs average F1 score. For the cropping layers, the accuracy of the regions of interest positions as detected in the original images is not very critical since the resolution of the images already are reduced to 28×28.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EAC-Net construction and training.</head><p>To make the C-Net converge more quickly, we build the C-Net on top of the pretrained E-net, thus leading to the Enhancing and Cropping Net (EAC-Net). So the features obtained from Group 4 have already been pretrained for AU detection. During implementation, we have found that feature values obtained from the last convolutional layer of Group 4 are very large and make the C-Net unable to converge. So a local response normalization layer is added before C-Net convolutional layers. The local response layer normalization algorithm follows Eq. 2:</p><formula xml:id="formula_1">x i = x i (k + αΣ j x 2 j ) β<label>(2)</label></formula><p>In our experiments, k=2, α=0.002 and β=0.75. x i , x i are the model extracted feature values before and after applying the normalization, while j =, 1..., 9 denotes the 9 neighboring 2D feature pixels around x i (including itself). All the individual convolutional layers are followed by fully connected layers with a fixed size of 150. We then concatenate the fully connected layers. The rest is similar to FVGG and E-Net. AU detection is different from a regular classification task in the sense that instead of classifying images into one object category, multiple AUs can co-occur simultaneously. Thus this is a multi-label binary classification problem. Cross entropy as in <ref type="bibr" target="#b20">[22]</ref> is used to measure the loss for this kind of problem. In our loss function (Eq. 3), we added offsets to prevent the number from becoming too large:</p><formula xml:id="formula_2">Loss = -Σ(l • log( p + 0.05 1.05 ) + (1 -l) • log( 1.05 -p 1.05 )) (3)</formula><p>where l is the ground truth label for one certain AU, p is the regressed number by the trained model for the certain AU ranging from 0 to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Evaluation Methods</head><p>The most popular datasets for AU detection are CK+ <ref type="bibr" target="#b26">[28]</ref>, BP4D <ref type="bibr" target="#b25">[27]</ref> and DISFA <ref type="bibr" target="#b27">[29]</ref>. AU datasets are harder to obtain compared to other tasks such as image classification. This is because there are multiple AUs in one face which requires much more manual labeling work. Here we give a brief review of the AU datasets referred and compared in this paper.</p><p>DISFA: 26 people are involved in the DISFA dataset. The subjects are asked to watch videos while spontaneous facial expressions of the subjects are obtained. The AUs are labeled with intensities from 0 to 5. We can obtain more than 100,000 AU-labeled images from the video, but there are much more inactive images than the active ones. The diversity of people also makes it hard to train a robust model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BP4D:</head><p>There are 23 female and 18 male young adults involved in the BP4D dataset. Both 2D and 3D videos are captured while the subjects show different facial expressions. Each subject participates in 8 sessions of experiments, so there are 328 videos captured in total.</p><p>AUs are labeled by watching the videos, and the valid AU frames in each video vary from several hundred to thousands. There are around 140,000 images with AU labels that we can use.</p><p>To train a deep learning model, we need larger numbers of image samples, and the diversity of the samples is also important. Similar to the experiment settings in <ref type="bibr" target="#b20">[22]</ref>, we choose BP4D to train our model. We first split the dataset to 3 folds based on subjects. Each time two folds are used for training and the third fold for testing. For the DISFA dataset, all samples are used for testing the BP4D trained model.</p><p>The balance of data is very important in training deep learning models. For our task of multi-label learning, this is even more challenging since several AUs are not independent of each other. The original occurrence rate for the 12 selected AUs is shown in the first row of Table <ref type="table" target="#tab_1">2</ref>. We can clearly see that the AUs are divided into two groups. AUs 6, 7, 10, 12, 14 and 17 are more representative than the minor AUs 1, 2, 4, 9, 11 and 12. The minor AUs are not presented in many of the image samples. If we just pick all images that do include the less representative AUs, the occurrence rate is shown in table 2, the second row.</p><p>We can see that even with only the image samples that include the less representative AUs, the occurrence rates are still imbalanced. And we still need to keep the other samples to maintain the data diversity. Thus, we have finally decided to try to keep the balance of training data by changing the selection rate during training. For all the training samples, we used to equally and randomly pick a fixed number of images. To compensate for the less occurred AUs, we increase their rates during random picking operation by 4 to 7 times. Then the occurred rates by doing this are shown in Table <ref type="table" target="#tab_1">2</ref>, the third row, which is more balanced.</p><p>Both the F1 score and the average accuracy are used to measure the performance of AU detection. In a binary classification scenario, especially when samples are not balanced, the F1 score can better describe the performance of an algorithm <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. The F1 score includes two components: precision (p) and recall (r). The precision is also called the positive predictive value and is the fraction of true positive predictions to all positive predictions. The recall is also called sensitivity and is the fraction of true positive predictions to all ground-truth positive samples. Knowing p and r, we can obtain F1 using Eq. 4:</p><formula xml:id="formula_3">F 1 = 2p • r p + r<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation Details and Results</head><p>In our deep learning models, the basic VGG structure is employed as the base, so all of the input images to the networks are resized to 224x224 to become compatible with the structure. In order to maintain consistence in creating the attention map for the E-Net and the ROI cropping parameters for the cropping layers, we use the facial landmark data provided by the BP4D and DISFA datasets. We use a fixed learning rate of 0.0001 and a momentum of 0.9 in our training. For updating the model, SGD is employed. FVGG and E-Net models converge after about 100 epochs and the EAC-Net converges after about 500 epochs. We trained our models using a workstation with a GeForce GTX TITAN X graphics card, and it took 2-3 hours (100-150 epochs) for FVGG or E-Net and 10-15 hours (for 500-800 epochs) for EAC-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on BP4D dataset</head><p>We trained three models using the BP4D dataset: finetuned VGG (FVGG), E-Net, and EAC-Net. The accuracy and F1 score for all 12 selected AUs, and the average accuracy and the average F1 score are listed in Tables <ref type="table" target="#tab_2">3</ref> and<ref type="table" target="#tab_3">4</ref>, respectively. We also list the results from the state of the art work DRML <ref type="bibr" target="#b20">[22]</ref> using deep learning and traditional approaches LSVM <ref type="bibr" target="#b20">[22]</ref>, and JPML <ref type="bibr" target="#b12">[13]</ref> in same settings for comparison.</p><p>As shown in Tables <ref type="table" target="#tab_2">3</ref> and<ref type="table" target="#tab_3">4</ref>, for the BP4D dataset, compared to the state of the art approaches, the VGG finetuned model (FVGG) has a higher average accuracy, but the average F1 score does not outperform the state of the art. Note that in our proposed approach, we do not perform any preprocessing to the input images. For the more representative AUs -6, 7, 12, 14 and 17, the network is able to better predict the AU labels than the state of the art approaches, without being told the position of the AUs. We believe that  this is due to the depth of the VGG model, which can learn very deep features, and the pooling layers, which make the AU detection robust to position and orientation shifts. For the less representative AUs -1, 2, 4, 15, 23 and 24, however, the texture is less discriminative for instance around the eyebrow area. Also, the occurrence rates are still smaller than the other AUs even after the balancing, making the training more challenging. The E-Net results show both better average accuracy and F1 scores than the state of the art approaches and the VGG fine-tuning net. On average, the improvement using E-Net over FVGG in the average F1 score and average accuracy are 8.3% and 10.1%, respectively. The results show the effectiveness of our proposed enhancing layers.</p><p>To explore more details of the E-Net, we extract the feature maps from multiple layers in the E-Net and VGG fine-tuning Net. In Figure <ref type="figure" target="#fig_3">5</ref>, the last feature map of Group 4 convolutional layers from each of the two structures, FVGG and E-Net, are visualized. Each feature map is 512×28×28, i.e.-with 512 feature arrays, each of 28×28. We visualized the 512 feature map into a 32×16 arrays of 28×28 images. We can clearly see that the attention map made a big difference in the output feature maps. In the VGG feature map, the hot areas or the attention areas do not have a meaningful focus. In some areas, even the edges are highlighted as valuable features. The neural network has no idea which region to look into. While in the E-Net feature map, we can clearly see the network is concentrating on the area on the face, mainly the regions enhanced by the attention map. This can make the E-Net extract more valuable features for AU detection. Comparison to Methods from FERA 2015. The FG 2015 Facial Expression Recognition and Analysis Challenge (FERA2015 <ref type="bibr" target="#b36">[38]</ref>) used BP4D as the challenge dataset to encourage participants to propose high quality AU estimation methods. We compared with the two baseline methods (bs-geo using geometric features and bs-app using appearance features) <ref type="bibr" target="#b36">[38]</ref> and two top-performance methods: the cross dataset learning and person-specific normalization approach (CLPN) <ref type="bibr" target="#b38">[40]</ref> and the shallow CNN approach (CNN) <ref type="bibr" target="#b39">[41]</ref>. We trained our model with FERA 2015 training data and compared with the proposed methods FERA 2015 data to show the EAC-Net performance. Table <ref type="table" target="#tab_4">5</ref> shows the F1 of the comparison. the EAC-Net performs the best on average, with a 5.7% margin over the best in the state-of-the-art.</p><p>Finally, our EAC-Net achieves the best average performance in AU detection in terms of both F1 score and accuracy measures. Compared to the state of the art approaches, The EAC-Net shows improvement of F1 scores in all the  AU detections results, except for AU2 (DRML) and AU15 (JPML), and of accuracy measures for all the 12 AUs. We can also see that the F1 score and accuracy measures all have improved from E-Net to EAC-Net: even though the cropping layers of the C-Net only slightly increases the average accuracy by 0.7%, the F1 score, which is a more appropriate indicator of the performance of the algorithm, increases by 3.5% over the E-Net. We know that the major role of the cropping layers is for a "smart" alignment, so it might not have a significant effect on the faces of the BP4D data, which are mostly close to frontal view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on DISFA dataset</head><p>We follow the setting in <ref type="bibr" target="#b20">[22]</ref> to evaluate our approach on the DISFA dataset. In our evaluation with DISFA dataset, we used the BP4D pretrained model; the same was used in the state-of-the-art work <ref type="bibr" target="#b20">[22]</ref>. That is because the DISFA is not a suitable dataset for deep learning, for the 3 following reasons: 1) In DISFA, the number of people is very small; only 26 people are involved in the dataset, therefore training a model on such a small dataset could be easily over-fitted.</p><p>2) Most of the AU occurrence rates are very small, therefore this imbalance will affect the training of a deep learning model. 3) For a dataset like DISFA with limited active samples, fine-tuning on a pretrained model for a similar task is a better choice. Therefore we use our pretrained finetuned FVGG, E-Net and EAC Net to extract features from the DISFA images. Afterward, we use linear regression to transform our 1x2048 features to 1x8 AU prediction labels. 27 subjects are split into 3 folds to make sure the predictions are independent.</p><p>The AU detection accuracy and F1 score are shown in Tables <ref type="table" target="#tab_5">6</ref> and<ref type="table" target="#tab_6">7</ref>. Compared to the state of the art approaches, we see more significant improvement than that with BP4D. The F1 score of the EAC-Net increases by 4.1% over the E-Net, and the average accuracy increases by 4.9%. More importantly, the improvement yielded by C-Net is more significant than by the E-Net. Note that the significant improvement on DISFA is due to the following reasons:</p><p>(1) Our proposed approach is capable of balancing the training datasets. The DISFA dataset is more imbalanced than BP4D. If we directly use all the raw data, the AU occurrence rates of AU 4, 12, and 25 are much higher than the others (AU 1, 2, 6, 9, and 26), as shown in Table <ref type="table" target="#tab_7">8</ref>. Our preprocessing in balancing the data can improve the AU detection results.</p><p>(2) Our approach is more robust in dealing with wild images. The DISFA subjects have small rotation angles to the frontal view, so normalization is usually required in most the state-of-the-art approaches, while our approach only needs to know the approximate landmarks positions on the faces. This makes our approach much more robust dealing with faces which are not in frontal view. This will be further shown in Subsection 5.5 on AU detection on face images with large head poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">iEAC: Intensity Estimation Using the EAC-Net</head><p>In BP4D, 5 AUs (AU6, AU10, AU12, AU14, AU17 ) are with intensity labels, thus we use our trained EAC-Net as the base model, then change the last multi-label classification layer to a multi-label regression layer, leading to our iEAC-Net (EAC-Net for intensity estimation). To optimize the model, we map AU intensities to the range of [0,1], therefore the outputs of the modified model are also mapped to [0,1] via a sigmoid function. We compute the mean of absolute differences as loss to guide the iEAC model to converge. In our experiment, the iEAC model converges after 6000 iteration with a batch size of 256.</p><p>We run the fully automated intensity estimation and evaluated with both the MSE (Mean Square Error) and PCC (Pearson Correlation Coefficient) metrics as advised by the FERA 2015 baseline <ref type="bibr" target="#b36">[38]</ref>. We compared with both the two baseline methods (bs-geo and bs-app) and two top results reported in the FERA 2015 challenge: the Metric Learning for Kernel Regression approach (MLKR) <ref type="bibr" target="#b37">[39]</ref>, and the shallow CNN approach (sCNN) <ref type="bibr" target="#b39">[41]</ref>, with the same settings. The comparison of the intensity estimation results (with both MSE and PCC( measures) is shown in Table <ref type="table" target="#tab_8">9</ref>. The experiment shows that the iEAC-Net yields better performance in most of the AUs in terms of the MSE and PCC metrics, with margin of 0.065 in MSE and 3% in PCC evaluation. This indicates that our proposed EAC-Net not only can perform well in AU occurrence detection, but also can provide reliable estimation in AU intensity prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">EAC-Net with Partially Occluded Faces</head><p>The AU detection on partially occluded faces is a very challenging problem. Without complete faces, which could be occluded by hands, glasses, hats, and other people, traditional landmark or patch-based approaches may fail to detect AUs. A few researchers <ref type="bibr">[31] [32]</ref> have tried to learn partial facial regions to in order to detect AUs. Since our EAC-Net has both region enhancing and region-based CNN models, we would like to test if our model can have a good performance in AU detection when the faces are partially occluded.</p><p>There are three main reasons for us to conduct this experiment. First of all, our model learned enhanced features and cropped local features in the enhancing and cropping layers of EAC-Net. After the cropping layers, the local convolutional features are merged and learned as a combined feature, therefore we believe during this process, some interesting correlation can be learned. Thus even without raw input data for some of the regions, our EAC-Net might still be able to detect the remaining AUs on a partially occluded face. Second, if the AUs are detectable from partial faces, it is necessary to extend our investigation to which parts of a face convey more information than others for AU detection. Third, we are also interested in knowing if it is possible to predict all AUs without the primary parts of the face visible for some AUs. If this is true, this will imply that the AUs mainly perceivable from the occluded parts might be guessed based on the learned AU correlations in our model. Given above reasons, we conduct the following experiment on AU detection with occluded faces. As shown in Figure <ref type="figure" target="#fig_4">6</ref>, we used our EAC-Net model directly to test on the BP4D faces where only upper, lower, left and right half-face are visible, respectively. By investigating the model performance, we would like to know which parts of the faces are more important in determining AU states. To make sure the results is better than a random guess, we also ran the test with the test data where whole faces are occluded. For a fair comparison with our previous AU detection results using the EAC-Net on original face images in BP4D, the same settings with BP4D training are used. In our experiment, we not only detect the "perceivable" AUs on the occluded faces but also try to predict the "unperceivable" ones, such as predicting the lower face AUs from the upper face images.</p><p>The occluded face AU detection results are shown in Table <ref type="table" target="#tab_9">10</ref>. In the table, "Lower" means only lower half of the face are used for testing, the same goes for "Upper", "Left", and "Right". For "None", we simply set all the pixel values to be 0s and feed the zero image into the EAC-Net. For a "None" face, the input to the EAC-Net will be a zero image, therefore the outcome of the EAC-Net is also a fixed  result, which means that the occurrence rate of each AU is the only factor for the "recognition" score. The occurrence rate will have impact on the recall and precision score of AU, thus determine the F1 score of AU prediction. For columns Lower and Upper, since some AUs cannot be seen when running such a test on occluded faces, we use underlined numbers to note certain AUs results are "guessed" by the EAC-Net model. Some interesting conclusions can be made based on the results. Here are several observations:</p><p>(1) We can predict the "unseen" AUs status with partial faces. Comparing our "unseen" AU predictions (underlined numbers in Table <ref type="table" target="#tab_9">10</ref>) to a totally unavailable result, we can see that the EAC-Net always provides betterthan-random-guess results with the four kinds of half occluded face images, although there are obvious differences among the four cases.</p><p>(2) It seems that the lower half of a face is the most useful part for AU detection. If we only look into the lower part of each face for AU detection, the EAC-Net model can produce comparable results to the DRML <ref type="bibr" target="#b20">[22]</ref> and FVGG result and yield much better performance than the one from the other three parts (i.e., upper, left, and right). Even for some unseen AUs, like AU1(Inner brow raiser), AU2 (outer brow raiser), AU4 (Brow lower), the results are similar to or better than the results generated from "Up" face where these areas are seen.</p><p>(3) We also notice that the face AU correlation is important in single AU detection. For some obviously independent AUs like AU7 (Lid tighter), the appearance features may be enough and the "visibility" of that part can produce the best result (74% for AU7 in Upper faces). But for AUs like brow related and lip related ones, they are more dependent on the occurrence of some other AUs. For example, results could be different on detection of AU17 and AU 23 with the lower part or entire face being used. Including the upper face information can help infer whether the AUs of "chin raiser" and "lip tighter" are active.</p><p>(4) The upper/lower half-faces overall can generate better results than left/right half-faces, which means the proposed EAC-Net might not learn the symmetrical relationship of the face very well. This is an interesting finding that inspires us to design a model with such a function in our future work.</p><p>From the occluded faces experiment, we can see the proposed EAC-Net can predict reliable AU results with only half faces, especially with lower half face images. This may be a good sign for some scenes where only lower faces can be seen, such that the upper faces are occluded by sun glasses and hats. Knowing the correlation of AUs plays an important role in AU detection for design better AU detection models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">pEAC: EAC-Net on Faces with Large Head Poses</head><p>In our previous experiments, we have found that our EAC-Net not only exhibits excellent performance in near frontal static face images for AUs detection, but also its ability in dealing with AUs detection and prediction on partially occluded facial images. However, in real-world scenarios, it is rare for people to show their spontaneous expressions without changing head poses. In our algorithm design section, we have hypothesized that our approach is adaptive to facial poses since we have created the attention map and the local convolutional networks are based on facial landmarks. To verify whether the proposed approach can really predict AUs with large head poses, we evaluate our algorithm on a newly derived dataset with different views of face images, which were generated from the BP4D database as the FERA 2017 challenge dataset <ref type="bibr" target="#b32">[34]</ref>.</p><p>The dataset used in the FERA 2017 challenge was generated from BP4D and BP4D+ <ref type="bibr" target="#b33">[35]</ref>. BP4D+ is an expanded version of BP4D with more subjects in multi-modality. In our previous experiments, we employed the 2D version of BP4D to train the E-Net and EAC-Net. In fact, the BP4D and BP4D+ have also captured a 3D face model for each face image frame. In the FERA 2017 challenge, in order to create an AU dataset with multiple poses, the 3D face model of each face is rotated by pitch angles of -40, -20, and 0 degrees, and yaw angles of-40, 0, and 40 degrees, respectively, from its frontal pose, so in total there are nine poses for each face in total. Figure <ref type="figure" target="#fig_5">7</ref> shows one example, and Table <ref type="table" target="#tab_0">11</ref> lists the pitch and yaw angles of each of the nine poses. As a result, nine videos with corresponding nine head poses are generated for each video sequence of a subject from the BP4D dataset.   In our experiment, we train our EAC-Net by extracting the provided videos with 9 poses from the FERA 2017 training set. We note the EAC model trained on face images with large head poses as pEAC-Net. The number of images for training is about 260,000, and similar balancing strategy as we used in 2 is applied boosting up the numbers of samples of the under-represented AUs. We use the same training configurations as we did in frontal view BP4D AU detection.</p><p>For comparison purpose, the FERA 2017 organizers <ref type="bibr" target="#b32">[34]</ref> have provided AU detection baseline results on both its validation and test datasets, which are both from BP4D+. However, at the moment, only the performance results of the baseline approach on the validation (development) dataset is available to researchers. Therefore we only compare the results of our EAC approach with the baseline results on the validation dataset. We also use the landmarks provided by the challenge organizers, so the configurations of the dataset for testing on the validation dataset are the same with the baseline approach.</p><p>To further investigate whether our model can avoid face frontalization by directly applying AU detection to face images with large head poses, we have also aligned 1,000 randomly selected images from each of the 9 poses. We applied the pEAC-Net trained on face images with large head poses to the aligned face images in an attempt to verify if the alignment would improve or degrade the AU detection performance.</p><p>The overall F1 and accuracy scores on all the nine poses are reported in Table <ref type="table" target="#tab_11">12</ref>, against the results of the baseline approach on the FERA development dataset. Table <ref type="table" target="#tab_12">13</ref> and Table <ref type="table" target="#tab_13">14</ref> show the F1 scores and accuracy values on face images of the nine individual poses, respectively. To make it easier to compare, the baseline results for all the 9 poses on the validation dataset are also listed. Table <ref type="table" target="#tab_14">15</ref> lists the F1 and accuracy results of AU detection with aligned BP4D face images within 9 pose views.</p><p>From the results, we have several interesting observations as follows:</p><p>(1) Based on the pEAC-Net AU detection results on the BP4D face images with large head poses, the pEAC-Net shows its ability to detect AUs from facial images with different poses. Comparing to the baseline approach, there is a 10.6% improvement in F1 score. Although it is not fair to compare this with the EAC-Net results on the frontal view data of BP4D, we can see the performance improvement is at the same level. This shows that the pEAC-Net is able to detect AUs on face images with large head poses.</p><p>(2) The dataset of BP4D face images with large head poses provides 9 fixed view angles. We test our pEAC-Net on the 9 poses and find that in all the 9 angles, the pEAC-Net can make a better AU prediction than the baseline approach. Same to the finding from the baseline approach, the best single view result that our pEAC-Net has achieved is also on the pose #5, which is the F1-score at 58.3% for the 20-degree pitch angle from the frontal view. This is interesting and might be due to the appearance of facial expressions when viewed from 20-degree angle being better than the frontal view in 3D space, while the face is not largely occluded. Note that this finding is also similar or compatible to the finding for non-frontal view facial expression classification reported in <ref type="bibr" target="#b34">[36]</ref>, where the similar performance was also achieved for 30-degree view as compared to the other view angles in recognizing six prototypic facial expressions from the static 3DFE database <ref type="bibr" target="#b35">[37]</ref>.</p><p>(3) Another observation we can make is that with larger angle orientations, the performance is generally becoming worse due to more information loss. However, our pEAC-Net performs relatively much more robust to view angle changes: the lowest F1 scores of our pEAC-Net are comparable to the highest F1 scores with the baseline approach.</p><p>(4) The aligned face AU detection experiment gives us many valuable insights into our pEAC-Net model. Our    pEAC-Net can directly work on face images with large head poses and predict better AU results on all the nine poses than the results with an alignment pre-processing step. This shows the orientation invariant ability of our pEAC-Net model in dealing with head pose variations on AU detections, although large pose may still affect the AU detection due to information loss. As a comparison, alignment is not only time consuming (from only 30 ms to 2500 ms per image) but probably also degrades the fidelity of original facial expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>In this paper, we have proposed a region of interest based approach for AU detection. We design the enhancing net (E-Net) to force the neural network to pay more attention to AU interest regions on face images. We have also proposed the cropping net (C-Net) to ensure that individual networks learn features in "aligned" facial areas. This makes our EAC-Net -the integration of the E-Net and C-Net -more robust to facial shifts and orientation differences.</p><p>We have evaluated our proposed E-Net and EAC-Net against the state-of-the-art approaches. The AU detection results show that our approach can achieve better performance on commonly used AU datasets. With deep pretrained models and a "smarter" way to focus on interest regions, the proposed approach shows its power in AU detection on multiple datasets including BP4D and DISFA.</p><p>We have further tested our EAC-Net on BP4D occluded face data and found that our learning model is able to detect, and to a certain extent, predict AUs even with half faces visible, especially for lower-half faces. In order to investigate if our EAC-Net can perform well when captured faces are in large head poses, we have trained and tested our EAC-Net on a BP4D AU dataset -from the FERA 2017 challenge -with face images showing large head poses. By comparing to the challenge baseline results, we can see our EAC-Net model leads to significant improvement in overall AU detection as well as in each individual pose of the 9 views. We also compared the AU detection results with and without pre-processing for face alignment, and the results show that the alignment even worsens the AU detection performance, meanwhile increasing the processing time.</p><p>We believe that the idea of using the AU centers to generate attention maps should be also important in other face computing problems, such as facial expression recognition, attention monitoring and pain detection. In all these applications, some regions of a face convey more important information, therefore paying more attention to these regions will probably make the performance better. The attention map idea can also be generated to other non-facial applications where domain knowledge is either available or can be learned.</p><p>In a follow-on work, we have been exploring the integration of temporal information into the EAC-Net framework using LSTM <ref type="bibr" target="#b40">[42]</ref> to deal with the wild video AU detection problem. In the future, we will extend our work to finding more responsive areas for the enhancing and cropping nets rather than manually locating the positions at present. Automatic generation of attention maps may be possible by adding a RCNN detection net; this will also be our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Skipping layer connection in Residual Net (left) and E-Net (right)</figDesc><graphic coords="4,335.15,241.08,205.70,94.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Attention map generation. Left: landmarks(blue) and AU centers (green) on a face; Right: attention map of the face</figDesc><graphic coords="5,64.69,459.93,218.63,100.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>First, the VGG fine-tuned net (FVGG) is trained as the baseline for our proposed E-Net. EAC-Net training is based on pretrained E-Net with new C-Net layers. Throughout the training, 50 images are randomly selected from the whole training dataset as a batch, and an epoch has 20 batches. The</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Visualization of selected feature maps from Group 4, with FVGG only (left) and with E-Net (right).</figDesc><graphic coords="8,58.20,351.67,231.60,129.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Occluded faces for AU detection.</figDesc><graphic coords="9,353.40,333.93,169.20,128.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Nine head poses of a face in the BP4D AU dataset with large head poses from FERA 2017 data [34]. TABLE 11: Pitch and yaw angles for the 9 poses, and the detected frames using pEAC-Net from the validation (development) dataset (For each pose, there are around 11,000 images)</figDesc><graphic coords="11,76.40,43.70,459.20,67.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>.3 11.11 28.6 25.6 27.8 16.0 56.0 78.0 72.0 60.0 68.0 70.0 42.0 48.0 .0 66.0 58.0 52.0 60.0 46.0 46.0 52.0 Avg 45.9 37.0 44.6 48.1 50.5 51.3 44.5 44.3 37.6 55.8 56.2 56.0 56.8 60.2 63.6 55.6 55.4 56.0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,104.50,43.70,403.00,272.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 :</head><label>1</label><figDesc>Rules for defining AU centers</figDesc><table><row><cell>AU index</cell><cell>AU Name</cell><cell>AU Center</cell></row><row><cell>1</cell><cell>Inner Brow Raiser</cell><cell>1/2 scale above inner brow</cell></row><row><cell>2</cell><cell>Outer Brow Raiser</cell><cell>1/3 scale above outer brow</cell></row><row><cell>4</cell><cell>Brow Lowerer</cell><cell>1/3 scale below brow center</cell></row><row><cell>6</cell><cell>Cheek Raiser</cell><cell>1 scale below eye bottom</cell></row><row><cell>7</cell><cell>Lid Tightener</cell><cell>Eye center</cell></row><row><cell>10</cell><cell>Upper Lip Raiser</cell><cell>Upper lip center</cell></row><row><cell>12</cell><cell>Lip Corner Puller</cell><cell>Lip corner</cell></row><row><cell>14</cell><cell>Dimpler</cell><cell>Lip corner</cell></row><row><cell>15</cell><cell>Lip Corner Depressor</cell><cell>Lip corner</cell></row><row><cell>17</cell><cell>Chin Raiser</cell><cell>1/2 scale below lip</cell></row><row><cell>23</cell><cell>Lip Tightener</cell><cell>Lip center</cell></row><row><cell>24</cell><cell>Lip Pressor</cell><cell>Lip center</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 :</head><label>2</label><figDesc>BP4D samples balancing for AU occurrences in training</figDesc><table><row><cell>AU</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>7</cell><cell>10</cell><cell>12</cell><cell>14</cell><cell>15</cell><cell>17</cell><cell>23</cell><cell>24</cell></row><row><cell>Original</cell><cell cols="12">0.24 0.18 0.23 0.44 0.52 0.58 0.57 0.43 0.15 0.36 0.19 0.16</cell></row><row><cell cols="13">Minor AU occurred 0.56 0.43 0.40 0.47 0.57 0.64 0.59 0.56 0.35 0.58 0.46 0.39</cell></row><row><cell>After balancing</cell><cell cols="12">0.39 0.32 0.33 0.45 0.54 0.60 0.56 0.49 0.30 0.50 0.33 0.30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3 :</head><label>3</label><figDesc>F1 score comparison of AU detection on BP4D dataset</figDesc><table><row><cell>AU</cell><cell cols="6">LSVM JPML [13] DRML [22] FVGG E-Net EAC</cell></row><row><cell>1</cell><cell>23.2</cell><cell>32.6</cell><cell>36.4</cell><cell>27.8</cell><cell>37.6</cell><cell>39.0</cell></row><row><cell>2</cell><cell>22.8</cell><cell>25.6</cell><cell>41.8</cell><cell>27.6</cell><cell>32.1</cell><cell>35.2</cell></row><row><cell>4</cell><cell>23.1</cell><cell>37.4</cell><cell>43.0</cell><cell>18.3</cell><cell>44.2</cell><cell>48.6</cell></row><row><cell>6</cell><cell>27.2</cell><cell>42.3</cell><cell>55.0</cell><cell>69.7</cell><cell>75.6</cell><cell>76.1</cell></row><row><cell>7</cell><cell>47.1</cell><cell>50.5</cell><cell>67.0</cell><cell>69.1</cell><cell>74.5</cell><cell>72.9</cell></row><row><cell>10</cell><cell>77.2</cell><cell>72.2</cell><cell>66.3</cell><cell>78.1</cell><cell>80.8</cell><cell>81.9</cell></row><row><cell>12</cell><cell>63.7</cell><cell>74.1</cell><cell>65.8</cell><cell>63.2</cell><cell>85.1</cell><cell>86.2</cell></row><row><cell>14</cell><cell>64.3</cell><cell>65.7</cell><cell>54.1</cell><cell>36.4</cell><cell>56.8</cell><cell>58.8</cell></row><row><cell>15</cell><cell>18.4</cell><cell>38.1</cell><cell>33.2</cell><cell>26.1</cell><cell>31.6</cell><cell>37.5</cell></row><row><cell>17</cell><cell>33.0</cell><cell>40.0</cell><cell>48.0</cell><cell>50.7</cell><cell>55.6</cell><cell>59.1</cell></row><row><cell>23</cell><cell>19.4</cell><cell>30.4</cell><cell>31.7</cell><cell>22.8</cell><cell>21.9</cell><cell>35.9</cell></row><row><cell>24</cell><cell>20.7</cell><cell>42.3</cell><cell>30.0</cell><cell>35.9</cell><cell>29.1</cell><cell>35.8</cell></row><row><cell>Avg</cell><cell>35.3</cell><cell>45.9</cell><cell>48.3</cell><cell>43.8</cell><cell>52.1</cell><cell>55.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell cols="6">: Accuracy comparison of AU detection on BP4D</cell></row><row><cell>dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AU</cell><cell cols="6">LSVM JPML [13] DRML [22] FVGG E-Net EAC</cell></row><row><cell>1</cell><cell>20.7</cell><cell>40.7</cell><cell>55.7</cell><cell>27.2</cell><cell>71.1</cell><cell>68.9</cell></row><row><cell>2</cell><cell>17.7</cell><cell>42.1</cell><cell>54.5</cell><cell>56.0</cell><cell>72.9</cell><cell>73.9</cell></row><row><cell>4</cell><cell>22.9</cell><cell>46.2</cell><cell>58.8</cell><cell>80.5</cell><cell>77.4</cell><cell>78.1</cell></row><row><cell>6</cell><cell>20.3</cell><cell>40.0</cell><cell>56.6</cell><cell>72.3</cell><cell>76.9</cell><cell>78.5</cell></row><row><cell>7</cell><cell>44.8</cell><cell>50.0</cell><cell>61.0</cell><cell>64.1</cell><cell>70.7</cell><cell>69.0</cell></row><row><cell>10</cell><cell>73.4</cell><cell>75.2</cell><cell>53.6</cell><cell>72.4</cell><cell>75.7</cell><cell>77.6</cell></row><row><cell>12</cell><cell>55.3</cell><cell>60.5</cell><cell>60.8</cell><cell>69.1</cell><cell>82.8</cell><cell>84.6</cell></row><row><cell>14</cell><cell>46.8</cell><cell>53.6</cell><cell>57.0</cell><cell>52.8</cell><cell>56.7</cell><cell>60.6</cell></row><row><cell>15</cell><cell>18.3</cell><cell>50.1</cell><cell>56.2</cell><cell>67.4</cell><cell>77.6</cell><cell>78.1</cell></row><row><cell>17</cell><cell>36.4</cell><cell>42.5</cell><cell>50.0</cell><cell>61.2</cell><cell>69.3</cell><cell>70.6</cell></row><row><cell>23</cell><cell>19.2</cell><cell>51.9</cell><cell>53.9</cell><cell>72.2</cell><cell>80.2</cell><cell>81.0</cell></row><row><cell>24</cell><cell>11.7</cell><cell>53.2</cell><cell>53.9</cell><cell>77.0</cell><cell>82.3</cell><cell>82.4</cell></row><row><cell>Avg</cell><cell>32.2</cell><cell>50.5</cell><cell>56.0</cell><cell>64.4</cell><cell>74.5</cell><cell>75.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 5 :</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell cols="5">F1 score comparison of AU detection on BP4D</cell></row><row><cell cols="4">dataset with FERA 2015 approaches</cell><cell></cell><cell></cell></row><row><cell cols="6">AU bs-geo [38] bs-geo [38] CLPN [40] sCNN [41] EAC</cell></row><row><cell>1</cell><cell>18.8</cell><cell>18.0</cell><cell>26.0</cell><cell>39.9</cell><cell>39.2</cell></row><row><cell>2</cell><cell>18.5</cell><cell>15.9</cell><cell>25.0</cell><cell>34.6</cell><cell>31.7</cell></row><row><cell>4</cell><cell>19.7</cell><cell>22.5</cell><cell>25.0</cell><cell>31.7</cell><cell>40.4</cell></row><row><cell>6</cell><cell>64.5</cell><cell>67.1</cell><cell>73.0</cell><cell>71.8</cell><cell>67.1</cell></row><row><cell>7</cell><cell>79.9</cell><cell>75.1</cell><cell>80.0</cell><cell>77.6</cell><cell>84.7</cell></row><row><cell>10</cell><cell>80.1</cell><cell>79.9</cell><cell>84.0</cell><cell>79.7</cell><cell>86.0</cell></row><row><cell>12</cell><cell>80.1</cell><cell>79.2</cell><cell>82.0</cell><cell>79.3</cell><cell>87.6</cell></row><row><cell>14</cell><cell>72.0</cell><cell>66.6</cell><cell>72.0</cell><cell>68.1</cell><cell>60.4</cell></row><row><cell>15</cell><cell>23.8</cell><cell>13.9</cell><cell>34.0</cell><cell>23.5</cell><cell>46.7</cell></row><row><cell>17</cell><cell>31.1</cell><cell>24.5</cell><cell>33.0</cell><cell>36.8</cell><cell>56.0</cell></row><row><cell>23</cell><cell>32.0</cell><cell>23.9</cell><cell>34.0</cell><cell>30.9</cell><cell>38.3</cell></row><row><cell>Avg</cell><cell>47.3</cell><cell>44.2</cell><cell>51.6</cell><cell>52.2</cell><cell>57.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 6 :</head><label>6</label><figDesc>F1 score comparison of AU detection on DISFA dataset</figDesc><table><row><cell>AU</cell><cell cols="6">LSVM APL [22] DRML [22] FVGG E-Net EAC</cell></row><row><cell>1</cell><cell>10.8</cell><cell>11.4</cell><cell>17.3</cell><cell>32.5</cell><cell>37.2</cell><cell>41.5</cell></row><row><cell>2</cell><cell>10.0</cell><cell>12.0</cell><cell>17.7</cell><cell>24.3</cell><cell>6.1</cell><cell>26.4</cell></row><row><cell>4</cell><cell>21.8</cell><cell>30.1</cell><cell>37.4</cell><cell>61.0</cell><cell>47.4</cell><cell>66.4</cell></row><row><cell>6</cell><cell>15.7</cell><cell>12.4</cell><cell>29.0</cell><cell>34.2</cell><cell>52.5</cell><cell>50.7</cell></row><row><cell>9</cell><cell>11.5</cell><cell>10.1</cell><cell>10.7</cell><cell>1.67</cell><cell>13.4</cell><cell>80.5</cell></row><row><cell>12</cell><cell>70.4</cell><cell>65.9</cell><cell>37.7</cell><cell>72.1</cell><cell>71.1</cell><cell>89.3</cell></row><row><cell>25</cell><cell>12.0</cell><cell>21.4</cell><cell>38.5</cell><cell>87.3</cell><cell>84.2</cell><cell>88.9</cell></row><row><cell>26</cell><cell>22.1</cell><cell>26.9</cell><cell>20.1</cell><cell>7.1</cell><cell>43.5</cell><cell>15.6</cell></row><row><cell>Avg</cell><cell>21.8</cell><cell>23.8</cell><cell>26.7</cell><cell>40.2</cell><cell>44.4</cell><cell>48.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 7 :</head><label>7</label><figDesc>Accuracy comparison of AU detection on DISFA dataset</figDesc><table><row><cell>AU</cell><cell cols="6">LSVM APL [22] DRML [22] FVGG E-Net EAC</cell></row><row><cell>1</cell><cell>21.6</cell><cell>32.7</cell><cell>53.3</cell><cell>82.7</cell><cell>75.1</cell><cell>85.6</cell></row><row><cell>2</cell><cell>15.8</cell><cell>27.8</cell><cell>53.2</cell><cell>83.6</cell><cell>82.5</cell><cell>84.9</cell></row><row><cell>4</cell><cell>17.2</cell><cell>37.9</cell><cell>60.0</cell><cell>74.1</cell><cell>74.5</cell><cell>79.1</cell></row><row><cell>6</cell><cell>8.7</cell><cell>13.6</cell><cell>54.9</cell><cell>64.2</cell><cell>77.4</cell><cell>69.1</cell></row><row><cell>9</cell><cell>15.0</cell><cell>64.4</cell><cell>51.5</cell><cell>87.1</cell><cell>84.0</cell><cell>88.1</cell></row><row><cell>12</cell><cell>93.8</cell><cell>94.2</cell><cell>54.6</cell><cell>67.8</cell><cell>70.1</cell><cell>90.0</cell></row><row><cell>25</cell><cell>3.4</cell><cell>50.4</cell><cell>45.6</cell><cell>78.6</cell><cell>73.8</cell><cell>80.5</cell></row><row><cell>26</cell><cell>20.1</cell><cell>47.1</cell><cell>45.3</cell><cell>61.7</cell><cell>68.6</cell><cell>64.8</cell></row><row><cell>Avg</cell><cell>27.5</cell><cell>46.0</cell><cell>52.3</cell><cell>74.9</cell><cell>75.7</cell><cell>80.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 8 :</head><label>8</label><figDesc>AU occurrence rates in DISFA dataset</figDesc><table><row><cell>AU1</cell><cell>AU2</cell><cell>AU4</cell><cell>AU6</cell><cell>AU9</cell><cell>AU12</cell><cell>AU25</cell><cell>AU26</cell></row><row><cell cols="7">4.9% 4.3% 15.2% 7.8% 4.1% 12.88% 27.7%</cell><cell>8.8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 9 :</head><label>9</label><figDesc>MSE and PCC evaluation of AU intensity estimation</figDesc><table><row><cell></cell><cell></cell><cell cols="2">MSE evaluation</cell><cell></cell><cell></cell><cell></cell><cell>PCC evaluation</cell><cell></cell><cell></cell></row><row><cell>AU</cell><cell cols="9">bs-geo [38] bs-app [38] sCNN [41] iEAC bs-geo [38] bs-app [38] sCNN [41] MLKR [39] iEAC</cell></row><row><cell>AU6</cell><cell>1.004</cell><cell>1.366</cell><cell>1.287</cell><cell>0.864</cell><cell>0.698</cell><cell>0.644</cell><cell>0.664</cell><cell>0.763</cell><cell>0.766</cell></row><row><cell>AU10</cell><cell>0.897</cell><cell>1.209</cell><cell>1.249</cell><cell>0.82</cell><cell>0.757</cell><cell>0.686</cell><cell>0.735</cell><cell>0.752</cell><cell>0.779</cell></row><row><cell>AU12</cell><cell>0.738</cell><cell>1.092</cell><cell>0.928</cell><cell>0.851</cell><cell>0.816</cell><cell>0.768</cell><cell>0.788</cell><cell>0.865</cell><cell>0.820</cell></row><row><cell>AU14</cell><cell>1.227</cell><cell>1.526</cell><cell>1.686</cell><cell>1.24</cell><cell>0.650</cell><cell>0.521</cell><cell>0.591</cell><cell>0.477</cell><cell>0.516</cell></row><row><cell>AU17</cell><cell>0.806</cell><cell>0.819</cell><cell>0.757</cell><cell>0.57</cell><cell>0.184</cell><cell>0.225</cell><cell>0.329</cell><cell>0.548</cell><cell>0.671</cell></row><row><cell>Mean</cell><cell>0.934</cell><cell>1.202</cell><cell>1.181</cell><cell>0.869</cell><cell>0.621</cell><cell>0.569</cell><cell>0.621</cell><cell>0.681</cell><cell>0.71</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 10</head><label>10</label><figDesc></figDesc><table><row><cell></cell><cell cols="6">: F1 score comparison of AU detection on partially</cell></row><row><cell cols="4">occluded faces of BP4D dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell>AU</cell><cell cols="6">Lower Upper Right Left None EAC</cell></row><row><cell>1</cell><cell>31.8</cell><cell>27.4</cell><cell>31.4</cell><cell>25.2</cell><cell>27.7</cell><cell>39.0</cell></row><row><cell>2</cell><cell>30.0</cell><cell>31.6</cell><cell>34.6</cell><cell>32.4</cell><cell>4.5</cell><cell>35.2</cell></row><row><cell>4</cell><cell>21.8</cell><cell>29.1</cell><cell>21.1</cell><cell>28.7</cell><cell>22.8</cell><cell>48.6</cell></row><row><cell>6</cell><cell>70.5</cell><cell>54.9</cell><cell>39.7</cell><cell>52.9</cell><cell>64.8</cell><cell>76.1</cell></row><row><cell>7</cell><cell>72.3</cell><cell>74.4</cell><cell>66.4</cell><cell>70.1</cell><cell>58.6</cell><cell>72.9</cell></row><row><cell>10</cell><cell>77.0</cell><cell>64.6</cell><cell>60.9</cell><cell>62.6</cell><cell>52.6</cell><cell>81.9</cell></row><row><cell>12</cell><cell>75.0</cell><cell>67.6</cell><cell>57.9</cell><cell>59.9</cell><cell>56.9</cell><cell>86.2</cell></row><row><cell>14</cell><cell>58.5</cell><cell>51.6</cell><cell>48.2</cell><cell>45.7</cell><cell>44.5</cell><cell>58.8</cell></row><row><cell>15</cell><cell>15.0</cell><cell>14.8</cell><cell>7.3</cell><cell>18.4</cell><cell>3.8</cell><cell>37.5</cell></row><row><cell>17</cell><cell>58.1</cell><cell>39.3</cell><cell>38.7</cell><cell>37.8</cell><cell>6.0</cell><cell>59.1</cell></row><row><cell>23</cell><cell>28.6</cell><cell>18.9</cell><cell>27.1</cell><cell>12.5</cell><cell>13.14</cell><cell>35.9</cell></row><row><cell>24</cell><cell>16.3</cell><cell>13.0</cell><cell>4.3</cell><cell>7.6</cell><cell>4.9</cell><cell>35.8</cell></row><row><cell>Avg</cell><cell>46.3</cell><cell>40.6</cell><cell>36.5</cell><cell>37.8</cell><cell>29.8</cell><cell>55.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 12</head><label>12</label><figDesc></figDesc><table><row><cell cols="5">: AU detection results on BP4D images with large</cell></row><row><cell cols="2">head poses: all 9 poses</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">FERA Baseline</cell><cell cols="2">Our pEAC-Net</cell></row><row><cell>AU</cell><cell>F1</cell><cell>Accuracy</cell><cell>F1</cell><cell>Accuracy</cell></row><row><cell>1</cell><cell>15.4</cell><cell>57.0</cell><cell>27.2</cell><cell>90.3</cell></row><row><cell>4</cell><cell>17.2</cell><cell>52.0</cell><cell>33.2</cell><cell>88.3</cell></row><row><cell>6</cell><cell>56.4</cell><cell>67.6</cell><cell>69.9</cell><cell>85.3</cell></row><row><cell>7</cell><cell>72.7</cell><cell>64.2</cell><cell>80.8</cell><cell>85.4</cell></row><row><cell>10</cell><cell>69.2</cell><cell>63.8</cell><cell>83.4</cell><cell>88.3</cell></row><row><cell>12</cell><cell>64.7</cell><cell>66.0</cell><cell>80.2</cell><cell>88.2</cell></row><row><cell>14</cell><cell>62.2</cell><cell>62.2</cell><cell>62.1</cell><cell>78.6</cell></row><row><cell>15</cell><cell>14.6</cell><cell>30.7</cell><cell>25.1</cell><cell>90.7</cell></row><row><cell>17</cell><cell>22.4</cell><cell>48.5</cell><cell>34.2</cell><cell>78.2</cell></row><row><cell>23</cell><cell>20.7</cell><cell>37.3</cell><cell>26.1</cell><cell>89.1</cell></row><row><cell cols="2">Avg 41.6</cell><cell>54.9</cell><cell>52.2</cell><cell>86.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 13 :</head><label>13</label><figDesc>Comparison of F1 scores between the FERA baseline and our pEAC-Net on faces with the 9 poses</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">FERA Baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Our pEAC-Net</cell><cell></cell><cell></cell><cell></cell></row><row><cell>AU</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell></row><row><cell>1</cell><cell cols="18">10.3 15.0 13.6 19.3 19.6 17.1 18.0 14.5 12.3 18.8 37.5 22.2 10.1 40.0 66.7 40.1 33.3 40.0</cell></row><row><cell>4</cell><cell cols="9">15.0 15.9 14.8 19.1 19.0 18.3 20.2 20.2 17.5</cell><cell>7.0</cell><cell cols="7">40.0 33.3 40.0 30.7 66.7 25.0 18.2</cell><cell>8.0</cell></row><row><cell>6</cell><cell cols="18">50.5 55.7 13.4 68.9 74.7 72.4 56.0 53.2 49.3 66.7 66.7 72.2 60.8 68.2 80.0 69.2 75.0 52.1</cell></row><row><cell>7</cell><cell cols="18">72.1 72.9 41.3 74.6 79.7 78.7 71.6 74.7 75.8 66.7 62.9 73.1 75.7 85.7 85.7 83.7 84.7 73.3</cell></row><row><cell>10</cell><cell cols="18">55.4 71.0 64.2 77.7 77.6 75.0 63.9 67.9 65.9 85.0 69.2 76.9 83.3 90.9 75.0 62.8 92.3 82.8</cell></row><row><cell>12</cell><cell cols="18">52.2 67.8 18.4 78.6 80.9 77.1 59.6 63.8 60.1 74.3 68.5 85.1 74.2 86.7 66.7 64.3 84.2 72.0</cell></row><row><cell>14</cell><cell cols="2">51.5 56.3</cell><cell>9.0</cell><cell cols="15">67.5 72.4 74.4 61.9 67.0 67.8 66.7 51.1 51.2 68.9 59.3 50.0 60.0 61.2 74.1</cell></row><row><cell>15</cell><cell cols="10">13.1 15.0 14.2 14.6 14.6 14.6 14.3 15.9 15.2 57.1</cell><cell>5.1</cell><cell cols="4">36.4 25.0 28.6 10.2</cell><cell>7.0</cell><cell cols="2">40.0 11.8</cell></row><row><cell>17</cell><cell cols="18">17.3 25.1 23.5 24.2 24.6 24.1 22.0 21.1 19.5 23.5 36.4 34.7 35.3 46.2 50.0 58.8 21.4 44.4</cell></row><row><cell>23</cell><cell cols="11">22.7 22.9 19.9 20.8 19.6 16.6 20.1 20.1 20.8 44.4 36.3</cell><cell>7.3</cell><cell cols="2">57.1 47.1</cell><cell>6.3</cell><cell>9.0</cell><cell cols="2">46.2 11.7</cell></row><row><cell cols="19">Avg 36.0 41.8 23.2 46.5 48.2 46.8 40.8 41.8 40.4 50.3 46.9 48.5 52.1 58.3 54.1 46.4 55.7 46.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 14 :</head><label>14</label><figDesc>Comparison of accuracy scores between the FERA baseline and our pEAC-Net on faces with the 9 poses .6 51.6 71.2 73.0 73.0 60.8 53.3 52.4 76.0 54.0 58.0 82.0 56.00 98.0 76.0 62.0 86.0 15 39.8 21.8 10.2 67.8 21.5 12.1 36.8 28.5 37.8 94.0 88.0 86.0 88.0 90.0 94 92.0 88.0 70.0 17 69.9 46.8 24.6 74.1 59.8 52.2 46.4 30.6 32.3 74.0 58.0 70.0 78.0 72.0 88.0 86.0 56.0 70.0 23 27.1 27.5 55.4 50.8 46.6 59.4 34.9 18.9 15.4 90.0 86.0 80.0 94.0 82.0 94.0 90.0 86.0 70.0 Avg 50.7 50.5 44.4 67.3 62.4 60.5 55.4 52.0 51.3 83.2 73.4 76.4 85.8 78.8 95.0 85.0 79.6 79.2</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">FERA Baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Our pEAC-Net</cell><cell></cell><cell></cell><cell></cell></row><row><cell>AU</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell></row><row><cell>1</cell><cell cols="13">25.2 35.9 32.9 56.3 58.0 50.0 72.9 90.2 91.5 82.0 80.0 72.0 88.0</cell><cell>82.0</cell><cell cols="4">98.0 94.0 92.0 88.0</cell></row><row><cell>4</cell><cell cols="13">30.0 37.2 14.3 45.6 55.4 62.7 50.1 81.0 92.0 90.0 82.0 84.0 94.0</cell><cell>82.0</cell><cell cols="4">94.0 88.0 82.0 70.0</cell></row><row><cell>6</cell><cell cols="13">73.2 72.7 69.6 81.3 82.9 79.1 66.7 47.3 35.7 82.0 80.0 80.0 82.0</cell><cell>72.0</cell><cell cols="4">94.0 84.0 76.0 78.0</cell></row><row><cell>7</cell><cell cols="13">60.6 64.2 53.9 69.5 73.8 72.8 61.0 60.4 61.7 74.0 60.0 72.0 82.0</cell><cell>80.0</cell><cell cols="4">98.0 86.0 82.0 84.0</cell></row><row><cell>10</cell><cell cols="13">56.3 65.0 71.1 74.9 72.2 67.5 61.2 56.0 50.3 88.0 68.0 76.0 88.0</cell><cell>88.0</cell><cell cols="4">96.0 74.0 90.0 90.0</cell></row><row><cell>12</cell><cell cols="13">62.7 70.7 60.9 81.0 81.1 76.4 63.6 53.9 43.6 82.0 78.0 86.0 82.0</cell><cell>84.0</cell><cell cols="4">96.0 80.0 82.0 86.0</cell></row><row><cell>14</cell><cell cols="2">61.8 62</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 15 :</head><label>15</label><figDesc>AU detection results on BP4D faces with large head poses using pEAC-Net with a face frontalization preprocessing</figDesc><table><row><cell>F1 Score</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>IEEE TRANS. ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGEMENT</head><p>This work is supported by the National Science Foundation through Award EFRI -1137172, and VentureWell (formerly NCIIA) through Award 10087-12. The material is also based upon the work supported in part by the National Science Foundation under grants CNS-1629898 and CNS-1205664.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erika</forename><forename type="middle">L</forename><surname>Rosenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Deep Feature based Multi-kernel Learning Approach for Video Emotion Recognition</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farnaz</forename><surname>Abtahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM on International Conference on Multimodal Interaction</title>
		<meeting>the 2015 ACM on International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="483" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02274</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Saliency detection by multi-context deep learning</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1265" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fully automatic facial action unit detection and temporal analysis</title>
		<author>
			<persName><forename type="first">Michel</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 Conference on Computer Vision and Pattern Recognition Workshop (CVPRW&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="149" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multiconditional Latent Variable Model for Joint Facial Action Unit Detection</title>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Eleftheriadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ognjen</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3792" to="3800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A dynamic texturebased approach to recognition of facial actions and their temporal models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Koelstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">Y</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions onPattern Analysis Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1940" to="1954" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">EmotioNet: An Accurate, Real-Time Algorithm for the Automatic Annotation of a Million Facial Expressions in the</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Benitez-Quiroz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ramprakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleix</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint patch and multi-label learning for facial action unit detection</title>
		<author>
			<persName><forename type="first">Kaili</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Sheng</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yongqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangfei</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2015. 2013</date>
			<biblScope unit="page" from="3304" to="3311" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE International Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep learning the dynamic appearance and shape of facial action units</title>
		<author>
			<persName><forename type="first">Michel</forename><surname>Shashank</surname></persName>
		</author>
		<author>
			<persName><surname>Valstar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Selective transfer machine for personalized facial action unit detection</title>
		<author>
			<persName><forename type="first">Wen</forename><forename type="middle">-</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffery</forename><forename type="middle">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3515" to="3522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Facial action unit event detection by cascade of tasks</title>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Sheng</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffery</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2400" to="2407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Confidence preserving machine for facial action unit detection</title>
		<author>
			<persName><forename type="first">Jiabei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Sheng</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3622" to="3630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Constrained joint cascade regression framework for simultaneous facial action unit recognition and facial landmark detection</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">How much training data for facial action unit detection?</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laszlo</forename><forename type="middle">A</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torre</forename><surname>De La</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face and Gesture Recognition (FG), 2015 11th IEEE International Conference and Workshops on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Action unit intensity estimation using hierarchical partial least squares</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziad</forename><surname>Al-Halah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hazam</forename><surname>Kemal Ekenel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face and Gesture Recognition (FG), 2015 11th IEEE International Conference and Workshops on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep Region and Multi-Label Learning for Facial Action Unit Detection</title>
		<author>
			<persName><forename type="first">Kaili</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Sheng</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3391" to="3399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploiting sparsity and co-occurrence structure for action unit recognition</title>
		<author>
			<persName><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Vasisht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Kapoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face and Gesture Recognition (FG), 2015 11th IEEE International Conference and Workshops on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Auaware deep networks for facial expression recognition</title>
		<author>
			<persName><forename type="first">Mengyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face and Gesture Recognition (FG), 2013 10th IEEE International Conference and Workshops on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName><forename type="first">Vahid</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1867" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">BP4D-Spontaneous: a high-resolution spontaneous 3D dynamic facial expression database</title>
		<author>
			<persName><forename type="first">Xing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaun</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="692" to="706" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zara</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition-Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Disfa: A spontaneous facial action intensity database</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mavadati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">H</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="151" to="160" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fera 2015-second facial expression recognition and analysis challenge</title>
		<author>
			<persName><forename type="first">Michel</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">M</forename><surname>Timur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Mehu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face and Gesture Recognition (FG), 2015 11th IEEE International Conference and Workshops on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">From facial parts responses to face detection: A deep learning approach</title>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen-Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3676" to="3684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automatically recognizing facial expression: Predicting engagement and frustration</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Grafsgaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">B</forename><surname>Wiggins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristy</forename><forename type="middle">Elizabeth</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">N</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Educational Data Mining</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deepface: closing the gap to human-level performance in face verification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">FERA 2017-Addressing Head Pose in the Third Facial Expression Recognition and Analysis Challenge</title>
		<author>
			<persName><forename type="first">Michel</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrique</forename><surname>Sanchez-Lozano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laszlo</forename><forename type="middle">A</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04174</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multimodal spontaneous emotion corpus for human behavior analysis</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><forename type="middle">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Umur</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaun</forename><surname>Ciftci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiyuan</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><forename type="middle">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3438" to="3446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-view facial expression recognition</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhou</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 8th International Conference on Automatic Face and Gesture Recognition (FGR08)</title>
		<imprint>
			<date type="published" when="2008-09">Sept. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A 3D facial expression database for facial behavior research</title>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhou</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Rosato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 7th International Conference on Automatic Face and Gesture Recognition (FG06)</title>
		<imprint>
			<date type="published" when="2006-04">April 2006</date>
			<biblScope unit="page" from="211" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fera 2015-second facial expression recognition and analysis challenge</title>
		<author>
			<persName><forename type="first">Michel</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">M</forename><surname>Timur Almaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Mehu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face and Gesture Recognition (FG), 2015 11th IEEE International Conference and Workshops on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Facial action unit intensity prediction via hard multi-task metric learning for kernel regression</title>
		<author>
			<persName><forename type="first">Jeremie</forename><surname>Nicolle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Chetouani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face and Gesture Recognition (FG), 2015 11th IEEE International Conference and Workshops on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cross-dataset learning and person-specific normalisation for automatic action unit detection</title>
		<author>
			<persName><forename type="first">Tadas</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marwa</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face and Gesture Recognition (FG), 2015 11th IEEE International Conference and Workshops on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep learning based facs action unit occurrence and intensity estimation</title>
		<author>
			<persName><forename type="first">Amogh</forename><surname>Gudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Emrah</forename><surname>Tasli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><forename type="middle">M</forename><surname>Den Uyl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Maroulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face and Gesture Recognition (FG), 2015 11th IEEE International Conference and Workshops on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Action Unit Detection with Region Adaptation, Multi-labeling Learning and Optimal Temporal Fusing</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farnaz</forename><surname>Abtahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03067</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2017)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
