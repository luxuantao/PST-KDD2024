<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pooling in image representation: The visual codeword point of view</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">S</forename><surname>Avila</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Pierre et Marie Curie</orgName>
								<orgName type="institution" key="instit2">UPMC-Sorbonne Universities</orgName>
								<address>
									<addrLine>LIP6, 4 place Jussieu</addrLine>
									<postCode>75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Federal University of Minas Gerais</orgName>
								<orgName type="institution" key="instit2">NPDI Lab -DCC/UFMG</orgName>
								<orgName type="institution" key="instit3">Belo Horizonte</orgName>
								<address>
									<region>MG</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">N</forename><surname>Thome</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Pierre et Marie Curie</orgName>
								<orgName type="institution" key="instit2">UPMC-Sorbonne Universities</orgName>
								<address>
									<addrLine>LIP6, 4 place Jussieu</addrLine>
									<postCode>75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">M</forename><surname>Cord</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Pierre et Marie Curie</orgName>
								<orgName type="institution" key="instit2">UPMC-Sorbonne Universities</orgName>
								<address>
									<addrLine>LIP6, 4 place Jussieu</addrLine>
									<postCode>75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">E</forename><surname>Valle</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">State University of Campinas</orgName>
								<orgName type="institution" key="instit2">RECOD Lab -DCA/FEEC</orgName>
								<orgName type="institution" key="instit3">UNICAMP</orgName>
								<address>
									<settlement>Campinas</settlement>
									<region>SP</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">A</forename><forename type="middle">De A</forename><surname>Araújo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Federal University of Minas Gerais</orgName>
								<orgName type="institution" key="instit2">NPDI Lab -DCC/UFMG</orgName>
								<orgName type="institution" key="instit3">Belo Horizonte</orgName>
								<address>
									<region>MG</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Université Pierre et Marie Curie</orgName>
								<orgName type="institution" key="instit2">UPMC-Sorbonne Universities</orgName>
								<address>
									<addrLine>LIP6, 4 place Jussieu</addrLine>
									<postCode>75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">Federal University of Minas Gerais</orgName>
								<orgName type="institution" key="instit2">NPDI Lab -DCC/UFMG</orgName>
								<orgName type="institution" key="instit3">Belo Horizonte</orgName>
								<address>
									<region>MG</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pooling in image representation: The visual codeword point of view</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CFFF325C9AF56AA067D347175E1BCA2A</idno>
					<idno type="DOI">10.1016/j.cviu.2012.09.007</idno>
					<note type="submission">Received 17 November 2011 Accepted 26 September 2012</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Image classification Image representation Pattern recognition</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we propose BossaNova, a novel representation for content-based concept detection in images and videos, which enriches the Bag-of-Words model. Relying on the quantization of highly discriminant local descriptors by a codebook, and the aggregation of those quantized descriptors into a single pooled feature vector, the Bag-of-Words model has emerged as the most promising approach for concept detection on visual documents. BossaNova enhances that representation by keeping a histogram of distances between the descriptors found in the image and those in the codebook, preserving thus important information about the distribution of the local descriptors around each codeword. Contrarily to other approaches found in the literature, the non-parametric histogram representation is compact and simple to compute. BossaNova compares well with the state-of-the-art in several standard datasets: MIRFLICKR, ImageCLEF 2011, PASCAL VOC 2007 and 15-Scenes, even without using complex combinations of different local descriptors. It also complements well the cutting-edge Fisher Vector descriptors, showing even better results when employed in combination with them. BossaNova also shows good results in the challenging real-world application of pornography detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual information, in the form of digital images and videos, has become so omnipresent in computer databases and repositories, that it can no longer be considered a ''second class citizen'', eclipsed by textual information. In that scenario, image classification and visual concept detection are becoming critical tasks. In particular, the pursuit of automatic identification of complex semantical concepts represented in images has motivated researchers in areas as diverse as Information Retrieval, Computer Vision, Image Processing and Artificial Intelligence <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. Though the ultimate goal of reliable concept identification remains elusive, the last decade has witnessed two important breakthroughs in that direction: the development of very discriminant low-level local features, inspired on Computer Vision approaches; and the emergence of mid-level aggregate representations, based on the quantization of those features, in the so-called ''Bag-of-Words'' model <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. Those advances in feature extraction and representation have closely followed a previous turning point on statistical learning, represented by the maturity of kernel methods and support vector machines <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Our aim is content-based concept detection in images and videos, with a novel representation that enriches the Bag-of-Words model. Bag-of-Words representations can be understood as the application of two critical steps <ref type="bibr" target="#b8">[9]</ref>: coding, which quantizes the image local features according to a codebook; and spatial pooling, which summarizes the codes obtained into a single feature vector. Traditionally, the coding step simply associates the image local descriptors to the closest element in the codebook, and the spatial pooling takes the average of those codes over the entire image.</p><p>Several trends are discernible on the mid-level representations recently proposed: the preservation of global spatial information, leading to the almost universal association to the Spatial Pyramids scheme <ref type="bibr" target="#b9">[10]</ref>; and the concern with the integrity of the low-level descriptor information, which culminates in representations inspired from signal reconstruction. As a consequence, we have observed the steady inflation of feature vector sizes.</p><p>In this work, we propose BossaNova, a mid-level representation based on a histogram of distances between the descriptors found in the image and those in the codebook. The fundamental change is an enhancement of the pooling in order to preserve a richer portrait of the information gathered during the coding: instead of compacting all information pertaining to a codeword into a single scalar, the proposed pooling scheme produces a distance distribution. In order to accomplish that goal, BossaNova departs from the parametric models commonly found in the literature (e.g., <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>), by employing histograms. That non-parametric approach allows us to conciliate the need to preserve low-level descriptor information and keeping the mid-level feature vector at a reasonable size. A preliminary version of the representation <ref type="bibr" target="#b15">[16]</ref> has allowed us to gain several insights into the benefits of the nonparametric choice and to explore the compromises between the opposite goals of discrimination versus generalization, representativeness versus compactness. Since BossaNova embodies the accomplishment of that preliminary work, this paper presents several new aspects:</p><p>An extensive theoretical analysis (presented in Sections 2 and 3) that gives a unified perspective of both BoW and BOSSA models. A novel coding scheme based on semi-soft codeword assignment, that avoids the instability inherent to the use of hard codeword assignment on high-dimensional spaces. A novel normalization scheme, that renders the representation more robust to the sparsity brought by large codebooks. A new weighting scheme to balance the importance of different parts of the representation is also presented. A novel extension with the complementary state-of-the-art mid-level representation based on Fisher Vectors.</p><p>The remainder of this text is organized as follows. In Section 2, we formalize the Bag-of-Words model for images, and give a summary survey of the most important work which lead to its development, concluding with a brief commentary on the current state of the art. In Section 3, we give a detailed description of our approach BossaNova, both in terms of theoretical background and implementation, including a unified theoretical framework for BoW and BOSSA. In Section 4, we present our empirical results, comparing of BossaNova performance with state-of-the-art methods in several dataset, validating its enhancements over the previously proposed Bossa representation, and studying its behavior as its key parameters change. In Section 5, we explore BossaNova in the real world application of pornography detection, which because of its high-level conceptual nature, involves large intra-class appearance variations. With Section 6, we conclude the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In this section, we survey the literature on image representations based on the Bag-of-Words model. As its name suggests, that model is inspired from textual Information Retrieval, which contributed important ideas throughout its evolution. Here, however, we restrict our scope to works on visual information. Instead of an exhaustive survey, we opt for a more formal development: our aim is to lay out the mathematical cornerstones common to all BoW representations, exploring how those cornerstones have been established in early works, and how they are evolving in very recent works.</p><p>In order to get the mid-level feature vector, the standard processing pipeline follows three steps <ref type="bibr" target="#b8">[9]</ref>: (i) low-level local descriptor extraction, (ii) coding, and (iii) pooling. Classification algorithms (like support vector machines) are then trained on the mid-level feature vectors obtained.</p><p>As far as we know, the NeTra toolbox <ref type="bibr" target="#b16">[17]</ref> was the first work to follow that scheme, proposing dense grids of color points, and unsupervised learning to build the codebook, using the LBG algorithm. The Retin system <ref type="bibr" target="#b17">[18]</ref> is based upon a similar scheme, using local Gabor feature vectors, and learning the codebook with Kohonen self-organized maps. The technique was definitively popularized with the intuitive ''Video Google'' formalism <ref type="bibr" target="#b4">[5]</ref>, which makes explicit the parallels between the BoW models for visual and textual documents, while employing SIFT local features, and building the BoW using a three-step pipeline.</p><p>Let us denote the ''Bag-of-Features'' (BoF), i.e., the unordered set of local descriptors extracted from an image, by X ¼ fx j g; j 2 f1; . . . ; Ng, where x j 2 R d is a local feature vector and N is the number of local features (either fixed grid points, either detected points of interest) in the image.</p><p>Many feature detectors have been proposed to get salient areas, affine regions and points of interest <ref type="bibr" target="#b18">[19]</ref> on images. However, in contrast to the task of matching a specific image or object, methods for category classification show better performance when using a uniform feature sampling over a dense grid on the image <ref type="bibr" target="#b19">[20]</ref>.</p><p>Let us suppose we have obtained (e.g., by unsupervised learning) a codebook, or visual dictionary C ¼ fc m g; c m 2 R d ; m 2 f1; . . . ; Mg, where M is the number of codewords, or visual words. C represents the matrix d Â M of all codeword coordinates, one codeword per column. Note that the codewords are in the same space of the low-level local descriptors ðR d Þ. Note also that, unless otherwise noted, all our vectors are column vectors.</p><p>Obtaining the codebook is essential for the ''Bag-of-Words'' (BoW) model, since the representation will be based on the codewords. Currently, the vast majority of methods obtains the codebook using unsupervised learning over a sample of local descriptors from the training images, usually using k-means. However, more sophisticated techniques have been proposed to learn the codebook, with both supervised <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21]</ref>, and unsupervised (e.g., restricted Boltzmann machines <ref type="bibr" target="#b21">[22]</ref>) learning.</p><p>The construction of the BoW representation can be decomposed into the sequential steps of coding and pooling <ref type="bibr" target="#b8">[9]</ref>. The coding step projects the local descriptors onto the codebook elements; while the pooling step aggregates the projected codes into a vector. The global aim is gaining invariance to nuisance factors (positioning of the objects, changes in the background, small changes in appearance, etc.), while preserving the discriminating power of the local descriptors.</p><p>The coding step can be modeled by a function f: R d ! R M as f(x j ) = a j (see Fig. <ref type="figure">1</ref>). It can be understood as an activation function for the codebook, activating each of the codewords according to the local descriptor. In the classical BoW representation, the coding function activates only the codeword closest to the descriptor, assigning zero weight to all others:</p><formula xml:id="formula_0">a m;j ¼ 1 iff m ¼ arg min k2f1;...;Mg kx j À c k k 2 2</formula><p>where a m,j is the mth component of the encoded vector a j . That scheme corresponds to a hard coding or hard quantization over the dictionary. The resulting binary code is very sparse, but suffers from instabilities when the descriptor being coded is on the boundary of proximity of several codewords <ref type="bibr" target="#b5">[6]</ref>.</p><p>Because of that, alternatives to that standard scheme have been recently developed. Sparse coding <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23]</ref> modifies the optimization scheme by jointly considering reconstruction error and sparsity of Fig. <ref type="figure">1</ref>. Matrix representation H of the BoW model, with columns X related to the low-level local descriptors, and rows C related to the codebook. The coding function f for a given descriptor x j corresponds to column j, and may be interpreted as how much that descriptor activates each codeword. The pooling function g for a given visual word c m corresponds to a summarization of row m and may be interpreted as the aggregation of the activations of that codeword. The final representation is a vector z (not shown), containing those aggregated activations, for each codeword. the code, using the well-known property that regularization with the ' 1 -norm, for a sufficiently large regularization parameter k, induces sparsity:</p><formula xml:id="formula_1">a j ¼ arg min a kx j À Ck 2 2 þ kkak 1</formula><p>One of the strengths of that approach is that one can learn the dictionary with the same scheme, but optimizing over C and a. Efficient tools have been proposed to get tractable solutions <ref type="bibr" target="#b20">[21]</ref>.</p><p>Another possibility is soft coding <ref type="bibr" target="#b5">[6]</ref>. It is based on a soft assignment to each visual word, weighted by distances/similarities between descriptors and codewords. Soft assignment results in dense code vectors, which is undesirable, among other reasons, because it leads to ambiguities due to the superposition of the components in the pooling step. Therefore, several intermediate strategies -known as ''semi-soft'' coding -have been proposed, often applying the soft assignment only to the k nearest neighbors (k-NN) of the input descriptor <ref type="bibr" target="#b23">[24]</ref>.</p><p>The pooling step takes place after the coding, and can also be represented by a function, such as g: fa j g j21;...;N ! R M as: g({a j }) = z which can be used to get a single scalar value on each row of the H matrix (see Fig. <ref type="figure">1</ref>). Traditional BoW considers the sum pooling operator:</p><formula xml:id="formula_2">gðfa j gÞ ¼ z : 8m; z m ¼ X N j¼1 a m;j<label>ð1Þ</label></formula><p>When using sparse or soft coding, the max pooling is often preferred<ref type="foot" target="#foot_0">1</ref> :</p><formula xml:id="formula_3">z : 8m; z m ¼ max j2f1;...;Ng a m;j</formula><p>The vector z 2 R M is the final image representation, used for classification. Max pooling is common in biologically-inspired computational architectures, such as convolutional neural networks <ref type="bibr" target="#b24">[25]</ref> and HMAX models <ref type="bibr" target="#b25">[26]</ref>. Extensions to the traditional pooling operation have been also proposed recently. The most powerful technique is the Spatial Pyramid Matching (SPM) strategy <ref type="bibr" target="#b9">[10]</ref>. It is a pooling that considers a fixed predetermined spatial image pyramid. The previously described pooling is operated over each block of the pyramid, then concatenated into a large vector (#blocks Â M).</p><p>Boureau et al. <ref type="bibr" target="#b26">[27]</ref> stepped forward in considering both SPM and local pooling over the codes. That latter work also gives a new perspective to other recent powerful approaches VLAD <ref type="bibr" target="#b13">[14]</ref> or Super-Vector Coding <ref type="bibr" target="#b14">[15]</ref> as specific pooling operations. In those aggregated methods, locality constraints are incorporated during the pooling step: only descriptors belonging to the same clusters are pooled together.</p><p>Another BoW improvement belonging to the aggregated coding class is the Fisher Kernel approach proposed by Perronnin et al. <ref type="bibr" target="#b10">[11]</ref>. It is based on the use of the Fisher kernel framework popularized by Jaakkola and Haussler <ref type="bibr" target="#b27">[28]</ref>, with Gaussian Mixture Models (GMM) estimated over the whole set of images. That approach may be viewed as a generalization to the second order of the Super-Vector approach <ref type="bibr" target="#b14">[15]</ref>. Indeed, the final image representation is also a vector concatenating vectors over each mixture term. Picard and Gosselin <ref type="bibr" target="#b28">[29]</ref> generalize it to higher orders, but computational complexity, vector size and difficulty in estimating higher-order moments with confidence, limit the practicality of pushing the orders beyond the second.</p><p>In a previous work <ref type="bibr" target="#b15">[16]</ref>, we had proposed another extension to pooling, called BOSSA, by considering no more a scalar output for each row as in Eq. ( <ref type="formula" target="#formula_2">1</ref>), but a vector, summarizing the distribution of the a m,j . That strategy allows keeping more information, related to the confidence of the detection of each visual word c m in the image.</p><p>We propose in this work a new pooling method, called Bossa-Nova, that generalizes our previous pooling strategy, with a new assignment and normalization strategy that makes the representation more effective, while keeping all advantages of BOSSA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">BossaNova scheme</head><p>BossaNova is based upon a new pooling strategy, and integrates several improvements over the original BOSSA representation <ref type="bibr" target="#b15">[16]</ref>. We open this section by reviewing the pooling formalism for the three representations: BoW, BOSSA and BossaNova, allowing us to contrast the differences between the latter two and the former. We then detail the improvements of BossaNova over BOSSA, including the weighting scheme to balance the word-count (BoW) and the distances-histogram (BOSSA) parts of the vectors, the semi-soft coding scheme and the improved normalization. The implementation details are then briefly discussed. Finally, we conclude this section with an analysis of how BossaNova and Fisher Vectors can be expected to complement each other well when combined into a single feature vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">New pooling formalism</head><p>As hinted in the previous section, for representations based on the BoW model, the pooling step is critical. It compacts all the information contained in the individually encoded local descriptors into a single feature vector, thus producing a mid-level feature convenient for use with classifiers like SVM.</p><p>When pooling, there is a compromise between the invariance obtained and the ambiguities introduced. Invariance to different backgrounds or object positioning is obtained because the final codewords will be activated despite the precise positioning of the descriptors. However, since all activations are combined, ambiguities can arise, if different concepts represented in the image (e.g., a person and a car) end up activating sets of codewords that overlap too much. The following step of classification will have difficulty in separating those concepts.</p><p>One way to mitigate that problem is to preserve more information about the encoded descriptors during the pooling step. Instead of a simple sum of the activations, like in the classical BoW, more detailed information can be kept.</p><p>In BOSSA and BossaNova, we propose estimating the distribution of the descriptors around each codeword. We choose a nonparametric estimation of the descriptors distribution, by computing a histogram of distances between the descriptors found in the image and each codebook element.</p><p>More formally, and keeping the same notations used in Section 2 and in Fig. <ref type="figure">1</ref>, the proposed pooling function g estimates the probability density function of a m : g(a m ) = pdf(a m ), by computing the following histogram of distances z m,k :</p><formula xml:id="formula_4">g : R N ! R B a m ! gða m Þ ¼ z m z m;k ¼ card x j ja m;j 2 k B ; k þ 1 B ! k B P a min m and k þ 1 B 6 a max m<label>ð2Þ</label></formula><p>where B denotes the number of bins of each histogram z m , and a min m ; a max m Â Ã limits the range of distances for the descriptors considered in the histogram computation. On BOSSA, only the upper range was limited, but we have since observed that, due to a known effect of the ''curse of dimensionality'', distances between descriptors seldom fall below a certain range, making some bins of BOSSA histograms always zero. The double range makes better use of the representation space.</p><p>The function g represents the discrete (over B bins) density distribution of the distances a m,j between the codeword c m and the local descriptors of an image. That is illustrated in Fig. <ref type="figure" target="#fig_0">2</ref>. Note that a m,j , introduced in Fig. <ref type="figure">1</ref>, traditionally quantifies a similarity between the descriptor x j and the codeword c m , while in our pooling formalism, it represents a dissimilarity (indeed, a distance). That choice makes illustrations clearer and more intuitive, and no generality is lost, since estimating a similarity pdf for a m,j from our model is straightforward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">BossaNova improvements</head><p>The novel pooling strategy presented in previous section is the basis of both BOSSA and BossaNova. The latter, however, presents several improvements over the former, which we explore now. The effectiveness of those improvements is evaluated in Section 4.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Weighting BoW and BOSSA</head><p>The main result of the pooling step is a local histogram z m for each codeword c m . We concatenate those histograms to form the feature vector. In addition, we propose incorporating an additional scalar N m for each codeword, counting the number of local descriptors falling close to that codeword. That value corresponds to a classical BoW term, accounting for a raw measure of the presence of the visual word c m in the image. Previously <ref type="bibr" target="#b15">[16]</ref>, we simply concatenated the BoW and BOSSA components, implicitly assigning equal importance to the components z m and N m .</p><p>We propose here to weight z m and N m , setting thus the relevance of each term in BossaNova. We apply a weight factor s to each N m value, rewriting our image representation z as: z ¼ ½½z m;k ; sN m T ; ðm; kÞ 2 f1; Mg Â f1; Bg ð 3Þ</p><p>As illustrated in Fig. <ref type="figure" target="#fig_4">3</ref>, z is a vector of size D = M Â (B + 1). The weighted factor s is learned via cross-validation on a training/validation sub-set.</p><p>Eq. ( <ref type="formula">3</ref>) lets us interpret BossaNova as an improvement over the BoW representation, through the use of an additional term coming from the a more informative pooling function. Recently, that idea of enriching BoW representations with extra knowledge from the set of local descriptors has been explored on several representations. It can be found, for example, on Fisher Vectors <ref type="bibr" target="#b11">[12]</ref> and Super-Vector Coding <ref type="bibr" target="#b14">[15]</ref>. Those works, however, opt by paramet-ric models that lead to very high-dimensional image representations. By using a simple histogram of distances to capture the relevant information, our approach remains very flexible and keeps the representation compact. In the experiments (Section 4), we show that we can reach performances close to the ones of the Fisher Vectors, with a much smaller descriptor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Localized soft-assignment coding</head><p>Our previous work <ref type="bibr" target="#b15">[16]</ref> employed hard assignment on the coding step, for both the BOSSA (histograms) and BoW (raw counts) components of the feature vector. In BossaNova, we propose a soft-assignment coding strategy, for both components. Soft-assignment is chosen because it has been shown to considerably enhance the results over hard assignment, without incurring the computational costs of sparse coding <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23]</ref>. In addition, a recent evaluation <ref type="bibr" target="#b23">[24]</ref> reveals that well-designed soft coding can perform as well or even better than sparse coding.</p><p>Soft-assignment coding attenuates the effect of coding errors induced by the quantization of the descriptor space. Different soft coding strategies have been presented and evaluated by Gemert et al. <ref type="bibr" target="#b5">[6]</ref>, the most successful approach being the one they call ''codeword uncertainty''. Other authors <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30]</ref> point out the importance of locality in the coding, an issue we will address in Section 3.4, and that leads us to a localized, ''semi-soft'' coding scheme.</p><p>Thus, like Liu et al. <ref type="bibr" target="#b23">[24]</ref>, we consider only the k-nearest visual words in coding a local descriptor, and we perform for those neighbors a ''codeword uncertainty'' soft assignment. Let us consider a given local descriptors x j , and its k closest visual words c m . The soft assignment a m,j to the visual word c m is computed as follows: </p><formula xml:id="formula_5">a m;j ¼ exp Àb m d 2 ðx j ;cmÞ P K m 0 ¼1 exp Àb m 0 d 2 ðx j ;c m 0 Þ<label>ð4Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Normalization</head><p>In BossaNova, the third improvement over BOSSA <ref type="bibr" target="#b15">[16]</ref> is a twostep signature normalization.</p><p>The first step in that normalization is motivated by the following observation: as the number of visual words increases, BOSSA becomes sparser. That is also the case for most BoW-like representations: Perronnin et al. <ref type="bibr" target="#b11">[12]</ref> have also observed that effect, which is indeed a direct consequence of the ratio between the number of local descriptors and the mid-level representation vector size. They observe that similarities become less reliable when the vector signatures become too sparse, proposing a power normalization to alleviate that drawback. Therefore, we choose to incorporate that normalization into the BossaNova representation.</p><p>Formally, the power normalization consists of applying the following operator in each histogram bin z m,k :</p><formula xml:id="formula_6">hðz m;k Þ ¼ signðz m;k Þjz m;k j d ; 0 &lt; d 6 1<label>ð5Þ</label></formula><p>In our experiments, we consider d = 0.5, which has shown in preliminary experiments to provide better performance. The second step is an ' 2 -normalization applied to the final vector. In contrast, BOSSA did not implement the power normalization and employed an ' 1 block-norm strategy instead of the ' 2 . Our experiments show that the change has improved the results. m , the idea is to consider only descriptors that are ''close enough'' to the center, and to discard the remaining ones. For a min m , the idea is to avoid the empty regions that appear around each codeword, in order to avoid wasting space in the final descriptor.</p><p>The fact that descriptors seldom, if ever, fall close to the codewords is a counter-intuitive consequence of the geometry of high-dimensional spaces. Fig. <ref type="figure" target="#fig_2">4</ref> illustrates the phenomenon, displaying the average density of SIFT descriptors on the neighborhood of codewords, in MIRFLICKR dataset. It is clear that the number of SIFT descriptors for a min m &lt; 0:4 Á r m is negligible (see Section 4.8.4). Note that the parameters may act jointly to the locality constraints defined in Section 3.2.2: a descriptor x j that is the k-NN from a center c m is not considered for generating the signature if</p><formula xml:id="formula_7">d 2 ðx j ; c m Þ &gt; a max m .</formula><p>In BossaNova, a min m and a max m are set up differently for each codeword c m . Since our visual dictionary is created using k-means, we take advantage of the knowledge about the ''size'' of the clusters, given by the standard deviations r m . We set up the bounds as a min m ¼ k min Á r m and a max m ¼ k max Á r m , as shown in Fig. <ref type="figure" target="#fig_3">5</ref>. In practice, the three parameters of the BossaNova become B (M being fixed), k min and k max .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">BossaNova and aggregated methods: complementary</head><p>Although alternative pooling strategies have recently been explored (e.g. max pooling), average pooling remains the most com-monly employed scheme for aggregating local descriptors. As pointed out by Boureau et al. <ref type="bibr" target="#b26">[27]</ref>, incorporating locality constraints during coding or pooling is mandatory for extracting a meaningful image representation when using average pooling. That is especially the case for state-of-the-art local descriptors such as SIFT or HOG that cannot be averaged without considerably loosing information. For example, if we do not consider any coding step (i.e. M = d, f = I d in Fig. <ref type="figure">1</ref>), aggregating SIFT or HOG descriptors   with average pooling would produce a global histogram of gradient orientation for the image. Thus, if care is not taken, the pooling step makes the representation uninformative for classification.</p><p>In aggregated methods such as Fisher Vectors <ref type="bibr" target="#b10">[11]</ref>, VLAD <ref type="bibr" target="#b13">[14]</ref> or Super-Vector Coding <ref type="bibr" target="#b14">[15]</ref>, the locality constraints are mainly incorporated during the pooling step. In that class of methods, since the coding step is much more accurate (for each codeword, a vector is stored instead of a simple scalar value with standard BoW coding schemes), the authors often claim that they can afford to use a codebook of limited size (e.g. M $ 100) and get very good performances. However, reducing the codebook size intrinsically increases the hypervolume of each codeword in the descriptor space. That naturally decreases the range of the locality constraints that can be incorporated during pooling: all local descriptors falling into a (now larger) codeword are averaged together. Therefore, we argue that average pooling used in aggregate methods may lack locality, as soon as the distribution of local descriptors becomes multi-modal inside a codeword. For example, Fisher Vectors model the distribution of local descriptors in each codeword with a single Gaussian. When that Gaussian assumption does not hold, the pooled representation may be unrepresentative of the local descriptor statistics. That is illustrated in Fig. <ref type="figure" target="#fig_5">6</ref>. Fig. <ref type="figure" target="#fig_5">6a</ref> shows an illustration of a cluster around codeword c m with local descriptors x j having two different modes (i.e. sub-clusters). When averaging the codes during pooling, we get for c m a pooled vector P j c m À x j that is far away from any local descriptors x j . In contrast to that, BossaNova representation uses additional locality constraints during the pooling, since only the feature vectors x j that are close to the codewords c m are pooled together, as shown in Fig. <ref type="figure" target="#fig_5">6b</ref>. The pooled representation is thus able to capture the statistics of the local descriptors.</p><p>On the other hand, when the Gaussian assumption is fulfilled, aggregated methods provide powerful signatures thanks to the improved accuracy of the coding step. The two mid-level representations are thus complementary, and we can expect improving performances by combining them. In a supervised learning task, the classifier is supposed to select the most relevant pooling strategy for each cluster, in a discriminative manner. As shown in the experiments (Section 4), we report that combining BossaNova with Fisher Vectors indeed improves classification performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head><p>We choose four standard datasets to perform our experiments: MIRFLICKR <ref type="bibr" target="#b30">[31]</ref>, ImageCLEF 2011 <ref type="bibr" target="#b31">[32]</ref>, PASCAL VOC 2007 <ref type="bibr" target="#b32">[33]</ref> and 15-Scenes <ref type="bibr" target="#b9">[10]</ref>. Each dataset is briefly described at the moment of its first use, in Section 4.2.</p><p>After describing our experimental setup, we show our results, which we organized in three groups. First, a comparison with state-of-the-art methods, including both experiments with methods we have reimplemented ourselves, and published results reported in the literature. In order to make that comparison possible, we follow carefully the experimental protocol of each dataset. In what concerns the methods we reimplemented, we compare BossaNova to our previous work, BOSSA <ref type="bibr" target="#b15">[16]</ref>, but also to one of the best methods currently available, the Fisher Vectors <ref type="bibr" target="#b11">[12]</ref>. In order to provide a control baseline, we also employ the classical Bag-of-Words (BoW).</p><p>Next, we evaluate the impact of the three proposed improvements of BossaNova over BOSSA, analyzing the isolated and joint impact of each enhancement on the new representation.</p><p>Finally, we explore the key aspects of the parametric space of our technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup</head><p>The low-level feature extraction has a big influence in the quality of the results, and, if not controlled can easily become a nuisance factor in the experiments. Therefore, to make the comparisons fair, we use the same low-level descriptors for all techniques evaluated. For all datasets, we have extracted SIFT descriptors <ref type="bibr" target="#b33">[34]</ref> on a dense spatial grid, with the step-size corresponding to half of the patch-size, over 8 scales separated by a factor of 1.2, and the smallest patch-size set to 16 pixels.</p><p>As a result, roughly 8000 local descriptors are extracted from each image of MIRFLICKR, ImageCLEF 2011 and PASCAL VOC 2007 datasets, and close to 2000 local descriptors from each image of 15-Scenes. The dimensionality of the SIFT is reduced from 128 to 64 by using Principal Component Analysis (PCA). That setup for local descriptor extraction proves to give very good performances in standard image datasets, as reported in <ref type="bibr" target="#b19">[20]</ref>.</p><p>To learn the codebooks, we apply the k-means clustering algorithm with Euclidean distance over one million randomly sampled descriptors. For Fisher Vectors (FV) <ref type="bibr" target="#b11">[12]</ref>, the descriptor distribution is modeled using a GMM, whose parameters (w, l, R) are also trained over one million randomly sampled descriptors, using an EM algorithm. For all mid-level representations, we incorporate spatial information using the standard spatial pyramidal matching scheme <ref type="bibr" target="#b9">[10]</ref>. In total, eight spatial cells are extracted for MIR-FLICKR, ImageCLEF 2011 and PASCAL VOC 2007, 21 spatial cells for 15-Scenes.</p><p>One-versus-all classification is performed by SVM classifiers. We use a linear SVM for FV, since it is well known that non-linear kernels do not improve performances for those representations, see <ref type="bibr" target="#b11">[12]</ref>. For Bag-of-Words <ref type="bibr" target="#b4">[5]</ref>, BOSSA <ref type="bibr" target="#b15">[16]</ref> and BossaNova, we use a non-linear Gauss-' 2 kernel. Kernel matrices are computed as exp(Àcd(x, x 0 )) with d being the distance and c being set to the inverse of the pairwise mean distances.</p><p>Significance tests for the differences between the means were performed using a t-test, paired over the dataset classes. For the analysis of the improvements brought by each enhancement of BossaNova over BOSSA, we have also employed a factorial ANOVA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison of state-of-the-art methods</head><p>We compare BossaNova to other representations, perform our own re-implementation of those techniques. The methods chosen were: BossaNova (BN), the method proposed in this paper. BOSSA <ref type="bibr" target="#b15">[16]</ref>, our previous work, which BossaNova improves, chosen to empirically validate those improvements. Fisher Vectors (FV) <ref type="bibr" target="#b11">[12]</ref>, one of the best mid-level representations currently reported in the literature <ref type="bibr" target="#b19">[20]</ref>. The combination BN + FV, chosen to evaluate the methods' complementarity (Section 3.4). Bag-of-Words (BoW) <ref type="bibr" target="#b4">[5]</ref>. A classical histogram of visual words, obtained with hard quantization coding and average pooling; it constitutes a control baseline for the other methods.</p><p>When available, we also report the best results available for each dataset. That allows us to evaluate other recent methods that build upon the standard baseline BoW, e.g. recent methods using sparse coding and max pooling <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>It is important to note that, although we have chosen for BN parameters we believed were good, in the interest of a fair comparison, we have not fine-tuned it for each dataset. Therefore, the numbers reported do not represent the limit of the performance achievable by the method (in a few cases higher results are achieved in this same paper in Section 4.8, were we do explore the parameters more thoroughly). It is also important to consider that most results reported in the literature employ different lowlevel descriptor extraction schemes, and that this step has a large impact on the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results for MIRFLICKR</head><p>The MIRFLICKR dataset <ref type="bibr" target="#b30">[31]</ref> contains 25,000 images collected from the Flickr photo sharing social network. 2 The dataset provides metadata, in the form of associated labels and tags, but we consider only the visual content for the feature extraction. The dataset is split into a collection of 15,000 training images and 10,000 test images, as defined by the standardized challenge ''Visual Concept/Topic Recognition'' <ref type="bibr" target="#b30">[31]</ref>. All images are manually annotated for 38 concepts, including categories that describe the presence of specific object (car, bird, dog), categories that are concrete but less spatially localized (clouds, night, sky), and more abstract categories (indoor, food, structures). The classification performance is evaluated using the standard metric for this dataset, the Mean Average Precision (MAP).</p><p>Table <ref type="table">1</ref> shows the results over MIRFLICKR, and details the parameter settings for each method. Among the methods we have tested ourselves, all differences are significant with at least 99% confidence, except for BN and FV, whose difference is not significant. Results published in the literature, unfortunately, do not include significance tests or confidence intervals.</p><p>We can notice that all the recent methods improve the classification performance over the BoW baseline: the BOSSA representation published in <ref type="bibr" target="#b15">[16]</ref> outperforms BoW with 1.2% absolute improvement (2.3% relative improvement). That illustrates the relevance of improving the pooling scheme, as we do in this paper.</p><p>If we now compare the BOSSA to the proposed BossaNova, we observe an increase from 52.7% to 54.4%. That shows the benefits brought out by the weight factor, soft coding and new normalization proposed in Section 3.2 (further explored in Section 4.7). Furthermore, BossaNova is tied with Fisher Vectors, the current stateof-the-art method. Note that our representation (12,288 dimensions for each spatial cell) is about 3 times smaller than FV (32,768 dimensions for each spatial cell). Also, we observe that our method is better than Fisher Vectors for 22 out of 38 concepts. <ref type="foot" target="#foot_2">3</ref>Finally, we can notice the considerable improvement obtained when combining BossaNova and FV, reaching a MAP of 56.0%. This corresponds to a remarkable success of the complementariness of BossaNova and Fisher Vector representations. The combination surpasses both individual methods for 31 out of 38 concepts while performing similarly for the seven remaining concepts.</p><p>From the literature, we choose the baseline dataset result <ref type="bibr" target="#b34">[35]</ref>, and the best, as far as we know, result published <ref type="bibr" target="#b35">[36]</ref>. The baseline performances <ref type="bibr" target="#b34">[35]</ref> are quite low, 14% below our re-implementation of the classical BoW (Table <ref type="table">1</ref>). The main reason is the features employed there, global image descriptors, which are much outperformed by highly discriminant local descriptors such as SIFT.</p><p>In comparison to <ref type="bibr" target="#b35">[36]</ref>, BossaNova performs better for 29 out of 38 concepts, and its MAP increases from 53.0% to 56.0%. It is notable BossaNova employs only SIFT to build the mid-level representation, while <ref type="bibr" target="#b35">[36]</ref> combines 15 different image representations, including SIFT.</p><p>To the best of our knowledge, ours is the best result reported to date on MIRFLICKR dataset, using a single low-level feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results for ImageCLEF 2011</head><p>The ImageCLEF 2011 <ref type="foot" target="#foot_3">4</ref> contains four main tasks: Medical Retrieval, Photo Annotation, Plant identification and Wikipedia Retrieval. We present our results for the ImageCLEF 2011 Photo Annotation task <ref type="bibr" target="#b31">[32]</ref>, which consists of 18,000 Flickr images. The training set of 8000 images includes annotations, EXIF data, and Flickr user tags, but we consider only the visual content for the feature extraction. The annotation challenge is performed on 10,000 images. The image set is annotated with 99 concepts that describe the scene (indoor, outdoor, landscape), depicted objects (car, animal, person), the representation of image content (portrait, graffiti, art), events (travel, work) or quality issues (overexposed, underexposed, blurry). The metric employed is the MAP.</p><p>Table <ref type="table">2</ref> gives the results, both the ones implemented and tested by us, and the ones reported on literature. With at least 99% confidence, all differences were significant for the methods we have implemented ourselves. Once again, we note a considerable improvement of performance from BOSSA to BossaNova, from 32.9% to 35.3%. Furthermore, the combination of BossaNova and Fisher Vector representations outperforms the other methods.</p><p>We also compare our results with those of the five best systems reported in the literature. In the ImageCLEF 2011 Photo Annotation task, each group registered for the challenge is restricted to a maximum of 5 runs. Table <ref type="table">2</ref> shows the best run for each group, with the restriction to results that employed only the visual information.</p><p>The best system during the competition <ref type="bibr" target="#b40">[41]</ref> reported 38.8% MAP, employing non-sparse multiple kernel learning and multitask learning. They apply SIFT and color channel combinations to 2 http://www.flickr.com.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Image classification MAP (%) results of BossaNova (BN), standard implemented stateof-the-art representations and published methods on MIRFLICKR <ref type="bibr" target="#b30">[31]</ref>. (1) BoW: M = 4096; (2) BOSSA: M = 2048, B = 6, k min = 0, k max = 2, as in <ref type="bibr" target="#b15">[16]</ref>, (3) FV: GMM with 256 Gaussians, as in <ref type="bibr" target="#b11">[12]</ref>; (4) BN: M = 4096, B = 2, k min = 0, k max = 2, s = 10 À3 . Best results are in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAP (%)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implemented methods</head><p>BoW <ref type="bibr" target="#b4">[5]</ref> 51.5 BOSSA <ref type="bibr" target="#b15">[16]</ref> 52.7 FV <ref type="bibr" target="#b11">[12]</ref> 54.3 BN (ours) 54.4 BN + FV (ours) 56.0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Published results</head><p>Huiskes et al. <ref type="bibr" target="#b34">[35]</ref> 37.5 Guillaumin et al. <ref type="bibr" target="#b35">[36]</ref> 53.0 build different extensions of the BoW models with respect to sampling strategies and BoW mappings. The system of <ref type="bibr" target="#b39">[40]</ref>   <ref type="bibr" target="#b36">[37]</ref> is based on the BoW model. They apply feature fusion of the opponent SIFT descriptor and the GIST descriptor. Moreover, a postclassification processing step is incorporated in order to refine classification results based on rules of inference and exclusion between concepts. As we can notice, all those top-performing systems employ complex combinations of several low-level features to achieve their good results.</p><p>In view of that, our results of 35.3% for BN, and 38.4% for BN + FV, are remarkably good, since we employ just SIFT descriptors. Moreover, the performance our method can be further improved by feature combination expansions <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Results for PASCAL VOC 2007</head><p>The PASCAL VOC 2007 dataset <ref type="bibr" target="#b32">[33]</ref> consists of 9,963 images, from 20 object categories. Those images are split into three subsets: training (2,501 images), validation (2,510 images) and test (4,952 images). The following experimental results are obtained on trainval/test sets. To tune the C-SVM parameter, we use the train/val sets. Classification performance is measured by MAP across all classes, a metric chosen to facilitate the comparison with the literature.</p><p>Table <ref type="table">3</ref> shows the results, detailing the parameter settings for each method. For the methods we (re-) implemented, all differences were significant with at least 99% confidence. Again, we observe a considerable improvement of performance from BOSSA to BossaNova, from 54.4% to 58.5%. The combination BN + FV still outperforms all other methods. For some categories its absolute improvement in MAP reached up to 10% (up to 37.7% of relative improvement), especially well-known challenging ones (e.g. bottle, cow, sheep).</p><p>Table <ref type="table">3</ref> also shows the comparison with published results. The comparison with <ref type="bibr" target="#b12">[13]</ref> is particularly relevant, because we employ the same low-level descriptor extraction as them, although our representation ends up being more compact. The LLC method of <ref type="bibr" target="#b29">[30]</ref> is evaluated with HOG descriptors. LLC was also evaluated on extremely dense SIFT descriptors (sampling step of 3 pixels, compared to 16 used in our experiments), roughly 70,000 per image, obtaining a MAP of 53.8% with a codebook of 4,000 words <ref type="bibr" target="#b19">[20]</ref>.</p><p>The best reproducible results currently known are 58.2% for Super-Vector (SV) coding <ref type="foot" target="#foot_4">5</ref> and 61.7% for FV <ref type="bibr" target="#b19">[20]</ref>. Those results are encouraging, since the SIFT descriptors employed on those experiments are extremely dense. As observed by Chatfield et al. themselves, denser sampling yield higher classification accuracies for all techniques, a result which we have also observed in preliminary tests on BOSSA and BossaNova.</p><p>Again, the combined BN + FV results show the complementarity of those methods. The performance is practically tied with the best reproducible results reported in the literature, but using SIFT features nearly 10Â less dense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Results for 15-Scenes</head><p>The 15-Scenes dataset <ref type="bibr" target="#b9">[10]</ref> contains 4485 images of 15 natural scene categories. Following the standard experimental setup, we randomly select 100 images per class for training and the remaining images for testing. We average the classification accuracy over 30 random train/test splits.</p><p>Results, both the ones implemented and tested by us, and the ones reported on the literature are shown in Table <ref type="table">4</ref>. Once again, we observe that BossaNova method surpasses BOSSA with a absolute improvement of 2.4% (relative improvement of 2.9%), validating the improvements of the method. In comparison to Fisher Vectors, BossaNova classification performance is peculiarly inferior: this is the dataset showing the largest difference. We must note for one single class (industrial) our result is much lower than expected, weighting down the averages. When combining Bossa-Nova and Fisher Vector methods, that issue is solved, and the combination is better than FV in isolation. The combination BN + FV surpasses both individual methods for 13 out of 15 natural scene categories.</p><p>We also compare our results with those of the best systems reported in the literature. BossaNova outperforms considerably the methods reported by <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b9">[10]</ref>, using improved BoWs with sparse coding and max pooling.</p><p>If we take our best result (88.9%), we observe that it is better than the result of <ref type="bibr" target="#b12">[13]</ref>, obtained with spatial Fisher Vectors. Again, that comparison is relevant since both Krapac et al. and we employ similar low-level local descriptor extractions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Image classification MAP (%) results of BossaNova (BN), standard implemented stateof-the-art representations and published methods on ImageCLEF 2011 Photo Annotation Task <ref type="bibr" target="#b31">[32]</ref>. (1) BoW: M = 4096; (2) BOSSA: M = 4096, B = 2, k min = 0, k max = 2; (3) FV: GMM with 256 Gaussians, as in <ref type="bibr" target="#b11">[12]</ref>; (4) BN: M = 4096, B = 2, k min = 0.4, k max = 2, s = 10 À3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAP (%)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implemented methods</head><p>BoW <ref type="bibr" target="#b4">[5]</ref> 31.2 BOSSA <ref type="bibr" target="#b15">[16]</ref> 32.9 FV <ref type="bibr" target="#b11">[12]</ref> 36.8 BN (ours) 35.3 BN + FV (ours) 38.4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Published results</head><p>Mbanya et al. <ref type="bibr" target="#b36">[37]</ref> 33.5 Le and Satoh <ref type="bibr" target="#b37">[38]</ref> 33.7 van de Sande and Snoek <ref type="bibr" target="#b38">[39]</ref> 36.7 Su and Jurie <ref type="bibr" target="#b39">[40]</ref> 38.2 Binder et al. <ref type="bibr" target="#b40">[41]</ref> 38.8</p><p>Table <ref type="table">3</ref> Image classification MAP (%) results of BossaNova (BN), standard implemented stateof-the-art representations and published methods on PASCAL VOC 2007 dataset <ref type="bibr" target="#b32">[33]</ref>.</p><p>(1) BoW: M = 4096; (2) BOSSA: M = 4096, B = 2, k min = 0, k max = 2; (3) FV: GMM with 256 Gaussians, as in <ref type="bibr" target="#b11">[12]</ref>; (4) BN: M = 4096, B = 2, k min = 0.4, k max = 2 s = 10 À3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAP (%)</head><p>Implemented methods BoW <ref type="bibr" target="#b4">[5]</ref> 53.2 BOSSA <ref type="bibr" target="#b15">[16]</ref> 54.4 FV <ref type="bibr" target="#b11">[12]</ref> 59.5 BN (ours) 58.5 BN + FV (ours) 61.6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Published results</head><p>Krapac et al. <ref type="bibr" target="#b12">[13]</ref> 56.7 Wang et al. <ref type="bibr" target="#b29">[30]</ref> 59.3 Chatfield et al. <ref type="bibr" target="#b19">[20]</ref> 61.7 Fig. <ref type="figure" target="#fig_7">7</ref> illustrates the confusion matrix for our best classification performance. Not surprisingly, confusion occurs between indoor classes (e.g. bedroom, living room, kitchen), urban architecture classes (e.g. inside city, street, tall building) and also between natural classes (e.g. coast, open country). Our result reaches near state-ofthe-art performance for that dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">BOSSA to BossaNova improvements analysis</head><p>In Section 4.2, we show that BossaNova shows good results when compared to state-of-the-art works, and, in particular, that it considerably outperforms BOSSA <ref type="bibr" target="#b15">[16]</ref>. To further quantify the performance gains, we propose in this section to evaluate the individual performance increase brought out by each of the three proposed improvements: learning the weighting between BoW and BOSSA (Section 3.2.1), using a localized-soft coding strategy (Section 3.2.2), and applying a new normalization to the final vector representation (Section 3.2.3).</p><p>The joint activation of the three steps leads to eight different configurations where the performance of the corresponding mid-level representation is evaluated (denoted as Weight, Soft and Norm in Tables <ref type="table" target="#tab_2">5</ref> and<ref type="table">6</ref>). Then, we apply a statistical t-test to attest the significance of the difference between two given configurations. We perform the test for paired samples, i.e., we evaluate the performance of two configurations on N different folds of train/test images and compute the difference between the performance metrics on each fold. The confidence interval (CI) for the average difference is computed using a Student-t model <ref type="bibr" target="#b43">[44]</ref>, and the difference is considered significative if the interval does not include zero. For the tests in this section, we ask for a confidence of 95%.</p><p>Table <ref type="table" target="#tab_2">5</ref> shows the evaluation of the eight different configurations on the 15-Scenes database, for N = 30 folds. We can see that the performances, measured by accuracy, monotonically increase from configuration 1 (BOSSA) to 8 (BossaNova). When only one improvement is added to BOSSA (configurations 2, 3 and 4), the performance gain is always significant. That already proves the relevance of the three modifications proposed in this paper. When two improvements are incorporated, the performances increase are significant when compared to BOSSA <ref type="bibr" target="#b0">(1)</ref>, but also when compared to configurations with only one improvement: configurations 5, 6 and 7 are all significantly better than the best configuration with one improvement <ref type="bibr" target="#b3">(4)</ref>. When all three improvements are added, the difference is again significant: 8 is better than 6 and 7, the best configurations including two improvements.</p><p>Testing just for the difference between BOSSA (1) and BossaNova <ref type="bibr" target="#b7">(8)</ref> allows us to set the confidence to the large value of 99.9% and still obtain a CI of <ref type="bibr">[1.18, 3.72]</ref>, showing that the difference is significant.</p><p>We apply the same setup on the PASCAL VOC database. Here, the performance metric is the MAP, computed over the 20 classes for N = 10 folds. <ref type="foot" target="#foot_6">6</ref> The same conclusions apply: each improved configuration significantly outperforms its predecessor, as illustrated in Table <ref type="table">6</ref>.</p><p>Again, the difference between BOSSA (1) and BossaNova ( <ref type="formula">8</ref>) is significant with a large confidence. For 99.9% confidence, the CI is <ref type="bibr">[3.21, 4.65]</ref>.</p><p>For both datasets we have also tested the influence of the parameters using a factorial analysis of variance (ANOVA) <ref type="bibr" target="#b43">[44]</ref>. In both cases, the models obtained were highly significant (with confidence above 99.9%) for all three improvements, confirming the results above. In addition, the ANOVA allows to measure the relative impact of each influence. For the more challenging VOC dataset, the soft assignment coding explains almost 48% of the improvements, while the two-step normalization explains about 31%. The BoW-BOSSA weighting, in isolation, is responsible for only 3% of the variation, but there is a cross-effect between the weighting and the soft coding that accounts for another 9%. The impact of the coding is clearly the largest, but the importance of the normalization is quite surprising, especially considering the optimization of that step is often neglected in the literature. The impact of codebook size M on BossaNova classification performance is shown on Table <ref type="table">7</ref>, which clearly shows that larger codebooks lead to higher accuracy. BoW performance, however, stops growing at 4096 visual words.</p><p>As stated in Section 4.2, the performances reported in Table <ref type="table">1</ref> correspond to a BossaNova with good parameters, but not strongly fine-tuned. Therefore, our representation can reach an even higher evaluating performances. Here, we use random folds to obtain the necessary number of runs for statistical analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head><p>Image classification accuracy (%) results of BossaNova (BN), standard implemented state-of-the-art representations and published methods on 15-Scenes dataset <ref type="bibr" target="#b9">[10]</ref>.</p><p>(1) BoW: M = 4096; (2) BOSSA: M = 4096, B = 2, k min = 0, k max = 2; (3) FV: GMM with 256 Gaussians, as in <ref type="bibr" target="#b11">[12]</ref>; (4) BN: M = 4096, B = 2, k min = 0, k max = 2, s = 10 À3 . The table shows the means and standard deviations of 30 accuracy measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy (%)</head><p>Implemented methods BoW <ref type="bibr" target="#b4">[5]</ref> 81.1 ± 0.6 BOSSA <ref type="bibr" target="#b15">[16]</ref> 82.9 ± 0.5 FV <ref type="bibr" target="#b11">[12]</ref> 88.1 ± 0.2 BN (ours) 85.3 ± 0.4 BN + FV (ours) 88.9 ± 0.3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Published results</head><p>Yang et al. <ref type="bibr" target="#b22">[23]</ref> 80.3 ± 0.9 Lazebnik et al. <ref type="bibr" target="#b9">[10]</ref> 81.4 ± 0.5 Boureau et al. <ref type="bibr" target="#b8">[9]</ref> 85.6 ± 0.2 Krapac et al. <ref type="bibr" target="#b12">[13]</ref> 88.2 ± 0.6 score of 55.2% with a dictionary of size M = 8192. However, the last improvement from 4096 to 8192 is not that high, suggesting that the growth will soon stop growing. Meanwhile, the representation has doubled in size. Hence, we define as our standard setting M = 4096 in order to get a good tradeoff between effectiveness and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.2.">Comparison with hierarchical BoW</head><p>We contrast BossaNova to a Hierarchical BoW (H-BoW) since there are some similarities between our pooling approach and a 2-step descriptor space clustering. The pooling performed in Bossa-Nova can indeed be regarded as a special form of clustering, where the second-level of clustering corresponds to regions that are equally spaced from the center. On the other hand, in a standard H-BoW, the second-level clusters are similar to the first-level ones (e.g. hyper-sphere, if ' 2 norm is used for clustering).</p><p>We claim that the special shape of the second-level clustering, which is based on the idea of pooling descriptors depending on their similarity to the center, is better founded that a naive 2-level clustering (with Euclidean distance). To achieve that comparison, we build a 2-level hierarchical codebook using BossaNova codebook size (M) at the first-level, and BossaNova histograms bin count plus one (B + 1) at the second-level. That makes the comparison fair, allocating the same size for both representations. For in-stance, BossaNova with a codebook of size M = 4096 and two bins per histogram (B = 2), will be compared with a H-BoW first-level of 4096 and second-level of 3 clusters (both representations are therefore of size 4096 Â 3 Â 8, 8 being the spatial cells of the SPM scheme).</p><p>Table <ref type="table" target="#tab_1">8</ref> compares BossaNova with H-BoW on the MIRFLICKR dataset. For each codebook size, we observe that BossaNova is superior to H-BoW, and that the difference tends to grow as the (first-level) codebook size grows. That confirms the relevance of the improved pooling scheme introduced in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.3.">Bin quantization</head><p>We next investigate how BossaNova classification performance is affected by the number of bins (B). Using M = 4096, the number of bins is varied among 2, 4 and 6. The results of our experiments are shown in Table <ref type="table">9</ref>.</p><p>First, we observe that increasing the number of bins yields a slight amelioration in performance. However, the growth depends on the topic of MIRFLICKR dataset: for 30 out of 38 concepts the performance increases by 0.2%-1.9% and for 3 isolated concepts (bird(r), car(r), sea(r)) the performance decreases slightly, by 0.2%.</p><p>Once again, further investigations will certainly provide optimized parameters but with a higher complexity. We handled default parameters to 2 here in order to get compact representations. For k min = 0.4 and k max = 2, corresponding to 95% of the total SIFT descriptors on the whole dataset, we obtain a MAP = 54.9% which is better than the range of k min = 0 and k max = 2 (MAP = 54.4%). That is in accordance with our intuition in Section 3.3.</p><p>Interestingly, we observe considerable improvements for the most of the concepts (up to 1%) and also a decrease for some ones (up to 0.5%). That suggests that setting a k min and even k max per visual word seems to be useful to exploit as future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.5.">Scalability issues</head><p>When applied in a classification context, the proposed BossaNova representation is used in conjunction to Gauss-' 2 non-linear kernels, because we empirically notice that non-linear features maps boost performances (see Section 4). Non-linear kernels are known to be slower for training and testing. Note that for the databases we evaluate (MIRFLICKR, ImageCLEF 2011, VOC 2007, 15-Scenes), using non-linear kernels was still reasonable for training and testing. However, it becomes intractable for large-scale problems, i.e. databases with more than one million images.</p><p>We want to stress that recent works focused on approximating non-linear kernels by linear ones, by providing approximated features maps <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref>. In most of the case, the approximated representations reach about the same level of performances than the exact kernels. Therefore, we hope that using those strategies upon BossaNova is a viable way to handle large-scale classification tasks. The precise evaluation of those compromises is part of our future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Application: pornography detection</head><p>We have evaluated our approach in a real-world application, pornography detection. Pornography is less straightforward to define than it may seem at first, since it is a high-level semantic category, not easily translatable in terms of simple visual characteristics. Though it certainly relates to nudity, pornography is a different concept: many activities which involve a high degree of body exposure (swimming, boxing, sunbathing, etc.) have nothing to do with it. That is why systems based on skin detection <ref type="bibr" target="#b46">[47]</ref> often accuse false positives in contexts like beach shots or sports.</p><p>A commonly used definition is that pornography is the portrayal of explicit sexual matter with the purpose of eliciting arousal. That raises several challenges. First and foremost what threshold of explicitness must be crossed for the work to be considered pornographic? Some authors deal with that issue by further dividing the classes <ref type="bibr" target="#b47">[48]</ref> but that not only fall short of providing a clear cut definition, but also complicates the classification task. The matter of purpose is still more problematic, because it is not an objective property of the document. Here, we have opted to keep the evaluation conceptually simple, by assigning only two classes (porn and non-porn). On the other hand, we took great care to make them representative.</p><p>The Pornography dataset contains nearly 80 h of 400 pornographic and 400 non-pornographic videos. For the pornography class, we have browsed websites which only host that kind of material (solving, in a way, the matter of purpose). The dataset consists of several genres of pornography and depicts actors of many ethnicities, including multi-ethnic ones. For the non-pornography class, we have browsed general-public purpose video network and selected two samples: 200 videos chosen at random (which we called ''easy'') and 200 videos selected from textual search queries like ''beach'', ''wrestling'', ''swimming'', which we knew would be particularly challenging for the detector (''difficult''). Fig. <ref type="figure" target="#fig_6">8</ref> shows selected frames from a small sample of the dataset, illustrating the diversity of the pornographic videos and the challenges of the ''difficult'' non-pornographic ones.</p><p>We preprocess the dataset by segmenting videos into shots. An industry-standard segmentation software<ref type="foot" target="#foot_7">7</ref> has been used. On average there are 20 shots per video. As it is often done in video analysis, a key frame is selected to summarize the content of the shot into a static image. Although there are sophisticated ways to choose the key frame, in this proof-of-concept application, we opted to simply selected the middle frame of each video shot.</p><p>In the experiments, we follow the experimental setup applied on our previous work on BOSSA <ref type="bibr" target="#b15">[16]</ref>. Our main aim is to compare the performance of our proposed method in this paper with our previous one. As a low-level local descriptor, we employ HueSIFT <ref type="bibr" target="#b48">[49]</ref>, a SIFT variant including color information, which is particularly relevant for our dataset. The 165-dimensional HueSIFT descriptors are extracted densely every 6 pixels. For a fair comparison, we use the same vocabulary M constructed in <ref type="bibr" target="#b15">[16]</ref> by kmeans clustering algorithm, with M fixed as 256.</p><p>For classification, we apply the setup described in Section 4 and we use a 5-fold cross-validation to tune the best C parameter. We report the image classification performance by using the MAP, and the video classification by accuracy rate, where the final video label is obtained by majority voting over the images. Table <ref type="table">10</ref> shows the results of our experiments over Pornography dataset, and details the parameter settings for each method.</p><p>Once again, BossaNova outperforms both BoW and BOSSA representations. Comparing BOSSA with BoW, we already notice a considerable improvement of 3.2% and 4.1% for image and video classification, respectively. If we now compare BossaNova with BOSSA, we also observe a considerable increase of 1.8% and 2.4% for image and video classification, respectively. That confirms the advantages introduced by BossaNova representation.</p><p>Here, it is instructive to study the fail cases. First, we inspect the misclassified non-pornographic videos. That corresponds to very challenging non-pornographic videos: breastfeeding sequences, sequences of children being bathed, and beach scenes. BoW gave a wrong classification for almost all those clips. The analysis of the most difficult pornographic videos revealed that the method has difficult when the videos are of very poor quality (typical of amateur porn, often uploaded from webcams) or when the clip is only borderline pornographic, with few explicit elements. BoW also had difficulty with those clips, misclassifying many of them.</p><p>Moreover, it is interesting to see that for all three methods the video classification scores are inferior to the image classification scores. That can be explained by the fact that some pornographic videos have the additional difficulty of having very few shots with pornographic content (typically 1 or 2 takes among several dialog shots or cut scenes), giving no allowance for classification errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have introduced in this paper a visual data classification scheme based on a novel representation that enriches the Bag-of-Words model.</p><p>BossaNova representation is interesting from the conceptual, technical and empirical points of view. From a conceptual point of view, its elegant non-parametric conception avoids unnecessary hypothesis about the data distribution. From a technical point of view, the simple vector computation, the ease of implementation and the relatively compact feature vector obtained are non-negligi-ble advantages, especially when tackling datasets which are becoming progressively larger in scale and scope. The empirical comparisons in concept detection, both in a very general task using the MIRFLICKR, ImageCLEF 2011, PASCAL VOC 2007 and 15-Scenes datasets, and in a the specialized task of pornography detection, show the advantage of BossaNova when compared to both traditional techniques and cutting-edge approaches.</p><p>In addition, BossaNova geometric properties lead us to predict an interesting complementarity with the Fisher Vector representations, which was confirmed empirically on several standard datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 10</head><p>Comparison of the proposed BossaNova with BOSSA and BoW methods on the Pornography dataset. MAP (%) is computed at image classification level, and Accuracy rate is reported for video classification. For each method, we use their tested configuration parameters, namely (1) BoW: M = 256, (2) BOSSA and BossaNova: M = 256, B = 10, k min = 0, k max = 3. The bold values indicates the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAP (frames)</head><p>Acc. rate (videos) BoW <ref type="bibr" target="#b4">[5]</ref> 91.4 ± 1 83.0 ± 3 BOSSA <ref type="bibr" target="#b15">[16]</ref> 94.6 ± 1 87.1 ± 2 BN (ours) 96.4 ± 1 89.5 ± 1</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>where d 2 (</head><label>2</label><figDesc>x j , c m ) is the (Euclidean) distance between c m and x j . The parameter b m regulates the softness of the soft-assignment (the bigger it is, the hardest the assignment). The main difference between our approach and the one of Liu et al.<ref type="bibr" target="#b23">[24]</ref> is that we allow b m to vary for each codeword, while they use a global b parameter, determined by cross-validation. Since our codewords c m correspond to cluster centers obtained by a k-means algorithm, we take advantage of the standard deviation r m of each cluster c m to setup b m ¼ r À2 m .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. For each center c m , we obtain a local histogram z m . The colors indicate the discretized distances from the center c m to the local descriptors shown by the black dots. For each colored bin z m,k , the height of the histogram is equal to the number of local descriptors x j , whose discretized distance to codeword c m fall into the k th bin. We can note that if B = 1, the histogram z m reduces to a single scalar value counting the number of feature vectors x j falling into center c m . Therefore, the proposed histogram representation can be considered as a consistent generalization of BoW pooling step. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</figDesc><graphic coords="4,59.02,549.58,198.71,120.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Average density of SIFT descriptors in the neighborhood of codewords in MIRFLICKR dataset, showing that descriptors seldom, if ever, are closer than a certain threshold to the codewords. That counter-intuitive phenomenon is a consequence of the ''curse of dimensionality''.</figDesc><graphic coords="5,337.66,282.16,198.71,114.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Illustration of the range of distances a min m ; a max m Â Ã which defines the bounds of the histogram. The hatched area corresponds to the bounds. Local descriptors outside those bounds are ignored.</figDesc><graphic coords="5,344.86,460.52,184.31,154.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Overview of BossaNova vector construction. (a) Extraction of the low-level local descriptors (SIFT) over a dense grid. (b) Codebook/visual vocabulary creation with kmeans on a sample of one million descriptors. (c) Our pooling strategy: computation of local histograms z m for each c m codeword. Localized soft-assignment (''semi-soft assignment'') is used for coding. (d) Counting the number of feature vectors x j falling into each codeword c m (again, using semi-soft assignment). (e) Two-step normalization: power normalization followed by ' 2 -normalization. (f) Weighting of the histogram (z m ) and counting components (N m ), by applying a weight factor s on the latter. (g) Final BossaNova representation.</figDesc><graphic coords="5,87.87,67.92,425.87,141.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Aggregated methods, e.g. Vectors<ref type="bibr" target="#b11">[12]</ref>, may lack locality during pooling for small codebooks, whereas BossaNova does not. In counterpart, aggregated methods are more accurate during the coding steps, making the two representation complementary. See discussion in Section 3.4.</figDesc><graphic coords="6,37.76,584.39,241.26,110.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>4. 8 .</head><label>8</label><figDesc>BossaNova parameter evaluation 4.8.1. Codebook size</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Confusion matrix for the 15-Scenes dataset. The average classification rates for individual classes are listed along the diagonal, and the columns are the true classes.</figDesc><graphic coords="9,54.31,269.01,227.85,228.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>4. 8 . 4 .</head><label>84</label><figDesc>Minimum distance a min m We also study the effects of the minimum distance a min m on Bos-saNova classification performance. Using the test values of Bossa-Nova parameters (i.e., B = 2, M = 4096, k max = 2, and for semi-soft coding k-NN = 10), we set k min based on Fig. 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Illustration of the diversity of the pornographic videos (top row) and the challenges of the ''difficult'' non-pornographic ones (middle row). The easy cases are shown at bottom row. The huge diversity of cases in both pornographic and non-pornographic videos makes that task very challenging.</figDesc><graphic coords="11,59.53,67.92,482.02,183.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>uses many features, such as SIFT, HOG, Texton, Lab-1948, SSIM, and Canny, aggregating them by a BoW into a global histogram. Fisher Vectors and contextual information were used as enhancement of the BoW models. The method of<ref type="bibr" target="#b38">[39]</ref> employs several color SIFT features with Harris-Laplace and dense sampling, and apply the SVM clas-</figDesc><table /><note><p><p><p>sifier. The system of</p><ref type="bibr" target="#b37">[38]</ref> </p>also use numerous features. As global features, they use color moments, color histogram, edge orientation histogram and local binary patterns; and as local features, keypoint detectors such as Harris Laplace, Hessian Laplace, Harris Affine, and dense sampling are used to extract SIFT descriptors. Again, classification is performed with a SVM classifier. The approach of</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 8</head><label>8</label><figDesc>Comparison of BossaNova (BN) wrt Hierarchical BoW performance (MAP (%)) on MIRFLICKR dataset<ref type="bibr" target="#b30">[31]</ref>. BN: B = 2, k min = 0, k max = 2, k-NN = 10, s = 10 À3 .</figDesc><table><row><cell></cell><cell>Codebook size</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1024</cell><cell>2048</cell><cell>4096</cell></row><row><cell>BN</cell><cell>51.8</cell><cell>52.9</cell><cell>54.4</cell></row><row><cell>H-BoW</cell><cell>50.6</cell><cell>51.3</cell><cell>51.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5</head><label>5</label><figDesc>Impact of the proposed improvements to the BossaNova on 15-Scenes. We use M = 4096, B = 2, k min = 0, k max = 2. ''Weight'': the weighted factor s, No = no cross-validation, Yes = cross-validation. ''Soft'': soft assignment coding, No = hard-assignment, Yes = localized soft assignment. ''Norm'': normalization, No = ' 1 block normalization, Yes = power normalization + ' 2 -normalization.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Depending on the sparse optimization scheme, the a m,j values may be negative. If that occurs, the following pooling is usually applied: z: "m, z m = max j2{1,. . .,N} ka m,j k. S. Avila et al. / Computer Vision and Image Understanding xxx (2012) xxx-xxx</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Please cite this article in press as: S.Avila  et al., Pooling in image representation: The visual codeword point of view, Comput. Vis. Image Understand. (2012), http://dx.doi.org/10.1016/j.cviu.2012.09.007</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The detailed per-class performances for all datasets are available at http:// www.npdi.dcc.ufmg.br/bossanova/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>http://www.imageclef.org/2011.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Even if<ref type="bibr" target="#b14">[15]</ref> published on this dataset a score of</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_5"><p>64.0% using SV coding,<ref type="bibr" target="#b19">[20]</ref> show that the SV coding is about 58.2%, and the difference results from non-trivial optimizations not described in their paper, making it extremely hard to reproduce.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6"><p>Note that in the VOC 2007 database, the train/val/test folds are generally fixed for</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_7"><p>http://www.stoik.com/products/svc/.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Florent Perronnin and Jorge Sánchez for the attentive support in understanding their Fisher Vector method. We thank David Picard for the Java Machine Learning library. Funding is provided by CAPES/COFECUB 592/08/10, CNPq 14.1312/2009-2, ANR 07-MDCO-007-03 and FAPESP.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>b Confidence intervals for the MAP differences. The difference is significant if its confidence interval does not contain zero (see text).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 7</head><p>Codebook size impact on BossaNova (BN) and BoW performance (MAP (%)) on MIRFLICKR dataset <ref type="bibr" target="#b30">[31]</ref>.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Content Based Image Retrieval at the End of the Early Years</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Content-based multimedia information retrieval: state of the art and challenges</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Djeraba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>Communications, and Applications</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image retrieval: ideas, influences, and trends of the new age</title>
		<author>
			<persName><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Combining visual dictionary, kernelbased similarity and learning strategy for image category retrieval</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gosselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Philipp-Foliguet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Video Google: a text retrieval approach to object matching in videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Veenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Geusebroek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Word Ambiguity</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<title level="m">Machine Learning in Computer Vision</title>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cunningham</surname></persName>
		</author>
		<title level="m">Machine Learning Techniques for Multimedia: Case Studies on Organization and Retrieval, Cognitive Technologies</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning mid-level features for recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Beyond bags of features: spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Dance, Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Modeling spatial layout with fisher vectors for image categorization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Krapac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeeky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Image classification using super-vector coding of local image descriptors</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">BOSSA: extended BoW formalism for image classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Araújo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">NETRA: A toolbox for navigating large image databases</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Multimedia Systems</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">RETIN: a content-based image indexing and retrieval system</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fournier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Philipp-Foliguet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and Applications Journal, Special issue on image indexation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2/3</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A comparison of affine region detectors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schaffalitzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The devil is in the details: an evaluation of recent feature encoding methods</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Online learning for matrix factorization and sparse coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unsupervised and supervised visual codes with Restricted Boltzmann Machines</title>
		<author>
			<persName><forename type="first">H</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">defense of soft-assignment coding</title>
		<imprint>
			<publisher>ICCV</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Sparse feature learning for deep belief networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Extended coding and pooling in the HMAX model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Theriault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Ask the locals: multi-way local pooling for image recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploiting generative models in discriminative classifiers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving image similarity with vectors of locally aggregated tensors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gosselin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Locality-constrained linear coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Huiskes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The MIR Flickr Retrieval Evaluation</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>MIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The CLEF 2011 photo annotation and conceptbased retrieval tasks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liebetrau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The PASCAL visual object classes challenge 2007 results</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">VLFeat -an open and portable library of computer vision algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Huiskes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
		<title level="m">New trends and ideas in visual concept detection: the mir Flickr retrieval evaluation initiative</title>
		<imprint>
			<publisher>MIR</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Multimodal semi-supervised learning for image classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Sample selection, category specific features and reasoning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mbanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gerke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hentschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ndjiki-Nya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Japan at ImageCLEF 2011 photo annotation task</title>
		<author>
			<persName><forename type="first">D.-D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
		<author>
			<persName><surname>Nii</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The University of Amsterdam&apos;s concept detection system at ImageCLEF</title>
		<author>
			<persName><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Semantic contexts and fisher vectors for the ImageCLEF 2011 photo annotation task</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">The joint submission of the TU Berlin and Fraunhofer FIRST (TUBFI) to the ImageCLEF2011 photo annotation task</title>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kawanabe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An efficient system for combining complementary kernels in complex visual categorization tasks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning geometric combinations of Gaussian kernels with alternating Quasi-Newton algorithm</title>
		<author>
			<persName><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rakotomamonjy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>ESANN</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">The art of computer systems performance analysis: techniques for experimental design, measurement, simulation, and modeling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Efficient additive kernels via explicit feature maps</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Using the Nyström method to speed up kernel machines</title>
		<author>
			<persName><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Screening for objectionable images: a review of skin detection techniques</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Donnellan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molloy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IMVIP</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Bag-of-visual-words models for adult image classification and filtering</title>
		<author>
			<persName><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pimenidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>ICPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Evaluating color descriptors for object and scene recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
