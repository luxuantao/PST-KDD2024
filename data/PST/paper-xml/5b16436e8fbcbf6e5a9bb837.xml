<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Articulated Multi-Instrument 2D Pose Estimation Using Fully Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaofei</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Medical Image Computing</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Umbo Computer Vision Inc</orgName>
								<address>
									<settlement>San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Kurmann</surname></persName>
							<email>thomas.kurmann@artorg.unibe.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Medical Image Computing</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ping-Lin</forename><surname>Chang</surname></persName>
							<email>ping-lin.chang@umbocv.com</email>
							<affiliation key="aff0">
								<orgName type="department">Medical Image Computing</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maximilian</forename><surname>Allan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Medical Image Computing</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Umbo Computer Vision Inc</orgName>
								<address>
									<settlement>San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sebastien</forename><surname>Ourselin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Medical Image Computing</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Umbo Computer Vision Inc</orgName>
								<address>
									<settlement>San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Raphael</forename><surname>Sznitman</surname></persName>
							<email>raphael.sznitman@artorg.unibe.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Medical Image Computing</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Kelly</surname></persName>
							<email>j.d.kelly@ucl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Medical Image Computing</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Division of Surgery and Interventional Science</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Danail</forename><surname>Stoyanov</surname></persName>
							<email>danail.stoyanov@ucl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Medical Image Computing</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Umbo Computer Vision Inc</orgName>
								<address>
									<settlement>San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">P.-L</forename><surname>Chang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">ARTORG Center for Biomed-ical Enginnering Research</orgName>
								<orgName type="institution">University of Bern</orgName>
								<address>
									<settlement>Bern</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Articulated Multi-Instrument 2D Pose Estimation Using Fully Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0716E11E26146D68554466E178374549</idno>
					<idno type="DOI">10.1109/TMI.2017.2787672</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMI.2017.2787672, IEEE Transactions on Medical Imaging IEEE TRANSACTIONS ON MEDICAL IMAGING 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Surgical instrument detection</term>
					<term>articulated pose estimation</term>
					<term>fully convolutional networks</term>
					<term>surgical vision</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Instrument detection, pose estimation and tracking in surgical videos is an important vision component for computer assisted interventions. While significant advances have been made in recent years, articulation detection is still a major challenge. In this paper, we propose a deep neural network for articulated multi-instrument 2D pose estimation, which is trained on a detailed annotations of endoscopic and microscopic datasets. Our model is formed by a fully convolutional detection-regression network. Joints and associations between joint pairs in our instrument model are located by the detection subnetwork and are subsequently refined through a regression subnetwork. Based on the output from the model, the poses of the instruments are inferred using maximum bipartite graph matching. Our estimation framework is powered by deep learning techniques without any direct kinematic information from a robot. Our framework is tested on single-instrument RMIT data, and also on multi-instrument EndoVis and in vivo data with promising results. In addition, the dataset annotations are publicly released along with our code and model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>R OBOTIC surgery systems, such as the da Vinci ® (Intu- itive Surgical Inc, CA), have introduced a powerful platform for articulated instrument control in minimally invasive surgery (MIS) through tele-operation of the surgical camera and specialised dexterous instruments. The next generation of such platforms is likely to incorporate a more significant component of computer assisted intervention (CAI) system support through software, multi-modal data visualisation and analytical tools to better understand the surgical process and progress. Real-time knowledge of the instruments' pose with respect to anatomical structures and the viewing coordinate frame is a crucial piece of information for such systems focused on providing assistive or autonomous surgical capabilities. While in principle with robotic instruments, the robot joint encoder data can be used to retrieve the pose information, in the da Vinci ® , the kinematic chain involves 18 joints, which is more than 2 meters long. This is challenging for accurate absolute position sensing and requires time-consuming handeye calibration between the camera and the robot coordinates. On cable driven systems the absolute error can be up to 1 inch, which means the positional accuracy is potentially too low for tracking applications without visual correction <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. Recent developments in endoscopic computer vision have resulted in advanced approaches for 2D instrument detection for minimally invasive surgery. Most of these methods have focused on semantic segmentation of the image or on single landmark detection on the instrument tip, which cannot represent the full pose of an instrument or include articulation. Additional challenges to articulated tracking in surgical video are because information inferred from video directly can suffer from occlusions, noise and specularities, perspective changes and bleeding or smoke in the scene.</p><p>Image-based surgical instrument detection and tracking is attractive because it relies purely on equipment already in the operating theatre <ref type="bibr" target="#b3">[4]</ref>. Likewise pose estimation from images has been shown to be feasible in different specialisations, such as retinal microsurgery <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, neurosurgery <ref type="bibr" target="#b7">[8]</ref> and MIS <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. While both detection and tracking are difficult, pose estimation presents additional challenges due to the complex articulation structure. Most image-based methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref> often extract low-level visual features from keypoints or regions to learn offline or online part appearance templates by using machine learning algorithms. Such lowlevel feature representations usually suffer from a lack of semantic interpretation, which means they cannot capture the high level category appearance. To improve robustness, it is possible to integrate external constraints such as surgical CAD models <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref> or robotic kinematics <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b12">[13]</ref>, but the essential image-driven approach is still central to provide robust and generalisable systems.</p><p>Deep convolutional neural networks have emerged as the method of choice for various visual tasks <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. In the past few years, it has been applied to medical image datasets and deep networks have been developed for various medical applications such as segmentation <ref type="bibr" target="#b17">[18]</ref> or recognition tasks <ref type="bibr" target="#b18">[19]</ref>. The methodology has been demonstrated to be effective in instrument presence detection <ref type="bibr" target="#b19">[20]</ref> or localization <ref type="bibr" target="#b20">[21]</ref>. Additionally, networks for semantic instrument segmentation have also been proposed and shown to be effective in real-time performance <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. In <ref type="bibr" target="#b23">[24]</ref>, the pose estimation task is reformulated as heatmap regression and is estimated concurrently with semantic instrument segmenta- tion. However, few methods are yet able to jointly detect the instrument contour and to estimate articulation from it.</p><p>Following the deep learning paradigm, in this paper, we present a novel 2D pose estimation framework for articulated endoscopic surgical instruments, which involves a fully convolutional detection-regression network (FCN) and a multiinstrument parsing component. The overall scheme is able to effectively localize instrument joints and also to estimate the articulation model. To measure articulation performance, we used the single-instrument RMIT dataset, and we also reannotated instrument joints of the multi-instrument dataset presented at the EndoVis Challenge, MICCAI'15 for training our network. Our method achieves very compelling performance and illustrates some interesting capabilities including transfer between different instrument sets within the EndoVis data and also between phantom settings, and in vivo robotic prostatectomy surgery data. The high-level of detail annotations which we have created as part of this study will naturally be made available for future research as well as our model and code (See Fig. <ref type="figure" target="#fig_6">7</ref>) <ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODS</head><p>The overall pipeline of our deep convolutional neural network based framework is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. In this section, we first define the instrument joint structure. Then, we introduce the objective and architectural design of each module of our detection-regression FCN. In our detection-regression architecture, the detection module guides the subsequent regression module to focus on the joint parts, and the regression module helps the detection module to localize joints more precisely. Finally, we describe how the network output is integrated for inferring the poses of multiple instruments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Articulation Model Architecture</head><p>The pose of an articulated instrument can be represented in different ways. For example, it could take advantage of kinematic information by using joint relative orientation. Our work relies purely on visual cues. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, an articulated instrument is decomposed as a skeleton of individual joint parts. We define a joint pair as two joints which are connected within the skeleton. Based on the articulation, instruments in different datasets are represented with a similar tree structure which is made up of N joints and M joint pairs. Therefore, the instrument pose estimation task is reduced to detecting the location of individual joint parts, and if there are multiple instruments present in the image, joints of the same instrument should be correctly associated after localization. Our bi-branch model architecture is inspired by CMUPose <ref type="bibr" target="#b14">[15]</ref>. Joint locations and associations between joint pairs are learnt jointly via two branches of same encoderdecoder predication process. In each of the blocks, features or predictions from each branch capture different structural information about the instrument and are concatenated for the next block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Joint Detection and Association Subnetwork</head><p>We design our bi-branch joint detection and association network inspired by the recent success of FCNs <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Since joints could overlap with each other, some pixels may belong to multiple joints. Therefore, we choose to use multiple binary cross-entropy instead of multi-class cross-entropy to train our network. By treating it as multiple binary-class problem, the ground truth we generate can reflect overlapping joint occlusion.</p><p>In our bi-branch network, the first branch is used to predict N individual joint probability maps, one for each joint; and the second branch is used to predict the M joint association probability maps, one for each joint pair. Therefore, the ground truth for the detection subnetwork is constructed as a set of N + M binary maps. Similar to the original U-Net <ref type="bibr" target="#b17">[18]</ref>, we used the popular downsampling-upsampling FCN architecture. The encoder-decoder network architecture concept is widely used for semantic segmentation problems since it transfers from classification to dense pixel-wise prediction probability maps with the same size as the input image. Fully connected layers can be turned into convolution layers, which has the advantages such as reduced number of parameters, faster forward-backward pass speed or taking images of arbitrary sizes <ref type="bibr" target="#b16">[17]</ref>. We also augmented our model with skip connections by fusing features from different layers to refine the spatial output precision. We take the Shaft-End joint pair as example, and illustrate the corresponding ground truth in Fig. <ref type="figure" target="#fig_2">3</ref>. For joint ground truth map (Fig. <ref type="figure" target="#fig_2">3 (c-d</ref>)), the pixels located within a certain radius r d of the labelled location are considered as the joint, and are set to 1, and the remaining pixels are considered as background, and are set to 0. To reflect the connection relationship and to measure the association of correct joints, the association ground map is constructed as shown in Fig. <ref type="figure" target="#fig_2">3 (b</ref>). The pixels within distance r d to the line connecting the joints are set to foreground, which form a rotated rectangle and are set to 1, other pixels are considered as background and are set to 0. The specifications of the network are shown in Tab. I. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, high level encoder features are concatenated with the upsampled decoder output. Instead of pooling operations, we use strided convolution for downsampling and also eliminate fully connected layers and use all convolutional layers following the recent examples from the literature <ref type="bibr" target="#b16">[17]</ref>. It is trained with a per-pixel binary cross-entropy loss function L d which is defined as:</p><formula xml:id="formula_0">L d = 1 (M + N )Ω M +N k=1 x∈Ω p k x log pk x + 1 -p k x log 1 -pk x (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where p k x and pk x denotes the ground truth value and the corresponding sigmoid output at pixel location x in the frame domain Ω of the kth probability map.  </p><formula xml:id="formula_2">Downsample CBR 3 × 3, 1 × 1 64 × h × w Branch SBR1 2 × 2, 2 × 2 64 × h /2 × w /2 Branch CBR1 3 × 3, 1 × 1 64 × h /2 × w /2 CBR1 1 × 1, 1 × 1 128 × h /2 × w /2 Branch SBR2 2 × 2, 2 × 2 128 × h /4 × w /4 Branch CBR2 3 × 3, 1 × 1 128 × h /4 × w /4 CBR2 1 × 1, 1 × 1 256 × h /4 × w /4 Branch SBR3 2 × 2, 2 × 2 256 × h /8 × w /8 Branch CBR3 3 × 3, 1 × 1 256 × h /8 × w /8 CBR3 1 × 1, 1 × 1 512 × h /8 × w /8 Branch SBR4 2 × 2, 2 × 2 512 × h /16 × w /16 Branch CBR4 3 × 3, 1 × 1 512 × h /16 × w /16 CBR4 1 × 1, 1 × 1 1024 × h /16 × w /16 Upsample Branch DBR1 2 × 2, 2 × 2 256 × h /8 × w /8 Branch CBR1 3 × 3, 1 × 1 256 × h /8 × w /8 CBR1 1 × 1, 1 × 1 512 × h /8 × w /8 Branch DBR2 2 × 2, 2 × 2 128 × h /4 × w /4 Branch CBR2 3 × 3, 1 × 1 128 × h /4 × w /4 CBR2 1 × 1, 1 × 1 256 × h /4 × w /4 Branch DBR3 2 × 2, 2 × 2 64 × h /2 × w /2 Branch CBR3 3 × 3, 1 × 1 64 × h /2 × w /2 CBR3 1 × 1, 1 × 1 128 × h /2 × w /2 Branch DBR4 2 × 2, 2 × 2 32 × h × w Branch CBR4 3 × 3, 1 × 1 32 × h × w CBR4 1 × 1, 1 × 1 64 × h × w CBS 1 × 1, 1 × 1 (M + N ) × h × w</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE II THE NETWORK SPECIFICATIONS FOR REGRESSION SUBNETWORK: THE KERNEL SIZE AND STRIDE, AND THE OUTPUT SIZE (CHANNEL × HEIGHT × WIDTH) OF EACH LAYER. THE REGRESSION NETWORK IS FED WITH THE CONCATENATION OF THE INPUT IMAGE AND THE DETECTION OUTPUT MAPS, AND OUTPUTS STACKED (M + N ) PROBABILITY MAPS WITH THE SAME SIZE AS THE INPUT IMAGE.</head><p>Kernel (Size, Stride) Output (C×H×W)</p><formula xml:id="formula_3">CBR1 3 × 3, 1 × 1 64 × h × w CBR2 3 × 3, 1 × 1 128 × h × w CBR3 3 × 3, 1 × 1 256 × h × w CBR4 3 × 3, 1 × 1 256 × h × w CBR5 1 × 1, 1 × 1 256 × h × w CB 1 × 1, 1 × 1 (M + N ) × h × w</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Regression Subnetwork</head><p>From the pixel-wise prediction output of the detection network, we could obtain coarse location of each joints, but in order to obtain precise location of the joints, we add a regression network following the detection network (see Fig. <ref type="figure" target="#fig_0">1</ref>).</p><p>The input of the network is the concatenation of the input image and the stacked M + N output probability maps of the detection network, with the latter acting as a semantic guidance for the regression network to focus on the joint parts and their structural relationships. Previous work <ref type="bibr" target="#b13">[14]</ref> showed that directly regressing single points from an input frame is highly non-linear, so instead of regressing single points, the network will produce stacked joint density maps, which have the same size as the input image. The network contains five Conv+Batch Normalization+ReLU (CBN) blocks, followed by a Conv+Batch Normalization (CB) block. The specifications of the network is shown in Tab. II.</p><p>In Fig. <ref type="figure" target="#fig_3">4</ref>, we illustrate the Shaft-End joint pair ground truth maps for the regression subnetwork. For joint ground truth maps (Fig. <ref type="figure" target="#fig_3">4 (c-d</ref>)), each joint annotation corresponds to an density map which is formed with a 2D Gaussian centred at the labelled point location. And the association ground truth density maps are represented with a Gaussian distribution along the joint pair centre line, with a standard deviation σ shown in Fig. <ref type="figure" target="#fig_3">4 (b)</ref>. Therefore, the goal of the regression subnetwork is to regress the density maps from the input image with the guidance of the detection probability maps. It is trained with the mean squared loss L r which we define as:</p><formula xml:id="formula_4">L r = 1 (M + N )Ω M +N k=1 x∈Ω h k x -hk x 2<label>(2)</label></formula><p>where h k x and hk x represent the ground truth and the predicted value at pixel location x ∈ Ω of the kth density map, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Multi-Instrument Parsing</head><p>After obtaining the output density maps of all the joints from the detection and regression framework, non-maximum suppression (NMS) <ref type="bibr" target="#b24">[25]</ref> is performed on the joint density maps to obtain potential joint candidates. NMS is popularly used in deep learning and generally in computer vision to eliminate redundant candidates. It selects high-scoring candidate and skips ones that are close to an already selected candidate.</p><p>As shown in Fig. <ref type="figure" target="#fig_4">5</ref>, instead of a fully connected graph (Fig. <ref type="figure" target="#fig_4">5</ref>(a)), where every pair is connected, the instrument structure is relaxed into a tree graph (Fig. <ref type="figure" target="#fig_4">5(b)</ref>) with minimal number of connections. The tree graph can be further decomposed into a set of joint pairs, for which the matching is decided independently (Fig. <ref type="figure" target="#fig_4">5(c)</ref>). The bipartite matching sub-problem then can be solved by maximum bipartite matching <ref type="bibr" target="#b25">[26]</ref>. To eliminate outliers and connect the right joints for each instrument, the association density maps from the network output are used to measure the association of joint candidate pairs: the association score is defined as the sum of accumulated pixel values along the line connecting the joint candidates on the corresponding association density map. The association score of any possible joint candidate pair is used to construct the weighted bipartite graphs. After finding the matching with maximum score of the chosen joint pairs, the ones which share the same joint can be assembled into full poses of multiple instruments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>Our proposed pose estimation framework is evaluated on a single-instrument retinal dataset and on multi-instrument endoscopic datasets. The statistics of each dataset are summarized in Tab. III. The original and our proposed annotations are demonstrated in Fig. <ref type="figure" target="#fig_6">7 (a-b</ref>). The original annotation is retrieved from the robotic system, which includes the location of the intersection point between the instrument axis and the border between plastic and metal on the shaft, normalized Shaft-to-Head axis vector and the tip angle. For training and evaluating our network, we construct a high quality multi-joint annotation for this dataset. For each instrument, five joints including Left, Right Clasper, Head, Shaft and End joint are annotated. Compared to our multiple joint annotations, the original annotations only provide limited and non-intuitive pose information for training and testing purposes. We manually labelled 940 frames of the training data (4479 frames) and 910 frames for the test data (4495 frames). The label and frame number of the EndoVis dataset are summarized in the lower part of Tab. III, and frame examples from each sequence are shown in the middle rows of Fig. <ref type="figure" target="#fig_5">6</ref>. It is worth mentioning that in the additional video sequences in the test set there is a EndoWrist Curved Scissor instrument which does not appear in the training set.</p><p>To test the performance against noise, we also add Fractional Brownian Motion noise <ref type="bibr" target="#b28">[29]</ref> on the test data in order to simulate smoke effect during surgery (see Fig. <ref type="figure" target="#fig_6">7 (c-d</ref>)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-instrument In vivo Dataset</head><p>Additionally, to test the framework performance on in vivo data, we labelled 123 frames of video clips (1220 frames) which are obtained from robotic prostatectomy surgery conducted at University College London Hospitals NHS Foundation Trust (UCLH) with resolution of 1920 × 1080 pixels. Frame examples from the in vivo data are shown in the bottom row of Fig. <ref type="figure" target="#fig_5">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training and Runtime Analysis</head><p>We implemented our framework in Lua and Torch7 <ref type="foot" target="#foot_3">4</ref> . The training data is augmented by horizontal and vertical flipping, and is resized to 288×384 pixels for RMIT data, and 256×320 pixels for EndoVis and in vivo data to fit in GPU memory. The detection radius r d is set to 10 pixels for RMIT data, and to 15 pixels for EndoVis and in vivo data. The regression standard deviation σ is set to 20 pixels. The radius of NMS is set to equal the detection radius r d . The network is trained on a single Nvidia GeForce GTX Titan X GPU using stochastic gradient descent (SGD) with an initial learning rate of 0.001 and momentum of 0.98. The learning rate progressively decreases every 10 epochs by 5%. The processing speed achieves 8.7 fps for videos, with the network inferencing taking 24 ms and the multi-instrument parsing step taking 89 ms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments</head><p>The following sections first outline the performance comparisons of our framework on the single-instrument RMIT dataset. Then, to understand the detection-regression architecture, we perform an ablation study and report its performance on the multi-instrument EndoVis dataset. Finally, we finetune the model to test the performance on the in vivo dataset.</p><p>1) RMIT Experiments: We trained the network with all four joints and we report performance by two different metrics: the Root-Mean-Square (RMS) distance (pixels) <ref type="bibr" target="#b26">[27]</ref> and the strict Percentage of Correct Parts (strict PCP) <ref type="bibr" target="#b29">[30]</ref>. The RMS distance reflects the localization accuracy of a single joint, it is evaluated as correct if the estimated joint location and the ground truth is within the threshold. Meanwhile the strict PCP estimates the localization of a joint pair and is considered correct if the distances between two connected joints are both smaller than α times the ground truth length of the connection pair. The evaluation results are shown in Tab. IV and Tab. V. We report the average RMS error distance, only on frames which the instruments are correctly detected (within the threshold measure). The same criteria applies for other datasets evaluated in the paper. We also compared the result against the state-of-the-art methods in Tab. VI and Tab. VII. In previous papers as listed in Tab. VI and Tab. VII, only recall score is reported. Approximate numbers are obtained through the accuracy threshold graphs from the papers, which do not provide the precise number. Analogously to previous methods, the recall score is evaluated by means of threshold measure (15 pixels) for the separate joint of the pose predictions and α for strict PCP is set to 0.5.</p><p>For the proposed methods, the average joint distance error for the test set is 4.87 pixels with the same recall and precision score of 94.33%, and the average strict PCP recall score is 96.94%. Some of the test set results are shown in Fig. <ref type="figure" target="#fig_7">8</ref>. Even under different lighting conditions, the model can predict the pose of the instrument correctly. It is interesting to point out that even though the association map used is constructed using a straight line, it still works on titled instruments (see the bottom line of Fig. <ref type="figure" target="#fig_7">8</ref> for example). This implies that the rectangle association maps are learnt to indicate the connection relationships between joint pairs. The trained network predict joint pair connections by not only relying on the instrument pixels, but also on the learnt joint relations and spatial contextual information. As we listed in Tab. VI, previous methods mainly focus on the evaluation of Shaft joint, except for SRNet <ref type="bibr" target="#b30">[31]</ref>, where our performance is on par with SRNet. The recall score of the End joint is the lowest (86.51%) among the four joints, due to its ambiguous annotation and image blur. SRNet uses a different strategy by explicitly modelling the instrument joints and their presence, which simultaneously predicts the instrument number and their pose. By assuming a known maximum number of instrument in the field of view, it bypasses the joint detection and association twostage process, so can be trained in an end-to-end fashion. Adding prior could help constrain the problem, compared to SRNet, we want to treat the task as general as possible, so our model does not rely on any prior knowledge of the number of instrument, theoretically it can predict pose of arbitrary number of instrument, which one of the potential strengths of our framework.</p><p>2) EndoVis Experiments: Since our annotation is limited, we used our network with five joints using all the training data generated from high quality our annotation. First, we perform an ablation study <ref type="foot" target="#foot_5">6</ref> to understand the detection-regression architecture. In Tab. VIII the average precision, recall score and RMS distance (pixels) of each joint for all the test data are reported. With a threshold of 20 pixels for the original resolution of 720 × 576 pixels, the average joint distance error for the test data set is 6.96 pixels with a recall score of 82.99% and a precision score of 83.70%.</p><p>In the ablation experiment, we compared the performance of five different models, including detection-only, shallow regression-only, deep regression-only, single-branch detectionregression and our proposed bi-branch detection-regression model. For the detection-only model, we use the output probability maps from the detection subnetwork for direct pose estimation. We also trained two regression-only models, a shallow one with the same architecture as the regression submodule in our detection-regression model and the input is the RGB frame without the detection probability maps, the deep one whose architecture is the same as the detectiononly model and with Gaussian regression ground truth. For    As seen from the ground truth binary map in Fig. <ref type="figure" target="#fig_2">3</ref>, the pixels belonging to the joint have the same weight, which lead to bad localization of joints.</p><p>We also observe that both regression-only models have better   We infer that one of the reasons is that the size of the training data is relatively small, which affects model generalization.</p><p>The regression-only models are capable of predicting the location of joints without any guidance. However, regression is empirically too localized, which supports small spatial context <ref type="bibr" target="#b13">[14]</ref>, the process of regressing from original input image to joint location directly can be difficult. By combining detection and regression, the detection module guides where to focus and provides spatial contextual information between joints for the regression module, by using the probability output from the detection module as structural guidance, the regression module facilitates the detection module to localize the joints more precisely. The performance of both detectionregression models show the improvement, and furthermore, our network takes less time to train compared to regressiononly model. The single-branch model achieves the performance of 76.77%/86.16% for recall and precision, which is nearly as good as the bi-branch model. We would like to point out that single-branch and bi-branch models are essentially similar. We choose bi-branch architecture here to conceptually separate the training of joint and joint association into two branches. Similar to the RMIT dataset result, the lower score for the End joint (76.81%/77.71%) is reasonable since it does not have distinct features and even the manual annotation has high variance. If the threshold is relaxed to 30 pixels, the recall and precision score of the End joint increase to 89.78% and 90.68% respectively. For the Head joint with the lowest recall and precision (75.82%/76.81%), as we have mentioned before, the two additional sequences of the test dataset exhibit a Curved Scissor instrument which is not seen in the training set. In Fig. <ref type="figure">9</ref> and Fig. <ref type="figure" target="#fig_0">10</ref>, we show some pose estimation examples from the test set. We observe that our model works well on self occlusion, as shown in the first row of Fig. <ref type="figure" target="#fig_0">10</ref>. This is credited to: 1) the model learns the spatial relationship between joints, even if a joint is occluded, it can be inferred from other joints; 2) the training data contains self occlusion examples that can be used by the model for handling self occlusion. As we can see, the left EndoWrist Curved Scissor instrument has a different shape compared to the right EndoWrist Needle Driver instrument, which explains the relatively low score for the Head joint. But our model is general enough to detect individual parts of this new instrument. Clearly, the generalisation to an unseen new instrument is limited to certain degree. Although the left Curved Scissor instrument has different appearance, it shares the same joint configuration with the Needle Driver instrument. The results we display show that with limited training data, our model is still capable of generalising to some degree.</p><p>From Fig. <ref type="figure" target="#fig_9">11</ref> and Tab. VIII we can also see that under smoke simulations the performance on test data only decrease slightly to 82.68% for recall and 82.62% for precision, with distance errors of 6.99 pixels. Please see the supplementary video for more qualitative results <ref type="foot" target="#foot_6">7</ref> .</p><p>In Fig. <ref type="figure" target="#fig_10">12</ref>, we have presented two failure cases on the test set. When one instrument is occluded by another one (Fig. <ref type="figure" target="#fig_10">12  (a</ref>)), the model can not infer the occluded joints, we think it is due to the lack of training data on instrument overlap, which causes the model fail to learn or handle the complex situation. We can compare this to the self-occlusion (first row of Fig. <ref type="figure" target="#fig_0">10</ref>). Since the training data covers self-occlusion, the model can well detect the self-occluded joints. We also show in Fig. <ref type="figure" target="#fig_10">12</ref> (b) that some joints of the new Curved Scissor instrument are not well localized, e.g. the Head joint. Our model has extended certain generalizability to unseen instrument, but obviously compared to the Needle Driver instrument in the training data, the performance is less robust.</p><p>3) In vivo Experiments: We fine-tuned the EndoVis trained model on 80% of the labelled data (97 frames) with a fixed learning rate 0.0001 for 10 epochs, and tested on the whole sequence. The in vivo video sequence we use is with high resolution 1920 × 1080 pixels, so we set the threshold as 50 pixels for evaluation. In Tab. X, it is shown that the average distance errors are reduced to 9.81 and 13.42 pixels for the train and validation set respectively, with the threshold of 50 pixels for the original resolution. Examples of the in vivo data are shown in Fig. <ref type="figure" target="#fig_11">13</ref> and the pose estimation of the whole video is also included in our supplementary material. Note that we did not perform any temporal processing in any of our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>In this paper, we have proposed a deep neural network based framework for 2D pose estimation of multiple articulated instruments in surgical images and video. The methodology performs detection of the instruments and their degrees of freedom without using kinematic information from robotic encoders or external tracking sensors. The work, to the best of our knowledge, represents a novel attempt to perform imagebased articulated pose estimation at this level of detail and can potentially be extended to handle even more complicated flexible articulation by incorporating additional joint nodes.  In our approach, joints and the associations between joint pairs are first detected and then refined in a detectionregression FCN. To obtain the final pose of all the instruments in an image, association probabilities are used as a measurement to connect joint pairs for each instrument by maximum bipartite matching. The framework has been trained and evaluated on RMIT, EndoVis and in vivo datasets with detailed annotations adding to existing challenge data labels. Interestingly, our experiments show that our model exhibits some generalizability to new unseen instrument, and has good robustness under smoke simulation. The performance on the in vivo datasets demonstrates the capacity of our framework to handle real surgical scenes. Our model will be publicly released to support research in the field.</p><p>A current limitation of our method is that it is limited to 2D inference and a natural extension would be to explore the estimation of 3D articulation. This seems plausible when using stereo configurations which are available within the EndoVis data for example and can potentially be used to formulate both the detection and the pose estimation in a joint space of both views. Additionally, it will be interesting to explore the sequential tracking of articulated instruments. This could potentially be achieved by probing the motion information that can be learnt through recurrent neural networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The pipeline of the proposed pose estimation framework and the detection-regression FCN architectural design. The output of the network integrates the associated joints and assembles them into the final poses for all instruments in the frame.</figDesc><graphic coords="2,48.96,56.07,514.08,173.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Based on the articulation, instruments in different datasets are represented with similar skeletons. They are decomposed into N joints and M joint pairs. Joints are represented by colour dots, and joint pairs are connected by black lines. (Top) The EndoWrist Needle Driver instrument is made up of 5 joints and 4 joint pairs; (Bottom) The Retinal instrument is made up of 4 joints and 3 joint pairs.</figDesc><graphic coords="3,48.96,56.07,252.00,202.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Detection subnetwork ground truth example for a Shaft-End joint pair: the binary map for Shaft-End pair association (b), the Shaft (c) and End (d) joint.</figDesc><graphic coords="3,311.98,481.68,252.00,215.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Regression subnetwork ground truth example for Shaft-End joint pair: the density map for Shaft-End pair association (b), the Shaft (c) and End (d) joint.</figDesc><graphic coords="4,311.98,56.07,252.00,210.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Graph relaxing for instrument structure: (a) Fully connected graph; (b) Tree structure graph; (c) A set of bipartite graphs after relaxation, the matching of joint pairs are decided independently.</figDesc><graphic coords="4,311.98,534.06,252.00,101.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Example images from each sequence of test data in the datasets: (Top row) single-instrument Retinal Microsurgery Instrument Tracking (RMIT) dataset; (Middle rows) multi-instrument EndoVis challenge dataset; (Bottom row) multi-instrument in vivo dataset.</figDesc><graphic coords="5,48.96,312.33,251.99,222.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The original (a) and our proposed (b) annotation for EndoVis challenge dataset, smoke effect simulation (c) and simulation overlaid on the frame (d).</figDesc><graphic coords="6,48.96,56.07,252.01,215.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Result examples of RMIT test set with our model. The frame is trimmed around the instrument for better demonstration. It is difficult to localize some joints due to its ambiguous annotation, image blur or specularities.</figDesc><graphic coords="7,48.96,301.80,514.13,209.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Fig. 9. Result examples with an unseen EndoWrist Curved Scissor instrument in the EndoVis test set with our model. (a) The original frame; (b) the estimated pose; joint (c1-5) and association (e1-4) probability output from detection subnetwork; joint (d1-5) and association (f1-4) density output from regression subnetwork.</figDesc><graphic coords="8,48.96,88.01,514.11,184.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Examples of smoke-simulated EndoVis test set. Our network is able to detect instruments which are not seen in the training data even under smoke simulation.</figDesc><graphic coords="9,48.96,67.33,514.10,205.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Examples of failure cases of EndoVis test set. (a) Occluded joints are miss detected; (b) The head joint of the new Curved Scissor instrument on the left is not well localized.</figDesc><graphic coords="10,311.98,56.07,252.00,101.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Examples of in vivo data with our fine-tuned model. The results demonstrate the capacity of our framework to be applied to real surgical scenes.</figDesc><graphic coords="11,48.96,56.07,514.11,217.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I THE</head><label>I</label><figDesc>NETWORK SPECIFICATIONS FOR THE DETECTION SUBNETWORK: THE KERNEL SIZE AND STRIDE, AND THE OUTPUT SIZE (CHANNEL × HEIGHT × WIDTH) OF EACH LAYER. THE ORIGINAL DIMENSION OF THE INPUT IMAGE</figDesc><table /><note><p>IS 3 × h × w, AND THE NETWORK OUTPUTS STACKED (M + N ) PROBABILITY MAPS WITH THE SAME SIZE AS THE INPUT IMAGE. Kernel (Size, Stride) Output (C×H×W)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III LABEL</head><label>III</label><figDesc>/FRAME NUMBER SUMMERY OF THE RMIT AND ENDOVIS DATASET.</figDesc><table><row><cell>Seq1</cell><cell>Seq2</cell><cell>Seq3</cell><cell>Seq4</cell><cell>Seq5</cell><cell>Seq6</cell><cell>Total</cell></row><row><cell></cell><cell></cell><cell></cell><cell>RMIT Dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Train Data</cell><cell></cell><cell></cell><cell></cell></row><row><cell>201 / 201</cell><cell>111 / 111</cell><cell>265 / 271</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>577 / 583</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Test Data</cell><cell></cell><cell></cell><cell></cell></row><row><cell>201 / 201</cell><cell>111 / 111</cell><cell>266 / 276</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>578 / 588</cell></row><row><cell></cell><cell></cell><cell></cell><cell>EndoVis Dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Train Data</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">210 / 1107 240 / 1125</cell><cell cols="2">252 / 1124 238 / 1123</cell><cell>-</cell><cell>-</cell><cell>940 / 4479</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Test Data</cell><cell></cell><cell></cell><cell></cell></row><row><cell>80 / 370</cell><cell>76 / 375</cell><cell>76 / 375</cell><cell>76 / 375</cell><cell cols="3">301 / 1500 301 / 1500 910 / 4495</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV QUANTITATIVE</head><label>IV</label><figDesc>RESULTS OF THE RMIT DATASET: PRECISION AND THE DISTANCE ERROR BETWEEN GROUND TRUTH AND THE ESTIMATE OF EACH JOINT. THE THRESHOLD IS SET TO 15 PIXELS FOR THE ORIGINAL RESOLUTION OF 640 × 480 PIXELS. Recall (%) / Precision (%) / Distance (px) of the RMIT Dataset (T hres = 15 px)</figDesc><table><row><cell>Tip1</cell><cell>Tip2</cell><cell>Shaft</cell><cell>End</cell><cell>Total</cell></row><row><cell></cell><cell></cell><cell>Train set</cell><cell></cell><cell></cell></row><row><cell>100.0 / 100.0 / 2.14</cell><cell>100.0 / 100.0 / 2.28</cell><cell>100.0 / 100.0 / 1.72</cell><cell>100.0 / 100.0 / 2.38</cell><cell>100.0 / 100.0 / 2.13</cell></row><row><cell></cell><cell></cell><cell>Test set</cell><cell></cell><cell></cell></row><row><cell>99.13 / 99.13 / 5.26</cell><cell>97.58 / 97.58 / 4.61</cell><cell>94.12 / 94.12 / 4.93</cell><cell>86.51 / 86.51 / 4.68</cell><cell>94.33 / 94.33 / 4.87</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V QUANTITATIVE</head><label>V</label><figDesc>RESULTS OF THE RMIT DATASET: THE STRICT PCP SCORE OF THE ESTIMATE OF EACH JOINT PAIR.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VI QUANTITATIVE</head><label>VI</label><figDesc>RECALL PERFORMANCE COMPARISON WITH THE STATE-OF-THE-ART METHODS ON THE RMIT TEST SET 5The Recall Score of the RMIT Test Set (T hres = 15 px)</figDesc><table><row><cell></cell><cell>Tip1</cell><cell>Tip2</cell><cell>Shaft</cell><cell>End</cell><cell>Total</cell></row><row><cell>DDVT [27]</cell><cell>-</cell><cell>-</cell><cell>&lt; 85.0</cell><cell>-</cell><cell>-</cell></row><row><cell>POSE [28]</cell><cell>-</cell><cell>-</cell><cell>88.9</cell><cell>-</cell><cell>-</cell></row><row><cell>RTOA [7]</cell><cell>-</cell><cell>-</cell><cell>94.3</cell><cell>-</cell><cell>-</cell></row><row><cell>SRNet [31]</cell><cell>98.6</cell><cell>94.1</cell><cell>96.2</cell><cell>91.2</cell><cell>95.0</cell></row><row><cell>Proposed</cell><cell>99.1</cell><cell>97.6</cell><cell>94.1</cell><cell>86.5</cell><cell>94.3</cell></row></table><note><p>the single-branch model, we fuse two branches of the detection submodule into only one branch with double size of the feature maps of our model. The performance comparison of different</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VII QUANTITATIVE</head><label>VII</label><figDesc>STRICT PCP SCORE COMPARISON WITH THE STATE-OF-THE-ART METHODS ON THE RMIT TEST SETThe Strict PCP Score of the RMIT Test Set (α = 0.5)</figDesc><table><row><cell></cell><cell>Tip1-Shaft</cell><cell>Tip2-Shaft</cell><cell>Shaft-End</cell><cell>Total</cell></row><row><cell>POSE [28]</cell><cell>≈ 95.0</cell><cell>≈ 90.0</cell><cell>-</cell><cell>-</cell></row><row><cell>Proposed</cell><cell>99.13</cell><cell>97.58</cell><cell>94.12</cell><cell>96.94</cell></row></table><note><p>models is summarized in Tab. IX. The bad performance of the detection-only model (32.19%/14.41% for recall and precision score) is expected.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VIII QUANTITATIVE</head><label>VIII</label><figDesc>RESULTS OF THE ENDOVIS DATASET: PRECISION AND THE DISTANCE ERROR BETWEEN GROUND TRUTH AND THE ESTIMATE OF EACH JOINT. FOR THE EndoVis DATASET, THE THRESHOLDS ARE SET TO 20 AND 30 PIXELS FOR THE ORIGINAL AND SMOKE-SIMULATED TEST DATA WITH THE RESOLUTION OF 720 × 576 PIXELS.It is interesting that the precision score for deep model (97.67%) is higher than that for the shallow model (71.53%), while either shallow or deep regression-only models achieve similar recall performance (66.46% for shallow model and 65.06% for the deeper model). Deeper architecture does not help to achieve better recall performance in the experiment.</figDesc><table><row><cell>performance.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Recall (%) / Precision (%) / Distance (px) of the EndoVis Dataset</cell><cell></cell></row><row><cell>LeftClasper</cell><cell>RightClasper</cell><cell>Head</cell><cell>Shaft</cell><cell>End</cell><cell>Total</cell></row><row><cell></cell><cell></cell><cell cols="2">Train set (T hres = 20 px)</cell><cell></cell><cell></cell></row><row><cell>100.0 / 99.95 / 2.43</cell><cell>100.0 / 99.95 / 2.53</cell><cell>99.57 / 99.68 / 2.34</cell><cell>100.0 / 99.95 / 2.74</cell><cell>99.89 / 99.84 / 6.73</cell><cell>99.89 / 99.87 / 3.36</cell></row><row><cell></cell><cell></cell><cell cols="2">Test set (T hres = 20 px)</cell><cell></cell><cell></cell></row><row><cell>86.28 / 86.65 / 5.03</cell><cell>85.49 / 85.82 / 5.40</cell><cell>75.82 / 76.81 / 6.55</cell><cell>90.55 / 91.50 / 8.63</cell><cell>76.81 / 77.71 / 9.17</cell><cell>82.99 / 83.70 / 6.96</cell></row><row><cell></cell><cell></cell><cell cols="2">Test set (T hres = 30 px)</cell><cell></cell><cell></cell></row><row><cell>89.29 / 89.67 / 5.57</cell><cell>87.86 / 88.19 / 5.99</cell><cell>80.05 / 80.99 / 7.37</cell><cell>94.51 / 95.42 / 9.38</cell><cell>89.78 / 90.68 / 11.58</cell><cell>88.30 / 88.99 / 7.98</cell></row><row><cell></cell><cell></cell><cell cols="2">Smoke Test set (T hres = 20 px)</cell><cell></cell><cell></cell></row><row><cell>83.85 / 83.48 / 5.25</cell><cell>82.69 / 82.27 / 5.72</cell><cell>74.89 / 75.07 / 6.50</cell><cell>89.73 / 89.71 / 8.62</cell><cell>82.25 / 82.55 / 8.86</cell><cell>82.68 / 82.62 / 6.99</cell></row><row><cell></cell><cell></cell><cell cols="2">Smoke Test set (T hres = 30 px)</cell><cell></cell><cell></cell></row><row><cell>88.30 / 88.02 / 6.13</cell><cell>86.81 / 86.41 / 6.68</cell><cell>78.02 / 78.30 / 7.19</cell><cell>95.66 / 95.66 / 9.76</cell><cell>91.32 / 91.48 / 10.60</cell><cell>88.02 / 87.97 / 8.07</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE IX</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">ABLATION STUDY OF THE DETECTION-REGRESSION MODEL ARCHITECTURE ON ENDOVIS TEST SET</cell><cell></cell></row><row><cell></cell><cell cols="4">Recall (%) / Precision (%) / Distance (px) of the EndoVis Test Set (Thres = 20 px)</cell><cell></cell></row><row><cell>LeftClapser</cell><cell>RightClasper</cell><cell>Head</cell><cell>Shaft</cell><cell>End</cell><cell>Total</cell></row><row><cell></cell><cell></cell><cell cols="2">Detection-only Network</cell><cell></cell><cell></cell></row><row><cell>32.58 / 14.28 / 7.94</cell><cell>24.51 / 11.05 / 6.22</cell><cell>29.40 / 13.19 / 6.75</cell><cell>40.27 / 18.03 / 8.87</cell><cell>34.18 / 15.49 / 7.57</cell><cell>32.19 / 14.41 / 7.47</cell></row><row><cell></cell><cell></cell><cell cols="2">Shallow Regression-only Network</cell><cell></cell><cell></cell></row><row><cell>67.73 / 72.94 / 4.86</cell><cell>81.26 / 84.49 / 4.34</cell><cell>66.48 / 73.35 / 6.18</cell><cell>75.16 / 80.65 / 7.58</cell><cell>41.65 / 46.23 / 9.06</cell><cell>66.46 / 71.53 / 6.41</cell></row><row><cell></cell><cell></cell><cell cols="2">Deep Regression-only Network</cell><cell></cell><cell></cell></row><row><cell>65.75 / 98.79 / 3.63</cell><cell>61.81 / 93.35 / 3.80</cell><cell>66.48 / 99.34 / 5.12</cell><cell>66.65 / 99.40 / 6.84</cell><cell>64.62 / 97.47 / 7.15</cell><cell>65.06 / 97.67 / 5.31</cell></row><row><cell></cell><cell></cell><cell cols="2">Single-branch Detection-Regression Network</cell><cell></cell><cell></cell></row><row><cell>78.90 / 88.13 / 4.70</cell><cell>81.04 / 90.27 / 5.44</cell><cell>74.07 / 83.74 / 7.24</cell><cell>79.56 / 88.94 / 7.72</cell><cell>70.27 / 79.71 / 9.22</cell><cell>76.77 / 86.16 / 6.87</cell></row><row><cell></cell><cell></cell><cell cols="2">Proposed Bi-branch Detection-Regression Network</cell><cell></cell><cell></cell></row><row><cell>86.28 / 86.65 / 5.03</cell><cell>85.49 / 85.82 / 5.40</cell><cell>75.82 / 76.81 / 6.55</cell><cell>90.55 / 91.55 / 8.63</cell><cell>76.81 / 77.71 / 9.17</cell><cell>82.99 / 83.70 / 6.96</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE X QUANTITATIVE</head><label>X</label><figDesc>RESULTS OF THE IN VIVO DATASET: PRECISION AND THE DISTANCE ERROR BETWEEN GROUND TRUTH AND THE ESTIMATE OF EACH JOINT. FOR THE IN VIVO DATA, THE THRESHOLD IS SET TO 50 PIXELS FOR THE ORIGINAL RESOLUTION OF 1920 × 1080 PIXELS.</figDesc><table><row><cell>LeftClasper</cell><cell>RightClasper</cell><cell>Head</cell><cell>Shaft</cell><cell>End</cell><cell>Total</cell></row><row><cell></cell><cell></cell><cell cols="2">Train set</cell><cell></cell><cell></cell></row><row><cell>97.94 / 96.39 / 7.84</cell><cell>97.94 / 96.39 / 8.40</cell><cell>100.0 / 98.97 / 9.61</cell><cell>100.0 / 100.0 / 10.39</cell><cell>98.97 / 98.97 / 12.81</cell><cell>98.97 / 98.14 / 9.81</cell></row><row><cell></cell><cell></cell><cell cols="2">Validation set</cell><cell></cell><cell></cell></row><row><cell>98.08 / 96.15 / 13.91</cell><cell>94.23 / 92.31 / 12.54</cell><cell>96.15 / 94.23 / 12.01</cell><cell>100.0 / 100.0 / 13.86</cell><cell>92.31 / 92.31 / 14.77</cell><cell>96.15 / 95.00 / 13.42</cell></row></table><note><p>Recall (%) / Precision (%) / Distance (px) of the In vivo Dataset (T hres = 50 px)</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/surgical-vision/EndoVisPoseAnnotation</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://sites.google.com/site/sznitr/code-and-datasets training set including all the first halves of the sequences (577 frames), and a testing set using the second halves (578 frames).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://endovissub-instrument.grand-challenge.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>http://torch.ch/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>To maintain notation consistency, the Shaft and End joint in our paper correspond respectively to End Shaft and Start Shaft joint in previous papers.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>An ablation study refers to evaluating how the performance is affected by removing some part of the model.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>This paper has supplementary downloadable material available at http: //ieeexplore.ieee.org, provided by the authors. This includes a video file which contains experimental results of the proposed framework and a readme file. This material is 44.3 MB in size.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>Xiaofei Du Author is supported by the China Scholarship Council (CSC) scholarship. This work was supported by the EPSRC (EP/N013220/1, EP/N022750/1, EP/N027078/1, NS/A000027/1, EP/P012841/1), The Wellcome Trust (WT101957, 201080/Z/16/Z) and the EU-Horizon2020 project EndoVESPA (H2020-ICT-2015-688592).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Real-time 3d tracking of articulated tools for robotic surgery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Giannarou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="386" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning features on robotic surgical tools</title>
		<author>
			<persName><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="38" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Feature classification for tracking articulated surgical tools</title>
		<author>
			<persName><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention-MICCAI</title>
		<imprint>
			<biblScope unit="page" from="592" to="600" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vision-based and marker-less surgical tool detection and tracking: a review of the literature</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bouget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jannin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="633" to="654" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Articulated object tracking by rendering consistent appearance parts</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Pezzementi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Voros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="3940" to="3947" />
		</imprint>
	</monogr>
	<note>ICRA&apos;09</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visual tracking using the sum of conditional variance</title>
		<author>
			<persName><forename type="first">R</forename><surname>Richa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sznitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="2953" to="2958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Real-time online adaption for robust instrument tracking and pose estimation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Rieke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Vizcaíno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Di San Filippo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="422" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Detecting surgical tools by modelling local appearance and global shape</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Riffaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jannin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2603" to="2617" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3d tracking of laparoscopic instruments using statistical and geometric modeling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duchateau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cinquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Voros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="203" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image based surgical instrument pose estimation with multi-class labelling and optical flow</title>
		<author>
			<persName><forename type="first">M</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hawkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="331" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast part-based classification for instrument detection in minimally invasive surgery</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sznitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="692" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Combined 2d and 3d tracking of surgical instruments for invasive and robotic-assisted surgery</title>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hawkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1109" to="1119" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Articulated surgical tool detection using virtually-rendered templates</title>
		<author>
			<persName><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Assisted Radiology and Surgery (CARS)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="717" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08050</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Endonet: A deep architecture for recognition tasks on laparoscopic videos</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Twinanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shehata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Mathelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="86" to="97" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Addressing multi-label imbalance problem of surgical tool detection using cnn</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szengel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zachow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Assisted Radiology and Surgery</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Detection and localization of robotic tools in robot-assisted surgery videos using deep neural networks for region proposal and detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Guru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real-time segmentation of non-rigid surgical tools based on deep learning and tracking</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>García-Peraza-Herrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gruijthuijsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Devreker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Attilakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deprest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vander Poorten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Computer-Assisted and Robotic Endoscopy</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="84" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Toolnet: Holistically-nested real-time segmentation of robotic surgical tools</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Garcia-Peraza-Herrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fidon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gruijthuijsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Devreker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Attilakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deprest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Poorten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08126</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Concurrent segmentation and localization for tracking of surgical instruments</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rieke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Vizcaíno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="664" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient non-maximum suppression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Neubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition, 2006. ICPR 2006. 18th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="850" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast algorithms for weighted bipartite matching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Steger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weißl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Experimental and Efficient Algorithms</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="476" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Datadriven visual tracking in retinal microsurgery</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sznitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Richa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention-MICCAI</title>
		<imprint>
			<biblScope unit="page" from="568" to="575" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Surgical tool tracking and pose estimation in retinal microsurgery</title>
		<author>
			<persName><forename type="first">N</forename><surname>Rieke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alsheakhali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Di San Filippo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="266" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Noise fractals and clouds</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wellons</surname></persName>
		</author>
		<ptr target="http://nullprogram.com/blog/2007/11/20/,online" />
		<imprint>
			<date type="published" when="2007-11-20">20-Nov-2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Progressive search space reduction for human pose estimation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marin-Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008">2008. 2008. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Kurmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Marquez Neila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sznitman</surname></persName>
		</author>
		<title level="m">Simultaneous Recognition and Pose Estimation of Instruments in Minimally Invasive Surgery</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="505" to="513" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
