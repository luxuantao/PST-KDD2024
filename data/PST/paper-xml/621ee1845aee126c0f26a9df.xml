<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">First-generation Memory Disaggregation for Cloud Platforms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-01">1 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Huaicheng</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Berger</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stanko</forename><surname>Novakovic</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lisa</forename><surname>Hsu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dan</forename><surname>Ernst</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pantea</forename><surname>Zardoshti</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Monish</forename><surname>Shah</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ishwar</forename><surname>Agarwal</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><surname>Hill</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marcus</forename><surname>Fontoura</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ricardo</forename><surname>Bianchini</surname></persName>
						</author>
						<author>
							<persName><forename type="first">?</forename><surname>Carnegie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mellon</forename><surname>University</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Microsoft</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Microsoft</forename><surname>Azure</surname></persName>
						</author>
						<title level="a" type="main">First-generation Memory Disaggregation for Cloud Platforms</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-01">1 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.00241v1[cs.OS]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In Azure, up to 25% of memory is stranded, i.e., it is leftover after the servers' cores have been rented to VMs. Memory disaggregation promises to reduce this stranding. However, making disaggregation practical for production cloud deployment remains challenging. For example, RDMA-based disaggregation involves too much overhead for common workloads and transparent latency management is incompatible with virtualization acceleration. The emerging Compute Express Link (CXL) standard offers a low-overhead substrate to build memory disaggregation while overcoming these challenges. This paper proposes a first-generation CXL-based disaggregation system that meets the requirements of cloud providers. Our system includes a memory pool controller, and prediction-based system software and distributed control plane designs. Its predictions of VM latency sensitivity and memory usage allow it to split workloads across local and pooled memory while mitigating the higher pool latency.</p><p>Our analysis of production clusters shows that small pools of 8-32 sockets are sufficient to reduce stranding significantly. It also shows that ?50% of all VMs never touch 50% of their rented memory. In emulated experiments with 150+ workloads, we show our pooling approach incurs a configurable performance loss between 1-5%. Finally, we show that disaggregation can achieve a 9-10% reduction in overall DRAM, which represents hundreds of millions of dollars in cost savings for a large cloud provider.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Motivation. DRAM has emerged as a key cost driver for public cloud providers. For example, DRAM makes up 50% of the server cost for Azure <ref type="bibr" target="#b0">[1]</ref>. Due to long-standing scaling challenges <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>, DRAM cost is expected to grow even further in the future. Meanwhile, DRAM alternatives are either in their early stages or hitting production and deployment hurdles <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. Thus, DRAM efficiency has been a top priority for cloud providers, but remains challenging to achieve. For example, Google reports that only ?40% of DRAM is used in its production clusters <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>At Azure, we find that a major contributor to DRAM inefficiency is platform-level memory stranding. Memory stranding occurs when a server's cores are fully rented to virtual machines (VMs), but unrented memory remains. With the cores exhausted, the remaining memory is unrentable on its own, and is thus stranded. Surprisingly, we find that up to 25% of DRAM may become stranded at any given moment (see ?2). Thus, reducing memory stranding can have an outsized impact on hardware costs as the provider can purchase less DRAM. A promising direction is the pooling of memory through memory disaggregation <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>.</p><p>Unfortunately, memory disaggregation incurs a potential performance penalty in that memory from the pool has higher access latency. To achieve the most DRAM savings, public cloud providers would prefer to hide the additional latency from (1) the vast majority of customers who are not expert developers and (2) workloads that do not manage memory placement and performance explicitly. The challenge is achieving this in a practical and broadly applicable manner.</p><p>Memory disaggregation for the public cloud. In more detail, making memory disaggregation practical for the public cloud faces multiple functional challenges. First, we must preserve customer inertia, i.e., require no modifications to customer workloads or the guest OS. Second, the system must be compatible with virtualization acceleration techniques such as direct I/O device assignment to VMs <ref type="bibr">[14,</ref><ref type="bibr" target="#b13">15]</ref> and SR-IOV <ref type="bibr" target="#b14">[16]</ref>. Third, the system must be available as commodity hardware.</p><p>Due to these requirements, most of the prior memory disaggregation work does not apply: custom hardware-based designs <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">[17]</ref><ref type="bibr" target="#b16">[18]</ref><ref type="bibr" target="#b17">[19]</ref><ref type="bibr" target="#b18">[20]</ref>, systems that require changes to the VM guest <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">[21]</ref><ref type="bibr" target="#b20">[22]</ref><ref type="bibr" target="#b21">[23]</ref><ref type="bibr" target="#b22">[24]</ref><ref type="bibr" target="#b23">[25]</ref><ref type="bibr" target="#b24">[26]</ref><ref type="bibr" target="#b25">[27]</ref><ref type="bibr" target="#b26">[28]</ref><ref type="bibr" target="#b27">[29]</ref><ref type="bibr" target="#b28">[30]</ref><ref type="bibr" target="#b29">[31]</ref><ref type="bibr" target="#b30">[32]</ref><ref type="bibr" target="#b31">[33]</ref><ref type="bibr" target="#b32">[34]</ref><ref type="bibr" target="#b33">[35]</ref><ref type="bibr" target="#b34">[36]</ref><ref type="bibr" target="#b35">[37]</ref>, and implementations that rely on page faults <ref type="bibr" target="#b13">[15]</ref> are not deployable in the cloud today (see ?4.1).</p><p>Fortunately, the emerging Compute Express Link (CXL) interconnect standard <ref type="bibr" target="#b36">[38]</ref> greatly facilitates fast and deployable disaggregated memory. CXL enables native load/store (ld/st) accesses to disaggregated memory for Intel, AMD, and ARM processors <ref type="bibr" target="#b37">[39]</ref><ref type="bibr" target="#b38">[40]</ref><ref type="bibr" target="#b39">[41]</ref> and Samsung, Micron, and SK Hynix memory modules <ref type="bibr">[42,</ref><ref type="bibr" target="#b40">43]</ref>. CXL also has a low performance impact: directly-connected CXL memory can be accessed with roughly the same latency as a NUMA hop. CXL has become the best option for memory disaggregation by superceding competing protocols <ref type="bibr" target="#b41">[44,</ref><ref type="bibr" target="#b42">45]</ref>.</p><p>Open questions. While CXL defines the disaggregation protocol, many questions remain open at every layer of the stack. At the hardware layer, how should the provider construct a memory pool with CXL? How should it balance the pool size with the higher latency of larger pools? At the system software layer, how should the provider manage and expose the pooled memory to guest OSes? How much additional memory latency can cloud workloads tolerate? At the distributed system layer, how should the provider schedule VMs on machines with CXL memory? Could predictions of memory behavior and latency sensitivity help produce better schedules? If so, how accurate are the predictions?</p><p>Though memory disaggregation is related to NUMA and two-tier memory systems, the prior work on these topics does not answer the above questions. NUMA policies focus on moving compute close to memory, e.g., a process or VM is scheduled on cores in the same NUMA domain as the memory pages it accesses <ref type="bibr" target="#b43">[46]</ref><ref type="bibr" target="#b44">[47]</ref><ref type="bibr" target="#b45">[48]</ref>. This is not applicable to CXL where memory pools do not have local cores. Two-tier systems focus on migrating hot pages to local memory <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b46">[49]</ref><ref type="bibr" target="#b47">[50]</ref><ref type="bibr" target="#b48">[51]</ref><ref type="bibr" target="#b49">[52]</ref><ref type="bibr" target="#b50">[53]</ref>. Page migration is not supported in public cloud platforms due to virtualization accelerators ( ?4). Thus, providers require a new way to overcome the challenges of higher memory access latency. Our work: CXL-based disaggregation for public clouds. Our work answers the above open questions. First, through extensive measurements ( ?3.1) in Azuredatacenters, we find that stranding hotspots emerge broadly across clusters. This insight leads us to a small memory pool design between 8-32 sockets to significantly reduce stranding. Larger pools do further reduce stranding but yield diminishing returns.</p><p>Second, we introduce the first CXL-based full-stack memory disaggregation design that satisfies the requirements of a cloud platform. Our design consists of the following components at each layer of the datacenter stack.</p><p>Hardware layer: We propose a new multi-ported external memory controller directly connected to 8-32 sockets via CXL. Each socket has some fast local memory but can access a portion of the memory pool at slightly higher latency (equivalent to 1 NUMA hop).</p><p>System software layer: Our design manages pool memory in large memory slices to achieve practical resource overheads. It exposes pool memory to a VM's guest OS as a zero-core virtual NUMA (zNUMA) node (i.e., a node with memory but no cores, like Linux's CPU-less NUMA <ref type="bibr" target="#b51">[54]</ref>), which effectively biases memory allocations away from the zNUMA node and improves performance.</p><p>Distributed system software layer: Our control plane relies on predictions of VM memory latency sensitivity and the amount of memory will actually be touched. We call the untouched memory "frigid". VMs that are predicted to be insensitive to memory latency are fully backed by pool DRAM, whereas memory-sensitive VMs are provisioned with a zNUMA node corresponding to their frigid memory. We use the predictions for scheduling at VM deployment time, and asynchronously manages the allocation of pooled memory to servers and changes a VM placement when it detects that its prediction was incorrect.</p><p>Our evaluation based on VM traces from 100 production clusters shows that we can achieve a 9-10% reduction in overall DRAM needs with each pool connected to 32 sockets. These savings correspond to hundreds of millions of dollars for a large cloud provider. Using more than 150 workloads in a CXL emulation testbed, we also show that most workloads are insensitive to CXL latency even when their entire working set is allocated in pool memory. However, 25% of them suffer at least a 20% performance loss. In terms of memory access behaviors, our results show that ?50% of all VMs never touch 50% of their rented memory. Our models predict this percentage accurately, as well as whether a workload is memory latency-sensitive. When workloads are properly split between local and pooled memory, slowdowns are configurable to be lower than 1-5%. Exposing the zNUMA node to the guest OS is very effective at producing this split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions. Our main contributions are as follows:</head><p>? The first public characterization of memory stranding and frigid memory at a large public cloud provider. ? The first CXL-based full-stack disaggregation design that is practical and performant for broad deployment. ? An accurate prediction model for latency and resource management at datacenter scale. ? An extensive performance evaluation and analysis with over 150 workloads that validates our design. ? To-be-open-sourced research artifacts, including our emulation testbed, production traces, prediction models, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Memory stranding. Cloud VMs demand a vector of resources (cpu, memory, etc.) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b52">[55]</ref><ref type="bibr" target="#b53">[56]</ref><ref type="bibr" target="#b54">[57]</ref>. Scheduling a variety of VM types with highly heterogeneous resource demands onto servers leads to a multi-dimensional bin-packing problem <ref type="bibr" target="#b53">[56,</ref><ref type="bibr" target="#b55">[58]</ref><ref type="bibr" target="#b56">[59]</ref><ref type="bibr" target="#b57">[60]</ref>, which remains a theoretical challenge <ref type="bibr" target="#b57">[60]</ref>. VM schedulers are also subject to many other constraints, such as spreading VMs across multiple failure domains. As a result, it is difficult to provision servers that closely match the resource demands of the incoming VM mix.</p><p>When the DRAM-to-core ratio of VM arrivals and the server resources does not match, tight packing becomes more difficult and resources may end up stranded. We define a resource as stranded when it is technically available to be rented to a customer, but is practically unavailable as some other resource has exhausted. The typical scenario for memory stranding is that all cores have been rented, but there is still memory available in the server.</p><p>Reducing stranding via disaggregation. There are multiple techniques that can reduce memory stranding. For example, oversubscribing cores <ref type="bibr" target="#b58">[61,</ref><ref type="bibr" target="#b59">62]</ref> enables more memory to be rented. However, oversubscription only applies to a subset of VMs for performance reasons. Oversubscription also requires a few unrented cores and, thus, cannot address memory stranding. In fact, our stranding measurements at Azure( ?3.1) include multiple clusters that enable oversubscription and still show significant memory stranding.</p><p>The approach we target is to disaggregate a portion of memory into a pool that is accessible by multiple hosts <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. This breaks the fixed nature of server hardware configuration. If memory can be dynamically reassigned to different hosts at different times, we can shift memory resources to where they are needed, instead of relying on each individual server to be configured for all cases pessimistically. Thus, we can provision servers close to the average DRAM-to-core ratios and tackle deviations via the memory pool.</p><p>Disaggregation via CXL. CXL provides a practical basis for memory disaggregation in the form of a low-latency interconnect protocol and allows CPUs to access CXL memory using regular ld and st instructions. The bandwidth of a ?8 CXL link approximates that of a DDR5 channel.</p><p>While the CXL 2.0 specification [63] standardizes a protocol for memory pooling, the standard leaves open key questions in the design and implementation of a memory pool. For example, while the API defines device discovery and doorbells for host/CXL communication, it does not prescribe how the host manages pool memory. Pool memory management is complicated by hardware resource constraints 1  which forces memory allocation to happen at 1GB granularity (slices). and poses memory fragmentation challenges similar to huge pages <ref type="bibr" target="#b60">[64,</ref><ref type="bibr" target="#b61">65]</ref>. Another example is that, while CXL defines APIs for pool management, it does not define the process of reassigning pool memory between hosts and how to schedule VMs on pool memory.</p><p>3 Memory Stranding and Latency Sensitivity</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Stranding at Azure</head><p>In this section, we quantify the severity of memory stranding and frigid memory at Azureusing production data.</p><p>Dataset. We measure stranding in 100 cloud clusters over a 1 Each address range block that is onlined/offlined during pool management requires an invididual entry in the host-managed device memory (HDM) decoder. The number of entries in combination with the pool memory sizes enforces a 1GB entry size.</p><p>75-day period. These clusters host mainstream VM workloads and are representative of the majority of the server fleet. We select clusters with similar deployment years, but spanning all major regions on the planet. A trace from each cluster contains millions of per-VM arrival/departure events, with the time, duration, resource demands, and server-id.</p><p>Memory stranding. Figure <ref type="figure" target="#fig_0">1a</ref> shows the daily average amount of stranded DRAM across clusters, bucketed by the percentage of scheduled CPU cores. In clusters where 75% of CPU cores are scheduled for VMs, 6% of memory is stranded. This grows to over 10% when ?85% of CPU cores are allocated to VMs. This makes sense since stranding is an artifact of highly utilized nodes, which correlates with highly utilized clusters. Outliers are shown by the error bars, representing 5 th and 95 th percentiles. At 95 th , stranding reaches 25% during high utilization periods. Individual outliers even reach 30% stranding.</p><p>Figure <ref type="figure" target="#fig_0">1b</ref> shows stranding over time across 8 racks. A workload change (around day 36) suddenly increased stranding significantly. Furthermore, stranding can affect many racks concurrently (e.g., racks 2, 4-7) and it is generally hard to predict which clusters/racks will have stranded memory.</p><p>NUMA spanning. Many VMs are small and can fit on a single socket. On two-socket systems, the hypervisor at Azureseeks to schedule such VMs entirely (cores and memory) on a single NUMA node. In rare cases, we see NUMA spanning where a VM has all of its cores on one socket and a small amount of memory from another socket. We find that spanning occurs for about 2% of VMs and fewer than 1% of memory pages, on average. Savings from pooling. Azurecurrently does not pool memory. However, by analyzing its VM-to-server traces, we can estimate the amount of DRAM that could be saved via pooling. Figure <ref type="figure" target="#fig_0">1c</ref>   3). This graph shows the performance sensitivity of representative workloads to additional memory latency (as in CXL). X-axis shows 158 representative workloads; Y represents the normalized performance slowdown (i.e., compared to workload performance backed by local DRAM). Workloads: "Proprietary" denotes Azure's internal production workloads, e.g., databases, web search, machine learning, and analytics. The rest are open-source workloads, such as YCSB (A-F) <ref type="bibr" target="#b62">[66]</ref> with in-memory stores (Redisk/VoltDB), in-memory computing Spark workloads in HiBench <ref type="bibr" target="#b63">[67]</ref>, graph processing (GAPBS) <ref type="bibr" target="#b64">[68]</ref>, and high-performance computing benchmark sets such as SPEC CPU <ref type="bibr" target="#b65">[69]</ref>, PARSEC <ref type="bibr" target="#b66">[70]</ref>, and SPLASH2x <ref type="bibr" target="#b67">[71]</ref>.</p><p>this effect diminishes for larger pools. For example, with a fixed 50% pool DRAM, a pool with 32 sockets saves 12% of DRAM while a pool with 64 sockes saves 13% of DRAM. Note that allocating a fixed 50% of memory to pool DRAM leads to significant performance loss compared to socketlocal DRAM ( ?6). Our design overcomes this challenge with multiple techniques ( ?4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary and implications.</head><p>From this analysis, we draw a few important observations and implications for memory disaggregation:</p><p>? We observe 3-16% of stranded memory in production, with some clusters reaching 30%. ? Almost all VMs fit into one NUMA node.</p><p>? Pooling memory across 16-32 sockets can reduce cluster memory demand by 10%. This suggests that memory disaggregation can produce significant cost reductions but assumes that a high percentage of DRAM can be allocated on memory pools. When implementing DRAM pools with cross-NUMA latencies, providers must carefully mitigate potential performance impacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">VM Memory Usage at Azure</head><p>Figure <ref type="figure" target="#fig_0">1d</ref> shows the distribution of percentage of frigid memory (computed as 100 minus the peak memory usage) across our cloud clusters. We calculate the peak memory usage of each VM as the maximum utilization over its lifetime based on the amount of "committed" memory, spot-checked by scanning the VMs' page-access bits ( ?5). Generally, we find that while VM memory usage varies across clusters, all clusters have a significant fraction of VMs with frigid memory. Overall, the 50 th percentile is 50% frigid memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary and implications.</head><p>From this analysis, we draw key observations and implications:</p><p>? VM memory usage varies widely. ? In the cluster with the least amount of frigid memory, still over 50% of VMs have more than 20% frigid memory. Thus, there is plenty of frigid memory that can be disaggregated at no performance penalty.</p><p>? The challenges are (1) predicting how much frigid memory each VM is likely to have and ( <ref type="formula">2</ref>) confining the VM's accesses to local memory. Our design addresses both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Workload Sensitivity to Memory Latency</head><p>To understand the performance impact of CXL latency for a broad variety of workloads in Azure's datacenters, we evaluate 158 workloads under two scenarios where the workload memory is fully backed by either (1) local DRAM or (2) emulated CXL memory. The experimental setups are detailed in ?6.1. Figure <ref type="figure" target="#fig_1">2</ref> presents the workloads' slowdowns when their entire working sets are allocated in CXL memory, compared to having them entirely in local DRAM. We find that 20% of the 158 workloads experience no slowdown under CXL. An additional 23% of the workloads see less than 5% slowdowns. At the same time, some workloads see severe slowdowns: 25% of the workloads take &gt;20% performance hits and 12% will even suffer more than 30% performance degradations.</p><p>Different workload classes are affected differently, e.g., GAPBS (graph processing) workloads generally see higher slowdowns. However, the variability within each workload class is typically much higher than across workload classes. For example, within GAPBS even the same graph kernel reacts very differently to CXL latency, based on different graph datasets. Overall, every workload has at least one workload with less than 5% slowdown and one workload with more than 25% slowdown (except SPLASH2x).</p><p>Azure's proprietary workloads are less impacted than the overall workload set. Of the 13 production workloads, 6 do not see noticeable impact (&lt;1%); 2 see ?5% slowdown; and the remaining half are impacted by 10-28%. This is in part because these production workloads are NUMA-aware and often include data placement optimizations.</p><p>Summary and implications. While the performance of some workloads is insensitive to disaggregated memory latency, some are heavily impacted, experiencing up to 51% slowdowns. This motivates our design decision to include socket-local DRAM alongside pool DRAM to mitigate CXL latency impact for those latency-sensitive workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Memory Disaggregation System Design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Design Goals and Requirements</head><p>Cloud platforms rely on hypervisor-based virtualizationincluding for their container and serverless offerings <ref type="bibr" target="#b68">[72]</ref> -to isolate multiple tenants. Deploying disaggregation at scale within this platform faces multiple requirements, which we categorize into performance goals (PG1-PG3) and functional requirements (FR1-FR3).</p><p>PG1: VM performance. It must be comparable or better than today's offerings. Small VMs have their entire memory backed by socket-local DRAM 98% of the time ( ?3.1). We seek to achieve performance within a small and configurable margin of socket-local DRAM for almost all VMs. We call this margin the performance degradation margin (PDM).</p><p>PG2: Resource efficiency. Providers seek low overheads within the platform software and hardware. For example, CPU cores lost to virtualization overhead cannot be rented out to users [14, <ref type="bibr" target="#b69">73,</ref><ref type="bibr" target="#b70">74]</ref>. Another constraint is the number of available memory decoder entries ( ?2).</p><p>PG3: Small blast radius. Providers seek to offer their VMs high availability while minimizing capacity that cannot be used due to component failure. We thus need to minimize the impact (blast radius) of failures for additional components introduced for memory disaggregation.</p><p>FR1: Customer inertia. Users expect VM compatability to run their OSes/applications without any modifications. Requiring changes limits adoption. Thus, a disaggregation solution must be able to work with opaque and unchanged VMs, guest OSes, and applications. FR2: Compatibility. Providers rely on virtualization accelerators to improve I/O performance [14, <ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b69">73,</ref><ref type="bibr" target="#b70">74]</ref>. For example, direct I/O device assignment (DDA) <ref type="bibr">[14,</ref><ref type="bibr" target="#b13">15]</ref> and Single Root I/O Virtualization (SR-IOV) <ref type="bibr" target="#b69">[73,</ref><ref type="bibr" target="#b70">74]</ref> allow I/O requests to bypass the hypervisor stack. These accelerators are enabled by default for all VM types at Azure.</p><p>DDA and SR-IOV require the whole address range of a VM to be statically pinned in hypervisor-level page tables <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b71">[75]</ref><ref type="bibr" target="#b72">[76]</ref><ref type="bibr" target="#b73">[77]</ref><ref type="bibr" target="#b74">[78]</ref><ref type="bibr" target="#b75">[79]</ref>. As a consequence, memory disaggregation cannot rely on hypervisor page faults, which would be needed to deploy existing RDMA-based systems at the platform level <ref type="bibr" target="#b27">[29]</ref><ref type="bibr" target="#b28">[30]</ref><ref type="bibr" target="#b29">[31]</ref><ref type="bibr" target="#b30">[32]</ref><ref type="bibr" target="#b31">[33]</ref><ref type="bibr" target="#b32">[34]</ref><ref type="bibr" target="#b33">[35]</ref><ref type="bibr" target="#b34">[36]</ref><ref type="bibr" target="#b35">[37]</ref>. For the same reason, we cannot migrate pages between local memory and a pool as used in existing two-tier memory systems <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b46">[49]</ref><ref type="bibr" target="#b47">[50]</ref><ref type="bibr" target="#b48">[51]</ref><ref type="bibr" target="#b49">[52]</ref>. In fact, VM memory allocations can only be changed at VM start or through live migration that temporarily disables acceleration <ref type="bibr" target="#b76">[80]</ref>.</p><p>There are a few options to overcome static pinning. First, the Page Request Interface extension of the PCIe Address Translation Service (ATS/PRI) [81] allows DMA operations to be initiated without requiring memory to be pinned. How-  ever, ATS/PRI devices are not available today due to implementation challenges. Second, device vendors have proposed their own standard for page fault handling <ref type="bibr" target="#b13">[15]</ref>, but this approach does not apply to multiple vendors and device types. Third, virtual IOMMUs <ref type="bibr" target="#b71">[75,</ref><ref type="bibr" target="#b74">78,</ref><ref type="bibr" target="#b75">79]</ref> allow finegrained pinning but require significant guest OS changes. FR3: Commodity hardware. Disaggregation must be deployable on commodity hardware that is available from multiple vendors to avoid single-source supply chain risks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Design Overview</head><p>Our hardware design ( ?4.3) comprises a new external memory controller connected via CXL ( ?2). CXL surfaces pool memory as a load/store device to the CPU, facilitating adding pooled memory to existing applications without software modifications (FR1). Codesigning the hardware with the system software enables us to achieve the functional requirements (FR1-3) and low resource overheads (PG2). We achieve a small blast radius (PG3) via redundant controllers and minimal interleaving ( ?4.4). We achieve high VM performance (PG1) via an intelligent VM scheduling pipeline and distributed performance monitoring and mitigation components ( ?4.5). Our scheduling and monitoring pipelines use novel prediction models ( ?4.6) to ensure that VM performance does not exceed the PDM. Finally, deploying disaggregation in the cloud requires a simple and robust design. Thus, we avoid architectural support for complex features, e.g., cross-pool coherent shared memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hardware Layer</head><p>We introduce a new shared memory module, constructed as an external memory controller (EMC) ASIC with DDR memory. The EMC is a CXL memory device that supports both the configuration and CXL.mem <ref type="bibr" target="#b77">[82]</ref> protocols for pool memory accesses. Each EMC hosts four 80-bit DDR5 channels of pool DRAM and offers multiple CXL links to allow CPU sockets to access the memory.</p><p>As the EMC is a multiported device, it manages arbitration of requests and tracks ownership of memory regions assigned to various hosts. Further, the EMC must support the same reliability, availability and serviceability capabilities <ref type="bibr" target="#b78">[83,</ref><ref type="bibr" target="#b79">84]</ref> of server-grade memory controllers, such as memory error correction, management, and isolation.</p><p>Our memory pool comprises multiple EMCs to minimize the chance of server downtime. Each CPU socket connects to multiple EMCs, for example with a ?8 CXL link which matches the bandwidth of a DDR5 channel. The number of EMCs and attached DDR modules can be scaled independently to meet capacity goals for different clusters. Figure <ref type="figure" target="#fig_2">3</ref> overviews the design and the expected latency contributed by each component. The simplest pool (e.g., 4-8 sockets) would comprise a small set of EMCs, with each directly connected to all sockets in a single large server chassis. For this case, cable lengths are short enough to not require a retimer, which keeps the average-case latency estimate to 67ns (Figure <ref type="figure" target="#fig_2">3</ref>). A larger pool (e.g., 32 sockets) can be configured by connecting each EMC device's ports to a different subset of the CPU sockets. This enables sharing across a significantly larger set of hosts to provide higher stranding reduction. However, larger pools will require retimers to extend the signal reach far enough for all sockets. Retimers add about 10ns of latency in each direction <ref type="bibr" target="#b80">[85,</ref><ref type="bibr" target="#b81">86]</ref>, bringing the overall access latency to 87ns.</p><p>As we observed in ?3.1, VMs at Azuredo not typically span multiple sockets. We target this common case by targeting a single-socket coherence domain <ref type="foot" target="#foot_0">2</ref> . The pool implements an ownership model based on 1GB memory slices ( ?4.4). At the hardware level, the EMC checks all memory accesses for access permission, i.e., whether requestor and owner of the cacheline's address range match. Disallowed accesses result in fatal memory errors.</p><p>A single-socket coherency domain has advantages over standard cache-coherent multiprocessor systems. First, cache coherence significantly increases cost and performance overheads, especially for latency-sensitive workloads. Second, separating the pool memory from the local socket memory opens up flexibility in system memory configurations, providing the ability to compose systems in a finer-grained manner that can better match the different DRAM/core averages seen by different clusters ( ?3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">System Software Layer</head><p>Our system software involves 3 components to manage pools, handle failures, and expose pool memory to VMs.</p><p>Pool management. A pool comprises multiple hosts with each host running a separate hypervisor and operating system within a separate cache coherent domain (one socket). As pool memory is not cache-coherent across hosts, it is explicitly moved between hosts. We call this task pool management, as shown in Figure <ref type="figure" target="#fig_3">4</ref>. It involves two challenges: 1) implementing the control paths for pool-level memory assignment and 2) preventing pool memory fragmentation. Assignment works by enabling/disabling host memory at runtime from the Pool Manager, which implements the CXL 2.0 fabric manager workflow. The Pool Manager is connected to EMCs and CPU sockets via a low-power management bus (e.g., <ref type="bibr" target="#b86">[91]</ref>), as shown in Figure <ref type="figure" target="#fig_2">3</ref>. To allocate pool memory, the Pool Manager uses the bus to communicate to the EMC as well as the driver on the host (via interrupts).</p><p>From the host's point of view, pool memory is mapped but in an offline state at boot time. The BIOS discovers socket-local and pool capacity through CXL device discovery and maps them to the OS address space through the ACPI SRAT tables <ref type="bibr" target="#b87">[92]</ref>. Once mapped, the pool address range is marked hot-pluggable and "not enabled". When the host receives an interrupt, our driver reads which address range is hot-plugged and whether this is an onlining or offlining operation. The driver then communicates with the OS memory manager to bring the memory online/offline.</p><p>As discussed in ?2, pool memory has to be onlined and offlined in 1GB slices due to hardware resource constraints. If these large slices lead to memory fragmentation, we face inefficiencies as the contiguous 1GB address range must be free before we can offline a 1GB slice for reassignment to another host. We overcome fragmentation using two techniques. First, pool memory is allocated to VMs in 1GBaligned increments ( ?4.5). While this prevents fragmentation due to VM starts and completions, our experience has shown that memory allocations from host agents and drivers can still fragment slice memory. Our second technique is a special-purpose memory partition for pool memory. The memory manager ensures that only the hypervisor can allocate memory from this partition. Host agents and drivers allocate memory in another memory partition, which effectively contains fragmentation.</p><p>With these optimizations, our initial experiments with offlining 1 GB slices on the host show that it takes 10-100 milliseconds per GB. In contrast, onlining memory has insignificant overheads on the order of microseconds per GB. This means that shifting memory after a 128GB VM is completed would require tens of seconds, which is insufficient for VM-start SLOs at Azure. This observation is reflected in our asynchronous offlining process ( ?4.5). Exposing pool memory to VMs. A key building block of our latency management mechanism is to expose pool memory as a zNUMA node. To implement a zNUMA node, the hypervisor adds a memory block (node memblk) without any associated CPU cores (no entry in the node cpuid) in the ACPI SLIT/SRAT tables <ref type="bibr" target="#b87">[92]</ref>. Figure <ref type="figure" target="#fig_4">5</ref> shows how this appears from the point of view of a Linux VM. The figure also shows that the hypervisor exposes the correct latency in the NUMA distance matrix (numa slit) to facilitate the guest OS's memory management.</p><p>When we expose a zNUMA node to any modern guest OS, it preferentially allocates memory from the local NUMA node before going to zNUMA. Recall the observation on VM memory usage in Figure <ref type="figure" target="#fig_0">1d</ref>. This means that, if we can predict the amount of frigid memory for a VM ( ?4.6), and size the zNUMA for that VM, then we can effectively focus the VMs memory allocations on its local and lower-latency NUMA node. The VM's guest OS can also use NUMAaware memory management <ref type="bibr" target="#b47">[50,</ref><ref type="bibr" target="#b88">93]</ref>.</p><p>As mentioned above, we avoid fragmentation by backing each zNUMA node with contiguous 1GB-aligned address ranges from the pool memory partition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Distributed Control Plane Layer</head><p>Figure <ref type="figure" target="#fig_6">6</ref> shows the two tasks performed by our control plane: (A) predictions to allocate memory during VM scheduling and (B) QoS monitoring and resolution. Predictions and VM scheduling (A). We use ML-based prediction models ( ?4.6) to decide how much pool memory can be allocated to a VM during scheduling. After a VM request arrives (A1), the scheduler queries the distributed ML serving system (A2) for a prediction on how much local memory to allocate for the VM. The scheduler then informs the Pool Manager about the target host and associated pool memory needs (A3). The Pool Manager triggers a memory onlining workflow using its configuration bus connections  A) The VM scheduler uses ML-based predictions that identify latency-sensitive VMs and their likely amount of frigid memory to decide on VM placement (see Figure <ref type="figure">7</ref>). B) The monitoring pipeline reconfigures VMs if quality-of-service (QoS) targets are not met.</p><p>to the EMCs and host (A4). As described in ?4.4, memory onlining can be completed in less than a millisecond even for large VMs. VM terminations trigger memory offlining, which the Pool Manager performs in the background of VM creation. Finally, the VM scheduler informs the host's hypervisor to start the VM using a zNUMA node matching the onlined memory amount. QoS monitoring (B). After VMs are allocated, we continuously inspect their performance via the QoS monitor. The monitor queries hypervisor and hardware performance counters (B1) and uses an ML model of latency sensitivity ( ?4.6) to decide whether the VM is being negatively affected by its pool memory latency. If the impact exceeds a threshold, the monitor triggers a live VM migration <ref type="bibr" target="#b76">[80]</ref> via the migration manager (B2), which effects the migration (B3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Prediction Models</head><p>Our VM scheduling (A) and QoS monitoring (B) algorithms rely on two prediction models as shown in Figure <ref type="figure">7</ref>. Predictions for VM scheduling (A). For scheduling, we first check if we can correlate a workload history with the VM requested. This works by checking if there have been previous VMs with the same metadata as the request VM, e.g., the customer's subscription, VM types, and location. This is based on the observation that VMs from the same subscription tend to exhibit similar behaviors <ref type="bibr" target="#b52">[55]</ref>.</p><p>If we have prior workload history, we make a prediction on whether this VM is likely to be memory latency insensitive, i.e., its performance would be within the PDM while using only pool memory. (Model details appear below.) Latencyinsensitive VMs are allocated entirely on pool DRAM.</p><p>If the VM has no workload history or is predicted to be latency-sensitive, we predict the minimum amount of frigid memory over its lifetime. Interestingly, frigid memory predictions with only generic VM metadata such as VM type, guest OS, and location are accurate ( ?6). VMs without frigid memory are allocated entirely with local DRAM. VMs with a FM &gt; 0 percentage of frigid memory are allocated with FM% of pool memory and a corresponding zNUMA node; the remaining memory is allocated on local DRAM.</p><p>If we underpredict FM, the VM will not touch the slower pool memory as the guest OS prioritizes allocating local DRAM. If we overpredict FM, we rely on the QoS monitor for mitigation. Importantly, we always keep a VM's memory mapped in hypervisor page tables at all times. This means that even if our predictions happen to be incorrect, performance does not fall off a cliff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QoS monitoring (B).</head><p>For zNUMA VMs, we monitor if it overpredicted the amount of frigid memory during scheduling. For VMs entirely backed with pool DRAM and zNUMA VMs with less frigid memory than predicted, we use the sensitivity model to determine whether the VM workload is suffering excessive performance loss. If not, the QoS monitor initiates a live VM migration to a configuration allocated entirely on local DRAM.</p><p>Model details and compatibility with opaque VMs. The two types of prediction models, latency insensitivity and frigid memory, must work for opaque VMs. We achieve this requirement by relying on hardware and hypervisor telemetry. To predict latency sensitivity and performance impact, we rely on core-PMU performance counters motivated by Intel's top-down method for performance analysis (TMA) <ref type="bibr" target="#b89">[94,</ref><ref type="bibr" target="#b90">95]</ref>. We remark that, while TMA was developed for Intel, its relevant parts are available on AMD and ARM as well <ref type="bibr" target="#b91">[96]</ref>. In TMA, the CPU's pipeline usage is broken down into hierarchical categories such as "memory bound", which is defined as pipeline stalls due to memory loads and stores. We further subdivide memory boundedness into "DRAM latency bound" and other components. Based on these metrics and sampled measurements of performance slowdowns, we train a model that predicts whether the slowdown exceeds the PDM. Our evaluation discusses the accuracy of this approach ( ?6) on a set of 150 workloads. In a production deployment, we will further augment our model with labels gathered from internal systems running in Azure's cloud as these systems can make performance metrics available to us.</p><p>To train our memory footprint model, we rely on hypervisor telemetry. We conservatively define memory footprint as the peak number of pages that the VM touches during its lifetime. This footprint is available by scanning access bits in the hypervisor page table ( ?5). Access bti scans are also used in VM working set size estimation <ref type="bibr" target="#b92">[97,</ref><ref type="bibr" target="#b93">98]</ref>; however, we use a more conservative approach that resets access bits only every 30 minutes to minimize VM performance overheads. We further augment the access bit scanning data with an existing approximate memory usage counter that is available for 98% of VMs in Azureas a hypervisor counter.</p><p>Parameterization of prediction models. Our latency insensitivity model is parameterized to stay below a target rate of false positives (FP), i.e., workloads it incorrectly specifies as latency insensitive but which are actually sensitive to memory latency. This parameter enforces a tradeoff as the percentage of workloads that are labeled as latency insensitive (LI) is a function of FP. For example, a small rate of FP like 0.1% also enforces a small percentage of LI say 5%.</p><p>Similarly, our frigid memory model is paramterized to stay below a target rate of overpredictions (OP), i.e., workloads that have less frigid memory than predicted and thus will use memory pages on the zNUMA node. This parameter also enforces a tradeoff as the percentage of frigid memory (FM) is a function of OP. For example, a small rate of OP like 0.1% also enforces a small percentage of FM say 3%.</p><p>With two models and their respective parameters, we need to decide how to balance FP and OP between the two models, while maximizing the average amount of memory that is allocated on the CXL pool. This optimization problem is instantiated with the given performance degradation margin (PDM) and the percentage of VMs that can be allowed to exceed the degradation goal (X).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>maximize(#LI PDM ) + (%FM)</head><p>subject to(FP PDM ) + (OP) ? X</p><p>(1)</p><p>Note that X essentially defines how often the QoS monitor has to engage and initiate live migrations. Thus, we instatiate X based on a budget for live migrations. Besides X and the PDM, our design has no other parameters as it automatically solves the optimization problem from Eq.( <ref type="formula">1</ref>). The models have no other parameters as we rely on their respective framework's default hyperparameters ( ?5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Implementation</head><p>While we do not yet have hardware for our new EMC, implementing and deploying our design requires extensive testing within Azure's system software and distributed control plane. At the same time, we find that existing two-socket servers from Intel can near-perfectly emulate the latency characteristics of our memory pool. We thus implement and test our software changes on existing servers both in the lab as well as on production nodes.</p><p>System software. This implementation comprises three parts. First, we emulate a single-socket system with a CXL pool on a two-socket server by disabling all cores in one socket, while keeping its memory accessible from the other socket. This memory mimics the pool. Second, we change Azure's hypervisor to instantiate arbitrary zNUMA topologies. We extend the API between the control plane and the host to pass the desired zNUMA topology to the hypervisor.</p><p>Third, we implement support in Azure's hypervisor for the telemetry required for training our models. To associate hardware counters with individual VMs, we extend the virtual core struct with a copy of the virtual core's hardware counter state. This enables us to consistently associate hardware counters with a VM as it gets scheduled on different physical cores. Specifically, the hypervisor copies the virtual hardware counter state to physical core registers when the virtual core is scheduled. When the virtual core is descheduled, we save the hardware register state in the virtual core struct. To enable accurate footprint telemetry, we enable access bit scanning in hypervisor-level address translation tables. Since our implementation only needs to estimate the memory footprint, we scan and reset access bits every 30 minutes, which adds virtually no overhead.</p><p>Distributed control plane. We train our prediction models by aggregating daily telemetry into a central database. The latency insensitivity model uses a simple random forest (RandomForest) from Scikit-learn <ref type="bibr" target="#b94">[99]</ref> to classify whether a workload exceeds the PDM. The model uses a set of 200 hardware counters as supported by current Intel processors. The frigid memory model uses a gradient boosted regression model (GBM) from LightGBM <ref type="bibr" target="#b95">[100]</ref> and makes a quantile regression prediction with a configurable target percentile. After exporting to ONNX <ref type="bibr" target="#b96">[101]</ref>, the prototype adds the prediction (the size of zNUMA) on the VM request path using a custom inference serving system similar to <ref type="bibr" target="#b97">[102]</ref><ref type="bibr" target="#b98">[103]</ref><ref type="bibr" target="#b99">[104]</ref>. Azure's VM scheduler incorporates zNUMA requests and pool memory as an additional dimension into its bin packing, similar to other cluster schedulers <ref type="bibr" target="#b53">[56,</ref><ref type="bibr" target="#b100">[105]</ref><ref type="bibr" target="#b101">[106]</ref><ref type="bibr" target="#b102">[107]</ref><ref type="bibr" target="#b103">[108]</ref><ref type="bibr" target="#b104">[109]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>We seek to answer the following questions in the evaluation: -What is the performance of zNUMA VMs on production and lab nodes? ( ?6.2 and ?6.3) -How accurate are our prediction models? ( ?6.4) -What are the end-to-end DRAM savings? ( ?6.5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>We evaluate the performance of our prototype using a large set of cloud workloads. Specifically, we evaluate 158 workloads spanning in-memory databases and KV-stores (Redis <ref type="bibr" target="#b105">[110]</ref>, VoltDB <ref type="bibr">[111]</ref>, and TPC-H on MySQL <ref type="bibr" target="#b106">[112]</ref>), data and graph processing (Spark <ref type="bibr" target="#b63">[67]</ref> and GAPBS <ref type="bibr" target="#b64">[68]</ref>), HPC (SPLASH2x <ref type="bibr" target="#b67">[71]</ref>), CPU and shared-memory benchmarks (SPEC CPU <ref type="bibr" target="#b65">[69]</ref> and PARSEC <ref type="bibr" target="#b66">[70]</ref>), and a range of Azure's internal workloads (Proprietary). Figure <ref type="figure" target="#fig_1">2</ref> overviews these workloads. We additionally use VM scheduling simulations to quantify the DRAM savings for a deployment at scale. Prototype setup. We run experiments on production servers at Azureand similarly-configured lab servers. The production servers use two 24-core Intel Skylake 8157M with 256GB of DDR4 on each socket. For socket-local DRAM, we measure an access latency of 78ns and bandwidth exceeding 80GB/s. The zNUMA memory latency is 64ns higher which is slightly faster than a CXL pool. The bandwidth when using only zNUMA memory is around 30GB/sabout 3/4 of the bandwidth of a CXL ?8 link. Our BIOS disables hyper-threading, turbo-boost, and C-states.</p><p>We use performance when entirely backed by socket-local DRAM as our baseline. We present performance as normalized slowdowns, i.e., the ratio to the baseline. Performance metrics are workload specific, e.g., job runtime, throughput and 99 th tail latency, etc.</p><p>Each experiment involves running the application with one of 7 zNUMA sizes (as percentages of the workload's memory footprint in Figure <ref type="figure" target="#fig_7">9</ref>). With at least three repetitions of each run and 158 workloads, our evaluation spans more than 3,500 experiments and 10,000 machine hours. All of these experiments were performed on lab servers; we repeat spot check for outliers on production servers.</p><p>Simulations and model evaluation. Our simulations are based on traces of production VM requests and their placement on servers. The traces are from randomly selected 100 clusters across 34 datacenters globally over 75 days.</p><p>The simulator implements different memory allocation policies and tracks each server and each pool's memory capacity at second accuracy. Generally, the simulator schedules VMs on the same nodes as in the trace and changes their memory allocation to match the policy. For rare cases where a VM does not fit on a server, e.g., due to insufficient pool memory, the simulator moves the VMs to another server.</p><p>The traces also include resource logs which we use to evaluate our frigid memory prediction model. We observe that 80% of VMs have sufficient history to make a sensitivity prediction. However, the trace does not include the workload's perceived performance (opaque VMs). We thus evaluate our latency sensitivity model based on our 158 workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">zNUMA VMs on Production Nodes</head><p>We perform a small-scale experiment on Azureproduction nodes to validate our intuition on zNUMA VMs. The experiment evaluates four internal workloads: an audio/video conferencing application, a database service, a key-value store, and a business analytics service (interactive data visualization and business intelligence). To see the effectiveness of zNUMA, we assume a correct prediction of frigid memory, i.e., the local footprint fits into the VM's local vNUMA node. Figure <ref type="figure">8</ref> shows access bit scans over 48 hours from the video workload and a table that shows the traffic to the zNUMA  node for the four workloads. Finding 1. The access bit scan shows that accesses are spread out within the local vNUMA node but rarely happen on the zNUMA node. The zNUMA nodes are effective at containing the memory access to the local vNUMA node. However, a small fraction of accesses goes to the zNUMA node. We suspect that this is in part due to the guest OS memory manager's metadata that is explicitly allocated on each vNUMA node. We find that the video workload sends fewer than 0.25% of memory accesses to the zNUMA node. Similarly, the other three workloads send 0.06-0.38% of memory access to the zNUMA node. Implications. With a negligible fraction of memory accesses on zNUMA, we deduce that performance impacts from the higher latency of CXL will likely have negligible effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">zNUMA VMs in the Lab</head><p>We scale up our evaluation to 158 workloads in a lab setting. Since we fully control these workloads, we can now also explicitly measure their performance. We rerun each workload for differently-sized zNUMA nodes between 0-100% of the workload's footprint. Note that for latency sensitive workloads, we regularly size the zNUMA to match the footprint (100%). Thus, this is effectively a sensitivity study and shows what happens without our frigid memory prediction model. Figure <ref type="figure" target="#fig_7">9</ref> shows a CDF of slowdowns for 0-90% local DRAM. Finding 2. With a correct prediction of frigid memory, workload slowdowns are usually 0%. For a few workloads, slowdowns are ?2% due to variability of the workload across of repetitions (noise).</p><p>Implications. This performance result is expected since very few accesses are issued to the zNUMA node ( ?6.2). This validates that few accesses translates into minimal performance impact. Our evaluation can thus assume no performance impact under correct predictions of frigid memory ( ?6.5). Finding 3. For overpredictions of frigid memory (and correspondingly undersized local vNUMA nodes), performance generally improves with increasing percentage of local DRAM. We find that good performance requires significant amounts of local DRAM. For example with 25% local DRAM (orange line in Figure <ref type="figure" target="#fig_7">9</ref>), we see the slowdown distribution largely overlaps with the distribution for 0% local DRAM. Furthermore, we find that a long tail remains even for high percentages of local DRAM. As we increase the percentage of local memory to 40%, and 90%, the slowdown decreases, but the tail remains. For example, with 10% of CXL memory (gray line in the figure), almost 80% of workloads experience less than 5% degradation. However, the last 5% of workloads continue to experience more than 20% performance degradation and the last 1% of workloads see 30%. We use DAMON to verify that these workloads indeed actively access their entire working set.</p><p>Implications. Allocating a fixed percentage of pool DRAM to VMs leads to significant performance slowdowns. There are only two strategies to reduce this impact: 1) identify which workloads will see slowdowns and 2) allocate frigid memory on the pool. We employ both strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Performance of Prediction Models</head><p>We evaluate our prediction models ( ?4.6) and its combined prediction model based on Eq.(1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1">Predicting Latency Sensitivity</head><p>We seek to predict whether a VM is latency insensitive, i.e., whether running the workload on pool memory would stay within the performance degradation margin (PDM). We tested the model for PDM between 1-10%, but report details only for 5%. Other PDM values lead to qualitatively similar results. We compare simple thresholds on memory and DRAM boundedness <ref type="bibr" target="#b89">[94,</ref><ref type="bibr" target="#b90">95]</ref> to our RandomForest ( ?5).</p><p>Figure <ref type="figure" target="#fig_8">10a</ref> shows the correlation between DRAM boundedness and performance slowdowns. Figure <ref type="figure" target="#fig_8">10b</ref> shows the model's false positive as a function of the percentage of workloads labeled as latency insensitive, similar to a precision-recall curve <ref type="bibr" target="#b107">[113]</ref>. Error bars show 99 th percentiles from a 100-fold validation based on randomly split- ting into equal-sized training and testing datasets. Finding 4. While DRAM boundedness is correlated with slowdown, we find examples where high slowdown occurs even for a small percentage of DRAM boundedness (points at the top-left corner in Figure <ref type="figure" target="#fig_8">10a</ref>). For example, multiple workloads exceed 20% slowdown with just two percent of DRAM boundedness.</p><p>Implication. This shows the general hardness of predicting whether workloads exceed the PDM. Predictors will always have some statistical errors. Finding 5. We find that "DRAM bound" significantly outperforms "Memory bound" (Figure <ref type="figure" target="#fig_8">10b</ref>). Our RandomForest performs slightly better than "DRAM bound".</p><p>Implication. Our RandomForest can place 30% of workloads on the pool with only 2% of false positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2">Predicting Frigid Memory</head><p>We seek to predict the amount of frigid memory over a VM's future lifetime ( ?4.6). We evaluate this model using metadata and resource usage logs from 100 clusters over 75 days. The model is trained on VMs that started and ended during the first 15 days and evaluated on the subsequent 60 trace days. Figure <ref type="figure" target="#fig_8">10c</ref> compares our GBM model to a simple policy where a fixed fraction of memory is assumed to be frigid across all VMs. The figure shows the rate of overpredictions as a function of the average amount of frigid memory (higher is better). Finding 6. We find that the GBM model is 3.5? more accurate than the static policy, e.g., when labeling 10% of memory as frigid, GBM overpredicts only 2.5% of VMs while the static policy overpredicts 9%.</p><p>Implication. Our prediction model can identify 20% of frigid memory while only overpredicting 4% of VMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.3">Combined Prediction Models</head><p>We evaluate our combined prediction model (Eq.( <ref type="formula">1</ref>)) based on the number of "scheduling mispredictions", i.e., the fraction of VMs that will exceed the PDM. This incorporates the  overpredictions of frigid memory, how much the model overpredicted, and the probability of this overprediction leading to a workload exceeding the PDM. Figure <ref type="figure" target="#fig_8">10d</ref> showing scheduling mispredictions as a function of the average amount of cluster DRAM that is allocated on its pools. Finding 7. Our combined model outperforms both of the individual models by finding their optimal combination. Implication. We can schedule 40% of DRAM on pools with only 2% of scheduling mispredictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">End-to-end Reduction in Stranding</head><p>We evaluate our design's end-to-end performance while constraining its rate of scheduling mispredictions. Each scheduling misprediction will have to be mitigated by the QoS monitor and a potential VM live migration.</p><p>Figure <ref type="figure" target="#fig_9">11</ref> shows the reduction in overall memory across all clusters as a function of pool scope for "full design", a variant with only frigid-memory predictions, and a strawman static allocation policy. We evaluate two scenarios, Figure <ref type="figure" target="#fig_9">11a</ref> shows PDM =5% and scheduling mispredictions X=2% and Figure <ref type="figure" target="#fig_9">11a</ref> shows PDM =1% and scheduling mispredictions X=6%. We compare our design to a static strawman policy. In the first scenario, the strawman statically allocates each VM with 15% of pool DRAM. About 10% of VMs would touch the pool DRAM (Figure <ref type="figure" target="#fig_8">10c</ref>). Of those touching pool DRAM, we'd expect that about 1  4 would see a slowdown exceeding a PDM =5% (Figure <ref type="figure" target="#fig_7">9</ref>). So, this strawman would have about 2.5% of scheduling mispredictions. In the second scenario, the strawman statically allocates each VM with 10% of pool DRAM and has about 6% scheduling mispredictions. Finding 8. At a pool scope of 32 sockets and in the first scenario, we reduce overall DRAM requirements by 10%, while our frigid-only variant reduces DRAM by 7%, and static reduces DRAM by less than 4%. In the second scenario, we reduce DRAM requirements by slightly less. Implication. We can safely reduce DRAM cost and can make up for a more aggressive performance target (PDM) by relying more on online mitigations using its QoS monitor. Finding 9. Throughout the simulations, our pool memory offlining speeds remain are below 1GB/s for 99.99% and below 10GB/s for 99.999% of VM starts. Implication. Our design is practical as it stays within the memory offlining limits of our prototype (10GB/s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Table <ref type="table" target="#tab_5">1</ref> compares our design to state-of-the-art memory disaggregation approaches at the hardware (HW), hypervisor/ host (VMM/OS), runtime, and application (App) levels. Hardware-level disaggregation: Hardware-based disaggregation designs <ref type="bibr">[13, 17-20, 114, 115]</ref> are not easily deployable as they do not rely on commodity hardware. For instance, ThymesisFlow <ref type="bibr" target="#b12">[13]</ref> proposes an FPGA-based rack-scale memory disaggregation design on top of Open-CAPI <ref type="bibr" target="#b110">[116]</ref>. It shares some similar goals with our work, however, its design and performance target are fundamentally different. ThymesisFlow advocates application changes for performance, while we focuse on platform-level MLdriven pool memory management that is transparent to users. Hypervisor/OS level disaggregation: Hypervisor/OS level approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">[28]</ref><ref type="bibr" target="#b27">[29]</ref><ref type="bibr" target="#b28">[30]</ref><ref type="bibr" target="#b29">[31]</ref><ref type="bibr" target="#b30">[32]</ref><ref type="bibr" target="#b31">[33]</ref><ref type="bibr" target="#b32">[34]</ref><ref type="bibr" target="#b33">[35]</ref><ref type="bibr" target="#b34">[36]</ref><ref type="bibr" target="#b35">[37]</ref> rely on page faults and access monitoring to maintain the working set in local DRAM. Such OSbased approaches bring significant overhead, jitter, and are incompatible with virtualization acceleration (e.g., DDA). Runtime and application level disaggregation: Runtimebased disaggregation designs <ref type="bibr" target="#b19">[21]</ref><ref type="bibr" target="#b20">[22]</ref><ref type="bibr" target="#b21">[23]</ref> propose customized APIs for remote memory access. Similarly, some applications (e.g., databases) are specifically designed to leverage remote memory for scalability and performance <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b112">[118]</ref><ref type="bibr" target="#b113">[119]</ref><ref type="bibr" target="#b114">[120]</ref>. While effective, this approach requires developers to explicitly use these mechanisms at the application level. Memory tiering: Prior works have considered the broader impact of extended memory hierarchies and how to handle them <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b49">52,</ref><ref type="bibr" target="#b115">[121]</ref><ref type="bibr" target="#b116">[122]</ref><ref type="bibr" target="#b117">[123]</ref>. For example, Google achieves 6?s latency via proactive hot/cold page detection and compression <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b48">51]</ref>. Thermostat <ref type="bibr" target="#b49">[52]</ref> uses an online page classification method for efficient application-transparent and hugepage-aware two-tier memory management. Nimble <ref type="bibr" target="#b47">[50]</ref> optimizes Linux's page tracking mechanism to tier pages for increased migration bandwidth. HeMem <ref type="bibr" target="#b46">[49]</ref> manages page migrations between tiered DRAM/NVM with low-overhead and asynchronous policies (e.g., cpu events sampling). Our work takes a different ML-based approach looking at memory disaggregation design at the platform-level and is generally orthogonal to these prior works. ML for systems: ML is increasingly applied to tackle systems problems, such as cloud efficiency <ref type="bibr" target="#b52">[55,</ref><ref type="bibr" target="#b58">61]</ref>, memory/storage optimizations <ref type="bibr" target="#b118">[124,</ref><ref type="bibr" target="#b119">125]</ref>, microservices <ref type="bibr" target="#b120">[126]</ref>, caching/prefetching policies <ref type="bibr" target="#b121">[127,</ref><ref type="bibr" target="#b122">128]</ref>. We uniquely apply ML methods for frigid memory prediction to support pooled memory provisioning to VMs without jeopardizing QoS. Coherent memory and NUMA optimizations: Traditional cache coherent NUMA architectures <ref type="bibr" target="#b123">[129]</ref> use specialized interconnects to implement a shared address space. There are also many system-level optimizations for NUMA, such as NUMA-aware data placement <ref type="bibr" target="#b124">[130]</ref> and proactive page migration <ref type="bibr" target="#b88">[93]</ref>. Distributed Shared Memory (DSM) systems offer a coherent global address space across the network (e.g., Ethernet) <ref type="bibr" target="#b125">[131,</ref><ref type="bibr" target="#b126">132]</ref>. AutoNUMA <ref type="bibr" target="#b43">[46]</ref> and other NUMA scheduling policies <ref type="bibr" target="#b44">[47,</ref><ref type="bibr" target="#b45">48]</ref> try to balance compute and memory across NUMA nodes, but is not applicable to CXL memory as it has no local cores. Our design does not require coherence as allocated pool memory to a VM will not be shared with others. zNUMA's zero-core nature requires rethinking of existing optimizations which are largely optimized for symmetric NUMA systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>DRAM costs are an increasing cost factor for cloud providers. This paper is motivated by an analysis of stranded and allocated-but-unused memory across 100 production cloud clusters. We proposed the first CXL-based full-stack memory disaggregation design that satisfies the requirements of cloud providers. Our design comprises contributions at the hardware, system software, and distributed system layers. Our results showed that a first-generation memory disaggregation can reduce the amount of needed DRAM by 9-10%. This translates into an overall reduction of 4-5% in cloud server cost.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Memory stranding. (a) Stranding increases significantly as more CPU cores are scheduled, the error bars indicate the 5 th and 95 th percentile (outliers in red); (b) stranding dynamics change over time and can affect a broad set of servers, (c) small pools of around 32 sockets are sufficient to significantly reduce overall memory needs; and (d) VMs have a significant amount of frigid memory.</figDesc><graphic url="image-1.png" coords="3,182.49,72.15,126.00,99.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Workload performance slowdown under additional 64ns memory latency ( ?3.3). This graph shows the performance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Our hardware design. Each CPU connects to several external memory controllers (EMC). Each EMC offers CXL ports for several sockets and DDR ports for several DIMMs, which constitute the pool. Dashed lines around each CPU and EMC indicate separate failure domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Typical pool management task. Pool memory isassigned to at most one host at a time. This figure shows the flow when a VM (VM2) terminates on a host (H1) at time t=0 and the memory is needed on another host (H2). H1's memory manager first offlines each memory slice previously used by VM2 (t=1). The Pool Manager (PM) then reassigns these slices to H2 (t=2). H2's manager then onlines these slices and starts the new VM (t=3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: zNUMA. zNUMA as seen from inside a Linux VM Minimizing blast radius. To minimize the blast radius of any single component and facilitate memory hot-plugging, hosts do not use interleaving across their attached EMCs. Consequently, each socket and each EMC is a separate failure domain, as shown by the dashed boxes in Figure 3. If an EMC fails, only the VMs using that specific component/link will be affected, while VMs running on local DRAM and other EMCs can continue to run. If a CPU socket fails, the associated pool memory can be freed and reallocated to other sockets. If the Pool Manager fails, VMs can continue to run and access memory, but we cannot reallocate pool memory.Exposing pool memory to VMs. A key building block of our latency management mechanism is to expose pool memory as a zNUMA node. To implement a zNUMA node, the hypervisor adds a memory block (node memblk) without any associated CPU cores (no entry in the node cpuid) in the ACPI SLIT/SRAT tables<ref type="bibr" target="#b87">[92]</ref>. Figure5shows how this appears from the point of view of a Linux VM. The figure also shows that the hypervisor exposes the correct latency in the NUMA distance matrix (numa slit) to facilitate the guest OS's memory management.When we expose a zNUMA node to any modern guest OS, it preferentially allocates memory from the local NUMA node before going to zNUMA. Recall the observation on VM memory usage in Figure1d. This means that, if we can predict the amount of frigid memory for a VM ( ?4.6), and size the zNUMA for that VM, then we can effectively focus the VMs memory allocations on its local and lower-latency NUMA node. The VM's guest OS can also use NUMAaware memory management<ref type="bibr" target="#b47">[50,</ref><ref type="bibr" target="#b88">93]</ref>.As mentioned above, we avoid fragmentation by backing each zNUMA node with contiguous 1GB-aligned address ranges from the pool memory partition.</figDesc><graphic url="image-2.png" coords="7,55.20,72.00,237.72,59.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Control plane workflow ( ?4.5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Slowdown CDFs under mixed local/pool memory. This figure shows the CDFs of workload performance slowdown under various mixed local and pool memory, e.g., 25% (orange line) represents 25% of workload memory is backed by local DRAM and the rest 75% by pool memory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Performance of our prediction models. (a) shows the correlation between DRAM bound and slowdowns; (b) compares heuristics to a ML-based prediction model for whether a workload exceeds 5% performance degradation; (c) compares a static to a ML-based prediction models for frigid memory; (d) shows the tradeoff between average allocation of pool memory and mispredictions, which have to be mitigated via live migrations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Memory savings under performance constraints ( ?6.5). Simulated end-to-end evaluation of memory savings under two different performance scenarios: (a) PDM =5% and scheduling mispredictions X=2%. (b) PDM =1% and scheduling mispredictions X=6%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Our prediction models ( ?4.6). Our two prediction models (dark grey) rely on telemetry (green boxes) that is available for all VM types, including third-party opaque VMs.</figDesc><table><row><cell>(A) VM scheduling</cell><cell>Workload history? Latency insensitive? Frigid memory? Entirely pool DRAM HW counter history Pool DRAM = frigid Entirely local DRAM Existing VM metadata Yes No Yes No Yes No</cell><cell>Decision Prediction Runtime model input Legend:</cell></row><row><cell>(B) QoS monitoring</cell><cell>Hypervisor telemetry Latency insensitive? No VM live migration Continue monitoring HW counters Yes Overpredicted frigid? Continue monitoring No Yes</cell><cell>model Action</cell></row><row><cell cols="2">Figure 7:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Effectiveness of zNUMA ( ?6.2). We schedule latency sensitive workloads with two vNUMA nodes. The local vNUMA node is large enough to cover the workload's footprint and the zNUMA nodes holds the VM's remaining memory on the CXL pool. Access bit scans, e.g., for Video (right), show that this configuration indeed minimizes traffic to the zNUMA node.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Workload Traffic to</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>zNUMA</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Video</cell><cell>0.25%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>DB</cell><cell></cell><cell>0.06%</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">KV store</cell><cell>0.11%</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Analytics</cell><cell>0.38%</cell></row><row><cell>1 Figure 8: 0 .2 .4 .6 .8</cell><cell></cell><cell></cell><cell cols="3">Local-DRAM% 90% 80% 60% 40% 25% 0%</cell></row><row><cell>0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell></row><row><cell></cell><cell></cell><cell cols="2">Slowdown (%)</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>Memory disaggregation approaches ( ?7). Our design satisfies key requirements for public cloud datacenters.</figDesc><table><row><cell>H W [1 3 , 1 7 -2 0 , 1 1 5 , 1 1 7 ] V M M /O S [2 9 -3 7 ] R u n ti m e [2 1 -2 3 ] A p p [2 4 -2 7 ] O u r d e si g n</cell></row><row><cell>Inertia</cell></row><row><cell>Performance</cell></row><row><cell>Deployability</cell></row><row><cell>VirtAccel</cell></row><row><cell>CpuEfficiency</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>With rising per-socket core counts, industry trends point towards singlesocket servers<ref type="bibr" target="#b82">[87]</ref><ref type="bibr" target="#b83">[88]</ref><ref type="bibr" target="#b84">[89]</ref><ref type="bibr" target="#b85">[90]</ref>. When deploying on two-socket hosts, one would connect both sockets of each host to the EMC to achieve symmetric one-NUMA-hop latency to all DDR memory.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Gen-Z</forename><surname>Cxl And</surname></persName>
		</author>
		<ptr target="https://www.nextplatform.com/2020/04/03/cxl-and-gen-z-iron-out-a-coherent-interconnect-strategy/" />
		<title level="m">Iron Out A Coherent Interconnect Strategy</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Memory Scaling: A Systems Architecture Perspective</title>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Memory Workshop (IMW)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ArchShield: Architectural Framework for Assisting DRAM Scaling by Tolerating High Error Rates</title>
		<author>
			<persName><forename type="first">Prashant</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dae-Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moinuddin</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 40th Annual International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Main Memory Scaling: Challenges and Solution directions</title>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">More than Moore Technologies for Next Generation Computer Design</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Technology Scaling Challenge and Future Prospects of DRAM and NAND Flash Memory</title>
		<author>
			<persName><forename type="first">Sung-Kye</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Memory Workshop (IMW)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scaling and Performance Challenges of Future DRAM</title>
		<author>
			<persName><forename type="first">Shigeru</forename><surname>Shiratake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Memory Workshop (IMW)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<ptr target="https://semiengineering.com/the-next-new-memories/" />
		<title level="m">The Next New Memories</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<ptr target="https://www.forbes.com/sites/tomcoughlin/2021/03/16/micron-ends-3d-xpoint-memory/" />
		<title level="m">Micron Ends 3D XPoint Memory</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Evaluating Job Packing in Warehouse-scale Computing</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madhukar</forename><surname>Korupolu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Wilkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Cluster Computing (Cluster)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">the Next Generation</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Tirmazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><forename type="middle">E</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gene</forename><surname>Zhijing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mor</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Harchol-Balter</surname></persName>
		</author>
		<author>
			<persName><surname>Wilkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 EuroSys Conference (EuroSys)</title>
		<meeting>the 2020 EuroSys Conference (EuroSys)<address><addrLine>Borg</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">143</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Network Requirements for Resource Disaggregation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sagar</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Karandikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangjin</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachit</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvia</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName><surname>Shenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting>the 12th USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<biblScope unit="volume">137</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Remote Memory in the Age of Fast Networks</title>
		<author>
			<persName><forename type="first">Marcos</forename><forename type="middle">K</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Calciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Deguillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratap</forename><surname>Subrahmanyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lalith</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiran</forename><surname>Tati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajesh</forename><surname>Venkatasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM Symposium on Cloud Computing (SoCC)</title>
		<meeting>the 8th ACM Symposium on Cloud Computing (SoCC)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Thymes-isFlow: A Software-Defined, HW/SW Co-Designed Interconnect Stack for Rack-Scale Memory Disaggregation</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Syrivelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Gazzetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panos</forename><surname>Koutsovasilis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Katrinis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hofstee</surname></persName>
		</author>
		<idno>MICRO- 53</idno>
	</analytic>
	<monogr>
		<title level="m">53rd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Page Fault Support for Network Controllers</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Lesokhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haggai</forename><surname>Eran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shachar</forename><surname>Raindel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sagi</forename><surname>Grimberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liran</forename><surname>Liss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muli</forename><surname>Ben-Yehuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Tsafrir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM International Conference on Architectural Support for Programming Languages and Operating Systems (AS-PLOS)</title>
		<meeting>the 22nd ACM International Conference on Architectural Support for Programming Languages and Operating Systems (AS-PLOS)</meeting>
		<imprint>
			<biblScope unit="volume">145</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">SR-IOV Networking in Xen: Architecture, Design and Implementation</title>
		<author>
			<persName><forename type="first">Yaozu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Rose</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>tualization</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Welcome to Zombieland: Practical and Energy-efficient Memory Disaggregation in a Datacenter</title>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Nitu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Teabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Tchana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canturk</forename><surname>Isci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hagimont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EuroSys Conference</title>
		<meeting>the 2018 EuroSys Conference</meeting>
		<imprint/>
	</monogr>
	<note>EuroSys) [140</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Project PBerry: FPGA Acceleration for Remote Memory</title>
		<author>
			<persName><forename type="first">Irina</forename><surname>Calciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Puddu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aasheesh</forename><surname>Kolli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Nowatzyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratap</forename><surname>Subrahmanyam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Workshop on Hot Topics in Operating Systems (HotOS XVII)</title>
		<meeting>the 17th Workshop on Hot Topics in Operating Systems (HotOS XVII)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rack-scale Disaggregated Cloud Data Centers: The dReDBox Project Vision</title>
		<author>
			<persName><forename type="first">K</forename><surname>Katrinis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Syrivelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pnevmatikatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zervas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Theodoropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Koutsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hasharoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Raho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Espina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lopez-Buedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nemirovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Klos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berends</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Design Automation and Test in Europe</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Disaggregated Memory for Expansion and Sharing in Blade Servers</title>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jichuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><forename type="middle">N</forename><surname>Mudge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">K</forename><surname>Parthasarathy Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 36th Annual International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">AIFM: High-Performance, Application-Integrated Far Memory</title>
		<author>
			<persName><forename type="first">Zhenyuan</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malte</forename><surname>Schwarzkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcos</forename><forename type="middle">K</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Belay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting>the 14th USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<biblScope unit="volume">139</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semeru: A Memory-Disaggregated Managed Runtime</title>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyuan</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Bond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Netravali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miryung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoqing</forename><forename type="middle">Harry</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting>the 14th USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<biblScope unit="volume">139</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rethinking Software Runtimes for Disaggregated Memory</title>
		<author>
			<persName><forename type="first">Irina</forename><surname>Calciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Talha Imran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Puddu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanidhya</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Maruf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aasheesh</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><surname>Kolli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<biblScope unit="volume">138</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">FaRM: Fast Remote Memory</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Dragojevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dushyanth</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orion</forename><surname>Hodson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th USENIX Symposium on Networked Systems Design and Implementation (NSDI)</title>
		<meeting>the 11th USENIX Symposium on Networked Systems Design and Implementation (NSDI)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Remote Regions: a Simple Abstraction for Remote Memory</title>
		<author>
			<persName><forename type="first">Marcos</forename><forename type="middle">K</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Calciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Deguillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanko</forename><surname>Novakovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratap</forename><surname>Subrahmanyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lalith</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiran</forename><surname>Tati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajesh</forename><surname>Venkatasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 USENIX Annual Technical Conference (ATC)</title>
		<meeting>the 2018 USENIX Annual Technical Conference (ATC)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Disaggregating Persistent Memory and Controlling Them Remotely: An Exploration of Passive Disaggregated Key-Value Stores</title>
		<author>
			<persName><forename type="first">Shin-Yeh</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiying</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 USENIX Annual Technical Conference (ATC)</title>
		<meeting>the 2020 USENIX Annual Technical Conference (ATC)</meeting>
		<imprint>
			<biblScope unit="volume">144</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Dario</forename><surname>Korolija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Koutsoukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimberly</forename><surname>Keeton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Taranov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejan</forename><surname>Milojicic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><surname>Farview</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07102</idno>
		<title level="m">Disaggregated Memory with Operator Offloading for Database Engines</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">System-level Implications of Disaggregated Memory</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshio</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Renato</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvin</forename><surname>Auyoung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jichuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parthasarathy</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Symposium on High Performance Computer Architecture (HPCA-18)</title>
		<meeting>the 18th International Symposium on High Performance Computer Architecture (HPCA-18)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient Memory Disaggregation with Infiniswap</title>
		<author>
			<persName><forename type="first">Juncheng</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngmoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mosharaf</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><forename type="middle">G</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI)</title>
		<meeting>the 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI)</meeting>
		<imprint>
			<biblScope unit="volume">141</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Le-goOS: A Disseminated, Distributed OS for Hardware Resource Disaggregation</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiying</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>the 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Software-Defined Far Memory in Warehouse-Scale Computers</title>
		<author>
			<persName><forename type="first">Andres</forename><surname>Lagar-Cavilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junwhan</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suleiman</forename><surname>Souhlal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neha</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radoslaw</forename><surname>Burny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakeel</forename><surname>Butt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jichuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Chaugule</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junaid</forename><surname>Shahid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Thelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Yurtsever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parthasarathy</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>the 24th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<biblScope unit="volume">142</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Disaggregated Cloud Memory with Elastic Block Management</title>
		<author>
			<persName><forename type="first">Kwangwon</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seunghyub</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehyuk</forename><surname>Huh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers (TC)</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019-01">January 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Can Far Memory Improve Job Throughput?</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Amaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Branner-Augmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcos</forename><forename type="middle">K</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurojit</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvia</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Shenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 EuroSys Conference (EuroSys)</title>
		<meeting>the 2020 EuroSys Conference (EuroSys)</meeting>
		<imprint>
			<biblScope unit="volume">143</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Effectively Prefetching Remote Memory with Leap</title>
		<author>
			<persName><forename type="first">Al</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mosharaf</forename><surname>Maruf</surname></persName>
		</author>
		<author>
			<persName><surname>Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 USENIX Annual Technical Conference (ATC)</title>
		<meeting>the 2020 USENIX Annual Technical Conference (ATC)</meeting>
		<imprint>
			<biblScope unit="volume">144</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">FluidMem: Full, Flexible, and Fast Memory Disaggregation for the Cloud</title>
		<author>
			<persName><forename type="first">Blake</forename><surname>Caldwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepideh</forename><surname>Goodarzy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangtae</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Rozner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngbin</forename><surname>Im</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Distributed Computing Systems (ICDCS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hierarchical Orchestration of Disaggregated Memory</title>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers (TC)</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2020-06">June 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Mitigating the Performance-Efficiency Tradeoff in Resilient Memory Disaggregation</title>
		<author>
			<persName><forename type="first">Youngmoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Al Maruf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mosharaf</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><forename type="middle">G</forename><surname>Shin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09727</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Compute Express</forename><surname>Link</surname></persName>
		</author>
		<ptr target="https://www.computeexpresslink.org" />
		<title level="m">The Breakthrough CPU-to-Device Interconnect</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<ptr target="https://www.tomshardware.com/news/intel-sapphire-rapids-xeon-scalable-specifications-and-features" />
		<title level="m">Sapphire Rapids Uncovered: 56 Cores, 64GB HBM2E, Multi-Chip Design</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<ptr target="https://www.computeexpresslink.org/post/cxl-consortium-member-spotlight-arm" />
		<title level="m">CXL Consortium Member Spotlight: Arm</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Su</surname></persName>
		</author>
		<ptr target="https://www.amd.com/en/press-releases/2021-11-08-amd-unveils-workload-tailored-innovations-and-products-the-accelerated" />
		<title level="m">AMD Unveils Workload-Tailored Innovations and Products at The Accelerated Data Center Premiere</title>
		<imprint>
			<date type="published" when="2021-11">November 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<ptr target="https://news.samsung.com/global/samsung-unveils-industry-first-memory-module-incorporating-new-cxl-interconnect-standard" />
		<title level="m">Samsung Unveils Industry-First Memory Module Incorporating New CXL Interconnect Standard</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cxl</forename><surname>Absorbs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gen-Z</forename></persName>
		</author>
		<ptr target="https://www.nextplatform.com/2021/11/23/finally-a-coherent-interconnect-strategy-cxl-absorbs-gen-z/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Kennedy</surname></persName>
		</author>
		<ptr target="https://www.servethehome.com/cxl-may-have-just-won-as-amd-joins-cxl/" />
		<title level="m">CXL May Have Just Won as AMD Joins CXL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Autonuma: the other approach to numa scheduling</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Corbet</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/488709/" />
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">vnuma-mgr: Managing vm memory on numa platforms</title>
		<author>
			<persName><forename type="first">Subramanya</forename><surname>Dulloor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><surname>Schwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on High Performance Computing</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Optimizing Virtual Machine Consolidation Performance on NUMA Server Architecture for Cloud Workloads</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41th Annual International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 41th Annual International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Scalable Tiered Memory Management for Big Data Applications and Real NVM</title>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Raybuck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Stamler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mattan</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><surname>Hemem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 28th ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<biblScope unit="volume">136</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Nimble Page Management for Tiered Memory Systems</title>
		<author>
			<persName><forename type="first">Zi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Nellans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>the 24th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<biblScope unit="volume">142</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<ptr target="https://www.kernel.org/doc/html/latest/vm/zswap.html" />
		<title level="m">Linux Memory Management Documentation -zswap</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Thermostat: Applicationtransparent Page Management for Two-tiered Main Memory</title>
		<author>
			<persName><forename type="first">Neha</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 22nd ACM International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<publisher>ASP-LOS</publisher>
			<biblScope unit="volume">145</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Page Placement in Hybrid Memory Systems</title>
		<author>
			<persName><forename type="first">Luiz</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Gorbatov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Bianchini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Supercomputing (ICS)</title>
		<meeting>the 25th International Conference on Supercomputing (ICS)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Lameter</surname></persName>
		</author>
		<ptr target="https://events19.linuxfoundation.org/wp-content/uploads/2017/11/" />
		<title level="m">The-Flavors-of-Memory-Su pported-by-Linux-their-Use-and-Benefit-Christoph-Lameter-Jump-Trading-LLC.pdf</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Flavors of Memory supported by Linux, Their Use and Benefit</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Resource Central: Understanding and Predicting Workloads for Improved Resource Management in Large Cloud Platforms</title>
		<author>
			<persName><forename type="first">Eli</forename><surname>Cortez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><surname>Bonde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Muzio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Russinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Fontoura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Bianchini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 26th ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<biblScope unit="volume">134</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Protean: VM Allocation Service at Scale</title>
		<author>
			<persName><forename type="first">Ori</forename><surname>Hadary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishai</forename><surname>Menache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhisek</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Esaias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Greeff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Star</forename><surname>Dion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shailesh</forename><surname>Dorminey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Russinovich</surname></persName>
		</author>
		<author>
			<persName><surname>Moscibroda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting>the 14th USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<biblScope unit="volume">139</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Large-scale cluster management at Google with Borg</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Pedrosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madhukar</forename><surname>Korupolu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Oppenheimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Wilkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 EuroSys Conference (EuroSys)</title>
		<meeting>the 2015 EuroSys Conference (EuroSys)</meeting>
		<imprint>
			<biblScope unit="volume">135</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Heuristics for Vector Bin Packing</title>
		<author>
			<persName><forename type="first">Rina</forename><surname>Panigrahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lincoln</forename><surname>Uyeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Udi</forename><surname>Wieder</surname></persName>
		</author>
		<ptr target="https://www.microsoft.com/en-us/research/publication/heuristics-for-vector-bin-packing/" />
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multi-Resource Packing for Cluster Schedulers</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Grandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srikanth</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriram</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Akella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Special Interest Group on Data Communication (SIGCOMM)</title>
		<meeting>the ACM Special Interest Group on Data Communication (SIGCOMM)</meeting>
		<imprint>
			<biblScope unit="volume">146</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Tight Bounds for Online Vector Bin Packing</title>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Reuven</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seny</forename><surname>Kamara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Shepherd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th ACM symposium on Theory of Computing (STOC)</title>
		<meeting>the 45th ACM symposium on Theory of Computing (STOC)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">SmartHarvest: Harvesting Idle CPUs Safely and Efficiently in the Cloud</title>
		<author>
			<persName><forename type="first">Yawen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kapil</forename><surname>Arya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marios</forename><surname>Kogias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manohar</forename><surname>Vanga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Bhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neeraja</forename><forename type="middle">J</forename><surname>Yadwadkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameh</forename><surname>Elnikety</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Bianchini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 EuroSys Conference (EuroSys)</title>
		<meeting>the 2021 EuroSys Conference (EuroSys)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Sameh Elnikety, Marcus Fontoura, and Ricardo Bianchini. Providing SLOs for Resource-Harvesting VMs in Cloud Platforms</title>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Ambati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">??igo</forename><surname>Goiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><surname>Vieira Frujeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alper</forename><surname>Gun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Corell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sekhar</forename><surname>Pasupuleti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Moscibroda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting>the 14th USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<biblScope unit="volume">139</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Coordinated and Efficient Huge Page Management with Ingens</title>
		<author>
			<persName><forename type="first">Youngjin</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangchen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Rossbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmett</forename><surname>Witchel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting>the 12th USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<biblScope unit="volume">137</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Making Huge Pages Actually Useful</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Panwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravinda</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><surname>Gopinath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>the 23rd ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Benchmarking Cloud Serving Systems with YCSB</title>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erwin</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghu</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Sears</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM Symposium on Cloud Computing (SoCC)</title>
		<meeting>the 1st ACM Symposium on Cloud Computing (SoCC)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<ptr target="https://github.com/Intel-bigdata/HiBench" />
		<title level="m">HiBench: The Bigdata Micro Benchmark Suite</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">Scott</forename><surname>Beamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krste</forename><surname>Asanovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.03619</idno>
		<title level="m">The gap benchmark suite</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title/>
		<author>
			<persName><surname>Spec Cpu</surname></persName>
		</author>
		<ptr target="https://www.spec.org/cpu" />
		<imprint>
			<date type="published" when="2017">2017. 2017, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The PARSEC Benchmark Suite: Characterization and Architectural Implications</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bienia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaswinder Pal</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">PAR-SEC3.0: A Multicore Benchmark Suite with Network Stacks and SPLASH-2X</title>
		<author>
			<persName><forename type="first">Xusheng</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yungang</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bienia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News (CAN)</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Firecracker: Lightweight Virtualization for Serverless Applications</title>
		<author>
			<persName><forename type="first">Alexandru</forename><surname>Agache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brooker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreea</forename><surname>Florescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Iordache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Liguori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rolf</forename><surname>Neugebauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Piwonka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana-Maria</forename><surname>Popa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th USENIX Symposium on Networked Systems Design and Implementation (NSDI)</title>
		<meeting>the 17th USENIX Symposium on Networked Systems Design and Implementation (NSDI)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">LeapIO: Efficient and Portable Virtual NVMe Storage on ARM SoCs</title>
		<author>
			<persName><forename type="first">Huaicheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanko</forename><surname>Novakovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaibhav</forename><surname>Gogte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriram</forename><surname>Govindan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><forename type="middle">R K</forename><surname>Ports</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Bianchini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Haryadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Gunawi</surname></persName>
		</author>
		<author>
			<persName><surname>Badam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>the 25th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<biblScope unit="volume">133</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<ptr target="httpp://www.pcisig.com/specifications/iov/singleroot" />
		<title level="m">Single-Root Input/Output Virtualization</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">coIOMMU: A Virtual IOMMU with Cooperative DMA Buffer Tracking for Efficient Memory Management in Direct I/O</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Kun Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaozu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 USENIX Annual Technical Conference (ATC)</title>
		<meeting>the 2020 USENIX Annual Technical Conference (ATC)</meeting>
		<imprint>
			<biblScope unit="volume">144</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">On the DMA Mapping Problem in Direct Device Assignment</title>
		<author>
			<persName><forename type="first">Ben-Ami</forename><surname>Yassour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muli</forename><surname>Ben-Yehuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orit</forename><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd ACM International Conference on Systems and Storage (SYSTOR)</title>
		<meeting>the 3rd ACM International Conference on Systems and Storage (SYSTOR)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Protection Strategies for Direct Access to Virtualized I/O Devices</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Willmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Rixner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USENIX Annual Technical Conference (ATC)</title>
		<meeting>the USENIX Annual Technical Conference (ATC)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muli</forename><surname>Ben-Yehuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibm</forename><surname>Research</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Tsafrir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Assaf</forename><surname>Schuster</surname></persName>
		</author>
		<title level="m">Proceedings of the 2011 USENIX Annual Technical Conference (ATC)</title>
		<meeting>the 2011 USENIX Annual Technical Conference (ATC)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Efficient IOMMU Emulation</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">The Turtles Project: Design and Implementation of Nested Virtualization</title>
		<author>
			<persName><forename type="first">Muli</forename><surname>Ben-Yehuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zvi</forename><surname>Dubitzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Factor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Har'el</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Liguori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orit</forename><surname>Wasserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben-Ami</forename><surname>Yassour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>the 9th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Vm live migration at scale</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Ruprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Shiraev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Harmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maya</forename><surname>Spivak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miche</forename><surname>Baker-Harvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018-03">March 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<ptr target="https://www.computeexpresslink.org/download-the-specification" />
		<title level="m">CXL Specification</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<author>
			<persName><forename type="first">Availability</forename><surname>Reliability</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serviceability</forename></persName>
		</author>
		<ptr target="https://www.intel.com/content/dam/develop/external/us/en/documents/emca2-integration-validation-guide-556978.pdf" />
		<title level="m">RAS) Integration and Validation Guide for the Intel Xeon Processor E7 Family</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<ptr target="https://www.amd.com/system/files/2017-06/AMD-EPYC-Brings-New-RAS-Capability.pdf" />
		<title level="m">AMD EPYC brings new RAS capability</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<ptr target="https://www.microchip.com/en-us/about/blog/learning-center/cxl--use-cases-driving-the-need-for-low-latency-performance-reti" />
		<title level="m">CXL Use-cases Driving the Need For Low Latency Performance Retimers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<ptr target="https://www.asteralabs.com/videos/aries-smart-retimer-for-pcie-gen-5-and-cxl/" />
		<title level="m">Enabling PCIe 5.0 System Level Testing and Low Latency Mode for CXL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">New tau vms deliver leading price-performance for scaleout workloads</title>
		<author>
			<persName><surname>Google</surname></persName>
		</author>
		<ptr target="https://cloud.google.com/blog/products/compute/google-cloud-introduces-tau-vms" />
		<imprint>
			<date type="published" when="2021-06">June 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Amazon ec2 m6g instances</title>
		<author>
			<persName><surname>Aws</surname></persName>
		</author>
		<ptr target="https://aws.amazon.com" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<author>
			<persName><forename type="first">Tony</forename><surname>Harvey</surname></persName>
		</author>
		<ptr target="https://www.gartner.com/en/documents/3894873/use-single-socket-servers-to-reduce-costs-in-the-data-ce" />
		<title level="m">Use Single-Socket Servers to Reduce Costs in the Data Center</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">W</forename><surname>Hormuth</surname></persName>
		</author>
		<ptr target="https://www.nextplatform.com/2019/04/24/why-single-socket-servers-could-rule-the-future/" />
		<title level="m">Why Single-socket Servers Could Rule the Future</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<ptr target="https://www.mipi.org/specifications/i3c-sensor-specification" />
		<title level="m">MIPI I3C Bus Sensor Specification</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Advanced Configuration and Power Interface Specification</title>
		<author>
			<persName><surname>Uefi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">AutoNUMA: Optimize Memory Placement for Memory Tiering System</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/835402/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">A Top-Down Method for Performance Analysis and Counters Architecture</title>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Yasin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<ptr target="https://software.intel.com/content/www/us/en/develop/documentation/vtune-cookbook/top/methodologies/top-down-microarchitecture-analysis-method.html" />
		<title level="m">Top-down Microarchitecture Analysis Method</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Top-Down Characterization Approximation Based on Performance Counters Architecture for AMD Processors</title>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Jarus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Oleksiak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Simulation Modelling Practice and Theory</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Machine Learning in Virtualization: Estimate a Virtual Machine&apos;s Working Set Size</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Melekhova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Sixth International Conference on Cloud Computing (CLOUD)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">More Accurate Estimation of Working Set Size in Virtual Machines</title>
		<author>
			<persName><surname>Ahmed A Harby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><forename type="middle">F</forename><surname>Sherif F Fahmy</surname></persName>
		</author>
		<author>
			<persName><surname>Amin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine Learning in Python</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ga?l</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Lightgbm: A Highly Efficient Gradient Boosting Decision Tree</title>
		<author>
			<persName><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<author>
			<persName><surname>Onnx</surname></persName>
		</author>
		<author>
			<persName><surname>Open</surname></persName>
		</author>
		<ptr target="https://onnx.ai/" />
		<title level="m">Neural Network Exchange: the Open Standard for Machine Learning Interoperability</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Tensorflow-serving: Flexible, High-performance ML Serving</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Olston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiril</forename><surname>Gorovoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremiah</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinu</forename><surname>Rajashekhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Sukriti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Soyke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the 31st Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Clipper: A Low-Latency Online Prediction Serving System</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Crankshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guilio</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI)</title>
		<meeting>the 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI)</meeting>
		<imprint>
			<biblScope unit="volume">141</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Resource Central: Understanding and Predicting Workloads for Improved Resource Management in Large Cloud Platforms</title>
		<author>
			<persName><forename type="first">Eli</forename><surname>Cortez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><surname>Bonde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Muzio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Russinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Fontoura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Bianchini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 26th ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<biblScope unit="volume">134</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Oppenheimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brewer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">Wilkes</forename><surname>Borg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omega</forename><surname>Kubernetes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Paragon: Qos-aware scheduling for heterogeneous datacenters</title>
		<author>
			<persName><forename type="first">Christina</forename><surname>Delimitrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Tarcil: Reconciling Scheduling Speed and Quality in Large Shared Clusters</title>
		<author>
			<persName><forename type="first">Christina</forename><surname>Delimitrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th ACM Symposium on Cloud Computing (SoCC)</title>
		<meeting>the 6th ACM Symposium on Cloud Computing (SoCC)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Omega: flexible, scalable schedulers for large compute clusters</title>
		<author>
			<persName><forename type="first">Malte</forename><surname>Schwarzkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Konwinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Abdel-Malek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Wilkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 EuroSys Conference (EuroSys)</title>
		<meeting>the 2013 EuroSys Conference (EuroSys)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Large-scale Cluster Management at Google with Borg</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Pedrosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madhukar</forename><surname>Korupolu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Oppenheimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Wilkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 EuroSys Conference (EuroSys)</title>
		<meeting>the 2015 EuroSys Conference (EuroSys)</meeting>
		<imprint>
			<biblScope unit="volume">135</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title/>
		<author>
			<persName><surname>Redis</surname></persName>
		</author>
		<ptr target="https://redis.io" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Tpc-H</forename><surname>Benchmark</surname></persName>
		</author>
		<ptr target="http://www.tpc.org/tpch" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">The Relationship Between Recall and Precision</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Buckland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fredric</forename><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">A Hardware-Software Co-Designed Disaggregated Memory System</title>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuhao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiying</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)<address><addrLine>Clio</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Optically Connected Memory for Disaggregated Data Centers</title>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Gazman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Hattink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mauricio</forename><forename type="middle">G</forename><surname>Palma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meisam</forename><surname>Bahadori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruth</forename><surname>Rubio-Noriega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lois</forename><surname>Orosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madeleine</forename><surname>Glick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keren</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodolfo</forename><surname>Azevedo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 32nd International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">OpenCAPI Consortium</orgName>
		</author>
		<ptr target="https://opencapi.org/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">MIND: In-Network Memory Management for Disaggregated Data Centers</title>
		<author>
			<persName><forename type="first">Yanpeng</forename><surname>Seung Seob Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 28th ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<biblScope unit="volume">136</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Using RDMA Efficiently for Key-Value Services</title>
		<author>
			<persName><forename type="first">Anuj</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Special Interest Group on Data Communication (SIGCOMM)</title>
		<meeting>the ACM Special Interest Group on Data Communication (SIGCOMM)</meeting>
		<imprint>
			<biblScope unit="volume">146</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Scale-Out ccNUMA: Exploiting Skew with Strongly Consistent Caching</title>
		<author>
			<persName><forename type="first">Vasilis</forename><surname>Gavrielatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonios</forename><surname>Katsarakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arpit</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolai</forename><surname>Oswald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Nagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EuroSys Conference</title>
		<meeting>the 2018 EuroSys Conference</meeting>
		<imprint/>
	</monogr>
	<note>EuroSys) [140</note>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">The End of a Myth: Distributed Transactions Can Scale</title>
		<author>
			<persName><forename type="first">Erfan</forename><surname>Zamanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carsten</forename><surname>Binnig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Kraska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International Conference on Very Large Data Bases (VLDB)</title>
		<meeting>the 43rd International Conference on Very Large Data Bases (VLDB)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">HeteroOS: OS Design for Heterogeneous Memory Management in Datacenters</title>
		<author>
			<persName><forename type="first">Sudarsun</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ada</forename><surname>Gavrilovska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishal</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Schwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 44th Annual International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Exploiting the Byte-Accessibility of SSDs within a Unified Memory-Storage Hierarchy</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Abulila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Sharma Mailthody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nam</forename><forename type="middle">Sung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwu</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>the 24th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)<address><addrLine>Flat-Flash</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">142</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Exploring the Design Space of Page Management for Multi-Tiered Memory Sys-tems</title>
		<author>
			<persName><forename type="first">Jonghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonkyo</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeongseob</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 USENIX Annual Technical Conference (ATC)</title>
		<meeting>the 2021 USENIX Annual Technical Conference (ATC)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Learning on Distributed Traces for Data Center Storage Systems</title>
		<author>
			<persName><forename type="first">Giulio</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Maas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Conference on Machine Learning and Systems (MLSys)</title>
		<meeting>the 4th Conference on Machine Learning and Systems (MLSys)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Learning-based Memory Allocation for C++ Server Workloads</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Mahdi</forename><surname>Javanmard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>the 25th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<biblScope unit="volume">133</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Sinan: ML-Based and QoS-Aware Resource Management for Cloud Microservices</title>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuangzhuang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Edward</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Delimitrou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<biblScope unit="volume">138</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">A Hierarchical Neural Model of Data Prefetching</title>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akanksha</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milad</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parthasarathy</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calvin</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<biblScope unit="volume">138</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Applying Deep Learning to the Cache Replacement Problem</title>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Shi</surname></persName>
			<affiliation>
				<orgName type="collaboration">MICRO-52</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Xiangru</forename><surname>Huang</surname></persName>
			<affiliation>
				<orgName type="collaboration">MICRO-52</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Akanksha</forename><surname>Jain</surname></persName>
			<affiliation>
				<orgName type="collaboration">MICRO-52</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Calvin</forename><surname>Lin</surname></persName>
			<affiliation>
				<orgName type="collaboration">MICRO-52</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">52nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">The SGI Origin: A ccNUMA Highly Scalable Server</title>
		<author>
			<persName><forename type="first">James</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lenoski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 24th Annual International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Thread and Memory Placement on NUMA Systems: Asymmetry Matters</title>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Lepers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivien</forename><surname>Qu?ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Fedorova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 USENIX Annual Technical Conference (ATC)</title>
		<meeting>the 2015 USENIX Annual Technical Conference (ATC)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Implementation and Performance of Munin</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">B</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">K</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willy</forename><surname>Zwaenepoel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 13th ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Memory Coherence in Shared Virtual Memory Systems</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hudak</forename><surname>Paul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems (TOCS)</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1989-11">November 1989</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
