<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Monitored Adaptive Cache Warm-Up for Microprocessor Simulation</title>
				<funder>
					<orgName type="full">AMD</orgName>
				</funder>
				<funder ref="#_9xQbH3v">
					<orgName type="full">IBM</orgName>
				</funder>
				<funder>
					<orgName type="full">Intel</orgName>
				</funder>
				<funder ref="#_8rxVqzP">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yue</forename><surname>Luo</surname></persName>
							<email>luo@ece.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lizy</forename><forename type="middle">K</forename><surname>John</surname></persName>
							<email>ljohn@ece.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lieven</forename><surname>Eeckhout</surname></persName>
							<email>leeckhou@elis.ugent.be</email>
							<affiliation key="aff1">
								<orgName type="institution">Ghent University</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Monitored Adaptive Cache Warm-Up for Microprocessor Simulation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Simulation is the most important tool for computer architects to evaluate the performance of new computer designs. However, detailed simulation is extremely time consuming. Sampling is one of the techniques that effectively reduce simulation time. In order to achieve accurate sampling results, microarchitectural structure must be adequately warmed up before each measurement.</p><p>In this paper, a new technique for warming up microprocessor caches is proposed. The simulator monitors the warm-up process of the caches and decides when the caches are warmed up based on simple heuristics. In our experiments the Self-Monitored Adaptive (SMA) warm-up technique on average exhibits only 0.2% warm-up error in CPI. SMA achieves smaller warm-up error with only 1/2~1/3 of the warm-up length of previous methods. In addition, it is adaptive to the cache configuration simulated. For simulating small caches, the SMA technique can reduce the warm-up overhead by an order of magnitude compared to previous techniques. Finally, SMA gives the user some indicator of warm-up error at the end of the cycle-accurate simulation that helps the user to gauge the accuracy of the warm-up.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Simulation of standard benchmarks has been the most popular method for computer architects to study design tradeoffs. Modern benchmarks are no longer small kernels or synthesized toy programs. Instead, they are very close to real world programs and often take a long time to execute. Moreover, modern superscalar microprocessors are becoming increasingly complex; and so are the simulators modeling the processors. As a result, running benchmarks on detailed microarchitecture simulation models can take prohibitively large simulation times.</p><p>To reduce simulation time, several techniques have been proposed, among which sampling and reduced input sets are the most commonly used. In this paper, we focus on sampling. In a sampled simulation the original full instruction stream is divided into nonoverlapping chunks of continuous instructions. Each chunk is a basic simulation unit, or a sampling unit. The sampling unit size is the number of instructions in each chunk. The pre-sample of a sampling unit refers to the instructions before this sampling unit up to the end of the previous sampling unit. A sample consists of the selected chunks that are actually simulated and measured. The number of sampling units in a sample is the sample size. Recently, Wunderlich et al. applied sampling theory to microarchitecture simulation <ref type="bibr" target="#b1">[2]</ref>. Under the assumption of no measuring error, they showed that CPI can be estimated to within an error of 3% with 99.7% confidence by measuring fewer than 50 million instructions per benchmark. This accounts for only 0.029% of the average dynamic instructions executed for a benchmark program. It appears that sampling has effectively solved the problem of long simulation time.</p><p>However, the above results are obtained under the assumption that the number of cycles or CPI for each sampling unit can be accurately measured. The CPI of each sampling unit depends not only on the instructions executed in the unit, but also on the initial state of all microarchitecture structures at the beginning of this unit. The initial state is, in turn, the result of the execution of all the instructions before the sampling unit. Executing a limited number of instructions before a sampling unit to get (approximately) correct initial state is known as warming up the microarchitecture. The number of instructions used for warm-up before a sampling unit is its warm-up length. For small structures like the ROB, the reservation station, and the register file, thousands of instructions are enough to put them into correct state. However, some structures in the microprocessor like the branch target buffer, and the caches can hold thousands to millions of bytes. It is difficult to ensure that they are in the correct initial state before every sampling unit in the simulation. If the initial state is not correct, the error can be large. For example, Haskins et al reported that in their experiment, ignoring warm-up could result in an error as high as 15% in simulated CPI <ref type="bibr" target="#b11">[12]</ref>. Thus adequate warm-up is critical to the accuracy of sampled simulation. Warm-up does not only affect accuracy but also incurs overhead and increases simulation time. When simulating a processor with large caches, a large number of instructions may be needed for adequate warm-up, which prolongs the simulation. Therefore, warm-up issue is very important in sampled simulation. A good warm-up scheme should achieve a desired level of accuracy while devoting as few instructions as possible for warm-up.</p><p>We believe that warm-up is still an important issue in sampled microprocessor simulation and deserves active research. But there is an opinion that warm-up is largely solved and little reduction in simulation time can be accomplished with better warm-up techniques. For example, MRRL is claimed to have achieved 90% of the maximum possible simulation speed <ref type="bibr" target="#b11">[12]</ref>. However, careful analysis of the experiment reveals functional simulation as the bottleneck because every benchmark is simulated functionally from beginning to end. In our simulation environment, the relative speed of functional simulation (no microarchitecture simulation at all), functional warm-up (only cache and branch predictor simulation), and cycle-accurate simulation, is 1:1/2.8:1/16 <ref type="foot" target="#foot_0">1</ref> . Fifty 1 million-instruction sampling units are used in <ref type="bibr" target="#b11">[12]</ref>. Suppose that a benchmark is 100 billion instructions long and on average each sampling unit needs 30 million instructions for warm-up<ref type="foot" target="#foot_1">2</ref> . Then the percentages of time spent in functional simulation, functional warmup and cycle simulation are 95.17%, 4.06%, and 0.77%. It is obvious that the functional simulation is the bottleneck, so even getting rid of warm-up overhead altogether will provide little benefit. However, if the user saves the checkpoints or the traces for each sampling unit, she no longer needs to run the benchmark from beginning to end and is able to simulate for each sampling unit directly. In this case, removing the warm-up overhead will give 6.25 times speedup in simulation! Therefore, better warm-up technique is still highly desired. (Reducing the storage cost of checkpoints/traces is among our future research.)</p><p>In this paper, we study the warm-up process of the processor caches and propose a self-monitored adaptive warm-up scheme for simulation. The simulator monitors the warm-up process of the caches and decides when the caches are warmed up based on a simple heuristic. Unlike previous research, this method is both adaptive to the characteristics of the benchmark and the cache configuration being simulated. Overall, it achieves very good accuracy with lower warm-up overhead than previously proposed techniques.</p><p>In the next section, we survey the existing warm-up techniques and discuss the strength and weakness of each technique. To overcome these weaknesses, we present our new self-monitored adaptive cache warmup scheme in Section 3. Our proposed technique is evaluated experimentally in Section 4. Finally, we conclude and discuss future research in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Obviously the most accurate way to warm up the caches is to do cache simulation throughout the benchmark execution. This is how the SMARTS scheme <ref type="bibr" target="#b1">[2]</ref> does the warm-up. The simulator switches between functional warm-up and cycle-accurate simulation. During functional warm-up, the simulator executes the program without simulating the pipeline stages, but the caches and the branch predictors are simulated. During cycle simulation, the simulator does detailed cycle accurate simulation. Therefore, the only error in warm-up is introduced by not simulating the effect of out-of-order execution and wrong path execution on the caches during functional warm-up. It has been shown that this error is small <ref type="bibr" target="#b1">[2]</ref>  <ref type="bibr" target="#b2">[3]</ref>. Although this warm-up scheme is by far the most accurate, it is still not satisfactory. First, always simulating caches can be a waste of resource. According to sampling theory, for a specific accuracy, the sample size should be determined by the variability in the population. If the benchmark does a lot of repetition, only a tiny fraction of the instruction stream is needed. However, the scheme requires that caches be simulated for every instruction, which is inefficient. Secondly, always warming up the cache makes distributed simulation hard. For sampling methods such as SimPoint <ref type="bibr" target="#b13">[14]</ref> and Variance SimPoint <ref type="bibr" target="#b14">[15]</ref>, where a small number of relatively large sampling units are taken, each sampling unit can be simulated on different machines in parallel to greatly improve the overall simulation speed.</p><p>However, constantly warming up caches as in SMARTS make it difficult to distribute the simulation on multiple machines.</p><p>Another simple warm-up scheme is to devote a fixed number of instructions to warm up the cache. This is called "PRIME" scheme <ref type="bibr" target="#b5">[6]</ref> following Crowley et. al.'s terminology <ref type="bibr" target="#b3">[4]</ref>.</p><p>At an extreme, zero instructions are used to warm up the cache before cycle-simulating a sampling unit. Then two types of assumption can be made about the initial state of the cache. If all cache lines are assumed to be invalid, it is called "COLD" scheme, which, obviously, overestimates the number of cache misses. If, on the other hand, the cache state is assumed to be the same as the state at the end of last sampling unit, the scheme is called "STITCH" <ref type="bibr" target="#b4">[5]</ref>. The efficacy of these assumptions depends on workload, cache organization, and choice of sampling parameters. The user is often left with a result whose error the user has no idea unless he/she does the full simulation. An additional problem with prime is the lack of guideline for the user to choose the number of instructions to warm up the cache.</p><p>The problem of cache warm-up is that the state of the cache is unknown at the beginning of each observation. In other words, since portions of the trace are unexamined between observations, it is unknown whether the first reference to each cache block will be a hit or a miss. Such references are referred to as coldstart references. Laha et al <ref type="bibr" target="#b6">[7]</ref> proposed not counting these cold-start references when calculating cache misses. This effectively assumes that the miss rate for the cold-start references is equivalent to the miss rate for all other references. Wood et al <ref type="bibr" target="#b7">[8]</ref> show that this assumption is usually not true. The miss rate for the cold-start references is higher than the overall miss rate. Employing a renewal theoretical model, Wood et al propose a method to estimate the miss rate for the cold-start references by observing the average live and dead time for each cache line. These two methods can be used to calculate the cache miss rate from sampled trace, but not directly applicable to microarchitectural simulation to get CPI.</p><p>Haskins and Skadron have proposed two techniques to determine the warm-up length for a sampling unit. The Minimal Subset Evaluation (MSE) <ref type="bibr" target="#b8">[9]</ref> technique uses formulas derived from combinatorics and probability theory to calculate, for some user-chosen probability p, the number of memory references prior to each sampling unit that must be modeled in order to achieve accurate cache state. This work only handles warm-up for the first-level data and instruction caches. In their second technique, they measure the Memory Reference Reuse Latency (MRRL) <ref type="bibr" target="#b11">[12]</ref>, which refers to the elapsed time measured in number of instructions between a reference to some memory address and the next reference to the same address. Instructions in a sampling unit and its pre-sample are profiled to get the distribution of MRRL. Given a p-value (p%) the warm-up length is the p-percentile of the distribution. Because most of the instructions used to calculate the distribution of MRRL are from the pre-sample, it is hard to guarantee that the instructions in the sampling unit also follow the distribution.</p><p>To avoid this problem, Eeckhout et al proposed the Boundary Line Reuse Latency (BLRL<ref type="foot" target="#foot_2">3</ref> ) method <ref type="bibr" target="#b9">[10]</ref>, in which every memory reference in a sampling unit is directly examined instead of relying on aggregated distribution. For a reference r to address a generated by instruction I in the sampling unit, they search for a reference r' to the same address a in the pre-sample. If reference r' is found and it is generated by instruction I', then warming up from I' can guarantee that we know whether r is a hit or miss when LRU replacement policy is used. Given a p-value like 90%, the instructions in the pre-sample of the sampling unit are scanned. The warm-up length for the sampling unit is chosen such that 90% of the unique references in the sampling unit whose addresses are referenced in the pre-sample are covered by the warm-up instructions.</p><p>Neither MRRL nor BLRL takes the cache organization into consideration. The cache warm-up process depends on both the workload and the cache organization. The methods discussed so far only consider the workload. A small direct mapped cache is intuitively easier to warm up than a large highly associative one, but these methods calls for the same warm-up length given the same p-value. Therefore, the cache-independent methods may be overkill depending on the cache being simulated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Self-monitored adaptive warm-up</head><p>As discussed in the previous section, none of the existing warm-up technique is satisfactory. One major problem with MRRL and BLRL is that they do not take cache configuration into account. Different caches may require different warm-up length even for the same benchmark. Therefore, using any fixed pvalue in the techniques may result in under-warm-up or over-warm-up for different caches. When we carefully examine the previous techniques, we see that none of them are really warm-up method per se. The warm-up method itself is simulating instructions before each sampling units. All the methods just help the user to decide when the warm-up is enough, so why not monitor the warm-up process in the simulator to decide whether the warm-up is enough? This is exactly the rationale behind the self-monitored adaptive (SMA) warm-up technique.</p><p>In SMA warm-up, as in the previous techniques, the simulator does functional warm-up before switching to detailed cycle accurate simulation.</p><p>During the functional warm-up, the caches are accessed but no pipeline stages are simulated. The warm-up process of the cache is monitored. The simulator switches to cycle simulation as soon as the cache is deemed "warmed up". Therefore, the warm-up length is not fixed but adaptive. Unlike previous approaches, this technique implicitly considers both the workload characteristics and the cache organization. Fewer instructions will be used for warming up a small directmapped cache than for a large highly associative one.</p><p>To monitor the cache warm-up process, all the cache blocks are initialized to the cold-start state before the functional warm-up. The address/tag in a cold-start block is unknown because it depends on the previous instructions, which were not simulated. When a cache access is initiated, the set index to the cache can be calculated. If the memory address is not found in this set and one or more cache block in this set is in cold-start state, then we call the cache access a cold-start access. It is not known whether a cold-start access will result in a cache miss or a hit. When data is brought to a cold-start state cache block, the block changes to the "valid" state. Once a cache block leaves the cold-start state, it never goes back to this state again. We call any state other than the cold-start state a known state.</p><p>Two aspects of the warm-up process are monitored. Firstly, the simulator keeps track of the percentage of the cache blocks in cold-start state. This number monotonously decreases during warm-up. If no cache block is in cold-start state, the cache is completely warmed up. We can guarantee that the outcome of every future reference is known. Secondly, the simulator monitors the number of cold-start accesses during an interval. When the cache is large, or the working set of the program is small, it may take too long to completely warm up the cache. In this case, the cache is deemed warmed up when the number of coldstart accesses is below a user-defined threshold. Unlike a completely warmed up cache, there is no guarantee that all future reference will access blocks in known state. However, the possibility of cold-start accesses is low.</p><p>The detailed information on choice of parameters for the interval size and threshold is given in the next section. Monitoring the warm-up process is a very low overhead operation, it only increments or decrements a couple of counters at a cold-start cache access. There is no time overhead for accessing cache blocks in known state. The number of cold-start accesses usually decreases quickly.</p><p>Another problem with the previous methods is that the user generally does not know how accurate the warm-up was after the simulation. She has to rely on previously published validated results. However, the user's configuration may not be the same as in the published paper. SMA can give the user some indication of the accuracy of the warm-up after the simulation.</p><p>After switching to cycle accurate simulation, the simulator continues to count the number of cold-start accesses. In this way, after the simulation the user knows how much of all the cache misses are due to cold-start accesses.</p><p>In the experiment we count a cold-start access as a cache miss. So the number of cold-start accesses is usually the upper bound of the overestimation of cache misses. For example, if during cycle accurate simulation of 1 million instructions the user only sees 20 cold-start cache references, then she knows that the overestimation of cache misses is very unlikely to go above 20 and the CPI result should be fairly accurate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and results</head><p>Ten benchmarks from SPECint 2000, listed in Table <ref type="table" target="#tab_0">1</ref>, are used in our experiment. The programs, downloaded from the SimpleScalar web site <ref type="bibr" target="#b15">[16]</ref>, are compiled for the Alpha ISA. All the experiments are done on our modified SimpleScalar v3.0 <ref type="bibr" target="#b0">[1]</ref>. Table <ref type="table" target="#tab_1">2</ref> shows the main processor configuration used in our experiment. This configuration is adapted from the SMARTS paper <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Variability in warm-up process</head><p>Much research has been done on devising and comparing warm-up techniques, but few of the projects shed light on the warm-up process itself. We have done experiments to study how the cache warm-up process proceeds. In this section, we only present one important issue in cache warm-up, the variability in the warm-up length. The effectiveness of the new warmup technique depends on the variability. If a constant warm-up length is good for all situations, then the PRIME method with fixed warm-up length will be the best. However, if the warm-up length changes widely, then a good warm-up technique needs to adapt to all the factors that affect the warm-up process.</p><p>Each benchmark execution is divided into segments of 100 million instructions. We study the cache warmup process of each segment, so the simulator sets all cache blocks to cold-start state at the beginning of each segment. We track the warm-up process in each segment. For L1 data cache, we record, for each segment, the warm-up length needed to put every cache block in known state. Table <ref type="table" target="#tab_2">3</ref> lists the average and the standard deviation of warm-up length. The L2 cache may not completely warm up at the end of 100 million instruction segments, so we record the warmup length needed to warm up 50% of the cache blocks for each segment. The statistics for the L2 cache warm-up length is shown in Table <ref type="table" target="#tab_3">4</ref>. These warm-up lengths are not the warm-up length required in simulation.</p><p>Nevertheless they reflect the large variability in the warm-up process. As we can see, the warm-up length is different for different benchmarks. It is also widely different within one benchmark. The standard deviation of the warm-up length of different segments is as large as the mean in many cases. Therefore, devoting fixed number of instructions to warm-up as in PRIME method is not good. Comparing Table <ref type="table" target="#tab_2">3</ref> and Table <ref type="table" target="#tab_3">4</ref> we also see that the large L2 cache needs much longer warm-up than the small L1 cache. Therefore, it is important for a good warm-up method to also take into consideration of the cache configuration. MRRL and BLRL both adapt to the different segment in a benchmark but they cannot adapt to the cache configuration. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with MRRL and BLRL</head><p>In this section we compare SMA with the two most recently proposed warm-up techniques, MRRL and BLRL. We compare both the warm-up length and the accuracy in CPI. In the experiment, we choose a sampling unit size of 1 million instructions. This sampling unit size was used in MRRL paper <ref type="bibr" target="#b11">[12]</ref>, and Variance SimPoint <ref type="bibr" target="#b14">[15]</ref>.</p><p>In this section, each benchmark execution is divided into segments of 200 million instructions. We use 200 million instruction segment size instead of 100 million in the previous section to give larger gap between sampling units for more accurate profiling in MRRL and BLRL. One sampling unit is chosen from each segment.</p><p>In SMA the sampling units are not previously determined but rather depend on the cache warm-up process. Once the cache is deemed warmed up enough, the simulator executes 4,000 instructions in cycle accurate mode to warm up the pipeline as suggested in <ref type="bibr" target="#b1">[2]</ref>, and then the CPI of 1 million instruction sampling unit is measured. As discussed in the above section, the L2 cache may not be completely warmed up with reasonable number of instructions so we cannot use complete warm-up as the only criterion for the caches. Therefore, we choose the following simple heuristic to judge if the cache is warmed up. At the end of each interval, we calculate the average number of cold-start accesses for the last N intervals. If the average number of cold-start references falls below a threshold T, we assume that the cache is warmed up enough and end the functional warm-up. Because this method requires warm-up of at least N intervals, to take advantage of segments that reach complete warm-up quickly, we also monitor the number of cache blocks in the cold-start state in the cache. The functional warm-up also ends as soon as the cold-start state blocks drop to zero. For L1 data cache, we use N=20, T=10. For L1 instruction cache, we use N=10, T=1. For L2 cache, we use N=20, T=15.</p><p>For MRRL and BLRL the sampling units are chosen to be the same as those in SMA. 4,000 instructions are also simulated in the cycle-accurate mode before each sampling units to warm up the pipeline. The profiler for MRRL was downloaded from its author's website <ref type="bibr" target="#b17">[18]</ref>.</p><p>Although not implemented in the current simulator, we hope to further improve simulation speed by distributed simulation. When sampling units are distributed to different machines, the end state of one sampling unit cannot be used as the beginning state of another sampling unit. Therefore, in our experiment caches are emptied before warming up each sampling unit.</p><p>The final error in CPI in sampled simulation comes from two sources: the sampling error per se and the warm-up error. To fairly compare different warm-up techniques, only the warm-up error should be measured, so we additionally run a simulation with full cache warm-up. In this simulation the caches are always simulated between every sampling units as in SMARTS <ref type="bibr" target="#b1">[2]</ref>. The sampling units and the cycleaccurate warm-up are the same in all of our simulations, so the difference between the CPI of a warm-up technique and the CPI of full cache warm-up is the warm-up error.</p><p>Table <ref type="table" target="#tab_4">5</ref> compares SMA with MRRL and BLRL. The heuristic in SMA relies on the warm-up history to predict whether the cache is warmed up enough in the next sampling unit, so SMA may mispredict and end functional warm-up prematurely. In Table <ref type="table" target="#tab_4">5</ref> the average error is only about 0.2%, so SMA is very accurate and rarely mispredicts.</p><p>For MRRL, we choose the p-value to be 99.9%, which is the default value suggested in <ref type="bibr" target="#b11">[12]</ref>. For BLRL, we use the p-value of 90%. Both methods are also accurate, exhibiting an average error of 0.4% and 0.3%. However, SMA clearly shows the overall advantage. The SMA technique requires only 1/3 of the warm-up length of MRRL or 1/2 of the warm-up length of BLRL yet it achieves an error that is smaller than the other two techniques.</p><p>SMA is better in both warm-up length and accuracy, so changing the p-value for all benchmarks will not affect the overall conclusion. Using different p-values for different benchmarks may improve the overall result of MRRL and BLRL, but asking the user to fine tune the p-value for each benchmark and different processor configuration is not practical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Adaptivity to cache configuration</head><p>Unlike previous methods, SMA adapts to the cache configuration being simulated. In the last section, we show how SMA performs with a cache size that is common to workstations. To evaluate its adaptivity, in this section we simulate a small cache configuration that is typical in an embedded processor. Table <ref type="table" target="#tab_5">6</ref> shows the cache configuration used in our experiment, which is modeled after Intel XScale PXA255 embedded processor. Although SPECint is not the best benchmark suite for embedded processor, we still choose it so that we can compare with the warm-up length for the large cache configuration. Because we do not need to profile for MRRL or BLRL in this experiment we use a segment size of 100 million instructions to increase sample size. Using the same warm-up heuristic parameters as in the previous section, the average warm-up length per sampling unit for different benchmarks is shown in Figure <ref type="figure" target="#fig_0">1</ref>. The first bar for each benchmark shows the warm-up length for the large cache configuration in Table <ref type="table" target="#tab_1">2</ref>, and the second bar is the warm-up length for the small cache configuration in Table <ref type="table" target="#tab_5">6</ref>. It is clear that SMA adapts well to the cache configuration. For the small caches the warm-up length is on average only 1/6 of that required by the large caches. Neither MRRL nor BLRL can adapt to the cache configuration. Using the warm-up length for MRRL or BLRL in the previous section for the small caches will result in 15~20X larger overhead than SMA. The only way to reduce warm-up length for the two techniques is to reduce the p-value. However, to come up with a good p-value for each configuration by experiment is highly impractical and defeats the goal of reducing simulation time.</p><p>SMA reduces warm-up overhead for small caches, but we also need to make sure that it does not compromise the accuracy of warm-up. We do not have a good microarchitecture configuration for Intel XScale PXA255 so we did not do the cycle accurate simulation. Instead, we do the cache simulation and measure the accuracy in cache misses. Inadequate warm-up will cause overestimation of cache misses and eventually lead to error in CPI. Table <ref type="table" target="#tab_6">7</ref> shows the absolute error in the number of data cache misses per sampling units compared with full cache warm-up. This error is extremely small so SMA does not lose accuracy when adapting to the small caches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and discussion</head><p>Sampling can greatly reduce simulation time. However, effective sampling requires efficient and accurate warm-up of microarchitecture structures. In this paper, we studied the warm-up process of microprocessor caches. It is found that the warm-up process varies widely for different benchmarks, for different portion in one benchmark execution, and for different cache configurations.</p><p>Based on this observation, we propose the self-monitored adaptive cache warm-up scheme. The simulator monitors the cache warm-up process and decides when the warm-up is enough based on a simple heuristic. The experiments show that SMA is accurate, exhibiting an average warm-up error of about 0.2%. SMA does not only offers superior overall accuracy but also reduces the warm-up length to 1/2 ~ 1/3 of two recently proposed methods. Unlike previous methods, SMA is adaptive to cache configuration so it can reduce warmup overhead by an order of magnitude for simulating small caches. Because SMA continues to monitor the cache accesses during cycle accurate simulation, the user can get the number of cold-start cache accesses in each sampling unit as an indicator of the accuracy of the warm-up.</p><p>SMA has one weakness: the user does not know beforehand when the cycle-accurate simulation begins. This is not a problem for simple random sampling, but poses difficulty for sampling techniques that use a predefined set of sampling units such as SimPoint. Currently we are looking into statistical sampling SMA also looks promising for warming up other microarchitecture structures such as the branch predictor and the value predictor. Both of them share the same property with caches that once an element is warmed up, it never goes back to cold-start state again, so they are also candidate for SMA. Unlike caches, one access to a branch table element is not sufficient to put it into a known state, so designing accurate warmup method by tracking reuse latency as in MRRL or BLRL is not easy, but monitoring the warm-up process with Vengroff et al's deterministic finite automaton <ref type="bibr" target="#b16">[17]</ref> may be much simpler. Extending SMA to warm up other structures is also part of our future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Average warm up length per sampling unit for different cache configurations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 . Benchmarks, their data set and dynamic instruction count. The data set name is appended to the benchmark name</head><label>1</label><figDesc></figDesc><table><row><cell>Benchmark</cell><cell># of</cell></row><row><cell></cell><cell>Instructions</cell></row><row><cell></cell><cell>(million)</cell></row><row><cell>gcc-166</cell><cell>46, 918</cell></row><row><cell>bzip2-source</cell><cell>108,878</cell></row><row><cell>crafty</cell><cell>191,883</cell></row><row><cell>eon-cook</cell><cell>80,614</cell></row><row><cell>gap</cell><cell>269,036</cell></row><row><cell>gzip-graphic</cell><cell>103,706</cell></row><row><cell>mcf</cell><cell>61,867</cell></row><row><cell>twolf</cell><cell>346,485</cell></row><row><cell>vortex-1</cell><cell>118,977</cell></row><row><cell>vpr-route</cell><cell>84,069</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 . Processor configuration Parameter 8-way (baseline)</head><label>2</label><figDesc></figDesc><table><row><cell>Machine Width</cell><cell>8</cell></row><row><cell>RUU/LSQ size</cell><cell>128/64</cell></row><row><cell></cell><cell>32KB 2-way L1 I &amp; D, 2</cell></row><row><cell>Memory System</cell><cell>ports,</cell></row><row><cell></cell><cell>Unified 1M 4-way L2</cell></row><row><cell></cell><cell>4-way 128 entries</cell></row><row><cell>ITLB / DTLB</cell><cell>4-way 256 entries</cell></row><row><cell></cell><cell>200 cycle miss penalty</cell></row><row><cell>L1/L2/Memory Latency</cell><cell>1/12/100 cycles</cell></row><row><cell></cell><cell>4 I-ALU</cell></row><row><cell>Functional Units</cell><cell>2 I-MUL/DIV 2 FP-ALU</cell></row><row><cell></cell><cell>1 FP-MUL/DIV</cell></row><row><cell></cell><cell>Combined 2K tables</cell></row><row><cell>Branch Predictor</cell><cell>7 cycle misprediction penalty</cell></row><row><cell></cell><cell>1 prediction/cycle</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 . Warm-up length for warming up all cache blocks in L1 data cache (in 100,000 instructions).</head><label>3</label><figDesc></figDesc><table><row><cell cols="2">Benchmark Average</cell><cell>Standard</cell><cell cols="2">Max Min</cell></row><row><cell></cell><cell></cell><cell>Deviation</cell><cell></cell><cell></cell></row><row><cell>bzip2-source</cell><cell>17.80</cell><cell>16.82</cell><cell>184</cell><cell>1</cell></row><row><cell>gcc-166</cell><cell>9.98</cell><cell>15.92</cell><cell>145</cell><cell>1</cell></row><row><cell>crafty</cell><cell>110.61</cell><cell>51.59</cell><cell>439</cell><cell>21</cell></row><row><cell>eon-cook</cell><cell>27.87</cell><cell>14.36</cell><cell>106</cell><cell>7</cell></row><row><cell>gap</cell><cell>8.08</cell><cell>10.28</cell><cell>167</cell><cell>1</cell></row><row><cell>gzip-graphic</cell><cell>4.62</cell><cell>2.88</cell><cell>14</cell><cell>1</cell></row><row><cell>mcf</cell><cell>1.54</cell><cell>4.02</cell><cell>45</cell><cell>1</cell></row><row><cell>twolf</cell><cell>2.82</cell><cell>15.04</cell><cell>687</cell><cell>2</cell></row><row><cell>vortex-1</cell><cell>14.46</cell><cell>15.01</cell><cell>141</cell><cell>1</cell></row><row><cell>vpr-route</cell><cell>3.59</cell><cell>3.94</cell><cell>66</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 . Warm-up length for warming up 50% cache blocks in L2 cache (in 100,000 instructions).</head><label>4</label><figDesc></figDesc><table><row><cell>Benchmark</cell><cell cols="2">Average Standard</cell><cell cols="2">Max Min</cell></row><row><cell></cell><cell></cell><cell>Deviation</cell><cell></cell><cell></cell></row><row><cell>bzip2-source</cell><cell>177.32</cell><cell>233.17</cell><cell>999</cell><cell>1</cell></row><row><cell>gcc-166</cell><cell>546.30</cell><cell>331.57</cell><cell>999</cell><cell>1</cell></row><row><cell>crafty</cell><cell>303.68</cell><cell>165.11</cell><cell>986</cell><cell>28</cell></row><row><cell>eon-cook</cell><cell>155.47</cell><cell>272.07</cell><cell>998</cell><cell>2</cell></row><row><cell>gap</cell><cell>136.75</cell><cell>62.20</cell><cell>788</cell><cell>3</cell></row><row><cell>gzip-graphic</cell><cell>837.82</cell><cell>245.56</cell><cell>999</cell><cell>5</cell></row><row><cell>mcf</cell><cell>15.41</cell><cell>73.19</cell><cell>810</cell><cell>1</cell></row><row><cell>twolf</cell><cell>34.76</cell><cell>22.38</cell><cell>920</cell><cell>8</cell></row><row><cell>vortex-1</cell><cell>208.94</cell><cell>84.66</cell><cell>874</cell><cell>5</cell></row><row><cell>vpr-route</cell><cell>52.69</cell><cell>35.47</cell><cell>232</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 . Comparison of SMA with MRRL and BLRL Avg warm up length per sampling unit (1000 instructions)</head><label>5</label><figDesc></figDesc><table><row><cell>Error in CPI</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 . Configuration for small caches</head><label>6</label><figDesc></figDesc><table><row><cell>Cache</cell><cell>Block</cell><cell>Associ-</cell><cell># of</cell><cell>Replacement</cell></row><row><cell></cell><cell>Size</cell><cell>ativity</cell><cell>Sets</cell><cell>Policy</cell></row><row><cell></cell><cell>(bytes)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>L1 Data</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell>LRU</cell></row><row><cell>L1 Instr</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell>LRU</cell></row><row><cell>L2</cell><cell></cell><cell></cell><cell>None</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 . Absolute error in the number of data cache misses per sampling unit.</head><label>7</label><figDesc></figDesc><table><row><cell>Benchmark</cell><cell>Error</cell></row><row><cell>bzip2-source</cell><cell>0</cell></row><row><cell>gcc-166</cell><cell>0</cell></row><row><cell>crafty</cell><cell>0.002614</cell></row><row><cell>eon-cook</cell><cell>0</cell></row><row><cell>gap</cell><cell>0</cell></row><row><cell>gzip-graphic</cell><cell>0</cell></row><row><cell>mcf</cell><cell>0</cell></row><row><cell>twolf</cell><cell>0.00318</cell></row><row><cell>vortex-1</cell><cell>0</cell></row><row><cell>vpr-route</cell><cell>0</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The relative speed numbers are highly dependent on the simulator and the configuration being simulated.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>No warm up length number is given in<ref type="bibr" target="#b11">[12]</ref>. This number is based on our experiment with MRRL. See Section 4.2.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The method was not given an official name in<ref type="bibr" target="#b9">[10]</ref>. It is called "Boundary Line Reuse Latency" because it is equivalent to profiling the memory reuses that cross the boundary line between a sampling unit and its pre-sample.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research is partially supported by the <rs type="funder">National Science Foundation</rs> under grant number <rs type="grantNumber">0113105</rs>, by <rs type="funder">IBM</rs> through a <rs type="grantName">CAS award</rs> and a SUR grant, and by <rs type="funder">AMD</rs>, <rs type="funder">Intel</rs> and Microsoft corporations. <rs type="person">Lieven Eeckhout</rs> is a Postdoctoral Fellow of the <rs type="institution">Fund for Scientific Research -Flanders (Belgium) (F.W.O. Vlaanderen</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_8rxVqzP">
					<idno type="grant-number">0113105</idno>
				</org>
				<org type="funding" xml:id="_9xQbH3v">
					<orgName type="grant-name">CAS award</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The SimpleScalar tool set, version 2.0</title>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Austin</surname></persName>
		</author>
		<idno>1342</idno>
		<imprint>
			<date type="published" when="1997-06">June 1997</date>
		</imprint>
		<respStmt>
			<orgName>Computer Sciences Department, University of Wisconsin-Madson</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SMARTS: Accelerating microarchitecture simulation via rigorous statistical sampling</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Wunderlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual International Symposium on Computer Architecture</title>
		<meeting>the 30th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2003-06">June 2003</date>
			<biblScope unit="page" from="84" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Precise and accurate processor simulation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Cain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lepak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Lipasti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th Workshop On Computer Architecture Evaluation Using Commercial Workloads (CAECW)</title>
		<imprint>
			<date type="published" when="2002-02">February 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the Use of Trace Sampling for Architectural Studies of Desktop Applications</title>
		<author>
			<persName><forename type="first">P</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Baer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1999 SIGMETRICS Conference</title>
		<meeting>the 1999 SIGMETRICS Conference</meeting>
		<imprint>
			<date type="published" when="1999-05">May 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An analytical cache model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hennessy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="184" to="215" />
			<date type="published" when="1989-05">May 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Trace driven simulation using sampled traces</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh Hawaii International Conference on System Sciences Vol. I: Architecture</title>
		<meeting>the Twenty-Seventh Hawaii International Conference on System Sciences Vol. I: Architecture</meeting>
		<imprint>
			<date type="published" when="1994-01">January 1994</date>
			<biblScope unit="page" from="211" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Accurate low-cost methods for performance evaluation of cache memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Laha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Iyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1325" to="1335" />
			<date type="published" when="1988-11">November 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A model for estimating trace-sample miss ratios</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kessler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGMETRICS Conference for the Measurement and Modeling of Computer Systems</title>
		<meeting>the ACM SIGMETRICS Conference for the Measurement and Modeling of Computer Systems</meeting>
		<imprint>
			<date type="published" when="1991-06">June 1991</date>
			<biblScope unit="page" from="79" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Minimal Subset Evaluation: rapid warm-up for simulated hardware state</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Haskins</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Skadron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the International Conference on Computer Design</title>
		<meeting>the International Conference on Computer Design</meeting>
		<imprint>
			<date type="published" when="2001">Sept, 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Accurately warmed-up trace samples for the evaluation of cache memories. High Performance Computing Symposium</title>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Callens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">De</forename><surname>Bosschere</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<pubPlace>Orlando, Florida</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<title level="m">Picking Statistically Valid and Early Simulation Points, International Conference on Parallel Architectures and Compilation Techniques</title>
		<imprint>
			<date type="published" when="2003-09">September 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Memory Reference Reuse Latency: Accelerated Sampled Microarchitecture Simulation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Haskins</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Skadron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 IEEE International Symposium on Performance Analysis of Systems and Software</title>
		<meeting>the 2003 IEEE International Symposium on Performance Analysis of Systems and Software</meeting>
		<imprint>
			<date type="published" when="2003-03">Mar. 2003</date>
			<biblScope unit="page" from="195" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
		<title level="m">Timekeeping in the Memory System: Predicting and Optimizing Memory Behavior. The 29th International Symposium on Computer Architecture (ISCA-29)</title>
		<imprint>
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatically characterizing large scale program behavior</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems pp</title>
		<meeting>the International Conference on Architectural Support for Programming Languages and Operating Systems pp</meeting>
		<imprint>
			<date type="published" when="2002-10">October 2002</date>
			<biblScope unit="page" from="45" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Erez</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><surname>Calder</surname></persName>
		</author>
		<title level="m">Picking Statistically Valid and Early Simulation Points , International Conference on Parallel Architectures and Compilation Techniques</title>
		<imprint>
			<date type="published" when="2003-09">September 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Llc</forename><surname>Simplescalar</surname></persName>
		</author>
		<ptr target="http://www.simplescalar.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Partial sampling with reverse state reconstruction: A new technique for branch predictor performance estimation</title>
		<author>
			<persName><forename type="first">Darren</forename><surname>Vengroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guang</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Symposium On High-Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Harskins</surname></persName>
		</author>
		<ptr target="http://www.cs.virginia.edu/~jwh6q/mrrl-web/" />
		<title level="m">Memory Reference Reuse Latency: Rapid Warm Up for Sampled Microarchitecture Simulation</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
