<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How to Understand Masked Autoencoders</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-02-08">8 Feb 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shuhao</forename><surname>Cao</surname></persName>
							<email>s.cao@wustl.edu</email>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Xu</surname></persName>
							<email>peng.xu@eng.ox.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Clifton</surname></persName>
							<email>david.clifton@eng.ox.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Washington University</orgName>
								<address>
									<settlement>St. Louis</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">How to Understand Masked Autoencoders</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-02-08">8 Feb 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2202.03670v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Masked Autoencoders (MAE) Are Scalable Vision Learners" revolutionizes the self-supervised learning that not only achieves the state-of-the-art for image pretraining, but also is a milestone that bridged the gap between the visual and linguistic masked autoencoding (BERT-style) pretrainings. However, to our knowledge, to date there are no theoretical perspectives to explain the powerful expressivity of MAE. In this paper, we, for the first time, propose a unified theoretical framework that provides a mathematical understanding for MAE. Particularly, we explain the patch-based attention approaches of MAE using an integral kernel under a non-overlapping domain decomposition setting. To help the researchers to further grasp the main reasons of the great success of MAE, based on our framework, we contribute five questions and answer them by insights from operator theory with mathematical rigor.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>"Masked Autoencoders (MAE) Are Scalable Vision Learners" <ref type="bibr" target="#b30">[31]</ref> (illustrated in Figure <ref type="figure" target="#fig_0">1</ref>) recently introduces a ground-breaking self-supervised paradigm for image pretraining. This seminal method makes great contributions at least in the following respects:</p><p>(1) MAE achieves the state-of-the-art on self-supervised pretraining on ImageNet-1K dataset <ref type="bibr" target="#b20">[21]</ref>,</p><p>outperforming the strong competitors (e.g., BEiT <ref type="bibr" target="#b4">[5]</ref>) by a clear margin in a simpler and faster manner. Particularly, a vanilla Vision Transformer (ViT) <ref type="bibr" target="#b22">[23]</ref> Huge backbone based MAE achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. This inspiring phenomenon motivates the researchers to re-consider the ViT variants in self-supervised contexts. Moreover, MAE achieves good transfer performance in downstream tasks beating the supervised pretraining and shows promising scaling behavior.</p><p>(2) MAE is a generative learning method and beats the contrastive learning competitors (e.g., MoCo v3 <ref type="bibr" target="#b17">[18]</ref>) that have dominated vision self-pretraining in recent years.</p><p>(3) MAE is a mile-stone that bridged the gap between the visual and linguistic masked autoencoding (BERT-style <ref type="bibr" target="#b21">[22]</ref>) pretrainings. Actually, the previous work prior to MAE fails to apply the masked autoencoding pretrainings on the visual domain. Thus, MAE paves a path that "Selfsupervised learning in vision may now be embarking on a similar trajectory as in NLP" <ref type="bibr" target="#b30">[31]</ref>.</p><p>The great success of MAE is interpreted by its authors as "We hypothesize that this behavior occurs by way of a rich hidden representation inside the MAE" <ref type="bibr" target="#b30">[31]</ref>. However, to the best of our knowledge, to date, there are no theoretical viewpoints to explain the powerful expressivity of MAE. Unfortunately, the theoretical analysis for the expressivity of ViT based models is so challenging that it is still under-studied.</p><p>Our Contributions In this paper, we, for the first time, propose a unified theoretical framework that provides a mathematical understanding for MAE. Particularly, we not only rethink MAE by regarding each image's embedding as a learned basis function in certain Hilbert spaces instead of a 2D pixel grid, but also explain the patch-based attention approaches of MAE from the operator theoretic perspective of an integral kernel under a non-overlapping domain decomposition setting.</p><p>To help the researchers to further grasp the main reasons for the great success of MAE, based on our proposed unified theoretical framework, we contribute five questions, and for the first time answer them partially by insights from rigorous mathematical theory:</p><p>Q1: How is the representation space of MAE formed, optimized, and propagated through layers? A: We illustrate that the attention mechanism in MAE is equivalent to a learnable integral kernel transform, and its representation power is dynamically updated by the Barron space with the positional embeddings that work as the coordinates in a high dimensional feature space. See Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q2: Why and how does patchifying contribute to MAE?</head><p>A: We prove that the random patch selecting of MAE preserves the information of the original image, while reduces the computing costs, under common assumptions on the low-rank nature of images. This paves a theoretical foundation for the patch-based neural networks/models including but not limited to MAE or ViT. See Section 3.</p><p>Q3: Why are the internal representations in the lower and higher layers of MAE not significantly different? A: We have provided a new theoretical justification for the first time by proving the stability of the internal presentations. The great success of MAE benefits from a main reason that the scaled dot-product attention in the ViT-backbone provides stable representations during the cross-layer propagation. Furthermore, we view the skip-connection for the attention mechanism from a completely new perspective: representing an approximated solution explicitly to a Tikhonov-regularized Fredholm integral equation. See Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q4: Is decoder unimportant for MAE?</head><p>A: No. We argue that the decoder is vital to helping encoder to build better representations, even if decoder is discarded after pretraining. Due to the bigger patch dimension in the MAE decoder, it allows the representation space in the encoder enriched much more often by functions from the Barron space to learn a better basis. See Section 6.</p><p>Q5: Does MAE reconstruct each masked patch merely inferred from its adjacent neighbor patches? A: No. We prove that the latent representations of the masked patches are interpolated globally based on an inter-patch topology that is learned by the attention mechanism. See Section 6.</p><p>Overall, our proposed unified theoretical framework provides a mathematical understanding for MAE and can be used to understand the intrinsic traits of the extensive patch and self-attention based models, not limited to MAE or ViT.</p><p>The rest of this paper is organized as follows: Section 2 summarizes related work. Based on our proposed theoretical framework, some mathematical understandings for the encoder and decoder of MAE are presented in Section 4 through Section 6. We draw some conclusions in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Vision Transformer (ViT) <ref type="bibr" target="#b22">[23]</ref> is a strong image-oriented network based on a standard Transformer <ref type="bibr" target="#b43">[44]</ref> encoder. It has an image-specific input pipeline where the input image needs to be split into fixedsize patches. After going through the linearly embed layer and adding with position embeddings, all the patch-wise sequences will be encoded by a standard Transformer encoder. ViT and its variants have been widely used in various computer vision tasks (e.g., recognition <ref type="bibr" target="#b42">[43]</ref>, detection <ref type="bibr" target="#b6">[7]</ref>, segmentation <ref type="bibr" target="#b35">[36]</ref>), and meanwhile work well for both supervised <ref type="bibr" target="#b42">[43]</ref> and self-supervised <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b14">15]</ref> visual learning. Recently, some pioneering works provide further understanding for ViT, e.g., its internal representation robustness <ref type="bibr" target="#b37">[38]</ref>, the continuous behavior of its latent representation propagation <ref type="bibr" target="#b38">[39]</ref>. However, the theoretical analysis for the expressivity of ViT based models is so challenging that it is still under-studied.</p><p>Masked Autoencoder (MAE) <ref type="bibr" target="#b30">[31]</ref> is essentially a denoising autoencoder <ref type="bibr" target="#b44">[45]</ref>, which has a straightforward motivation that randomly mask patches of the input image and reconstruct the missing pixels. MAE works based on two key designs: (i) asymmetric encoder-decoder architecture where the encoder takes in only the visible patches and the lightweight decoder reconstructs the target image, (ii) high masking ratio (e.g., 75%) for the input image yields a nontrivial and meaningful self-supervisory task. Its great success is attributed to "a rich hidden representation inside the MAE" <ref type="bibr" target="#b30">[31]</ref>. However, to the best of our knowledge, to date, there are no theoretical viewpoints to explain the powerful expressivity of MAE.</p><p>Mathematical Theory Related to Attention Self-attention <ref type="bibr" target="#b43">[44]</ref> is essentially processing each input as a fully-connected graph <ref type="bibr" target="#b50">[51]</ref>. Therefore, as aforementioned, we start from a more general perspective of topological spaces <ref type="bibr" target="#b12">[13]</ref> to rethink MAE, by regarding each image as a graph connecting patches instead of 2D pixel grid. Meanwhile, we study the patch-based attention approaches of MAE through the operator theory for the Fredholm integral equations <ref type="bibr" target="#b0">[1]</ref>, to formulate the dot-product attention matrix as an integral kernel <ref type="bibr" target="#b28">[29]</ref>, i.e., a learned Green's function to reconstruct solutions to the partial differential equations e.g.. <ref type="bibr" target="#b25">[26]</ref>. The skip-connection can be then explained through two perspectives, first as the term corresponding to the Tikhonov regularization <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b47">48]</ref>, or Neural Ordinary Differential Equations (ODE) <ref type="bibr" target="#b16">[17]</ref>. On the other hand, the patchification is connected with the Domain Decomposition Methods (DDM) <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b51">52]</ref>.</p><p>3 Patch is All We Need?</p><p>Patchifying has become a standard practice in Transformer-based CV models since ViT <ref type="bibr" target="#b22">[23]</ref>, see also <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b36">37]</ref>. In this section, we try to answer the question "Why and how does patchifying contribute to MAE?" from a perspective of the domain decomposition method to solve an integral equation.</p><p>We first consider the full grid Ω I ⊗ I with I = [0, 1/N, . . . , (N − 1)/N ], then the patchification of Ω is essentially a non-overlapping domain decomposition method (DDM) <ref type="bibr" target="#b15">[16]</ref>. DDM is commonly used in solving integral/partial differential equations bearing similar forms with (10), e.g., see <ref type="bibr" target="#b41">[42]</ref>, as well as image denoising problems <ref type="bibr" target="#b51">[52]</ref>.</p><p>DDM practices "divide and conquer", which is a general methodology suitable for lots of science and engineering disciplines: the original domain of interest is decomposed into much smaller subdomains. Each subproblem associated with subdomains can be more efficiently solved by the same algorithm, especially when the algorithm has a super-linear dependence on the grid size. The attention mechanism computational complexity and storage requirement have a quadratic dependence on n, and this translates a quartic dependence on the coarse grid size p (how many patches along one axis) as n = p 2 . Without patchification, we can see the operation in ( <ref type="formula" target="#formula_9">6</ref>) is of O(N 4 ) that renders the algorithm unattainable.</p><p>In this setup, Ω is first partitioned to equal-sized non-overlapping subdomains:</p><formula xml:id="formula_0">Ω = ∪ n i=1 Ω i , int(Ω i ) ∩ int(Ω j ) = ∅ as well as int(Ω i ) int(Ω j ) if i = j.</formula><p>For the simplicity of the presentation, we consider a single-channel image and define the space of bounded variation (BV) on Ω as BV (Ω), and for any unpatchified image u</p><formula xml:id="formula_1">∈ BV (Ω) ⊂ R N ×N |u| BV (Ω) := xi∈Ω    xj ∈Nx i w ij (u(x i ) − u(x j )) 2    1 2 ,<label>(1)</label></formula><p>where N xi denotes the grid points that are connected with x i through an undirected edge x i x j , and w ij is some positive weights measuring the interaction strength. When being viewed in the continuum such that u is a discrete sampling of the function u : R 2 → R + , this norm approximates |∇u| L 1 and measures the smoothness of an image, and is widely used in graph/image recovery e.g., see <ref type="bibr" target="#b7">[8]</ref>.</p><p>After the patchification we have u =</p><formula xml:id="formula_2">n 2 i=1 E i u i</formula><p>, where E i : Ω i → Ω is an extension operator that maps the patch pixel values on Ω i to Ω by zero-padding. It is straightforward to see that the representation consist of the concatenated patch matrices, and is in</p><formula xml:id="formula_3">Π n 2 i=1 BV (Ω i ).</formula><p>Here we denote</p><formula xml:id="formula_4">Y := ⊕ n 2 i=1 E i BV (Ω i ).</formula><p>By (1), the original BV space is only a subspace of the decomposed product space, i.e., BV (Ω) ⊂ Y, while the reverse is not true. The reason is that by patchification, the underlying function spaces have completely lost the inter-patch topological relations: e.g., how the inter-patch pixel intensities change? There is also another analytical set of questions to answer: how to choose the size of the patch? If bigger patches are used, it then requires a bigger embedding dimension, thus harder to formulate the bases for the representation space. On the other hand, if smaller patches are used, because of the aforementioned quartic dependence on the grid size, the inter-patch topology is much harder and more expensive to learn.</p><p>In the following proposition, we shall show that, if an image has a low rank structure, then there exists a semi-randomly chosen set of patches that can represent the original image. This selection has a very high mask ratio, that is, the representation using a small fraction e.g., p patches out of all the patches. Meanwhile, we assume that there exists an embedding that is able to represent the original image, in the sense that the difference between the reconstruction and the original image is small measured under | • | BV (Ω) .</p><p>Following the standard practices <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b42">43]</ref>, we consider the ViT-base case in MAE where the dimension of the feature embedding matches the original dimension of the pixel counts in the any given patch: u ∈ R N ×N R n 2 ×d , i.e., for each patch's embedding in the MAE encoder, there exists a continuous embedding that maps R n 2 ×d → R N ×N , from the patch embeddings to images. Proposition 3.1 (Existence of a near optimal patch embedding). For any u ∈ BV (Ω) ⊂ R N ×N , and an equal-sized n × n patchification of u, let the patch grid size N c := N/n, assuming there exists a rank r approximation to u with r &lt; N c and error , and a unique patch is randomly chosen from each row such that the final selection's columns set is a permutation of {1, • • • , n 2 }, then there exists an embedding y ∈ R p×d such that</p><formula xml:id="formula_5">u − R(y) BV (Ω) &lt; C(r, N c , p) ,<label>(2)</label></formula><p>where R is a rank preserving reconstruction operator R : R p×d → R N ×N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Attention in MAE: a Kernel Perspective</head><p>In this section, we reexamine the attention block present in both encoder and decoder layers in MAE from multiple perspectives. The first and foremost understanding comes from the operator theory for the integral equations. By formulating the scaled dot-product attention as a nonlinear integral transform, learning the latent representation is equivalent to learning a set of basis in a Hilbert space. Moreover, upon adopting the kernel interpretation of the attention mechanism, a single patch is characterized by not only its own learned embedding, but also how it interacts with all other patches (reproducing property of a kernel).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Self-attention: A Nonlinear Integral Kernel Transform</head><p>For MAE <ref type="bibr" target="#b30">[31]</ref>, each input image patch is projected as a 1D token embedding. Following the practice in <ref type="bibr" target="#b30">[31]</ref>, we also omit the shared class token added to the embedding that can be removed from the  pretraining of MAE. Given an attention block in an encoder or a decoder layer of MAE, the input and output are defined respectively as y in := y, y out ∈ R p×d . When an image has not been masked, it has p = n 2 patches, where n is a positive integer. The embedding dimension is d. In y, a positional embedding X := {x i } p i=1 ⊂ R 1×d , such that x i is associated with the i-th patch, is added. Each x i can be viewed as a coordinate in high dimension. We note that the patch ordering map i → x i is injective, since the first component of x i is the polar coordinate in 2D, and as such the relative position of each patch can be recovered from x i . For simplicity, the analysis done in this section applies to a single head.</p><p>Here we briefly review the self-attention from <ref type="bibr" target="#b43">[44]</ref>. The query Q, key K, value V are generated by three learnable projection matrices</p><formula xml:id="formula_6">W Q , W K , W V ∈ R d×d : Q = yW Q , K = yW K , V = yW V . The scaled dot-product attention is to obtain z ∈ R p×d : z = Attn s (y) := Softmax QK / √ d V.<label>(3)</label></formula><p>Then the softmax attention Attn(•) with a global receptive field works as the following nonlinear mapping:</p><formula xml:id="formula_7">Attn : R p×d → R p×d , y → LN y + z + FFN LN(y + z)) ,<label>(4)</label></formula><p>where LN(•) denotes the Layer Normalization (LN) <ref type="bibr" target="#b1">[2]</ref> that essentially is a learnable column scaling with a shift, and FFN(•) is a standard two-layer feedforward neural network (FFN) applied to the embedding of each patch.</p><p>Upon closer inspection of the scaled dot-product attention (3), for z, the j-th element of its i-th row z i is obtained by</p><formula xml:id="formula_8">(z i ) j = A i• • v j ,<label>(5)</label></formula><p>and the i-th row A i• of the attention matrix A is</p><formula xml:id="formula_9">A i• = e (QK / √ d)i• p j=1 e (QK / √ d)ij =: Softmax(q i K / √ d),<label>(6)</label></formula><p>i.e., using the diagram in Figure <ref type="figure" target="#fig_2">2</ref>, we have</p><formula xml:id="formula_10">z i = A i•   v 1 . . . v n   = p j=1 A ij v j .<label>(7)</label></formula><p>Due to the softmax, A i• contains the coefficients for the convex combination of the vector representations {v j } p j=1 . This basis set {v j } p j=1 form the V's row space, and it further forms each row of the output z by multiplying with A. As a result, the scaled dot-product attention has the following basis expansion form:</p><formula xml:id="formula_11">z i := p j=1 A(q i , k j ) v i ,<label>(8)</label></formula><p>where u i denotes the i-th row of certain latent representation u ∈ {z, Q, K, V}. The key term in (8), A(•, •), denotes the attention kernel, which maps each patch's embedding represented by the rows of Q, K to a measure of how they interact. From (8), we shall see that the representation space for an encoder layer in MAE is spanned by the row space of V, and is being nonlinearly updated layer-wise. This means, the embedding for each patch serves as a basis to form the representation space for the current attention block, whereas the row-wise attention weights are the Barycentric coordinates for a simplex in R 1×d .</p><p>Here we further assume that there exist a set of feature maps for query, key, and value e.g., see <ref type="bibr" target="#b19">[20]</ref>.</p><p>For z, the feature map z ∈ V that maps R 1×d → BV (Ω i ), i.e., z i = z(x i ), we can then define an asymmetric kernel function κ(•,</p><formula xml:id="formula_12">•) : R 1×d × R 1×d → R, A(q i , k j ) = α −1 (x i ) q i , k j = α −1 (x i ) q(x i ), k(x j ) =: α −1 (x i ) κ(x i , x j ),<label>(9)</label></formula><p>where α(x i ) :</p><formula xml:id="formula_13">= d l=1 (e qiK / √ d ) l . Now the discrete kernel A(•, •)</formula><p>with vectorial input is rewritten to an integral kernel κ(•, •) whose inputs are positions. As a result, using the formulation above, we can express the scaled dot-product attention approximately as a nonlinear integral transform:</p><formula xml:id="formula_14">z(x) = α −1 (x) x ∈X q(x) • k(x ) v(x ) δ x ≈ α −1 (x) ω κ(x, x )v(x ) dµ(x ),<label>(10)</label></formula><p>where δ x is the Dirac measure associated with the position at x. For the second equation, with a slight abuse of the order presentation, we assume that there exists κ(•, •) and a Borel measure µ(•) such that κ(•, •) has a reproducing property under the integral by dµ(•), which shall be elaborated in the next paragraph. Here in this integral, the image domain ω is approximated by a patchified grid</p><formula xml:id="formula_15">X (0, 1 n • • • , n−1 n ) ⊗ (0, 1 n • • • , n−1 n ).</formula><p>Returning to the perspective of "embedding ≈ basis" for the representation space, the backpropagation from the output z(•) to update weights to obtain a new v(•) during pretraining can be interpreted as an iterative procedure to solve the Fredholm integral equation of the first kind in <ref type="bibr" target="#b9">(10)</ref>.</p><p>How to Obtain a Translation Invariant Kernel. We note that the formulation above in <ref type="bibr" target="#b9">(10)</ref> is closely related to Reproducing Kernel Hilbert Space (RKHS) <ref type="bibr" target="#b8">[9]</ref>. First, the dot-product in ( <ref type="formula" target="#formula_9">6</ref>) is re-defined to be</p><formula xml:id="formula_16">A i• := Softmax − γ(q i − k i )(Q − K) .<label>(11)</label></formula><p>Then, a pre-layer normalization scheme in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b48">49]</ref> or pre-inner product normalization in <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b11">12]</ref> and can be a cheap procedure to re-center the kernel. Rewritting the original dot-product for q, k ∈ R 1×d as follows</p><formula xml:id="formula_17">qk = 1 2 qq + 1 2 kk − 1 2 (q − k)(q − k) .<label>(12)</label></formula><p>Thus, a layer normalization for each row in Q and K before QK results the two moment terms above, qq and kk , in <ref type="bibr" target="#b11">(12)</ref> being relatively small versus the third term. Consequently, using merely the third term in <ref type="bibr" target="#b11">(12)</ref> to form <ref type="bibr" target="#b10">(11)</ref> suffices to provided a good approximation to the scaled dot-product attention. As an important consequence, the kernel becomes symmetric, and translation-invariant, which is arguably one of the nicest traits of CNN kernels. As a result, the normalized attention kernel κ(x, x ) based on ( <ref type="formula" target="#formula_16">11</ref>) is</p><formula xml:id="formula_18">(A) ij = γ q i − k i , q j − k j =: κ(x i , x j ).<label>(13)</label></formula><p>Moreover, κ(x, x ) can be written as K(x−x ) for K : R 1×d → R being the 1-dimensional Gaussian radial basis function (RBF) kernel, K(θ) = e −γ θ 2 . By the positivity and the symmetrcity, κ(•, •) becomes a reproducing kernel. We can define the following nonlinear integral operator K:</p><formula xml:id="formula_19">K(v)(x) := ω κ(x, x )v(x ) dµ(x ).<label>(14)</label></formula><p>In the traditional settings such that κ(x, x ) is not explicitly dependent on v(•), the vector version of the Mercer representation theorem for the integral kernel can be exploited <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b13">14]</ref>, and there exists an optimal representation space to approximate the solution to this integral equation: the eigenspace the nonlinear integral operator K. While for the scaled dot-product attention's integral transform interpretation, explicitly pinning down the eigenspace is impossible during the dynamic training procedure. Nevertheless, we can still obtain a representation expansion to show that the internal representations are propagated in a stable fashion (Theorem 5.1).</p><p>Relation to Other Methods. The integral transform formula derived in <ref type="bibr" target="#b9">(10)</ref> resembles the nonlocal means method (NL-means) in tasks of traditional image signal processing such as denoising, e.g., <ref type="bibr" target="#b9">[10]</ref>. In NL-means, κ(x, x ) is a non-learnable integral kernel, which measures the similarity between pixels at x and x . While in ViT, the layer-wise kernel κ(x, x ; θ) is learnable, and establishes the inter-patch topology (continuity, total variation, etc). One key common trait is that they are both normalized in the sense that for any x ∈ ω</p><formula xml:id="formula_20">α −1 (x) ω κ(x, x ) dµ(x ) = 1,<label>(15)</label></formula><p>which is facilitated by the row-wise softmax operation to enforce a unit row sum. We shall see in Section 5 that this plays a key role in obtaining a stable representation in ViT layers mathematically.</p><p>Compared with another popular approach, the Convolutional Neural Network (CNN)-based models, the MAE has several differences. In CNN, the translation-invariance is inherited automatically from the convolution operation, i.e., the CNN kernel depends only on the difference of the positions (x −x ). Moreover, K(•) is acting on a pixel level and locally supported, thus having a small receptive field. For CNN to learn long-range dependencies, the deep stacking of convolutions is necessary but then renders the optimization algorithms difficult <ref type="bibr" target="#b39">[40]</ref>. Moreover, repeating local operations around a small pixel patch is computationally inefficient in that the convolution filter sizes being small in CNN makes the "message passing" from distant positions back and forth difficult <ref type="bibr" target="#b46">[47]</ref>. In MAE, the translation invariance is obtained through proper normalization. The learned kernel acts on the basis function representing each patch. Moreover, the kernel map is globally supported, which means it can learn effectively the interaction between even far away patches. As a result, this global nature makes it easier to learn a better representation, thus greatly reduces the number of layers needed to complete the same task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Stable Representation Propagation in the Attention Block</head><p>In this section, we try to explain why the representations are continuously changing in a stable fashion in both the encoder and decoder of MAE, which is first observed in <ref type="bibr" target="#b38">[39]</ref>. In the following theorem, we have proved a key result that the softmax normalization plays a vital role in stabilizing the propagation of the representations through the layers.</p><p>Theorem 5.1. Assume that a trained attention block in the MAE/ViT encoder layer such that W Q − W K 2 are uniformly bounded above and below by two positive constants, and W V = I, and the attention kernel being in the form of (13) such that it is translation invariant and symmetric, let v (t) (•) and v (t+1) (•) be the feature maps for the input and output of the scaled dot-product attention (8), and v (t) ∈ BV (ω) ∩ V, then we have</p><formula xml:id="formula_21">v (t+1) − v (t) V ≤ C(d) 1 n v (t) V + |v (t) | BV . (<label>16</label></formula><formula xml:id="formula_22">)</formula><p>Proof Sketch and an Interpretation for Theorem 5.1. The softmax operation in (6) makes the attention matrix to have a row sum being 1. This normalization further translates to the integral kernel's integration being 1 in <ref type="bibr" target="#b14">(15)</ref>. This enables us to estimate of how the internal representations are propagated in a stable evolution from v (t) to v (t+1) . We can rewrite v (t) (x) Thus, the propagation can lead the following modulus of continuity representation</p><formula xml:id="formula_23">v (t) (x) = α −1 (x) ω κ(x, x )v (t) (x) dµ(x ),<label>(17) (a)</label></formula><formula xml:id="formula_24">(b) (c) (d) (e) (f) (g) (h) (i) (j)</formula><formula xml:id="formula_25">v (t+1) (x) − v (t) (x) = α −1 (x) ω κ(x, x )v (t) (x ) dµ(x ) − v (t) (x) = α −1 (x) ω κ(x, x ) v (t) (x ) − v (t) (x) dµ(x ).<label>(18)</label></formula><p>Using the argument in Section 4.1, κ(x, x ) = K(x − x ) thus a simple substitution can be exploited to get the following representation:</p><formula xml:id="formula_26">α −1 (x) ω K(x − x ) v (t) (x ) − v (t) (x) dµ(x ) = α −1 (x) ω ξ K(ξ) v (t) (x + ξ) − v (t) (x) dµ(ξ).<label>(19)</label></formula><p>By Mercer's theorem, there exists an eigenfunction expansion for κ(x, x ) = ∞ i=1 a i ψ i (x)ψ i (x ) that has a spectral decay. Then, the propagation bounds can be proved under the assumption that the current layer's learned feature map offers a "reasonable" approximation to the eigenspace of κ(•, •).</p><p>Overall, this bound describes that the propagation of the internal representation is continuously changing based on the inter-patch interaction encoded in the attention kernel. In the kernel interpretation of the attention mechanism, there is one key difference with the conventional kernel method: the conventional kernel measures the inter-sample similarity in the whole dataset; while the attention kernel κ(•, •) or κ(•, •) is built for a single instance, and learns inter-patch topological relations to build a better representation space for the MAE. This learned representation for a specific data sample determines how amenable it is for the downstream tasks.</p><p>Additionally, we note that this result applies to the layer-wise representation propagation in the decoder layers as well. An enlightening illustration is demonstrated in Figure <ref type="figure" target="#fig_3">3</ref>. There are multi-fold interesting aspects about this evolution diagram shown: (1) in the ViT-base MAE, the embedding dimension of its decoder is merely 512, which is less than the embedding dimension in the encoder (d = 768). Note that 768 = N c × N c × 3 which enables that the latent representation (a row vector) in a single patch can be directly reshaped to an image patch. Thus, we need a patch-wise upsampling projection; (2) this projection, which maps vectors in R 196×512 to those in R 196×768 resides in the MAE decoder. It should be noted that this upsampling projection is only connected with the last decoder layer. Heuristically, this projection may only upsample the last decoder layer's output z 8 ∈ R 196×512 to the desired embedding dimensions. Nevertheless, due to the stable representation propagation, the outputs from the previous decoder layers can also benefit from the last layer projection weights to obtain sensible reconstruction results, as illustrated in Figure <ref type="figure" target="#fig_3">3</ref>.</p><p>Skip-Connection as a Tikhonov Regularizer After presenting Theorem 5.1, one might ask: given the integral transform representation is already stable, what is the role of the skip-connection? Diverting from the original interpretation of battling the diminishing gradient for the skip-connection <ref type="bibr" target="#b31">[32]</ref>, here we offer a new perspective and some heuristics inspired by functional analysis to articulate the reason why using the skip-connection would make the representation propagation more stable, which is also observed in <ref type="bibr" target="#b38">[39]</ref>.</p><p>Knowing that the integral kernel interpretation of attention <ref type="bibr" target="#b9">(10)</ref> resembles the Fredholm integral equation of the first kind, we can interpret the skip-connection as a layer-wise Tikhonov regularization in the Fredholm integral equation of the second kind. Starting from ( <ref type="formula" target="#formula_14">10</ref>)</p><formula xml:id="formula_27">z(x) = α −1 ω κ(x, x )v(x )dµ(x ).<label>(20)</label></formula><p>This Fredholm intergral equation of the first kind is usually extremely ill-posed <ref type="bibr" target="#b27">[28]</ref>, in the sense that the solution, even if it exists, does not depend continuously on v(•).</p><p>If W V ≈ β −1 I, then for x ∈ X , we have that</p><formula xml:id="formula_28">z(x) ≈ βv(x) + α −1 ω κ(x, x )v(x )dµ(x ).<label>(21)</label></formula><p>This is the Fredholm integral equation of the second kind witha variable coefficient, the extra βv(•) term not only contributes to the well-posedness of this equation, but renders the numerical scheme to approximate a better v(•) more stable. Tikhonov firstly introduced this method in <ref type="bibr" target="#b40">[41]</ref>, see also <ref type="bibr" target="#b34">[35,</ref><ref type="bibr">Chapter 16</ref>]. Theorem 5.2 (Skip-connection can represent the minimizer to a Tikhonov-regularized integral equation functional). For K(•) using κ(•, •) as the integral kernel in <ref type="bibr" target="#b12">(13)</ref>, define the following functional</p><formula xml:id="formula_29">L := 1 2 α −1 (•)K(v) − z 2 V * + β K(v), v ,<label>(22)</label></formula><p>where u V := sup v∈V | u, v |/ v V denotes the dual norm. The Euler-Lagrange equation associated with min v L has the following form:</p><formula xml:id="formula_30">βu + α −1 K(u) = z in (K * (V)) .<label>(23)</label></formula><p>Note that <ref type="bibr" target="#b22">(23)</ref> bears exactly the same form with <ref type="bibr" target="#b20">(21)</ref>. Moreover, instead of the conventional L 2 -type Tikhonov regularizer v 2 V , here K(v), v is an induced norm by the positive (semi)definite integral operator K(•). The skip-connection term in <ref type="bibr" target="#b22">(23)</ref> comes from the Tikhonov regularization term in <ref type="bibr" target="#b21">(22)</ref>, and without it, the Euler-Lagrange equation reverts to the Fredholm integral equation of the first kind.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">MAE Decoder: Low-Rank Reconstruction Through Global Interpolation</head><p>During pretraining, the major function of the MAE decoder is to map the low-rank representation obtained by the MAE encoder to a reconstruction. The encoder embedding has a bigger dimension, yet is define on only a fraction of patches. The decoder reduces the embedding dimension, but is able to obtain the embedding for all p × p patches including the masked ones. Despite of the fact that the decoder in MAE is only used in pretraining, not downstream tasks, it plays a vital role in learning a "good" representation space for the MAE encoder.</p><p>Enrichment of the Representation Space through Positional Embedding Because the positional embedding x := n i=1 x i ∈ R p×d is added to the latent representation y in (3), the nonlinear universal approximator (FFN) in each attention block shall also contribute to learning a better representation. In every decoder layer, the basis functions in V are being constantly enriched by span{w j ∈ X : w j (x i ) = (FFN(y + x)) ij , 1 ≤ j ≤ d}. If we treat the positional embeddings {x i } as coordinate again, FFN(y + x) is a subset of the famous Barron space <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b2">3]</ref>, which has a rich and powerful representation power that can approximate smooth function arbitrarily well, e.g., see <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b49">50]</ref>. As a result, the representation space is dynamically updated layer by layer to try to build a more expressive representation to better characterize the inter-patch topology. FFNs themselves have no linear structure, however, the basis functions produced this way act as a building block to update the linear space for the expansion in ( <ref type="formula" target="#formula_11">8</ref>) and <ref type="bibr" target="#b9">(10)</ref>. In the MAE decoder, the number of basis (p 2 = 196) is much greater than that of the MAE encoder (0.25 • p 2 = 49), thus heuristically speaking, this basis update mechanism is mostly working in the decoder.</p><p>Reconstruction is a Global Interpolation. In MAE decoder, the mask token, a learnable vector shared by every masked patch, is concatenated to the unshuffled representation of the unmasked patches. To demonstrate that the reconstructed image is a global interpolation by using the unmasked patch embeddings, we first need to prove the following lemma, which states that the learned mask token embedding can be simply replaced by zero with an updated set of attention weights. For the simplicity, we denote the embedding dimension in the MAE decoder still as d. Lemma 6.1. Let m ∈ R 1×d be the learned mask token embedding that is shared by all masked patches, and let m(•) be its feature map. Denote the set of masked and unmasked patches' index as M and N , respectively, and we assume that |N | ≥ 2. For the input y ∈ R p×d that already contains the masked patches, i.e., all i ∈ M, y i = m, , there exists a new set of affine linear maps to generate { Q, K, V}, such that for any i = 1, . . . , n,</p><formula xml:id="formula_31">n j=1 A(q i , k j ) v i − j∈N A(q i , kj ) vi &lt; Cn −1 .<label>(24)</label></formula><p>With this lemma, we are already present our final result: for the first layer of the MAE decoder, the network interpolates the representation using global information from the embeddings learned by the MAE encoder, not just the ones from the nearby patches. Moreover, with Theorem 5.1, the MAE decoders continue to perform such a global interpolation in subsequent layers. For an empirical evidence, please refer to Figure <ref type="figure" target="#fig_3">3</ref>. Proposition 6.2 (Interpolation results for masked patches). For the embedding of every masked patch i ∈ M, let v (t+1) i be the output embedding of a decoder layer for this patch, and let v (t) j be the input from the encoder for j = 1, . . . , p, then v</p><formula xml:id="formula_32">(t+1) i is v (t+1) i = j∈N a j v (t) j ,<label>(25)</label></formula><p>for a set of weights based on unmasked patches a j (v i1 , . . . , v i k ), N := {i 1 , . . . , i k }. Moreover, the reconstruction quality is bounded above by the global reconstruction error of the unmasked patches,</p><formula xml:id="formula_33">Rv i − u i BV (Ωi) ≤ C sup j∈N Rv j − u BV (Ωj ) + Cn −1 u . (<label>26</label></formula><formula xml:id="formula_34">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>To the best of our knowledge, to date, there are no theoretical viewpoints to explain the powerful expressivity of MAE. In this paper, we, for the first time, propose a unified theoretical framework that provides a mathematical understanding for MAE. Particularly, we explain the patch-based attention approaches of MAE from a perspective of an integral kernel under a non-overlapping domain decomposition setting. To help the researchers to further grasp the main reasons for the great success of MAE, our mathematical proof contributes the following major conclusions:</p><p>(1) The attention mechanism in MAE is a learnable integral kernel transform, and its representation power is dynamically updated by the Barron space with the positional embeddings that work as the coordinates in a high dimensional feature space.</p><p>(2) The random patch selecting of MAE preserves the information of the original image, while reduces the computing costs, under common assumptions on the low-rank nature of images. This paves a theoretical foundation for the patch-based neural networks/models including but not limited to MAE or ViT.</p><p>(3) The great success of MAE benefits from a main reason that the scaled dot-product attention built-in ViT provides the stable representations during the cross-layer propagation.</p><p>(4) In MAE, the decoder is vital to helping the encoder to build better representations, while decoder is discarded after pretraining.</p><p>(5) The latent representations of the masked patches are interpolated globally based on an inter-patch topology that is learned by the attention mechanism.</p><p>Furthermore, our proposed theoretical framework can be used to understand the intrinsic traits of not only the ViT-based models but also even the extensive networks/models made by patch and self-attention.</p><p>and applying G −1 on K(•, u) ∈ V , it reads for any w ∈ W α −1 (•)G −1 z, • − K(u), • , G −1 ( K(w), • ) + K(u), v = α −1 φ u , G −1 ( K(u), • ) + K(u), w = α −1 K(w), φ u + K(u), w = 0.</p><p>(</p><formula xml:id="formula_35">)<label>38</label></formula><p>As a result, combining <ref type="bibr" target="#b37">(38)</ref>, <ref type="bibr" target="#b35">(36)</ref>, and the self-adjointness of K, we have the system in the continuum becomes φ u , v + α −1 K(u), v = z, v , ∀v ∈ V, α −1 K(w), φ u + β K(u), w = 0, ∀q ∈ Q.</p><p>The first equation implies that:</p><formula xml:id="formula_37">φ u + α −1 K(u) = z in V .<label>(40)</label></formula><p>Hence, by w ∈ W ⊆ V, when plugging w to the second equation in <ref type="bibr" target="#b35">(36)</ref> we have</p><formula xml:id="formula_38">z − α −1 K(u) + βu, K(w) = 0 for w ∈ W,<label>(41)</label></formula><p>and the theorem follows.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of the Masked Autoencoder (MAE) in [31], with Vision Transformer (ViT) backbone. (Used under CC BY-NC 4.0 license from https://github.com/facebookresearch/ mae.) Best viewed in color.</figDesc><graphic url="image-1.png" coords="2,167.40,123.17,51.18,51.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: The i-th patch embedding from the scaled dot-product attention is a convex combination of the attention weights, which encodes the interactions among all patch embeddings. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The internal representation propagation during evaluation for a random sample from ImageNet-1K validation set. (a) input of MAE encoder; (b) output of MAE encoder, concatenated with the learnable mask token embedding that is shared among all masked patches; (c)-(i) the latent representations, i.e., the outputs of the decoder layers #1 to #7; (j) output of the last decoder layer (#8). (We use the official released model and pretrained weights at https://github.com/ facebookresearch/mae.) See texts for details. Best viewed in color.</figDesc><graphic url="image-16.png" coords="8,115.35,171.73,71.28,71.28" type="bitmap" /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Proof of Theorem 5.1 Assumption A.1 (Assumption of Theorem 5.1). To prove Theorem 5.1, we assume the following conditions hold true, (B 1 ) The underlying Hilbert space for the latent representation is V ⊂ L 2 (ω, µ)) d , i.e., there is no a priori inter-channel continuity, and the inter-channel topology is learned from data.</p><p>(B 2 ) For a BV function on ω, there exists a smooth extension to R 2 such that the extension is stable in | • | BV (Ω) (see e.g., <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b10">[11,</ref><ref type="bibr">Theorem 2]</ref>).</p><p>(B 3 ) For each v ∈ V, there exists a smooth K such that κ(x, x ) = K(x − x ), and K(x − x ) ≤ C γ e −γ x−x with a uniform constant C γ that only depends on γ.</p><p>(B 4 ) For each v ∈ V, α(x) := ω κ(x, x ) dµ(x ), and α is bounded below by a positive constant C α uniformly. Theorem 5.1 (Stability result, rigorous version). Under the assumption of Assumption A.1, we have</p><p>Proof. First we extend the kernel function from ω to the whole R 2 by</p><p>With a slight abuse of notation we still denote the extension Ext(κ(•, •)) still as κ(•, •), and the normalization (15) still holds:</p><p>Without loss of generality, we also assume that the Thus, under assumption (B 1 )-(B 2 ), we have the original integral on ω can be written as on the full plane</p><p>Similarly by <ref type="bibr" target="#b28">(29)</ref>,</p><p>Therefore,</p><p>Using κ(x, x ) = K(x − x ), let ξ = x − x:</p><p>By v (t) ∈ BV (R 2 ), and ω being compact, there exists δ &gt; 0 such that</p><p>Thus, for any</p><p>Taking sup x∈ω yields the result.</p><p>B Proof of Theorem 5.2</p><p>, where u ∈ W ⊆ V is the solution to the integral equation in <ref type="bibr" target="#b22">(23)</ref>, and W is the solution subspace. Clearly, η u ∈ V defines a bounded functional on V for (V, •, • ). By Riesz representation theorem, there exists an isomorphism</p><p>Then, define</p><p>Thus, the functional L(v) can be written as:</p><p>Taking the Gateaux derivative lim τ →0 dL(u + τ w)/dτ in order to find the critical point(s) u ∈ W, we have for any perturbation w ∈ W such that u + τ w ∈ W</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Numerical solution of fredholm integral equations of the second kind</title>
		<author>
			<persName><forename type="first">Kendall</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weimin</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theoretical Numerical Analysis</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="473" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Breaking the curse of dimensionality with convex neural networks</title>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="629" to="681" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10853</idno>
		<title level="m">Adaptive input representations for neural language modeling</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">BEiT: BERT pre-training of image transformers</title>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Universal approximation bounds for superpositions of a sigmoidal function</title>
		<author>
			<persName><surname>Andrew R Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information theory</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="930" to="945" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Toward transformerbased object detection</title>
		<author>
			<persName><forename type="first">Josh</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kislyuk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09958</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graph signal recovery via primal-dual algorithms for total variation minimization</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Hannak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerald</forename><surname>Matz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="842" to="855" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Reproducing kernel Hilbert spaces in probability and statistics</title>
		<author>
			<persName><forename type="first">Alain</forename><surname>Berlinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Thomas-Agnan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A review of image denoising algorithms, with a new one</title>
		<author>
			<persName><forename type="first">Antoni</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bartomeu</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Michel</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale modeling &amp; simulation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="490" to="530" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Maz&apos;ya. Potential Theory and Function Theory for Irregular Regions</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">U D</forename><surname>Burago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Burago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seminars in mathematics. Consultants Bureau</title>
				<imprint>
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Choose a transformer: Fourier or galerkin</title>
		<author>
			<persName><forename type="first">Shuhao</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">35th Conference on Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">NeurIPS 2021</date>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Gunnar Carlsson. Topology and data. Bulletin of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="255" to="308" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Vector valued reproducing kernel hilbert spaces of integrable functions and mercer theorem</title>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Carmeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernesto</forename><forename type="middle">De</forename><surname>Vito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Toigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="377" to="408" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Domain decomposition algorithms</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tarek P</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><surname>Mathew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta numerica</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="61" to="143" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Ricky Tq Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName><surname>Duvenaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07366</idno>
		<title level="m">Neural ordinary differential equations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02057</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14794</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
				<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Eigenvalues of integral operators defined by smooth positive definite kernels. Integral Equations and Operator Theory</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><surname>Menegatto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="61" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Analysis II</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gerhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analysis</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>International Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deepgreen: deep learning of green&apos;s functions for nonlinear boundary value problems</title>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">E</forename><surname>Craig R Gin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Shea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Brunton</surname></persName>
		</author>
		<author>
			<persName><surname>Kutz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The theory of tikhonov regularization for fredholm equations. 104p</title>
		<author>
			<persName><surname>Cw Groetsch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Boston Pitman Publication</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Lectures on Cauchy&apos;s problem in linear partial differential equations</title>
		<author>
			<persName><forename type="first">Jacques</forename><surname>Hadamard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Courier Corporation</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Bounded integral operators on L 2 spaces</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Halmos</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Viakalathur</forename><surname>Shankar Sunder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">96</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Transformer in transformer</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">35th Conference on Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">NeurIPS 2021</date>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Query-key normalization for transformers</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raj</forename><surname>Prudhvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Dachapally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Shantaram Pawar</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
				<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="4246" to="4253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A proof that rectified deep neural networks overcome the curse of dimensionality in the numerical approximation of semilinear heat equations</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Hutzenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnulf</forename><surname>Jentzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuan</forename><forename type="middle">Anh</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SN partial differential equations and applications</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Linear Integral Equations</title>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Kress</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Sayak</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Vision transformers are robust learners. AAAI</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Do vision transformers see like convolutional neural networks?</title>
		<author>
			<persName><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Solution of incorrectly formulated problems and the regularization method</title>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Nikolajevits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tihonov</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soviet Math</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1035" to="1038" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Domain decomposition methods-algorithms and theory</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Toselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olof</forename><surname>Widlund</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
				<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning deep transformer models for machine translation</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01787</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A reliable and fast method for the solution of fredhol integral equations of the first kind based on tikhonov regularization</title>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Weese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer physics communications</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10524" to="10533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Finite neuron method and convergence analysis</title>
		<author>
			<persName><forename type="first">Jinchao</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Computational Physics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1707" to="1745" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multigraph transformer for free-hand sketch recognition</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Bregmanized nonlocal regularization for deconvolution and sparse reconstruction</title>
		<author>
			<persName><forename type="first">Xiaoqun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanley</forename><surname>Osher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="253" to="276" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
