<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Set of Selected SIFT Features for 3D Facial Expression Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Stefano</forename><surname>Berretti</surname></persName>
							<email>berretti@dsi.unifi.it</email>
						</author>
						<author>
							<persName><forename type="first">Alberto</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
							<email>delbimbo@dsi.unifi.it</email>
						</author>
						<author>
							<persName><forename type="first">Pietro</forename><surname>Pala</surname></persName>
							<email>pala@dsi.unifi.it</email>
						</author>
						<author>
							<persName><forename type="first">Boulbaba</forename><forename type="middle">Ben</forename><surname>Amor</surname></persName>
							<email>boulbaba.benamor@telecom-lille1.fr</email>
						</author>
						<author>
							<persName><forename type="first">Mohamed</forename><surname>Daoudi</surname></persName>
							<email>mohamed.daoudi@telecom-lille1.fr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Sistemi e Informatica Università di Firenze Firenze</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Institut TELECOM</orgName>
								<address>
									<country>TELECOM Lille</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">LIFL</orgName>
								<orgName type="institution">Université de Lille</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Set of Selected SIFT Features for 3D Facial Expression Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">53800BA12A3AC7967A4C83A4F8772D42</idno>
					<idno type="DOI">10.1109/ICPR.2010.1002</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D facial expression recognition</term>
					<term>feature selection</term>
					<term>svm classification</term>
					<term>2010 International Conference on Pattern Recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, the problem of person-independent facial expression recognition is addressed on 3D shapes. To this end, an original approach is proposed that computes SIFT descriptors on a set of facial landmarks of depth images, and then selects the subset of most relevant features. Using SVM classification of the selected features, an average recognition rate of 77.5% on the BU-3DFE database has been obtained. Comparative evaluation on a common experimental setup, shows that our solution is able to obtain state of the art results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Methods capable to automatically recognize facial expressions are required in several different areas, such as computer graphics and human-machine interaction. Early work on this subject analyzed facial expressions in 2D images and videos by tracking facial features and measuring the amount of facial movements. These approaches were inspired by the pioneering work of Ekman <ref type="bibr" target="#b0">[1]</ref>, that proposed a categorization of basic facial expressions into six classes representing anger, disgust, fear, happiness, sadness and surprise. Recently, the increasing availability of effective devices capable to acquire high resolution 3D data, has determined a progressive shift from 2D to 3D approaches. This is mainly motivated by the fact that deformations of facial morphology due to expression changes in 3D are expected to be more easily detectable than in 2D. Moreover, the availability of new 3D facial expression databases, like that constructed at the Binghamton University (BU-3DFED) <ref type="bibr" target="#b10">[11]</ref>, has further pushed the research on 3D facial expression recognition. In general, previous work on 3D facial expression recognition can be categorized as based on: generic facial model or feature classification. In the first category, a general face model is constructed using dense correspondence and some prior knowledge on a set of training faces. For example, in <ref type="bibr" target="#b5">[6]</ref> an elastically deformable model algorithm that establishes correspondence among a set of faces is proposed. Fitting the elastically deformable model to unknown faces enables face recognition invariant to facial expressions and facial expression recognition with unknown identity. In <ref type="bibr" target="#b1">[2]</ref>, the shape of an expressional 3D face is approximated as the sum of a basic facial shape component, and an expressional shape component. The two components are separated by learning a reference face for each input non-neutral 3D face, then a facial expression descriptor is constructed which accounts for the depth changes of rectangular regions around eyes and mouth. Approaches in the second category, extract features from 3D facial scans and classify them into different expressions. In <ref type="bibr" target="#b9">[10]</ref>, the face is subdivided into regions using manually annotated landmarks, and surface curvatures and their principal directions in the regions are categorized and used to recognize different facial expressions. Comparison with results obtained on the BU-3DFED using the Gaborwavelet and the Topographic Context 2D appearance feature based methods, showed that the 3D solution outperformed the 2D methods. In <ref type="bibr" target="#b7">[8]</ref>, six Euclidean distances between some facial landmarks labeling the 3D faces of the BU-3DFED, have been selected and used to form a distance vector and train a neural network classifier. In <ref type="bibr" target="#b8">[9]</ref>, a set of normalized Euclidean distances between 83 facial landmarks of the BU-3DFED are extracted. Then, maximizing the average relative entropy of marginalized class-conditional feature distributions, just the most informative distances are retained and classified using a regularized multi-class AdaBoost algorithm.</p><p>A few recent works have shown that salient keypoints and local descriptors can be effectively used to describe 3D objects. In <ref type="bibr" target="#b4">[5]</ref>, a 3D keypoint detector and descriptor inspired to the Scale Invariant Feature Transform (SIFT) <ref type="bibr" target="#b2">[3]</ref>, has been designed and used to perform 3D face recognition. In <ref type="bibr" target="#b3">[4]</ref>, SIFT are used to detect and represent salient points in multiple 2D range images derived from 3D face models for the purpose of 3D face recognition. In 2D, SIFT descriptors have been also used to perform 2D expression recognition from non-frontal face images <ref type="bibr" target="#b11">[12]</ref>.</p><p>In this work we propose to use local face descriptors to perform 3D facial expression recognition. Differently from existing approaches, we exploit the local characteristics of the face by computing SIFT descriptors around a small set of facial landmarks identified on range images, and using them as feature vector to represent the face. Then, a feature selection approach is used to identify the subset of most features features among the set of SIFT features. The selected features are finally used to feed a set of Support Vector Machines (SVM) classifiers. Experimentation on the BU-3DFED, shows that the proposed approach ia capable to achieve state of the art results, without using neutral scans and just relying on few landmarks that, in perspective, can be automatically identified.</p><p>The rest of the paper is organized as follows: In Sect.II, the salient features of the BU-3DFED are summarized. In Sect.III, the SIFT descriptor is briefly presented with its adaptation to our case. The feature selection approach used to reduce the set of SIFT features, and the SVM based classification of the selected features are addressed in Sect.IV. Results obtained with the proposed approach and a comparative evaluation are reported in Sect.V. Finally, discussion and conclusions are given in Sect.VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. THE BU-3D FACIAL EXPRESSION DATABASE</head><p>The BU-3DFED has been recently designed to provide 3D facial scans of a large population of different subjects, each showing a set of prototypical emotional states at various levels of intensities. There are a total of 100 subjects in the database (56 female and 44 male), well distributed across different ethnic groups. During the acquisition, each subject was asked to perform the neutral facial expression as well as the six basic facial expressions, i.e., anger (AN), disgust (DI), fear (FE), happiness (HA), sadness (SA), and surprise (SU). Each facial expression has four levels of intensities (low, middle, high and highest), except the neutral facial expression that has only one intensity level (2500 3D facial expression scans in total). Each 3D face scan is also associated with a cropped 3D face mesh, and a set of 83 landmarks manually located on the eyebrows, eyes, nose, mouth and face contour. As an example, Fig. <ref type="figure" target="#fig_0">1</ref> shows the six basic facial expressions of a sample 3D face at the highest level of intensity.</p><p>an di fe ha sa su </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SIFT DESCRIPTORS OF RANGE FACIAL IMAGES</head><p>In order to capture salient features characterizing different facial expressions in 3D, we followed the idea to use local descriptors around landmarks of the face. To this end, we used the SIFT feature extraction algorithm, adapting it to our particular case. In the original formulation <ref type="bibr" target="#b2">[3]</ref>, SIFT has been defined for 2D gray-scale images and cannot be directly applied to 3D face scans. However, the 3D information of scanned faces can be captured through range images that use the gray-scale of every pixel to represent the depth of a scan.</p><p>According to this, facial landmarks located in important morphological regions of the face are used as keypoints (so by-passing the SIFT keypoint detector), and the SIFT feature extractor is run on these keypoints so as to obtain a SIFT descriptor. Briefly, a SIFT descriptor of a small image patch, for example of size 4 × 4, is computed from the gradient vector histograms of the pixels in the patch. There are 8 possible gradient directions, and therefore the total size of the SIFT descriptor is 4 × 4 × 8 = 128 elements. This descriptor is normalized to enhance invariance to changes in illumination, and transformed to ensure invariance to scale and rotation as well. These properties make the SIFT descriptor capable to provide compact and powerful local representations of the range image and, consequently, of the face surface. In the case of BU-3DFED, we performed some steps to derive the facial landmarks and to transform 3D face scans into range images. The BU-3DFED, provides 83 landmarks with every 3D face, but in a preliminary experimentation we found that SIFT descriptors extracted on many of these are not significant to discriminate between different facial expressions. So, we considered a subset including 20 landmarks on the mouth, 2 on the nose, and 5 on the face contour (27 in total). Further, we identified 85 additional landmarks on the face to be used as keypoints. These are selected by uniformly sampling the lines connecting on the face surface some fiducial points, and have the advantage to be automatically located just starting from the start and end points of the lines. The subset of BU-3DFED landmarks, and the lines to which the additional landmarks belong to are shown in Fig. <ref type="figure" target="#fig_1">2</ref>(a), whereas their number and grouping are reported in the table of Fig. <ref type="figure" target="#fig_1">2(b)</ref>.</p><p>Once the overall set of landmarks has been identified, face scans of the BU-3DFED are transformed to range images considering a frontal view of the scan. Before to extract the range images, some preprocessing was also applied to the face scans in the dataset. In particular, a sphere of radius 130mm centered on the nose tip (automatically detected and included in the landmarks set) was used to crop the 3D face.</p><p>Then, spikes in the 3D face were removed using median filtering in the z-coordinate. Holes were filled using cubic interpolation, and 3D scans were re-sampled on an uniform square grid at 0.7mm resolution. Finally, we re-mapped the landmarks identified on the 3D scans to the corresponding range images.</p><p>SIFT descriptors have been extracted using the following setting: (i) For each range image, the 27+85 landmarks have been used as keypoints where to compute SIFT descriptors; (ii) SIFT descriptors are computed at scale equal to 3, whereas the preferred SIFT orientation angle is computed; (iii) The orientation histograms of 4 × 4 sample regions of each keypoint are used to calculate the SIFT descriptor. By computing the 128-dimensional SIFT descriptor at the 27+85 sparse keypoints, a 14336-dimensional feature vector is obtained to represent each range image.</p><p>To reduce the dimensionality and improve the significativeness of the description, just the most relevant features have been selected and classified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SELECTION AND CLASSIFICATION OF RELEVANT SIFT FEATURES</head><p>Feature selection is mainly motivated by the dimensionality curse, which states that in presence of a limited number of training samples, each one represented as a feature vector in R n , the mean accuracy does not always increase with vector dimension n. Rather, the classification accuracy increases until a certain dimension of the feature vector, and then decreases. Therefore, the challenge is to identify m out of the n features which yield similar, if not better, accuracies as compared to the case in which all the n features are used in a classification task. In the proposed analysis, feature selection is performed using the minimalredundancy maximal-relevance (mRMR) model <ref type="bibr" target="#b6">[7]</ref>. For a given classification task, the aim of mRMR is to select a subset of features by taking into account the ability of features to identify the classification label "l", as well as the redundancy among the features, according to the following equation:</p><formula xml:id="formula_0">max xi∈Sn-Sm-1 I(x i , l) - 1 m -1 xj ∈Sm-1 I(x j , x i ) . (1)</formula><p>In this equation, having a subset S m-1 of m -1 features variables x i , the feature x i ∈ {S n -S m-1 } that determines a subset {x i , S m-1 } maximizing the relevance of features with the class label whereas penalizing redundancy among them, is added. In the same equation, I(., .) measures the mutual information between two discrete random variables (i.e., the difference between the Shannon's entropy of the first variable, and the conditional entropy of the first variable given the second one).</p><p>In our approach, the mRMR algorithm is applied to the set of 14336-dimensional feature vectors representing the faces v f = (f 1 , . . . , f 14336 ). Each vector is constructed by concatenating the 128-dimensional SIFT descriptors for the face landmarks, orderly from 1 to 27+85. A data discretization is applied to the vectors as preprocessing step. This is obtained by computing the mean value µ k and the standard deviation σ k for every feature f k . Then, discretized values fk are obtained according to the following rule:</p><formula xml:id="formula_1">fk =    2 if f k &lt; µ k -α • σ k 3 if µ k -α • σ k ≤ f k ≤ µ k + α • σ k 4 if f k &gt; µ k + α • σ k ,<label>(2)</label></formula><p>where the α parameter (set equal to 0.2 in our experiments) regulates the width of the discretization interval.</p><p>The overall set of discretized feature vectors is used to feed the mRMR algorithm so as to determine the features which are more relevant in discriminating between different facial expressions of 3D face scans of different subjects. The facial expression recognition problem is a multi-classification task that, in our approach, is faced as a combination of separated instances of one-vs-all classification subproblems. For each subproblem, face scans showing one expression are assumed as targets (positive examples), whereas all the other scans with any different expression are considered as negative examples. Repeatedly, the target expression is changed among the six basic expressions provided by the BU-3DFED, so that the sets of positive and negative examples change. Due to this, mRMR feature selection is performed independently for every classification subproblem. This results into different features providing the minimal-redundancy and maximal-relevance for the purpose of discriminating across different facial expressions. Then, just the most relevant features identified for every expression are retained from the original feature vectors in order to train the classifiers. In particular, in the expression recognition experiments we found optimal results using 12, 12, 16, 8, 14, and 12 features out of the 14336, respectively, for the anger, disgust, fear, happy, sad, and surprise expressions. The selected features are then used to perform facial expression recognition using a maxima rule between six one-vs-all SVM classifiers, each with a radial basis function kernel of standard deviation equal to one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head><p>Experiments on the BU-3DFED have been performed using a similar setup as in <ref type="bibr" target="#b1">[2]</ref>. In particular, since average recognition accuracies can vary from experiment to experiment, in order to permit a fair generalization and obtain stable expression recognition accuracies, we have run 100 independent experiments and averaged the results. In every experiment, 60 randomly selected subjects are considered, each with the two scans of highest-intensities expressions for each of the six basic facial expressions (i.e., 720 scans per experiment). The random selection of the subjects approximately guarantees that the person and gender independency are preserved, and a good distribution of the subjects across the various ethnic groups is achieved. In each experiment, six one-vs-all SVM classifiers, one for each expression, are trained and tested using the selected features as determined in Sect.IV, and 10-fold cross validation (1000 train and test sessions in total). Using the mRMR features, the results of 3D facial expression classification are reported in Tab.I, considering the average confusion matrix as performance measure. It can be observed that some expressions (like happiness and surprise) are recognized with very high accuracies, whereas it results more difficult to identify sadness (high confusion with anger) and fear (which is confused mainly with disgust). The overall recognition rate is equal to 77.54%.</p><p>Finally, in Tab.II the results of our approach are compared against those reported in <ref type="bibr" target="#b1">[2]</ref>. In fact, in Gong et al. <ref type="bibr">(Gong)</ref> [2], the performance of the approaches in Wang et al. <ref type="bibr">(Wang)</ref> [10], Soyel et al. <ref type="bibr">(Soyel)</ref>  <ref type="bibr" target="#b7">[8]</ref>, and Tang et al. <ref type="bibr">(Tang)</ref>  <ref type="bibr" target="#b8">[9]</ref>, are obtained on a same experimental setting. The set up used in this work is more challenging in that, differently from <ref type="bibr" target="#b1">[2]</ref> where 60 subjects were selected, in our case the 60 selected subjects varied randomly from experiment to experiment. In particular, it can be observed that our approach outperforms other solutions, with larger differences with respect to works that do not use neutral scans, like <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b9">[10]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this paper, we investigate the problem of person independent facial expression recognition from 3D facial scans. We propose an original feature selection approach based on minimizing the redundancy between features, maximizing, at the same time, their relevance in terms of mutual information, and apply it to SIFT descriptors computed on a set of facial landmarks given on 3D face scans. Using a multi-class SVM classification, and a large set of experiments on the publicly available BU-3DFED, an average facial expression recognition rate at the state of the art is obtained.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Basic facial expressions of a sample face (highest level of intensity).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. (a) The subset of the BU-3DFED landmarks, and the lines along which the 85 additional landmarks are located; (b) Number of landmarks for every face component they belong to.</figDesc><graphic coords="2,318.95,258.11,78.34,110.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table I AVERAGE</head><label>I</label><figDesc>CONFUSION MATRIX.    </figDesc><table><row><cell></cell><cell>An</cell><cell>Di</cell><cell>Fe</cell><cell>Ha</cell><cell>Sa</cell><cell>Su</cell></row><row><cell>An</cell><cell>81.7%</cell><cell>0.9%</cell><cell>3.3%</cell><cell>4.2%</cell><cell>8.1%</cell><cell>1.7%</cell></row><row><cell>Di</cell><cell>3.3%</cell><cell>73.6%</cell><cell>2.6%</cell><cell>7.8%</cell><cell>0.0%</cell><cell>12.6%</cell></row><row><cell>Fe</cell><cell>2.6%</cell><cell>14.5%</cell><cell>63.6%</cell><cell>9.2%</cell><cell>0.8%</cell><cell>9.2%</cell></row><row><cell>Ha</cell><cell>0.9%</cell><cell>4.5%</cell><cell>6.9%</cell><cell>86.9%</cell><cell>0.8%</cell><cell>0.0%</cell></row><row><cell>Sa</cell><cell>30.1%</cell><cell>0.0%</cell><cell>0.0%</cell><cell>3.4%</cell><cell>64.6%</cell><cell>1.8%</cell></row><row><cell>Su</cell><cell>1.8%</cell><cell>1.7%</cell><cell>1.7%</cell><cell>0.0%</cell><cell>0.0%</cell><cell>94.8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table II AVERAGE</head><label>II</label><figDesc>(AVG) EXPRESSION RECOGNITION RATES FOR OUR APPROACH AND THE WORKS IN<ref type="bibr" target="#b1">[2]</ref>,<ref type="bibr" target="#b9">[10]</ref>,<ref type="bibr" target="#b7">[8]</ref>,<ref type="bibr" target="#b8">[9]</ref>.</figDesc><table><row><cell></cell><cell>This work</cell><cell>Gong</cell><cell>Wang</cell><cell>Soyel</cell><cell>Tang</cell></row><row><cell>AVG</cell><cell>77.54%</cell><cell>76.22%</cell><cell>61.79%</cell><cell>67.52%</cell><cell>74.51%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank the region Nord-Pas de Calais, France, for a visiting Professorship to Stefano Berretti under the program Ambient Intelligence. This research was also supported partially by the project FAR3D ANR-07-SESU-004.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Universals and cultural differences in facial expressions of emotion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nebraska Symposium on Motivation</title>
		<meeting><address><addrLine>Lincoln, NE</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1972">1972</date>
			<biblScope unit="page" from="207" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic facial expression recognition on a single 3d face by exploring shape deformation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Multimedia</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-10">Oct. 2009</date>
			<biblScope unit="page" from="569" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant key points</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004-11">Nov. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3d face recognition using multiview key point matching</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Advanced Video and Signal Based Surveillance</title>
		<meeting><address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09">Sept. 2009</date>
			<biblScope unit="page" from="290" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Keypoint detection and local feature matching for textured 3d face recognition. Int</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2008-08">Aug. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bilinear models for 3-d face and facial expression recognition</title>
		<author>
			<persName><forename type="first">I</forename><surname>Mpiperis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Malassiotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Strintzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="498" to="511" />
			<date type="published" when="2008-09">Sept. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Feature selection based on mutual information: Criteria of max-dependency, maxrelevance and min-redundancy</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1226" to="1238" />
			<date type="published" when="2005-08">Aug. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Facial expression recognition using 3d facial feature distances</title>
		<author>
			<persName><forename type="first">H</forename><surname>Soyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Demirel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Image Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2007-08">Aug. 2007</date>
			<biblScope unit="page" from="831" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3d facial expression recognition based on automatically selected features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Anchorage, AK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06">June 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3d facial expression recognition based on primitive surface feature distribution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006-06">June 2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1399" to="1406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A 3d facial expression database for facial behavior research</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rosato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Automatic Face and Gesture Recognition</title>
		<meeting><address><addrLine>Southampton, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-04">Apr. 2006</date>
			<biblScope unit="page" from="211" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A novel approach to expression recognition from non-frontal face images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision</title>
		<meeting>IEEE Int. Conf. on Computer Vision<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09">Sept. 2009</date>
			<biblScope unit="page" from="1901" to="1908" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
