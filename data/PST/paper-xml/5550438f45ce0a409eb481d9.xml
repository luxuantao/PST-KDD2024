<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual query expansion with or without geometry: Refining local descriptors by feature aggregation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">INRIA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">INRIA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Visual query expansion with or without geometry: Refining local descriptors by feature aggregation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E79D472A38799759AB23C67EE03C630B</idno>
					<idno type="DOI">10.1016/j.patcog.2014.04.007</idno>
					<note type="submission">Received 3 October 2013 Received in revised form 26 March 2014 Accepted 2 April 2014</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Image retrieval Query expansion Hamming embedding</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a query expansion technique for image search that is faster and more precise than the existing ones. An enriched representation of the query is obtained by exploiting the binary representation offered by the Hamming Embedding image matching approach: the initial local descriptors are refined by aggregating those of the database, while new descriptors are produced from the images that are deemed relevant.</p><p>The technique has two computational advantages over other query expansion techniques. First, the size of the enriched representation is comparable to that of the initial query. Second, the technique is effective even without using any geometry, in which case searching a database comprising 105k images typically takes 79 ms on a desktop machine. Overall, our technique significantly outperforms the visual query expansion state of the art on popular benchmarks. It is also the first query expansion technique shown effective on the UKB benchmark, which has few relevant images per query.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper considers the problem of image and object retrieval in image databases comprising up to millions of images. The goal is to retrieve the images describing the same visual object(s) as the query. In many applications, the query image is submitted by a user and must be processed in interactive time.</p><p>Most of the state-of-the-art approaches derive from the seminal Video-Google technique <ref type="bibr" target="#b0">[1]</ref>. It describes an image by a bag-ofvisual-words (BOVW) representation, in the spirit of the bag-ofwords frequency histograms used in text information retrieval. This approach benefits from both the powerful local descriptors <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> such as the SIFT, and from indexing techniques inherited from text information retrieval such as inverted files <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. Exploiting the sparsity of the representation, BOVW is especially effective for large visual vocabularies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>This analogy with text representation is a long-lasting source of inspiration in visual matching systems, and many image search techniques based on BOVW have their counterparts in text retrieval. For instance, some statistical phenomenons such as burstiness or co-occurrences appear both in texts <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> and images <ref type="bibr" target="#b9">[10]</ref><ref type="bibr">[11]</ref><ref type="bibr" target="#b11">[12]</ref> and are addressed in similar ways.</p><p>One of the most successful techniques in information retrieval is the query expansion (QE) principle <ref type="bibr" target="#b12">[13]</ref>, which is a kind of automatic relevance feedback. The general idea is to exploit the reliable results returned by an initial query to produce an enriched representation, which is re-submitted in turn to the search engine. If the initial set of results is large and accurate enough, the new query retrieves some additional relevant elements that were not present in the first set of results, which dramatically increases the recall.</p><p>Query expansion has been introduced to the visual domain by Chum et al. <ref type="bibr" target="#b13">[14]</ref>, who proposed a technique implementing the QE principle and specifically adapted to visual search. Several extensions have been proposed to improve this initial QE scheme <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>. Although these variants have improved the accuracy, they suffer from two inherent drawbacks which severely affect the overall complexity and quality of the search: First, they require a costly geometrical verification step, which provides the automatic annotation of the relevant set and is typically performed on hundreds of images.</p><p>Second, the augmented query representation contains signifi- cantly more non-zero components than the original one, which severely slows down the search. It is reported <ref type="bibr" target="#b16">[17]</ref> that typically ten times more components are non-zeros. Since querying the inverted file has linear complexity in the number of features contained in the query vector, the second query is therefore one order of magnitude slower than the first.</p><p>Expansion methods that do not use any costly geometrical verification are typically based on an off-line stage with quadratic Contents lists available at ScienceDirect journal homepage: www.elsevier.com/locate/pr complexity in the number of database images <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>. They are thus limited to collections of small and fixed size.</p><p>In another line of research, several techniques address the loss in quantization underpinning BOVW, such as the use of multiple assignment <ref type="bibr" target="#b20">[21]</ref> or soft quantization <ref type="bibr" target="#b21">[22]</ref>. In a complementary manner, the Hamming Embedding (HE) technique <ref type="bibr" target="#b22">[23]</ref> dramatically improves the matching quality by refining the descriptors with binary signatures. HE is not compatible with existing QE techniques because these assume a vector representation of the images. A noticeable exception is the transitive QE, which does not explicitly exploit the underlying image representation. However, this variant is not satisfactory with respect to query time and performance.</p><p>This paper, for the first time, proposes a novel way to exploit the QE principle in a system that individually matches the local descriptors, namely the HE technique. The new query expansion technique is both efficient and precise, thanks to the following two contributions:</p><p>First, we modify the selection rule for the set of relevant images so that it does not involve any spatial verification. The images deemed relevant provide additional descriptors that are employed to improve the original query representation. Unlike other QE methods, it is done on a per-descriptor basis and not on the global BOVW vector. Fig. <ref type="figure">1</ref> depicts an example of images and features that are selected by our method to refine the original query.</p><p>The second key property of our method is that the set of local features is aggregated to produce new binary vectors defining the new query image representation. This step drastically reduces the number of individual features to be considered when submitting the enriched query.</p><p>To our knowledge, it is the first time that a visual QE is successful without any geometrical information: the only visual QE technique <ref type="bibr" target="#b13">[14]</ref> that we are aware of performs poorly compared with other variants such as the average query expansion (AQE). In contrast, our technique used without geometry reaches or outperforms the state of the art. Interestingly, it is effective even when a query has few corresponding images in the database, as shown by our results on the UKB image recognition benchmark <ref type="bibr" target="#b5">[6]</ref>. Incorporating geometrical information in the pipeline further improves the accuracy. As a result, we report a large improvement compared to the state of the art. We further demonstrate the superiority of our method compared to a simple combination of HE with QE: the property of feature aggregation not only reduces the expanded query complexity, but further improves performance.</p><p>The paper is organized as follows. Section 3 introduces our core image system and Section 7 a post-processing technique for SIFT descriptors that is shown useful to improve the efficiency of the search. Section 4 introduces our Hamming Query Expansion (HQE) method and Section 5 describes our key aggregation strategy of local features. Section 6 describes how to exploit geometrical information with HQE. The experimental results presented in Section 8 demonstrate the superiority of our approach over concurrent visual QE approaches, with respect to both complexity and search quality, on the Oxford5k, Oxford105k and Paris benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Chum et al. <ref type="bibr" target="#b13">[14]</ref> were the first to translate the query expansion principle to the visual domain. Most of the variants they propose rely on a spatial verification method, which filters out the images that are not geometrically consistent with the query. The authors investigate several methods to build a new query from the images deemed relevant. The average query expansion is of particular interest and usually considered as a baseline, as it is the most efficient variant <ref type="bibr" target="#b13">[14]</ref> and provides excellent results. It is conceptually simple: a new term-frequency inverse document frequency (TFIDF) vector is obtained as the average of the results assumed correct and spatially back-projected to the original image.</p><p>Following this first work, a number of QE variants and extensions have been proposed <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>. Using incremental spatial reranking, the query representation is updated by each spatially verified image and extended out of the initial query region <ref type="bibr" target="#b15">[16]</ref>. Another extension is to learn, on-the-fly, a discriminative linear classifier <ref type="bibr" target="#b16">[17]</ref> to define the new query instead of the average in AQE.</p><p>Other kinds of expansion have been proposed for fixed image collections <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24]</ref>. They rely on the off-line pairwise matching of all image pairs and aim at identifying the features coming from the same object using spatial verification, which is rather costly as the complexity is quadratic in the number of images. They also assume that the image collection is fixed: the selection depends on a given set of images. These methods are also related to other methods exploiting the neighborhood of the images within a given collection <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b17">18]</ref>, in particular by updating the comparison metric or by employing reciprocal nearest neighbors as a filtering rule. For instance, Qin et al. <ref type="bibr" target="#b17">[18]</ref> construct a graph that links related images, and uses k-reciprocal nearest neighbors at query time to define a new similarity function that re-orders the images. Again, the cost of constructing and storing the graph in memory is impracticable for large datasets. In a similar spirit, Shen et al. <ref type="bibr" target="#b18">[19]</ref> exploit the Fig. <ref type="figure">1</ref>. Query image (left) and the features selected (yellowþcyan) from the retrieved images to refine the original query. The features in red are discarded. Cyan features correspond to visual words that appear in the query image, and yellow ones to visual words that were not present in it. The selection of the depicted images and features has not involved any geometrical information. (For interpretation of the references to color in this figure caption, the reader is referred to the web version of this article.) ranked lists of independent queries issued with top-ranked images. Query time increases significantly as it is linear to the number of those queries. A recent graph-based method combines multiple similarity measures to perform re-ranking <ref type="bibr" target="#b19">[20]</ref>. The cost of such an offline procedure can be undertaken only for small collections. In the work of Chum and Matas <ref type="bibr" target="#b24">[25]</ref>, the quadratic cost is addressed by starting from seed query images, yet their method requires a costly spatial verification stage.</p><p>The query expansion method of Kuo et al. <ref type="bibr" target="#b25">[26]</ref> is also related to our work. They also use a set of binary vectors for an image representation and try to identify database image regions which are similar to the query. The initial representation is not enhanced, but new independent queries are rather issued and a final fusion is performed on the ranked lists. Li et al. <ref type="bibr" target="#b26">[27]</ref> straightforwardly use binary descriptors but only to select the relevant matches. As in other QE methods, their method relies on geometry and produces a larger set of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The core image system</head><p>This section describes the image search system based on Hamming Embedding upon which our query expansion techniques are built. This baseline method follows the guideline of the existing HE technique <ref type="bibr" target="#b22">[23]</ref>, which proceeds as follows. An image is represented by a set P of local SIFT descriptors <ref type="bibr" target="#b2">[3]</ref> extracted with the Hessian-Affine detector <ref type="bibr" target="#b27">[28]</ref>.</p><p>BOVW and Hamming Embedding: The descriptors are quantized using a flat k-means quantizer, where k determines the visual vocabulary size. A descriptor p A P is then represented by a quantization index, called a visual word v(p). Computing and normalizing the histogram of visual words produces the BOVW representation. It can also be seen as a voting system in which all descriptors assigned to a specific visual word are considered as matching with a weight related to the inverse document frequency <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>In order to refine the quality of the matches and to provide more reliable weights to the votes, the HE technique <ref type="bibr" target="#b22">[23]</ref> further refines each descriptor p by a binary signature b(p), providing a better localization of the descriptor by subdividing the quantization cell v(p). HE compares two local descriptors q and p that are assigned to the same visual word vðpÞ ¼ vðqÞ by computing the Hamming distance hðq; pÞ ¼ J bðqÞÀbðpÞ J 1 between their binary signatures. If the Hamming distance is above a predefined threshold h t , the descriptors are considered as non-matching and zero similarity is attached. A significant benefit <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b9">10]</ref> in accuracy is obtained by weighting the vote as a decreasing function of the Hamming distance. In this paper, we adopt the Gaussian function used in <ref type="bibr" target="#b9">[10]</ref> with s equal to one fourth of the binary signature size.</p><p>The burstiness phenomenon in images was first revealed and tackled by Jégou et al. <ref type="bibr" target="#b9">[10]</ref>. It takes into account descriptors that individually trigger multiple matches between specific pairs of images, which is often the case because of repetitive structures, or features which are abnormally common across all database images. Several normalizations have been proposed, from which we adopt the one that down-weights a given match score by the square root of the number of matches associated with the corresponding query descriptor <ref type="bibr" target="#b9">[10]</ref>. This strategy is similar to the successful component-wise power-law normalization later proposed for BOVW or Fisher Kernels <ref type="bibr" target="#b28">[29]</ref>, but here applied to a voting technique.</p><p>Multiple assignment (MA): BOVW and HE handle descriptors assigned to the same visual word. However quantization losses are introduced when truly matching descriptors are assigned to different visual words. This has been addressed by assigning multiple visual words to each descriptor <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. We apply MA on the query side only in order to keep memory requirements unchanged <ref type="bibr" target="#b22">[23]</ref>. In the rest of the paper, the initial method that assigns a descriptor a single visual word is denoted by SA (single assignment) to distinguish it with MA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">HE with query expansion</head><p>This section defines a query expansion technique based on HE and not involving any geometrical information. We revisit the different stages involved in the QE principle. We first describe how reliable images are selected from the initial result set. Then we detail the way an enriched query is produced from the images deemed relevant. The key subsequent aggregation step and the use of geometry will be introduced later in Sections 5 and 6, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Selection of reliable images</head><p>As in all query expansion methods <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>, the core image search system processes an initial query. The resulting set is analyzed to identify a subset of reliable images that are likely to depict the query object, and therefore to provide additional features that will be subsequently exploited in the augmentation stage. In the following, we will denote the local features of the query image by Q, and those of a given database image by P. As a criterion to determine the relevant images, we count the number CðQ; PÞ of "strict" feature correspondences between the query and images in the short-list. It is given by CðQ; PÞ ¼ jfðq; pÞ A Q Â P : hðq; pÞ r h</p><formula xml:id="formula_0">⋆ t gj;<label>ð1Þ</label></formula><p>where the threshold h t ⋆ is lower than the Hamming embedding threshold h t used for initial ranking. Such a lower threshold allows for a higher true positive to false positive ratio of matches <ref type="bibr" target="#b22">[23]</ref>. It provides a strict way to count correspondences in a manner that resembles the number of RANSAC inliers commonly used to verify the images <ref type="bibr" target="#b6">[7]</ref>. It is less precise than RANSAC, yet it has the advantage of not using any geometry. It is therefore much faster. Fig. <ref type="figure" target="#fig_0">2</ref> illustrates, for a pair of images, the matching features obtained using BOVW and HE. We consider two different thresholds for HE to show the impact of the strict threshold h ⋆ t ¼ 16 on selected features. Observe that HE matching filters out many false matches compared to BOVW. With a lower threshold value, the filtering is not far in quality from that of a spatial matching method.</p><p>An image is declared reliable if at least c t correspondences are satisfied, which formally leads to define the set of reliable images as</p><formula xml:id="formula_1">L Q ¼ fP : CðQ; PÞ Zc t g:<label>ð2Þ</label></formula><p>In practice, only the images short-listed in the initial search are considered as candidates for the set of reliable images. In our experiments, we count the number of correspondences with Eq.</p><p>(1) only for the top 100 images. Fig. <ref type="figure" target="#fig_1">3</ref> shows examples of queries and the corresponding reliable images. Although some negative images are selected and some positive ones are not, the result is not far from what spatial verification would produce. This suggests that selecting reliable images with HE and a low threshold is sufficient for the purpose of QE, as proposed in this section. The proposed procedure for detecting reliable images gives a rate of 92.4% true positive instances in L Q . Note that this is achieved without any geometry information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Feature set expansion</head><p>First, let us recall that a feature descriptor is described by both a visual word and a binary signature. Our augmentation strategy, i. e., how we introduce new local features in the representation, is partly based on the selection of visual words that are not present in the original query.</p><p>Since a large proportion of the reliable images depicts the same object, the visual words frequently occurring in the images of the reliable set L Q are likely to depict the query object rather than the background. Our selection strategy is simple and consists in selecting the most frequent visual words occurring in L Q . More precisely, we sort the visual words contained in the images of L Q by the number of reliable images in which they appear. The top ranked words are selected and define the set of reliable visual words V, which may include both visual words that are present or absent in the query image. The latter are referred to as the augmented visual words. Their count is controlled by a parameter α to ensure that the number of reliable visual words in the new query is proportional to that of the original query, as</p><formula xml:id="formula_2">jV\V Q j ¼ α Á jV Q j;<label>ð3Þ</label></formula><p>where V Q is the set of visual words occurring in the query.</p><p>A typical value of parameter α is 0.5 (see Section 8). The initial query set is enriched with the features of the reliable images assigned to the reliable visual words. Define as</p><formula xml:id="formula_3">G ¼ fp A P : P A L Q 4 vðpÞ A Vg ð<label>4Þ</label></formula><p>the union of all features of reliable images assigned to some reliable words. It defines the set of database features used to augment the initial query. In other terms, this set is merged with the initial query feature set to construct the augmented query as</p><formula xml:id="formula_4">Q E ¼ Q [ G:<label>ð5Þ</label></formula><p>Fig. <ref type="figure" target="#fig_2">4</ref> depicts some features from reliable images assigned to reliable visual words. Observe that, even without any spatial information, selected visual words are detected on the foreground object. Moreover, each visual word corresponds not only to similar image patches, but often to the exact same patch of the object, as if spatial matching was used. This appears to be the case for either visual words which appear (top) or miss (bottom) in the query.</p><p>A simple way to construct an enriched query is to use the expanded set of features as the new image representation. However, similar to the existing QE strategies, such an approach leads to a high complexity because the number of features explodes. We observe that it is typically multiplied by a factor ranging from 10 to 20 for typical values of α, as analyzed in Section 8. This drawback is shared by other effective techniques on query expansion <ref type="bibr" target="#b16">[17]</ref>, for which this problem leads to produce a BOVW vector having 10 times more non-zero components than the initial one. In the next section, we address this issue by proposing an aggregation strategy that drastically reduces the number of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">QE with feature aggregation</head><p>The average query expansion technique <ref type="bibr" target="#b13">[14]</ref> averages BOVW vectors to produce the new query. In this section, we explain how local descriptors are individually refined or created from binary signatures of the set of reliable features. At this stage, the augmented set contains multiple instances representing the same visual patches, either in the initial query or not. Descriptors associated to the same patch are expected to have similar binary signatures. The strategy presented below implicitly exploits this underlying property to produce the new set of query descriptors which is less redundant.</p><p>First, note that the selection strategies for images and features presented in the previous subsections introduce a few false positives in the augmented feature set. This is the cost to pay for not performing the selection with a stringent spatial matching technique: our inliers are not selected as reliably as in other query expansion methods. The aggregation operation proposed hereafter comes as a complement on our selection method, as it is robust enough to false positives. In contrast, averaging over normalized TFIDF vectors of similar images <ref type="bibr" target="#b13">[14]</ref>, as done in AQE, is sensitive to background and noisy features.</p><p>Our aggregation scheme is inspired by methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> such as the VLAD technique, which aggregates the descriptors per visual word to produce a vector representation of an image. In our method, we aggregate the features of Q E that are assigned to the same visual word. Therefore, our technique produces exactly one binary signature per visual word occurring in Q E . Our motivation is that the true matching patches are likely to overrule the false positives. This actually happens in practice because the true correspondences are more numerous and are associated with more consistent binary signatures.</p><p>Our aggregation scheme is related to the recent work of Tolias et al. <ref type="bibr" target="#b31">[32]</ref>, where descriptors are aggregated per visual word for query and database images individually. A selectivity function is employed to appropriately weight the similarity scores. Our approach differs in that we rather aggregate descriptors collected from many images instead of a single one. In their work, aggregation consistently improves the performance in all cases. It is attributed to the way burstiness is handled. As a result, the voting scheme ensures that at most one correspondence is established for each visual word, and therefore at most one for each descriptor.</p><p>For each visual word v appearing in Q E , a new binary signature b(v) is obtained by computing the median values over all the bit vectors occurring in Q E and assigned to v. If the numbers of 0 and 1 are equal for a particular bit component, the tie is arbitrarily resolved by assigning either 0 or 1 with equal probabilities. This new set of descriptors comprises exactly one binary signature per visual word and serves as the new query, which is then submitted to the system.</p><p>In the remainder of this paper, we refer to the method described in this section as Hamming Query Expansion (HQE).</p><p>Remark. HQE differs from a simple combination of HE with QE. Firstly, our QE scheme is the first not to use any geometrical information in order to identify relevant images. Secondly, only the most frequent visual words appearing among relevant images are collected, avoiding the inclusion of false matches to the expanded query. Finally, the proposed feature aggregation, in addition to drastically reduce the expanded query size, further improves the performance, as shown in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Geometrical information</head><p>This section proposes a variant of our method to further eliminate some incorrect matches by including some spatial information in the loop. For this reason and as shown later in the experimental section, it is not as fast as the HQE strategy proposed in Sections 4 and 5. However, this approach further improves the performance and is therefore interesting in some situations where one would trade an interactive time against any improvement in accuracy.</p><p>It proceeds as follows. The matches are collected with the regular HE technique, i.e., they are returned by the first query. Instead of calculating the number of correspondences with Eq. ( <ref type="formula" target="#formula_0">1</ref>), we rely on the number C g ðQ; PÞ of inliers found with a spatial matching technique. For this purpose, we have used the spatial verification procedure proposed by Philbin et al. <ref type="bibr" target="#b6">[7]</ref>. Similar to other QE techniques, this procedure is applied on the top ranked images only. An image is declared reliable if the number of inliers is above a pre-defined threshold. The estimation of the affine transformation is then further exploited to filter the expanded feature set. As first suggested by Chum et al. <ref type="bibr" target="#b13">[14]</ref>, the matching features associated with the reliable images are projected back to the query image plane. Those falling out of the query image borders are filtered out.</p><p>The remaining steps of this variant then become similar to the HQE method of Sections 4 and 5. The only difference is that the input set of reliable features is different. Therefore, we first select the reliable visual words and perform the feature set expansion. The aggregation is similarly applied to produce one binary vector per visual word. Note that, the reliable images, as detected by spatial matching, are ranked in top positions.</p><p>Fig. <ref type="figure" target="#fig_3">5</ref> depicts the descriptors selected for the HQE expanded set with and without geometry. Notice that even without geometry, most of the selected features are localized on the target object. The geometry effectively filters out the remaining features that do not lie on the query object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Implementation details</head><p>In this section we introduce a new post-processing stage for SIFT descriptors, whose interest is evaluated in different setups.</p><p>Root-SIFT: It was recently shown <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33]</ref> that square rooting the components of SIFT descriptors improves the search performance. This is done either by L 1 -normalizing the SIFT descriptor <ref type="bibr" target="#b16">[17]</ref> prior to the square-rooting operation or, equivalently, by <ref type="bibr" target="#b32">[33]</ref> squarerooting the components and normalizing the resulting vector in turn with respect to L 2 . This operation amounts to computing the Table <ref type="table">1</ref> Evaluation with respect to mAP (performance) and IF (efficiency) of several postprocessing procedures for SIFT: RootSIFT <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33]</ref> (denoted by ffi p ) and shifting (denoted by -μ). Post-processed descriptors are used to create the codebook, perform assignment to visual words and create the binary signatures used in HE. We have performed 10 independent experiments on Oxford5k with distinct vocabularies (k ¼ 16k) to report mean performance and standard deviations. Hellinger distance instead of the Euclidean one. The impact of this scheme is evaluated in Table <ref type="table">1</ref> on the Oxford5k building benchmark <ref type="bibr" target="#b6">[7]</ref> for both BOVW and HE, without the burstiness processing. Following the standard evaluation protocol, we measure the mean average precision (mAP). In order to cope with the variability of the results due to the sensitivity of k-means to the initial random seeds, we average the results over 10 runs with different vocabularies and report the standard deviation. We observe an improvement provided by square-rooting the components, which is statistically significant when used with HE. However, as a side-effect of this processing, we observe that Root-SIFT introduces an unexpected complexity overhead, resulting from less balanced inverted lists. The undesirable impact of uneven inverted lists was first noticed by Nister and Stewénius <ref type="bibr" target="#b5">[6]</ref> and is commonly measured by the imbalance factor (IF) <ref type="bibr" target="#b22">[23]</ref>, which is a multiplicative factor reflecting the deviation from perfectly balanced lists. For instance, IF¼2 means that, on average, two times more individual descriptor comparisons are performed compared to the case where the lists have equal lengths. Table <ref type="table">1</ref> shows that this negative effect, which was not reported for this RootSIFT variant <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33]</ref>, is statistically significant.</p><p>Shift-SIFT: In order to reduce this undesirable effect, we introduce another processing method for SIFT descriptors referred to as shift-SIFT. It is inspired by the approach proposed for BOVW vectors <ref type="bibr" target="#b33">[34]</ref>, which aims at handling "negative evidences" by centering the descriptors and L 2 normalizing them in turn. It gives more importance in the comparison to the components which are close to 0, and improves the performance in the case of BOVW vectors.</p><p>Table <ref type="table">1</ref> shows the interest of this shifting strategy applied to SIFT descriptors. We have used SA and no burstiness normalization in this experiment (conclusions are similar in other setups). Performance is mainly unaffected. Yet, this approach provides more balanced lists and therefore reduces the search complexity by about 4% at no cost, as reflected by the IF measure. We use this shifting strategy combined with L 2 Root-SIFT <ref type="bibr" target="#b32">[33]</ref> in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Experiments</head><p>This experimental section first introduces the datasets, gives details about the experimental setup, and evaluates the impact of the parameters and variants. Our technique is then compared with the state of the art on visual query expansion before a discussion on the complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">Datasets and experimental setup</head><p>Datasets and measures: Query expansion techniques are only effective if the dataset consists of several relevant images for a given query. We evaluate the proposed method on two publicly available datasets of this kind, namely Oxford5k Buildings <ref type="bibr" target="#b6">[7]</ref> and Paris <ref type="bibr" target="#b21">[22]</ref>, but also on a dataset where queries have only few corresponding images, that is UKB <ref type="bibr" target="#b5">[6]</ref>. Following the standard evaluation protocols, we report mean Average Precision (mAP) for the two first and use the score definition associated with UKB: the average number of correct images ranked in first 4 positions (from 0 to 4). As for other QE works, the large scale experiments are carried out on the Oxford105k dataset, which augments Oxford5k with 100k additional distractor images.</p><p>Features and experimental setup: For Oxford5k and Paris, we used the modified Hessian-Affine detector proposed by Perdoch et al. <ref type="bibr" target="#b34">[35]</ref> to detect local features. The extracted SIFT descriptors have been subsequently post-processed by using the L 2 Root-SIFT and shift-SIFT procedure, as described in Section 3. For UKB, we have used the same features provided by the authors of the papers <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b17">18]</ref>. We follow the more realistic, less biased approach, of learning the vocabulary on an independent dataset. That is, when we use Oxford5k for evaluation, the vocabulary is learned with features of Paris and vice versa. Similarly, learning the medians of Hamming Embedding is carried out on the independent dataset.</p><p>Unless otherwise stated, we use a visual vocabulary comprising k¼ 65,536 visual words, binary signatures of 64 dimensions, and apply HE with weights and burstiness normalization. In all our experiments, the reliable images for our approach, either without or with spatial matching, are selected among top 100 ones returned by the baseline system. When using MA, it is applied on the query side using the 3 nearest visual words to limit the computational overhead of using more.</p><p>MA produces more correspondences than single assignment (SA), therefore the probability of finding a false positive match is increased even with spatial matching and the matching parameters should be stricter <ref type="bibr" target="#b22">[23]</ref>. We set the minimum number of correspondences to c t ¼ 4 with SA and to c t ¼ 5 with MA.</p><p>Two factors introduce some randomness in the measure with our approach: the random projection matrix (in HE) and the random decision used to resolve ties when aggregating binary signatures. Therefore, each experiment is performed 5 times using distinct and independent parameters. We report the average performance and standard deviation to assess the significance of our measurements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">Impact of the parameters</head><p>Thresholds: The strict threshold h ⋆ t is constant and set to 16 in all our experiments. Fig. <ref type="figure">6</ref> shows the impact of the parameter h t . The performance is not very sensitive around the optimal values attained at h t ¼ 22 or h t ¼ 24, depending on the setup. Note Fig. <ref type="figure">6</ref>. Impact of h t on the performance of HE and HQE.</p><p>already that HQE gives a significant improvement compared to the HE baseline. In the rest of our experiments, we set h t ¼ 24 in all cases, similar to most works based on HE. We have fixed α ¼ 0:5 for this preliminary experiment, which implies that the size of the new query is at most 1.5 times larger than the initial one. In practice, it is much smaller thanks to the descriptors aggregation. See, for instance, Table <ref type="table" target="#tab_1">3</ref> to compare the average number of descriptors used in the original and augmented queries.</p><p>The parameter α (see Section 4) controls the size of the augmented query. Fig. <ref type="figure" target="#fig_5">7</ref> presents its impact on the performance. HQE without spatial matching rapidly attains its maximum in performance and then decreases. This suggests that not too many visual words should be selected because the additional ones will introduce many outliers compared to inliers. In contrast, spatial matching filters out most of the outliers: using more descriptors is better because the added ones are inliers in their majority. As a compromise between performance and complexity, we set α ¼ 0:5 and 1.0 without and with spatial matching, respectively.</p><p>The vocabulary size: k is critical in most methods based on pure BOVW. Fig. <ref type="figure" target="#fig_6">8</ref> shows that it is not the case with HE and our techniques, which achieves excellent performance for all the sizes. This confirms observations in prior work <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>Weights, burstiness and HQE: Table <ref type="table" target="#tab_0">2</ref> summarizes the respective contributions of the different elements of our search engine. First note the large gain already achieved by weighting the Hamming distances in HE, using MA and applying the burstiness procedure <ref type="bibr" target="#b9">[10]</ref>. Note also the even larger improvement obtained by using our HQE technique, either with or without spatial matching.</p><p>Aggregation: Table <ref type="table" target="#tab_1">3</ref> reveals the double benefit of the local feature aggregation method proposed in Section 5 with respect to performance and query efficiency. Merging the binary signatures   reduces the expanded query size and has a positive impact on complexity, as quantitatively measured in Table <ref type="table" target="#tab_1">3</ref>: the aggregation step reduces by about one order of magnitude the size of the enriched query, which becomes comparable to that of the initial query.</p><p>In addition, Table <ref type="table" target="#tab_1">3</ref> also shows that this step significantly and consistently improves the performance. To demonstrate this, we have compared HQE (i.e., with aggregation) to a method which issues the expanded query defined by Eq. ( <ref type="formula" target="#formula_4">5</ref>), i.e., prior to aggregation. As already discussed in Section 5, our interpretation is that aggregating binary signatures filters out noisy features and removes the redundant features at the same time. Merging the features derived from the reliable images can also give rise to multiple matches per descriptor, yet those are effectively handled by aggregation <ref type="bibr" target="#b31">[32]</ref>.</p><p>Detailed performance on Oxford5k: Table <ref type="table" target="#tab_2">4</ref> presents some detailed performances and statistics we have collected on Oxford5k for HE and HQE. Our selection strategy for reliable images, even without spatial matching, does not suffer from the variability of the number of true similar images in the database, with an exception on Cornmarket, where HQE without spatial matching selects a few false positives as reliable images. Also observe that HQE notably outperforms HQE-SP for the Bodleian queries. It is because HQE-SP is stricter and does not select enough reliable images. This suggests that a weaker spatial matching model <ref type="bibr" target="#b36">[37]</ref> could offer a good compromise to select these images.</p><p>More features: All our experiments are conducted with features extracted using the default threshold for the Hessian-Affine detector <ref type="bibr" target="#b34">[35]</ref> to allow for a direct comparison with the literature. Using a lower threshold for the "cornerness" value produces a larger set of features. It might be useful for image matching but might also add noisy features and therefore arbitrary matches.</p><p>Table <ref type="table" target="#tab_3">5</ref> investigates the impact of cornerness on both our methods and existing BOVW and HE baselines. With the default threshold, the software produces a total number of 12.53M features on Oxford5k. By using two smaller thresholds, we produced two other sets of features comprising 21.92M and 27.59M features. Table <ref type="table" target="#tab_3">5</ref> shows that BOVW's performance increases with the medium-sized set, but its performance drops with the larger one. In contrast, HE benefits from having more features. The performance of the two larger sets is comparable, which suggests that HE better handles noisy matches in a better way and can use more features. As a consequence, HQE performs in a similar way. The performance increases up to mAP ¼89.4 for HQE with geometry and MA, which is a large improvement over the state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.">Comparison with the state of the art</head><p>Oxford5k, Paris6k and Oxford105k: Table <ref type="table" target="#tab_4">6</ref> compares the QE proposed method with previously published results on the same datasets. For a fair comparison, we have included the scores of QE methods that use the same local feature detector <ref type="bibr" target="#b34">[35]</ref> as input and learned the vocabulary on an independent dataset. In this table, we also include the scores for our method when using 128-bit signatures for HE, which are better at the cost of higher memory usage and a slightly larger complexity.</p><p>Interestingly, even without spatial matching, our method outperforms all methods in Oxford105k and Paris6k dataset. HQE-SP outperforms them in all three datasets. All of the compared methods rely on spatial matching to verify similar images and expand the initial query. Moreover, the work of Mikulik et al. <ref type="bibr" target="#b37">[38]</ref> requires a costly off-line phase and assigns the descriptors with a very large vocabulary of 16 M, thereby impacting the overall efficiency.</p><p>To our knowledge, the performance of our method is the best reported to date on Oxford5k, Paris6k and Oxford105k, when learning the vocabulary on an independent dataset (89.1 was reported <ref type="bibr" target="#b16">[17]</ref> by learning it on the Oxford5k comprising the relevant images). In addition, all these techniques are likely to be complementary, as they consider orthogonal aspects to improve the performance. UKB <ref type="bibr" target="#b5">[6]</ref> is a dataset with few corresponding images per query (4, including the query image). QE techniques are therefore not expected to perform well, and accordingly we are not aware of any competitive result reported with a QE method on this dataset. For this set only, we reduce the short-list of images selected in the short-list to reflect the expected result set. Table <ref type="table" target="#tab_5">7</ref> shows that HQE improves the performance significantly compared to the HE baseline and is therefore effective even with few relevant images. It performs similar to other state-of-the-art techniques that perform well on this dataset. Note that these best techniques all require to cross-match (off-line) the whole image collection with itself, which may be infeasible on a large scale (quadratic complexity).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.">Complexity: timings and query size</head><p>First, note that the initial query includes a binary signature per local feature and several features can be assigned to the same visual word for a given image, especially with MA. In addition, the expanded query set, as defined before aggregation, is much larger as several images contribute to it with their reliable features, as previously shown in Table <ref type="table" target="#tab_1">3</ref>. Thanks to HQE, only one binary signature per visual word is kept. This favorably impacts the complexity of the enriched query in terms of the number of signatures. On average, the total number of features increases only by a small factor after aggregation, to be compared with queries which are one order of magnitude larger for other QE techniques.</p><p>Table <ref type="table">8</ref> reports the average search times when querying Oxford105k. They have been measured on a single core desktop machine (3.2 GHz). The spatial matching has been estimated by an external software and is included in the query time, unlike the SIFT extraction and quantization times. As expected, the search times are competitive for HQE without geometry, even when MA is used. As a reference, best time reported for QE with spatial matching <ref type="bibr" target="#b34">[35]</ref> is 509 ms on Oxford105k on a 4 Â 3.0 GHz machine. In addition, the cost of assignment to visual words is much smaller for our method with 65k visual words compared to the one of their method which needs up to 1 M visual words to obtain optimal performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusion</head><p>This paper makes several contributions related to visual query expansion. First, we introduce a query expansion method which is effective without using any geometry. While the general belief is that spatial verification is required to select the relevant images used to build the augmented query, exploiting the Hamming Embedding technique with a stringent selection rule and an aggregation strategy, we already achieve state-of-the-art performance. This method has a low complexity. We then show that combining our Hamming query expansion with geometry further improves the results and significantly outperforms the state of the art.</p><p>In future work, we will investigate how to incorporate weak spatial matching models <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b22">23]</ref> in our query expansion method, in order to find a compromise between a costly spatial verification or not using geometry at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 8</head><p>Average query times for the HE baseline and our technique with and without spatial matching, measured on Oxford105k when using a single processor core. These timings do not include the description part (extracting and quantizing the SIFT descriptors), which does not depend on database size. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Matching features using BOVW (top), HE with ht ¼ 24 (middle) and HE with ht ¼ 16 (bottom).</figDesc><graphic coords="3,329.00,351.53,215.06,372.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Examples of query images (left) and the corresponding top ranked lists by the baseline retrieval system. Images (not) selected as reliable are marked with (gray) green border. (For interpretation of the references to color in this figure caption, the reader is referred to the web version of this article.)</figDesc><graphic coords="4,40.70,421.49,500.22,298.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Sample reliable images and features assigned to reliable visual words, when geometry is not used. Left: query image. Top: features assigned to reliable visual words that appear in the query image. Bottom: features in the set of augmented visual words. Note: we only show a subsample of the actual reliable visual words. Each color represents a distinct visual word. (For interpretation of the references to color in this figure caption, the reader is referred to the web version of this article.)</figDesc><graphic coords="5,64.04,58.61,477.20,246.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Features selected for the expanded set of a particular query image (left) without (middle) and with spatial matching (right). With spatial matching: features backprojected out of the bounding box are rejected (red), while the rest (blue and green) are kept. Those assigned to reliable visual words are shown in green. Without spatial matching: features assigned to reliable visual words are shown in cyan or yellow, with yellow being the ones assigned specifically to augmentation visual words and cyan the ones assigned to visual words that appear in the query. Rejected are shown in red. (For interpretation of the references to color in this figure caption, the reader is referred to the web version of this article.)</figDesc><graphic coords="6,82.64,477.29,420.32,220.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Impact of α. Performance of HQE when varying the number of new visual words in the expanded query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Impact of the vocabulary size on the performance of HE, HQE and HQE with spatial matching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,52.88,572.33,479.81,143.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc>Mean average precision for separate components comprising the proposed method. Initial method is the original Hamming Embedding without weights. W ¼ weighting similarities. MA¼ multiple assignment.</figDesc><table><row><cell>W</cell><cell>burst</cell><cell>MA</cell><cell>HQE</cell><cell>SP</cell><cell>Oxford5k</cell><cell>Paris6k</cell><cell>Oxford105k</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>66.9</cell><cell>65.7</cell><cell>55.5</cell></row><row><cell>Â</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70.4</cell><cell>68.4</cell><cell>59.6</cell></row><row><cell>Â</cell><cell>Â</cell><cell></cell><cell></cell><cell></cell><cell>71.7</cell><cell>70.2</cell><cell>62.9</cell></row><row><cell>Â</cell><cell>Â</cell><cell>Â</cell><cell></cell><cell></cell><cell>75.4</cell><cell>72.0</cell><cell>68.0</cell></row><row><cell>Â</cell><cell>Â</cell><cell>Â</cell><cell>Â</cell><cell></cell><cell>83.0</cell><cell>80.6</cell><cell>79.0</cell></row><row><cell>Â</cell><cell>Â</cell><cell>Â</cell><cell>Â</cell><cell>Â</cell><cell>86.8</cell><cell>81.5</cell><cell>82.6</cell></row><row><cell></cell><cell></cell><cell>BOVW</cell><cell></cell><cell></cell><cell>53.3</cell><cell>54.8</cell><cell>44.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc>Performance and average query size jQ j for the baseline HE, HQE and the use of the same expanded query before aggregation (HQE/b.a.). Note that the aggregation procedure is a key step: not only it significantly reduces the complexity (number of features), but it also improves the performance.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>mAP</cell><cell></cell><cell>jQ j</cell><cell></cell></row><row><cell></cell><cell></cell><cell>SA</cell><cell>MA</cell><cell>SA</cell><cell>MA</cell></row><row><cell>Oxford5k</cell><cell>HE</cell><cell>71.7</cell><cell>75.4</cell><cell>1362</cell><cell>4088</cell></row><row><cell></cell><cell>HQE/b.a.</cell><cell>79.0</cell><cell>82.0</cell><cell>11,937</cell><cell>27,345</cell></row><row><cell></cell><cell>HQE</cell><cell>80.7</cell><cell>83.0</cell><cell>1810</cell><cell>5030</cell></row><row><cell>Paris6k</cell><cell>HE</cell><cell>70.2</cell><cell>72.0</cell><cell>1460</cell><cell>4382</cell></row><row><cell></cell><cell>HQE/b.a.</cell><cell>76.6</cell><cell>77.3</cell><cell>35,982</cell><cell>66,665</cell></row><row><cell></cell><cell>HQE</cell><cell>80.2</cell><cell>80.6</cell><cell>1843</cell><cell>5045</cell></row><row><cell>Oxford105k</cell><cell>HE</cell><cell>62.9</cell><cell>68.0</cell><cell>1362</cell><cell>4088</cell></row><row><cell></cell><cell>HQE/b.a.</cell><cell>73.5</cell><cell>76.5</cell><cell>12,176</cell><cell>28,699</cell></row><row><cell></cell><cell>HQE</cell><cell>75.6</cell><cell>79.0</cell><cell>1810</cell><cell>5030</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc>Oxford5k dataset: Summary of the number of ground truth images, the number of reliable images and the performance for HE, HQE and HQE with spatial matching. We report the average value of jLQj per building, i.e., the number of automatically detected reliable images in the short-list of 100 top-ranked ones.</figDesc><table><row><cell>Building</cell><cell>GT</cell><cell>HE</cell><cell>HQE</cell><cell></cell><cell>HQE-SP</cell><cell></cell></row><row><cell></cell><cell></cell><cell>mAP</cell><cell>jLQj</cell><cell>mAP</cell><cell>jLQj</cell><cell>mAP</cell></row><row><cell>All Souls</cell><cell>183</cell><cell>78.2</cell><cell>47.0</cell><cell>94.6</cell><cell>44.8</cell><cell>97.3</cell></row><row><cell>Ashmolean</cell><cell>56</cell><cell>63.7</cell><cell>10.9</cell><cell>76.1</cell><cell>9.9</cell><cell>80.8</cell></row><row><cell>Balliol</cell><cell>30</cell><cell>72.7</cell><cell>15.8</cell><cell>81.0</cell><cell>8.0</cell><cell>82.1</cell></row><row><cell>Bodleian</cell><cell>54</cell><cell>66.4</cell><cell>33.4</cell><cell>94.5</cell><cell>19.8</cell><cell>86.9</cell></row><row><cell>Christ Church</cell><cell>211</cell><cell>74.9</cell><cell>39.5</cell><cell>75.7</cell><cell>45.1</cell><cell>90.7</cell></row><row><cell>Cornmarket</cell><cell>22</cell><cell>69.5</cell><cell>9.6</cell><cell>64.9</cell><cell>6.4</cell><cell>71.4</cell></row><row><cell>Hertford</cell><cell>55</cell><cell>87.7</cell><cell>41.5</cell><cell>95.0</cell><cell>43.5</cell><cell>98.3</cell></row><row><cell>Keble</cell><cell>18</cell><cell>93.0</cell><cell>9.5</cell><cell>96.5</cell><cell>7.6</cell><cell>99.5</cell></row><row><cell>Magdalen</cell><cell>157</cell><cell>29.9</cell><cell>15.6</cell><cell>36.5</cell><cell>8.8</cell><cell>48.6</cell></row><row><cell>Pitt Rivers</cell><cell>16</cell><cell>100.0</cell><cell>9.7</cell><cell>99.7</cell><cell>7.0</cell><cell>100.0</cell></row><row><cell>Radcliffe</cell><cell>569</cell><cell>93.9</cell><cell>97.1</cell><cell>98.5</cell><cell>96.0</cell><cell>98.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5</head><label>5</label><figDesc>More features: Performance comparison on Oxford5k using lower detector threshold values, i.e., larger sets of local features. Binary signatures of 128 bits are used.</figDesc><table><row><cell># features</cell><cell></cell><cell>mAP</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>MA</cell><cell>12.53 M</cell><cell>21.92 M</cell><cell>27.59 M</cell></row><row><cell>BOVW</cell><cell></cell><cell>54.9</cell><cell>58.7</cell><cell>55.2</cell></row><row><cell>HE</cell><cell></cell><cell>74.2</cell><cell>78.6</cell><cell>78.3</cell></row><row><cell>HQE</cell><cell></cell><cell>81.0</cell><cell>84.8</cell><cell>84.4</cell></row><row><cell>HQE-SP</cell><cell></cell><cell>85.3</cell><cell>88.1</cell><cell>88.5</cell></row><row><cell>HQE-SP</cell><cell>Â</cell><cell>88.0</cell><cell>89.4</cell><cell>89.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6</head><label>6</label><figDesc>Performance comparison with state-of-the-art methods on Oxford5k, Paris6k and Oxford105k. The standard deviation is obtained from 5 measurements.</figDesc><table><row><cell>Method</cell><cell>SP</cell><cell>MA</cell><cell>Oxford5k</cell><cell>Paris6k</cell><cell>Oxford105k</cell></row><row><cell>Perdoch [35]</cell><cell>Â</cell><cell></cell><cell>78.4</cell><cell>N/A</cell><cell>72.8</cell></row><row><cell>Perdoch [35]</cell><cell>Â</cell><cell>Â</cell><cell>82.2</cell><cell>N/A</cell><cell>77.2</cell></row><row><cell>Mikulik [38]</cell><cell>Â</cell><cell>Â</cell><cell>84.9</cell><cell>82.4</cell><cell>79.5</cell></row><row><cell>Chum [16]</cell><cell>Â</cell><cell></cell><cell>82.7</cell><cell>80.5</cell><cell>76.7</cell></row><row><cell>Arandjelovic [17]</cell><cell>Â</cell><cell></cell><cell>80.9</cell><cell>76.5</cell><cell>72.2</cell></row><row><cell>HQE</cell><cell></cell><cell></cell><cell>80.7 70.9</cell><cell>80.2 7 0.2</cell><cell>76.6 7 1.1</cell></row><row><cell>HQE-SP</cell><cell>Â</cell><cell></cell><cell>83.7 70.7</cell><cell>80.0 7 0.2</cell><cell>79.4 70.6</cell></row><row><cell>HQE</cell><cell></cell><cell>Â</cell><cell>83.0 70.9</cell><cell>80.6 7 0.2</cell><cell>79.0 71.0</cell></row><row><cell>HQE-SP</cell><cell>Â</cell><cell>Â</cell><cell>86.8 70.3</cell><cell>81.5 7 0.3</cell><cell>82.6 70.4</cell></row><row><cell>HQE128bits</cell><cell></cell><cell></cell><cell>81.0 7 0.5</cell><cell>81.5 7 0.2</cell><cell>76.9 7 0.6</cell></row><row><cell>HQE-SP128bits</cell><cell>Â</cell><cell></cell><cell>85.3 70.4</cell><cell>81.3 7 0.3</cell><cell>80.8 70.5</cell></row><row><cell>HQE128bits</cell><cell></cell><cell>Â</cell><cell>83.8 70.3</cell><cell>82.8 7 0.1</cell><cell>80.4 70.5</cell></row><row><cell>HQE-SP128bits</cell><cell>Â</cell><cell>Â</cell><cell>88.07 0.3</cell><cell>82.8 70.2</cell><cell>84.07 0.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7</head><label>7</label><figDesc>UKB: comparison with state-of-the-art methods.</figDesc><table><row><cell>Jégou [10]</cell><cell>Jégou [21]</cell><cell>Qin [18]</cell><cell>HE-MA</cell><cell>HQE-MA</cell></row><row><cell>3.64</cell><cell>3.68</cell><cell>3.67</cell><cell>3.59</cell><cell>3.67</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>G. Tolias, H. Jégou / Pattern Recognition ∎ (∎∎∎∎) ∎∎∎-∎∎∎</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Please cite this article as: G. Tolias, H. Jégou, Visual query expansion with or without geometry: Refining local descriptors by feature aggregation, Pattern Recognition (2014), http://dx.doi.org/10.1016/j.patcog.2014.04.007i</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was done in the context of the Project Fire-ID, supported by the Agence Nationale de la Recherche (ANR-12-CORD-0016). We would also like to thank the reviewers for their valuable comments and effort to improve the paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of interest statement</head><p>None declared.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video Google: a text retrieval approach to object matching in videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Local grayvalue invariants for image retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mohr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="530" to="534" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Term-weighting approaches in automatic text retrieval</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Manag</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="513" to="523" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inverted files versus signature files for text indexing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ramamohanarao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Database Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="453" to="490" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scalable recognition with a vocabulary tree</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nistér</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stewénius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="2161" to="2168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distribution of content words and phrases in text and language modeling</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Lang. Eng</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="15" to="59" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Producing high-dimensional semantic spaces from lexical co-occurrence</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behav. Res. Methods, Instrum. Comput</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="203" to="208" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the burstiness of visual elements</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of co-occurrences in sparse high dimensional data</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image categorization using fisher kernels of non-iid image models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Relevance feedback and query expansion</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
	<note>Introduction to Information</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Total automatic query expansion with a generative feature model for object retrieval</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Logo retrieval with a contrario visual query expansion</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International conference on Multimedia</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Total recall II: query expansion revisited</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Perdoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Three things everyone should know to improve object retrieval</title>
		<author>
			<persName><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hello neighbor: accurate object retrieval with k-reciprocal nearest neighbors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gammeter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L V</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Object retrieval and localization with spatially-constrained similarity measure and k-nn re-ranking</title>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>IEEE</publisher>
		</imprint>
		<respStmt>
			<orgName>Computer Vision and Pattern Recognition</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Query specific fusion for image retrieval</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Accurate image search using the contextual dissimilarity measure</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Harzallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="11" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lost in quantization: Improving particular object retrieval in large scale image databases</title>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving bag-of-features for large scale image search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="316" to="336" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Better matching with fewer features: the selection of useful features in large database recognition problems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Turcot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision Workshop</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large-scale discovery of spatially related images</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="371" to="377" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Query expansion for hash-based image object retrieval</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Query expansion enhancement by fast binary matching</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scale and affine invariant interest point detectors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="86" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving the Fisher kernel for large-scale image classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into compact codes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">To aggregate or not to aggregate: selective match kernels for image search</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hamming embedding similaritybased image classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benmokhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multidisciplinary Research</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Negative evidences and co-occurrences in image retrieval: the benefit of PCA and whitening</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient representation of local geometry for large scale object retrieval</title>
		<author>
			<persName><forename type="first">M</forename><surname>Perdoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image retrieval for image-based localization revisited</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kobbelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Speeded-up, relaxed spatial matching</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning a fine vocabulary</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mikulík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perdoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">He is now a post-doctoral researcher in INRIA Rennes, working mainly in the area of Computer Vision with focus on large scale image retrieval</title>
	</analytic>
	<monogr>
		<title level="m">Giorgos Tolias holds a Ph</title>
		<imprint/>
		<respStmt>
			<orgName>D. in Electrical and Computer Engineering from the National Technical University of Athens</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">He is a former student of the Ecole Normale Supérieure de Cachan. He joined INRIA Grenoble as a permanent researcher in 2006 and moved to INRIA Rennes in 2009</title>
	</analytic>
	<monogr>
		<title level="m">Computer Science from the University of Rennes, defended in 2005 on joint source channel coding</title>
		<imprint/>
	</monogr>
	<note>His research interests focuses on large scale indexing and coding techniques</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
