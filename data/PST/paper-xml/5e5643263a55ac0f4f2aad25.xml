<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">END-TO-END TRAINING OF A LARGE VOCABULARY END-TO-END SPEECH RECOGNITION SYSTEM</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-12-22">22 Dec 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chanwoo</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sungsoo</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kwangyoun</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mehul</forename><surname>Kumar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiyeon</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kyungmin</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Changwoo</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Abhinav</forename><surname>Garg</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Eunhyang</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Minkyoo</forename><surname>Shin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shatrughan</forename><surname>Singh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Larry</forename><surname>Heck</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dhananjaya</forename><surname>Gowda</surname></persName>
						</author>
						<title level="a" type="main">END-TO-END TRAINING OF A LARGE VOCABULARY END-TO-END SPEECH RECOGNITION SYSTEM</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-12-22">22 Dec 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1912.11040v1[eess.AS]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>{chanw.com</term>
					<term>ss216.kim</term>
					<term>ky85.kim</term>
					<term>mehul3.kumar</term>
					<term>jstacey7.kim</term>
					<term>k.m.lee</term>
					<term>cw1105.han abhinav.garg</term>
					<term>sc.ehkim.jin</term>
					<term>mk0211.shin</term>
					<term>shatrughan.s</term>
					<term>larry.h</term>
					<term>d.gowda}@samsung.com end-to-end speech recognition</term>
					<term>distributed training</term>
					<term>example server</term>
					<term>data augmentation</term>
					<term>acoustic simulation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present an end-to-end training framework for building state-of-the-art end-to-end speech recognition systems.</p><p>Our training system utilizes a cluster of Central Processing Units (CPUs) and Graphics Processing Units (GPUs). The entire data reading, large scale data augmentation, neural network parameter updates are all performed "on-the-fly". We use vocal tract length perturbation [1] and an acoustic simulator [2] for data augmentation. The processed features and labels are sent to the GPU cluster. The Horovod allreduce approach is employed to train neural network parameters. We evaluated the effectiveness of our system on the standard Librispeech corpus [3] and the 10,000-hr anonymized Bixby English dataset. Our end-to-end speech recognition system built using this training infrastructure showed a 2.44 % WER on test-clean of the LibriSpeech test set after applying shallow fusion with a Transformer language model (LM). For the proprietary English Bixby open domain test set, we obtained a WER of 7.92 % using a Bidirectional Full Attention (BFA) end-to-end model after applying shallow fusion with an RNN-LM. When the monotonic chunckwise attention (MoCha) based approach is employed for streaming speech recognition, we obtained a WER of 9.95 % on the same Bixby open domain test set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In recent years, deep learning techniques have significantly improved speech recognition accuracy <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. This improvement has come about from the shift from Gaussian Mixture Model (GMM) to the Feed-Forward Deep Neural Networks (FF-DNNs), FF-DNNs to Recurrent Neural Network (RNN) and in particular the Long Short-Term Memory (LSTM) networks <ref type="bibr" target="#b8">[9]</ref>. Thanks to these advances, voice assistant devices such as Google Home <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref> , Amazon Alexa or Samsung Bixby <ref type="bibr" target="#b10">[11]</ref> are being used at many homes and on personal devices.</p><p>Recently there has been increasing interest in switching from the conventional Weighted Finite State Transducer (WFST) based decoder using an Acoustic Model (AM) and a Language Model (LM) to a complete end-to-end all-neural speech recognition systems <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. These complete end-to-end systems have started surpassing Thanks to Samsung Electronics for funding this research. The authors are thankful to Executive Vice President Seunghwan Cho, Ravichander Vipperla, Nicholas Lane, and members of Speech Processing Lab at Samsung Research.</p><p>the performance of the conventional WFST-based decoders with a very large training database <ref type="bibr" target="#b14">[15]</ref>, a better choice of target unit such as Byte Pair Encoded (BPE) subword units, and an improved training methodology such as Minimum Word Error Rate (MWER) training <ref type="bibr" target="#b15">[16]</ref>.</p><p>Another important aspect in building high-performance speech recognition systems is the amount and the coverage of the training data. To build high performance speech recognition systems for conversational speech, we need to use a large amount of speech data covering various domains <ref type="bibr" target="#b16">[17]</ref>. In <ref type="bibr" target="#b17">[18]</ref>, it has been shown that we need a very large training set (âˆ¼125,000 hours of semi-supervised speech data) to achieve high speech recognition accuracy for difficult tasks like video captioning. To train neural networks using such large amounts of speech data, we usually need multiple Central Processing Units (CPUs) or Graphics Processing Units (GPUs) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>With widespread adoption of voice assistant speakers, far-field speech recognition has become very important. In far-field speech recognition, the impacts of reverberation and noise are much larger than those in near-field cases. Traditional approaches to far-field speech recognition include noise robust feature extraction algorithms <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>, or multi-microphone approaches <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>. More recently, approaches using data augmentation has been gaining popularity for far-field speech recognition <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35</ref>]. An "acoustic simulator" <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b35">36]</ref> is used to generate simulated speech utterances for millions of different room dimensions, a wide distribution of reverberation time and signal-to-noise ratio. In a similar spirit, Vocal Tract Length Perturbation (VTLP) has been proposed <ref type="bibr" target="#b36">[37]</ref> to tackle the speaker variability issue. As shown in our recent paper <ref type="bibr" target="#b0">[1]</ref>, VTLP is especially useful when the speaker variability in the training database is not sufficient. For these kinds of data augmentation, processing on CPUs is more desirable than processing on GPUs. Due to this, we have proposed an end-to-end training approach using Example Servers (ES) and workers. Example servers are typically run on the CPU cluster performing data reading, data augmentation, and feature extraction <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b35">36]</ref>. In this paper, we describe the structure of our end-to-end training system to train an end-to-end speech recognition system. This training system has several advantages over previous systems described in <ref type="bibr" target="#b35">[36]</ref>. First, instead of using the QueueRunner, we use a more efficient data queue using tf.data in Tensorflow <ref type="bibr" target="#b37">[38]</ref>. Second, instead of pre-calculating information about room configurations and room impulse responses in the acoustic simulator, these are calculated onthe-fly. Thus, the entire training system runs on-the-fly. Additionally, instead of using the parameter server-worker structure, we use an allreduce approach implemented in the Horovod <ref type="bibr" target="#b38">[39]</ref> distributed training framework, which has been shown to be more efficient. The Fig. <ref type="figure">1</ref>: The Samsung Research end-to-end training framework for building an end-to-end speech recognition system with multi CPU-GPU clusters and on-the-fly data processing and augmentation pipeline.</p><p>system described in <ref type="bibr" target="#b18">[19]</ref>, is designed to train the acoustic model part of the speech recognition system where as our training system trains the complete end-to-end speech recognition system. The rest of the paper is organized as follows: We describe the entire training system structure in detail in Sec. 2. The structure of the end-to-end speech recognition system is described in Sec. 3. Experimental results that demonstrates the effectiveness of our speech recognition system is presented in Sec. 4. We conclude in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">OVERALL STRUCTURE OF THE END-TO-END SPEECH RECOGNITION</head><p>In this section, we describe the overall structure of our end-to-end training system. Fig. <ref type="figure">1</ref> shows how the entire system is structured.</p><p>Our system consists of a cluster of CPUs and a cluster of GPUs. Each GPU node of the GPU cluster has eight Nvidiaâ„¢P-40, P-100 or V-100 GPUs and two Intel E5-2690 v4 CPUs. Each of these CPUs has 14 cores. The large box on the left hand side of Fig. <ref type="figure">1</ref> denoted "GPU cluster" shows a typical GPU node with N GPUs. The large box on the right shows a "CPU cluster" of M CPUs, each running an independent data pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Training job launch</head><p>The main process of the training system runs on one of CPU cores of the GPU cluster. This CPU portion of the GPU node is represented as a box in the right hand side of the GPU node box. When the training job starts, this main training process launches multiple example server jobs on the CPU cluster using the IBM Platform LSF <ref type="bibr" target="#b39">[40]</ref>. In Fig. <ref type="figure">1</ref>, this launching process is represented by a dashed arrow from the CPU portion of the GPU node to the CPU cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Data reading using an example queue</head><p>In the CPU cluster, each CPU runs one example server which reads speech utterance and transcript data from sharded TFRecords defined in Tensorflow <ref type="bibr" target="#b37">[38]</ref>. The TFRecord format is a simple format in Tensorflow for storing a sequence of binary records. To support efficient reading using multiple CPUs, we use sharded TFRecords.</p><p>To read large-scale data efficiently in parallel, we use an example queue shown in the left side of Fig. <ref type="figure">1</ref>. The original speech waveform data, transcripts, and meta data are stored in sharded TFRecords. The data pipeline is implemented using tf.data in Tensorflow <ref type="bibr" target="#b37">[38]</ref>, and contains the data augmentation and feature extraction blocks. These tf.data APIs are efficient in building complex pipelines by applying a series of elementary operations. We perform data interleaving and parallel reading using tf.contrib.data.parallel interleave, shuffling using tf.data.Datatset.shuffle, and padding using tf.data.Dataset.padded batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Data augmentation and feature extraction</head><p>To improve robustness against speaker variability, we apply an onthe-fly VTLP algorithm on the input waveform <ref type="bibr" target="#b0">[1]</ref>. The warping factor is generated randomly for each input utterance. Unlike conventional VTLP approaches in <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b40">41]</ref>, we resynthesize the processed speech. The purpose of doing this is to apply VTLP before applying the acoustic simulator to the input waveform. This is quite natural that data augmentation to model speaker variability should be performed before the data augmentation to model acoustic variability. One more advantage is that this resynthesis approach enables us to use a window length optimal for VTLP different from that used in feature processing. We apply a blinear transformation <ref type="bibr" target="#b41">[42]</ref> to per- form frequency warping to model speaker variability due to the difference in the vocal tract length. In the bilinear transformation, the relation between the input and output discrete-time frequencies is given by:</p><formula xml:id="formula_0">Ï‰ â€² k = Ï‰ k + 2 tan âˆ’1 (1 âˆ’ Î±) sin(Ï‰ k ) 1 âˆ’ (1 âˆ’ Î±) cos(Ï‰ k ) . (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where</p><formula xml:id="formula_2">Ï‰ k = 2Ï€k K is the input discrete-time frequency , Ï‰ â€² k = 2Ï€k â€² K</formula><p>is the output discrete-time frequency, and K is the DFT size. More details about our VTLP algorithm is described in detail in <ref type="bibr" target="#b0">[1]</ref>. The acoustic simulator in Fig. <ref type="figure">1</ref> is similar to what we described in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b35">36]</ref>. One difference compared to our previous one in <ref type="bibr" target="#b1">[2]</ref> is that we do not pre-calculate room impulse responses, but instead they are calculated on-the-fly. For feature processing we use tf.data.Dataset.map API. Instead of using the more conventional log-mel or MFCC features, we use the power mel filterbank energies, since it shows slightly better performance <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b42">43]</ref>. Motivated by our previous research of using power-law nonlinearity with a power coefficient between 1 15 <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46</ref>] and 1  10 [47], we apply the power-law nonlinearity of (â€¢) 1 15 to the mel filterbank coefficients. We refer to this feature as power-mel filterbank coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Parameter calculation and update</head><p>The features and the target labels are sent to the GPU cluster using the ZeroMQ <ref type="bibr" target="#b47">[48]</ref> asynchronous messaging queue. Each example server sends these data asynchronously to the CPU portion of the GPU node as shown in Fig 1 <ref type="figure">.</ref> Using these data, neural network parameters are calculated and updated using an Adam optimizer and the Horovod <ref type="bibr" target="#b38">[39]</ref> allreduce approach.</p><p>Fig. <ref type="figure" target="#fig_1">3</ref> shows how many CPUs in the example server per GPU are required to provide sufficient data to the GPU cluster. In this experiment, we used a 10,000-hr anonymized Bixby English training set. We trained a streaming end-to-end model using the Monotonic CHunkwise Attention (MoCha) algorithm <ref type="bibr" target="#b48">[49]</ref>. The details about our MoCha implemention are discussed in <ref type="bibr" target="#b49">[50]</ref>. In the example server, we ran the VTLP data augmentation <ref type="bibr" target="#b0">[1]</ref>, acoustic simulator <ref type="bibr" target="#b1">[2]</ref> and feature extraction modules shown in Fig. <ref type="figure" target="#fig_0">2</ref>. In this experiment, we used four Nvidiaâ„¢V-100 GPUs with 32-GB memory in the GPU cluster. Fig. <ref type="figure" target="#fig_1">3a</ref> shows how much time is required to finish one epoch of training. When data augmentation is not applied, 65.6hours were required to finish one epoch of training. Fig. <ref type="figure" target="#fig_1">3a</ref> shows us that three CPUs per GPU (total 12 CPUs for 4 GPUs) are required to obtain a comparable throughput. If we use four or five CPUs per GPU, as shown in Fig. <ref type="figure" target="#fig_1">3a</ref>, the training is even slightly faster than the case without the example-server-based data-augmentation. We think that this happened because of more efficient data processing with the example server. When we do not perform data augmentation using example servers, feature extraction and data reading are performed on a limited numbers of CPUs inside the GPU cluster, which might add some latency during the training. Thus, it is possible that the training with data augmentation using example servers may be even slightly faster than the baseline case without data-augmentation using example servers.</p><p>Fig. <ref type="figure" target="#fig_1">3b</ref> shows the portion of the time used forTensorflow computation. This portion of time is defined by: tsession = Time Spent in Tensorflow Session Elapsed Time .</p><p>(</p><formula xml:id="formula_3">)<label>2</label></formula><p>If GPUs in the GPU cluster are not given sufficient amount of data, these GPUs will remain idle. Thus, tsession in ( <ref type="formula" target="#formula_3">2</ref>) is a good indicator to see whether the example server provides sufficient amount of processed features. From this figure, we may conclude that three âˆ¼ four CPUs per GPU (total 12 âˆ¼ 16 CPUs for 4 GPUs) are required to keep GPUs busy enough. In our experiments using the 10,000-hr Bixby training set in Sec. 4, we used 8 GPUs and 40 CPUs (5 CPUs per GPU) during the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">STRUCTURE OF THE END-TO-END SPEECH RECOGNITION SYSTEM</head><p>We have adopted the RETURNN speech recognition system <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref> for training our end-to-end system with various modifications. Some of the important modifications are: replacing the input data pipeline with our proposed on-the-fly example server based pipeline with support for VTLP and acoustic simulation, implementing the Monotonic Chunkwise Attention (MoChA) <ref type="bibr" target="#b48">[49]</ref> for online streaming end-to-end speech recognition, minimum Word Error Rate (mWER) training, support for handling Korean language or script, our own scoring and Inverse Text Normalization (ITN) modules, support for power mel filterbank features <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b42">43]</ref>, etc. We have tried various types of training strategies for better performance <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref>. Our MoCha implementation and optimization are described in very detail in our another paper <ref type="bibr" target="#b49">[50]</ref>. The structure of our entire end-to-end speech recognition system is shown in Fig. <ref type="figure" target="#fig_0">2</ref>. x[m] and y l are the input power mel filterbank energy vector and the output label, respectively. m is the input frame index and l is the decoder output step index. c l is the context vector calculated as a weighted sum of the encoder hidden state vectors denoted as h enc [m]. The attention weights are computed as a softmax of energies computed as a function of the encoder hidden state h enc [m], the decoder hidden state h dec l , and the attention weight feedback Î² l [m] <ref type="bibr" target="#b51">[52]</ref>.</p><p>In <ref type="bibr" target="#b51">[52]</ref>, the peak value of the speech waveform is normalized to be one. However, since finding the peak sample value is not possible for online feature extraction, we do not perform this normalization. We modified the input pipeline so that the online feature generation can be performed. We disabled the clipping of feature range between -3 and 3, which is the default setting for the Librispeech experiment using MFCC features in <ref type="bibr" target="#b51">[52]</ref>. We conducted experiments using both the uni-directional and bi-directional Long Short-Term Memories (LSTMs) <ref type="bibr" target="#b8">[9]</ref> in the encoder. However, only the uni-directional LSTMs are used in the decoder. For online speech recognition experiments, we used the MoChA models <ref type="bibr" target="#b48">[49]</ref> with a chunk size of 2. In MoCha experiments, we used only the unidirectional LSTMs both in the encoder and the decoder to enable streaming recognition. For better stability in LSTM training, we use the gradient clipping by global norm <ref type="bibr" target="#b54">[55]</ref>, which is implemented as tf.clip by global norm API in Tensorflow <ref type="bibr" target="#b37">[38]</ref>. We use six layers of encoders and one layer of decoder followed by a softmax layer.</p><p>In performing shallow-fusion with an external LM, our approach is slightly different from the previously known approaches <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57]</ref> to obtain better performance. we used the following equation: </p><p>where we have an additional term Î»p log P (y l ) for subtracting the prior bias that the model has learned from the training corpus. In (3) L is the length of the output label hypothesis. Î»p and Î»lm are weights for the prior probability and the LM prediction probability, respectively. In (3), we represented sequences following the Python slice notation. For example, x[0 : M ] denotes the sequence of the input acoustic features of length M , and y0:L is a sequence of output labels of length L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL RESULTS</head><p>In this section, we present a summary of experimental results obtained with our end-to-end speech recognition systems built using the proposed Samsung Research end-to-end training framework. For near-field speech recognition experiments, we use the open source Librispeech database <ref type="bibr" target="#b2">[3]</ref>, as well as our in-house Bixby <ref type="bibr" target="#b10">[11]</ref>  Table <ref type="table">1</ref>: Word Error Rates (WERs) obtained using MFCC implemented in <ref type="bibr" target="#b57">[58]</ref> and power mel filterbank coefficients on the Librispeech corpus <ref type="bibr" target="#b2">[3]</ref>. For each WER number, the same experiment was conducted twice and averaged.  <ref type="table">1</ref>, we compare the performance between the baseline MFCC and the power-law of (â€¢) 1 15 features for a Bidirectional Full Attention (BFA) end-to-end model with an LSTM cell size of 1536 on the LibriSpeech database <ref type="bibr" target="#b2">[3]</ref>. Especially for test-other, which is a more difficult task, the power mel filterbank coefficients shows better performance than the baseline MFCC. Thus, we use the power-mel filterbank coefficients as the default feature in our end-to-end system. All the following results in this section were obtained using the power-mel filterbank coefficients. In Table <ref type="table" target="#tab_2">2</ref>, we show Word Error Rates (WERs) on the same Lib-riSpeech corpus for a BFA model using different window sizes and warping coefficient distributions, with and without using an external Recurrent Neural Network (RNN) Language Model (LM) <ref type="bibr" target="#b51">[52]</ref> built using the standard LibriSpeech LM corpus. The best performance was achieved when the window length is 50 ms and the warping coefficients are uniformly distributed between 0.8 and 1.2. We obtained 3.66 % WER on the test-clean database and 12.39 % WER on the test-other database without using an LM. Using this shallow-fusion technique with an RNN-LM, we achieved WERs of 2.85 % and 10.25 % on the Librispeech test-clean and test-other databases, respectively.  <ref type="table" target="#tab_3">3</ref> shows word error rates on the LibriSpeech testsets obtained by applying shallow-fusion with a Transformer LM <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b60">61]</ref> using <ref type="bibr" target="#b2">(3)</ref>. As shown in this table, we conducted experiments with different beam sizes, Î»p and Î»lm parameters in <ref type="bibr" target="#b2">(3)</ref>. The best result we obtained using a Transformer LM in Table <ref type="table" target="#tab_3">3</ref> is significantly better than the result we obtained with a LSTM LM in Table <ref type="table" target="#tab_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cell</head><p>In Table <ref type="table" target="#tab_4">4</ref>, we summarize our WER results for both the Lib-riSpeech and Bixby end-to-end ASR models. In the case of the Bixby model, we optionally used an external RNN-LM trained using around 65GB of the Bixby LM corpus with an architecture exactly similar to the LibriSpeech LM model used in <ref type="bibr" target="#b51">[52]</ref>. The cell sizes of the LibriSpeech model and the Bixby model in Table <ref type="table" target="#tab_4">4</ref> are 1536 and 1024 respectively. For comparison, the best WFST based conventional LSTM-HMM based ASR system gives a WER of 8.85% on the Bixby same open domain test set. We can see that our current Bixby end-to-end BFA model is âˆ¼10% better, while our MoChA streaming model is âˆ¼10% poorer compared to the conventional WFST based DNN-HMM system. The performance of our far-field end-to-end ASR model trained using the proposed structure in Fig. <ref type="figure" target="#fig_0">2</ref> is shown in Fig. <ref type="figure" target="#fig_4">4</ref>. In this experiment, we used the same  Far-field recording using this Bixby command set was performed by playing back utterances using a loud speaker at 5-meter distance in a real room. The reverberation time in this recording room was measured to be T60 = 430ms. We used two microphones on a prototype Galaxy Home Mini to record this far-field speech. The distance between two microphones is 6.8 cm. We simulated far-field additive noise by playing back three different types of noise using loud speakers. In Fig. <ref type="figure" target="#fig_4">4a</ref>, we used a single loud speaker located at 1-meter distance from the microphone to simulate direct noise from a television. In Figs. <ref type="figure" target="#fig_4">4b and 4c</ref>, we used four loud speakers oriented to different directions to simulate diffuse noise. On the prototype Galaxy home Mini device, the two-microphone signals are enhanced using a beamformer based on the Neural Network supported Generalized Eigenvalue (NN-GEV) algorithm <ref type="bibr" target="#b58">[59]</ref>. In Fig. <ref type="figure" target="#fig_4">4</ref>, we evaluated speech recognition accuracy using three different systems. NBP+VTLP+AS denotes a system which uses the VTLP system and the acoustic simulator described in this paper for data augmentation in model training, and additionally uses this NN-GEV-based beamformer for signal enhancement. NBF in this future stands for this Neural Beam Former (NBF). VTLP+AS denotes a system employing the VTLP system and the acoustic simulator without using this beamformer. baseline denotes a system which was trained using utterances recorded in close-talking environments without any further processing. As can be seen in these figures, data augmentation technique significantly enhances speech recognition accuracy under the far-field environments. We also observe that the data augmentation algorithm described in Sec. 2 does not harm the clean performance, thus we may use the same data augmentation both for the close-talking and the far-field environments. In case of the direct TV noise in Fig. <ref type="figure" target="#fig_4">4a</ref> and babble noise in Fig. <ref type="figure" target="#fig_4">4c</ref>, we observe that further improvement is achieved by employing a NN-GEV-based beamformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>We presented a new end-to-end training framework and strategies for training state-of-the-art end-to-end speech recognition systems.</p><p>Our training system utilizes a cluster of Central Processing Units (CPUs) and Graphics Processing Units (GPUs). The entire data reading, large scale data augmentation, neural network parameter updates are performed on-the-fly using example servers and sharded TFRecords and tf.data. We use vocal tract length perturbation and an acoustic simulator for data augmentation. Horovod allreduce approach is employed to train the neural network pa-rameters using Adam optimizer. We evaluated the effectiveness of our system on the standard Librispeech corpus <ref type="bibr" target="#b2">[3]</ref> and 10,000-hr anonymized Bixby English training and test sets both in near-field as well as far-field scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: The structure of the entire end-to-end speech recognition system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: The efficiency of the example server with respect to the number of CPUs per GPU: (a) The required time to process a single epoch during the training phase and (b) the percentage of Tensorflow computation time defined by (2). The blue horizontal dotted lines in each figure represent the case when data augmentation with example servers is not employed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>y l | x[0 : M ], y 0:l ) âˆ’ Î»p log P (y l ) + Î»lm log P (y l |y 0:l ) ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>usage training and test sets for English. The LibriSpeech dataset consists of around 960 hours of training data consisting of 281,241 utterances. The evaluation set consists of the official 5.4 hours test-clean and 5.1 hours test-other data. The Bixby training set consists of approximately 10,000 hours of anonymized Bixby usage data. The evaluation set consists of around 1,000 open domain utterances. As mentioned in Sec. 2.4, we used 8 GPUs in a GPU cluster and 40 CPUs in an example server when training the model using the Bixby training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Speech recognition accuracy at different Signal-to-Noise Ratios (SNRs) under three different noisy conditions: direct TV noise (a) , music noise (b), and babble noise (c). NBF, VTLP, and AS stand for Neural Beam Former (NBF) [59], Vocal Tract LengthPerturbation (VTLP)<ref type="bibr" target="#b0">[1]</ref> , and Acoustics Simulator (AS)<ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b35">36]</ref>, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Word Error Rates (WERs) obtained with VTLP processing with different warping factor Î± distribution, and with and without an RNN LM. The warping factor Î± is the constant controlling warping in (1).</figDesc><table><row><cell cols="2">Warping Factor</cell><cell cols="3">0.7 âˆ¼ 1.3 0.8 âˆ¼ 1.2 0.9 âˆ¼ 1.1</cell></row><row><cell>Without RNN-LM</cell><cell>test-clean test-other average</cell><cell>3.82 % 12.50 % 8.16 %</cell><cell>3.66 % 12.39 % 8.03 %</cell><cell>3.86 % 12.35 % 8.11 %</cell></row><row><cell>With RNN-LM</cell><cell>test-clean test-other average</cell><cell>2.93 % 10.40 % 6.67 %</cell><cell>2.85 % 10.25 % 6.55 %</cell><cell>2.96 % 10.13 % 6.55 %</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Word Error Rates (WERs) obtained with VTLP processing using shallow-fusion with a Transformer LM with different beam sizes. The window length is 50 ms, and the warping factor distribution is 0.8 âˆ¼ 1.2.</figDesc><table><row><cell>Beam Size</cell><cell>12</cell><cell>24</cell><cell>36</cell><cell>48</cell></row><row><cell>Î»p</cell><cell>0.005</cell><cell>0.004</cell><cell>0.003</cell><cell>0.002</cell></row><row><cell>Î»lm</cell><cell>0.45</cell><cell>0.46</cell><cell>0.48</cell><cell>0.48</cell></row><row><cell>test-clean</cell><cell cols="4">2.49 % 2.45 % 2.44 % 2.45 %</cell></row><row><cell>test-other</cell><cell cols="4">8.76 % 8.40 % 8.29 % 8.22 %</cell></row><row><cell>average</cell><cell cols="4">5.63 % 5.43 % 5.37 % 5.34 %</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Summary of Word Error Rates (WERs) obtained for different LibriSpeech and Bixby near-field end-to-end ASR models with and without an RNN LM.</figDesc><table><row><cell>Models</cell><cell></cell><cell>BFA</cell><cell>MoChA</cell></row><row><cell>LibriSpeech</cell><cell>w/o LM</cell><cell>3.66 %</cell><cell>6.78 %</cell></row><row><cell>(1536-cell)</cell><cell>RNN-LM</cell><cell>2.85 %</cell><cell>5.54 %</cell></row><row><cell>test-clean</cell><cell>Transformer LM</cell><cell>2.44 %</cell><cell>-</cell></row><row><cell>Bixby</cell><cell>w/o LM</cell><cell cols="2">8.25 % 10.77 %</cell></row><row><cell>(1024-cell)</cell><cell>RNN-LM</cell><cell>7.92 %</cell><cell>9.95 %</cell></row><row><cell cols="4">ancesare sampled from the anonymized Bixby usage log. Examples</cell></row><row><cell cols="4">in this test set include "Set an alarm for tomorrow at 6 a.m", "Tell</cell></row><row><cell cols="4">me remaining time of my timers", "Play the most latest added song",</cell></row><row><cell>and so on.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improved Vocal Tract Length Perturbation for a State-of-the-Art End-to-End Speech Recognition System</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gowda</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2019-3227</idno>
		<ptr target="http://dx.doi.org/10.21437/Interspeech.2019-3227" />
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH-2019</title>
				<meeting><address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09">Sept. 2019</date>
			<biblScope unit="page" from="739" to="743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generation of large-scale simulated utterances in virtual rooms to train deep-neural networks for far-field speech recognition in google home</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2017-1510</idno>
		<ptr target="http://dx.doi.org/10.21437/Interspeech.2017-1510" />
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017, 2017</date>
			<biblScope unit="page" from="379" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Librispeech: An asr corpus based on public domain audio books</title>
		<author>
			<persName><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Acoust., Speech, and Signal Processing</title>
				<imprint>
			<date type="published" when="2015-04">April 2015</date>
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An investigation of deep neural networks for noise robust speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Acoust. Speech, and Signal Processing</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="7398" to="7402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Feature learning in deep neural networks -studies on speech recognition tasks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
				<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving the speed of neural networks on CPUs</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning and Unsupervised Feature Learning NIPS Workshop</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multichannel signal processing with deep neural networks for automatic speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Variani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<date type="published" when="2017-02">Feb. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11">Nov. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Acoustic modeling for Google Home</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caroselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pundak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K-C</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Variani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Siohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weintraub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shannon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-08">Aug. 2017</date>
			<biblScope unit="page" from="399" to="403" />
		</imprint>
	</monogr>
	<note>in INTERSPEECH-2017</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Samsung bixby</title>
		<ptr target="http://www.samsung.com/bixby/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<date type="published" when="2016-03">March 2016</date>
			<biblScope unit="page" from="4960" to="4964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A comparison of sequence-tosequence models for speech recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2017-233</idno>
		<ptr target="http://dx.doi.org/10.21437/Interspeech.2017-233" />
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017, 2017</date>
			<biblScope unit="page" from="939" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">State-of-theart speech recognition with sequence-to-sequence models</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gonina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<date type="published" when="2018-04">April 2018</date>
			<biblScope unit="page" from="4774" to="4778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Minimum word error rate training for attention-based sequence-to-sequence models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<date type="published" when="2018-04">April 2018</date>
			<biblScope unit="page" from="4839" to="4843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Toward domain-invariant speech recognition via large scale training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pundak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elfeky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Spoken Language Technology Workshop (SLT)</title>
				<imprint>
			<date type="published" when="2018-12">Dec 2018</date>
			<biblScope unit="page" from="441" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural speech recognizer: Acoustic-to-word lstm model for large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2017-1566</idno>
		<ptr target="http://dx.doi.org/10.21437/Interspeech.2017-1566" />
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH-2017</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3707" to="3711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">End-to-end training of acoustic models for large vocabulary continuous speech recognition with tensorflow</title>
		<author>
			<persName><forename type="first">E</forename><surname>Variani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bagby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2017-1284</idno>
		<ptr target="http://dx.doi.org/10.21437/Interspeech.2017-1284" />
		<imprint>
			<date type="published" when="2017">2017, 2017</date>
			<biblScope unit="page" from="1641" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Accurate, large minibatch SGD: training imagenet in 1 hour</title>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1706.02677" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Power-Normalized Cepstral Coefficients (PNCC) for Robust Speech Recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="page" from="1315" to="1329" />
			<date type="published" when="2016-07">July 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A new perceptually motivated MVDR-based acoustic front-end (PMVDR) for robust automatic speech recognition</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">H</forename><surname>Yapanel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H L</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="142" to="152" />
			<date type="published" when="2008-02">Feb. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Robust speech recognition using temporal masking and thresholding algorithma</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Stern</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-09">Sept. 2014</date>
			<biblScope unit="page" from="2734" to="2738" />
		</imprint>
	</monogr>
	<note>in INTERSPEECH-2014</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Integrating DNN-based and spatial clustering-based mask estimation for robust MVDR beamforming</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Higuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kinoshita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Acoust., Speech, Signal Processing</title>
				<imprint>
			<date type="published" when="2017-03">March 2017</date>
			<biblScope unit="page" from="286" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Robust MVDR beamforming using time-frequency masks for online/offline ASR in noise</title>
		<author>
			<persName><forename type="first">T</forename><surname>Higuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Acoust., Speech, Signal Processing</title>
				<imprint>
			<date type="published" when="2016-03">March 2016</date>
			<biblScope unit="page" from="5210" to="5214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved MVDR Beamforming Using Single-Channel Mask Prediction Networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH-2016</title>
				<imprint>
			<date type="published" when="2016-09">Sept 2016</date>
			<biblScope unit="page" from="1981" to="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic selection of thresholds for signal separation algorithms based on interaural delay</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Eom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH-2010</title>
				<imprint>
			<date type="published" when="2010-09">Sept. 2010</date>
			<biblScope unit="page" from="729" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Two-microphone source separation algorithm based on statistical modeling of angle distributions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Khawand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Acoustics, Speech, and Signal Processing</title>
				<imprint>
			<date type="published" when="2012-03">March 2012</date>
			<biblScope unit="page" from="4629" to="4632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sound source separation algorithm using phase difference and angle distribution modeling near the target</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Chin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH-2015</title>
				<imprint>
			<date type="published" when="2015-09">Sept. 2015</date>
			<biblScope unit="page" from="751" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sound source separation using phase difference and reliable mask selection selection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<date type="published" when="2018-04">April 2018</date>
			<biblScope unit="page" from="5559" to="5563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-style training for robust isolated-word speech recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lippmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
				<imprint>
			<date type="published" when="1987-04">Apr 1987</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="705" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Spectral distortion model for training phasesensitive deep-neural networks for far-field speech recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nongpiur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<date type="published" when="2018-04">April 2018</date>
			<biblScope unit="page" from="5729" to="5733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Two-stage data augmentation for low-resourced speech recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsakalidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2016-1386</idno>
		<ptr target="http://dx.doi.org/10.21437/Interspeech.2016-1386" />
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH-2016</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2378" to="2382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2019-2680</idno>
		<ptr target="http://dx.doi.org/10.21437/Interspeech.2019-2680" />
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="2613" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Robust speech recognition using small power boosting algorithm</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Automatic Speech Recognition and Understanding Workshop</title>
				<imprint>
			<date type="published" when="2009-12">Dec. 2009</date>
			<biblScope unit="page" from="243" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient implementation of the room simulator for training deep neural network acoustic models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Variani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2018-2566</idno>
		<ptr target="http://dx.doi.org/10.21437/Interspeech.2018-2566" />
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH-2018</title>
				<imprint>
			<date type="published" when="2018-09">Sept 2018</date>
			<biblScope unit="page" from="3028" to="3032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Vocal tract length perturbation (vtlp) improves speech recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learn. (ICML) Workshop on Deep Learn. Audio, Speech, Lang. Process</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2019-3216</idno>
		<ptr target="https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi" />
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)</title>
				<meeting><address><addrLine>Savannah, GA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Horovod: fast and easy distributed deep learning in TensorFlow</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Balso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05799</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ibm</forename><surname>Ibm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lsf</forename><surname>Spectrum</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Version 10 Release 1.0, Configuration Reference</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Data augmentation for deep neural network acoustic modeling</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
				<imprint>
			<date type="published" when="2015-09">Sept 2015</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1469" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Vocal tract length normalization for large vocabulary continuous speech recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<idno>CMU-CS-97-148</idno>
		<ptr target="https://www.lti.cs.cmu.edu/sites/default/files/CMU-LTI-97-150-T.pdf" />
		<imprint>
			<date type="published" when="1997-05">May 1997</date>
		</imprint>
		<respStmt>
			<orgName>School of Computer Science, Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Power-law nonlinearity with maximally uniform distribution criterion for improved neural network training in automatic speech recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gowda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
				<imprint>
			<date type="published" when="2019-12">Dec. 2019</date>
		</imprint>
	</monogr>
	<note>accepted</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Feature extraction for robust speech recognition based on maximizing the sharpness of the power distribution and on power flooring</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Acoustics, Speech, and Signal Processing</title>
				<imprint>
			<date type="published" when="2010-03">March 2010</date>
			<biblScope unit="page" from="4574" to="4577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Power-normalized cepstral coefficients (pncc) for robust speech recognition</title>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Acoustics, Speech, and Signal Processing</title>
				<imprint>
			<date type="published" when="2012-03">March 2012</date>
			<biblScope unit="page" from="4101" to="4104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Signal processing for robust speech recognition motivated by auditory processing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010-12">Dec. 2010</date>
			<pubPlace>Pittsburgh, PA USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Feature extraction for robust speech recognition using a power-law nonlinearity and power-bias subtraction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH-2009</title>
				<imprint>
			<date type="published" when="2009-09">Sept. 2009</date>
			<biblScope unit="page" from="28" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Zero mq</title>
		<ptr target="http:zeromq.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Monotonic chunkwise attention</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Hko85plCW" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018-04">Apr. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">attention based on-device streaming speech recognition with large speech corpus</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gowda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
				<imprint>
			<date type="published" when="2019-12">Dec. 2019</date>
		</imprint>
	</monogr>
	<note>accepted</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">RETURNN: the RWTH extensible training framework for universal recurrent neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Doetsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kulikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>SchlÃ¼ter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Acoust., Speech, and Signal Processing</title>
				<imprint>
			<date type="published" when="2017-03">March 2017</date>
			<biblScope unit="page" from="5345" to="5349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Improved training of end-to-end attention models for speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>SchlÃ¼ter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2018-1616</idno>
		<ptr target="http://dx.doi.org/10.21437/Interspeech.2018-1616" />
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH-2018</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multitask multi-resolution char-to-bpe cross-attention decoder for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gowda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2019-3216</idno>
		<ptr target="http://dx.doi.org/10.21437/Interspeech.2019-3216" />
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH-2019</title>
				<meeting><address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09">Sept. 2019</date>
			<biblScope unit="page" from="2783" to="2787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Improved multi-stage training of online attention-based encoder-decoder models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gowda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
				<imprint>
			<date type="published" when="2019-12">Dec. 2019</date>
		</imprint>
	</monogr>
	<note>accepted</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>III-1310-III-1318</idno>
		<ptr target="http://dl.acm.org/citation.cfm?id=3042817.3043083" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on International Conference on Machine Learning</title>
				<meeting>the 30th International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
	<note>ser. ICML&apos;13. JMLR.org</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">On using monolingual corpora in neural machine translation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Â¸</forename><surname>GÃ¼lc Â¸ehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1503.03535</idno>
		<ptr target="http://arxiv.org/abs/1503.03535" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A comparison of techniques for language model integration in encoder-decoder speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Spoken Language Technology Workshop (SLT)</title>
				<imprint>
			<date type="published" when="2018-12">Dec 2018</date>
			<biblScope unit="page" from="369" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">librosa: Audio and music signal analysis in python</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcvicar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Python in Science Conference</title>
				<editor>
			<persName><forename type="first">K</forename><surname>Huff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</editor>
		<meeting>the 14th Python in Science Conference</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Neural network based spectral mask estimation for acoustic beamforming</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Drude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<date type="published" when="2016-03">March 2016</date>
			<biblScope unit="page" from="196" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin ; I. Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">R</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">RWTH ASR systems for librispeech: Hybrid vs attention -w/o data augmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>LÃ¼scher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kitza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>SchlÃ¼ter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1905.03072" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1905">1905.03072. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
