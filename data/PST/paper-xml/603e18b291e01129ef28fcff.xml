<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Topic Modelling Meets Deep Neural Networks: A Survey</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-02-28">28 Feb 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">He</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Data Science and Artificial Intelligence</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dinh</forename><surname>Phung</surname></persName>
							<email>dinh.phung@monash.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Data Science and Artificial Intelligence</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Viet</forename><surname>Huynh</surname></persName>
							<email>viet.huynh@monash.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Data Science and Artificial Intelligence</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Jin</surname></persName>
							<email>yuan.jin@monash.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Data Science and Artificial Intelligence</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lan</forename><surname>Du</surname></persName>
							<email>lan.du@monash.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Data Science and Artificial Intelligence</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wray</forename><surname>Buntine</surname></persName>
							<email>wray.buntine@monash.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Data Science and Artificial Intelligence</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Topic Modelling Meets Deep Neural Networks: A Survey</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-02-28">28 Feb 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2103.00498v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Topic modelling has been a successful technique for text analysis for almost twenty years. When topic modelling met deep neural networks, there emerged a new and increasingly popular research area, neural topic models, with over a hundred models developed and a wide range of applications in neural language understanding such as text generation, summarisation and language models. There is a need to summarise research developments and discuss open problems and future directions. In this paper, we provide a focused yet comprehensive overview of neural topic models for interested researchers in the AI community, so as to facilitate them to navigate and innovate in this fastgrowing research area. To the best of our knowledge, ours is the first review focusing on this specific topic.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A powerful technique for text analysis, topic modelling has enjoyed success in various applications in machine learning, natural language processing (NLP), and data mining for almost two decades. A topic model is applied to a collection of documents and aims to discover a set of latent topics, each of which describes an interpretable semantic concept. Bayesian probabilistic topic models (BPTMs) have been the most popular and successful series of models, with latent Dirichlet allocation (LDA) the best known representative. A BPTM usually specifies a probabilistic generative model that generates the data of a document with a structure of latent variables sampled from pre-specified distributions connected by Bayes' theorem. Topics are captured by these latent variables. Like other Bayesian models, the learning of a BPTM is done by a (Bayesian) inference process (e.g. variational inference (VI) and Monte Carlo Markov Chain sampling).</p><p>Despite their success, conventional BPTMs started to show signs of fatigue in the era of big data and deep learning: 1) Given a specific BPTM, its inference process usually needs to be customised accordingly and the inference complexity may grow significantly as the model complexity grows. Unfortunately, it is also hard to automate the design of the inference processes. 2) The inference processes for conventional BPTMs can be hard to scale efficiently on large text collections or to leverage parallel computing facilities like GPUs.</p><p>3) It is usually inconvenient to integrate BPTMs with other deep neural networks (DNNs) for joint training.</p><p>With the recent developments in DNNs and deep generative models, there has been an emerging research direction which aims to leverage DNNs to boost performance, efficiency, and usability of topic modelling, named neural topic models (NTMs). With appealing flexibility and scalability, NTMs have gained a huge research following, with more than a hundred models and variants developed to date. Moreover, NTMs have been used in important NLP tasks including text generation, document summarisation, and translation, areas to which conventional topic models are harder to apply. Therefore, it is important to properly summarise research developments, categorise existing approaches, identify remaining issues, and discuss open problems and future directions. To the best of our knowledge, a comprehensive review specifically focusing on NTMs has not been published. In this paper, we would like to fill this gap by providing an overview for interested researchers who want to develop new NTMs and/or to apply NTMs in their domains. The notable contributions of our paper can be summarised as follows: 1) We propose a taxonomy of NTMs where we categorise existing models based on their backbone framework. 2) We first provide an informative discussion and overview of the background and evaluation methods for NTMs and conduct a focused yet comprehensive review, offering detailed comparisons of the variants in different categories of NTMs with applications. 3) We identify the limitations of existing methods and analyse the possible future research directions for NTMs.</p><p>The rest of this paper is organised as follows. Section 2 introduces the background, definitions, and evaluations. Section 3 and 4 review NTMs with various backbone frameworks. Section 5 discusses the applications. The current challenges and future directions are discussed in Section 6. Note that given the two page limit of references, we may only keep the most relevant papers to NTMs.</p><p>2 Background, Definition, and Evaluation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Background and Definition</head><p>The most important idea of a topic model is the modelling of the three key entities: document, word, and topic.</p><p>Notations of Data A topic model works on a corpus (i.e., a collection of documents), where a document, by its nature, can be represented as a sequence of words, which can be denoted by a vector of natural numbers, s ∈ N L , where L is the length of the document and s j ∈ {1, • • • , V } is the index in the vocabulary (with the size of V ) of the token for the j th (j ∈ {1, • • • , L}) word. A more common representation in topic modelling is the bag-of-words model, which represents a document by a vector of word counts, b ∈ Z V ≥0 , where b v indicates the occurrences of the vocabulary token v ∈ {1, • • • , V } in the document. One can readily obtain b for a document from its word sequence vector s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notations of Latent Variables</head><p>A central concept is a topic, which is usually interpreted as a cluster of words, describing a specific semantic meaning. A topic is or can be normalised into a distribution over the tokens in the vocabulary, named word distribution, t ∈ ∆ V , where ∆ V is a V dimensional simplex and t v indicates the weight or relevance of token v under this topic. Usually, a document's semantic content is assumed to be captured or generated by one or more topics shared across the corpus. Therefore, a document is commonly associated with a distribution (or a vector that can be normalised into a distribution) over K (K ≥ 1) topics, named topic distribution, z ∈ ∆ K , where z k indicates the weight of the k th topic for this document. We further use D, Z, and T to denote the corpus with all the document data, the collections of topic distributions of all the documents, and the collections of word distributions of all the topics, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notations of Architectures and Learning</head><p>With these notations, the task for a topic model is to learn the latent variables of T and Z from the observed data D. More formally, a topic model learns a projection parameterised by θ from a document's data to its topic distribution: z = θ(b) and a set of global variables for the word distributions of the topics: T . In order to learn these parameters, one generates or reconstructs a document's BoW data from its topic distribution, which is modelled by another projection parameterised by φ: b = φ(z, T ). Note that the majority of topic models belong to the category of probabilistic generative models, where z and b are latent and observed random variables assumed to be generated from certain distributions respectively. The projection from the latent variables to the observed ones is named the generative process, which we further denote as: b ∼ p b φ (z, T ) where z is sampled from the prior distribution z ∼ p z . While the inverse projection is named the inference process, denoted as z ∼ q z θ (b), where q z is the posterior distribution of z. For NTMs, these probabilities are typically parameterised by deep neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluation</head><p>It is still challenging to comprehensively evaluate and compare the performance of topic models including NTMs. Based on the nature and applications of topic models, the commonly-used metrics are as follows.</p><p>Predictive accuracy It has been common to measure the log-likelihood of a model on held-out test documents, i.e., the predictive accuracy. A more popular metric based on log-likelihood is perplexity, which captures how surprised a model is of new (test) data and is inversely proportional to average log-likelihood per word. Although log-likelihood or perplexity gives a straight numerical comparison between models, there remain issues: 1) As topic models are not for predicting unseen data but learning interpretable topics and representations of seen data, predictive accuracy does not reflect the main use of topic models. 2) Predictive accuracy does not capture topic quality. Predictive accuracy and human judgement on topic quality are often not correlated <ref type="bibr" target="#b6">[7]</ref>, and even sometimes slightly anti-correlated. 3) The estimation of the predictive probability is usually intractable for Bayesian models and different papers may apply different sampling or approximation techniques. For NTMs, the computation of log-likelihood is even more inconsistent, making it harder to compare the results across different papers.</p><p>Topic Coherence Experiments show topic coherence (TC) computed with the coherence between a topic's most representative words (e.g, top 10 words) is inline with human evaluation of topic interpretability <ref type="bibr" target="#b31">[32]</ref>. Various formulations have been proposed to compute TC, we refer readers to <ref type="bibr" target="#b48">[49]</ref> for more details. Most formulations require to compute the general coherence between two words, which are estimated based on word co-occurrence counts in a reference corpus. Regarding TC: 1) The ranking of TC scores may vary under different formulations. Therefore, it is encouraged to report TC scores of different formulations or report the average score.</p><p>2) The choice of the reference corpus can also affect the TC scores, due to the change of lexical usage, i.e, the shift of word distribution. For example, computing TC for a machine learning paper collection with a tweet dataset as reference may generate inaccurate results. Popular choices of the reference corpus are the target corpus itself or an external corpus such as a large dump of Wikipedia. 3) To exclude less interpretable "background" topics, one can select the topics (e.g., top 50%) with the highest TC and report the average score over those selected topics <ref type="bibr" target="#b68">[69]</ref> or to vary the proportion of the selected topics (e.g, from 10% to 100%) and plot TC score at each proportion <ref type="bibr" target="#b69">[70]</ref>.</p><p>Topic Diversity Topic diversity (TD), as its name implies, measures how diverse the discovered topics are. It is preferable that the topics discovered by a model describe different semantic topical meanings. Specifically, <ref type="bibr" target="#b10">[11]</ref> defines topic diversity to be the percentage of unique words in the top 25 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Downstream Application Performance</head><p>The topic distribution z of a document learned by a topic model can be used as the semantic representation of the document, which can be used in document classification, clustering, retrieval, visualisation, and elsewhere. For document classification, one can train a classification model with the topic distributions learned by a topic model as features and report the classification performance to compare different topic models. Document clustering can be conducted by two strategies: 1) Similar to classification, one can perform a clustering model (e.g. K-means with different numbers of clusters) on the topic distributions, such as in <ref type="bibr" target="#b69">[70]</ref>; 2) Alternatively, topics can be viewed as "soft" clusters of documents. Thus, one can use the most significant topic of a document (i.e., the topic with the largest weight in the topic distribution) as the cluster assignment, such as in <ref type="bibr" target="#b41">[42]</ref>. For document retrieval, we can use the distance of the topic distributions of two documents as their semantic distance and report retrieval accuracy as a metric of topic modelling <ref type="bibr" target="#b29">[30]</ref>. For qualitative analysis, a popular choice is to use visualisation techniques (e.g., t-SNE) on z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Topic Models with Amortised Variational Inference</head><p>The recent success of deep generative models such as variational autoencoders (VAEs) and amortised variational inference (AVI) <ref type="bibr">[28; 48]</ref> has shed light on extending the generative process and amortising the inference process of BPTMs, which is the most popular framework for NTMs. We name this series of models VAE-NTMs. The basic framework of a VAE follows the description in Section 2.1, where b and z are the observed and latent variables respectively and the generative and inference processes are modelled by the DNNbased decoder and encoder respectively. Following <ref type="bibr">[28; 48]</ref>, one can learn a VAE model by maximising the Evidence Lower BOund (ELBO) of the marginal likelihood of the BoW data b in terms of θ, φ, and T :</p><formula xml:id="formula_0">E z∼q z [log p(b | z)] − KL [q z p z ] ,</formula><p>where the RHS term is the Kullback-Leiber (KL) divergence. To compute/estimate gradients, tricks like reparameterisations are usually used to back-propagate gradients through the expectation in the LHS term and approximations are applied when the analytical form of the KL divergence is unavailable.</p><p>To adapt the VAE framework for topic modelling, there are two key questions to be answered: 1) Different from other applications, the input data of topic modelling has its unique properties, i.e., b is a high-dimensional, sparse, count-valued vector and s is a variable-length sequential data. How to deal with such data is the first question for designing a VAE topic model. 2) Interpretability of topics is extremely important in topic modelling. When it comes to a VAE model, how to explicitly or implicitly incorporate the word distributions of topics (i.e., T ) to interpret the latent representations or each dimension remains another question. <ref type="bibr" target="#b36">[37]</ref> proposes the first answers to the above questions, where the decoder is developed by specifying the data distribution p b as: p b := Multi softmax T T z + c . Here z ∈ R K models the topic distribution of a document, T ∈ R K×V models the words distributions of the topics, and c ∈ R V is the bias. That is to say, φ := {c}<ref type="foot" target="#foot_0">1</ref> and T := {T}. For the encoder which takes b as input and outputs (the samples of) z, the paper follows the original VAE: p z := N (0, diag K (1)); q z := N (µ, diag K (σ 2 )), where π = θ 0 (b), µ = θ 1 (π), and log σ = θ 2 (π). Here, θ := {θ 0 , θ 1 , θ 2 }, all of which are multi-layer perceptrons (MLPs). To better address the above questions, various configurations of the prior distribution p z , data distribution p b , posterior distribution q z , as well as different architectures of the decoder φ, encoder θ, word distributions of the topics T , have been proposed for VAE-NTMs.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the taxonomy of VAE-NTMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Variants of Distributions</head><p>Given the knowledge and experience of BPTMs, z's prior plays an important role in the quality of topics and document representations in topic models. Thus, various constructions of the prior distributions and their corresponding posterior distributions have been proposed for VAE-NTMs, aiming to be better alternatives to the normal distributions used in the original models.</p><p>Variants of Prior Distributions for z. Note that the application of Dirichlet is one of the key successes of LDA for encouraging topic smoothness and sparsity. For VAE-NTMs, one can apply: p z := Dir(α 0 ) and q z := Dir(θ(b)). However, it is difficult to develop an effective reparameterisation function (RF) for Dirichlet, making it hard to compute the gradient of the expectation in ELBO. Therefore, various approximations have been proposed. For example, <ref type="bibr" target="#b51">[52]</ref> uses the Laplace approximation, where Dirichlet samples are approximated by these sampled from a logistic normal distribution, whose mean and co-variance are specifically configured. Recall that the Dirichlet distribution can be simulated by normalising gamma variables, which still do not have noncentral differentiable RF but are easier to approximate. Several works have been proposed in this line, such as using the Weibull distribution as the approximation of gamma in <ref type="bibr" target="#b67">[68]</ref>, approximating the cumulative distribution function of gamma with an auxiliary uniform variable in <ref type="bibr" target="#b24">[25]</ref>, and leveraging the proposal function of a rejection sampler of the gamma distribution as the RF in <ref type="bibr" target="#b3">[4]</ref>. Recently, <ref type="bibr" target="#b54">[55]</ref> proposes to tackle this challenge by using the so-called rounded RF, which approximates Dirichlet samples by those drawn from the rounded posterior distribution. In addition to the above methods that are specific to topic modelling or VAEs, other general approaches for distributions without RF can also be used in VAE-NTMs, such as those in <ref type="bibr">[50; 38]</ref>. Other than Dirichlet, <ref type="bibr" target="#b35">[36]</ref> introduces a Gaussian softmax (GSM) function in the encoder: q z := softmax N (µ, diag K (σ 2 )) and <ref type="bibr" target="#b50">[51]</ref> proposes to use a logistic-normal mixture distribution for the prior of z. To further enhance the sparsity in z, <ref type="bibr" target="#b33">[34]</ref> introduces to use the sparsemax function to replace the softmax in GSM.</p><p>Nonparametric Prior for z. Bayesian Nonparametrics such as the Dirichlet processes and Indian Buffet Processes have been successfully applied in Bayesian topic modelling, enabling to automatically infer the number of topics (i.e., K). As a flexible construction of Dirichlet processes, the stickbreaking process (SBP) is able to generate probability vectors with infinite dimensions, which has been used to the prior of z in VAE-NTMs. Given z ∼ SBP(α 0 ), we have</p><formula xml:id="formula_1">z 1 = v 1 and z k = v k j&lt;k (1 − v j ) for k &gt; 1, where v k ∼ Beta(1, α 0 ).</formula><p>This procedure can be viewed as iteratively breaking a lengthone stick into multiple ones and the k th iteration breaks the stick at the point of v k . Although not for NTMs, <ref type="bibr" target="#b38">[39]</ref> proposes to use SBP to generate z for VAEs, where VI is done by various approximations to the beta distribution of v k with truncation. <ref type="bibr" target="#b42">[43]</ref> adapts this SBP construction for VAE-NTMs and also proposes to impose an SBP on the corpus level, which serves as the prior for the document-level SBP, form-  ing into a hierarchical model. In <ref type="bibr" target="#b35">[36]</ref>, the break points v k are generated from a posterior modelled by a recurrent neural network (RNN) with normal noises as input, making the model able to automatically infer K in a truncation-free manner. Recently, <ref type="bibr" target="#b61">[62]</ref> uses the truncated (gamma) negative binomial process to generate discrete vectors for z (i.e. each entry of z is equivalently generated by an independent Poisson distribution), which gives the model certain ability to be nonparametric.</p><p>Variants of Data Distribution p b . In addition to imposing different distributions on z, <ref type="bibr" target="#b70">[71]</ref> proposes to replace the multinomial data distribution used in other NTMs with the negative-binomial distribution to capture overdispersion: b ∼ NB(φ 0 (z), φ 1 (z)), where two separate decoders φ 0 and φ 1 are proposed to generate the two parameters of the negative-binomial distribution from z.</p><p>Variants of Word Distributions T . Conventionally, the collection of the word distributions of the topics T is a K ×V matrix, i.e., T ∈ R K×V with KV free parameters to learn. In NTMs, it has been popular to factorise the matrix into a product of topic and word embeddings, meaning that the relevance between a topic and a word is captured by their distance in the embedding space. This construction has been studied in details in <ref type="bibr">[26; 11; 13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Correlated and Structured Topics</head><p>Topics discovered by conventional topic models like LDA are usually independent. An important research direction is to explicitly capture topic correlations (e.g. pairwise relations between topics) or structures (e.g. tree structures of topics), which has been studied in NTMs as well. Following the framework of VAE with Householder flow, which enables to draw z from the normal posterior with a non-diagonal covariance matrix, <ref type="bibr" target="#b34">[35]</ref> develops a more efficient centralised transformation flow for NTMs, which is able to discover pairwise topic correlations by the covariance matrix. In terms of treestructured topics, <ref type="bibr" target="#b23">[24]</ref> introduces to generate a series of topics from the root to the leaf of a topic tree with a doubly-recurrent neural network <ref type="bibr" target="#b0">[1]</ref>. When applied in topic modelling, the gamma belief network (GBN) can be viewed as a Bayesian model that also discovers three-structured topics, whose inference is done by Gibbs sampling. <ref type="bibr" target="#b67">[68]</ref> introduces the NTM counterpart of GBN, which leverages AVI as the inference process and significantly improves the test time of GBN. <ref type="bibr" target="#b13">[14]</ref> proposes an structured VAE-NTM that discovers topics with respect to different aspects, specialising in modelling user reviews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">NTMs with Meta-data</head><p>Conventionally, topic models learn from documents in an unsupervised way. However, documents are usually associated with rich sets of meta-data on both document and word levels, such as document labels, authorships, and pre-trained word embeddings, which can be used to improve topic quality or document representation quality for supervised tasks (e.g., accuracy of predicting document meta-data). <ref type="bibr" target="#b5">[6]</ref> proposes a VAE-NTM that is able to incorporate various kinds of meta-data, where the BoW data b of a document and its labels (e.g., sentiment) are generated with a joint process conditioned on the document's covariates (e.g., publication year) in the decoder and the encoder generates z by conditioning on all types of data of the document: BoW, covariates, and labels. Instead of specifying the generative model as a directed network as in most of topic models, <ref type="bibr" target="#b28">[29]</ref> introduces the logistic LDA model whose generative process can be viewed as an undirected graph. In addition to the BoW data, a document's label is also an observed variable in the graph. Following a few assumptions of factorisation in the generative process, the paper manually specifies the complete conditional distributions in the graph with the interactions between the latent variables captured by neural networks. The inference is done by the mean-field VI and z in the model is further trained to be more discriminative for the classification of labels. Given a set of documents with labels, <ref type="bibr" target="#b59">[60]</ref> uses a VAE-NTM to model a document's BoW data and an RNN classifier to predict a document's label based on its sequential data in a joint training process. The paper combines the two models by introducing an attention mechanism in the RNN which takes documents' topics into account. <ref type="bibr" target="#b1">[2]</ref> proposes to incorporate relational graphs (e.g. citation graph) of documents into NTMs, where the topic distributions of two document are fed into a network with MLPs to predict whether they should be connected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">NTMs for Short Texts</head><p>Texts generated on the internet (e.g., tweets, news headlines and product reviews) can be short, meaning that each indi-vidual document contains insufficient word co-occurrence information. This results in degraded performance for both BPTMs and NTMs. To tackle this issue, one can limit a model's capacity and to enhance the contextual information of short texts. <ref type="bibr" target="#b66">[67]</ref> proposes a combination of an NTM and a memory network for short text classification in a similar spirit to <ref type="bibr" target="#b59">[60]</ref>. The main difference is the memory network instead of RNN is responsible for classification, which is informed by the topic distributions learned by the NTM. To enhance the contextual information of short documents, <ref type="bibr" target="#b73">[74]</ref> proposes an NTM whose encoder is a graph neural network (GNN) taking the biterms graph of the words in sampled documents as inputs and outputting the topic distribution for the whole corpus. The model also learns a decoder that reconstructs the input biterms graph. Despite the novel idea, the model might not be able to generate the topic distribution of an individual document. To limit a short document to focus on several salient topics, <ref type="bibr" target="#b32">[33]</ref> introduces to use the Archimedean copulas to regularise the discreteness of topic distributions for short texts. <ref type="bibr" target="#b14">[15]</ref> proposes an NTM with reinforced content by limiting the number of the active topics for each short document and informing the word distributions of the topics by using pretrained word embeddings. <ref type="bibr" target="#b62">[63]</ref> introduces an NTM with vector quantisation over z, i.e., a document's topic distribution can only be one vector in the learned dictionary in the vector quantisation process. In addition to maximising the likelihood of the input documents, the paper introduces to minimise the likelihood of the negatively-sampled "fake documents". Although not directly addressing the short text problem for topic modelling, <ref type="bibr" target="#b20">[21]</ref> introduces NTMs for modelling microblog conversations, by leveraging their unique meta data and structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Sequential NTMs</head><p>The flexibility of VAE-NTMs enables to leverage various neural network architectures for the encoder and decoder.</p><p>With the help of sequential networks like RNNs, unlike other NTMs working with BoW data (i.e., b), sequential NTMs (SNTMs) usually take sequences of words of documents (i.e., s) and are able to capture the orders of words, sentences, and topics. <ref type="bibr" target="#b39">[40]</ref> proposes an SNTM working with s, which samples a topic for each sentence of an input document according to z and then generates the word sequence of the sentence with an RNN conditioned on the sentence's topic. Note that z is attached to a document and shared across all its sentences. In <ref type="bibr" target="#b65">[66]</ref>, given s, a word's topic is conditioned on its previous word's and this order dependency is captured by a long short-term memory (LSTM) model. At the similar period of time, <ref type="bibr" target="#b11">[12]</ref> independently proposes an SNTM whose generative process is similar to <ref type="bibr" target="#b65">[66]</ref>, with an additional variable modelling stop words and several variants in the inference process. Recently, <ref type="bibr" target="#b43">[44]</ref> proposes to use an LSTM with attentions as the encoder taking s as input, where the attention incorporates topical information with a context vector that is constructed by topic embeddings and document embeddings. <ref type="bibr" target="#b46">[47]</ref> introduces an SNTM that is related to <ref type="bibr" target="#b11">[12]</ref>, where instead of marginalising out the discrete topic assignments, the paper proposes to generate them from an RNN model. This helps to avoid using reparameterisation tricks in the variational in-ference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">NTMs with Pre-trained Language Models</head><p>Recently, pre-trained transformer-based language models such as BERT are becoming ubiquitous in NLP. Pre-trained on large corpora, such models usually have a fine-grained ability to capture aspects of linguistic context, which can be partially represented by contextual word embeddings. These contextual word embeddings can provide richer context information than BoW or sequential data, which has been recently used to assist the training of topic models. Instead of using the BoW or sequential data of a document as the input of the encoder, <ref type="bibr" target="#b2">[3]</ref> proposes to use the document embedding vector generated by Sentence-BERT <ref type="bibr" target="#b45">[46]</ref> and to keep the remaining part of an NTM the same as <ref type="bibr" target="#b51">[52]</ref>. <ref type="bibr" target="#b53">[54]</ref> shows that the clusters obtained by performing clustering algorithms (e.g., Kmeans) on the contextual word embeddings generated by various pre-trained models can be interpreted as topics, similar to those discovered by LDA. Having similar ideas with <ref type="bibr">[67; 60]</ref>, <ref type="bibr" target="#b7">[8]</ref> proposes to combine an NTM with a fine-tuned BERT model by concatenating the topic distribution and the learned BERT embedding of a document as the features for document classification. <ref type="bibr" target="#b21">[22]</ref> proposes an NTM learned by distilling knowledge from a pre-trained BERT model. Specifically, given a document, the BERT model generates the predicted probability for each word then the paper introduces to average those probabilities to generate a pseudo BoW vector for the document. An NTM following <ref type="bibr" target="#b5">[6]</ref> is used to reconstruct both the actual and pseudo BoW data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">NTMs based on Other Frameworks</head><p>Besides VAE-NTMs, there are other frameworks for NTMs that also draw research attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">NTMs based on Autoregressive Models</head><p>VAE-NTMs gained popularity after VAEs were invented. Before that, NTMs based on the autoregressive framework had been studied. Specifically, <ref type="bibr" target="#b29">[30]</ref> proposes an autoregressive NTM, named DocNADE, similar to the spirit of RNNs, where the predictive probability of a word in a document is conditioned on its hidden state, which is further conditioned on the previous words. A hidden unit can be interpreted as a topic and a document's hidden states capture its topic distribution. The learning is done by maximising the likelihood of the input documents. Recently, <ref type="bibr" target="#b17">[18]</ref> extends DocNADE by introducing a structure similar to the bi-directional RNN, which allows to model bi-directional dependencies between words. <ref type="bibr" target="#b18">[19]</ref> combines DocNADE with an LSTM for incorporating external knowledge. <ref type="bibr" target="#b19">[20]</ref> extends DocNADE into the life long learning settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">NTMs based on Generative Adversarial Nets</head><p>Besides VAEs, generative adversarial networks (GANs) are another popular series of deep generative models. Recently, there are a few attempts on adapting the GAN framework for topic modelling. <ref type="bibr" target="#b56">[57]</ref> proposes a GAN generator that takes a random sample of the Dirichlet distribution as a topic distribution z and generates the word distributions of a "fake" document conditioning on z. A discriminator is introduced to distinguish between generated word distributions and real word distributions obtained by normalising the TF-IDF vectors of real documents. Although the proposed model is able to discover interpretable topics, it cannot learn topic distributions for documents. To address this issue, <ref type="bibr" target="#b55">[56]</ref> introduces an additional encoder that learns z for a given document. Moreover, z is concatenated with the word distribution of a document as a real datum and z is concatenated with the generated word distribution as a fake datum. The discriminator is designed to distinguish between the real and fake ones. <ref type="bibr" target="#b22">[23]</ref> further extends the above model with a CycleGAN framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">NTMs based on Graph Neural Networks</head><p>Instead of viewing a document as a sequence or bag of words, one can consider the graph presentations of a corpus of documents. This perspective enables leveraging a variety of GNNs to discover latent topics. As discussed in Section 3.4, <ref type="bibr" target="#b73">[74]</ref> views a collection of documents as a biterm word graph. While <ref type="bibr">[65; 73]</ref> model a corpus by a bipartite graph with documents and words as two separate parties and connected by the occurrences of words in documents. For the former, it directly uses the word occurrences of documents as the weights of the connections between them and for the latter, it uses TF-IDF values instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">NTMs based on Other Frameworks</head><p>In addition to the above frameworks, other kinds of NTMs have also been developed. An NTM is developed in <ref type="bibr" target="#b4">[5]</ref> that takes n-gram embeddings (obtained from word embeddings) and a document index as input and then predicts whether an ngram is in the document. <ref type="bibr" target="#b8">[9]</ref> proposes an autoencoder model for NTMs where the neurons in the hidden layer of the autoencoder compete with each other, focusing them to be specialised in recognising specific data patterns. <ref type="bibr" target="#b44">[45]</ref> proposes an NTM based on matrix factorisation. <ref type="bibr" target="#b15">[16]</ref> proposes a reinforcement learning framework for NTMs, where the encoder and decoder of an NTM are kept. In addition, an agent takes actions to select the topical-coherent words from a document and uses the selected words as the input document for the encoder. The reward to the agent is the topic coherence of the reconstructed document from the decoder. <ref type="bibr" target="#b40">[41]</ref> adapts the framework of Wasserstein auto-encoders (WAEs), which minimises the Wasserstein distance between reconstructed documents from the decoder and real documents, similarly to VAE-NTMs. <ref type="bibr" target="#b69">[70]</ref> recently introduces a NTM based on optimal transport, which directly minimises the optimal transport distance between the topic distribution learned by an encoder and the word distribution of a document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Applications of NTMs</head><p>Although just recently developed, NTMs have been actively used in various applications. Compared with conventional topic models, NTMs have the appealing advantages of flexibility: 1) NTMs are flexible in representing topic distributions of documents and word distributions of topics with either probability vectors or embeddings, and are more easily incorporated into broader models.</p><p>2) The inference process of NTMs can usually be formulated as an optimisation process with gradients, which is more conveniently integrated with other DNN models for joint training. Many DNN models used for language such as RNNs, transformers, and attention might not be able to capture longrange dependency well. On the contrary, working on BoW data, NTMs are good at learning global semantic representations for long texts, which can serve complementary information to the above models. This leads to a wide range of applications of NTMs in NLP such as language models <ref type="bibr">[31; 58; 64; 17; 27]</ref>, text generation <ref type="bibr">[53; 59]</ref>, and summarisation <ref type="bibr">[10; 72; 61]</ref>. Due to the space limit of the references, a detailed list of application papers are omitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>In this paper, we have reviewed neural topic models, the most popular research trend of topic modelling in the deep learning era. A variety of NTMs based on different frameworks have been developed and due to the appealing flexibility, effectiveness, and efficiency, NTMs show a promising potential in a range of applications. In addition to providing an overview of existing approaches of NTMs, we would like to discuss the following challenges and opportunities. 1) Better evaluation: As stated in Section 2.2, evaluation of topic models is challenging. This is mainly because there has not been a unified system of evaluation metrics, making the comparisons across different NTMs harder due to the variety of frameworks, architectures and datasets. For example, VAE-NTMs calculate perplexity using the ELBO, attached to the models with variational inference, which cannot be compared with models without ELBO. Also for topic coherence and downstream performance, the evaluation processes, metrics, settings usually vary in different papers. As a topic model should be evaluated with comprehensive metrics, it could be tendentious to only use one kind of metric (e.g., topic coherence), which can reflect just one aspect of a model. Therefore, unified platforms and benchmarks for NTMs are needed.</p><p>2) Richer architectures and applications: Compared to BPTMs, NTMs offer better flexibility for representing topic distributions for documents and word distributions for topics. Particularly, projecting documents, topics, and words into a unified embedding space transforms the thinking of the relationships between the three. Given this flexibility, NTMs are expected to get integrated with the most recent neural architectures and play a unique role in richer applications. 3) More external knowledge: With the development of topic models including NTMs, people have not stopped seeking to leverage external knowledge to help the learning, from document meta-data to pre-trained word embeddings. Recentlyproposed pre-trained language models (e.g., BERT) provide more advanced, finer-grained, and higher-level representations of semantic knowledge (e.g., contextual word embeddings over global embeddings), which can be leveraged in NTMs to boost performance. Although the marriage between NTMs and language models is still an emerging area, we expect to see more developments in this important direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A taxonomy of the papers regarding neural topic models</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">With a slight abuse of notation, we use θ and φ to denote the projections or the parameters of the projections.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tree-structured decoding with doubly-recurrent neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Alvarez-Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural relational topic models for scientific article analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pre-training is a hot topic: Contextualized document embeddings improve topic coherence</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Terragni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hovy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Decoupling sparsity and smoothness in the Dirichlet variational autoencoder topic model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Burkhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kramer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A novel neural topic model and its supervised extension</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural models for documents with metadata</title>
		<author>
			<persName><forename type="first">D</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Reading tea leaves: How humans interpret topic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">TopicBERT for energy efficient document classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Runkler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">KATE: K-competitive autoencoder for text</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Enhancing extractive text summarization with topic-aware graph neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Topic modeling in embedding spaces</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Topi-cRNN: A recurrent neural network with long-range semantic dependency</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Coherence-aware neural topic modeling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structured neural topic models for reviews</title>
		<author>
			<persName><forename type="first">B</forename><surname>Esmaeili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Van De Meent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<title level="m">Context reinforced neural topic modeling over short texts</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural topic model with reinforcement learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pergola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent hierarchical topic-guided RNN for language generation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Document informed neural autoregressive topic models with distributional prior</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Buettner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Texttovec: Deep contextualized neural autoregressive topic models of language with distributed compositional prior</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Buettner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural topic modeling with continual lifelong learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Runkler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schuetze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Interaction-aware topic model for microblog conversations through network embedding and user attention</title>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving neural topic models using knowledge distillation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Hoyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural topic modeling with cycle-consistent adversarial training</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Treestructured neural topic model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Isonuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sakata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dirichlet variational autoencoder</title>
		<author>
			<persName><forename type="first">W</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I.-C</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">I</forename><surname>Choi</surname></persName>
		</author>
		<title level="m">Continuous semantic topic embedding model using variational autoencoder</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Topic structure-aware neural language model: Unified language model that maintains word and topic ordering by their embedded representations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kawamae</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Discriminative topic modeling with logistic LDA</title>
		<author>
			<persName><forename type="first">I</forename><surname>Korshunova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fedoryszak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A neural autoregressive topic model</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Topically driven neural language model</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Copula guided neural topic modelling for short texts</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sparsemax and relaxed Wasserstein for topic sparsity</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Neural variational correlated topic modeling</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Discovering discrete latent topics with neural variational inference</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neural variational inference for text processing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Reparameterization gradients through acceptance-rejection sampling algorithms</title>
		<author>
			<persName><forename type="first">C</forename><surname>Naesseth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Linderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Stick-breaking variational autoencoders</title>
		<author>
			<persName><forename type="first">E</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Melnyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<title level="m">Sengen: Sentence generating neural variational topic model</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Topic modeling with Wasserstein autoencoders</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Improving topic models with latent feature word representations</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Billingsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Nonparametric topic modeling with neural inference</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Panwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shailabh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><surname>Tan-Ntm</surname></persName>
		</author>
		<title level="m">Topic attention networks for neural topic modeling</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Neural sparse topical coding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence embeddings using siamese BERT-networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A discrete variational recurrent topic model without the reparametrization trick</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rezaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ferraro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Exploring the space of topic coherence measures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Röder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Both</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hinneburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The generalized reparameterization gradient</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Titsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Topic modeling using variational auto-encoders with Gumbel-softmax and logistic-normal mixture distributions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cristo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-F</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Autoencoding variational inference for topic models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A topic augmented text generation model: Joint learning of semantics and structural features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Topic modeling with contextualized word representation clusters. arXiv</title>
		<author>
			<persName><forename type="first">L</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mimno</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning VAE-LDA models with rounded reparameterization trick</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Neural topic modeling with bidirectional adversarial training</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<title level="m">ATM: Adversarial-neural topic model. Information Processing &amp; Management</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Topic compositional neural language model</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<editor>AISTATS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Topic-guided variational autoencoder for text generation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="166" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Neural topic model with attention for supervised learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Friendly topic assistant for transformer based abstractive summarization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Neural mixed counting models for dispersed topic discovery</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Short text topic modeling with topic distribution quantization and negative sampling decoder</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Dirichlet variational autoencoder for text modeling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Graph attention topic modeling network</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Latent LSTM allocation: Joint clustering and non-linear dynamic modeling of sequence data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Topic memory networks for short text classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Whai: Weibull hybrid autoencoding inference for deep topic modeling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Dirichlet belief networks for topic structure learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Neural topic model via optimal transport</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Buntine</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Variational autoencoders for sparse and overdispersed discrete data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<title level="m">Topicaware abstractive text summarization. arXiv</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Neural topic modeling by incorporating document relationship graph</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Graphbtm: Graph enhanced autoencoded variational inference for biterm topic model</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
