<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Dynamic Network Embedding for Link Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Taisong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Acoustics</orgName>
								<orgName type="laboratory">Key Laboratory of Speech Acoustics and Content Understanding</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country>101408 China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Institute of Acoustics</orgName>
								<orgName type="laboratory">Key Laboratory of Speech Acoustics and Content Understanding</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Florida State University</orgName>
								<address>
									<postCode>32304</postCode>
									<settlement>Tallahassee</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<postCode>60607</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Yan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Acoustics</orgName>
								<orgName type="laboratory">Key Laboratory of Speech Acoustics and Content Understanding</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yonghong</forename><surname>Yan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country>101408 China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Xinjiang Technical Institute of Physics and Chemistry</orgName>
								<orgName type="laboratory">Xinjiang Key Laboratory of Minority Speech and Language Information Processing</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Wulumuqi</settlement>
									<country>830011 China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Dynamic Network Embedding for Link Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D3CA1A774EE10E5F14135A21E198FA6E</idno>
					<idno type="DOI">10.1109/ACCESS.2018.2839770</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2018.2839770, IEEE Access This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2018.2839770, IEEE Access This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2018.2839770, IEEE Access</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>N ETWORK structured data can effectively model vari- ous types of linked data in the real world, in which nodes represent entities and edges indicate connections. Mining information from network is an important problem and is ubiquitous in real-world applications. For example, the recommendation system in Youtube or Amazon aims to predict the potential videos/products users can be interested in, which can be modeled as a user-item link prediction problem. The key point for these applications is how to learn useful information from network structures. One of the most effective representation learning approaches for networked data is network embedding, which aims to map the network into a low-dimensional space. Such a network embedding method is proved to be very effective in link prediction or classification tasks. It is generally applied to static networks such as bibliographic network, email network and online social network, etc.</p><p>As introduced in <ref type="bibr" target="#b0">[1]</ref>, few networks in the real-world are actually static but keep evolving with time. For instance, bibliographical network <ref type="bibr" target="#b1">[2]</ref>, online social network <ref type="bibr" target="#b2">[3]</ref> and email network <ref type="bibr" target="#b3">[4]</ref> change non-linearly from each snapshots. Representation learning for dynamic network is not an easy task, which needs to model both the network structure and temporal information properly to preserve network information.</p><p>In the past decades, many network embedding methods have been proposed. Most of these methods employ shallow models, such as IsoMAP <ref type="bibr" target="#b4">[5]</ref>, Laplacian Eigenmaps (LE) <ref type="bibr" target="#b5">[6]</ref> and LINE <ref type="bibr" target="#b6">[7]</ref>. They can efficiently extract information from a network since the shallow model structure has less cost in computation. However, the underlying network structure from high dimension to low-dimensional space is highly non-linear <ref type="bibr" target="#b7">[8]</ref>, which cannot be effectively captured by these methods because of their limited representation abilities. To solve this problem, some of the deep models such as SDNE <ref type="bibr" target="#b8">[9]</ref>, node2vec <ref type="bibr" target="#b9">[10]</ref> and DeepWalk <ref type="bibr" target="#b10">[11]</ref> have been proposed. Due to the deep neural network structure, these methods have a high ability to model the non-linear transformation of network structure. They can preserve linkage information for each node as well. However, these works have thus far focused on representation learning for static networks, in which they only have singular snapshot of nodes and relationships. To the best of our knowledge, dynamic network embedding is still an open problem to this context so far. Some network evolving methods like tRBM <ref type="bibr" target="#b11">[12]</ref>, ctRBM <ref type="bibr" target="#b2">[3]</ref> are competent to capture the evolution pattern. However, they have limited ability to predict links due to their shallow learning process.</p><p>Learning embedding from dynamic networks faces the following great challenges:</p><p>• Incorporating structure information: most embedding approaches try to preserve information of network by using its current structure. How to incorporate historical and current dataset to learn future representation is a great challenge for network embedding.</p><p>• Highly non-linear transformations: evolving nonlinearly over time is commonly seen in dynamic networks with periodic fluctuations. How to catch these non-linearities in dynamic linkage changing patterns for network representation learning is a difficult problem.</p><p>• Node interactions: a network can be presented as an adjacency matrix in each time slice, where the node local neighborhood structure can be effectively captured by the row vectors corresponding to the nodes in the adjacency matrix. Generally, most existing network embedding methods take the matrix as an input on the premise that vectors are independent to each other. However, interactions between nodes also contain structure information. How to model the correlation between nodes along with the change of network is an important factor for structure preservation.</p><p>In order to address structure information incorporation problem, we propose to use historical linkage status and current network structure to model network evolving patterns. Then, employing the trained model to infer the future network structure. This is motivated by the recent success of dynamic network learning method, which has been demonstrated to have a powerful inference ability for link prediction <ref type="bibr" target="#b2">[3]</ref> . In particular, our proposed model deploys previous structures to learn the node presentations of future networks. After that, the node representations can be used to infer the next linkage status of network.</p><p>In order to capture the non-linear transformations from historical snapshots, we propose a new deep model to learn vertex representations for dynamic networks. This is motivated by the temporal deep learning model Recurrent Neural Network (RNN), which has achieved substantial success in modeling sequential data in various disciplines, e.g., natural language processing and speech recognition. We design a multi-layer architecture which consists of the encoder and decoder layers respectively. The encoder layer accepts sequence data input and learns the latent representation through multiple non-linear functions successively. Then, the output is fed to the up-layer for decoding. Since there are various non-linear functions in encoder and decoder layers, we can map the historical data into highly non-linear latent space. As a result, the proposed deep model is able to learn the complicated transformations of each vertex.</p><p>To preserve the information of node interactions, we further propose to exploit interaction proximity in the learning process. As is known that nodes contacted in the history tend to connect in the future network. The interaction proximity is designed to measure such contact closeness between two nodes. Technically, traditional RNN-based deep models take the input samples independently to each other. It is capable to capture the transition pattern for each node, but ignores the correlations between node vectors. Thus, the original deep model may fail to utilize the abundant node correlation information to model the evolving patterns and infer network structure. To resolve such a shortcoming, a novel network interaction proximity term is introduced in this paper, which measures the correlations between nodes. The network interaction proximity term greatly enriches the proposed deep dynamic network embedding model, and make it possible to capture network internal connection structure.</p><p>To demonstrate our model's potential in real world scenarios, we conduct experiments on various categories of real-world networks and evaluate its performance on link prediction task specifically. All these used network data are dynamic in a certain time period. The result shows that compared with the state-of-the-art and several existing baseline methods, the proposed method can infer the networks to be prominently better and achieve substantial gains on various networks. Such a result demonstrates that our algorithm has the capacity to capture the network dynamics the link prediction task.</p><p>Our contributions are summarized as follows:</p><p>(1) We propose a deep learning model to perform embedding on dynamic networks. The method is able to capture the non-linear transformations of nodes and preserve dynamic networks' structure. To the best of our knowledge, we are among the first to learn dynamic network representations.</p><p>(2) The proposed new deep architecture, which can be employed as a generative model, to infer dynamic network embedding by using historical snapshots. It demonstrates a better performance on link prediction than traditional embedding methods and generative models.</p><p>(3) To optimize our deep model, we further introduce the interaction proximity concept, which preserves the information of node interactions along an evolving period. The results indicate that the deep model achieves substantial gains when interaction proximity has been considered.</p><p>The rest of the paper is arranged as follows. We will first define several concepts in Section II. Section III discusses our proposed dynamic network embedding method for link prediction. Section IV describes the experimental results, while the conclusions are presented in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROBLEM DEFINITION</head><p>We first generally declare the notations used in this paper, then we formally define several important terminologies and introduce the formulation of dynamic network embedding problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. NOTATIONS</head><p>In the sequel of this paper, we will use the lower case letters (e.g., x) to represent scalars, lower case bold letters (e.g., x) to denote column vectors, bold-face upper case letters (e.g., X) to denote matrices, and upper case calligraphic letters (e.g., X ) to denote sets. Given a matrix X, we denote X(i, :) and X(:, j) as the i th row and j th column of matrix X respectively. The (i th , j th ) entry of matrix X can be denoted as either X(i, j) or X i,j , which will be used interchangeably in this paper. We use X and x to represent the transpose of matrix X and vector x. For vector x, we represent its L p -norm as x p = ( i |x i | p ) 1 p . The Frobenius norm of matrix X can be represented as</p><formula xml:id="formula_0">X F = ( i,j |X i,j | 2 ) 1 2 .</formula><p>The element-wise product of vectors x and y of the same dimension is represented as x y, while the element-wise product of matrices X and Y of the same dimensions is represented as X Y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. DEFINITIONS</head><p>DEFINITION 1: (Network) A network can be represented by a graph: G =&lt; V, E &gt;, where V = {v 1 , ..., v n } refers to a set of nodes, and E ⊆ |V| × |V| represents a set of links among the nodes. Each edge e ⊆ E is an unordered pair e = (v i , v j ) and is associated with a weight w ij , which indicates the strength of the relation. For unweighted graph w ij = 1 and for weighted graph, w ij &gt; 0.</p><p>Considering the input of our deep model is a matrix, we denote the adjacency matrix as X ∈ R n×n . Each row of the matrix indicates a user's link vector, which can be presented as X(i, :). Each element of X is written as X ij , which means the link state between node i and node j.</p><p>DEFINITION 2: (Dynamic Network) In dynamic networks, we denote a series of snapshots as {G t-N , ..., G t-1 , G t }, which represent the state of the network at each time slice (N denotes the target time window size). We follow the dynamic network settings in <ref type="bibr" target="#b2">[3]</ref> that the nodes V remain constant while the edges E t change when network evolving. Hence, we can represent graph G at each time t as G t = (V, E t ).</p><p>The adjacency matrix in dynamic network is similar to the static network except the timestamps. For each time slice t, the adjacency matrix is presented as X t . The element is written as X t ij , which means the link state between node i and node j at time t.</p><p>In practice, a temporal deep model, like RNN, can be used for sequence data and model its pattern of changes. However, it treats each sample independently so that vertexes' interaction over time can not be captured for a dynamic network. Thus, we further proposed an interaction proximity that can optimize the temporal deep model by taking samples' correlation into account.</p><p>DEFINITION 3: (Interaction Proximity) The interaction proximity describes the closeness between vertexes. For v i and v j , if they have any linkage at any snapshot, there exists positive interaction proximity. If no edge is observed, their interaction proximity is 0.</p><p>Intuitively, people are more likely to contact with someone who has been acquainted before. Such a phenomenon has been observed in many fields. For example, in coauthorship network, researchers tend to cooperate with others who have published papers together. In mail network, most e-mails are sent to the people you have contacted before. The interaction proximity considers connections through all snapshots, then it use this contact frequency to define the acquaintance between users. Therefore, it can highly enrich the relationship of vertexes, and it is able to preserve contact information and alleviate network embedding problem.</p><p>With a deep model and interaction proximity, we investigate the problem of how to integrate them simultaneously to model the structure evolving for dynamic network representation learning. Formally, the problem is defined as follows:</p><p>DEFINITION 4: (Deep dynamic network embedding) Given a network with temporal information G, it can be sliced into a series snapshots as {G t-N , ..., G t-1 , G t }. The dynamic network embedding problem aims to learn the lowdimensional latent representations X t ∈ R n×d by using historical networks G h = {G t-N , ..., G t-1 }. The latent embeddings with dimension d |V| are able to capture and recover the network structure at time t. A general learning and inference process is illustrated in Figure <ref type="figure">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DDNE: DEEP DYNAMIC NETWORK EMBEDDING</head><p>In this section, we present a general framework, DDNE, which is capable of learning desirable node representations in dynamic networks. This framework is inspired by RNNsearch <ref type="bibr" target="#b12">[13]</ref>, which is proposed to cope with machine translation problem. Most of the neural machine translation models belong to a family of encoder-decoders <ref type="bibr" target="#b13">[14]</ref>  <ref type="bibr" target="#b14">[15]</ref>, which takes each sample independently for both encoder and decoder. However, our proposed method is well reshaped VOLUME 4, 2016 FIGURE 1: learning and inference process of link prediction and the correlations between samples are fully considered to preserve information of networks. In the subsection, we first introduce the gated recurrent unit. Then, we explicitly describe the proposed new architecture of deep dynamic embedding method. After that, we introduce the loss functions and the optimization of the algorithm. At last, we further discuss the training and inference of DDNE model.</p><p>Before introducing the deep dynamic embedding model, we define some of the terms and notations in Table <ref type="table">1</ref> which will be used later. Note that the adjacency vector contains input and target vectors, which is divided by different timestamps. the m-th layer bias of decoder c i the output of encoder for node i</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 1: Terms and Notations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. PRELIMINARY: GATED RECURRENT UNIT</head><formula xml:id="formula_1">h t-1 h t x t r t z t h t FIGURE 2: Framework of GRU[16]</formula><p>A gated recurrent unit (GRU) was proposed by Cho et al. <ref type="bibr" target="#b16">[17]</ref> to make each recurrent unit to adaptively capture dependencies of different time scales. Empirical experiments on sequential datasets demonstrated that the RNNs with the gating units (GRU-RNN and LSTM-RNN) clearly outperformed the traditional tanh-RNN in terms of prediction accuracy or convergence speed. Furthermore, the performance of GRU and LSTM is so comparable but the GRU generally makes faster progress than LSTM in terms of both the number of updates and actual CPU time <ref type="bibr" target="#b17">[18]</ref>. Thus, we take the GRU as encoder units to modeling the dynamic network data.</p><p>The GRU structure is illustrated in Figure <ref type="figure">2</ref>, and the functions are defined as follows:</p><formula xml:id="formula_2">         z t = σ(W z x t + U z h t-1 ) r t = σ(W r x t + U r h t-1 ) ht = tanh(Wx t + U(r t h t-1</formula><p>))</p><formula xml:id="formula_3">h t = (1 -z t ) h t-1 + z t ht</formula><p>The GRU can be treated as a black box. Given the current input x t and previous hidden state h t-1 , they merge two inputs and compute the current hidden state h t in some way. This mechanism provides an effective way to preserve historical information for each node, and we can use it to encode the network evolution process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. ENCODER-DECODER OF DDNE</head><p>In this paper, we proposed a deep architecture to perform dynamic network embedding, whose framework is shown in Figure <ref type="figure" target="#fig_1">3</ref>. In detail, to capture the historical dynamic pattern, we leverage a GRU to map the input sequence to a fixed-sized vector. This process can be treated as a GRU unfolds along with a series of time slices. We feed the input to the unit at each time slice and the unit calculates and updates a hidden state over time. Since GRU is known to learn problems with long range temporal dependencies and fast convergence, it makes the encoder efficient to capture the dynamic pattern by mapping the input to a highly nonlinear latent space. However, the learning process of GRU takes each sample independently, while the correlations between samples have not been considered. To address this problem, we propose the interaction proximity to exploit the pairwise closeness between vertexes. Specifically, the proposed interaction proximity tries to map two frequentlyconnected vertexes into similar latent space. This can be achieved by optimizing the similarity of the hidden states between two vertexes. By using GRU units and computing interaction proximity, the encoder can preserve the historical For the decoder part, we extend a Deep Neural Network (DNN) to embed the output of encoder into a hidden layer. This hidden layer preserves all structure informations of new network and condense each node's information into a k-dimension vector. The output of decoder is an inferable adjacency vector, which can be used to fit the new linkage state. We define a structure loss to describe the deviation between inference and new linkage. By jointly optimizing the interaction proximity and structure loss in the proposed encoder-decoder model, DDNE can preserve the highlynonlinear dynamic network structure and generate the new networks embedding well. In the following section, we will introduce how to realize the supervised deep model in detail.</p><formula xml:id="formula_4">GRU GRU GRU GRU GRU GRU V i V j Interaction Proximity c j c i Embeddings GRU GRU GRU GRU GRU GRU V i V V V j V V Interaction Proximity c j c c i Embeddings ENCODER DECODER ‫ݐ‬െܰ ሺ݅ǡ ǣ ሻ ‫ݐ‬െʹ ሺ݅ǡ ǣ ሻ ‫ݐ‬െͳ ሺ݅ǡ ǣ ሻ ‫ݐ‬െͳ ሺ݆ǡ ǣ ሻ ‫ݐ‬െʹ ሺ݆ǡ ǣ ሻ ‫ݐ‬െܰ ሺ݆ǡ ǣ ሻ ‫ݐ‬ ሺ݅ǡ ǣ ሻ ‫ݐ‬ ሺ݆ǡ ǣ ሻ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. LOSS FUNCTIONS</head><p>The loss function for the supervised deep model has two components. We first introduce the structure loss, which exploits the nodes' transitional patterns to preserve and predict the network dynamic transition structures.</p><p>The structure loss refers to how precise the predicted structure is. Thus, to minimize this deviation, it is required to model neighborhood's transition of each vertex. Given a dynamic network G, we first slice it evenly into several snapshots {G t-N , ..., G t-1 , G t }. Then, we can obtain its adjacency matrix X = {X t-N , ..., X t-1 , X t }. For each adjacency matrix X k , k ∈ [t -N, t], it contains n instances X k (i, :), ..., X k (n, :). The value of each instance is X k (i, :</p><formula xml:id="formula_5">) = {X k ij } n j=1 , X k ij &gt; 0 if</formula><p>and only if there exists connection between v i , v j during the time slice k. Therefore, there are various neighborhood structures for each vertex in a series of timestamps, and X provides the neighborhood structure and the temporal information of each node. With temporal adjacency matrices in X , we adapt the encoder-decoder model to capture the transitional patterns and predict new network structure.</p><p>To further analyze the preserving and inference process, we briefly review the key idea of encoder-decoder model. As we emphasized in the last section, the deep model is composed of two parts, i.e. the encoder and the decoder. The encoder is a GRU that reads each symbol of an input sequence x sequentially. As it reads each symbol, the hidden state of the GRU changes as:</p><formula xml:id="formula_6">h t = f (h t-1 + x t ),<label>(1)</label></formula><p>where f is a non-linear activation function. It is presented as an assemblage of GRU functions. After reading the end of the sequence, the output of encoder is a summary of all hidden states of the GRU. The decoder also consists of multiple non-linear functions, which maps the summary hidden states into representation space to infer network structure. Specifically, for one node i, given the temporal inputs X(i, :) = {X t-N (i, :), ..., X t-1 (i, :)}, the summary state for the encoder is shown as follows:</p><formula xml:id="formula_7">c i = [h t-N i , ..., h t-1 i ]<label>(2)</label></formula><formula xml:id="formula_8">h k i = [ - → h k i , ← - h k i ], k = {t -N, ..., t -1}<label>(3)</label></formula><formula xml:id="formula_9">- → h k i = f ( - → h k-1 i + - → X k (i, :)),<label>(4)</label></formula><formula xml:id="formula_10">← - h k i = f ( ← - h k-1 i + ← - X k (i, :)),<label>(5)</label></formula><p>where -→ h k i and ←h k i are based on Eq.( <ref type="formula" target="#formula_6">1</ref>), but they are fed with opposite time sequences as -→ X(i, :) = {X t-N (i, : ), ..., X t-1 (i, :)}, ← -X(i, :) = {X t-1 (i, :), ..., X t-N (i, :)}. Two reversed hidden states are concatenated into h k i , then all hidden states concatenated into the summary state c i . After obtaining c i , the hidden representations for each layer of decoder are presented as follows:</p><formula xml:id="formula_11">y (1) i = σ(W (1) c i + b (1) ) (6) y (m) i = σ(W (m) y (m-1) i + b (m) ), m = 2, ..., M<label>(7)</label></formula><p>where M is the number of hidden layer. After the calculation process of decoder, we can obtain the output y</p><formula xml:id="formula_12">(M ) i</formula><p>as the new structure inference Xt (i, :). The goal of decoder is minimizing the prediction error so that the output Xt (i, :) can fit the linkage state X t (i, :). We adopt cross entropy as the loss function, which is formulized as:</p><formula xml:id="formula_13">s = - n i=1 X t (i, :)log Xt (i, :) = - n i=1 n j=1 X t (i, j) log Xt (i, j)<label>(8)</label></formula><p>As a well-known assumption in dynamic network realms demonstrated, each vertex has a unique transitional pattern through time slices <ref type="bibr" target="#b2">[3]</ref>. By mapping the relevant information to latent space, the encoder has the exponential capability to capture non-linear variance. Furthermore, by minimizing the structure loss, the decoder can use the transition information to perform embedding and infer structure of the new network.</p><p>However, such a supervised learning process cannot be directly applied to our problems because of sparsity of networks. In the real-world networks, we observed that most of the nodes have limited number of neighbors. Such a phenomenon can be found in the adjacency matrix, in which the number of non-zero elements is far less than that of zero elements. Thus, if we directly use X t (i, :) as the learning target, the decoder is prone to predict the zero elements in the output vector. To address this problem, we impose more penalty to the prediction error of the non-zero elements than that of zero elements. Accordingly, the revised cross-entropy loss function is presented as:</p><formula xml:id="formula_14">L s = - n i=1 Z(i, :) X t (i, :)log Xt (i, :) = - n i=1 n j=1 Z(i, j)X t (i, j) log Xt (i, j)<label>(9)</label></formula><p>where Z(i, :) = {Z(i, j)} n j=1 . If X(i, j) = 0, Z(i, j) = 1, else Z(i, j) = α &gt; 1. Now by using the enhanced decoder model, the loss of non-zero elements can be highlighted and the linkage information will be preserved. Thus, the nodes' transitional patterns can be exploited to preserve and predict the network dynamic structure.</p><p>In a dynamic network, each node has a unique transitional pattern, however, a node's behavior is also influenced by its local neighbors <ref type="bibr" target="#b2">[3]</ref>. How to quantify the influence and preserve network internal structure are important. We address this problem by embedding the interaction proximity into the deep learning structure. The interaction proximity can be regarded as the closeness of a pair of nodes, which is measured by using their links in the previous snapshots. Intuitively, two nodes that have frequent connections are more likely to have similar latent representations. To formulate this idea, we define the loss function as follows:</p><formula xml:id="formula_15">L c = n u,v=1 N ij ||c i -c j || 2<label>(10)</label></formula><p>where N ij is the connection frequency of node i and j. If i, j have no edges in the historical network, N ij = 0, else N ij is amount of edges. We impose a penalty when similar nodes are mapped far away in the latent space. Thus, the frequently-connected nodes can have similar embeddings and tend to connect in the future network.</p><p>To capture the transition pattern and interaction proximity simultaneously, we propose a deep model to combine two loss functions and jointly minimize the following objective function: <ref type="bibr" target="#b10">(11)</ref> where β and γ are hyper parameters to make a trade off all loss functions. L reg is an L2-norm regularizer term to prevent overfitting problem, which is defined as</p><formula xml:id="formula_16">L all = L s + βL c + γL reg = - n i=1 Z(i, :) X t (i, :)log Xt (i, :) + β n i,j=1 N ij ||c i -c j || 2 + γL reg</formula><formula xml:id="formula_17">L reg = ||W z || F ||W r || F + ||W|| F + ||U z || F + ||U r || F + ||U|| F + M m=1 ||W (m) || F<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. TRAINING AND INFERENCE ON DDNE</head><p>Since our proposed deep model has an encoder-decoder architecture, training and inference is no more difficult than in the sequence to sequence methods. We first make a forward propagation to calculate the loss, then back propagate the loss and update parameters to make model fit the inputs. In detail, we use the stochastic gradient descent (SGD) algorithm Adadelta <ref type="bibr" target="#b18">[19]</ref> to automatically update and learn parameters. According to Eq.(1) to Eq.( <ref type="formula" target="#formula_11">7</ref>), the interaction proximity loss L c can be summarized as:</p><formula xml:id="formula_18">L c = f en (W z , W r , W, U z , U r , U)<label>(13)</label></formula><p>Then, the structure loss is:</p><formula xml:id="formula_19">L s = f all (W z , W r , W, U z , U r , U, W (m) , b (m) ) (14)</formula><p>The regularizer function in Eq.( <ref type="formula" target="#formula_17">12</ref>) also can be presented as:</p><formula xml:id="formula_20">L reg = f reg (W z , W r , U, U z , U r , U, W (m) ) (15)</formula><p>Now, the updates for the weights W z , W r , W, U z , U r , U have the same form as follows:</p><formula xml:id="formula_21">W z = W z -λ(∇f all (W z )+β∇f en (W z )+γ∇f reg (W z )) (16)</formula><p>The parameters W (m) , b (m) update as follows:</p><formula xml:id="formula_22">W (m) = W (m) - λ(∇f all (W (m) ) + γ∇f reg (W (m) )) b (m) = b (m) -λ∇f all (b (m) ), (<label>17</label></formula><formula xml:id="formula_23">)</formula><p>where λ is the learning rate, and ∇f (W) is the gradient of f at W. We train the model following the update rules above. After that, model parameters are fixed and predicting future network status follows the similar reason with inferencing. Specifically, we can shift the window one step towards future to obtain a fixed observation which contains the previous N -1 snapshots and the current snapshot. We fix this known observation as history, put it into deep model and perform a forward inference to get the network embeddings at t + 1. Now, these embeddings can be used to reconstruct network structure at t + 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we present both quantitative and qualitative experiment results on four real-word datasets. The experiment results demonstrate the effectiveness of the proposed DDNE model for dynamic network embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. DATASETS</head><p>In this paper, we use four datasets, including two email networks, one collaboration network and human contact network, for two real-world applications, i.e. link prediction. They are online in the Koblenz Network Collection (http://konect.uni-koblenz.de/). All networks have different sizes and attributes. Their statistic properties are shown in the Table <ref type="table" target="#tab_1">2</ref>. The arXiv dataset <ref type="bibr" target="#b19">[20]</ref> [2] is a collaboration graph of scientific paper authors from the arXiv's High Energy Physics -Phenomenology (hep-ph) section. This dataset has 10 years evolution history, ranging from 1991 to 2001, and the data increase steady in recent years. Hence, we chose five years <ref type="bibr">(1995 -1999)</ref> as five snapshots for our experiment. Each snapshot contains one-year network structure, and the last snapshot can be used as ground-truth of network inference.</p><p>Two email datasets (Enron and Radoslaw) contain email communication networks from two companies. For Enron email network <ref type="bibr" target="#b20">[21]</ref> [22], it consists of 1,148,072 emails sent among employees of Enron between 1999 and 2003.</p><p>Radoslaw dataset <ref type="bibr" target="#b22">[23]</ref> [24], it is the internal email communication network between employees of a mid-sized manufacturing company from 2010-01-01 to 2010-09-30. Nodes in both networks are individual employees and edges are individual emails. We sample five snapshots from Enron in every half year during 2001-01 to 2002-06 and denote them as E1 to E5. Radoslaw dataset is divided into nine slices (R1 to R9) ranging from January to September. We only extract five snapshots(R1 to R5) for link prediction experiments.</p><p>The Haggle dataset <ref type="bibr" target="#b24">[25]</ref> [26] is a real human contact network. This undirected network represents contacts between people measured by carried wireless devices. A node represents a person, and an edge between two persons shows that there was a contact between them. This dataset contains five-day records of 274 persons and we split it into five parts(H1 to H5) based on each day.</p><p>To summarize, we adopt five snapshots of temporal datasets for link prediction task. The first snapshot is used to construct the basic network, which contains all the nodes through all time slices. Snapshots 2, 3 and 4 are used for training models. After model training, we move one step forward, snapshots 3, 4 now can be adopted to inference the embedding and structure of snapshot 5. The ground-truth network structure in the last snapshot is used to validate inference result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. EVALUATION SCHEME</head><p>For link prediction task, we are trying to leverage historical linkage information to infer the embedding and structure of current network. It is essentially a binary classification problem. Given n nodes, we try to predict which pair of nodes will generate an edge. However, only a very small fraction of links actually exists, which will lead to a data imbalance problem.</p><p>Our experiments show that the existing links only constitute less than 1% of all possible links. This means that if we set all prediction result to zero, we can still achieve a high accuracy evaluation. Thus, in order to evaluate the performance properly, we use the following measurement:</p><p>• Area Under the ROC Curve (AUC): it is frequently employed on classification problem because it relates to the sensitivity (true positive rate) and the specificity (true negative rate) of a classifier. This metric is strictly bounded between 0 and 1. The larger the AUC is, the better the model performs.</p><p>• Mean Average Precision (MAP): mean average precision is an extension of average precision (AP) where we take average of all AP's to get the MAP. It estimates precision for every node and computes the average over all nodes. Compared with AUC, it considers whether all of the relevant items tend to get ranked highly. It is  </p><formula xml:id="formula_24">M AP = n 1 AP (i) |V |<label>(18)</label></formula><formula xml:id="formula_25">AP (i) = j precision@j(i)•∆i(j) |{∆i(j)=1}|<label>(19)</label></formula><p>where precision@j(i) =</p><formula xml:id="formula_26">|E pred i (1:j)∩E obs i | j</formula><p>, and E predi and E obsi are the predicted and observed edges for node i respectively. ∆ i (j) = 1 indicates that v i and v have a link. V is the query set for information retrieval. In our experiments, it represents a node set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. BASELINE METHODS</head><p>We use various methods as the baselines, which include unsupervised learning methods such as Common Neighbors, Katz <ref type="bibr" target="#b26">[27]</ref>, generative models tRBM and ctRBM, and network embedding algorithms such as DeepWalk, node2vec and SDNE. We make a full comparison with these baselines to show the capability of our embedding method in the task of link prediction. The brief introduction of baselines is listed as follows:</p><p>• Common Neighbors (CN): The CN metric is one of the most widespread measurements used in link prediction problem. A large number of the common neighbors make it easier to have a link between two nodes.</p><p>• Katz <ref type="bibr" target="#b26">[27]</ref>: It is based on the ensemble of all paths, and it counts all paths with different weights between two nodes.</p><p>• temporal Restricted Boltzmann Machine (tRBM) <ref type="bibr" target="#b11">[12]</ref> : A RBM based model which takes adjacency matrixes as inputs. It can be used to describe network's dynamics and predict network structure.</p><p>conditional temporal Restricted Boltzmann Machine (ctRBM) <ref type="bibr" target="#b2">[3]</ref>: A tRBM based model embeds the information of neighbor nodes.</p><p>• DeepWalk <ref type="bibr" target="#b10">[11]</ref>: It adopts random walk and skipgram model to generate network representations. The represented embeddings can be used to predict linkage state of network.</p><p>• node2vec <ref type="bibr" target="#b9">[10]</ref>: Similar to DeepWalk, node2vec preserves higherorder proximity between nodes by maximizing the probability of occurrence of subsequent nodes in fixed length random walks <ref type="bibr" target="#b27">[28]</ref>.</p><p>• SDNE <ref type="bibr" target="#b8">[9]</ref>: It is an autoencoder based deep model, which defines loss functions of the first-order or second-order proximity to preserve network internal and network dynamic transition structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. PARAMETER SETTINGS</head><p>We proposed a deep model in this paper, which can represent network into a low dimension space. It has different embedding size with different datasets. For the small datasets such as Radoslaw (151 nodes) and Haggle (91 nodes), we set the output has the same size as the input.</p><p>For the large datasets such as ArXiv (4122 nodes) and Enron(11670 nodes), the dimension of output is set to 1024. If we use higher dimensionality, the performance almost remains unchanged or even becomes worse. The hyperparameters of α, β and γ are tuned by using grid search on the validation set. For different datasets, the parameters for baselines are different, and all are tuned to be optimal. Other default settings include: the learning rate of deep model is set as 0.0001; the history window size is set to 2 so that we can infer the embedding by two historical snapshots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. EXPERIMENTAL RESULTS</head><p>After setting the parameters, we conduct the link prediction task in four real-world networks. We first leverage historical snapshots and current linkage status to train DDNE model.</p><p>After training, as all parameters are adapted to the dataset, we shift the window one step towards future to obtain a fixed observation which contains the previous N -1 snapshots and the current snapshot. Now, the DDNE is performed as a generative model. We fix this known observation as history, and put it into deep model, such that we can obtain the representations for each vertex and then use the obtained representations to predict the network status at t+1. For the generative models such as tRBM and ctRBM, we adopt the same strategy for the experiments. For embedding methods such as DeepWalk, node2vec, SDNE, a combined historical network that merges all previous snapshots into one graph is directly employed. As <ref type="bibr" target="#b9">[10]</ref> [9] demonstrated, they can learn the representations of each vertex and then make predictions for the unobserved links.   information but ignore the transitional information of dynetworks. In the proposed method, both the structure information and transition of networks are well considered so that it is capable to infer the new networks' status. Furthermore, we find that the DDNE has better performances than the DDNE-without IP, which indicates the interaction information effectively facilitates the embedding and inference of new network. To further specify the AUC results of DDNE model, we illustrate the Receiver Operating Characteristic (ROC) on four datasets. The illustration is shown in Figure <ref type="figure" target="#fig_4">4</ref>. For each dataset, there are different number of thresholds since they have different amount of samples. The area under the ROC curve equals to the AUC result, which is presented in Table <ref type="table" target="#tab_3">3</ref>. We adopt sklearn <ref type="bibr" target="#b28">[29]</ref> to complete the ROC calculation. It worth noting that sklearn sometimes decides to drop some non-useful thresholds, resulting in thresholds to be less than distinct values.</p><p>4 reports MAP performances on link prediction. Similar results can be achieved as in the AUC evaluation. The proposed method achieves substantial gains than benchmarks. An interest finding is that DDNE-without IP slightly better than DDNE on Enron dataset. This is because in the sparse network, the user interactions are rare through all snapshots, DDNE may not capture these interactions in training process.</p><p>To further explore the experimental results, we conduct a statistical analysis to examine whether the DDNE's performance is significantly different from baselines. Generally, there are several tests to investigate statistical significance, such as student's t-test, ANOVA tests etc. We decide to use two-sample t-test since it can determine whether two population means are different. Specifically, we first conduct 30-time experiments for each method and each dataset From the tables, we can see that except for the AUC result with DDNE and Katz (p-value=0.058), the proposed model performs significantly different than all baselines in terms of four datasets. These p-values also give strong backing to the AUC and MAP results in Table <ref type="table" target="#tab_3">3</ref> and Table <ref type="table" target="#tab_5">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. PARAMETER SENSITIVITY ANALYSIS</head><p>We further investigate the parameter sensitivity in this section. Specifically, we evaluate how different length of the historical time and different values of hyper-parameter α and β can affect the link prediction results.</p><p>Influence of history length. In our work, three datasets are split into five snapshots, while only Radoslaw dataset has nine time slices. Thus, we conduct this experiment on Radoslaw to examine how the different choices of parameters affect the performance of DDNE. We vary the number of historical snapshots from 1 to 7, to demonstrate the effect of varying this parameter. Except for the parameter being tested, all other parameters assume default values. The result shows that DDNE is not very sensitive to the number of historical snapshots. As we can see in Figure <ref type="figure" target="#fig_5">5</ref>(a) there is no significant accuracy improvement after the number of snapshots increases. The reason is that a small dense network may have similar linkage states in each snapshot, and the increment of historical links does not enrich the information for network inference.</p><p>Influence of Hyper Parameter α. In our model, we set α as a hyper parameter to control the inference weight of the non-zero elements in training graph. The larger the α, the model will more prone to predict the non-zero elements. The result is shown in Figure <ref type="figure" target="#fig_5">5</ref>(b). For the sparse networks such as ArXiv and Enron, we can see that the performance raises and then keep stable when the weight of the nonzero elements increases. This is intuitive as the model pay more attention on linked nodes so that it can preserve linkage information among tremendous non-link node pairs.   However, for the dense networks (Radoslaw and Haggle), the performance has no improvement and even being worse when the weight α increases. This is because the nonzero and zero elements in training network is balanced, too large a weight on non-zero elements may overwhelm the zero elements and also introduce noises to deteriorate the performance. Therefore, it is always important to determine the weight of non-zero elements for each type of networks.</p><p>Influence of Hyper Parameter β. The parameter of β balances the weight of the structure loss and interaction proximity. When β = 0, the performance is totally determined by structure loss. The larger the β, the more the model concentrates on interaction proximity. When β = 1, both loss functions have same contributions to the performance. As we can see from Figure <ref type="figure" target="#fig_5">5</ref>(c), the evaluation metric AUC raises when the parameter β increases. However, when the weight of interaction proximity continuously increases, the performance starts to drop slowly. The reason is that too much interaction proximity may overwhelm the structure loss, and then it deteriorates the performance. It also demonstrates that both structure loss and interaction proximity are essential for network embedding methods to characterize and infer the network structure.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>slices X(i, :) = {X k (i, :)} t-N k=t the adjacency vectors of node i X the output of decoder Wz, Wr, W, Uz, Ur, U the weight matrices of GRU W(m)  the m-th layer weight of decoder b(m)  </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 3 :</head><label>3</label><figDesc>FIGURE 3: Framework of DDNE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2018.2839770, IEEE Access Author et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS calculated as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FIGURE 4 :</head><label>4</label><figDesc>FIGURE 4: Experiments on ROC curve</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FIGURE 5 :</head><label>5</label><figDesc>FIGURE 5: Experiments on parameters sensitivity</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 :</head><label>2</label><figDesc>Statistics of Networks</figDesc><table><row><cell>Characteristics</cell><cell>arXiv</cell><cell>Enron</cell><cell>Radoslaw</cell><cell>Haggle</cell></row><row><cell>Nodes</cell><cell>4,122</cell><cell>11,670</cell><cell>151</cell><cell>91</cell></row><row><cell>Edges</cell><cell>220,349</cell><cell>91,971</cell><cell>2,869</cell><cell>1,142</cell></row><row><cell>Avg degree</cell><cell>106</cell><cell>15</cell><cell>38</cell><cell>25</cell></row><row><cell>Max degree</cell><cell>859</cell><cell>879</cell><cell>139</cell><cell>55</cell></row><row><cell>Clus coefficient</cell><cell>56.46%</cell><cell>25.14%</cell><cell>64.48%</cell><cell>80.09%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 :</head><label>3</label><figDesc>AUC on different datasets of the link prediction</figDesc><table><row><cell>Approaches</cell><cell>arXiv</cell><cell>Enron</cell><cell>Radoslaw</cell><cell>Haggle</cell></row><row><cell>CN</cell><cell>0.8115</cell><cell>0.7551</cell><cell>0.8636</cell><cell>0.9621</cell></row><row><cell>Katz</cell><cell>0.8117</cell><cell>0.7581</cell><cell>0.8676</cell><cell>0.9658</cell></row><row><cell>tRBM</cell><cell>0.8476</cell><cell>0.8129</cell><cell>0.8478</cell><cell>0.9611</cell></row><row><cell>ctRBM</cell><cell>0.7997</cell><cell>0.7262</cell><cell>0.8645</cell><cell>0.9618</cell></row><row><cell>DeepWalk</cell><cell cols="2">0.6071 0.5230</cell><cell>0.5308</cell><cell>0.7269</cell></row><row><cell>node2vec</cell><cell>0.8341</cell><cell>0.9349</cell><cell>0.7813</cell><cell>0.7864</cell></row><row><cell>SDNE</cell><cell>0.8146</cell><cell>0.9437</cell><cell>0.8709</cell><cell>0.8452</cell></row><row><cell>DDNE-w/o IP</cell><cell>0.8809</cell><cell>0.9532</cell><cell>0.8741</cell><cell>0.9577</cell></row><row><cell>DDNE</cell><cell>0.9002</cell><cell>0.9602</cell><cell>0.9465</cell><cell>0.9736</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 compares</head><label>3</label><figDesc>AUC performances over the four datasets described in Section IV.A, in which DDNE-without IP is the proposed model without Interaction Proximity. The result shows that DDNE achieves the best among all baselines regardless whether the network is very sparse (Enron) or dense (Radoslaw, Haggle). It is worth noting that conventional embedding methods do not perform better than other baselines, since they only preserve the structure</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4 :</head><label>4</label><figDesc>MAP on different datasets of the link prediction</figDesc><table><row><cell>Approaches</cell><cell>arXiv</cell><cell>Enron</cell><cell>Radoslaw</cell><cell>Haggle</cell></row><row><cell>CN</cell><cell>0.1802</cell><cell>0.0169</cell><cell>0.2361</cell><cell>0.3798</cell></row><row><cell>Katz</cell><cell>0.1926</cell><cell>0.0182</cell><cell>0.392</cell><cell>0.4429</cell></row><row><cell>tRBM</cell><cell>0.2013</cell><cell>0.0278</cell><cell>0.3292</cell><cell>0.4035</cell></row><row><cell>ctRBM</cell><cell>0.2324</cell><cell>0.0228</cell><cell>0.3257</cell><cell>0.3995</cell></row><row><cell>DeepWalk</cell><cell cols="2">0.0127 0.0003</cell><cell>0.1033</cell><cell>0.3919</cell></row><row><cell>node2vec</cell><cell>0.1219</cell><cell>0.044</cell><cell>0.3259</cell><cell>0.521</cell></row><row><cell>SDNE</cell><cell>0.0287</cell><cell>0.0403</cell><cell>0.2816</cell><cell>0.4403</cell></row><row><cell>DDNE-w/o IP</cell><cell>0.3167</cell><cell>0.2065</cell><cell>0.4463</cell><cell>0.6054</cell></row><row><cell>DDNE</cell><cell>0.3464</cell><cell>0.1954</cell><cell>0.5377</cell><cell>0.733</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">: P value analysis about AUC</cell></row><row><cell>p-value</cell><cell>arXiv DDNE</cell><cell>Enron DDNE</cell><cell>Radoslaw DDNE</cell><cell>Haggle DDNE</cell></row><row><cell>CN</cell><cell>7.24E-109</cell><cell>7.22E-53</cell><cell>2.60E-28</cell><cell>0.0060</cell></row><row><cell>Katz</cell><cell>8.26E-109</cell><cell>1.67E-52</cell><cell>3.21E-27</cell><cell>0.0580</cell></row><row><cell>tRBM</cell><cell>1.16E-18</cell><cell>9.50E-35</cell><cell>7.85E-24</cell><cell>0.0206</cell></row><row><cell>ctRBM</cell><cell>4.44E-34</cell><cell>1.07E-47</cell><cell>2.29E-20</cell><cell>0.0385</cell></row><row><cell>DeepWalk</cell><cell>2.16E-64</cell><cell>1.91E-61</cell><cell>4.00E-59</cell><cell>2.60E-45</cell></row><row><cell>node2vec</cell><cell>4.96E-24</cell><cell></cell><cell>7.02E-37</cell><cell>1.87E-39</cell></row><row><cell>SDNE</cell><cell>2.37E-31</cell><cell>0.0014</cell><cell>1.48E-19</cell><cell>2.18E-29</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6 :</head><label>6</label><figDesc>P value analysis about MAP Table5and Table6present the statistical results of two metrics. Some elements in the table have the type like xE -y, which means x * 10 -y .</figDesc><table><row><cell>p-value</cell><cell>arXiv DDNE</cell><cell>Enron DDNE</cell><cell>Radoslaw DDNE</cell><cell>Haggle DDNE</cell></row><row><cell>CN</cell><cell>1.65E-45</cell><cell>4.40E-56</cell><cell>1.23E-66</cell><cell>2.31E-62</cell></row><row><cell>Katz</cell><cell>1.28E-43</cell><cell>6.68E-56</cell><cell>1.52E-48</cell><cell>1.87E-57</cell></row><row><cell>tRBM</cell><cell>5.76E-36</cell><cell>1.75E-41</cell><cell>2.50E-49</cell><cell>5.41E-55</cell></row><row><cell>ctRBM</cell><cell>2.66E-28</cell><cell>1.11E-38</cell><cell>8.48E-44</cell><cell>2.16E-53</cell></row><row><cell>DeepWalk</cell><cell>5.79E-55</cell><cell>1.02E-45</cell><cell>1.35E-62</cell><cell>4.88E-54</cell></row><row><cell>node2vec</cell><cell>1.77E-45</cell><cell>2.72E-42</cell><cell>1.91E-47</cell><cell>1.23E-43</cell></row><row><cell>SDNE</cell><cell>3.08E-51</cell><cell>1.33E-38</cell><cell>1.28E-53</cell><cell>6.02E-52</cell></row><row><cell cols="5">so that we obtain 30 samples for a method-dataset pair.</cell></row><row><cell cols="5">Then, we adopt t-test to compare our proposed model</cell></row><row><cell cols="5">DDNE with each baseline. The output of t-test is a p-value,</cell></row><row><cell cols="5">which indicates the strength of observations against null</cell></row><row><cell cols="5">hypothesis. In our experiments, a small p-value (typically</cell></row><row><cell cols="5">≤ 0.05) indicates that the means of two method-dataset</cell></row><row><cell cols="3">results are significantly different.</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>  VOLUME 4, 2016   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_1"><p>  VOLUME 4, 2016   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_2"><p>  VOLUME 4, 2016   </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ACKNOWLEDGMENT</head><p>This work is partially supported by the National Natural Science Foundation of China (Nos. 11590770-4, 61650202, 11722437, U1536117, 61671442, 11674352, 11504406, 61601453), the National Key Research and Development Program (Nos. 2016YFB0801203, 2016YFC0800503, 2017YFB1002803) and the Key Science and Technology Project of the Xinjiang Uygur Autonomous Region (No. 2016A03007-1).</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work is partially supported by the National Natural Science Foundation of China (Nos. 11590770-4, 61650202, 11722437, U1536117, 61671442, 11674352, 11504406, 61601453), the National Key Research and Development Program (Nos. 2016YFB0801203, 2016YFC0800503, 2017YFB1002803) and the Key Science and Technology Project of the Xinjiang Uygur Autonomous Region (No. 2016A03007-1).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper, we propose a Deep Dynamic Network Embedding, namely DDNE, for link prediction task. Specifically, to model the evolving pattern of each node, we design a new deep architecture, which can leverage historical linkage to make an embedding for new links. To further address the neighbor's influence problem, we exploit the interaction proximity in the hidden state to measure the similarity of nodes. By jointly optimizing them and put more weight on non-zero elements of output, the learned embeddings are new-linkage-preserved and are robust to sparse networks. Empirically, we compare the proposed model with traditional embedding methods and state-ofthe-art link prediction methods in a variety of datasets. Experiment results demonstrate that we achieve significant gains than other baselines.</p><p>Our future work will focus on how to learn representations for heterogeneous networks which have different type of nodes and edges. Furthermore, we will try to reduce the computation complexity of the proposed deep model. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evolution in social networks: A survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Spiliopoulou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Social network data analytics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="149" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph evolution: Densification and shrinkingdiameters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Knowledge Discovery from Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A deep learning approach to link prediction in dynamic networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 SIAM International Conference on Data Mining</title>
		<meeting>the 2014 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Community evolution in dynamic multi-mode networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Nazeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="677" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">De</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Line: Largescale information network embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
		<meeting>the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cauchy graph embedding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="553" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modeling human motion using binary latent variables</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1345" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>arXiv preprint arXiv:1406.1078</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Understanding lstm networks</title>
		<ptr target="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adadelta: an adaptive learning rate method</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">arxiv hep-ph network dataset -KONECT</title>
		<ptr target="http://konect.uni-koblenz.de/networks/ca-cit-HepPh" />
		<imprint>
			<date type="published" when="2016-10">Oct. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Enron network dataset -KONECT</title>
		<ptr target="http://konect.uni-koblenz.de/networks/enron" />
		<imprint>
			<date type="published" when="2016-10">Oct. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Enron corpus: A new dataset for email classification research</title>
		<author>
			<persName><forename type="first">B</forename><surname>Klimt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Machine Learning</title>
		<meeting>European Conf. on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Manufacturing emails network dataset -KONECT</title>
		<ptr target="http://konect.uni-koblenz.de/networks/radoslaw{\_}email" />
		<imprint>
			<date type="published" when="2016-10">Oct. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Matching organizational structure and social network extracted from email communication</title>
		<author>
			<persName><forename type="first">R</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Palus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kazienko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Business Information Processing</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="197" to="206" />
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Haggle network dataset -KONECT</title>
		<ptr target="http://konect.uni-koblenz.de/networks/contact" />
		<imprint>
			<date type="published" when="2016-10">Oct. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Impact of human mobility on opportunistic forwarding algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chaintreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Crowcroft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Diot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Mobile Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="606" to="620" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A new status index derived from sociometric analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="43" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Graph embedding techniques, applications, and performance: A survey</title>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ferrara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02801</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">He received the B.E. degree in communication engineering from the Jilin University</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TAISONG LI was born in Hunan</title>
		<meeting><address><addrLine>China; Jilin, China; Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">2011. 1990. 2013</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
		</imprint>
		<respStmt>
			<orgName>PhD candidate in Institute of Acoustics, University of Chinese Academy of Sciences (UCAS)</orgName>
		</respStmt>
	</monogr>
	<note>Scikitlearn: Machine learning in Python. from 2013 till now, respectively</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
