<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Low-Resource Speech Recognition Based on Improved NN-HMM Structures</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiusong</forename><surname>Sun</surname></persName>
							<idno type="ORCID">0000-0003-0232-7069</idno>
						</author>
						<author>
							<persName><forename type="first">Qun</forename><surname>Yang</surname></persName>
							<idno type="ORCID">0000-0001-6824-8473</idno>
						</author>
						<author>
							<persName><roleName>AND</roleName><forename type="first">Shaohan</forename><surname>Liu</surname></persName>
							<idno type="ORCID">0000-0001-6967-0262</idno>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Yuan</surname></persName>
							<idno type="ORCID">0000-0003-2261-1809</idno>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Nanjing University of Aeronautics and Astronautics</orgName>
								<address>
									<postCode>210016</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Collaborative Innovation Center of Novel Software Technology and Industrialization</orgName>
								<address>
									<postCode>210023</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Nanjing University of Aeronautics and Astronautics</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Low-Resource Speech Recognition Based on Improved NN-HMM Structures</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ACCESS.2020.2988365</idno>
					<note type="submission">Received February 6, 2020, accepted February 11, 2020, date of publication April 16, 2020, date of current version April 30, 2020.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Low-resource</term>
					<term>speech recognition</term>
					<term>multitask learning</term>
					<term>acoustic modeling</term>
					<term>feature combinations</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The performance of the ASR system is unsatisfactory in a low-resource environment. In this paper, we investigated the effectiveness of three approaches to improve the performance of the acoustic models in low-resource environments. They are Mono-and-triphone Learning, Soft One-hot Label and Feature Combinations. We applied these three methods to the network architecture and compared their results with baselines. Our proposal has achieved remarkable improvement in the task of mandarin speech recognition in the hybrid hidden Markov model -neural network approach on phoneme level. In order to verify the generalization ability of our proposed method, we conducted many comparative experiments on DNN, RNN, LSTM and other network structures. The experimental results show that our method is applicable to almost all currently widely used network structures. Compared to baselines, our proposals achieved an average relative Character Error Rate (CER) reduction of 8.0%. In our experiments, the size of training data is ∼10 hours, and we did not use data augmentation or transfer learning methods, which means that we did not use any additional data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION A. BACKGROUND</head><p>Speech is the most important means for humans to transmit information to each other. A voice carries rich information such as the speaker's intention, identity, and emotion. This makes automatic speech recognition with the goal of humancomputer interaction popular, and it has been a research hotspot in recent decades <ref type="bibr" target="#b0">[1]</ref>. Automatic Speech Recognition (ASR) refers to the task of an automatic conversion from speech to text by computer. In real life, speech recognition can provide a natural and smooth human-computer interaction method. ASR has many applications, such as Apple's Siri, Microsoft's Cortana, and Xiaomi's Xiao Ai. In recent years, with the improvement of computer hardware capability and the development of neural network theory, deep learning has been applied to Automatic Speech Recognition.</p><p>The associate editor coordinating the review of this manuscript and approving it for publication was Xiao-Yu Zhang .</p><p>Speech recognition systems based on phoneme level are currently mainly composed of acoustic models (AM), language models (LM) and Pronunciation models (PM). The acoustic model maps the acoustic features of each frames to the modeling unit, which is the phoneme. The language model corresponds the phoneme sequence obtained from the acoustic model to the sentence with the highest probability. The acoustic model is a kind of neural network structure, and it is also the main research point. Acoustic models are mainly divided into two types, one is the Neural Network Hidden Markov Models (NN-HMMs), and the other is the End-toend models, such as Encoder-decoder structure <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref> and Neural Network structure with Connectionist temporal classification (CTC) loss <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. The hybrid hidden Markov model (HMM) -neural network (NN) approach on phoneme level always need to take time to train GMM to align data before network training, and it can get better results in many tasks. Although the End-to-end models are currently developing rapidly, they always need a large amount of data to make the network convergence and their performance has not exceeded the NN-HMM structure, especially in low-resource speech recognition tasks, the models based HMM are much ahead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. RELATED WORK</head><p>As we all know, deep-learning <ref type="bibr" target="#b6">[7]</ref> relies on a large amount of data, so the performance of the ASR system will be unsatisfactory in a low-resource environment. Therefore, improving the ASR under the condition of low resource has become a research hotspot because the acquisition of labeled speech data is usually difficult <ref type="bibr" target="#b7">[8]</ref>. A common problem in lowresource environments is that the lack of training data often leads to overfitting of the neural network, which makes the model's performance on the test set worse. To prevent this problem, methods such as transfer learning, data augmentation, and unsupervised pre-training were born.</p><p>Transfer learning has been proposed for a long time, and SJ Pan et al. made a complete summary of it <ref type="bibr" target="#b8">[9]</ref>. It can make full use of the data in the non-target domain to train a better initial model. It has shown promising results in many tasks such as image recognition <ref type="bibr" target="#b9">[10]</ref>, speech recognition <ref type="bibr" target="#b10">[11]</ref>, etc. Unsupervised pre-training also uses additional data to train a better initial model, but unlike transfer learning, transcribed data is not necessary. Those data without label helps networks to and capture more intricate dependencies between parameters and get a good initial marginal distribution. It has shown promising results in several areas, including Computer Vision (CV) <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b40">[41]</ref>- <ref type="bibr" target="#b43">[44]</ref>, Natural Language Processing (NLP) <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> and so on <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref>.</p><p>Data augmentation <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref> has been proposed for the purpose of studying low-resource language speech recognition for a long time. Kanda et al. investigated three distortion methods -vocal tract length distortion, speech rate distortion and frequency-axis random distortion. They evaluated those methods with Japanese lecture recordings and get lower word error. <ref type="bibr" target="#b20">[21]</ref>. Jaitly et al. used Vocal Tract Length Perturbation (VTLP) to expand training data. When this technique is applied to TIMIT using Deep Neural Networks of different depths, the Phone Error Rate (PER) improved by an average of 0.65% on the test set <ref type="bibr" target="#b21">[22]</ref>. Ko et al. proposed a method that changing the speed of the audio signal, producing 3 versions of the original signal with speed factors of 0.9, 1.0 and 1.1. They present results on 4 different LVCSR tasks with training data ranging from 100 hours to 960 hours, to examine the effectiveness of audio augmentation in a variety of data scenarios. An average relative improvement of 4.3% was observed across the 4 tasks. As far, the method of changing speed has the lowest implementation cost and achieve stateof-the-art performance <ref type="bibr" target="#b22">[23]</ref>. In <ref type="bibr" target="#b23">[24]</ref>, A new method called SpecAugment is proposed and it consists of warping the features, masking blocks of frequency channels, and masking blocks of time steps. Data augmentation has been proved to be a simple and effective technique, not only in speech recognition but also in other fields such as image recognition <ref type="bibr" target="#b24">[25]</ref> and keyword search <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. However, these methods are equivalent to adding training data, and do not solve the problem of overfitting.</p><p>Regularization is a technique to discourage the complexity of the model. It does this by penalizing the loss function. This helps to solve the overfitting problem. L1 and L2 are the most common types of regularization. These update the general cost function by adding another term known as the regularization term. Due to the addition of this regularization term, the values of weight matrices decrease because it assumes that a neural network with smaller weight matrices leads to simpler models. Therefore, it will also reduce overfitting to quite an extent. In many tasks, L2 has proved to achieve better results than L1, so the L2 regularization is widely used.</p><p>Inspired by the above research results, we adjusted the NN model structures and investigated the effectiveness of three approaches to improve the performance of the acoustic models in low-resource environments. The Mono-and-Triphone learning (MAT) is based on multitask learning. Multitask learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better <ref type="bibr" target="#b29">[30]</ref>. We set up a second task to make both context-dependent (CD) and context-independent (CI) targets the learning goals of the network. Besides, we also investigated the effectiveness of the Soft One-hot Label (SOL). We used a new label encoding method based on Gaussian distribution to prevent the over-confidence of the models. We also compared the effect of different acoustic features on the acoustic model. At last, we applied all the three methods on the AMs based on HMM and achieve a remarkable result on the tasks of Mandarin speech recognition in a low-resource environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. OVERVIEW</head><p>This paper is organized as follows: In Sect. 2, we will introduce the Mono-and-Triphone learning method (MAT) based on multitask learning. In Sect. 3, we will introduce our new label encoding method named Soft One-hot label (SOL). In Sect 4, the result of our choice of feature combinations is be shown and discussed. In Sect. 5, we will present our experimental setup, behaviors of the method and results of the experiments. In Sect. 6, we will list the contributions of the proposed method explicitly and summarize this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MONO-AND-TRIPHONE LEARNING BASED ON MULTITASK LEARNING</head><p>In this subsection, we compare the Monophone (Context-Independent) target and Triphone. (Context-Dependent) target. Besides, we introduce the Mono-and-triphone learning method based on multitask learning.</p><p>The pronunciation of a word can be given as a series symbols that correspond to the individual units of sound that make up a word. These are called 'phonemes' or 'phones '. A monophone refers to a single phone. A triphone is simply a group of 3 phones in the form ''L − X + R'', where the ''L'' phone (i.e. the left-hand phone) precedes ''X'' phone and the ''R'' phone (i.e. the right-hand phone) follows it. Table <ref type="table" target="#tab_0">1</ref> shows an example of the conversion of a monophone declaration of the sentence 'I LIKE DOG.' to a triphone declaration. Sil denotes Silence, which means that this phoneme no left or right context phone. Lexical stress is indicated by means of a numeral {0,1,2} attached to a vowel.</p><p>Because triphone can better represent contextual information, when the triphone was proposed, it replaced the monophone and became the mainstream modeling method <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. It has also been shown to achieve better results than the monophone. However, there is a disadvantage to using triphones as DNN targets: there is no distinction between discrimination between different phones, and between different contexts of the same phone. The latter discrimination has a much more limited benefit to producing a more accurate phone hypothesis at test time, because our ultimate goal is just to get the correct phone, not the correct contextual information. However, the two discriminations are both treated equally in cross-entropy DNN training.</p><p>In addition, the number of modeling units of the triphone model is several times greater than that of the monophone model. In the CMU English dictionary, which has close to 130,000 word pronunciations, there are only 43 monophones, but there are close to 6000 triphones. This is also the case in Mandarin and any other language. Therefore, a second problem with triphone compared to monophone being the inherent data sparsity issue in having a large output layer. Increasing the number of output units obviously increases the number of weights to be trained between the output layer and final hidden layer, with fewer samples with fewer samples available to train each weight. Therefore, too many triphone modeling units and very little training data can easily lead to overfitting of the neural network.</p><p>To solve these problems, we investigated a new network structure based on multitask learning <ref type="bibr" target="#b29">[30]</ref>. Our proposed structure is not only trained to optimize a triphone crossentropy (CE) based loss and we give the network a second optimization task, which is the CE of monophone. The first task ''Tri-task'' is effectively a mapping from a set of T training frames to a set of Tri-labels, that is:</p><formula xml:id="formula_0">Tri-task : {t : 1 ≤ t ≤ T } ⇒ {Tri-labels} t ⇒ LB Tri t (1)</formula><p>where t denotes one frame, LB Tri t denotes its label under Tri-task.</p><p>The second task ''Mono-task'' is similar, except that the set of labels is different, and replaced with Mono-labels LB Mono t . These two tasks are combined by a hyper-parameter α, sharing the hidden layer of the neural network, and jointly optimizing the parameters of the neural network. Therefore, the final loss function as follow.</p><formula xml:id="formula_1">Loss = − T t log p(LB Tri t |x t ; θ Tri ) −α T t log p(LB Mono t |x t ; θ Mono ) (2)</formula><p>where x t denotes the acoustic features of each frame, θ Tri and θ Mono denote the parameters of the networks with different out layer. α denotes the weight of monophone loss. The loss function is minimized with respect to parameters θ when learning.</p><p>Although we have two output layers during training, we still use the triphone output layer as the final prediction result during prediction, which mainly considers that the triphone has a great advantage over monophone. The purposes of the monophone task are to effectively limit the complexity of the natural network and improve the generalization of it. Figure <ref type="figure" target="#fig_0">1</ref> shows our network structure. A shared representation between tri-task and mono-task is central to the MAT approach. When computing the gradients, the forward pass can be shared between both tasks, up to the two output layers. We think a single mono-task or a single tri-task is not sufficient. Mono-task is well-defined but not informative enough to guide to the model to a good hidden representation. Tri-task is high-dimensional and can provide more details about the contexts, but a high degree of contexts noise is there especially in a low-resource environment. Therefore, optimizing the two tasks together is a good option. Due to the addition of mono task, compared with the traditional structure, MAT attaches more importance <ref type="bibr">VOLUME 8, 2020</ref> to the correctness of intermediate phonemes. Although the correctness of context is also important, the correctness of intermediate phonemes is what we want. Besides, the addition of a second task also limits the complexity and improves the generalization of the acoustic model.</p><p>In order to prove the effectiveness of MAT in preventing neural networks from overfitting, we have conducted several comparative experiments on various network structures, such as DNN, BiRNN, BiGRU, and BiLSTM (hereinafter referred to as RNN, GRU and LSTM). Experiments show that the method achieves better results than baselines on the lowresource Mandarin recognition task. The details and results will be given in Sect. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SOFT ONE-HOT LABEL</head><p>Here we propose a mechanism based on Gaussian distribution to regularize the classifier layer of the network during training.</p><p>In classification tasks, one-hot is the most commonly used label encoding method. One-hot encoding can be defined as a process of converting categorical variables into a distribution that could be provided to ML algorithms to do a better job in prediction. The encoded label method and its distribution shows in Formula. <ref type="bibr" target="#b2">3</ref>.</p><formula xml:id="formula_2">class(k)(one − hot) = 1, (kislabel) 0, (kisnotlabel) (0 ≤ k ≤ K ) Distribution(one − hot) = [class(0), . . . , class(k), . . . , class(K )] = [0, . . . , 0, 1, 0, . . . 0]<label>(3)</label></formula><p>where K denotes the number of the classes and k denotes each category of all.</p><p>For each training example, our model uses the Softmax layer to compute the probability of each label k ∈ {0 . . .</p><formula xml:id="formula_3">K } p(k|x) = exp(z k ) K i=0 exp(z i )<label>(4)</label></formula><p>here, z i are the logits or unnormalized log probabilities. Then we will get a predicted probability distribution. Our optimization goal is to minimize the cross-entropy (CE) loss between the predicted distribution and the ground-truth label distribution.</p><p>Model over-confidence is promoted by the CE training criterion. For the baseline network, the training loss is minimized when the model concentrates all of its output distribution on the correct ground-truth category. This leads to very peaked probability distributions, effectively preventing the model from indicating sensible alternatives to a given triphone or monophone. Therefore, one-hot encoding labels often also leads to over-confidence and overfitting of the AM.</p><p>In addition, in speech recognition tasks, language models (LM) are often needed to re-score the probability scores derived from acoustic models by fusion as Formula. 5. The language model can linguistically correct the phoneme sequences generated by the acoustic model, and finally get better results. y * = arg max y log p(y|x) + λ log P LM (y) <ref type="bibr" target="#b4">(5)</ref> where P LM (y) is provided by the LM, y * denotes the final score, λ denotes the proportion of language score in the final score, x denotes the training example. However, the ability to language model rescoring is limited. Sometimes, the model-confidence and overfitting can cause the acoustic model (AM) scores of some wrong sequences too high, which may impact the ability of language model to find good solutions and to recover from errors.</p><p>In <ref type="bibr" target="#b39">[40]</ref>, the authors consider a simple technique of adding time-dependent Gaussian noise to the gradient at every training step. The added Gaussian noise improves the generalization of complicated neural networks because it can prevent the model from falling into the local minima during training. However, adding Gaussian noise to the gradient can not solve the problem of over-confidence.</p><p>Inspired by <ref type="bibr" target="#b39">[40]</ref>, we investigated a new label encoding method named ''Soft One-hot Label (SOL)''. It is a regularization mechanism to prevent the acoustic model to making over-confident predictions. The goal of our proposal is to reduce the gap between the probability of the correct category and the wrong categories. SOL can prevent peaked probability distributions and improve the generalization of the acoustic models. Besides, since we reduced the gap of correct and wrong categories, this reduces the AM score and enhances the ability to language model rescoring. Because we have very little audio data, the language model plays a very important role in correct the phoneme sequences.</p><p>In SOL, we don't use directly 0 and 1 to encode our labels into vectors. We give it more randomness. For the true classification, we still assign it a high probability, but for other classifications, we will not make them 0. Instead, they are assigned a small random variable that obeys the Gaussian distribution. We don't think it's a good idea to have a constant value for each category. This will make the neural network try to fit this invariant distribution and impact the adaptability, so the Gaussian distribution which increases the diversity of label vectors is a good choice. Formula. 6 shows the bottom of the next page, our label encoding method and one example of the label vector. where the hyper-parameter δdenotes the value of a high probability, the parameterµ is the mean or expectation of the distribution (and also its median and mode); and σ is its standard deviation,xdenotes a random value in a range and is used to calculate a random number that obeys the Gaussian distribution, K denotes the number of categories.</p><p>In this case, if we use the traditional Gaussian distribution, some generated random numbers Ran will be</p><formula xml:id="formula_4">Ran &lt; − 1 − δ K − 1 (7)</formula><p>and this causes some of the values in the label vector to be negative. As we all know, negative numbers will lead to logarithmic errors, so we add some restrictions to the random number that obeys the Gaussian distribution. To ensure that all values in the SOL vector are positive, we limit the generated Gaussian random numbers Ran in a range:</p><formula xml:id="formula_5">− 1 − δ K − 1 &lt; Ran &lt; 1 − δ K − 1<label>(8)</label></formula><p>In order to implement this limitation, we need to determine whether the random number meets the requirement every time it is generated. If it does not meet the requirement, it will be dropped and regenerated. The encoding result of SOL for the same label is also different because we add Gaussian perturbation. Without the SOL and MAT, the loss function is:</p><formula xml:id="formula_6">loss = − T t log(ce(p(t), q(t))) = − T t log n i=1 (p(t i ) log q(t i ))<label>(9)</label></formula><p>where ce() denotes the cross-entropy function, p(t) denotes the predicted probability distribution calculated by the neural network for the input x, q(t) denotes represents the Vector 0,1 encoded on the label by one-hot. t represents each frame in the training set. T denotes all the frames. n denotes the dimensions of the p(t) and q(t) distribution. We can get the gradient of backpropagation by calculating the partial derivative of the loss function. Then, we apply SOL and MAT to the final loss function and make some changes to Formula. 9. Therefore, we firstly define the predicted probability distribution, p(t) Tri  = θ Tri (x t ) (10)</p><formula xml:id="formula_7">p(t) Mono = θ Mono (x t )<label>(11)</label></formula><p>where θ tri and θ mono denote the parameters of the networks. They share all the hidden and input layers of the network but have a different output layer. BiLSTM, for example, they can also be represented as follows,</p><formula xml:id="formula_8">p(t) Tri = (soft max Tri • dropout 4 • batchnorm 4 • tanh •BiLSTM 4 • • • • •dropout 1 • batchnorm 1 • tanh •BiLSTM 1 )(x t ) (12) p(t) Mono = (soft max Mono • dropout 4 • batchnorm 4 • tanh • BiLSTM 4 • • • • • dropout 1 • batchnorm 1 • tanh • BiLSTM 1 )(x t ) (<label>13</label></formula><formula xml:id="formula_9">)</formula><p>where,</p><formula xml:id="formula_10">BiLSTM = linear • concat(LSTM forward , LSTM backward ) (14)</formula><p>Then we define the ground-truth label vector, q(t) Tri  = SOL(LB Tri t ) ( <ref type="formula">15</ref>)</p><formula xml:id="formula_11">q(t) Mono = SOL(LB Mono t ) (<label>16</label></formula><formula xml:id="formula_12">)</formula><p>where SOL function denotes the Formula. 6. Finally, the loss function with SOL and MAT can be got by modifying the Formula. 9:</p><formula xml:id="formula_13">loss = − T t log(ce(p(t) Tri , q(t) Tri SOL )) −α T t log(ce(p(t) Mono , q(t) Mono SOL )) = − T t log n i=1 (p(t i ) Tri log q(t i ) Tri SOL ) −α T t log n i=1 (p(t i ) Mono log q(t i ) Mono SOL )<label>(17)</label></formula><p>We apply SOL and MAT to the final loss function. The Gaussian perturbation in SOL changes the final loss function and makes the networks inclined to fit a more flexible and less peaked probability distribution. This perturbation can make the network not lead to overconfidence during training, so that the network has a better generalization. This method will increase our final loss but improve the performance of the AM.</p><p>We apply SOL to the baselines and the multitask learning mentioned in the Sect. 2. Experimental results will be present in Section. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. FEATURE COMBINATIONS</head><p>In this subsection we explore the effects of different features and combinations of features on the performance of AM.</p><p>In ASR, the most commonly used acoustic features are Mel Frequency Cepstral Coefficents (MFCC) and Filter banks (Fbank). Almost all speech recognition tasks choose one of these two. Although they have been shown to achieve good results in speech recognition, these two features do not eliminate the differences between different speakers and affect the performance of the acoustic model. Therefore, we propose to combine the traditional features with FMLLR features as the input of the neural networks.</p><formula xml:id="formula_14">class(k)(SOL) =      1 − i =label class(i)(SOL), (k is label) 1−δ K −1 + 1 √ 2πσ exp (x−µ) 2σ 2 2 , (k is notlabel) (0 ≤ k ≤ K ) Distribution(SOL) = [class(0)(SOL), . . . class(k)(SOL), . . . , class(K )(SOL)]</formula><p>= [0.0015, . . . , 0.0034, 0.934, 0.0019, . . . , 0.003] (6)</p><p>Feature-space Maximum Likelihood Linear Regression (FMLLR) was explored in <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> for speaker adaptive training and it is a feature space transform where we transform acoustic features for better fit to a speaker-independent (SI) model. We can get FMLLR features vector according to this formula:</p><formula xml:id="formula_15">ōt = A (n) o t + b (n) = W (n) ξ (t) (<label>18</label></formula><formula xml:id="formula_16">)</formula><p>where The combination of different acoustic features can make the speech signal of each frame more detailed and accurate. In particular, the addition of FMLLR features improves the generalization of the model to different speakers.</p><formula xml:id="formula_17">W (n) = [A (n) , b (n) ]</formula><p>In Sect. 5, a lot of experiments were conducted to select the combination of features. More details of the them will be shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENT A. BASELINE NN-HMM SYSTEM</head><p>All experiments are conducted on Pytorch-Kaldi platform <ref type="bibr" target="#b32">[33]</ref>. We use a single Nvidia TITAN Xp GPU to do single running. Most of the acoustic models are trained with 40-dimensional high resolution MFCC, 40-dimensional Fbank and 40-dimensional FMLLR feature. Those features were computed with a 25ms window and shifted every 10ms. The raw features are normalized via mean subtraction and variance normalization per speaker side.</p><p>Our baseline systems use Natural Networks (DNN, RNN, GRU or LSTM), modelling frame posterior probabilities over triphone units. All of our RNN structures are bidirectional. Unidirectional Recurrent neural networks of that type has the potential disadvantage that it can only take advantage of context information in one direction (usually the past). However, the bi-directional RNN structure makes full use of the context information in both directions, so it is proved to achieve better results. Figure <ref type="figure">.</ref> 2 shows our baseline framework and its components. Dropout <ref type="bibr" target="#b33">[34]</ref> is applied in our baselines. Dropout is an effective way to prevent neural networks from over-fitting. The key idea of dropout is to randomly drop units (along with their connections) from the neural network during the training process.</p><p>The standard test result in Mandarin speech recognition tasks is Character Error Rate (CER) and Word Error Rate (WER). CER is more convincing than WER in our task. The details of the four different baselines are shown in Table <ref type="table">.</ref> 2. Splice stands for whether we splice the features of a frame with adjacent frames, so that the model can learn more sequence characteristics. Because of the particularity of RNN structures, SPLICE does not need to be used on them. We set  the learning rate as the number of iterations decays to ensure that the network can reach the global minima faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. THE-STATE-OF-ART</head><p>Since our proposal (MAT + SOL) resembles a regularization method, our experiment compares the results with L2 regularization, which is the-state-of-art method. L2 regularization <ref type="bibr" target="#b34">[35]</ref> is a technique to discourage the complexity of the model. It does this by penalizing the loss function and the regularization term is the sum of the square of all feature weights like Formula. <ref type="bibr" target="#b18">19</ref>.</p><formula xml:id="formula_18">loss_function = loss + φ * w 2<label>(19)</label></formula><p>where φ denotes regularization parameter. This helps to prevent the overfitting problem by forcing the weights to be small but does not make them zero and does non-sparse solution.</p><p>To verify the effectiveness of our framework, we also compared it with the-state-of-art framework in a low-resource environment, TDNN-HMM based on Kaldi platform <ref type="bibr" target="#b38">[39]</ref>. TDNN-HMM has been proved to work much better than other models when there is very little data. It uses a method of TABLE 3. Comparison results of the experiments with mono-and-triphone learning and baselines. ''Mono 0.9'' means that the weight of mono loss is 0.9. it turn out to be baselines when the weight is 0.0.</p><p>sequence-discriminative training and the objective function we used in the training is LF-MMI (Lattice-Free Maximum Mutual Information) <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, which aims to maximize the probability of the target sequence, while minimizing the probability of all other sequences:</p><formula xml:id="formula_19">F MMI = U u=1 log p(o u |w u ; θ ) k p(w) p(o u ) = U u=1 log p(o u |w u ; θ ) k p(w) w p(o u |w u : θ ) k p(w )<label>(20)</label></formula><p>where o u and w u denote the observed sequences and the correct sequence labels. p(w) represents the prior probability of word sequence w and p(w ) represents a feasible sequence in the search space. θ represents the hyper-parameters of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. DATASET</head><p>Our experiments are conducted on a ∼10 hours training set consisting of 3000 Mandarin utterances. The training set is a subset of THCHS-30 <ref type="bibr" target="#b35">[36]</ref>, the dev set and test set are the same as those of THCHS-30. THCHS-30 involves more than 30 hours of speech signals recorded by a single carbon microphone at the condition of silent office. Most of the participants are young colleague students, and all are fluent in standard Mandarin. The sampling rate of the recording is 16, 000 Hz, and the sample size is 16 bits. The language model used in our experiments involves 48k words and is based on word 3-grams. The LM was trained using a text collection that was randomly selected from the corpus and Aishell-2 <ref type="bibr" target="#b36">[37]</ref> corpus. The training text involves 772, 000 sentences, amounting to 18 million words and 115 million Chinese characters. The LM was trained with the SRILM tool <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. RESULTS</head><p>In order to verify the effectiveness of our proposed methods, we have done a few of comparative experiments. We divided the experiments into three groups, each corresponding to a method to verify the effectiveness of a single method. Finally, we combined the three methods to calculate the best results we achieved. Doing so not only guarantees that all three methods can achieve positive results, but also proves which method has the greatest benefit on our baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) MAT</head><p>In this subsection, we verified the effectiveness of Mono-And-Triphone learning (MAT). We performed comparative implementation on all four tasks. The experimental results are shown in Table <ref type="table">.</ref> 3.</p><p>The CER results in the table strongly prove the effectiveness of MAT on four different structures.</p><p>Performance of acoustic models with different values of mono-weight (that isα, in Formula. 2) is present in Figure <ref type="figure" target="#fig_3">3</ref>. It shows clearly for CER curves on the test set when the value of mono-weight is increased from 0.0 to 1.0. If mono-weight is equal to 0.0, then it turns out to be the baselines. It can be seen that most experiments have improvement compared to baselines, which proves the effectiveness of MAT training. The best acoustic model is obtained when 0.9 is provided, with 2.6% (DNN), 3.5% (RNN), 3.7% (GRU) and 2.2% (LSTM) relatively CER reduction over baselines. It's easy to understand that when the value of mono-weight is too large, it performs worse than the baseline, which is due to the dominance of mono-loss in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) SOL</head><p>This set of experiments look at comparing the performance of the Soft One-hot label (SOL) and One-hot label (OL) on the test set. In experiments with SOL, we set the value of δ in Formula. 6 to 0.95, because it is necessary to ensure that a high probability is assigned to the ground-truth label.</p><p>We apply SOL to triphone learning task and MAT. On MAT, We have two types of labels ''mono-label'' and ''trilabel'', so we can apply SOL to a single task or to all tasks. SOL(Tri) denotes that we only apply SOL method on the trilabels. The experimental results are shown in Table <ref type="table">.</ref> 4.</p><p>The experimental results show that SOL can effectively alleviate over-fitting and improve the performance of the model, whether on MAT tasks or not. And it is better to use SOL on both mono-task and tri-task. On the Triphone learning tasks, SOL achieved a relative 1.9% reduction in CER on DNN-HMM, 1.8% on RNN-HMM, 0.9% on GRU-HMM and 1.1% on LSTM-HMM. On the MAT tasks, SOL(Tri+Mono) achieved 1.3% reduction on DNN-HMM, % 0.8 on RNN-HMM, 1.3% on GRU-HMM and 1.7% on LSTM-HMM. Not only that, but our results go beyond the L2 regularization method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) FEATURE CHOOSE</head><p>At last, we conduct experiments to compare the performance of feature combinations and gather all experimental results. We choose two or three of MFCC, FBANK, FMLLR to combine and train the acoustic model, then choose the best performing one as our final feature combination. As shown in Table <ref type="table">.</ref> 5, there are seven experiments, including three initial features and four combined features. These experiments are based on baselines, and neither MAT nor SOL is applied to them.</p><p>It can be seen in the table that the combination of features brings great benefits. In terms of a single feature, FMLLR achieves the best experimental results due to speaker adaptation. Fbank works least, especially on RNN and its variant structure. When the three features are combined, the model gets the best effect because the multiple features represent a frame of speech signal better. When we use all three features to train the AM, 7.6% (DNN), 7.4% (RNN), 5.5% (GRU) and 4.5% (LSTM) relative CER reduction over baselines are obtained.</p><p>It can be seen that different features give different representations of the same frame of speech signals. Although this slightly increased the complexity of the model, great gains were made. Therefore, we choose the combination of ''MFCC + FBANK + FMLLR'' as our model input features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) ALL THE METHODS</head><p>Finally, we applied all the three methods mentioned in this paper to our acoustic model modeling. We conducted experiments on all four baselines, and the experimental results are shown in table 6. It can be seen from the table that the three methods all can improve the hybrid hidden Markov model -neural network approach on phoneme level. Comto baseline, our proposals have achieved relative CER reductions of 7.8% (DNN), 11.0% (RNN), 8.6% (GRU) and 7.5% (LSTM) respectively. Compared with the-state-of-art L2 regularization method, our proposal MAT+SOL has also achieved some improvements. Compared to the result of the TDNN-HMM framework, all the four frameworks exceed it. The GRU-HMM framework achieves the-state-of-art result and relative CER reductions of 11.3% compared to TDNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we investigated the effects of three methods on the performance of acoustic modeling in low-resource environments. We conducted separate comparison experiments on each method on the Mandarin speech recognition task, and finally combined the three methods together. Experimental results show that all three methods can effectively improve the recognition accuracy. MAT+SOL is a new regularization method that can improve overfitting. It works better than L2 regularization especially in a low-resource environments, and our experiments prove that. SOL is a new label encoding method with Gaussian perturbation, which can prevent overconfidence of the model. Feature combination provides a new feature selection scheme for acoustic modeling. We believe they can also be applied to end-to-end models.</p><p>We only conducted experiments on the NN-HMM structure and not on the end-to-end model because the end-to-end model performed too poorly in low-resource environments.</p><p>In future research, we will continue to explore how to better limit the complexity of network models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 1 .</head><label>1</label><figDesc>FIGURE 1.The mono-and-triphone learning framework which is similar to multitask learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>stands for the transformation matrix and ξ (t) = [o T t , 1] T represents the extended feature vector. Before training the SI model, we have an initial matrix W (n) , then construct the transformed features iteratively train the new parameters of SI model. After many iterations, we can get a better W (n) for us to perform FMLLR feature transformation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIGURE 2 .</head><label>2</label><figDesc>FIGURE 2. The NN-HMM framework and its components.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FIGURE 3 .</head><label>3</label><figDesc>FIGURE 3. The CER curves for the test set with different value of mono-weight in the range [0.0, 1.0] on four baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>TABLE 6 .</head><label>6</label><figDesc>Comparison of the final results on four baselines. The value of mono-weight is 0.9 and the input features are MFCC + Fbank + FMLLR. The hyper-parameter of SOL is 0.95.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 .</head><label>1</label><figDesc>Comparison of the monophone declaration and triphone declaration of the sentence ''i like dog''.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 .</head><label>2</label><figDesc>The details of our four baselines.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 4 .</head><label>4</label><figDesc>Comparison results of the experiments with soft one-hot label and the without. There are two groups of comparison in the table, one is on the basis of baseline, and the other is on the structure of mat applied.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 5 .</head><label>5</label><figDesc>Comparison results of the three experiments with different features and for experiments with different combinations of features.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="73006" xml:id="foot_1">   VOLUME 8, 2020   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2">VOLUME 8, 2020   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="73010" xml:id="foot_3">   VOLUME 8, 2020   </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank Bo Wang from NARI Group Corporation (State Grid Electric Power Research Institute). The author Xiusong Sun would like to thank Die Hu for her support of English grammar and spelling.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>under Grant 1015-KFA19035, and in part by the project Research and Application of Intelligent Technology for Real-time Dispatching of Power Grid Based on Artificial Intelligence.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
				<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1764" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition,&apos;&apos; in Proc</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)</title>
		<imprint>
			<biblScope unit="page" from="4960" to="4964" />
			<date type="published" when="2016-03">Mar. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Endto-end attention-based large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)</title>
				<meeting>IEEE Int. Conf. Acoust., Speech Signal ess. (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2016-03">Mar. 2016</date>
			<biblScope unit="page" from="4945" to="4949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd Int. Conf. Mach. Learn</title>
				<meeting>23rd Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Advances in joint CTC-attention based End-to-End speech recognition with a deep CNN encoder and RNN-LM</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02737</idno>
		<ptr target="http://arxiv.org/abs/1706.02737" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep learning</title>
				<imprint>
			<date type="published" when="2015-05">May 2015</date>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page">436</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep neural network features and semi-supervised training for low resource speech recognition,&apos;&apos; in Proc</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Conf. Acoust., Speech Signal Process</title>
		<imprint>
			<biblScope unit="page" from="6704" to="6708" />
			<date type="published" when="2013-05">May 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jialin Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010-10">Oct. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transfer learning for image classification with sparse prototype representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transfer learning for speech and language processing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asia-Pacific Signal Inf. Process. Assoc. Annu. Summit Conf. (APSIPA)</title>
				<meeting>Asia-Pacific Signal Inf. ess. Assoc. Annu. Summit Conf. (APSIPA)</meeting>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
			<biblScope unit="page" from="1225" to="1237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
				<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">'</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Pre-trained language model representations for language generation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.09722</idno>
		<ptr target="http://arxiv.org/abs/1903.09722" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving language understanding with unsupervised learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>OpenAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Wav2vec: Unsupervised pre-training for speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05862</idno>
		<ptr target="http://arxiv.org/abs/1904.05862" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Improving speech emotion recognition via transformer-based predictive coding through transfer learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07691</idno>
		<ptr target="http://arxiv.org/abs/1811.07691" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The calculation of posterior distributions by data augmentation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Tanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">398</biblScope>
			<biblScope unit="page" from="528" to="540" />
			<date type="published" when="1987-06">Jun. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The art of data augmentation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Van Dyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Graph. Statist</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="50" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Tanner</surname></persName>
		</author>
		<title level="m">Tools for Statistical Inference: Observed Data and Data Augmentation Methods</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">67</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Elastic spectral distortion for low resource speech recognition with deep neural networks,&apos;&apos; in Proc</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Obuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Workshop Autom. Speech Recognit. Understand</title>
		<imprint>
			<biblScope unit="page" from="309" to="314" />
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Vocal tract length perturbation (VTLP) improves speech recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn. (ICML)</title>
				<meeting>Int. Conf. Mach. Learn. (ICML)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Audio augmentation for speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">'</forename><surname>Specaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08779</idno>
		<ptr target="http://arxiv.org/abs/1904.08779" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning augmentation policies from data</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">'</forename><surname>Autoaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<ptr target="http://arxiv.org/abs/1805.09501" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatic gain control and multi-style training for robust small-footprint keyword spotting with deep neural networks,&apos;&apos; in Proc</title>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Parada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nakkiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)</title>
		<imprint>
			<biblScope unit="page" from="4704" to="4708" />
			<date type="published" when="2015-04">Apr. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Data augmentation for robust keyword spotting under playback interference</title>
		<author>
			<persName><forename type="first">A</forename><surname>Raju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Panchapagesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Strom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.00563</idno>
		<ptr target="http://arxiv.org/abs/1808.00563" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Context-dependent modeling for acoustic-phonetic recognition of continuous speech,&apos;&apos; in Proc</title>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kimball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roucos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krasner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)</title>
		<imprint>
			<biblScope unit="page" from="1205" to="1208" />
			<date type="published" when="1985-04">Apr. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tree-based state tying for high accuracy acoustic modelling</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Odell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Woodland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop Human Lang</title>
				<meeting>Workshop Human Lang</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="307" to="312" />
		</imprint>
	</monogr>
	<note type="report_type">Technol</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Feature and model space speaker adaptation with full covariance Gaussians</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Int. Conf. Spoken Lang. Process</title>
				<meeting>9th Int. Conf. Spoken Lang. ess</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A maximum-likelihood approach to stochastic matching for robust speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Process</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="190" to="202" />
			<date type="published" when="1996-05">May 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The pytorch-kaldi speech recognition toolkit</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)</title>
				<meeting>IEEE Int. Conf. Acoust., Speech Signal ess. (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2019-05">May 2019</date>
			<biblScope unit="page" from="6465" to="6469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A vibrating mechanism to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wireless Commun. Mobile Comput. Conf. (IWCMC)</title>
				<imprint>
			<date type="published" when="2019-06">Jun. 2019</date>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
	<note>Proc. 15th Int</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast image reconstruction with L 2 regularization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Setsompop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Cauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adalsteinsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Magn. Reson. Imag</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="181" to="191" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">THCHS-30: A free chinese speech corpus</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01882</idno>
		<ptr target="http://arxiv.org/abs/1512.01882" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">AISHELL-2: Transforming mandarin ASR research into industrial scale</title>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.10583</idno>
		<ptr target="http://arxiv.org/abs/1808.10583" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">SRILM-an extensible language modeling toolkit</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Int. Conf. Spoken Lang. Process</title>
				<meeting>7th Int. Conf. Spoken Lang. ess</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A time delay neural network architecture for efficient modeling of long temporal contexts</title>
		<author>
			<persName><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th Annu</title>
				<meeting>16th Annu</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Adding gradient noise improves learning for very deep networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06807</idno>
		<ptr target="http://arxiv.org/abs/1511.06807" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">AdapNet: Adaptability decomposing encoder-decoder network for weakly supervised action recognition and localization</title>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2019.2962815</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst., early access</title>
		<imprint>
			<date type="published" when="2020-01-23">Jan. 23, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning transferable self-attentive representations for action recognition in untrimmed videos with weak supervision</title>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf</title>
				<meeting>AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2019-07">Jul. 2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9227" to="9234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bidirectional active learning: A twoway exploration into unlabeled and labeled data set</title>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3034" to="3044" />
			<date type="published" when="2015-12">Dec. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">He is currently pursuing the M.S. degree with the College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing. His current research interests include signal processing, speech recognition, machine learning</title>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neurocomputing</title>
				<meeting><address><addrLine>Nanjing, China; Nanjing, China; Nanjing; Nanjing, China; Nanjing; Nanjing, China; Nanjing</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">Jun. 2019. 2017. 2017</date>
			<biblScope unit="volume">345</biblScope>
			<biblScope unit="page" from="103" to="113" />
		</imprint>
		<respStmt>
			<orgName>Computer Science and Technology, Nanjing University ; Computer Science and Technology, Nanjing University of Aeronautics and Astronautics ; Computer Science and Technology, Nanjing University ; Computer Science and Technology, Nanjing University of Aeronautics and Astronautics ; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics</orgName>
		</respStmt>
	</monogr>
	<note>His current research interests include natural language processing, speech recognition, and machine learning. XIN YUAN received the B.S. degree from the Nanjing Institute of Technology. He is currently pursuing the M.S. degree with the. His current research interests include signal processing, speech recognition, and machine learning</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
