<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Haibo He, Senior Member, IEEE</roleName><forename type="first">Guoqian</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ping</forename><surname>Xie</surname></persName>
							<email>pingx@ysu.edu.cn</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Yufei</forename><surname>Tang</surname></persName>
							<email>tangy@fau.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">Yanshan University</orgName>
								<address>
									<postCode>066004</postCode>
									<settlement>Qihuangdao</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical, Computer and Biomedical Engineering</orgName>
								<orgName type="institution">University of Rhode Island</orgName>
								<address>
									<postCode>02881</postCode>
									<settlement>Kingston</settlement>
									<region>RI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical, Computer and Biomedical Engineering</orgName>
								<orgName type="institution">The University of Rhode Island</orgName>
								<address>
									<postCode>02881</postCode>
									<settlement>Kingston</settlement>
									<region>RI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">Yanshan University</orgName>
								<address>
									<postCode>066004</postCode>
									<settlement>Qinhuangdao</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer and Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">Florida Atlantic University</orgName>
								<address>
									<postCode>33431</postCode>
									<settlement>Boca Raton</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Institute for Sensing and Embedded Network Systems Engineering</orgName>
								<orgName type="institution">Florida Atlantic University</orgName>
								<address>
									<postCode>33431</postCode>
									<settlement>Boca Raton</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C11469501014335D0B643CF57A484EAB</idno>
					<idno type="DOI">10.1109/TIM.2017.2698738</idno>
					<note type="submission">received November 22, 2016; revised February 7, 2017; accepted March 11, 2017.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fault diagnosis</term>
					<term>multilevel-denoising (MLD) training</term>
					<term>stacked denoising autoencoders (SDAEs)</term>
					<term>vibration representation learning</term>
					<term>wind turbine (WT) gearbox</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Currently, vibration analysis has been widely considered as an effective way to fulfill the fault diagnosis task of gearboxes in wind turbines (WTs). However, vibration signals are usually with abundant noise and characterized as nonlinearity and nonstationarity. Therefore, it is quite challenging to extract robust and useful fault features from complex vibration signals to achieve an accurate and reliable diagnosis. This paper proposes a novel feature representation learning approach, named stacked multilevel-denoising autoencoders (SMLDAEs), with the aim to learn robust and discriminative fault feature representations through a deep network architecture for diagnosis accuracy improvement. In our proposed approach, we design an MLD training scheme, which uses multiple noise levels to train AEs. It enables to learn more general and detailed fault feature patterns simultaneously at different scales from the complex frequency spectra of the raw vibration data, and therefore helps enhance the feature learning and fault diagnosis capability. Furthermore, SMLDAE-based fault diagnosis is performed with an unsupervised representation learning procedure followed by a supervised fine-tuning process with label information for classification. Our approach is evaluated by using the field vibration data collected from a self-designed WT gearbox test rig. The results show that our proposed approach learned more robust and discriminative fault feature representations and achieved the best diagnosis accuracy compared with the traditional approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>G EARBOX is one of the critical components in wind turbines (WTs), and is known to have high downtimes and maintenance costs due to the complexity of their repair and maintenance procedures <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Even worse, WT gearboxes often suffer from various failures, especially in gears and bearings, due to constantly varying loads. To ensure the safety of the WT system and reduce the maintenance cost caused by gearbox failures, vibration-based fault diagnosis systems have been intensively investigated in recent years <ref type="bibr" target="#b2">[3]</ref>. However, vibration signals obtained from the gearbox are high dimensional and nonlinear, and therefore, it is quite challenging to extract effective and meaningful features from the raw vibration signals to achieve an accurate and reliable diagnosis.</p><p>Traditionally, vibration-based fault diagnosis contains three main steps: vibration data acquisition (DAQ), feature extraction, and fault classification <ref type="bibr" target="#b3">[4]</ref>. The diagnosis performance heavily relies on the effectiveness of feature extraction and classification methods, where the former generally plays a more important role than the latter. The main objective of feature extraction is to extract representative features, which can characterize the health conditions of the underlying machine and also help the downstream fault recognition task. In the past decades, a lot of attention has been paid on developing various signal processing algorithms to extract effective features from raw vibration data for diagnosis. These methods include basic time-domain statistical and spectral analysis, wavelet transform <ref type="bibr" target="#b4">[5]</ref>, Hibert-Huang transform <ref type="bibr" target="#b5">[6]</ref>, timefrequency manifold <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, sparse representation <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, and multiscale complexity measure <ref type="bibr" target="#b10">[11]</ref>, among others. However, feature representations extracted by these methods are often hand-designed, require significant amounts of domain knowledge and human labor, and do not generalize well to new diagnosis domains. Therefore, it is highly desirable to develop automatic feature extraction algorithms less dependent on prior knowledge or human labor to achieve an intelligent fault diagnosis.</p><p>Recently, representation learning in the context of deep learning has attracted considerable attentions in the machine learning community <ref type="bibr" target="#b11">[12]</ref>, which aims to automatically identify patterns and dependencies in data through a deep network architecture to learn compact and general representations that make it easier to extract useful information when building classifiers or predictors <ref type="bibr" target="#b12">[13]</ref>. Deep models can learn multilayer nonlinear transformations from the input data to the output representations, which is more powerful in feature extraction than those hand-crafted ways. Furthermore, deep models can progressively capture more abstract features at higher layers, corresponding to the hierarchical human vision system. In recent years, a number of deep learning algorithms have been proposed to automatically learn more abstract and useful representations via multiple nonlinear transformations through deep network architectures <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Also, recent studies have shown that the deep learning-based methods produce state-ofthe-art performance over those traditional feature extractors on many challenging classification and regression problems, such as image classification <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, face recognition <ref type="bibr" target="#b16">[17]</ref>, and speech recognition <ref type="bibr" target="#b17">[18]</ref>. Inspired by these successful achievements, in order to deal with complex vibration data, different unsupervised learning networks, such as deep belief network <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, stacked denoising autoencoders (SDAEs) <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, sparse AE <ref type="bibr" target="#b22">[23]</ref>, sparse filtering <ref type="bibr" target="#b23">[24]</ref>, and multilayered extreme learning machine <ref type="bibr" target="#b24">[25]</ref>, have been reported in the fault diagnosis field. These methods learn deep discriminative features directly from the raw data in time or frequency domain and greatly improve the diagnosis accuracy compared with traditional approaches <ref type="bibr" target="#b25">[26]</ref>, which provide a promising alternative solution to feature extraction for fault diagnosis.</p><p>In this paper, we focus on feature extraction from complex vibration data using the stacked denoising autoencoder (SDAE) method, mainly due to its capability of learning robust representations from noisy data. However, during the training process of DAE, we notice that a single fixed noise level is used in the literature, which can only capture partial information at a single corrupted scale. In fact, vibration signals measured from the WT gearbox not only characterize with abundant noise, nonlinearity, and nonstationarity caused by varying speeds and loads, but, more importantly, they generally present wideband and multiscale characteristics. In order to utilize detailed information at multiple scales from the input vibration data for effective nonlinear fault pattern features extraction, motivated by the work in <ref type="bibr" target="#b26">[27]</ref>, this paper proposes a new representation learning approach, named stacked multilevel-DAEs (SMLDAEs), to learn better feature representations and improve fault diagnosis performances. The main contributions of this paper are summarized as follows.</p><p>1) A new MLD training scheme for the traditional DAE has been proposed in this paper. Instead of a single fixed noise level, multiple noise levels are used to sequentially train a single AE model. Its key advantage is to capture more detailed information at different scales from input data. 2) Using the proposed MLD training scheme, SMLDAE is further developed to learn the high-level and abstract features at the higher layers through a deep network architecture. Furthermore, a fault diagnosis model based on SMLDAE is built to learn robust and discriminative representations and establish the complex mapping between vibration spectra and gearbox health conditions. 3) The proposed SMLDAE approach is evaluated and compared with the existing deep learning methods, stacked autoencoder (SAE) and SDAE, using the field vibration data from our self-designed WT gearbox test rig. SML-DAE yields better performance in terms of average diagnosis accuracy and cluster separability in the reduced 2-D feature map. Meanwhile, we provide a comprehensive analysis of the benefits of the designed MLD scheme on fault diagnosis performance improvements. 4) Furthermore, robustness performance evaluation of the proposed SMLDAE approach under different operation conditions has been conducted in this paper. A fault diagnosis model is built using the training data from one known operation condition, and then is tested on the testing data from another unseen operation condition. Under this challenging diagnosis task, our proposed SMLDAE method still presents a superior performance. The rest of this paper is organized as follows. Section II briefly reviews the theoretical background, including DAE and its stacked version SDAE. In Section III, we propose a new MLD training scheme to improve the traditional DAE and the corresponding SMLDAE for vibration data feature representation learning. Based on the proposed SMLDAE approach, a generic fault diagnosis procedure is also summarized in this section. In Section IV, experiments are carried out on our selfdesigned WT gearbox test rig to evaluate the effectiveness of the proposed method. Finally, the conclusions are drawn in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head><p>In this section, we first briefly review the basic algorithms, i.e., DAE and SDAE, respectively. Based on the traditional DAE and SDAE, a new MLDAE algorithm and its corresponding stacked version will be proposed in Section III.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Denoising Autoencoder</head><p>Vincent et al. <ref type="bibr" target="#b27">[28]</ref> proposed a modified version of a classical AE named DAE. The basic idea behind DAE is to reconstruct original input from a corrupted one with the goal of obtaining robust representation. By doing so, it can prevent the AE from just simply learning an identity mapping between the input and the reconstructed output, and capture more informative hidden patterns and obtain robust and powerful representations from the raw noisy data. Furthermore, DAE can generalize well and produce compounding benefits when it is stacked into a deep network. Specifically, like the conventional AE, DAE also consists of an encoder process and a decoder process. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>(a), before encoding, the original input data x are corrupted into x by means of a stochastic mapping x = q D (x|x). In general, two common choices, additive Gaussian noise and zero-masking noise where a fraction of input values is randomly forced to zero <ref type="bibr" target="#b28">[29]</ref>, can be used here for the original data corruption. In this paper, we will consider the latter as the corruption process. After that, the corrupted input x is then mapped to a hidden representation h by using where θ f = {W 1 , b 1 } is the parameter set containing a transformed weight matrix W 1 and a bias vector b 1 . Typically, the sigmoid function σ (x) = 1/(1 + exp(-x)) is used for the nonlinear deterministic mapping. The hidden layer code h can be viewed as a compression of input data with some loss when the number of hidden units is less than the number of input units. It can capture the main variations in the highdimensional input data and eliminate those less important information through dimension reduction.</p><formula xml:id="formula_0">h = f (x, θ f ) = σ (W 1 x + b 1 )<label>(1)</label></formula><p>Then, the hidden representation h is mapped back to a reconstruction output x as (2) through the decoder</p><formula xml:id="formula_1">x = g(h, θ g ) = σ (W 2 h + b 2 ) (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where θ g = {W 2 , b 2 }. Note that the key difference of DAE from the classical AE is that x is the reconstruction of x rather than the original input x. The training process of DAE is to find both optimal parameter sets θ f and θ g by minimizing the squared reconstruction error between x and x as follows:</p><formula xml:id="formula_3">L(x, x) = n i=1 x i -xi 2 (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where n is the number of input training data, and the learned features are embedded in W 1 . Once trained, new input data can be fed into the encoder (1) to perform a nonlinear transformation and obtain the corresponding hidden representation h for subsequent tasks, such as classifications and regressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Stacked Denoising Autoencoder</head><p>An SDAE is composed of multiple DAEs, in which they are treated as individual building blocks stacked in the deep architecture, with the aim of finding highly nonlinear and complex patterns in the data <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b28">[29]</ref>. In general, the whole training process of SDAE includes two steps: an unsupervised pretraining step and a supervised fine-tuning step, as shown in Fig. <ref type="figure" target="#fig_0">1(b</ref>) and (c), respectively. Given a set of training data, the learning of SDAE is started by a greedy layerwise pretraining procedure, which learns a stack of DAEs in the encoder network. The key concept in the greedy layerwise learning is to train one layer every time. As shown in Fig. <ref type="figure" target="#fig_0">1(b</ref>), DAE at the bottom layer is first trained with the raw data to obtain its hidden representations, and then, the obtained hidden representations are used as the input data for training the higher level DAE, and so on. Note that the input corruption is used for the initial denoising training of each individual layer <ref type="bibr" target="#b28">[29]</ref>. This pretraining process is task-free and focuses on the hierarchical representation learning from unlabeled data in an unsupervised manner.</p><p>After the layerwise pretraining, all hidden representation layers are stacked, and a logistic regression layer can further be added on top of the stacked AEs, yielding a deep architecture as shown in Fig. <ref type="figure" target="#fig_0">1(c</ref>). The parameters of the whole deep network are first initialized by the corresponding parameters learned in the pretraining phase, and then are fine-tuned with label information using the backpropagation (BP) algorithm. Specifically, in order to speed up the learning speed, the batch stochastic gradient descent (SGD) method with a momentum can be used to update the weights. In this way, the learned representations can capture more discriminative components in the raw data.</p><p>Recent studies have shown that the SDAEs greatly improve the generation performance of neural networks, and can learn more robust and composed features. SDAE has been extensively applied to various classification and recognition tasks, such as image classification <ref type="bibr" target="#b28">[29]</ref>, domain adaptation <ref type="bibr" target="#b29">[30]</ref>, object tracking <ref type="bibr" target="#b30">[31]</ref>, saliency detection <ref type="bibr" target="#b31">[32]</ref>, electrocardiogram active classification <ref type="bibr" target="#b32">[33]</ref>, and so on. In this paper, we will propose a new denoising training scheme to improve the representations learned based on the traditional SDAE algorithm, and apply it to vibration feature representation learning for fault diagnosis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. STACKED MULTILEVEL-DENOISING AUTOENCODER</head><p>FOR REPRESENTATION LEARNING The main idea of the proposed SMLDAE is to use multiple noise levels to train a single AE instead of a single fixed noise level used in the traditional DAE. Then, the SMLDAE-based fault diagnosis procedures are also proposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multilevel-Denoising Training</head><p>Traditionally, DAE uses a single fixed noise level to corrupt the original data and to learn feature representations from the corrupted data. Obviously, the noise level is an important tuning parameter in the DAE algorithm, which affects the degree of corruption of the input and also has a significant effect on the learned representation. On one hand, the noise level needs to be chosen by users in advance; on the other hand, the noise level is kept fixed during the whole training phase. Recent studies <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b33">[34]</ref> in the deep learning community, especially in image classification application domains, have shown that when the inputs are more heavily corrupted at a high noise level during training, the network tends to learn coarse-grained features; whereas when the input is only slightly corrupted at a low noise level, the network tends to learn fine-grained features. This motivates us to train the AE using multiple noise levels instead of a single fixed noise level, which enables to learn more general and detailed feature patterns simultaneously at different scales. Thus, the learned representation can contain more information hidden in data and greatly help the downstream recognition tasks. In this paper, the AE with the MLD training scheme is called MLDAE.</p><p>Fig. <ref type="figure">2</ref> shows the training process of MLDAE, which is a sequential training way using multiple noise levels υ 0 &gt; • • • &gt; υ T ≥ 0, where υ 0 and υ T are the initial noise level and the final noise level, respectively. In this way, general (i.e., coarse-grained) feature patterns are first learned, and then, those fine-grained patterns will further be captured as the noise levels gradually decrease. The detailed training procedure is summarized in Algorithm 1. First, a classical DAE is trained under the initial noise level υ 0 , and the optimal model parameters and hidden representation are obtained. Then, the trained DAE will be further sequentially trained under a sequence of noise levels. In fact, the whole training of MLDAE can be viewed as a fine-tuning process of model parameters (including weight matrices W 1 and W 2 and biases b 1 and b 2 ) under different noise levels from υ 0 to υ T , as shown in Fig. <ref type="figure">2</ref>. In this way, the learned representation h is also tuned to learn and incorporate more information at different scales from coarsegrained ones to fine-grained ones related with input data, thus enabling to discover more useful fault patterns hidden in data.</p><p>We can explain the MLDAE algorithm from the perspective of manifold learning. Conventional DAE can be seen as a way to define and learn a manifold from the given data <ref type="bibr" target="#b27">[28]</ref>. In MLDAE, the original training samples are corrupted by using different noise levels, thus leading to different learned representations, which may contain partial information at different scales. For a high-level noise, AE is forced to reconstruct the original data using only few input neurons; hence, it learns only about manifold around a larger neighborhood of input data and captures the general features of the input data. For a low-level noise, AE can reconstruct the original input more easily with more input neurons and looks at a smaller neighborhood around the manifold about the data to</p><formula xml:id="formula_5">Algorithm 1 MLDAE Training Algorithm 1: procedure MLDAE(x, υ 0 , υ, υ T , K ) 2:</formula><p>x ∈ R n×m is the input matrix, in which</p><formula xml:id="formula_6">x i ∈ [0, 1] m (1 ≤ i ≤ m) 3:</formula><p>υ 0 is the initial noise level 4:</p><p>υ T is the final noise level, υ T &lt; υ 0 5:</p><p>υ is the noise decreasing step size 6:</p><p>K is the iteration steps on the noise level υ t (0 &lt; t &lt; T )</p><p>7:</p><p># Train DAE using initial noise level υ 0 8:</p><p>x is corrupted into x0 with the corruption process q D (x 0 |x, υ 0 ) 9:</p><p>Train DAE using Eqs. ( <ref type="formula" target="#formula_0">1</ref>)-( <ref type="formula" target="#formula_3">3</ref>) until the model parameters </p><formula xml:id="formula_7">θ f = {W 1 , b 1 } and θ g = {W 2 , b 2 } converge,</formula><formula xml:id="formula_8">υ t := υ t -1 -υ 13:</formula><p>for 1 to K do 14:</p><p>x is corrupted into xt with the corruption process q D (x t |x, υ t )</p><p>15:</p><p>Take Eqs. ( <ref type="formula" target="#formula_0">1</ref>)-( <ref type="formula" target="#formula_3">3</ref>) to continue to train DAE using noise level υ t 16:</p><p>end for 17:</p><p># Model parameters and learned representation are fined tuned 18:</p><formula xml:id="formula_9">W 1 ←-W 1 + W 1 , b 1 ←-b 1 + b 1 19: W 2 ←-W 2 + W 2 , b 2 ←-b 2 + b 2 20: h ←-h + h 21:</formula><p>end for 22: end procedure capture detailed features. Therefore, AE with multiple noise levels can learn and incorporate different scale information about the data; thus, it enables to learn the manifold in a better way than the fixed noise level at a predefined value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Stacked Multilevel-Denoising Autoencoder</head><p>Vibration signals generally characterize with complex nonlinear patterns and correspondingly present complex frequency spectral distribution with wide bands and multiscale characteristics. For example, it is known that faults occurred in gearboxes will induce low-frequency impact components and demodulated sidebands in the frequency spectral distribution of vibration signals, while these fault patterns are generally easily masked by those inherent high-frequency components with dominant amplitudes. In turn, this will increase the difficulty of feature extraction. Therefore, in order to extract more useful fault pattern features at different scales hidden in input data and improve the fault diagnosis ability, an SMLDAE is proposed in this paper. In the same way as SDAE does in Fig. <ref type="figure" target="#fig_0">1(b)</ref>, multiple MLDAEs are stacked to form a deep network architecture to learn high-level representations from vibration data. Different from the training of SDAE, each DAE at each stacked layer of SMLDAE is trained using multiple noise levels. In this way, each AE can better uncover more hidden information in complex vibration data. Specifically, the training of a three-layer SMLDAE is shown in Fig. <ref type="figure">3</ref>. The three DAEs, i.e., DAE1, DAE2, and DAE3, are first trained under the initial noise level υ 0 , where the output of the hidden layer of the former DAE will be treated as the input of the next DAE. Next, DAE1 will be further sequentially trained under multiple noise levels from υ 1 to υ T less than the initial noise level υ 0 , and correspondingly, the model parameters, including weights and bias, are further fine-tuned. In the same way, DAE2 and DAE3 will also be trained using the proposed MLD scheme. In other words, MLDAE is trained at each stack layer. Finally, the model parameters and feature representation capturing nonlinear patterns are obtained. This deep network structure with the new denoising training scheme will help improve feature extraction and classification capability in fault diagnosis applications.</p><p>After the layerwise unsupervised learning, all representation layers are stacked to form a deep network, and a classification layer is added on the top of the SMLDAE. In this paper, the softmax layer is used for its simplicity. The network parameters are further fine-tuned using label information, thus enabling to enhance the within-class compactness of representation learned in the unsupervised learning phase, and result in more discriminative feature representations, which is beneficial for the following fault classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. SMLDAE for Fault Diagnosis</head><p>In this section, we summarize the fault diagnosis procedures based on our proposed SMLDAE approach, as shown in Fig. <ref type="figure" target="#fig_2">4</ref>. The fault diagnosis procedures mainly consist of three steps: signal acquisition, representation learning, and fault classification. In this paper, we use frequency spectra of raw vibration signals as the input for model training. This is mainly due to that frequency spectral show that how their constitutive components are distributed with discrete frequencies and provide clear information about the health conditions of the gearbox <ref type="bibr" target="#b20">[21]</ref>. In summary, we can perform </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Merits of SMLDAE</head><p>In general, some important merits of our proposed SMLDAE approach are summarized as follows.</p><p>1) We design an MLD training scheme to sequentially train each AE under different noise levels and fine-tune the learned feature mapping function. Thus, learned representations can capture more detailed information and useful patterns from different scales hidden in raw data, which are beneficial for fault classification.</p><p>2) The stacked network architecture can learn multiple nonlinear transformations, capturing main variations in raw data. Such a DNN is able to adaptively mine essential characteristics from vibration data and establish the complex nonlinear mapping between frequency spectral data and health conditions labels.</p><p>3) The learned robust feature representations can boost the training of the classifier and finally improve the diagnosis accuracy. In addition, the corruption of data using multiple noise levels in SMLDAE actually generates artificial data to expand the data size, which alleviates the small size problem of the training data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CASE STUDY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiment Setup and Data Description</head><p>In this paper, the proposed SMLDAE approach is applied to fault diagnosis of a WT gearbox. The experiments were carried out on the WT gearbox test rig in our laboratory as shown in Fig. <ref type="figure" target="#fig_3">5</ref>, which is designed for simulating the operation of a real WT system. It mainly consists of three critical components: a reducer composed of a 3-kW variablefrequency induction motor coupled with a speed reducer with a ratio of 40, a two-stage parallel gearbox with a total gear ratio of 20, and a 3-kW three phase permanent magnet synchronous generator connected with a load bank. The reducer is used to emulate the dynamics of a WT rotor, and the gearbox is used to emulate the gearbox in a WT. The lower speed at the output shaft of the reducer will increase through the gearbox to drive the generator for power generation. In our experiments, the generated power will be consumed by the load bank. We can simulate various common periodic faults and irregular faults in the gearbox, such as broken tooth, chipped tooth, wear of outer race of bearing, and so on. An accelerometer was In our experiments, we considered eight health conditions, involving one normal, three gear faults, three bearing faults, and a shaft fault. These considered fault conditions occurred on the second stage (i.e., the high speed end) of the gearbox, which are typically encountered in a practical WT gearbox. In each experiment, a damaged component is installed inside the test rig and other components are normal. Each experiment is carried out under ten operation conditions: five motor driving speeds and two different loads. The detailed information is summarized in Table <ref type="table">I</ref>. In this paper, the same health condition under different speeds and loads is treated as one class. There are 260 signals for each health condition under one load and speed, and each vibration signal contains 1024 data points. Therefore, the data set totally contains 260×10×8 = 20 800 samples, where each sample is frequency spectral of each vibration signal containing 512 Fourier coefficients, rather than the time-domain signal itself. For model training, we randomly select 12 800 sample for training and the rest for testing. For all the training data, each variable is linearly scaled to lie between 0 and 1. Accordingly, the testing data are rescaled according to the maximum and the minimum value of training data, thus ensuring both data sets in the same range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model Parameters</head><p>Here, we experimentally evaluate our SMLDAE approach using the gearbox vibration data set described in Section IV-A. In this paper, our SMDLAE model has four layers, where the input layer size of 512 and the output layer size of 8 are determined by the dimension of the samples and the number of health conditions, respectively. Other two hidden layers are set  the same size as 200. For each AE training, we use the sigmoid activation function for both the encoder and decoder. The mean squared error cost function is used as the reconstruction error in the unsupervised learning phase, while a cross-entropy error is used to evaluate the performance of the classification task in the supervised learning phase. For all model training, we use the SGD with a momentum for network optimization. The learning rate and the momentum are set to 0.05 and 0.99, respectively. In order to speed up the training process, we split the training set into mini-batches to update the model parameters. The batch size is set to 100 and the number of training epochs is 200.</p><p>We investigate the effects of two extra parameters, i.e., K and υ, described in Algorithm 1 on diagnosis performance. K is sampled from {10, 30, 50, 100, 150, 200, 250, 300}. The results are shown in Fig. <ref type="figure" target="#fig_4">6</ref>, where both parameters are evaluated under different initial noise levels υ 0 . The final noise level is set as υ T = 0.05. From Fig. <ref type="figure" target="#fig_4">6</ref>, regardless of υ = 0.05 or υ = 0.1, for all most choices of K greater than 50, the fault diagnosis accuracies always keep in a small range from 97.5% to 98%. This relative stable results indicate that our proposed SMLDAE approach is robust to the parameters ( υ, K ). Specifically, for the smaller values of K that is lower than 50, the diagnosis accuracies decrease to lower than 97.5%, especially when the initial noise level υ 0 &gt; 0.3. A possible reason for this is that the smaller values of K cannot ensure the model parameters convergence under multiple different noise levels, thus leading to a decreased performance. Therefore, in this paper, it is suggested to choose large values of K for better performance. In addition, the effects of noise levels in our proposed SMDLAE approach will be discussed in detail in Section IV-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance Analysis of SMLDAE-Based Representation Learning</head><p>In this section, we carefully investigate the learning process of SMLDAE with two hidden layers. As an example, we perform the SMLDAE training with the initial noise level υ 0 = 0.3 and the final noise level υ T = 0.05. K = 100 and υ = 0.05 in Algorithm 1 are used. Fig. <ref type="figure" target="#fig_5">7</ref> shows the training performance curve in both the unsupervised pretraining phase and the supervised fine-tuning phase, where two SDAE models with noise levels υ = 0.3 and υ = 0.05 are also trained and tested for comparison.</p><p>From Fig. <ref type="figure" target="#fig_5">7</ref>(a) and (b), for DAEs with the fixed noise level 0.3 and 0.05, the training mean squared error curves converge to a certain value as the number of training epochs increases. It is expected that the DAE with a smaller noise level obtains a smaller reconstruction error for the reason that the slightly corrupted input data are more easily reconstructed. For MLDAE, multiple noise levels from 0.3 decreasing to 0.05 with the step of υ = 0.05 are used, and hence, the errors continue to decrease every 100 epochs from the error value obtained by using the noise level 0.3. Clearly, MLDAE obtains the smallest reconstruction error, which means that it can more accurately capture fault feature patterns hidden in complex vibration data with the least information loss. While for supervised learning performance curves shown in Fig. <ref type="figure" target="#fig_5">7(c</ref>) and (d), SMLDAE achieves the smallest testing error in terms of cross entropy loss and the highest classification accuracy. This further demonstrates that the SMLDAE produces better feature learning ability than the traditional SDAE approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Benefits of Multilevel-Denoising Training Scheme</head><p>To demonstrate the advantages of the proposed MLD training scheme in the SMLDAE method over a single fixed noise level used in the traditional SDAE method for feature representation learning, we consider a model with two hidden layers to learn nonlinear fault features from the vibration spectral and compare SMLDAE and SDAE approaches in terms of fault classification accuracy on the testing data. For SDAE, the noise level υ is selected from {0, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6}. Note that the noise level υ = 0 corresponds to the traditional SAE. For SMLDAE, the initial noise level υ 0 is chosen from {0.1, 0.2, 0.3, 0.4, 0.5, 0.6}, and the corresponding final noise level υ T (υ T &lt; υ 0 ) can be smaller noise level value in the range [0.05,0.5]. In this paper, the smallest final noise level is 0.05, which means 5% components is randomly turned off. In addition, K = 200 and υ = 0.05 are used for the SMLDAE training.</p><p>Table <ref type="table" target="#tab_1">II</ref> summaries the fault classification results using the proposed SMLDAE approach with different combinations of υ 0 and υ T and the traditional SDAE with different given  noise levels. Obviously, our proposed SMLDAE approach using multiple noise levels obtains higher classification accuracy of over 97% in all combinations of the initial noise level υ 0 and the final noise level υ T than the traditional SDAE. This great improvement mainly contributes from its representation learning ability to capture different scale information and discover more useful and rich patterns hidden in data. In addition, we notice that the classification performance of SDAE outperforms that of the traditional SAE (υ = 0) under most considered noise levels, mainly due to robust representations learned by the SDAE approach. However, the performance of SDAE decreases as the noise level increases, especially when υ &gt; 0.5. This is mainly because the higher noise level will cause the loss of useful information, resulting in difficult reconstruction and poor classification performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Diagnosis Performance Evaluation and Comparison</head><p>Recent studies in the fault diagnosis field have shown that the deep learning-based representation learning approaches produce state-of-the-art performance over traditional handcrafted feature extraction approaches <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b23">[24]</ref>. Therefore, in this section, we evaluate our proposed SMLDAE method by comparing it with two existing deep learning methods, i.e., SAE <ref type="bibr" target="#b20">[21]</ref> and SDAE <ref type="bibr" target="#b21">[22]</ref>, as well as the traditional multilayer perceptrons (MLPs) neural network method. For a fair comparison, all model parameters are set the same as those of SMLDAE. The weights and bias parameters of MLP were randomly initialized, while the unsupervised pretraining strategy is used for SAE, SDAE, and the proposed SMLDAE. In addition, we consider different hidden layers from 1 to 4 for all methods to examine the effects of the model depth on diagnosis performance. All the methods are abbreviated, for example, SMLDAE-2 indicating that the number of hidden layers is two. Table <ref type="table" target="#tab_1">III</ref> gives the average diagnosis accuracies and standard deviations over ten random trials for each health condition and overall accuracy using different methods, where the best performance in terms of overall accuracy has been highlighted in bold. Furthermore, Fig. <ref type="figure" target="#fig_6">8</ref> shows the overall accuracy for all four methods with hidden layers from 1 to 4.</p><p>From Table <ref type="table" target="#tab_1">III</ref> and Fig. <ref type="figure" target="#fig_6">8</ref>, we can see that our proposed SMLDAE approach achieves the best diagnosis accuracy among all the methods, regardless of the number of hidden layers. Specifically, our SMLDAE approach with two hidden layers obtains an average overall diagnosis accuracy of 98.24% with a standard deviation of 0.13%. Also, SDAE is expected to perform better than SAE attributing to its denoising training process enabling to learn robust representations, while MLP performs the worst. In order to verify whether the performance improvement is significant or not, we conduct a paired t-test of our proposed SMLDAE method with respect to each of the other comparative approaches. We use a two-tailed t-test approach similar to the one in <ref type="bibr" target="#b34">[35]</ref> for validation, and the test results demonstrate that under the significance level of 0.05, the performance of our SMLDAE method is significantly different to the comparative methods under all three cases with different numbers of hidden layers, suggesting that the SMLDAE can significantly improve the performance in this case. The reason for this performance improvement is because SMLDAE can discover more robust and discriminative features to improve fault recognition capability. Furthermore, it can be observed that all the methods with two hidden layers perform better than those with one hidden layer, which means that the deep models can improve the feature learning ability. However, from Fig. <ref type="figure" target="#fig_6">8</ref>, we can observe that the diagnosis performances yield a slight decrease as the number of hidden layers L increases.</p><p>A possible reason for this is that the overfitting may occur for a deeper network, since more parameters need to be trained. Therefore, we suggest to use L = 2 for this paper.</p><p>It is well known that the training process of all methods is time-consuming. In this paper, we evaluate the training time for four considered methods with different hidden layers over ten random trials. In our experiments, an Inter Core i5-3475S CPU with 2.90 GHz with MATLAB R2015b environment on a 64-bit windows operation system is used. The average training time of our proposed method and that of other three methods are compared in Fig. <ref type="figure" target="#fig_7">9</ref>. It can be found that three AE-based models with the pretraining phase consume more time than the traditional MLP method. Specifically, our SMLDAE approach takes the most time among all four methods, since it requires the further training under multiple noise levels. However, as mentioned in Section III-C, all models are only trained off-line and then used for online fault diagnosis phase. From the perspective of application, the training time will not directly affect the performance of the diagnosis system. For practical applications, in order to improve the training efficiency, GPU-based computation platforms, widely used in the deep learning community, can be adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Feature Visualization and Evaluation</head><p>To explicitly show the better property of our proposed SMLDAE method, we do a 2-D feature visualization of the representations learned by SAE, SDAE, and SMLDAE, as well as the raw spectral representation. The t-SNE technique <ref type="bibr" target="#b35">[36]</ref>, which is an effective data visualization technique for highdimensional data, is used to reduce the dimensions of the representations, and the resulting 2-D maps as scatterplot are shown in Fig. <ref type="figure" target="#fig_8">10</ref>. Different colors represent different health conditions described in Table <ref type="table">I</ref>. From Fig. <ref type="figure" target="#fig_8">10</ref>, we can see  that the spectral features are randomly distributed in the 2-D mapping with a larger overlap under different health conditions, and therefore, it is difficult to accurately identify each health condition. But all other three learned feature representations present good cluster performances where data points of different health conditions are separated well enough, among which our proposed SMLDAE approach exhibits the best performance.</p><p>To further quantitatively evaluate cluster performance of learned features using different methods, two evaluation metrics, Davies-Bouldlin index (DBI) <ref type="bibr" target="#b36">[37]</ref> and silhouette index (SI) <ref type="bibr" target="#b37">[38]</ref>, are used. DBI measures the average of similarity between each cluster and its most similar one, which is calculated as follows:</p><formula xml:id="formula_10">DBI = 1 C C i=1 max j =i ( di + d j ) d i, j<label>(4)</label></formula><p>where C is the number of clusters, di is the average distance between each point in the i th cluster and the centroid of the i th cluster, d j is the average distance between each point in the j th cluster and the centroid of the j th cluster, and d i, j is the Euclidean distance between centroids of the i th and j th clusters. A lower DBI means a better cluster configuration. SI computes, for each point, a width depending on its membership in any cluster. This silhouette width is then an average over all observations. SI is a measure of how similar a point is to its own cluster compared with other clusters, which is defined as</p><formula xml:id="formula_11">SI = 1 n n i=1 (b i -a i ) max(a i , b i ) (5)</formula><p>where n is the total number of points, a i is the average distance between point i and all other points in the same cluster, and b i is the minimum average distance between point i and points in other different clusters. A higher SI indicates a better cluster performance.</p><p>Table <ref type="table" target="#tab_2">IV</ref> gives the evaluation metrics of cluster performances of learned features visualized in Fig. <ref type="figure" target="#fig_8">10</ref> using three different methods. In terms of DBI and SI, it can be found that our SMLDAE approach produced the lowest DBI value and the highest SI value, indicating the best performance. Both results from Fig. <ref type="figure" target="#fig_8">10</ref> and Table <ref type="table" target="#tab_2">IV</ref> further demonstrate that the unsupervised feature learning process can greatly improve the separability of different health conditions, and especially, the MLD training scheme used in SMLDAE can enhance the learned feature representations, and therefore results in better fault diagnosis accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Performance Evaluation Under Different Operation Conditions</head><p>In practical applications, gearboxes usually operate under complicated working environments, where the driving motor speeds and loads may change from time to time. Therefore, it is unrealistic to collect and label enough training samples to make the fault classifier robust to all operation conditions. Thus, it is significant to train a feature learner and classifier using samples collected under one known operation that are able to learn and classify the features from another unseen operation. In this case, it is more challenging to design such a robust fault diagnosis system against different operation conditions. To investigate how well our proposed SMLDAE approach performs in such a case, we design the following experiments.</p><p>1) The training data consist of eight health conditions under the motor speed of 400 r/min and the low load, which is denoted as 400L; the testing data containing eight health conditions from five different motor speeds with the high load, denoted as 200H, 300H, 400H, 500H, and 600H, are tested, respectively. 2) Similar to 1), the training data are from the motor speed of 400 r/min and the high load, denoted as 400H; the testing data, denoted as 200L, 300L, 400L, 500L, and 600L from five different motor speeds with the low load, are tested, respectively. In both experiments, the training data and the testing data are from different operations. In other words, we train the fault diagnosis model based on our proposed SMLDAE approach using the training data under one known operation and test the model using the testing data from another unseen operation. Both SAE and SDAE methods are considered for comparison. All three methods use the same network architecture with two hidden layers of 200 hidden units. The evaluation results for both experiments are listed in Tables V and VI, respectively. From Tables V and VI, the performance of all three methods dramatically decreases compared with the performance obtained in Section IV-E. An important reason is that the training samples and the testing samples are distributed differently over different operation conditions, so that the fault classifier built with the training data cannot effectively classify the testing data.</p><p>However, our proposed SMLDAE approach still outperforms other two methods in both experiments, which indicates that the SMLDAE can learn more robust features under the unseen operation conditions than SAE and SDAE, attributing to its MLD training scheme. Furthermore, SDAE performs better than SAE. This further proves that the corruption process used in SMLDAE and SDAE increases their robustness to the operation variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORK</head><p>This paper proposed a new representation learning approach, SMLDAEs, for feature extraction and classification of complex vibration signals from WT gearbox. By using multiple noise levels to corrupt the original input data and sequentially train AEs, more detailed information and useful fault patterns hidden in data were captured. We further showed the MLD training scheme can enhance the robustness and discriminability of the learned features, and can obtain significant improvements on fault diagnosis accuracy. Furthermore, by using the stacked deep network architecture, our proposed approach can learn robust high-level features from vibration spectral features, which are more discriminative over different health conditions of the WT gearbox. Experimental results showed that our approach significantly improved the conventional SDAE and achieved better diagnosis accuracies, which were further proofed by the separable property in the reduced 2-D feature map.</p><p>There are several interesting future research directions. For instance, in our current research, we add multiple noise levels in a linear descent way to train AEs. We are currently working on exploring more advanced schemes, such as marginalization <ref type="bibr" target="#b38">[39]</ref>, to add multiple noise levels. Also, in the case of imbalanced data distribution <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, how the proposed SMLDAE approach works with such skewed data distributions remains an open question and will be investigated in our future work. Finally, our approach showed a decreased performance in the case where the training data and the testing data are from different operation conditions. In order to address this issue, we plan to integrate the idea of transfer learning into our SMLDAE approach to deal with the different distributions resulted from different operation conditions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. SDAEs. (a) DAE. (b) Unsupervised pretraining for representation learning. (c) Supervised fine-tuning for classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. Illustration of the proposed MLDAE training. DAE is first trained under the initial noise level υ 0 and then sequentially trained using multiple noise levels from υ 1 to υ T to fine-tune the model parameters θ = {W 1 , W 2 , b 1 , b 2 }. Correspondingly, the learned hidden representations are also optimized in this way. In this figure, only W 1 and W 2 are illustrated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Flowchart of fault diagnosis based on the proposed SMLDAE approach. the fault diagnosis from the following two phases: off-line training and online testing. The goal of the off-line training phase is to learn robust and discriminative representations and train a deep neural network (DNN) classifier based on the proposed SMLDAE method through two steps of an unsupervised representation learning and a supervised training with label information, similar to the SDAE training shown in Fig. 1. During the testing phase, new input data are fed into the well-trained DNN classifier to give final diagnosis results. The detailed fault diagnosis procedures are summarized as follows. 1) Off-Line Training Phase: Step 1: Collect vibration signals from different health conditions and calculate the corresponding frequency spectral using Fourier transform as the training data set D train . Step 2: Perform automatic representation learning from original spectral features using the proposed SMLDAE approach in an unsupervised learning manner, and obtain robust and high-level feature representations. Step 3: Stack all representation layers and add a classifier layer on the final representation layer to form a DNN model, and train it by using the BP algorithm to fine-tune all parameters in a supervised manner. 2) Online Testing Phase: Step 1: Acquire online vibration signals and calculate the corresponding frequency spectral using Fourier transform as the testing data set D test . Step 2: Input D test to the trained DNN classifier obtained from the off-line training phase, and gives the corresponding health condition label which belongs to.</figDesc><graphic coords="6,299.63,181.37,111.98,73.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. WT gearbox test rig. (a) Whole test bench. (b) Tested gearbox.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Parameters effects of K and υ. (a) υ = 0.05 and (b) υ = 0.1 on diagnosis performance evaluated on the testing data set.</figDesc><graphic coords="7,53.03,58.13,242.78,102.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Training performance curve of SMLDAE with two hidden layers. Loss curve of (a) first DAE and (b) second DAE at the unsupervised pretraining phase. (c) Loss curve and (d) accuracy curve of SMLDAE at the fine-tuning phase. SDAE and SMLDAE are compared in these figures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8.Average diagnosis accuracies for four considered methods with different hidden layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Average training time for four considered methods with different hidden layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Visualization of different features. (a) Original spectral features. (b) SAE-based learned features. (c) SDAE-based learned features. (d) SMLDAE-based learned features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and the hidden representation h is learned.</figDesc><table><row><cell>10:</cell><cell># Continue to train using decreasing noise levels</cell></row><row><cell>11:</cell><cell>for t in 1, • • • , T do</cell></row><row><cell>12:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II FAULT</head><label>II</label><figDesc>DIAGNOSIS ACCURACY WITH DIFFERENT NOISE LEVELS. (%) N/A MEANS NOT APPLICABLE</figDesc><table><row><cell>TABLE III</cell></row><row><cell>FAULT DIAGNOSIS ACCURACY WITH DIFFERENT FEATURE REPRESENTATIONS (%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV CLUSTER</head><label>IV</label><figDesc></figDesc><table><row><cell>EVALUATION RESULTS FOR 2-D FEATURE</cell></row><row><cell>MAPPING USING DIFFERENT METHODS</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V FAULT</head><label>V</label><figDesc>DIAGNOSIS ACCURACY USING THE TRAINING DATA 400L. (%) TABLE VI FAULT DIAGNOSIS ACCURACY USING THE TRAINING DATA 400H. (%)</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China under Grant 61673336, in part by the Natural Science Foundation of Hebei Province under Grant F2016203421, in part by the Scientific Research Project of the Higher Education Institutions of Hebei Province under Grant ZD20131080, and in part by the National Science Foundation under Grant ECCS 1053717 and Grant CCF 1439011. The Associate Editor coordinating the review process was Dr. Ruqiang Yan.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>He is currently an Assistant Professor with the Department of Computer and Electrical Engineering and Computer Science and a Faculty Fellow with the Institute for Sensing and Embedded Network Systems Engineering, Florida Atlantic University, Boca Raton, FL, USA. His current research interests include power systems stability and control, smart grid, computational intelligence, and cyber-physical energy systems.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Performance assessment of wind turbine gearboxes using in-service data: Current approaches and future trends</title>
		<author>
			<persName><forename type="first">J</forename><surname>Igba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alemzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Durugbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Henningsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Renew. Sustain. Energy Rev</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="144" to="159" />
			<date type="published" when="2015-10">Oct. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Monitoring wind turbine gearboxes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Crabtree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Tavner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wind Energy</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="728" to="740" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A survey on wind turbine condition monitoring and fault diagnosis-Part II: Signals and signal processing methods</title>
		<author>
			<persName><forename type="first">W</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="6546" to="6557" />
			<date type="published" when="2015-10">Oct. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A review on machinery diagnostics and prognostics implementing condition-based maintenance</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K S</forename><surname>Jardine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Banjevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mech. Syst. Signal Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1483" to="1510" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Wavelets for fault of rotary machines: A review with applications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Process</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2014-03">Mar. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A morphological Hilbert-Huang transform technique for bearing fault detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Instrum. Meas</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2646" to="2656" />
			<date type="published" when="2016-11">Nov. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wavelet packet envelope manifold for fault diagnosis of rolling element bearings</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Instrum. Meas</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2515" to="2526" />
			<date type="published" when="2016-11">Nov. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sparse signal reconstruction based on time-frequency manifold for rolling element bearing fault signature enhancement</title>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Instrum. Meas</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="482" to="491" />
			<date type="published" when="2016-02">Feb. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fault diagnosis using a joint model based on sparse representation and SVM</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Instrum. Meas</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2313" to="2320" />
			<date type="published" when="2016-10">Oct. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sparse feature identification based on union of redundant dictionary for wind turbine gearbox fault diagnosis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="6594" to="6605" />
			<date type="published" when="2015-10">Oct. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rolling bearing fault detection and diagnosis based on composite multiscale fuzzy entropy and ensemble support vector machines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mech. Syst. Signal Process</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="746" to="759" />
			<date type="published" when="2017-02">Feb. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013-08">Aug. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning deep architectures for AI</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2012-09">Sep. 2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised local deep feature for image recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">351</biblScope>
			<biblScope unit="page" from="67" to="75" />
			<date type="published" when="2016-09">Sep. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Face detection using representation learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-Q</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">187</biblScope>
			<biblScope unit="page" from="19" to="26" />
			<date type="published" when="2016-09">Sep. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Exploiting deep neural networks for detection-based speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Siniscalchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="148" to="157" />
			<date type="published" when="2013-04">Apr. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Failure diagnosis using deep belief learning based health state classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tamilselvan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rel. Eng. Syst. Safety</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="124" to="135" />
			<date type="published" when="2013-07">Jul. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Construction of hierarchical diagnosis network based on deep learning and its application in the fault pattern recognition of rolling element bearings</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mech. Syst. Signal Process</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="92" to="104" />
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep neural networks: A promising tool for fault characteristic mining and intelligent diagnosis of rotating machinery with massive data</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mech. Syst. Signal Process</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="303" to="315" />
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fault diagnosis of rotary machinery components using a stacked denoising autoencoder-based health state identification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Process</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="377" to="388" />
			<date type="published" when="2017-01">Jan. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A sparse auto-encoder-based deep neural network approach for induction motor faults classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Measurement</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="171" to="178" />
			<date type="published" when="2016-07">Jul. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An intelligent fault diagnosis method using unsupervised feature learning towards mechanical big data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3137" to="3147" />
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Representational learning for fault diagnosis of wind turbine equipment: A multi-layered extreme learning machines approach</title>
		<author>
			<persName><forename type="first">Z.-X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energies</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">379</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deep learning and its applications to machine health monitoring: A survey</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">X</forename><surname>Gao</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1612.07640" />
		<imprint>
			<date type="published" when="2016-12">Dec. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Scheduled denoising autoencoders</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Geras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1406.3269" />
		<imprint>
			<date type="published" when="2014-07">Jul. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th Int. Conf. Mach. Learn</title>
		<meeting>25th Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010-12">Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th Int. Conf. Mach. Learn. (ICML)</title>
		<meeting>28th Int. Conf. Mach. Learn. (ICML)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning a deep compact image representation for visual tracking</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="809" to="817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Two-stage learning to predict human eye fixations via SDAEs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="487" to="498" />
			<date type="published" when="2016-02">Feb. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep learning approach for active classification of electrocardiogram signals</title>
		<author>
			<persName><forename type="first">M</forename><surname>Al Rahhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Alhichri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alajlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Melgani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">345</biblScope>
			<biblScope unit="page" from="340" to="354" />
			<date type="published" when="2016-09">Sep. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adaptive noise schedule for denoising autoencoder</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Neural Inf. Process</title>
		<meeting>Int. Conf. Neural Inf. ess</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="535" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A self-organizing learning array system for power quality classification based on wavelet transform</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Starzyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Power Del</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="286" to="295" />
			<date type="published" when="2006-01">Jan. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A cluster separation measure</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Bouldin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="224" to="227" />
			<date type="published" when="1979-04">Apr. 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Finding Groups in Data: An Introduction to Cluster Analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Rousseeuw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Wiley</publisher>
			<biblScope unit="volume">344</biblScope>
			<pubPlace>Hoboken, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Marginalized denoising autoencoders for domain adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 29th Int. Conf. Mach. Learn. (ICML)</title>
		<meeting>29th Int. Conf. Mach. Learn. (ICML)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="767" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1263" to="1284" />
			<date type="published" when="2009-09">Sep. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">RAMOboost: Ranked minority oversampling in boosting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1624" to="1642" />
			<date type="published" when="2010-10">Oct. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
