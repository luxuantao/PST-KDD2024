<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Sohini</forename><surname>Roychowdhury</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Optic Disc Boundary and Vessel Origin Segmentation of Fundus Images</orgName>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Dara</forename><forename type="middle">D</forename><surname>Koozekanani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Optic Disc Boundary and Vessel Origin Segmentation of Fundus Images</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Ophthalmology. Keshab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sam</forename><forename type="middle">N</forename><surname>Kuchinka</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Optic Disc Boundary and Vessel Origin Segmentation of Fundus Images</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Ophthalmology. Keshab</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Washington</orgName>
								<address>
									<region>Bothell</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Keshab</forename><forename type="middle">K</forename><surname>Parhi</surname></persName>
							<email>parhi@umn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Optic Disc Boundary and Vessel Origin Segmentation of Fundus Images</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Washington</orgName>
								<address>
									<region>Bothell</region>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">29DFA43B5BB9E3939FE09223ACB72BBB</idno>
					<idno type="DOI">10.1109/JBHI.2015.2473159</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/JBHI.2015.2473159, IEEE This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/JBHI.2015.2473159, IEEE Journal of Biomedical and Health Informatics</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>optic disc</term>
					<term>solidity</term>
					<term>major vessels</term>
					<term>centroid</term>
					<term>vessel origin</term>
					<term>overlap score</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a novel classification-based optic disc (OD) segmentation algorithm that detects the OD boundary and the location of vessel origin (VO) pixel. First, the green plane of each fundus image is resized and morphologically reconstructed using a circular structuring element. Bright regions are then extracted from the morphologically reconstructed image that lie in close vicinity of the major blood vessels. Next, the bright regions are classified as bright probable OD regions and non-OD regions using 6 region-based features and a Gaussian Mixture Model classifier. The classified bright probable OD region with maximum Vessel-Sum and Solidity is detected as the best candidate region for the OD. Other bright probable OD regions within 1-disc diameter from the centroid of the best candidate OD region are then detected as remaining candidate regions for the OD. A convex hull containing all the candidate OD regions is then estimated, and a best-fit ellipse across the convex hull becomes the segmented OD boundary. Finally, the centroid of major blood vessels within the segmented OD boundary is detected as the VO pixel location. The proposed algorithm has low computation time complexity and it is robust to variations in image illumination, imaging angles and retinal abnormalities. This algorithm achieves 98.8-100% OD segmentation success and OD segmentation overlap score in the range of 72-84% on images from the 6 public data sets of DRIVE, DIARETDB1, DIARETDB0, CHASE DB1, MESSIDOR and STARE in less than 2.14 seconds per image. Thus, the proposed algorithm can be used for automated detection of retinal pathologies such as glaucoma, diabetic retinopathy and maculopathy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Automated detection of optic disc (OD) is important for early detection of glaucoma <ref type="bibr" target="#b0">[1]</ref>  <ref type="bibr" target="#b1">[2]</ref>. Disc size, neuro-retinal rim and cup area features are used to detect a glaucomatous retina. OD and the position of vessel origin (VO) are the main anatomical features in retinal fundus images. The VO position is an important reference point for detecting the macula, and thereby grading macular pathologies such as diabetic maculopathy, macular edema and macular ischemia <ref type="bibr" target="#b2">[3]</ref>. Also, automated OD detection plays an important role in developing automated "diagnosis expert systems" for diabetic retinopathy (DR), as its segmentation is a key component to correctly identify other bright features in the images such as the bright lesions (hard exudates or cotton-wool spots) [4] <ref type="bibr" target="#b4">[5]</ref>. Besides the position of the OD, the VO seed point is another important feature of a fundus image that is needed for vessel tracking methods to detect vessel features like vessel tortuosity and vessel branching patterns <ref type="bibr" target="#b3">[4]</ref>. Automated detection of the complete OD region is a challenging problem due to the variation in size, shape, color of the OD across images along with the variation introduced by the field of view (FOV), inhomogeneous illumination and pathological abnormalities. In our earlier work <ref type="bibr" target="#b4">[5]</ref>, ODs in fundus images were detected in the neighborhood of thick blood vessels as a bright regions with minimum pixel-intensity sum and maximum region solidity, i.e., the Minimum Intensity Maximum Solidity MinIMaS algorithm. In this paper, we experimentally analyze the features of bright regions that appear in fundus images and converge to the same observation that the OD is a combination of bright regions lying in close vicinity of the major blood vessels and satisfying certain structural and pixel intensity constraints.</p><p>This paper makes three key contributions. First, a circular structuring element with half the diameter of the estimated OD is found to be more effective for bright OD region extraction when compared to horizontal linear structuring elements of length equal to the estimated OD diameter. Second, 6 characteristic features of bright OD regions are identified that separate the bright probable OD regions from the bright non-OD regions by classification using the Gaussian Mixture Model (GMM). Also, the top two selected features are indicative of thick major blood vessels in the bright region neighborhood (Vessel-Sum), and the compact structure of a bright region (Solidity). Maximization of these two features among classified bright probable OD regions locates the best candidate region for the OD in every fundus image regardless of the image FOV or the extent of image abnormality. The proposed OD segmentation algorithm achieves 99.93% success in detecting the OD boundary and VO with less than 12 pixels error on images with varying extent of abnormalities from 6 different public data sets where FOV is in the range [30 o to 50 o ]. Third, resizing and cross-validation of retinal data sets provides an optimal training data set (TRAIN50) to classify bright OD regions from the non-OD regions regardless of the image FOV or the extent of pathological atrophy.</p><p>The organization of this paper is as follows. In Section II, the existing OD detection and segmentation methods are reviewed. In Section III, the proposed OD segmentation algo-rithm is presented. In Section IV the experimental results of automated OD segmentation are presented. Conclusions and discussion are presented in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. OVERVIEW OF PRIOR WORK</head><p>Related work regarding OD detection using fundus images can be broadly categorized into two groups: 1) those based on the shape, color and structure of OD, and 2) those based on the properties of the retinal vasculature. The automated algorithms belonging to both these categories are discussed as follows.</p><p>The first category includes well-known methods that rely on the appearance and size of the OD <ref type="bibr" target="#b5">[6]</ref>, such as the method in <ref type="bibr" target="#b6">[7]</ref> that detects bright regions by mathematical morphology and detects the OD as the largest high contrast average circular shape area. The method in <ref type="bibr" target="#b7">[8]</ref> uses morphology followed by watershed transform for model-based OD detection. The method in <ref type="bibr" target="#b3">[4]</ref> first locates a pixel belonging to the OD region, and then applies boundary segmentation by morphology, edge detection and circular Hough transforms. Other well known methods in <ref type="bibr" target="#b8">[9]</ref> [10] use a line operator to capture circular brightness structure of the OD since the maximum and minimum variation along the linear operator has a specific pattern to locate the OD.</p><p>The second category of methods examine the retinal vasculature for locating the OD, since the OD is the point of entrance of the optic nerve and blood vessels which branch out into finer vessels through the retina. The method in <ref type="bibr" target="#b5">[6]</ref> applies a search on the branching network patterns formed by the blood vessels to converge to the region where most paths ended, followed by the application of Hough transform on all such regions to finally locate the OD. Another method in <ref type="bibr" target="#b10">[11]</ref> matches directional patterns of the retinal blood vessels to match the direction of OD vicinity. The method in <ref type="bibr" target="#b11">[12]</ref> localizes the OD by tracking the blood vessels using Gabor filters to detect the peaks in nodes via phase portrait analysis and locates the OD at the focal point of the vessels using Hough transform. The method in <ref type="bibr" target="#b12">[13]</ref> applies watershed transform for detecting the OD location and disc boundary using the information regarding the major vessel arcade. Besides, maximization of vessel pattern-based entropy is used to detect the location of the OD in <ref type="bibr" target="#b13">[14]</ref>. Some algorithms, however, have combined the two categories such as the method in <ref type="bibr" target="#b14">[15]</ref> that locates the OD based on the structure of the OD, the convergence of blood vessels at the OD and the variation in the size of the blood vessels entering the OD. Another method in <ref type="bibr" target="#b15">[16]</ref> uses the feature based knearest neighbor (kNN) classifier for training and extracting the OD. The method in <ref type="bibr" target="#b2">[3]</ref> uses an ensemble of pyramidal decomposition, edge detection, entropy-filter based detection, Hough transform and feature vector based algorithms for detecting the OD. Another method in <ref type="bibr" target="#b16">[17]</ref> uses the principal component image plane followed by removal of vessel regions, image inpainting, stochastic watershed transform and regional discrimination for the OD boundary detection. Also, the method in <ref type="bibr" target="#b1">[2]</ref> applies super-pixel classification to separate pixels of the disc from non-discs using histogram and center surround statistics. The recent works in <ref type="bibr" target="#b17">[18]</ref> [19] separate the vascular information from the inpainted vessel removed images and use radial symmetry based OD location strategy. In <ref type="bibr" target="#b17">[18]</ref>, the OD region is selected as the one having high 'vesselness' in its vicinity, while the method in <ref type="bibr" target="#b18">[19]</ref> uses complex contour fitting at increased resolution using inpainted images and vessel masks. Another recent work in <ref type="bibr" target="#b19">[20]</ref> uses 3 vessel distribution features: local vessel density, compactness and uniformity to find horizontal OD coordinate, followed by Hough Transform and parabola fitting to find the vertical OD coordinate. The other recent method in <ref type="bibr" target="#b20">[21]</ref> uses a set of iterative opening-closing morphological operations to produce a bright region enhances image followed by blood vessel confluence detection at the OD and Circular Hough Transform.</p><p>In this work we propose a novel supervised method that combines the two categories of OD segmentation and uses region-based features to provide the most robust OD region segmentation method so far, that is independent of the extent of pathology and imaging limitations. OD detection robustness is due to the optimally sized circular morphological structuring element to extract a bright region enhanced image and a unique combination of 6 features for detecting bright candidate regions for the OD. Also, region-based classification leads to low computational time complexity when compared to pixellevel classification strategies in <ref type="bibr" target="#b15">[16]</ref>. Further, the training data set identified for separating false positive bright regions from the probable OD regions is capable of locating the OD in images in all but 1 fundus image from 6 data sets that include images with peri-papillary atrophy and posterior staphyloma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MATERIALS AND METHOD</head><p>In this paper, we propose a novel supervised OD boundary segmentation and VO detection algorithm that is robust to variations in image illumination and retinal abnormalities. Although OD is generally the brightest element in retinal images, the entire OD region is not equally brightly illuminated. For accurate boundary detection of the complete OD, it is imperative to detect all bright OD regions. To facilitate this, we propose a three-step approach, where in the first step, bright regions in close vicinity of major blood vessels are detected by thresholding a morphologically reconstructed of the green plane image. In the second step, region-based classification is performed to retain only the bright probable OD regions among all the bright regions previously detected. Next, the best candidate region for the OD is identified among the bright probable OD regions. In the third step, the bright probable OD regions in the neighborhood of the best candidate OD region are identified as the remaining candidate regions for the OD. Next, a convex hull is created around all OD candidate regions and a best fit ellipse around this convex hull is the segmented OD boundary. Finally, the position of VO is determined at the centroid of the major blood vessels that lie within the segmented OD boundary. A flowchart outlining these three steps for OD detection is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data</head><p>For OD segmentation performance analysis, retinal images are manually annotated for the OD boundary (D) and the pixel corresponding to the VO (O) using the GIMP software <ref type="bibr" target="#b21">[22]</ref>. The proposed OD segmentation algorithm is analyzed on fundus images from the following publicly available data sets. To facilitate feature-based classification of bright OD regions 50 images are selected, with 10 images from 5 public data sets of DRIVE, DIARETDB0, DIARETDB1, CHASE DB1, and MESSIDOR, to create a training data set (named TRAIN50). To determine optimal pre-processing parameters, a pre-processing data set, which comprises of the images in TRAIN50 along with 10 images from the STARE data set, is used. The proposed algorithm is then tested on the remaining 1518 images from the 6 following data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Problem Formulation</head><p>OD in a retinal fundus image is generally the brightest and the most compact structure lying in the vicinity of blood vessels that are the thickest at the point of vessel origin and become narrow as they branch away from the OD <ref type="bibr">[13] [14]</ref>. The proposed algorithm incorporates this retinal property to extract bright regions in the neighborhood of thick major blood vessels as the OD <ref type="bibr" target="#b17">[18]</ref>[19] <ref type="bibr" target="#b20">[21]</ref>.</p><p>As a pre-processing step, the green plane of each color fundus image is resized (using bi-cubic interpolation) to ensure the retinal region of interest is the equivalent of 500 pixels in width and height captured using a 45 o FOV imaging camera. Thus the dimensions of resized images from the {DRIVE, DIARETDB0, DIARETDB1, CHASE DB1, MESSIDOR and STARE} data sets are {[530x550], [530x500], [530x500],</p><p>[410x410], [810x540], [410x410]}, respectively, as shown in the supplementary material. Following image resizing, the pixel intensities are scaled in the range [0,1], followed by contrast enhancement, resulting in image I. Next, I is morphologically eroded using a structuring element (se), followed by image reconstruction <ref type="bibr" target="#b28">[29]</ref>. Here, se could either be a linear (se l ) or circular (se d ) structuring element. The morphological reconstruction operation enhances the compactness of brightly illuminated regions that include bright lesions, OD and contrast variations around the blood vessels. The reconstructed image is then subtracted from I, scaled in [0,1], and subjected to contrast enhancement to yield a morphologically transformed image I r . The three steps of detecting bright regions in neighborhood region of major blood vessels followed by classification of bright probable OD regions and estimation of the OD boundary ( D) and the VO location ( Ô) are described below. The notations and their definitions used in this OD detection algorithm are defined in Table <ref type="table" target="#tab_0">I</ref>.</p><p>In the first step, to locate thick blood vessels in the vicinity of the OD region from image I as in <ref type="bibr" target="#b4">[5]</ref>, the major blood vessels and bright regions are detected separately. To extract the major blood vessel regions from I as in <ref type="bibr" target="#b29">[30]</ref>, a smoothened low-pass filtered image LP F (I), using median filter with window size [20x20] <ref type="bibr" target="#b4">[5]</ref>, is subtracted from I to obtain a highpass filtered image. The image corresponding to the absolute pixel values in this high-pass filtered image is then contrast adjusted and thresholded at pixel value 0.2 to extract the major blood vessels in binary image I v as shown in (1) <ref type="bibr" target="#b29">[30]</ref>. A binary image containing bright regions (I b ) is extracted by thresholding the morphologically reconstructed image I r at the Otsu's threshold pixel value <ref type="bibr" target="#b20">[21]</ref>. Next, an Overlap function is defined with a neighborhood parameter ν, such that if bright regions in I b are located less than 'ν' pixels away from the vessel regions in I v , bright regions (R e ) with area &gt; 50 squared pixels are detected from the binary image I b using (3). The process of selecting an ideal structuring element (se) and neighborhood parameter (ν) for a fundus images with varying FOVs is presented in Section III C.</p><formula xml:id="formula_0">I v = abs {[I -LP F (I)] &lt; 0} &gt; 0.2<label>(1)</label></formula><formula xml:id="formula_1">I b = {[I r &gt; Otsu's threshold(I r )} (2) R e = Overlap(I v , I b , ν), e ∈ {1, 2, ...n}<label>(3)</label></formula><p>Such that, ∀e, A(R e ) &gt; 50.</p><p>In the second step, structural and pixel-intensity based features are extracted corresponding to each bright region. Additionally, a new feature indicative of thick blood vessels in the vicinity of bright OD regions is computed for each bright region in 'R e ' as the Vessel-sum. For computing the Vessel-sum at each bright region (R e ), first a circular mask is generated whose center is at the centroid of R e with ρ pixels radius. Next, the circular mask is imposed on the major blood vessel image I v and the sum of all vessel pixel intensities lying within the masked circle is computed as the vessel-sum function (V essel-Sum(R e )). Fig. <ref type="figure" target="#fig_1">2</ref> demonstrates this process of computing the Vessel-sum for each bright region. Using the most significant region-based features, the bright regions in 'R e ' are classified as bright probable OD regions (R p ) and non-OD regions using GMM classifiers. Further analysis of the region-based features in Section III D demonstrates that Vessel-Sum and region solidity (Solidity(R e ) = A(Re) A(H(Re)) ) are most significant in identifying OD regions among the classified bright probable OD regions. Thus, the best bright candidate region for OD is detected as a bright probable OD region with Maximum Vessel-Sum and Solidity (MaxVeSS) as shown in <ref type="bibr" target="#b3">(4)</ref>.</p><formula xml:id="formula_2">∀R pk , [k = 1, 2....n k ], n k ≤ n, R p ⊆ R, (4) R OD = arg max k ||[Vessel-Sum(R pk ), Solidity(R pk )]|| 2 .</formula><p>In the third step, the classified bright probable OD regions whose centroids are closer to the best candidate OD region than ρ pixels, or 1-disc diameter (DD), are detected as the remaining candidate regions for the OD in image P (5). A convex hull (H) is then constructed around all the regions in P (6). Next, the centroid (Θ(H)), the major axis length (Φ(H)) and minor axis length (Ψ(H)) of the convex hull H are computed and an ellipse is estimated in the parametric way <ref type="bibr" target="#b30">[31]</ref> with center, major and minor axis lengths of [Θ(H), Φ(H), Ψ(H)], respectively. This best fit ellipse is resized to the original image dimensions and it is the segmented OD boundary ( D) in <ref type="bibr" target="#b6">(7)</ref>. The VO pixel ( Ô) is detected at the centroid of blood vessels obtained by superimposing the estimated OD boundary mask on the major blood vessel image (I v ) as shown in <ref type="bibr" target="#b7">(8)</ref>. The VO pixel location is the best estimate of the OD center using the proposed method.</p><formula xml:id="formula_3">∀[R pk ⊆ R|k ∈ 1, ..., n k ], R OD ⊆ R, (5) P = [R OD ∪ R pk : ||Θ(R pk ) -Θ(R OD )|| 2 ≤ ρ], k ⊆ k. H ← Convex Hull (P). (6) D ← Ellipse with [Φ(H), Ψ(H), Θ(H)]. (7) Ô = Θ( D • I v ).<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Selection of Structuring Element and Neighborhood Parameter</head><p>Selection of a structuring element for the morphological transformation of image I to I r is key to the detection of bright candidate regions for OD. Since the thick blood vessels fragment the OD region in several smaller sub-regions, morphological reconstruction of a fundus image achieves significant removal of the intensity irregularity introduced by the vessels in the OD region, thereby leading to a smoothing effect. The use of morphological reconstruction (image dilation followed by erosion and reconstruction) for OD detection is motivated by prior works in <ref type="bibr" target="#b20">[21]</ref>  <ref type="bibr" target="#b28">[29]</ref>.</p><p>While image erosion with a small structuring element can disintegrate the OD into several small bright regions with low compactness, a large structuring element can enlarge the OD boundary, thereby leading to incorrect OD boundary segmentation. Additionally, the neighborhood parameter 'ν' is critical for defining the neighboring regions of the major blood vessels. Here, we analyze 60 images from the preprocessing data set to identify an optimal structuring element and neighborhood parameter for fundus images. It is noteworthy that the bright regions obtained by thresholding a morphologically reconstructed image with an ideal structuring element within the neighborhood 'ν' of the major vessels will maximize the number of true positive bright pixels detected as OD and minimize the number of false positive non-OD pixels. Hence, the metric used to compare the performance of structuring elements and parameter 'ν' is positive prediction value (PPV, also known as precision), i.e., T P T P +F P , where TP represents the number of true positive pixels and FP represents the number of false positive pixels. In this analysis, the region of interest is confined to a 2-DD region in each image that is obtained as a circular region with '2ρ' radius.</p><p>We observe that for resized fundus images from [DRIVE, DIARETDB0, DIARETDB1, CHASE DB1, MES-SIDOR, STARE] data sets, the average manually annotated OD diameters in the pre-processing data set are ρ = [78.6, 74.8, 76.3, 77, 76.7, 73.4] pixels, respectively. Thus, for all resized test images, an average ρ = 76 pixels is used to detect the probable OD regions using <ref type="bibr" target="#b2">(3)</ref><ref type="bibr" target="#b3">(4)</ref><ref type="bibr" target="#b4">(5)</ref>. The impact of horizontal linear structuring elements with length 'l' pixels (se l ), where the length 'l' can be varied as a ratio with 'ρ', as se l ρ ∈ [0, 0.1...1], is shown in Fig. <ref type="figure" target="#fig_2">3</ref>. Here, we observe that for linear structuring elements, as the length of the element approaches 'ρ', the PPV of bright OD regions increases. However, in the second row of Fig. <ref type="figure" target="#fig_2">3</ref>, we observe that morphological transformation using a circular structuring element with diameter se d pixels results in the highest PPV when the structuring element has almost half the actual OD diameter, i.e, se d ρ ≈ 0.6. Following morphological reconstruction, we perform optimal parametrization of 'ν', which represents the neighboring regions of major blood vessels. For every pre-processing image, we compute the optimum ratio of se l ρ and se d ρ as 'ν' varies from <ref type="bibr">[1:150]</ref> in intervals of 10 pixels such that the PPV of bright OD region detection is maximized. Here the inherent assumption is that OD will lie within at most 150 pixels from the thick blood vessels while bright non-OD regions will be farther away from the major blood vessels. Fig. <ref type="figure" target="#fig_3">4</ref>(a) and (b) show the average trends of morphological transformation using linear and circular structuring elements, respectively, for detecting bright OD regions on the images, from training data set. In Fig. <ref type="figure" target="#fig_3">4</ref>(a) maximum PPV of 0.7384 occurs for ratio se l ρ = 0.85 and ν = 30 pixels. In Fig. <ref type="figure" target="#fig_3">4</ref>(b) maximum PPV of 0.7545 occurs for ratio se d ρ = 0.45 and ν = 20 -30 pixels. Thus, the circular structuring element is more accurate than the linear counterparts for OD detection purposes. Based on the analysis on the pre-processing data set, a circular structuring element of diameter 38 pixels and ν = 30 pixels are selected for all resized test images to ensure that significantly sized bright OD regions in the vicinity of major blood vessels are detected but not discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Classification of Bright Regions</head><p>Once the bright regions in the neighborhood of major blood vessels (R e ) are detected in the first step, the next step is to separate the regions that are more probable of being the OD from the false positive non-OD regions <ref type="bibr" target="#b29">[30]</ref>. For this purpose, 21 region-based features are extracted for each bright region. These 21 features comprise of 14 structural features, 4 pixel-intensity based features, 2 combination features and one Vessel-Sum feature. The 14 structural features include area, bounding box dimensions, convex area, eccentricity, equivalent diameter, Euler number, extent, filled area, major axis To identify the most discriminating features for OD regions among the 21 region-based features described above, we perform feature ranking and leave-one-out double crossvalidation on the 50 images from the training data set <ref type="bibr" target="#b29">[30]</ref>. Classification of bright OD regions (with classification label 1) from the bright non-OD regions (with classification label 0) is performed using GMM with 2 Gaussians (corresponding to bright regions with label 0 and 1, respectively). GMM classifier training involves estimation of the mean and variance of the 2 Gaussians. For testing/validation of the classifier, the probability of a bright region from a test image belonging to the Gaussian corresponding to the OD-region is computed and thresholded. In the first cross-validation step, at a time, one image is selected as the validation image and the features extracted from its bright regions are used to generate the validation data set. Bright regions from the remaining 49 images are used to generate the training data set, hence leave-one-out. Corresponding to the 50 images from training data set, 50 such training/validation data sets are thus generated. Each training data set is then subjected to feature ranking using the minimalredundancy-maximal-relevance (mRMR) criterion <ref type="bibr" target="#b31">[32]</ref>  <ref type="bibr" target="#b29">[30]</ref>. The mRMR criterion is based on mutual information from the individual features, such that, the features are ranked based on the top combination of features that have maximum relevance with the sample class labels and minimum redundancy <ref type="bibr" target="#b29">[30]</ref>.</p><p>The important observation in the end of leave-one-out double cross-validation is that the two features Vessel  Once the best candidate for OD (R OD ) is detected for each image, all classified bright probable OD regions within 1-DD from its centroid become candidates for the OD region as well. All the candidate OD regions are then populated in image P followed by estimation of a convex hull around all selected regions. Finally, a best-fit ellipse is estimated around this convex hull as the estimated OD ( D). The VO pixel is then detected at the centroid of major blood vessels within this estimated OD boundary ( Ô). An example of the proposed three-step algorithm is shown in Fig. <ref type="figure" target="#fig_6">6</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND RESULTS</head><p>The performance of the proposed OD segmentation algorithm is tested on fundus images from the 6 public data sets. The comparative assessment of the proposed method with respect to existing algorithms in terms of mean and standard deviation of the performance metrics and the distribution of performance metrics are presented below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Metrics</head><p>The OD segmentation performance metrics are described in terms of the area of overlap between the manually segmented actual OD (D) and the automated segmented OD ( D), and the distance in pixels between the segmented and manual ODs and VOs as shown in Fig. <ref type="figure" target="#fig_7">7</ref>. For each test image, the area of the manually marked OD also detected by the automated algorithm (TP, true positive), area of the falsely detected OD by the automated algorithm (FP, false positive), area of the actual OD missed by the automated segmentation (FN, false negative), and area within the retinal region that is not the OD (TN, true negative) are computed. The performance metrics to analyze the robustness of the proposed OD segmentation algorithm are described as follows.</p><p>• S (Overlap Score) -It denotes the fraction of overlapping area between the actual and segmented OD, where n D = n D = 120. The shortest distance of each sampled point on the actual disc from the sampled points on the segmented disc is computed using <ref type="bibr" target="#b8">(9)</ref>. Finally, the mean of these distances is computed using <ref type="bibr" target="#b9">(10)</ref>. Smaller M denotes better segmentation. .</p><formula xml:id="formula_4">∀ [X i ∈ D : i = 1, ...n D ], [Y j ∈ D : j = 1, ....n D ],<label>(9)</label></formula><formula xml:id="formula_5">m D, D (i ) = min j ||X i -Y j || 2 , m D,D (j ) = min i ||Y j -X i || 2 , M = 1 2    1 n D n D i m D, D (i ) + 1 n D n D j m D,D (j )    .<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Segmentation Performance</head><p>The average and standard deviation of the best OD segmentation performance metrics obtained per image in each test data set using the proposed algorithm is shown in Table <ref type="table" target="#tab_4">II</ref>. While the OD boundary segmentation analysis is performed on 1190 test images from the MESSIDOR data set, the VO location is detected in the 500 images that were manually annotated for this work and denoted by the symbol + in Table <ref type="table" target="#tab_4">II</ref>.</p><p>The best performance metrics on the data sets of [DRIVE, DIARETDB0, DIARETDB1, CHASE DB1, MESSIDOR and STARE] are observed at classifier threshold values of [0.4, 0.2, 0.3, 0.8, 0.45 and 0.6], respectively as shown in the supplementary material. The areas under Receiver Operating Characteristic Curves (AUC) obtained for classification of OD pixels from non-OD pixels in the retinal region of interest on images from all 6 data sets are consistently above 0.91. This illustrates the robustness of the classification and MaxVeSS method.</p><p>Next, the comparative performance of the proposed OD boundary segmentation algorithm with existing works is shown in Table <ref type="table" target="#tab_1">III</ref>. Here, we observe that for the DI-ARETDB1 data set, the proposed method is out-performed only by the stochastic watershed transform-based method in Morales et. al. <ref type="bibr" target="#b16">[17]</ref>. However, the [F P R, Acc] achieved by the proposed method and the Morales et. al. <ref type="bibr" target="#b16">[17]</ref> method are [0.0019, 0.9963] and [0.0028, 0.9957], respectively. Thus, the proposed method detects less false positive regions in the images from the DIARETDB1 data set. For the MES-SIDOR data set, the existing methods based on Circular Hough Transform <ref type="bibr" target="#b3">[4]</ref>  <ref type="bibr" target="#b20">[21]</ref>, and the method based on complex contour model fitting <ref type="bibr" target="#b18">[19]</ref> achieve higher average 'S' than the proposed method. This is due to the computationally simple parametric ellipse fitting method used by the proposed approach. However, most of the estimated OD boundaries using the proposed method are contained within the actual OD boundary, thus resulting in a low F P R and low average 'M '. Also, it is noteworthy that the proposed method is successful in OD segmentation in all images from the MESSIDOR data set due to the vessel and bright region intensity based features identified for classification. This is a significant improvement over existing methods in <ref type="bibr" target="#b3">[4]</ref> [17] <ref type="bibr" target="#b18">[19]</ref> [21] that fail to detect the OD in 2-14 images.</p><p>The STARE data set <ref type="bibr" target="#b27">[28]</ref> has been widely used to benchmark the accuracy of OD center detection algorithms <ref type="bibr" target="#b40">[41]</ref> [11] <ref type="bibr" target="#b19">[20]</ref>. In some works <ref type="bibr" target="#b41">[42]</ref>  <ref type="bibr" target="#b10">[11]</ref>, the OD center is considered to be the VO position. In Table <ref type="table" target="#tab_2">IV</ref>, the OD detection success rate, the average distance between the detected VO pixel and manually annotated OD center (∆) and the computation times are shown. We observe that the proposed method fails to detect the OD in only 1 image 'im0020'. This failure occurs since in this image OD is relatively darker than the OD surrounding regions and hence the morphological reconstruction step fails to identify the OD as a bright region. However, the existing work in Foracchia et. al. <ref type="bibr" target="#b40">[41]</ref> also fails in 2 images 'im0027, im0008' or 'im0041, im0026' by varying the detection tracks. The method in Youssif et. al. <ref type="bibr" target="#b10">[11]</ref> also fails in 'im0004', Lu † Mean and standard deviation computed for 500 manually marked images.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Success% ∆(pixel) Time (# failures) Lu et. al. <ref type="bibr" target="#b8">[9]</ref> 96.3 (3) 25 4.5min Lu <ref type="bibr" target="#b9">[10]</ref> 98.8 (1) 6 5s Hoover et. al. <ref type="bibr" target="#b27">[28]</ref> 89 (9) 29 15s Foracchia et. al. <ref type="bibr" target="#b40">[41]</ref> 97.5 (2) 23 2 min Youssif et. al. <ref type="bibr" target="#b10">[11]</ref> 98.8 (1) 26 3.5 min Zhang et. al. <ref type="bibr" target="#b19">[20]</ref> 98.8 (1) -3.4-11.5s Rangayyan et. al. <ref type="bibr" target="#b11">[12]</ref> 69. Examples of the best and worst OD segmentation performance obtained using the proposed algorithm on the images from all the 5 public data sets are shown in Fig. <ref type="figure" target="#fig_9">8</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Distribution of Performance metrics</head><p>The overlap score between manually annotated disc and the segmented disc is regarded as a significant indicator of OD segmentation accuracy <ref type="bibr" target="#b3">[4]</ref>  <ref type="bibr" target="#b12">[13]</ref>. Analysis of the S metric distribution on the test data sets is shown in Table <ref type="table" target="#tab_5">V</ref>. We observe that for all the test data sets, the proposed method has the highest percentage of images with S ≥ 0.7. For the MESSIDOR data set, the proposed method achieves 96.72% images with S ≥ 0.7, with 11 images more than the recent existing work <ref type="bibr" target="#b20">[21]</ref> in this range. In Table <ref type="table" target="#tab_5">V</ref>, the low percentage of images with S ≥ 0.8 using the proposed method when compared to the existing methods in <ref type="bibr" target="#b3">[4]</ref> <ref type="bibr">[19] [21]</ref> is because of the parametric ellipse fitting strategy with relatively low time complexity used for the OD boundary estimation. However, the proposed method segments the OD boundary and VO in all MESSIDOR images in an average of 2.14 seconds per image, which is significantly less than the existing methods that require more than 5-8 seconds per image for OD segmentation <ref type="bibr" target="#b3">[4]</ref>  <ref type="bibr" target="#b20">[21]</ref>.</p><p>The fit between the segmented and actual OD boundary has been extensively used to demonstrate the performance of OD segmentation algorithms in <ref type="bibr" target="#b39">[40]</ref>  <ref type="bibr" target="#b2">[3]</ref>. After resizing the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Cross-Validation of Training Data</head><p>To analyze the importance of training data on the proposed method, we perform a training data cross-validation experi-ment. Here, the GMM classifier is trained using all images from a particular data set and the trained classifier is then used to detect bright OD regions followed by OD boundary segmentation on all remaining data sets. The average S, M and success in OD segmentation achieved by varying the training data sets are shown in Table <ref type="table" target="#tab_6">VII</ref>. For MESSIDOR, 400 images from Lariboisiere subset are used as cross-training data.</p><p>We observe that since our proposed training data TRAIN50 combines the variability in features of bright regions from 5 different data sets, it achieves best segmentation metrics when compared to the other training data sets. Testing the MESSI-DOR data set while training on DIARETDB1 data set results in failure of OD segmentation in 2 images from Lariboisiere subset, where myelinated nerve fibers in close vicinity of the OD are falsely detected as the OD. Also, using the STARE data set for training while testing on MESSIDOR images, 9 images (2 from LATIM CHU de BREST, 5 from CHU de Etienne, 3 from Lariboisiere) have failed OD segmentation due to brighter surrounding OD regions. Testing on the STARE data set while training on [DRIVE, DIARETDB1, DIARETDB0, CHASE DB1, MESSIDOR] images caused OD segmentation failure in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b5">6]</ref> pathological images, where the complete OD is not visible. Thus, TRAIN50 is observed to be the optimal training dataset for images from all the 6 data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS AND DISCUSSION</head><p>In this paper, we propose a novel three-step classificationbased OD segmentation algorithm that is robust to fundus images with varying FOV, illuminations and pathologies. For the first step, circular structuring elements are found to be preferable over the linear structuring elements for extracting bright regions after morphological reconstruction. In the second step, double cross validation with feature ranking leads to feature reduction for classification of the bright probable OD regions from the non-OD regions. We observe that the bright OD regions comprise 11-18% of all the bright regions extracted per image. The GMM classifier is successful in reducing the number of false positive bright regions but it is not sufficient for detecting only the bright OD regions. Therefore, classification followed by maximization of the major vessel pixels and bright region solidity (MaxVeSS) detects a best bright candidate region for the OD. This method of detecting the bright candidate region for OD is analogous to the method of detecting a pixel within the OD neighborhood region in Lu <ref type="bibr" target="#b9">[10]</ref>, Lu et, al. <ref type="bibr" target="#b8">[9]</ref> and Mendonca et. al. <ref type="bibr" target="#b13">[14]</ref>. Also, the 6 features identified for bright OD region classification are successful in locating the OD candidate region in images with atrophy to the optic nerve, thereby resulting in 100% success on the MESSIDOR data set. These features are more effective than the methods that remove the vessels and rely on inpainted retinal images for OD segmentation <ref type="bibr" target="#b20">[21]</ref>  <ref type="bibr" target="#b18">[19]</ref>.</p><p>The proposed method has two limitations. First, although the parametric ellipse fitting strategy incurs low computational time complexity, it results in less percentage of images with S ≥ 0.8. Future efforts will be directed towards combining the proposed method with Circular Hough Transform and ellipse</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The proposed 3-step automated OD and VO segmentation algorithm.</figDesc><graphic coords="3,100.69,56.07,147.60,182.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Pre-processing steps for OD region detection. (a) Original image. (b) Morphologically reconstructed image (Ir). (c)Major vessels extracted (Iv). (d)Vessel-sum computation. A circular mask of ρ pixels radius (shown by the red circle) is superimposed on Iv to sum the number of vessel pixels in the neighborhood of bright region 'Re'.</figDesc><graphic coords="4,439.25,208.99,79.20,72.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Impact of linear and circular structuring elements for bright region detection. Top row represents morphologically transformed image Ir using a linear structuring element of length 'l' pixels, where the ratio se l ρ varies. Second row represents the PPV obtained for bright OD region detection using linear structuring element. Third row represents reconstruction by circular structuring element with varying diameter (ratio se d ρ varies). Fourth row represents the PPV obtained for bright region detection using circular structuring element.</figDesc><graphic coords="5,338.51,94.61,198.00,144.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Average PPV for bright OD region detection obtained on the training data set by varying the structuring element size and neighborhood parameter 'ν'. (a) PPV trend for linear structuring element. (b) PPV trend for circular structuring element. The red arrow points to the maximum PPV achieved in the process.</figDesc><graphic coords="6,66.50,56.07,215.99,72.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>-Sum and Solidity are consistently ranked as the top two in all the 50 classifiers while the ranks of the other 4 features vary. The scaled values of bright region features [V essel -Sum, Solidity, µ I , σ 2 I , A F , φ ψ ] in range [0,1] are comparatively analyzed for the bright OD regions and the bright non-OD regions. Bright OD regions have median values for these features as [0.701, 0.803, 0.376, 0.04, 0.97, 0.061], respectively. Bright non-OD regions have median feature values of [0.473, 0.556, 0.233, 0.002, 0.784, 0.07], respectively. Here, we observe that the median values of the features Vessel-Sum, Solidity, µ I , and A F are significantly different in the bright OD regions from the non-OD regions. Also, the feature σ 2 I has low variance for the non-OD regions, which refers to the intuitive fact that bright OD regions have a high variance in pixel intensities than non-OD regions since they are being intersected by thick blood vessels. E. Selection of the OD candidate regions In the second step of the proposed algorithm, bright regions in the neighborhood of major vessels are classified into bright probable OD regions (R p ) to eliminate false positive bright regions. However, certain bright regions corresponding to illumination artifacts and bright lesion regions close to the blood vessels get classified as bright probable OD regions as well. Thus, to detect the best candidate for the OD region (R OD ), we further analyze the scatter plot of the bright regions in each image across the top two ranked features, [Vessel-Sum, Solidity]. For the image in Fig. 2(a), the bright regions (R e ) subjected to classification are shown in Fig. 5(a). The regions that are classified as bright probable OD regions are shown in red while the non-OD regions are in blue in Fig. 5(b). Fig. 5(c) represents the scatter plot of all the bright regions in this image across the 2-feature space. Here, we observe that vectors associated with the bright OD regions have higher Euclidean norm than the vectors of the non-OD regions. Further analysis of the training images shows that the bright probable regions with maximum Vessel-Sum and Solidity (MaxVeSS) always correspond to a bright region within the OD or in the vicinity of the actual OD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Selection of R OD region. (a) Bright regions in close vicinity of major vessels (R). (b) GMM classification at threshold value 0.7 leads to bright probable OD regions (red) being separated from non-OD regions (blue). The R OD region is shown in yellow ad the actual bright OD regions are circled in green. (c) 2-D scatter plot of all bright regions corresponding to features Vessel-Sum and Solidity. The R OD region is characterized by maximum V essel -Sum and Solidity.</figDesc><graphic coords="6,338.51,528.78,198.00,126.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Example of the proposed 3-step automated OD segmentation algorithm.</figDesc><graphic coords="7,73.69,187.46,201.60,208.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The metrics used to evaluate the performance of automated OD segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>• SEN (Sensitivity) -It denotes the fraction of the actual OD area detected by the automated algorithm. Higher SEN denotes better segmentation. • F P R (False Positive Rate) -It denotes the fraction of falsely detected OD area. Lower F P R denotes better segmentation. • Acc (Accuracy) -It denotes the fraction of correctly identified pixels to the total number of pixels in the retinal region, i.e., Acc = T P +T N T P +T N +F P +F N . Higher Acc denotes better segmentation. • ∆ (VO error) -It denotes the Euclidean distance in pixels between the manually marked VO (O) and the detected VO ( Ô). Smaller ∆ denotes better segmentation. • Time-It denotes the average OD boundary segmentation and VO detection time per image for the particular data set measured in seconds. • Success (%) -It represents the percentage of images in a particular data set where the OD is successfully identified. Successful OD identification occurs when the automatically detected VO ( Ô) lies within the boundary of the manually segmented disc (D) [9] [14] [4][21]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Best and worst OD segmentation performances achieved using the proposed algorithm. The first two columns demonstrate the best OD segmentation cases. Third and fourth columns demonstrate the worst OD segmentation cases. The dotted blue outline represents the manually annotated OD (D) while the solid black circle represents the automatically segmented OD ( D). The cyan (*) represents the manually marked VO (O), while the black (*) represents the automated VO detected ( Ô).</figDesc><graphic coords="9,144.00,171.65,324.00,359.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I DEFINITION</head><label>I</label><figDesc>OF NOTATION. Pre-processed green plane of the fundus image. se l Linear structuring element with length 'l' pixels. se d Circular structuring element with diameter 'd' pixels. Ir Morphological reconstruction of the green plane image.</figDesc><table><row><cell>Notation</cell><cell>Meaning</cell></row><row><cell>I</cell><cell></cell></row><row><cell>Iv</cell><cell>Binary image containing major blood vessels obtained</cell></row><row><cell></cell><cell>by high-pass filtering and thresholding I.</cell></row><row><cell>I b</cell><cell>Bright region image obtained by thresholding Ir.</cell></row><row><cell>ν</cell><cell>Neighborhood parameter in pixels for detecting</cell></row><row><cell></cell><cell>bright regions in the vicinity of major vessels.</cell></row><row><cell>Overlap</cell><cell>Function that retains regions in I b that are closer</cell></row><row><cell>(Iv, I b , ν)</cell><cell>to the regions in Iv than ν pixels.</cell></row><row><cell>ρ</cell><cell>Manually estimated OD diameter.</cell></row><row><cell>Φ(Re)</cell><cell>Major axis length of a region R with index 'e'.</cell></row><row><cell>Ψ(Re)</cell><cell>Minor axis length of a region R with index 'e'.</cell></row><row><cell>A(Re)</cell><cell>Area of a region R with index number 'e'.</cell></row><row><cell>F (Re)</cell><cell>Area after holes in region R with index 'e' are filled.</cell></row><row><cell>Solidity(Re)</cell><cell>Ratio of the area and convex area for a region Re.</cell></row><row><cell>V essel-Sum</cell><cell>Function computing the sum of vessel pixels in</cell></row><row><cell>(Re, ρ)</cell><cell></cell></row></table><note><p>image Iv within a circular masked region generated at the centroid of bright region Re with radius ρ. µ I (Re) Average intensity of pixels in image I within region R with index number e. σ 2 (Re) Variance of pixel intensities in image I within region R with index number e. Rp Bright regions that are classified as bright probable OD regions after classification with GMM. Θ(Re) Centroid pixel of a region Re. R OD Best bright candidate region for OD. P Image obtained by combining the probable OD regions in Rp that are less than 'ρ' pixels away from R OD . H Convex hull containing all regions in the image. D Estimated OD boundary. Ô Segmented VO pixel.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III COMPARATIVE</head><label>III</label><figDesc>PERFORMANCE OF OD BOUNDARY SEGMENTATION WITH EXISTING WORKS. SUCCESS IN OD DETECTION IS REPRESENTED IN %.</figDesc><table><row><cell>Method</cell><cell>S</cell><cell>M</cell><cell>SEN</cell><cell>Success</cell></row><row><cell>DRIVE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Soparaphak et. al.[7]</cell><cell>0.179 (0.06)</cell><cell>20.94 (15.6)</cell><cell>0.80</cell><cell>98.61</cell></row><row><cell>Walter et. al.[8]</cell><cell>0.300 (0.13)</cell><cell>12.39 (8.3)</cell><cell>-</cell><cell>77.5</cell></row><row><cell>Seo et. al.[33]</cell><cell>0.316 (0.09)</cell><cell>11.19 (4.1)</cell><cell>-</cell><cell>95</cell></row><row><cell>Kande et. al.[34]</cell><cell>0.287 (0.08)</cell><cell>17.42 (8.1)</cell><cell>-</cell><cell>95</cell></row><row><cell>Stapor at. al.[35]</cell><cell>0.325 (0.15)</cell><cell>9.85 (6.0)</cell><cell>-</cell><cell>87.5</cell></row><row><cell>Lupascu et. al.[36]</cell><cell>0.403 (0.08)</cell><cell>8.05 (7.6)</cell><cell>-</cell><cell>95</cell></row><row><cell>Welfer et. al.[13]</cell><cell>0.415 (0.08)</cell><cell>5.74</cell><cell>-</cell><cell>100</cell></row><row><cell>Zeng et. al.[37]</cell><cell>0.559</cell><cell>10.24</cell><cell>0.65</cell><cell>85</cell></row><row><cell>Boykov et. al.[38]</cell><cell>0.553</cell><cell>9.97</cell><cell>0.74</cell><cell>95</cell></row><row><cell>Salazar et. al.[39]</cell><cell>0.707</cell><cell>6.68</cell><cell>0.84</cell><cell>97.5</cell></row><row><cell>Morales et. al.[17]</cell><cell>0.716 (0.17)</cell><cell>5.85 (10.2)</cell><cell>0.854</cell><cell>100</cell></row><row><cell>Proposed</cell><cell>0.807 (0.09)</cell><cell>5.01 (3.5)</cell><cell>0.88</cell><cell>100</cell></row><row><cell>DIARETDB1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Soparaphak et. al.[7]</cell><cell>0.298 (0.09)</cell><cell>16.31 (5.3)</cell><cell>0.80</cell><cell>93.70</cell></row><row><cell>Walter et. al.[8]</cell><cell>0.372 (0.11)</cell><cell>15.52 (5.3)</cell><cell>-</cell><cell>92.13</cell></row><row><cell>Seo et. al.[33]</cell><cell>0.353 (0.07)</cell><cell>9.74 (4.6)</cell><cell>-</cell><cell>80.89</cell></row><row><cell>Kande et. al.[34]</cell><cell>0.332 (0.05)</cell><cell>8.35 (3.2)</cell><cell>-</cell><cell>88.76</cell></row><row><cell>Stapor at. al.[35]</cell><cell>0.341 (0.09)</cell><cell>6.02 (5.6)</cell><cell>-</cell><cell>78.65</cell></row><row><cell>Lupascu et. al.[36]</cell><cell>0.309 (0.13)</cell><cell>13.81 (9.1)</cell><cell>-</cell><cell>86.51</cell></row><row><cell>Welfer et. al.[13]</cell><cell>0.434 (0.11)</cell><cell>8.31 (4.1)</cell><cell>-</cell><cell>97.7</cell></row><row><cell>Zeng et. al.[37]</cell><cell>0.384</cell><cell>17.49</cell><cell>0.55</cell><cell>75.28</cell></row><row><cell>Boykov et. al.[38]</cell><cell>0.540</cell><cell>10.74</cell><cell>0.76</cell><cell>85.4</cell></row><row><cell>Salazar et. al.[39]</cell><cell>0.757</cell><cell>6.38</cell><cell>0.86</cell><cell>96.7</cell></row><row><cell>Morales et. al.[17]</cell><cell>0.817 (0.13)</cell><cell>2.88 (3.0)</cell><cell>0.922</cell><cell>100</cell></row><row><cell>Proposed</cell><cell>0.802 (0.07)</cell><cell>4.82 (1.6)</cell><cell>0.88</cell><cell>100</cell></row><row><cell>MESSIDOR</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Yu et. al.[40]</cell><cell>0.83</cell><cell>7.7</cell><cell>-</cell><cell>99.08</cell></row><row><cell>Aquino et. al.[4]</cell><cell>0.86</cell><cell>-</cell><cell>-</cell><cell>98.83</cell></row><row><cell>Morales et. al.[17]</cell><cell>0.823 (0.14)</cell><cell>4.07 (6.1)</cell><cell>0.93</cell><cell>-</cell></row><row><cell>Giachetti et. al. [19]</cell><cell>0.88</cell><cell>-</cell><cell>-</cell><cell>99.83</cell></row><row><cell>Marin et. al. [21]</cell><cell>0.87</cell><cell>6.17</cell><cell>0.92</cell><cell>99.75</cell></row><row><cell>Proposed</cell><cell>0.84 (0.07)</cell><cell>3.9 (1.6)</cell><cell>0.90</cell><cell>100</cell></row><row><cell cols="5">et. al. [9] fails in 'im0044', while Zhang et. al. [20] fails</cell></row><row><cell cols="5">in 'im0041' due to the lack of sizable vessels. Also, the</cell></row><row><cell cols="5">proposed method has least computational time, but its OD</cell></row><row><cell cols="5">center detection is outperformed by Lu et. al. [9]. This occurs</cell></row><row><cell cols="5">since the proposed method detects the VO location as the best</cell></row><row><cell cols="5">estimate for the OD center. The high variability in imaging</cell></row><row><cell cols="5">angles of the STARE images causes partial visibility of OD,</cell></row><row><cell cols="5">thereby resulting in high variability of VO location and a high</cell></row><row><cell>∆ metric.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV PERFORMANCE</head><label>IV</label><figDesc>OF OD CENTER DETECTION FOR STARE DATA SET.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II AVERAGE</head><label>II</label><figDesc>BEST PERFORMANCE METRICS AND THEIR STANDARD DEVIATION () PER IMAGE USING THE PROPOSED METHOD ON THE TEST DATA SETS. †</figDesc><table><row><cell>Data</cell><cell>S</cell><cell>M</cell><cell>SEN</cell><cell>FPR</cell><cell>Acc</cell><cell>∆ (pixel)</cell><cell>AUC</cell><cell>Time (sec)</cell></row><row><cell>DRIVE</cell><cell>0.8067 (0.097)</cell><cell>5.01 (3.49)</cell><cell>0.878 (0.077)</cell><cell>0.0020 (0.0023)</cell><cell>0.991 (0.0043)</cell><cell>9.61 (4.21)</cell><cell>0.9561</cell><cell>1.27 (0.13)</cell></row><row><cell>DIARETDB1</cell><cell>0.8022 (0.074)</cell><cell>4.82 (1.59)</cell><cell>0.8815 (0.057)</cell><cell>0.0019 (0.001)</cell><cell>0.9963 (0.001)</cell><cell>7.80 (3.56)</cell><cell>0.9596</cell><cell>1.25 (0.12)</cell></row><row><cell>DIARETDB0</cell><cell>0.7761 (0.093)</cell><cell>4.91 (2.44)</cell><cell>0.8660 (0.068)</cell><cell>0.0022 (0.001)</cell><cell>0.9956 (0.002)</cell><cell>8.53 (4.76)</cell><cell>0.9333</cell><cell>1.25 (0.12)</cell></row><row><cell>CHASE DB1</cell><cell>0.8082 (0.075)</cell><cell>5.19 (2.03)</cell><cell>0.8962 (0.047)</cell><cell>0.0039 (0.001)</cell><cell>0.9914 (0.002)</cell><cell>8.19 (4.34)</cell><cell>0.9467</cell><cell>1.64 (0.11)</cell></row><row><cell>MESSIDOR</cell><cell>0.8373 (0.095)</cell><cell>3.93 (1.58)</cell><cell>0.9043 (0.072)</cell><cell>0.0018 (0.0008)</cell><cell>0.9956 (0.001)</cell><cell>7.51 (4.19)  †</cell><cell>0.971</cell><cell>2.14 (0.12)  †</cell></row><row><cell>STARE</cell><cell>0.7286 (0.15)</cell><cell>9.13 (6.41)</cell><cell>0.8380 (0.15)</cell><cell>0.011 (0.01)</cell><cell>0.9854 (0.02)</cell><cell>11.86 (11.73)</cell><cell>0.912</cell><cell>1.85 (0.12)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V PERCENTAGE</head><label>V</label><figDesc>OF IMAGES IN PARTICULAR OVERLAPPING INTERVALS.</figDesc><table><row><cell>Method</cell><cell cols="4">S ≥ 0.9 S ≥ 0.85 S ≥ 0.75 S ≥ 0.7</cell></row><row><cell>DRIVE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Zheng et. al.[37]</cell><cell>2.5</cell><cell>7.5</cell><cell>17.5</cell><cell>20</cell></row><row><cell>Boykov et. al.[38]</cell><cell>0</cell><cell>0</cell><cell>12.5</cell><cell>27.5</cell></row><row><cell>Salazar et. al.[39]</cell><cell>5</cell><cell>25</cell><cell>50</cell><cell>57.5</cell></row><row><cell>Proposed</cell><cell>16.7</cell><cell>40</cell><cell>83.3</cell><cell>93.3</cell></row><row><cell>DIARETDB1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Zheng et. al.[37]</cell><cell>1.12</cell><cell>2.25</cell><cell>5.62</cell><cell>10.11</cell></row><row><cell>Boykov et. al.[38]</cell><cell>2.25</cell><cell>8.99</cell><cell>28.09</cell><cell>38.2</cell></row><row><cell>Salazar et. al.[39]</cell><cell>11.24</cell><cell>32.58</cell><cell>60.67</cell><cell>70.78</cell></row><row><cell>Proposed</cell><cell>10.12</cell><cell>32.91</cell><cell>73.41</cell><cell>91.13</cell></row><row><cell>DIARETDB0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Proposed</cell><cell>4.16</cell><cell>26.67</cell><cell>68.34</cell><cell>82.5</cell></row><row><cell>CHASE DB1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Proposed</cell><cell>11.11</cell><cell>44.44</cell><cell>83.33</cell><cell>89</cell></row><row><cell>MESSIDOR</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Aquino et. al.[4]</cell><cell>46</cell><cell>73</cell><cell>90</cell><cell>93</cell></row><row><cell>Yu et. al.[40]</cell><cell>25</cell><cell>45</cell><cell>77</cell><cell>77</cell></row><row><cell>Giachetti at. al. [19]</cell><cell>59-62</cell><cell>78-82</cell><cell>89-92</cell><cell>92-94</cell></row><row><cell>Marin et. al. [21]</cell><cell>48.92</cell><cell>&lt;83.75</cell><cell>-</cell><cell>95</cell></row><row><cell>Proposed</cell><cell>20</cell><cell>47.56</cell><cell>82.85</cell><cell>96.72</cell></row><row><cell cols="5">segmented OD to the original image dimensions, the average</cell></row><row><cell cols="5">OD radius in the test images from the MESSIDOR data set</cell></row><row><cell cols="5">containing 400 images from three data subsets named: CHU de</cell></row><row><cell cols="5">St Etienne, LaTIM-CHU de BREST, and Lariboisiere are 102,</cell></row><row><cell cols="5">109 and 70 pixels, respectively. The quality of OD boundary</cell></row><row><cell cols="5">segmentation is measured in terms of the percentage of images</cell></row><row><cell cols="5">with 'M ' values less than (1/20), (1/10), (1/5) and (1/3) of</cell></row><row><cell cols="5">the OD radius in Table VI. The 'M ' metric for a segmented</cell></row><row><cell cols="5">OD contained within the actual OD boundary (F P R = 0) is</cell></row><row><cell cols="5">lower than segmented OD with higher F P R. In Table VI, we</cell></row><row><cell cols="5">observe that since the proposed method results in the lowest</cell></row><row><cell cols="5">FPR when compared to existing methods in [40] [4] [21], it</cell></row><row><cell cols="5">achieves highest percentage of images in Excellent, Good and</cell></row><row><cell cols="2">Fair segmentation categories.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI PERCENTAGE</head><label>VI</label><figDesc>OF IMAGES WITH M &lt; FRACTIONS OF OD RADIUS FOR THE MESSIDOR DATA SUB-SETS.</figDesc><table><row><cell></cell><cell>Excellent</cell><cell>Good</cell><cell>Fair</cell><cell></cell></row><row><cell>Method</cell><cell>(1/20)</cell><cell>(1/10)</cell><cell>(1/5)</cell><cell>(1/3)</cell></row><row><cell>CHU de St Etienne</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Yu et. al.[40]</cell><cell>29</cell><cell>64</cell><cell>89</cell><cell>98</cell></row><row><cell>Marin et. al. [21]</cell><cell>48.5</cell><cell>82.25</cell><cell cols="2">97.75 99.25</cell></row><row><cell>Proposed</cell><cell>59.25</cell><cell>97.75</cell><cell>99.25</cell><cell>100</cell></row><row><cell>LaTIM-CHU de BREST</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Yu et. al.[40]</cell><cell>33</cell><cell>64</cell><cell>87</cell><cell>96</cell></row><row><cell>Marin et. al. [21]</cell><cell>41.75</cell><cell>77</cell><cell>96.5</cell><cell>99.5</cell></row><row><cell>Proposed</cell><cell>59</cell><cell>99.25</cell><cell>100</cell><cell>100</cell></row><row><cell>Lariboisiere</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Yu et. al.[40]</cell><cell>34</cell><cell>71</cell><cell>90</cell><cell>98</cell></row><row><cell>Marin et. al. [21]</cell><cell>34</cell><cell>76</cell><cell>99.75</cell><cell>99</cell></row><row><cell>Proposed</cell><cell>56.41</cell><cell>97.69</cell><cell>100</cell><cell>100</cell></row><row><cell>All</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Yu et. al.[40]</cell><cell>32</cell><cell>66</cell><cell>89</cell><cell>97</cell></row><row><cell>Aquino et. al.[4]</cell><cell>40</cell><cell>79</cell><cell>97</cell><cell>97</cell></row><row><cell>Marin et. al. [21]</cell><cell>41.42</cell><cell>78.42</cell><cell cols="2">96.67 99.25</cell></row><row><cell>Proposed</cell><cell>58.5</cell><cell>98.5</cell><cell>99.75</cell><cell>100</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This research was supported in part by a grant from the Institute for Engineering and Medicine at the University of Minnesota and in part by an unrestricted departmental grant from Research to Prevent Blindness Inc, New York, NY. Sohini Roychowdhury</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DISCLOSURE</head><p>The contents of this paper are covered in the US Patent Application <ref type="bibr" target="#b42">[43]</ref>. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Optic disk and cup segmentation from monocular color retinal images for glaucoma assessment</title>
		<author>
			<persName><forename type="first">G</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnadas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1192" to="1205" />
			<date type="published" when="2011-06">June 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Superpixel classification based optic disc and optic cup segmentation for glaucoma screening</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N.-M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1019" to="1032" />
			<date type="published" when="2013-06">June 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Combining algorithms for automatic detection of optic disc and macula in fundus images</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kovacs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Harangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Peto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hajdu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="138" to="145" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detecting the optic disc boundary in digital fundus images using morphological, edge detection, and feature extraction techniques</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aquino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gegundez-Arias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1860" to="1869" />
			<date type="published" when="2010-11">nov. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dream: Diabetic retinopathy analysis using machine learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koozekanani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Parhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1717" to="1728" />
			<date type="published" when="2014-09">Sept 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Automatic localization of the optic disc in digital colour images of the human retina</title>
		<author>
			<persName><forename type="first">F</forename><surname>Haar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="report_type">M.S. Thesis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic detection of diabetic retinopathy exudates from non-dilated retinal images using mathematical morphology methods</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sopharak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Uyyanonvara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Barman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="720" to="727" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A contribution of image processing to the diagnosis of diabetic retinopathy-detection of exudates in color fundus images of the human retina</title>
		<author>
			<persName><forename type="first">T</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Massin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Erginay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1236" to="1243" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic optic disc detection from retinal images by a line operator</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="94" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Accurate and efficient optic disc detection and segmentation by a circular transformation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2126" to="2133" />
			<date type="published" when="2011-12">Dec 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Optic disc detection from normalized digital fundus images by means of a vessels&apos; direction matched filter</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Youssif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Z</forename><surname>Ghalwash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Ghoneim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="18" />
			<date type="published" when="2008-01">jan. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Detection of the optic nerve head in fundus images of the retina with Gabor filters and Phase Portrait analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rangayyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ayres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Digital Imaging</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="438" to="453" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Segmentation of the optic disk in color eye fundus images using an adaptive morphological approach</title>
		<author>
			<persName><forename type="first">D</forename><surname>Welfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Scharcanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Kitamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Dal Pizzol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">W B</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Marinho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="124" to="137" />
			<date type="published" when="2010-02">Feb. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic localization of the optic disc by combining vascular and intensity information</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Mendonca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sousa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mendonca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Campilho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="409" to="417" />
			<date type="published" when="2013-09">Sept 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automated diagnosis and image understanding with object extraction, object classification, and inferencing in retinal images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Goldbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moezzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Image Processing</title>
		<meeting>International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="1996-09">sep 1996</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="695" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast detection of the optic disc and fovea in color fundus photographs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Abrmoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="859" to="870" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic detection of optic disc based on PCA and mathematical morphology</title>
		<author>
			<persName><forename type="first">S</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Naranjo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Angulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alcaniz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="786" to="796" />
			<date type="published" when="2013-04">April 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The use of radial symmetry to localize retinal landmarks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Giachetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ballerini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Trucco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wilson</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0895611113001250" />
	</analytic>
	<monogr>
		<title level="m">retinal Image Analysis</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Accurate and reliable segmentation of the optic disc in digital fundus images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Giachetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ballerini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Trucco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">24001</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Novel accurate and fast optic disc detection in retinal images with vessel distribution and directional characteristics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1109/JBHI.2014.2365514</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Obtaining optic disc center and pixel region by automatic thresholding methods on morphologically processed fundus images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Gegundez-Arias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Suero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bravo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods and Programs in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="173" to="185" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Gnu image manipulation program</title>
		<ptr target="http://www.gimp.org/downloads/" />
		<imprint>
			<date type="published" when="1996-01">January 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ridge based vessel segmentation in color images of the retina</title>
		<author>
			<persName><forename type="first">J</forename><surname>Staal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abramoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="501" to="509" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Diaretdb0: Evaluation database and methodology for diabetic retinopathy algorithms</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kauppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kalesnykiene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-K</forename><surname>Kamarainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lensu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sorri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Uusitalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Klviinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pietil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Lappeenranta University of Technology, Finland, Tech. Rep.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Diaretdb1 diabetic retinopathy database and evaluation protocol</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kauppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kalesnykiene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-K</forename><surname>Kmrinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lensu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raninen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Voutilainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Uusitalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Klviinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pietil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 11th Conf. on Medical Image Understanding and Analysis (MIUA2007)</title>
		<meeting>of the 11th Conf. on Medical Image Understanding and Analysis (MIUA2007)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="61" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Chase db1</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">U</forename><surname>Research</surname></persName>
		</author>
		<ptr target="http://blogs.kingston.ac.uk/retinal/chasedb1/" />
		<imprint>
			<date type="published" when="2011-01">January 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Messidor: Digital retinal images</title>
		<ptr target="http://messidor.crihan.fr/download-en.php" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Locating the optic nerve in retinal image using the fuzzy convergence of bood vessels</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<date type="published" when="2003-08">August 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Locating the optic disc in retinal images using morphological techniques</title>
		<author>
			<persName><forename type="first">A</forename><surname>Suero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Gegúndez-Arias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bravo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IWBBIO</title>
		<imprint>
			<biblScope unit="page" from="593" to="600" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Blood vessel segmentation of fundus images by major vessel extraction and subimage classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koozekanani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Parhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1118" to="1128" />
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Using the ellipse to fit and enclose data points</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Loan</surname></persName>
		</author>
		<ptr target="http://www.cs.cornell.edu/cv/otherpdf/ellipse.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Feature selection based on mutual information criteria of max-dependency, max-relevance, and minredundancy</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2005-08">Aug 2005</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1226" to="1238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Measurement of ocular torsion using digital fundus image</title>
		<author>
			<persName><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th Annual International Conference of the IEEE Engineering in Medicine and Biology Society</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1711" to="1713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Segmentation of exudates and optic disk in retinal images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Subbaiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Savithri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics Image Processing</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="535" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Segmentation of fundus eye images using methods of mathematical morphology for glaucoma diagnosis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Stapor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Witonski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chrastek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Michelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Science -ICCS 2004, ser. Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Bubak</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Albada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Sloot</surname></persName>
		</editor>
		<editor>
			<persName><surname>Dongarra</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3039</biblScope>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automated detection of optic disc location in retinal images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lupascu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tegolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Di</forename><surname>Rosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Computer-Based Medical Systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="17" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Topology cuts: A novel min-cut/max-flow algorithm for topology preserving segmentation in nd images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="90" />
			<date type="published" when="2008-10">Oct. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graph cuts and efficient n-d image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Funka-Lea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="131" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Optic disc segmentation by incorporating blood vessel compensation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Salazar-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Third International Workshop On Computational Intelligence In Medical Imaging (CIMI)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fast localization of optic disc and fovea in retinal images for eye disease screening</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Barriga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Agurto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Echegaray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pattichis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zamora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Soliz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer-Aided Diagnosis</title>
		<imprint>
			<biblScope unit="volume">7963</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">796317</biblScope>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Detection of optic disc in retinal images by means of a geometrical model of vessel structure</title>
		<author>
			<persName><forename type="first">M</forename><surname>Foracchia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grisan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ruggeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1189" to="1195" />
			<date type="published" when="2004-10">oct. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Optic disc localization in retinal images using histogram matching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-S</forename><surname>Moin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Method and apparatus to locate the optic disc in fundus images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Parhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Patent Application Number 13/999</title>
		<imprint>
			<date type="published" when="2014-04-09">April 9 2014</date>
			<biblScope unit="volume">955</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
