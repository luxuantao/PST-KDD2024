<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Gabriel</forename><surname>Krummenacher</surname></persName>
							<email>gabriel@kru.li</email>
						</author>
						<author>
							<persName><forename type="first">Soon</forename><surname>Cheng</surname></persName>
							<email>chengsoon.ong@anu.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Stefan</forename><surname>Ong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Seijin</forename><surname>Koller</surname></persName>
							<email>stefan.koller@sbb.ch</email>
						</author>
						<author>
							<persName><forename type="first">Joachim</forename><forename type="middle">M</forename><surname>Kobayashi</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><surname>Buhmann</surname></persName>
							<email>jbuhmann@ethz.ch</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zürich</orgName>
								<address>
									<postCode>8092</postCode>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Zühlke Engineering AG</orgName>
								<address>
									<postCode>8952</postCode>
									<settlement>Schlieren</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Machine Learning Research Group</orgName>
								<orgName type="institution">CSIRO</orgName>
								<address>
									<postCode>Data61, 2601</postCode>
									<settlement>Canberra</settlement>
									<region>ACT</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Installations and Technology</orgName>
								<orgName type="institution">SBB AG</orgName>
								<address>
									<postCode>6005</postCode>
									<settlement>Luzern</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">He is now with Google</orgName>
								<orgName type="institution">ETH Zürich</orgName>
								<address>
									<postCode>8092, 8002</postCode>
									<settlement>Zürich, Zürich</settlement>
									<country>Switzerland., Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zürich</orgName>
								<address>
									<postCode>8092</postCode>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E0D5823E5017265086A5C452F0544676</idno>
					<idno type="DOI">10.1109/TITS.2017.2720721</idno>
					<note type="submission">received July 13, 2016; revised December 17, 2016; accepted June 11, 2017. The Associate Editor for this paper was K. Wang.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Machine learning</term>
					<term>statistical learning</term>
					<term>support vector machines</term>
					<term>pattern analysis</term>
					<term>railway safety</term>
					<term>railway accidents</term>
					<term>wavelet transforms</term>
					<term>supervised learning</term>
					<term>artificial neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Wheel defects on railway wagons have been identified as an important source of damage to the railway infrastructure and rolling stock. They also cause noise and vibration emissions that are costly to mitigate. We propose two machine learning methods to automatically detect these wheel defects, based on the wheel vertical force measured by a permanently installed sensor system on the railway network. Our methods automatically learn different types of wheel defects and predict during normal operation if a wheel has a defect or not. The first method is based on novel features for classifying time series data and it is used for classification with a support vector machine. To evaluate the performance of our method we construct multiple data sets for the following defect types: flat spot, shelling, and non-roundness. We outperform classical defect detection methods for flat spots and demonstrate prediction for the other two defect types for the first time. Motivated by the recent success of artificial neural networks for image classification, we train custom artificial neural networks with convolutional layers on 2-D representations of the measurement time series. The neural network approach improves the performance on wheels with flat spots and non-roundness by explicitly modeling the multi sensor structure of the measurement system through multiple instance learning and shift invariant networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>E ARLY detection of serious wheel defects on freight trains are an essential part in preventing damage to the railway infrastructure and in providing the train operators with timely information on necessary repairs, that can prevent further deterioration of the wheels.</p><p>Wheel defects of railway vehicles directly cause an increase in attrition of and damage to the railway infrastructure, e.g., the track systems or the civil engineering works, thereby adding additional costs to maintenance and repair and leading to a reduced lifetime and availability of rolling stock. The life span of the railway infrastructure is significantly shortened by the negative effects of wheel defects. The life span of railway bridges for instance is calculated with an assumed maximal dynamical load of 21 tons. Due to wheel defects the actually occurring dynamical load can be up to 50 tons, or 270% higher than the theoretically assumed maximum, thus shortening the life span. Wheel defects also accelerate crack-growth on the rail tracks and lead to premature failure of the rail system.</p><p>Another important effect caused by wheel defects are ground vibration and noise emissions. In the European Union (EU) Project "Railway Induced Vibration Abatement Solutions" (RIVAS) 1 27 partners from nine countries investigated the source and mitigation measures for noise and vibration emissions. They found that reducing wheel defects by wheel maintenance significantly reduces vibration and noise emissions directly <ref type="bibr" target="#b0">[1]</ref>. Therefore, it is recommended to use timely and targeted maintenance of train wheels as an economic means to reduce emissions <ref type="bibr" target="#b1">[2]</ref>. This measure is all the more important as the density and usage of modern railway networks is steadily increasing and failures quickly disrupt operation of the whole network or parts of it. Since 2008, all states in the EU are advised to employ noise emission ceilings. Switzerland started a noise abatement program based on emission ceilings that requires the infrastructure manager to curb emissions above the ceiling. This abatement programme leads to total costs of 1.5 billion CHF <ref type="bibr" target="#b2">[3]</ref>.</p><p>In this paper we propose a method of detecting defective wheels. This classification method promises to increase the reliability of the railway infrastructure, to reduce the cost of freight train operation and to save additional investments on noise protection measures. To reach this goal without the costly construction of further measurement sites or newly built sensors, we propose the use of statistical methods that allow us to automatically inspect the existing data and extract the information about defective wheels that is already present.</p><p>Our proposed methods do neither require a model of the measurement system, nor of train dynamics or wheel defects. The methods enable us to predict defects on wheels where there is no prior understanding of how these defects manifest themselves in the measurements. The methods detect and classify different types of defects based on measurements during normal operation where the trains pass the measurement sites in full operational speed. The features that we have developed for the use in supervised learning are general and can in principle be used for any time series data and are not restricted to specific defect types. In a second step we automatically learn features directly from the raw measurement signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Contribution</head><p>Our main contribution are two methods for automatic railway wheel defect detection and classification through vertical force measurements of trains running in full operational speed. For the first method we design novel wavelet features for time series data from multiple sensors and we learn a classifier using a support vector machine. For the second method we design and train convolutional neural networks for different wheel defect types by deep learning.</p><p>We evaluate our novel and other classical methods for wheel defect detection on two labeled data sets with different types of wheel defects, that we have constructed from calibration runs and from maintenance reports.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Related Work</head><p>While there has been research on machine learning methods for railway track inspection <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref> or condition based maintenance <ref type="bibr" target="#b6">[7]</ref>, to our knowledge machine learning methods for railway wheel defect detection have not been developed so far. There has been some research on sensor systems for wheel defect detection on freight trains. Nenov et al. <ref type="bibr" target="#b7">[8]</ref> analyses the signal from acceleration sensors and demonstrate that they can visually see a difference between the measurements of wheels with flat spots and good wheels but they do not propose a method for detection. Another related work <ref type="bibr" target="#b8">[9]</ref> advocates the use of Fibre Bragg Gating sensors for defect detection of rails to monitor track conditions. The authors investigate the wavelet decomposition of pressure signals but they do not propose a method or threshold for automatic defect detection. Jianhai et al. <ref type="bibr" target="#b9">[10]</ref> use continuous wavelet analysis of acceleration sensor data to visually inspect the measurements and conclude that there is a difference in the coefficients for wheel with flat spots and defect-free wheels.</p><p>Different kinds of track scales are in use in the field. They can in principle be used to detect flat spots. But to our knowledge they do not use machine learning to train a defect classifier. A general advantage of our proposed system is that the measurement system is relatively inexpensive, but we can show that it can still be used to detect wheel defects, thanks to our proposed machine learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MEASUREMENT SYSTEM AND DEFECT TYPES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Wheel Load Checkpoint</head><p>The infrastructure division of the Swiss railway operator SBB operates and maintains the one of the most heavily used railway network of the world. In 2010, 95.4 km of trains travelled one kilometer of track on average; this value documents the highest utilisation of network capacity in the world <ref type="bibr" target="#b10">[11]</ref>. Automatically monitoring trains and network are thus important to minimise the risk of incidents that quickly affect the scheduling of trains on the network. SBB infrastructure operates an integrated wayside train monitoring system that controls safety relevant aspects of the railway traffic and infrastructure.</p><p>As part of this system, the wheel load checkpoints (WLC) measure vertical force through strain gauges installed on the rails. These devices are used for observing maximal axle load, maximal train load, load displacement and grave wheel defects. Our study investigates the use of machine learning methods to defect and classify wheel defects based on the data obtained through these wheel load checkpoints.</p><p>Each WLC consists of four 1m long measurement bars with four strain gauges (referred to as sensors in the following) per measurement bar. Since on each side two measurement bars with 4 sensors are installed, each wheel that runs over the WLC is measured eight times at different parts of the wheel. Fig. <ref type="figure">1</ref> shows schematically the measurement of one wheel by one measurement bar. In this example a defect is directly observed by the measurement of the first sensor.</p><p>See Fig. <ref type="figure">2</ref> for a diagram of one sensor. The strain gauges are installed perpendicular on the centerline of the railroad track and they are combined into one vertical wheel force measurement. One sensor covers approximately 30cm of the wheel circumference.</p><p>The wheel load checkpoints are installed on multiple strategic sites on the railway network: ten on the border to Switzerland at the entrance to the railway network maintained by SBB and a dozen within the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Railway Wheel Defects</head><p>A relatively well understood wheel defect type is the flat spot or wheel flat. This defect occurs when the wheel stops rotating (for instance during an emergency brake) and is dragged along the track. Fig. <ref type="figure" target="#fig_1">3</ref> shows an image of a flat spot on a railway wheel of SBB and the corresponding idealized measurement obtained by the WLC if the flat spot directly hits a sensor of the measurement system. Grave wheel flats can be detected by looking at simple statistics (c.f. Section VI-B) of the measurement if the defect hits the sensor perfectly. To be able to detect flat spots that are less grave or that do not hit a sensor directly, more advanced machine learning methods are required. We demonstrate such cases on our first data set in Section VII-B.</p><p>Apart from flat spot, other common wheel defects on railway vehicles are non-roundness and shelling <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Wheels with non-roundness have a high influence on the vibration and noise emitted by a passing train and, therefore, they are an important type of defect to detect <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Non-roundness, in contrast to shelling and flat spot, is a non-discrete type of defect. This characterization means that the defect affects a large part of the wheel and changes its shape in a non-local way. We create an additional data set that contains the defect types flat spot, non-roundness and shelling (Section VI-C) and then, we compare the performance of our two machine learning methods in predicting these three defect types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. TIME SERIES REPRESENTATION</head><p>FOR DEFECT DETECTION An important step in any machine learning method is finding a representation of the original measurements that supports discrimination between different classes. For instance: the mean of the measurement signal of a wheel with or without a flat spot coincide if the weight of the axle is the same and the defect perfectly hits a sensor. The standard deviation on the other hand differs significantly, since the force exerted on the track is much higher for a wheel with the flat spot than for non-defective wheels. For other types of defects like shelling this observation does not hold, as the variance of the measured force does not significantly differ from a non-defective wheel, but there is a clear difference in higher frequency bands of the measurement, c.f. Fig. <ref type="figure" target="#fig_2">4</ref>. These observations suggest to decompose the signal by a multiscale wavelet analysis in order to extract indicative frequency features for time series data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Wavelet Transform</head><p>The Discrete Wavelet Transform (DWT) decomposes a signal over an orthonormal basis of dilated and transformed wavelets <ref type="bibr" target="#b13">[14]</ref>:</p><formula xml:id="formula_0">ψ j,k (t) = 1 √ 2 j ψ t -k2 j 2 j , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where ψ denotes mother wavelet, j and k the scale and shift parameters.</p><p>The orthogonal wavelets given by definition (1) at different scales 2 j resolve the original signal at different resolutions. The DWT can thus be employed to construct a multiresolution signal approximation <ref type="bibr" target="#b13">[14]</ref>. An equivalent way of calculating the DWT is by passing the original signal through a series of appropriate high-pass and low-pass filters and sub-sampling operations, where at each level the output of the high-pass filter is stored as the detail coefficients for that level and the output of the low-pass filter is decomposed further at the next level until level T = log(n) is reached, where n is the length of the original signal. If the high-pass and low-pass filters in this filter bank are derived from the child wavelets in Equation 1, the detail coefficients (C1, . . ., CT ) correspond exactly to the wavelet coefficients.</p><p>The wavelet transform has been extensively used in fields ranging from biomedical signal processing <ref type="bibr" target="#b14">[15]</ref>, geosciences <ref type="bibr" target="#b15">[16]</ref> to image compression <ref type="bibr" target="#b16">[17]</ref>. Since weight measurement signals and the defect effects on the signal are both localized in time and frequency the wavelet transform explicitly encodes this local perturbation and, therefore, has an advantage over the fourier transform in our application. The weight measurement signals also show a self-similar behavior which suggests the wavelet transformation as an adapted set of basis functions with approximately the same amount of power per frequency band.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Wavelet Features for Defect Detection</head><p>To extract features from the measurement signals of the wheels, we first compute the wavelet decomposition of each signal. Each time series is now represented by the distributions of the wavelet coefficients at the different levels of the multiscale decomposition. To represent the distribution of the coefficients, n moments of the empirical distribution of the coefficients are computed. This representation captures higher order behaviour while still maintaining invariance to shift or scale of the defects as measured by the sensors. The procedure is summarized by Algorithm 1 and the function to compute the central moments is given below by Equation <ref type="formula">2</ref>, where the average is used as the first moment.</p><formula xml:id="formula_2">moment 1 (x) = x, moment m&gt;1 (x) = 1 n n i=1 (x i -x) m (2) Algorithm 1 Wavelet Feature Computation Input: W t : coefficients at the t-th level of a T -level DWT. 1: k = 1 2: for t = 1 . . . T + 1 do 3: for m = 1 . . . M do 4: F k = moment m (W t ) 5: k += 1 6:</formula><p>end for 7: end for Output:</p><formula xml:id="formula_3">F k : k = 1, . . . , M • (T + 1) wavelet features.</formula><p>As explained in Section II-A, we observe eight signals for each wheel that we want to classify. To compute features for one wheel, we first concatenate the measurements of all the sensors and then compute the wavelet features on this single time series. When we are processing localized defects, like a flat spot, that are observable as a change in vertical force on one sensor, the specific information, which sensor has observed a defect, does not play a role due to the scale invariance of our feature construction method. For each sensor, the measurement signal can be divided into the regions of no load, raising slope, load measurement window and falling slope. Even though the load measurement window is relatively small we can still observe wheel defects that manifest themselves in one of the slopes or during the no load phase before and after the load measurement. To capture this information, a window of size three times the measurement window is used for feature construction. In all our experiments we use the Daubechies-5 wavelet family as basis functions <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Load Normalized Features</head><p>In addition to the wavelet features computed on the full concatenated signals of all the sensors we also compute wavelet features for each sensor separately. Whereas the feature construction based on the full signal pursued the strategy to capture as much information as possible, the goal here is to construct features that are normalized with respect to the load measurement.</p><p>To this end, we first subtract an idealized measurement curve from the signal of each sensor and then compute wavelet features with Algorithm 1 on the difference. Additionally we add the mean squared error of the signal to the measurement template as a feature per sensor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Measurement Site</head><p>Each wheel load checkpoint exhibits different physical characteristics due to small differences in the ground below the tracks and the curvature of the track before the checkpoint. These characteristics change the wheel load measurement. Small unevenness in the tracks also manifest themselves as noise or small bumps in the signal. Therefore, we add the site of the wheel load checkpoint as additional feature to enable different predictions based on the origin of the measurement site. We encode this information as a unary code or a one-hot vector, where every dimension represents a site and is 1 only for measurements from that site. When in the future a new measurement site would be built on the railway network, training data for the new site would need to be collected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Load</head><p>A train with different load, but the same waggons results in different wheel measurements for the same defect types, since the weight of the train plays a significant role how the defect exerts its pressure on the sensors. Another important reason to add information about the load to the feature set arises from the following observation: certain defect classes like nonroundness mostly change the average of a sensor reading, but only marginally affect higher order information. An oval wheel for instance will result in higher load measured by some of the sensors and lower load by others, but will not be detected as a defect wheel by individual load normalized measurements. The mean load of all the sensors, standard deviation over the mean load per sensor and the mean load for each sensor are added to the feature set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. AUTOMATIC REPRESENTATION LEARNING</head><p>An alternative to predefined feature representations are provided by deep neural networks that learn the features from data in a task specific way to maximize correct classification. In this section we introduce a learning method to automatically infer a representation of the measurements for the classification of wheel defects based on deep artificial neural network models (DNN). These models have gained considerable popularity in recent years, mostly due to their success in image classification and segmentation tasks <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, in speech recognition <ref type="bibr" target="#b20">[21]</ref> and quite recently in reinforcement learning for playing Go <ref type="bibr" target="#b21">[22]</ref>.</p><p>DNN for wheel defect detection alleviates the burden of the modeller to manually construct features and allows to learn representations from time series directly. Another benefit is the flexibility that comes with designing decision functions as stacked activation layers. This flexibility allows us to design a network specifically for certain defect types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. 2-Dimensional Time Series Representation</head><p>Motivated by the success of convolutional neural networks on image classification tasks <ref type="bibr" target="#b22">[23]</ref> we propose the use of 2D representations of the measurement signals for wheel defect detection. Recently Gramian Angular Fields (GAF) have been proposed <ref type="bibr" target="#b23">[24]</ref> as a 2-dimensional encoding of time series data. This representation has been shown to capture cross-temporal dependencies and to enhance classification performance when used as input to a convolution network. A GAF is constructed by first transforming the time series to polar coordinates and then computing trigonometric sums between all points (See Wang and Oates <ref type="bibr" target="#b23">[24]</ref> for details of the construction).</p><p>As a second 2D representation we also considered transforming the time series into the image of its 2D graph. This procedure is motivated by the fact that a human expert would also look at such a two-dimensional representation to classify wheel defects. The addition of the value of the signal as the second dimension allows the network to learn different filters for different values of the signal at the same point in time (the first dimension). The procedure is summarized in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Compute 2D Time Series Representation</head><formula xml:id="formula_4">Input: X = (X t ) 1≤t ≤N : time series. Input: r &gt; 0: resolution. Input: [V min , V max ]: window. 1: h = V max -V min r 2: M = 0 h×N 3: X = X -V min 4: for t = 1 . . . N do 5: M X t r ,t = 1 6: end for 7: for m = 1 . . . N -1 do 8: Set all entries touching the segment [M Xm r ,m , M X m+1 r ,m+1</formula><p>] to 1, drawing a line segment between the two points. 9: end for Output: M: 2D graph of time series X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. DNN Network Architecture</head><p>We use a Convolutional Neural Network (CNN) based architecture to automatically extract the discriminating features. Here, we considered the 8 signals of the WLC as different channels. Our networks are composed of two modules: the mono channel feature extracting layers and cross channel feature extracting layers respectively from bottom (input layer) to top (classification layer). The mono channel feature extracting layers take each channel independently and compute high level features in parallel that can then be processed by the cross channel feature extracting layers. Furthermore, the weight of the mono channel feature extracting layers are shared across all channels, allowing it to learn from all channels at once. This approach is both computationally efficient, and also well suited for the data set. Since each channel represents a load measurement of the wheel from one sensor of the WLC the network learns features from the signals and also a relationship between the signals.</p><p>1) Mono Channel Feature Extracting Layer: This module is a traditional CNN, composed of a sequence of convolutional layers, eventually followed by a fully connected layer: a) Convolutional layer: A convolutional layer is a combination of a number of filtering layers, each followed by a non-linearity and a pooling layer. The settings chosen for each of these layers are specified below. The filtering layer outputs convolutional products of the input by learnable filters with a fixed receptive field. Every filter layer is followed by an activation function. We use a Parametric Rectified Linear Unit (PReLU), as it better back-propagates the gradient compared to the tangent hyperbolic or sigmoid functions, which can easily saturate. The PReLU non-linearity also prevents neurones from "dying out" as can be the case for the popular ReLU units, by introducing a learnable non-zero slope to the negative side of the input <ref type="bibr" target="#b24">[25]</ref>.</p><formula xml:id="formula_5">P ReLU(x) = max(0, x) + a • min(0, x) , (<label>3</label></formula><formula xml:id="formula_6">)</formula><p>where a is an adaptable parameter.</p><p>The pooling layers reduce the resolution of the input time series and the learned features at each layer of the deep neural network. This max-pooling allows the classification to be robust to small variations of learned features at each layer. In all of our convolutional layers, we used a pooling layer with filters of size 2 × 2 applied with downsampling ratio of two, taking the maximum value among the four pixels in its receptive field.</p><p>b) Fully connected layer: Neurons in a fully connected layer have full connections to all units in the previous layer. The layer outputs biased linear combination of its input, followed by a non-linearity. As a non-linearity we used the hyperbolic tangent function (tanh).</p><p>2) Cross Channel Feature Extracting Layer: a) Cyclic permutation network: The cyclic permutation network (Fig. <ref type="figure" target="#fig_3">5</ref>) is designed to learn cross-sensor features invariant to a cyclic permutation of the eight recordings. Depending on its phase, a given wheel can generate a set of possible recordings, which is approximately stable by cyclic permutation of the eight recordings. This network architecture serves the purpose to encode this characteristic of cyclic invariance. The network works in the following way:</p><p>1) The Cyclic Permutation Network sits on top of the Mono channel feature extracting layers. It takes as input the set of high level features of each channel computed independently by the weight shared CNN (represented as a dashed red box right of the signal in Fig. <ref type="figure" target="#fig_3">5</ref>).</p><p>2) The network then distributes the set of 8 feature vectors v i (the colored vertical bars in Fig. <ref type="figure" target="#fig_3">5</ref>) across 8 permutation channels (the stack of colored horizontal bars in Fig. <ref type="figure" target="#fig_3">5</ref>), one for each possible cyclic permutation of the feature vectors. Each permutation channel concatenates the feature vectors following the order of its specific cyclic permutation. Note the distinction between "channels" and "permutation channels", as the former refers to a specific sensor recording, while the latter refers to a permutation of the input channels, and contains the high level features of all initial channels.</p><p>3) Afterwards, the concatenated vector within each permutation channel is fed into a sequence of fully connected layers that extracts cross channel features and outputs the classification probability for the respective cyclic permutation (The blue circles in Fig. <ref type="figure" target="#fig_3">5</ref>). 4) Finally, the multiple log-likelihoods (one for each permutation channel) are combined by returning the maximal log-likelihood for the defect class and the minimal log-likelihood for the non-defective class (The green dashed box in Fig. <ref type="figure" target="#fig_3">5</ref>). Formally, given a set of 8 feature vectors, (v i ) 1≤i≤8 , for a wheel the cyclic permutation network computes the probability of defect P D as:</p><formula xml:id="formula_7">P D = max p∈P f (v p(1) . . . v p(8) ), (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>where P is the set of all possible cyclic permutations of the numbers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref>, f (•) is the function performed by the fully connected layers and is the concatenation operator. b) Defect detection network for flat spots: For tasks like flat spot detection, it is not necessary to learn complex cross channel features. Since a flat spot is a discrete defect and usually manifests itself only in one sensor reading, the Multiple Instance Learning (MIL) setting <ref type="bibr" target="#b25">[26]</ref> is appropriate. In this setting a wheel is considered defective when at least one of the sensor readings is predicted defective. The Defect Detection Network encodes this idea by reducing the cross channel feature to the indicator function of whether a defect has been detected in one of the channels: is the likelihood for defect and P N i for non-defect from sensor i . Since P N i = 1 -P D i and 0 ≤ P D i ≤ 1:</p><formula xml:id="formula_9">M I L(x) = (min i (1 -P D i ), max i (P D i )). (<label>5</label></formula><formula xml:id="formula_10">)</formula><p>In Fig. <ref type="figure" target="#fig_4">6</ref> we depict the structure of the DNN that we use to train a model for the detection of flat spots.</p><p>We call the last layer MIL-Layer. It makes sure that if one measurement of the wheel captures the defect, the probability of the wheel having a defect is high. If defects are not "seen" by any sensor this probability will be low. Moreover, when training with defective wheels, only the error of the channel with the highest defect probability is backpropagated, thus preventing the Mono channel feature extracting layer to try to learn features for defective signals on signals that show no defect.</p><p>The MIL setting was already used for the SVM based MIL flat spot classifier in Krummenacher et al. <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Top Layer Features Learned by the DNN</head><p>In this section we look at the features learned by the DNN and compare the filters learned by the network on the 1-dimensional or 2-dimensional time series representation. The results in this section were obtained by training on data set 2 (SectionVI-C) and defect type flat spot.</p><p>Examples of top-layer filters learned by the DNN directly on the 1-dimensional time series, as well as the features extracted by them are shown in Fig. <ref type="figure" target="#fig_5">7</ref>. We can observe that the network  Fig. <ref type="figure" target="#fig_6">8</ref> shows the top layer filters learned by the DNN on the 2-dimensional representation of the time series, and their respective extracted features on a defective and nondefective wheel. In general, the filters learned on the 2D representation encode high gradients in intensity, qualitatively presenting clear white/black delimitation. This suggests that the model focuses on 2D shape recognition rather than 1D pattern recognition as seen in filters learned on the time series directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CLASSIFICATION OF WHEEL DEFECTS</head><p>Detection and classification of wheel defects amounts to infer from a vertical force measurement x of a wheel if a wheel is defective or not. Mathematically, a function f (•) either encode the binary information, that a defect is present or absent, or its defect class when we can differentiate the defect category. To achieve this goal we use sets of measurements of wheels to train decision functions for certain defect types and for non-defective wheels. We then use this training set of measurements and labels (the type of defect) to automatically find a function that is expected to predict the defects of wheels not seen during training accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Support Vector Machine</head><p>One of the most popular models to find such a function are Support Vector Machines (SVM) <ref type="bibr" target="#b27">[28]</ref>. A SVM finds a linear function parameterized by the vector w that maximally separates the two classes during training. It achieves this separation by maximizing the margin between the points of the two classes in feature space, or equivalently by minimizing the regularized empirical risk</p><formula xml:id="formula_11">R(w) = 1 n n i=1 max(0, 1 -y i (w x i + b)) + λ w 2 , (<label>6</label></formula><formula xml:id="formula_12">)</formula><p>where we minimize the empirical risk over the parameters (w, b), that encode the hyperplane separating the two classes.</p><formula xml:id="formula_13">y i ∈ (-1, +1</formula><p>) is the label (class membership) of the i th example in the training set, x i denotes the feature vector of the i th measurements and max(0, 1z) is the hinge loss.</p><p>Measurements of a new wheel x can now be classified with the following decision rule:</p><formula xml:id="formula_14">y := sgn(w x + b). (<label>7</label></formula><formula xml:id="formula_15">)</formula><p>This decision rule <ref type="bibr" target="#b6">(7)</ref> expresses its data dependence only by a scalar product between weights w and the feature vector x. Therefore, we can model non-linear decision functions by replacing the scalar product with a kernel. A convenient choice is a Gaussian radial basis kernel function of the form k(x i , x j ) = exp(-γ x ix j</p><p>2 ) on the feature vectors x i , x j . We can now express the minimization problem above (Equation <ref type="formula" target="#formula_11">6</ref>) in the dual and employ the kernel trick to learn parameters α i and get the new classification rule</p><formula xml:id="formula_16">y = sgn n i=1 α i y i k(x i , x) + b . (<label>8</label></formula><formula xml:id="formula_17">)</formula><p>To determine the optimal parameters for regularization λ and scale γ we maximize accuracy on cross-validation folds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Classification with DNN</head><p>If we replace the hinge loss function in Equation ( <ref type="formula" target="#formula_11">6</ref>) in the previous section with the logistic loss function log 1 + exp(-y i w x) we get regularized logistic regression. This optimization problem has the advantage that optimization algorithms estimate probabilities of the class likelihoods in addition to the binary labels. Using the softmax function instead of the logistic loss this benefit can be generalized to an arbitrary number of classes. We will use these probability estimates through a SoftMax-layer in our DNN to combine the output of multiple classifiers for different measurements of the same wheel.</p><p>For a given input and C classes, its log-likelihood for belonging to class i equals</p><formula xml:id="formula_18">p(v|i ) = log exp(v i ) C j =1 exp(v j ) , (<label>9</label></formula><formula xml:id="formula_19">)</formula><p>where (v i ) 1≤i≤C are the top-layer features of the network. The soft-max function above is not only used for DNNs but also in many multiclass classification methods, for instance for logistic regression or in dynamical system estimation with multiple model adaptive estimation (MMAE) <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> Unlike the previous section, where the classification function f (•) was modeled as a linear function in a Hilbert space, that takes a fixed representation of the measurements, DNNs model this function as a hierarchical structure (layers) of linear combinations and activation functions (non-linearities) directly on the time series of the measurement (Section IV).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DATA SETS AND MODELS</head><p>Two data sets from different sources are assembled to evaluate the performance of different methods for wheel defect detection and classification and to train various classifiers. For both data sets the signals that we use to predict a wheel defect are measured by the wheel load checkpoint (Section II-A). The annotations or labels that provide the information about the defectiveness and defect class of a wheel are collected from different sources. These data sets contain information about different types of defects as described in the following. We also describe what models and features we will use for the respective data sets in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Models and Features</head><p>On the first data set we compare the Wavelet-SVM with benchmark flat spot prediction methods. We show that it greatly outperforms prior art based on thresholding the dynamical coefficient (Eq. 10 below) and also on multiple instance learning with dynamic time warping.</p><p>The second data set serves to demonstrate that the Wavelet-SVM can accurately classify all three defect types. We also compare the performance of the deep learning models on different time series representations by showing that the cyclic permutation network outperforms the simpler neural networks and also the Wavelet-SVM for non-roundness. For flat spots, the neural network with features learned on the 2D time series representation also outperforms the Wavelet-SVM.</p><p>We use different models and features for different defect classes, as this allows us to model network structure and feature construction adaptively to the effects the defects have on the measurements. Thus the problem differs from standard multi-class classification where one model predicts a vector of class probabilities over all classes. Instead we are looking at independent binary classification tasks per defect class, where the task is to distinguish between one defect type and nondefective. This enables clear comparison between the different models.</p><p>As there are no known methods to predict nonroundness or shelling we compare to baseline methods on a data set with flat spots (data set 1). To evaluate our Wavelet-SVM on non-roundness and shelling as well we use data set 2 to estimate classification performance on all three defect classes. We have proposed two different DNNs for defect detection in Sec. IV-B: the cyclic permutation network (cyclic DNN) and the MIL-DNN. We use the cyclic DNN to predict non-roundness as this is a non-discrete defect type with large-scale effects. We take the maximum probability of defectiveness over multiple inputs. As the region of the wheel that rolls over the first sensor is arbitrary we want to be able to be invariant to a specific way of shifting the sensors. Thanks to the symmetric way and the distances at which the sensors are installed we can look at cyclic shifts of the concatenated signal of all sensors to simulate different scenarios. The DNN trained to learn these cyclic shift invariant features is described in Section IV-B2a. The MIL-DNN is used to predict flat spot on data set 2 as the multiple instance learning setting lends itself nicely to this defect type as explained in Sec. IV-B2b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Set 1: Calibration Run</head><p>To acquire a first training data set for flat spots, two wheels on different wagons were artificially damaged. The wagons were then added to a calibration train that was run over different measurement sites with different velocities and from both directions to calibrate the wheel load check points. This resulted in 1600 measurements, 50% of which are from a wheel with a flat spot.</p><p>We also consider another method to detect flat spots in this data set, that is not based on machine learning. It is a conservative threshold on the dynamic coefficient: a general measure of spread within one time series. For each sensor this coefficient is given by</p><formula xml:id="formula_20">d BW (x) = max(x) x ,<label>(10)</label></formula><p>where max and x refer to the maximum and average value of a sequence of measurements x, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Data Set 2: Reprofile Events</head><p>To generate data for training and testing a classifier that can predict additional types of wheel defects, we aggregated the time and date of reprofile events and linked them to railway wagons. We used two sources for these events: the protocols of repair workshops of freight trains and the regular maintenance measurements of passenger trains. These were annotated with a defect class by an expert before re-profiling the defective wheels. We then categorized measurements of the wheel load checkpoints of the same wagons around the date of re-profiling. Measurements up to a week before re-profiling were considered defective (according to the class label given by the expert), while measurements up to a week after re-profiling were considered defect free. Using this procedure we were able to obtain a large data set of annotated measurements from wheels of different defect classes over the span of multiple years. 1836 measurements are evaluated for flat spot detection, where 588 cases are classified as defective. For shelling, we received 6070 measurements, with 2678 being defective. For the non-roundness defect class, 688 cases out of 920 measurements are defective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. EXPERIMENTAL RESULTS</head><p>For performance evaluation of the methods we compute three metrics: accuracy, precision and recall. Whereas accuracy gives the total fraction of correctly classified wheels, precision measures the fraction of correctly predicted defects out of all predicted defects and recall the fraction of correctly predicted defects out of all defects <ref type="bibr" target="#b30">[31]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model Selection and Evaluation</head><p>For all the experiments in this section the performance shown is computed on a test set that was not used for training or model/parameter selection. To make the evaluation robust against chance we repeat each experiment multiple times on new random train/test splits and report average and standard deviation over these repetitions. For data set 1 we only report the average as the standard deviation was not reported for the benchmark method. For data set 1 50% of the data is hold out for testing, for data set 2 20%. For the Wavelet-SVM the average performance is computed over 10 repetitions, for the DNNs over three repetitions. Using less experiments for the DNNs is due to computational reasons and justified by the low standard deviation over repetitions in all experiments &lt;=2%. For the Wavelet-SVM three-fold cross-validation is performed on the training set to find the optimal hyper-parameters of the SVM and the Gaussian rbf kernel with grid-search on an exponentially spaced grid. For the DNN 10% of the training set were set aside as a validation set to benchmark performance online and decide on when to stop training.</p><p>As the class proportions for data set 2 are not balanced (c.f. Sec VII-C) training and evaluating the classifiers directly on this data would lead to bias and higher classification probability for the over-represented class. It would also make judging accuracy and comparing the methods and data sets hard, as the baseline for random chance would not be 50%. Therefore as a first step in all experiments we re-balance the class proportions of the data sets by randomly over-sampling the smaller class through sampling with replacement. While balanced data sets are useful for comparing methods and data sets, in a real-world setting the true proportion of the classes is important and mistakes for different types of error might have different cost. Therefore we recommend to give class probability estimates for each class when implementing such a system and then adapting a threshold for raising an alarm iteratively based on the test performance of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Set 1</head><p>In a study prior to this publication <ref type="bibr" target="#b26">[27]</ref>, this data set was used to empirically demonstrate the effectiveness of a new algorithm for MIL <ref type="bibr" target="#b25">[26]</ref>. Krummenacher et al. <ref type="bibr" target="#b26">[27]</ref> beat stateof-the art MIL algorithms on this data set and get a classification accuracy of 70% with ellipsoidal multiple instance learning (eMIL). In this study features based on the Global Alignment (GA) kernel for time-series <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> were used.</p><p>Using the features described in Section III with a SVM (Section V) we were able to improve accuracy to 92% (Table <ref type="table" target="#tab_0">I</ref>). With the current operational threshold of θ = 3 on the maximal dynamic coefficient (Eq. 10) an accuracy of 60% is achieved. This is relatively low, as with random guessing already 50% accuracy could be achieved. It is thus important to note that the precision of this method is perfect with 100% of reported wheels being defective. So even though the method misses defective wheels it never raises a false alarm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Data Set 2 -SVM</head><p>Equipped with our general method of constructing features from multiple wheel vertical force measurements (Section III) and learning a classifier from them (Section V) we are now ready to predict other types of wheel defects as well. We also evaluate the DNN based method (Section IV) in this section.</p><p>The SVM classifier (Section V) are trained on the labels obtained by this method for the defect types flat spot, shelling and non-roundness.</p><p>In Table <ref type="table" target="#tab_1">II</ref> the performance on the reserved test set is reported for each defect type including standard deviation over the permutations. The performance on shelling is the best out of the three defect types. This observation can be explained by the fact that the training set for this defect type was by far the largest, so we were able to train a classifier with higher accuracy. This defect type also affects the wheel globally, so it is harder to miss for the sensors than a flat spot. To improve the performance on flat spot and non-roundness we trained custom deep neural networks and give the results in the next section.</p><p>For the defect type non-roundness, the load normalized features based on the load observed by individual sensors (c.f. Section III) substantially contributed to an increase in accuracy. This effect can be explained by the observation that wheel non-roundness errors do not cause a large variation on the within measurement time series since they are a nondiscrete type of wheel defects. They do introduce variations between the different measurements per wheel on the other hand and so features based on averages per measurement sequence are important. We will improve the classification performance for flat spot and non-roundness in the next section by using a custom deep neural network (DNN) that is cyclicshift invariant for classification of these defect types.</p><p>One complication of this data set arises from the lack of knowledge if the wagon passes the wheel load checkpoint with the same orientation as the wheels were annotated in the workshop. This lack of information leads to uncertain labels for the class of defective wheels, as not all wheels on a wagon necessarily share a defect. For the class of nondefective wheels this uncertainty does not pose a problem, since all wheels of a wagon are re-profiled and therefore are non-defective in our data set. We deal with this problem by adding both possible orientations of each wagon to the data set for the defective class of wheels. This augmentation of the data set introduces additional noise to the learning problem during training as non-defective wheels might be labeled defective. Nonetheless, we are able to train classifiers with high accuracy for all three types of defects (flat spot, non-roundness, shelling) based on data generated from this source. Since during testing the same uncertainty exists and actually non-defective wheels might have a defect class assigned the error rate of the classifier appears to be over-reported . Therefore the numbers reported in Table II and in the next section are a lower bound on the performance of the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Data Set 2 -Deep Learning</head><p>Using the same data set as in the previous section we evaluate the deep learning method (Section IV) on the two defect types flat spot and non-roundness. To simplify the experiments we do not include additional features like speed, measurement site or template fit, but only consider the wheel vertical force measurements from the WLC sensors. Therefore, the performance of the SVM is slightly worse compared to the previous section.</p><p>To compute the 2D image of the time series we proceeded as following: first, the recording from each of the 8 channels have been preprocessed via PAA <ref type="bibr" target="#b23">[24]</ref>, with bin number N = 156. The GAF encoding as well as the 2D graph were computed for each channels (we took the following parameters for the 2D graph: V min = -4, V max = 6 as the window captures more than 99.9% of all the values, and r = V max -V min N = 10 N to generate square pictures of size N × N). Finally, the picture size was further reduced by averaging every 2 × 2 nonoverlapping pixels for computational reasons, resulting in 8 channels of size 78 × 78 for both GAF and 2D graph encoding.</p><p>To prevent overfitting to the training set and to enable the model to explore a larger parameter space, we augmented the data by adding Gaussian noise and by randomly shifting and re-scaling the time series before applying image transformations.</p><p>We applied dropout regularization <ref type="bibr" target="#b33">[34]</ref> on all the fully connected layers. To further improve generalization, we added an additional 2 weight regularization penalty term in the cost function ("weight decay") to encourage smooth solutions by favouring small weights. We have employed stochastic gradient descent with Nesterov Momentum <ref type="bibr" target="#b34">[35]</ref> to accelerate convergence.</p><p>The learning rate was set to decay inversely proportional to the number of epochs.</p><p>1) Flat Spots: In Table <ref type="table" target="#tab_2">III</ref> we compare the performance of the different DNN models and the Wavelet-SVM. The only deep model that is able to out-perform the accuracy of the Wavelet-SVM is based on the 2D image of the time series. All of the deep models have smaller standard deviation and higher precision.</p><p>2) Non-Roundness: In Table <ref type="table" target="#tab_3">IV</ref> we compare the performance of the cyclic DNN with the DNN used for flat spot prediction (Deep MIL), a DNN that is trained on the  concatenation of all the sensors (Deep Concat) and the Wavelet-SVM. Remember that the MIL-DNN used for flat spot prediction is trained by looking at the time series of each sensor individually and computing the loss on the sensor with highest probability of observing the defect. The performance of the different methods on the test set shows that MIL is an inadequate model for this type of defect since a wheel with a non-roundness defect can not be reliably identified on the basis of only one sensor measurement. This non-local behavior is in contrast to the challenge of predicting flat spot. Concatenating the sensors as is and not looking at the possible cyclic permutations resulted in training set accuracy similar to the cyclic shift network, but performance on the test set is significantly worse (Table <ref type="table" target="#tab_3">IV</ref>). Intuitively ignoring the permutations leads to over-fitting as the measurements in the test set might be shifted arbitrarily.</p><p>In comparison with the Wavelet-SVM the cyclic DNN shows higher accuracy and precision and reduced variance. Unlike the DNN for flat spot we only trained the cyclic DNN for non-roundness directly on the 1D time series, as the increase in parameters due to the concatenation of measurements of the sensors prohibited efficient training of the model on the 2D representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>We have presented two machine learning methods for defect detection on railway train wheels. The methods analyse multiple time series of the vertical force of a wheel under operational speed and output if a wheel has a defect or not. Both methods are trained automatically on measurements gathered from defective and non-defective wheels. The first method is based on novel general wavelet features for time series. The second method employs deep convolutional neural networks to automatically learn features from the time series directly or from a 2-dimensional representation. We design cyclic shift invariant artificial neural networks for the detection of wheel flats and non-round wheels that model the relationship between the measurements inherent to these defects. To evaluate our methods we collect two data sets from different sources and demonstrate improved performance for predicting flat spot, shelling and non-roundness.</p><p>The methods that were developed for this work are currently being implemented as part of the SBB wayside train monitoring system. To improve the quality of the training and test data RFID tags will be deployed to enable perfect association between defect labels and measurements. Further future work consists of integrating external features into the deep learning models, optimizing for precision and predicting severity scores for the defects. For the prediction of severity scores we obtained promising preliminary results on regressing the flat spot length using support vector regression <ref type="bibr" target="#b35">[36]</ref> and the wavelet features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. Multiple vertical wheel force measurements of a train wheel by the four sensors of one measurement bar. The wheel is affected by a discrete defect that manifests itself in the measurement of the first sensor. The remaining sensors do not directly observe the defect.</figDesc><graphic coords="2,336.95,113.09,51.86,78.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Picture of a serious flat spot on a train wheel of SBB (a) and the resulting idealized wheel load measurement (b). (Picture taken from Wikipedia/Bobo11)</figDesc><graphic coords="3,175.67,78.29,92.78,62.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Signals and wavelet coefficients at different levels (C1 to C3) of a defective (right) and non-defective (left) wheel. The power in the high frequency coefficients C2-C3 reveal the defect.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. of the cyclic permutation network that automatically learns cyclic shift invariant features. The red boxes on the left represent the weight shared CNN, the coloured bars designate features learned by the CNN, the stack of colored bars are permutations of the feature vectors, the blue dots the class log-likelihoods per permutation and the green box the final class probabilities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>6 .</head><label>6</label><figDesc>Structure of the MIL defect detection network for flat spots. The network consists of one CNN per measurement with weights shared across the networks. The defect likelihood of the whole wheel is given by the maximum defect likelihood across sensors. 1) It takes as input the set of classification probabilities of each channel computed independently by the Mono channel feature extracting layer. 2) It combines the multiple log-likelihoods by returning the maximal log-likelihood for the defect class and the minimal log-likelihood for the non-defective class. Given a set of s log-likelihoods for binary classification from s sensors x = (P D i , P N i ) 1≤i≤s , where P D i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Top layer filters (a) and features (b) learned by the 1-dimensional defect detection network for flat spots for a measurement of a defective (right) and non-defective (left) wheel.</figDesc><graphic coords="7,133.79,282.65,80.54,80.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Example of a top layer filter (a) and corresponding features of the signal of a non-defective (b) and defective (flat spot) (c) wheel learned on 2D representations.</figDesc><graphic coords="7,50.03,282.65,80.42,80.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I TEST</head><label>I</label><figDesc>SET PERFORMANCE ON DATA SET 1</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II TEST</head><label>II</label><figDesc>SET PERFORMANCE OF THE WAVELET-SVM ON DATA SET 2</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III TEST</head><label>III</label><figDesc>SET PERFORMANCE OF THE DEEP MODELS ON FLAT SPOTS IN DATA SET 2</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV TEST</head><label>IV</label><figDesc>SET PERFORMANCE OF THE DEEP MODELS ON NON-ROUNDNESS IN DATA SET 2</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Gabriel Krummenacher received the M.Sc. degree in computer science and the Ph.D. degree in machine learning from ETH Zürich in 2011 and 2016, respectively. In 2013, he was an Academic Guest with the NICTA Bioinformatics Group, Melbourne. From 2008 to 2009, he was a Software Engineering Intern with the Trading Technology Team, Axa Rosenberg, San Francisco. His research has been concerned with scalable methods for large-scale and robust learning, wheel defect detection, and sleep stage prediction with deep learning. He is interested in solving complex real-world problems arising from industry or the medical domain through machine learning.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In 2003, he joined ETH Zürich, where he is currently a Professor of Information Science and Engineering with the Computer Science Department. His current research interests cover machine learning, statistical learning theory and its relations to information theory, and applications of machine learning to challenging data analysis questions. The machine learning applications range from image understanding and medical image analysis, to signal processing, bioinformatics and computational biology. Special emphasis is devoted to model selection questions for the analysis of large scale heterogeneous data sets. He has served as an Associate Editor of the IEEE-TNN, the IEEE-TIP, and the IEEE-TPAMI.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Validation of wheel maintenance measures on the rolling stock for reduced excitation of ground vibration</title>
		<author>
			<persName><forename type="first">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Leibundgut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stallaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pesqueux</surname></persName>
		</author>
		<idno>SCP0-GA-2010-265754</idno>
	</analytic>
	<monogr>
		<title level="j">Trafikverket</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<pubPlace>Borlänge, Sweden</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
	<note>RIVAS</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">RIVAS-Mitigation measures on vehicles (WP5); experimental analysis of SBB ground vibration measurements and vehicle data,&quot; in Noise and Vibration Mitigation for Rail Transportation Systems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nélain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Müller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="531" to="538" />
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Future european noise emission ceilings: Threat or solution? A review based on Swiss and dutch ceilings,&quot; in Noise and Vibration Mitigation for Rail Transportation Systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Verheijen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Elbers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="71" to="78" />
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Anomalous tie plate detection for railroad inspection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pankanti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st Int. Conf. Pattern Recognit</title>
		<meeting>21st Int. Conf. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3017" to="3020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rail component detection, optimization, and assessment for automatic rail track inspection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pankanti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="760" to="770" />
			<date type="published" when="2014-04">Apr. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep multitask learning for railway track inspection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1509.05267" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Formalizing expert knowledge through machine learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Idé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Global Perspectives on Service Science</title>
		<imprint>
			<biblScope unit="page" from="157" to="175" />
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sensor system of detecting defects in wheels of railway vehicles running at operational speed</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nenov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dimitrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasilev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Piskulev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 34th Int</title>
		<meeting>34th Int</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="577" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Signature analysis on wheel-rail interaction for rail defect detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th IET Int. Conf. Railway Condition Monitor</title>
		<meeting>4th IET Int. Conf. Railway Condition Monitor</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Application of wavelet transform to defect detection of wheelflats of railway wheels</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jianhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhengding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Int. Conf. Signal Process</title>
		<meeting>6th Int. Conf. Signal ess</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="29" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Wayside train monitoring systems: Networking for greater safety</title>
		<author>
			<persName><forename type="first">W</forename><surname>Badran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Nietlispach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Railway Rev</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="14" to="21" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Out-of-round railway wheels-a literature survey</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Johansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Inst. Mech. Eng., F, J. Rail Rapid Transit</title>
		<imprint>
			<biblScope unit="volume">214</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="79" to="91" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">in Wheel-Rail Interface Handbook</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nielsen</surname></persName>
		</author>
		<editor>Sawston, U.K.</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Woodhead Publishing</publisher>
			<biblScope unit="page" from="245" to="279" />
		</imprint>
	</monogr>
	<note>Out-of-round railway wheels</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<title level="m">A wavelet Tour of Signal Processing</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A review of wavelets in biomedical applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Unser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aldroubi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1996-04">Apr. 1996</date>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="626" to="638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Wavelet analysis for geophysical applications</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Foufoula-Georgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rev. Geophys</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="385" to="412" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The JPEG 2000 still image compression standard</title>
		<author>
			<persName><forename type="first">A</forename><surname>Skodras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Christopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="36" to="58" />
			<date type="published" when="2001-09">Sep. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
		<title level="m">Ten Lectures Wavelets</title>
		<meeting><address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">61</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Vis. Pattern Recognit</title>
		<meeting>Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning: Methods and applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Signal Process</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="197" to="387" />
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Acoustic modeling using deep belief networks</title>
		<author>
			<persName><forename type="first">A.-R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Language Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="22" />
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Encoding time series as images for visual inspection and classification using tiled convolutional neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Oates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 29th AAAI Conf</title>
		<meeting>29th AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imageNet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Solving the multiple instance problem with axis-parallel rectangles</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lozano-Pèrez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="71" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ellipsoidal multiple instance learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Krummenacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 30th Int. Conf. Mach. Learn. (ICML)</title>
		<meeting>30th Int. Conf. Mach. Learn. (ICML)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="73" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optimal gene regulatory network inference using the Boolean Kalman filter and multiple model adaptive estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Imani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Braga-Neto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 49th Asilomar Conf. Signals, Syst. Comput</title>
		<meeting>49th Asilomar Conf. Signals, Syst. Comput</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="423" to="427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Performance enhancement of a multiple model adaptive estimator</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Maybeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Hanlon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Aerosp. Electron. Syst</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1240" to="1254" />
			<date type="published" when="1995-10">Oct. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J V</forename><surname>Rijsbergen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
			<publisher>Butterworth-Heinemann</publisher>
			<pubPlace>Newton, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast global alignment kernels</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="929" to="936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A kernel for time series based on global alignments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Vert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Birkenes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Matsui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="413" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 30th Int. Conf. Mach. Learn. (ICML)</title>
		<meeting>30th Int. Conf. Mach. Learn. (ICML)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A tutorial on support vector regression</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="199" to="222" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
