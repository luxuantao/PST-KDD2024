<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Geometric Scattering for Graph Data Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Feng</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational Math</orgName>
								<orgName type="institution" key="instit1">Sci-ence and Engineering</orgName>
								<orgName type="institution" key="instit2">Michigan State University</orgName>
								<address>
									<settlement>East Lansing</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Plant, Soil &amp; Microbial Sciences</orgName>
								<orgName type="institution">Michi-gan State University</orgName>
								<address>
									<settlement>East Lansing</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guy</forename><surname>Wolf</surname></persName>
							<email>&lt;guy.wolf@umontreal.ca&gt;</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Mathematics and Statistics</orgName>
								<orgName type="institution">Université de Montréal</orgName>
								<address>
									<settlement>Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><surname>Hirn</surname></persName>
							<email>&lt;mhirn@msu.edu&gt;.</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational Math</orgName>
								<orgName type="institution" key="instit1">Sci-ence and Engineering</orgName>
								<orgName type="institution" key="instit2">Michigan State University</orgName>
								<address>
									<settlement>East Lansing</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">Michigan State Uni-versity</orgName>
								<address>
									<settlement>East Lansing</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Matthew Hirn</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Geometric Scattering for Graph Data Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We explore the generalization of scattering transforms from traditional (e.g., image or audio) signals to graph data, analogous to the generalization of ConvNets in geometric deep learning, and the utility of extracted graph features in graph data analysis. In particular, we focus on the capacity of these features to retain informative variability and relations in the data (e.g., between individual graphs, or in aggregate), while relating our construction to previous theoretical results that establish the stability of similar transforms to families of graph deformations. We demonstrate the application of our geometric scattering features in graph classification of social network data, and in data exploration of biochemistry data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Over the past decade, numerous examples have established that deep neural networks (i.e., cascades of linear operations and simple nonlinearities) typically outperform traditional "shallow" models in various modern machine learning applications, especially given the increasing Big Data availability nowadays. Perhaps the most well known example of the advantages of deep networks is in computer vision, where the utilization of 2D convolutions enable network designs that learn cascades of convolutional filters, which have several advantages over fully connected network architectures, both computationally and conceptually. Indeed, in terms of supervised learning, convolutional neural networks (ConvNets) hold the current state of the art in image classification, and have become the standard machine learning approach to-Proceedings of the 36 th International Conference on Machine <ref type="bibr">Learning, Long Beach, California, PMLR 97, 2019.</ref> Copyright 2019 by the author(s).</p><p>wards processing big structured-signal data, including audio and video processing. See, e.g., <ref type="bibr">Goodfellow et al. (2016, Chapter 9</ref>) for a detailed discussion.</p><p>Beyond their performances when applied to specific tasks, pretrained ConvNet layers have been explored as image feature extractors by freezing the first few pretrained convolutional layers and then retraining only the last few layers for specific datasets or applications (e.g., <ref type="bibr" target="#b38">Yosinski et al., 2014;</ref><ref type="bibr" target="#b26">Oquab et al., 2014)</ref>. Such transfer learning approaches provide evidence that suitably constructed deep filter banks should be able to extract task-agnostic semantic information from structured data, and in some sense mimic the operation of human visual and auditory cortices, thus supporting the neural terminology in deep learning. An alternative approach towards such universal feature extraction was presented in <ref type="bibr" target="#b23">Mallat (2012)</ref>, where a deep filter bank, known as the scattering transform, is designed, rather than trained, based on predetermined families of distruptive patterns that should be eliminated to extract informative representations. The scattering transform is constructed as a cascade of linear wavelet transforms and nonlinear complex modulus operations that provides features with guaranteed invariance to a predetermined Lie group of operations such as rotations, translations, or scaling. Further, it also provides Lipschitz stability to small diffeomorphisms of the inputted signal.</p><p>Following recent interest in geometric deep learning approaches for processing graph-structured data (see, for example, <ref type="bibr" target="#b5">Bronstein et al. (2017)</ref> and references therein), several attempts have been made to generalize the scattering transform to graphs <ref type="bibr" target="#b40">(Zou &amp; Lerman, 2018;</ref><ref type="bibr" target="#b15">Gama et al., 2019)</ref> and manifolds <ref type="bibr" target="#b28">(Perlmutter et al., 2018)</ref>, which we will generally term "geometric scattering." These works mostly focus on following the footsteps of <ref type="bibr" target="#b23">Mallat (2012)</ref> in establishing the stability of their respective constructions to deformations of input signals or graphs. Their results essentially characterize the type of disruptive information eliminated by geometric scattering, by providing upper bounds for distances between scattering features, phrased in terms of a deformation size. Here, we further explore the notion of geometric scattering features by considering the complimentary question of how much information is retained by them, since stability alone does not ensure useful features in practice (e.g., a constant all-zero map would be stable to any deformation, but would clearly be useless). In other words, we examine whether a geometric scattering construction, defined and discussed in Sec. 3, can be used as an effective task-independent feature extractor from graphs, and whether the resulting representations provided by them are sufficiently rich to enable intelligible data analysis by applying traditional (Euclidean) methods.</p><p>We note that for Euclidean scattering, while stability is established with rigorous theoretical results, the capacity of scattering features to form an effective data representation in practice has mostly been established via extensive empirical examination. Indeed, scattering features have been shown effective in several audio (e.g., <ref type="bibr" target="#b8">Bruna &amp; Mallat, 2013a;</ref><ref type="bibr" target="#b0">Andén &amp; Mallat, 2014;</ref><ref type="bibr" target="#b22">Lostanlen &amp; Mallat, 2015;</ref><ref type="bibr" target="#b1">Andén et al., 2018)</ref> and image (e.g., <ref type="bibr" target="#b9">Bruna &amp; Mallat, 2013b;</ref><ref type="bibr" target="#b32">Sifre &amp; Mallat, 2014;</ref><ref type="bibr" target="#b27">Oyallon &amp; Mallat, 2015;</ref><ref type="bibr" target="#b2">Angles &amp; Mallat, 2018)</ref> processing applications, and their advantages over learned features are especially relevant in applications with relatively low data availability, such as quantum chemistry and materials science (e.g., <ref type="bibr" target="#b19">Hirn et al., 2017;</ref><ref type="bibr" target="#b13">Eickenberg et al., 2017;</ref><ref type="bibr">2018;</ref><ref type="bibr" target="#b6">Brumwell et al., 2018)</ref>.</p><p>Similarly, our examination of geometric scattering capacity focuses on empirical results on several data analysis tasks, and on two commonly used graph data types. Our results in Sec. 4.1 show that on social network data, geometric scattering features enable classic RBF-kernel SVM to match, if not outperform, leading graph kernel methods as well as most geometric deep learning ones. These experiments are augmented by additional results in Sec. 4.2 that show the geometric scattering SVM classification rate degrades only slightly when trained on far fewer graphs than is traditionally used in graph classification tasks. On biochemistry data, where graphs represent molecular structures of compounds (e.g., Enzymes or proteins), we show in Sec. 4.3 that scattering features enable significant dimensionality reduction. Finally, to establish their descriptive qualities, in Sec. 4.4 we use geometric scattering features extracted from enzyme data <ref type="bibr" target="#b4">(Borgwardt et al., 2005)</ref> to infer emergent patterns of enzyme commission (EC) exchange preferences in enzyme evolution, validated with established knowledge from <ref type="bibr" target="#b12">Cuesta et al. (2015)</ref>. Taken together, these results illustrate the power of the geometric scattering approach as both a relevant mathematical model for geometric deep learning, and as a suitable tool for modern graph data analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Graph Random Walks and Graph Wavelets</head><p>The Euclidean scattering transform is constructed using wavelets defined on R d . In order to extend this construction to graphs, we define graph wavelets as the difference between lazy random walks that have propagated at different time scales, which mimics classical wavelet constructions found in <ref type="bibr" target="#b24">Meyer (1993)</ref> and more recent constructions found in <ref type="bibr" target="#b11">Coifman &amp; Maggioni (2006)</ref>. The underpinnings for this construction arise out of graph signal processing, and in particular the properties of the graph Laplacian.</p><p>Let G = (V, E, W ) be a weighted graph, consisting of</p><formula xml:id="formula_0">n vertices V = {v 1 , . . . , v n }, edges E ⊆ {(v , v m ) : 1 ≤ , m ≤ n}, and weights W = {w(v , v m ) &gt; 0 : (v , v m ) ∈ E}.</formula><p>Note that unweighted graphs are considered as a special case, by setting</p><formula xml:id="formula_1">w(v , v m ) = 1 for each (v , v m ) ∈ E. Define the n × n (weighted) adjacency matrix A G = A of G by A(v , v m ) = w(v , v m ) if (v , v m</formula><p>) ∈ E and zero otherwise, where we use the notation A(v , v m ) to denote the ( , m) entry of the matrix A so as to emphasize the correspondence with the vertices in the graph and to reserve sub-indices for enumerating objects. Define the (weighted) degree of vertex v as</p><formula xml:id="formula_2">deg(v ) = m A(v , v m )</formula><p>and the corresponding diagonal n × n degree matrix D given by</p><formula xml:id="formula_3">D(v , v ) = deg(v ), D(v , v m ) = 0, = m.</formula><p>Finally, the n × n graph Laplacian matrix L G = L on G is defined as L = D − A, and its normalized version is</p><formula xml:id="formula_4">N = D − 1 /2 LD − 1 /2 = I − D − 1 /2 AD − 1 /2</formula><p>. We focus on the latter due to its close relationship with graph random walks.</p><p>The normalized graph Laplacian is a symmetric, real valued positive semi-definite matrix, and thus has n non-negative eigenvalues. Furthermore, if we set 0 = (0, . . . , 0) T to be the n × 1 vector of all zeroes, and d(v ) = deg(v ) to be the n × 1 degree vector, then one has Nd 1 /2 = 0 (where the square root is understood to be taken entrywise). Therefore 0 is an eigenvalue of N and we write the n eigenvalues of</p><formula xml:id="formula_5">N as 0 = λ 0 ≤ λ 1 ≤ • • • ≤ λ n−1 ≤ 2 with corresponding n × 1 orthonormal eigenvectors ϕ 0 , ϕ 1 , . . . , ϕ n−1 . If the graph G is connected, then λ 1 &gt; 0.</formula><p>In order to simplify the following discussion we assume that this is the case, although the discussion below can be amended to include disconnected graphs as well.</p><p>One can show ϕ 0 = d 1 /2 / d 1 /2 , meaning ϕ 0 is nonnegative. Since every other eigenvector is orthogonal to ϕ 0 (and thus must take positive and negative values), it is natural to view the eigenvectors ϕ k as the Fourier modes of the graph G, with a frequency magnitude proportional to λ k . The fact that ϕ 0 is in general non-constant, as opposed to the zero frequency mode on the torus or real line, reflects the non-uniform distribution of vertices in non-regular graphs. Let x : V → R be a signal defined on the vertices of the graph G, which we will consider as an n × 1 vector with entries x(v ). It follows that the Fourier transform of x can be defined as x(k) = x • ϕ k , where x • y is the standard dot product. This analogy is one of the foundations of graph signal processing and indeed we could use this correspondence to define wavelet operators on the graph G, as in <ref type="bibr" target="#b18">Hammond et al. (2011)</ref>. Rather than follow this path, though, we instead take a related path similar to <ref type="bibr" target="#b11">Coifman &amp; Maggioni (2006)</ref> and <ref type="bibr" target="#b15">Gama et al. (2019)</ref> by defining the graph wavelet operators in terms of random walks defined on G, which will avoid diagonalizing N and will allow us to control the "spatial" graph support of the filters directly.</p><p>Define the n × n lazy random walk matrix as P = 1 2 I + AD −1 . Note that the column sums of P are all one. It follows that P acts as a Markov operator, mapping probability distributions to probability distribution. We refer to P as a lazy random walk matrix since P t governs the probability distribution of a lazy random walk after t steps.</p><p>A single realization of a random walk is a walk (in the graph theoretic sense) v 0 , v 1 , v 2 , . . . in which the steps are chosen randomly; lazy random walks allow for v i = v i+1 . More precisely, suppose that µ 0 (v ) ≥ 0 for each vertex v and µ 0 1 = 1, so that µ 0 is a probability distribution on G. We take µ 0 (v ) as the probability of a random walk starting at vertex v 0 = v . One can verify that µ 1 = Pµ 0 is also a probability distribution; each entry µ 1 (v ) gives the probability of the random walk being located at v 1 = v after one step. The probability distribution for the location of the random walk after t steps is µ t = P t µ 0 .</p><p>The operator P can be considered a low pass operator, meaning that Px replaces x(v ) with localized averages of x(v ) for any x. Indeed, expanding out Px(v ) one observes that Px(v ) is the weighted average of x(v ) and the values x(v m ) for the neighbors v m of v . Similarly, the value</p><formula xml:id="formula_6">P t x(v ) is the weighted average of x(v ) with all values x(v m ) such that v m is within t steps of v .</formula><p>Low pass operators defined on Euclidean space retain the low frequencies of a function while suppressing the high frequencies. The random walk matrix P behaves similarly. Indeed, P is diagonalizable with n eigenvectors</p><formula xml:id="formula_7">φ k = D 1 /2 ϕ k and eigenvalues ω k = 1 − λ k/2. Let y x = D − 1 /2</formula><p>x be a density normalized version of x and set x t = P t x; then one can show</p><formula xml:id="formula_8">y xt = y x (0)ϕ 0 + n−1 k=1 ω t k y x (k)ϕ k .<label>(1)</label></formula><p>Thus, since 0 ≤ ω k &lt; 1 for k ≥ 1, the operator P t preserves the zero frequency of x while suppressing the high frequencies, up to a density normalization.</p><p>High frequency responses of x can be recovered in multiple different fashions, but we utilize multiscale wavelet transforms that group the non-zero frequencies of G into approximately dyadic bands. As shown in Mallat (2012, Lemma 2.12), wavelet transforms are provably stable operators in the Euclidean domain, and the proof of <ref type="bibr">Zou &amp; Lerman (2018, Theorem 5.1)</ref> indicates that similar results on graphs may be possible. Furthermore, the multiscale nature of wavelet transforms will allow the resulting geometric scattering transform (Sec. 3) to traverse the entire graph G in one layer, which is valuable for obtaining global descriptions of G. Following <ref type="bibr" target="#b11">Coifman &amp; Maggioni (2006)</ref>, define the n × n wavelet matrix at the scale 2 j as</p><formula xml:id="formula_9">Ψ j = P 2 j−1 − P 2 j = P 2 j−1 (I − P 2 j−1</formula><p>) .</p><p>(2)</p><p>A similar calculation as the one required for (1) shows that Ψ j x partially recovers y x (k) for k ≥ 1. The value Ψ j x(v ) aggregates the signal information x(v m ) from the vertices v m that are within 2 j steps of v , but does not average the information like the operator P 2 j . Instead, it responds to sharp transitions or oscillations of the signal x within the neighborhood of v with radius 2 j (in terms of the graph path distance). The smaller the wavelet scale 2 j , the higher the frequencies Ψ j x recovers in x. The wavelet coefficients up to the scale 2 J are:</p><formula xml:id="formula_10">Ψ (J) x(v ) = [Ψ j x(v ) : 1 ≤ j ≤ J] .</formula><p>(3)</p><p>Figure <ref type="figure" target="#fig_0">1</ref> plots the wavelets on two different graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Geometric Scattering on Graphs</head><p>A geometric wavelet scattering transform follows a similar construction as the (Euclidean) wavelet scattering transform of <ref type="bibr" target="#b23">Mallat (2012)</ref>, but leverages a graph wavelet transform.</p><p>In this paper we utilize the wavelet transform defined in (3) of the previous section, but remark that in principle any graph wavelet transform could be used (see, e.g., <ref type="bibr" target="#b40">Zou &amp; Lerman, 2018)</ref>. In Sec. 3.1 we define the graph scattering transform, in Sec. 3.2 we discuss its relation to other recently proposed graph scattering constructions <ref type="bibr" target="#b15">(Gama et al., 2019;</ref><ref type="bibr" target="#b40">Zou &amp; Lerman, 2018)</ref>, and in Sec. 3.3 we describe several of its desirable properties as compared to other geometric deep learning algorithms on graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Geometric scattering definitions</head><p>Machine learning algorithms that compare and classify graphs must be invariant to graph isomorphism, i.e., reindexations of the vertices and corresponding edges. A common way to obtain invariant graph features is via summation operators, which act on a signal x = x G that can be defined on any graph G, e.g., x(v ) = deg(v ). The geometric scattering transform, which is described in the remainder of this section, follows such an approach.</p><p>The simplest summation operator computes the sum of the responses of the signal x. As described in <ref type="bibr" target="#b35">Verma &amp; Zhang (2018)</ref>, this invariant can be complemented by higher order summary statistics of x, the collection of which are statistical moments, and which are also referred to as "capsules" in that work. For example, the unnormalized q th moments of x yield the following "zero" order scattering moments:</p><formula xml:id="formula_11">Sx(q) = n =1 x(v ) q , 1 ≤ q ≤ Q (4)</formula><p>We can also replace (4) with normalized (i.e., standardized) moments of x, in which case we store its mean (q = 1), variance (q = 2), skew (q = 3), kurtosis (q = 4), and so on.</p><p>In what follows we discuss the unnormalized moments since their presentation is simpler. The invariants Sx(q) do not capture the full variability of x and hence the graph G upon which the signal x is defined. We thus complement these moments with summary statistics derived from the wavelet coefficients of x, which will lead naturally to the graph ConvNet structure of the geometric scattering transform.</p><p>Observe, analogously to the Euclidean setting, that in computing Sx(1), which is the summation of x(v ) over V , we have captured the zero frequency of</p><formula xml:id="formula_12">y x = D − 1 /2 x since n =1 x(v ) = x • 1 = y x • d 1 /2 = d 1 /2 y x (0)</formula><p>. Higher order moments of x can incorporate the full range of frequencies in x, e.g.</p><formula xml:id="formula_13">Sx(2) = n =1 x(v ) 2 = n k=1</formula><p>x(k) 2 , but they are mixed into one invariant coefficient. We can separate and recapture the high frequencies of x by computing its wavelet coefficients Ψ (J) x, which were defined in (3). However, Ψ (J) x is not invariant to permutations of the vertex indices; in fact, it is equivariant. Before summing the individual wavelet coefficient vectors Ψ j x, though, we must first apply a pointwise nonlinearity. Indeed, define 1 = (1, . . . , 1) T to be the n × 1 vector of all ones, and note that P T 1 = 1, meaning that 1 is a left eigenvector of P with eigenvalue 1. It follows that Ψ T j 1 = 0 and thus</p><formula xml:id="formula_14">n =1 Ψ j x(v ) = Ψ j x • 1 = 1 T Ψ j x = 0.</formula><p>We thus apply the absolute value nonlinearity, to obtain nonlinear equivariant coefficients |Ψ (J) x| = {|Ψ j x| : 1 ≤ j ≤ J}. We use absolute value because it is equivariant to vertex permutations, non-expansive, and when combined with traditional wavelet transforms on Euclidean domains, yields a provably stable scattering transform for q = 1. Furthermore, initial theoretical results in <ref type="bibr" target="#b40">Zou &amp; Lerman (2018)</ref> and <ref type="bibr" target="#b15">Gama et al. (2019)</ref> indicate that similar graph based scattering transforms possess certain types of stability properties as well. As in (4), we extract invariant coefficients from |Ψ j x| by computing its moments, which define the first order geometric scattering moments:</p><formula xml:id="formula_15">Sx(j, q) = n =1 |Ψ j x(v )| q , 1 ≤ j ≤ J, 1 ≤ q ≤ Q (5)</formula><p>These first order scattering moments aggregate complimentary multiscale geometric descriptions of G into a collection of invariant multiscale statistics. These invariants give a finer partition of the frequency responses of x. For example, whereas Sx(2) mixed all frequencies of x, we see that Sx(j, 2) only mixes the frequencies of x captured by Ψ j .</p><p>First order geometric scattering moments can be augmented with second order geometric scattering moments by iterating the graph wavelet and absolute value transforms. These moments are defined as:</p><formula xml:id="formula_16">Sx(j, j , q) = n =1 |Ψ j |Ψ j x(v )|| q , 1 ≤ j &lt; j ≤ J 1 ≤ q ≤ Q ,<label>(6)</label></formula><p>which consists of reapplying the wavelet transform operator Ψ (J) to each |Ψ j x| and computing the summary statistics of the magnitudes of the resulting coefficients. The intermediate equivariant coefficients |Ψ j |Ψ j x|| and resulting invariant statistics Sx(j, j , q) couple two scales 2 j and 2 j within the graph G, creating features that bind patterns of smaller subgraphs within G with patterns of larger subgraphs (e.g., circles of friends of individual people with larger community structures in social network graphs). The transform can be iterated additional times, leading to third order features and beyond, and thus has the general structure of a graph ConvNet.</p><p>The collection of graph scattering moments Sx = {Sx(q), Sx(j, q), Sx(j, j , q)} (illustrated in Fig. <ref type="figure" target="#fig_2">2(a</ref>)) provides a rich set of multiscale invariants of the graph G. These can be used in supervised settings as input to graph classification or regression models, or in unsupervised settings to embed graphs into a Euclidean feature space for further exploration, as demonstrated in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Stability and capacity of geometric scattering</head><p>In order to assess the utility of scattering features for representing graphs, two properties have to be considered: stability and capacity. First, the stability property aims to provide an upper bound on distances between similar graphs that only differ by types of deformations that can be treated as noise. This property has been the focus of both Zou &amp; Lerman (2018) and <ref type="bibr" target="#b15">Gama et al. (2019)</ref>, and in particular the latter shows that a diffusion scattering transform yields features that are stable to graph structure deformations whose size can be computed via the diffusion framework <ref type="bibr" target="#b11">(Coifman &amp; Maggioni, 2006</ref>) that forms the basis for their construction. While there are some technical differences between the geometric scattering here and the diffusion scattering </p><formula xml:id="formula_17">. . . q q P 2 j−1 I − P 2 j−1 | . . . | . . . q q P 2 j−1 I − P 2 j−1 | . . . | P 2 j −1 I − P 2 j −1 | . . . | . . . q q Sx Ψj Ψj 1≤q≤Q</formula><p>(a) Representative zeroth-, first-, and second-order cascades of the geometric scattering transform for an input graph signal x.</p><formula xml:id="formula_18">G = (V, E, W ) x : V → R</formula><p>A dj ac en cy m at ri x:</p><formula xml:id="formula_19">A (v i , v j )</formula><p>Sig na l ve cto r:</p><p>x( v i )</p><p>Diffusion wavelets:</p><formula xml:id="formula_20">Ψj = P 2 j−1 − P 2 j P = 1 2 (I + AD −1 ) Ψj Scattering (a)</formula><p>x → Sx Traditional Euclidean algorithms (e.g., SVM/PCA) (b) Architecture for using geometric scattering of graph G and signal x in graph data analysis, as demonstrated in Sec. 4. in <ref type="bibr" target="#b15">Gama et al. (2019)</ref>, these constructions are sufficiently similar that we can expect both of them to have analogous stability properties. Therefore, we mainly focus here on the complementary property of the scattering transform capacity to provide a rich feature space for representing graph data without eliminating informative variance in them.</p><p>We note that even in the classical Euclidean case, while the stability of scattering transforms to deformations can be established analytically <ref type="bibr" target="#b23">(Mallat, 2012)</ref>, their capacity is typically examined by empirical evidence when applied to machine learning tasks (e.g., <ref type="bibr" target="#b7">Bruna &amp; Mallat, 2011;</ref><ref type="bibr" target="#b31">Sifre &amp; Mallat, 2012;</ref><ref type="bibr" target="#b0">Andén &amp; Mallat, 2014)</ref>. Similarly, in the graph processing settings, we examine the capacity of our proposed geometric scattering features via their discriminative power in graph data analysis tasks, which are described in detail in Sec. 4. We show that geometric scattering enables graph embedding in a relatively low dimensional Euclidean space, while preserving insightful properties in the data. Beyond establishing the capacity of our specific construction, these results also indicate the viability of graph scattering transforms as universal feature extractors on graph data, and complement the stability results established in <ref type="bibr" target="#b40">Zou &amp; Lerman (2018)</ref> and <ref type="bibr" target="#b15">Gama et al. (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Geometric scattering compared to other feed forward graph ConvNets</head><p>We  <ref type="bibr" target="#b16">Gatys et al., 2015)</ref>.</p><p>A graph wavelet transform Ψ (J) x decomposes the geometry of G through the lens of x, along different scales.</p><p>Graph ConvNet algorithms also obtain multiscale representations of G, but several works, including <ref type="bibr" target="#b3">Atwood &amp; Towsley (2016)</ref> and <ref type="bibr" target="#b39">Zhang et al. (2018)</ref>, propagate information via a random walk. While random walk operators like P t act at different scales on the graph G, per the analysis in Sec. 2 we see that P t for any t will be dominated by the low frequency responses of x. While subsequent nonlinearities may be able to recover this high frequency information, the resulting transform will most likely be unstable due to the suppression and then attempted recovery of the high frequency content of x. Alternatively, features derived from P t x may lose the high frequency responses of x, which are useful in distinguishing similar graphs. The graph wavelet coefficients Ψ (J) x, on the other hand, respond most strongly within bands of nearly non-overlapping frequencies, each with a center frequency k j that depends on Ψ j .</p><p>Finally, graph labels are often complex functions of both local and global subgraph structure within G. While graph ConvNets are adept at learning local structure within G, as detailed in <ref type="bibr" target="#b35">Verma &amp; Zhang (2018)</ref> they require many layers to obtain features that aggregate macroscopic patterns in the graph. This is due to the use of fixed size filters, which often only incorporate information from the neighbors of a vertex.</p><p>The training of such networks is difficult due to the limited size of many graph classification databases (see the supplementary information). Geometric scattering transforms have two advantages in this regard: (a) the wavelet filters are designed; and (b) they are multiscale, thus incorporating macroscopic graph patterns in every layer/order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Application &amp; Results</head><p>To establish the geometric scattering features as an effective graph representation for data analysis, we examine their performance here in four graph data analysis applications. machine learning algorithms, such as SVM for classification or PCA for dimensionality reduction, to perform downstream analysis. Our results show that our scattering features provide simplified representation (e.g., in dimensionality and extrapolation ability) of input graphs, which we conjecture is a result of their stability properties, while also being sufficiently rich to capture meaningful relations between graphs for predictive and descriptive purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Graph classification on social networks</head><p>As a first application of geometric scattering, we apply it to graph classification of social network data taken from <ref type="bibr" target="#b37">Yanardag &amp; Vishwanathan (2015)</ref>. In particular, this work introduced six social network data sets extracted from scientific collaborations (COLLAB), movie collaborations (IMDB-B &amp; IMDB-M), and Reddit discussion threads (REDDIT-B, REDDIT-5K, REDDIT-12K). There are also biochemistry data sets often used in the graph classification literature; for completeness, we include in the supplemental materials further results on these data sets. A brief description of each data set can also be found in the supplement.</p><p>The social network data provided by <ref type="bibr" target="#b37">Yanardag &amp; Vishwanathan (2015)</ref> contains graph structures but no associated graph signals. Therefore we compute the eccentricity (for connected graphs) and clustering coefficient of each vertex, and use these as input signals to the geometric scattering transform. In principle, any general node characteristic could be used, although we remark that x = d, the vertex degree vector, is not useful in our construction since Ψ j d = 0. After computing the scattering moments<ref type="foot" target="#foot_0">1</ref> of We evaluate the classification results of our SVM-based geometric scattering classification (GS-SVM) using ten-fold cross validation (explained in the supplement), which is standard practice in other graph classification works. We compare our results to 10 prominent methods that report results for most, if not all, of the considered datasets. Out of these, four are graph kernel methods: Weisfeiler-Lehman graph kernels (WL, <ref type="bibr" target="#b30">Shervashidze et al., 2011)</ref>, Graphlet kernels <ref type="bibr" target="#b29">(Shervashidze et al., 2009)</ref> Following the standard format of reported classification performances for these methods (per their respective references, see also the supplement), our results are reported in the form of average accuracy ± standard deviation (in percentages) over the ten cross-validation folds. We note that since some methods are not reported for all datasets, we mark N/A when appropriate.  <ref type="table" target="#tab_4">1</ref> one can see that the GS-SVM classifier matches or outperforms all but the two most recent methods, i.e., S2S-N2N-PP <ref type="bibr" target="#b33">(Taheri et al., 2018)</ref> and GIN <ref type="bibr" target="#b36">(Xu et al., 2019)</ref>. With regards to these two approaches, the GS-SVM outperforms S2S-N2N-PP <ref type="bibr" target="#b33">(Taheri et al., 2018</ref>) on 3 /6 datasets. Finally, while GIN <ref type="bibr" target="#b36">(Xu et al., 2019)</ref> outperforms geometric scattering on 5 /6 datasets, the results on since they perform slightly better than the un-normalized moments. Also we use J = 5 and q = 4 for all scattering feature generations. We performed graph classification under four training/validation/test splits: 80%/10%/10%, 70%/10%/20%, 40%/10%/50% and 20%/10%/70%. We did 10-fold, 5-fold and 2-fold cross validation for the first three splits. For the last split, we randomly formed a 10 folds pool, from which we randomly selected 3 folds for training/validation and repeated this process ten times. Detailed classification results can be found in the supplement. Following Sec. we discuss the classification accuracy on six social datasets under these splits. When the training data is reduced from 90% to 80%, the classification accuracy in fact increased by 0.047%, which shows the GS-SVM classification accuracy is not affected by the decrease in training size. Further reducing the training size to 50% results in an average decrease of classification accuracy of 1.40% while from 90% to 20% causes an average decrease of 3.00%. Fig. <ref type="figure" target="#fig_3">3</ref> gives a more nuanced statistical description of these results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Dimensionality reduction</head><p>We now consider the viability of scattering-based embedding for dimensionality reduction of graph data. As a representative example, we consider here the ENZYMES dataset introduced in <ref type="bibr" target="#b4">Borgwardt et al. (2005)</ref>, which contains 600 enzymes evenly split into six enzyme classes (i.e., 100 enzymes from each class). While the Euclidean notion of dimensionality is not naturally available in graph data, we note that graphs in this dataset have, on average, 124.2 edges, 29.8 vertices, and 3 features per vertex. Therefore, the data here can be considered significantly high dimensional in its original representation, which is not amenable to traditional dimensionality reduction techniques.</p><p>To perform scattering-based dimensionality reduction, we applied PCA to geometric scattering features extracted from input enzyme graphs in the data, while choosing the number of principal components to capture 99%, 90%, 80% and 50% explained variance. For each of these thresholds, we computed the mean classification accuracy (with ten-fold cross validation) of SVM applied to the GS-PCA low dimensional space, as well as the dimensionality of this space. The relation between dimensionality, explained variance, and SVM accuracy is shown in Fig. <ref type="figure" target="#fig_3">3</ref>, where we can observe that indeed geometric scattering combined with PCA enables significant dimensionality reduction (e.g., to R 16 with 90% exp. variance) with only a small impact on classification accuracy. Finally, we also consider the PCA dimensionality of each individual enzyme class in the data (in the scattering feature space), as we expect scattering to reduce the variability in each class w.r.t. the full feature space. Indeed, in this case, individual classes have 90% exp. variance PCA dimensionality ranging between 6 and 10, which is significantly lower than the 16 dimensions of the entire PCA space. We note that similar results can also be observed for the social network data discussed in previous sections, where on average 90% explained variances are captured by nine dimensions, yielding a drop of 3.81% in mean SVM accuracy; see the supplement for complete results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Data exploration: Enzyme class exchange preferences</head><p>Geometric scattering essentially provides a task independent representation of graphs in a Euclidean feature space. Therefore, it is not limited to supervised learning applications, and can be also utilized for exploratory graph-data analysis, as we demonstrate in this section. We focus our discussion in particular on the ENZYMES dataset described in the previous section. Here, geometric scattering features can be considered as providing "signature" vectors for individual enzymes, which can be used to explore interactions between the six top level enzyme classes, labeled by their Enzyme Commission (EC) numbers <ref type="bibr" target="#b4">(Borgwardt et al., 2005)</ref>. In order to emphasize the properties of scattering-based feature extraction, rather than downstream processing, we mostly limit our analysis of the scattering feature space to linear operations such as principal component analysis (PCA).</p><p>To explore the scattering feature space, and the richness of information captured by it, we use it to infer relations between EC classes. First, for each enzyme e, with scattering feature vector v e (i.e., with Sx for all vertex features x), we compute its distance from class EC-j, with PCA subspace C j , as the projection distance: dist(e, EC-j) = v e − proj Sj v e Then, for each enzyme class EC-i, we compute the mean distance of enzymes in it from the subspace of each EC-j class as D(i, j) = mean{dist(e, EC-j) : e ∈ EC-i}.  These distances are summarized in the supplement, as well as the proportion of points from each class that have their true EC as their nearest (or second nearest) subspace in the scattering feature space. In general, 48% of enzymes select their true EC as the nearest subspace (with additional 19% as second nearest), but these proportions vary between individual EC classes. Finally, we use these scattering-based distances to infer EC exchange preferences during enzyme evolution, which are presented in Fig. <ref type="figure" target="#fig_5">4</ref> and validated with respect to established preferences observed and reported in <ref type="bibr" target="#b12">Cuesta et al. (2015)</ref>. We note that the result there is observed independently from the ENZYMES dataset. In particular, the portion of enzymes considered from each EC is different between these data, since <ref type="bibr" target="#b4">Borgwardt et al. (2005)</ref> took special care to ensure each EC class in ENZYMES has exactly 100 enzymes in it. However, we notice that in fact the portion of enzymes (in each EC) that choose the wrong EC as their nearest subspace, which can be considered as EC "incoherence" in the scattering feature space, correlates well with the proportion of evolutionary exchanges generally observed for each EC in <ref type="bibr" target="#b12">Cuesta et al. (2015)</ref>, and therefore we use these as EC weights (see Fig. <ref type="figure" target="#fig_5">4</ref>). Our results in Fig. <ref type="figure" target="#fig_5">4</ref> demonstrate that scattering features are sufficiently rich to capture relations between enzyme classes, and indicate that geometric scattering has the capacity to uncover descriptive and exploratory insights in graph data analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented the geometric scattering transform as a deep filter bank for feature extraction on graphs, which generalizes the Euclidean scattering transform. A reasonable criticism of the scattering theory approach to understanding geometric deep learning is that it is not clear if the scattering model is a suitable facsimile for powerful graph neural networks that are obtaining impressive results on graph classification tasks and related graph data analysis problems. In this paper we showed that in fact, at least empirically, this line of criticism is unfounded and indeed further theoretical study of geometric scattering transforms on graphs is warranted. Our evaluation results on graph classification and data exploration show the potential of the produced scattering features to serve as universal representations of graphs. Indeed, classification using these features with relatively simple classifier models, dimension reduced feature sets, and small training sets nevertheless reach high accuracy results on most commonly used graph classification datasets. Finally, the geometric scattering features provide a new way for computing and considering global graph representations, independent of specific learning tasks. They raise the possibility of embedding entire graphs in Euclidean space and computing meaningful distances between graphs, which can be used for both supervised and unsupervised learning, as well as exploratory analysis of graph-structured data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Wavelets Ψj for increasing scale 2 j left to right, applied to Diracs centered at two different locations (marked by red circles) in two different graphs. Vertex colors indicate wavelet values (corresponding to colorbars for each plot), ranging from yellow/green indicating positive values to blue indicating negative values. Both graphs are freely available from PyGSP (2018).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Illustration of (a) the proposed scattering feature extraction (see eqs. 4, 5, and 6), and (b) its application for graph data analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. (a) Box plot showing the drop in SVM classification accuracy over social graph datasets when reducing training set size (horizontal axis marks portion of data used for testing); (b) Relation between explained variance, SVM classification accuracy, and PCA dimensions over scattering features in ENZYMES dataset.</figDesc><graphic url="image-5.png" coords="7,44.51,512.52,251.54,188.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Comparison of EC exchange preferences in enzyme evolution: (a) observed in Cuesta et al. (2015), and (b) inferred from scattering features via pref(EC-i, EC-j) := wj • min D(i,j) D(i,i) , D(j,i) D(j,j)</figDesc><graphic url="image-6.png" coords="8,55.44,503.90,114.66,101.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>, deep graph kernels (DGK,<ref type="bibr" target="#b37">Yanardag &amp; Vishwanathan, 2015)</ref>, and Weisfeiler-Lehman optimal assignment kernels (WL-OA,<ref type="bibr" target="#b21">Kriege et al., 2016)</ref>.</figDesc><table><row><cell>The other six are recent geometric deep learning algorithms:</cell></row><row><cell>deep graph convolutional neural network (DGCNN, Zhang</cell></row><row><cell>et al., 2018), 2D convolutional neural networks (2DCNN,</cell></row></table><note><ref type="bibr" target="#b34">Tixier et al., 2017)</ref>,Patchy-san (PSCN, Niepert et al., 2016, with k = 10), graph capsule convolutional neural networks (GCAPS-CNN,<ref type="bibr" target="#b35">Verma &amp; Zhang, 2018)</ref>, recurrent neural network autoencoders (S2S-N2N-PP,<ref type="bibr" target="#b33">Taheri et al., 2018)</ref>, and the graph isomorphism network (GIN,<ref type="bibr" target="#b36">Xu et al., 2019)</ref>.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Table 1 reports the results. The geometric scattering transform and related variants presented in Zou &amp; Lerman (2018) and Gama et al. (2019) is a mathematical model for graph ConvNets. However, it is natural to ask if this model accurately reflects what is done in practice. A useful model may not obtain state of the art performance, but should be competitive with the current state of the art, lest the model may not capture the underlying complexity of the most powerful methods. Examining Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>Comparison of the proposed GS-SVM classifier with leading graph kernel and deep learning methods on social graph datasets.</figDesc><table><row><cell></cell><cell>COLLAB</cell><cell>IMDB-B</cell><cell>IMDB-M</cell><cell>REDDIT-B</cell><cell>REDDIT-5K</cell><cell>REDDIT-12K</cell><cell></cell></row><row><cell>WL Graphlet WL-OA DGK</cell><cell>77.82 ± 1.45 73.42 ± 2.43 80.70 ± 0.10 73.00 ± 0.20</cell><cell>71.60 ± 5.16 65.40 ± 5.95 N/A 66.90 ± 0.50</cell><cell>N/A N/A N/A 44.50 ± 0.50</cell><cell>78.52 ± 2.01 77.26 ± 2.34 89.30 ± 0.30 78.00 ± 0.30</cell><cell>50.77 ± 2.02 39.75 ± 1.36 N/A 41.20 ± 0.10</cell><cell>34.57 ± 1.32 25.98 ± 1.29 N/A 32.20 ± 0.10</cell><cell>Graph kernel</cell></row><row><cell>DGCNN</cell><cell>73.76 ± 0.49</cell><cell>70.03 ± 0.86</cell><cell>47.83 ± 0.85</cell><cell>N/A</cell><cell>48.70 ± 4.54</cell><cell>N/A</cell><cell></cell></row><row><cell>2D CNN PSCN (k = 10) GCAPS-CNN S2S-P2P-NN</cell><cell>71.33 ± 1.96 72.60 ± 2.15 77.71 ± 2.51 81.75 ± 0.80</cell><cell>70.40 ± 3.85 71.00 ± 2.29 71.69 ± 3.40 73.80 ± 0.70</cell><cell>N/A 45.23 ± 2.84 48.50 ± 4.10 51.19 ± 0.50</cell><cell>89.12 ± 1.70 86.30 ± 1.58 87.61 ± 2.51 86.50 ± 0.80</cell><cell>52.21 ± 2.44 49.10 ± 0.70 50.10 ± 1.72 52.28 ± 0.50</cell><cell>48.13 ± 1.47 41.32 ± 0.42 N/A 42.47 ± 0.10</cell><cell>Deep learning</cell></row><row><cell>GIN-0 (MLP-SUM)</cell><cell>80.20 ± 1.90</cell><cell>75.10 ± 5.10</cell><cell>52.30 ± 2.80</cell><cell>92.40 ± 2.50</cell><cell>57.50 ± 1.50</cell><cell>N/A</cell><cell></cell></row><row><cell>GS-SVM</cell><cell>79.94 ± 1.61</cell><cell>71.20 ± 3.25</cell><cell>48.73 ± 2.32</cell><cell>89.65 ± 1.94</cell><cell>53.33 ± 1.37</cell><cell>45.23 ± 1.25</cell><cell></cell></row><row><cell cols="4">COLLAB and IMDB-B are not statistically significant, and</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">on the REDDIT datasets the geometric scattering approach</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">trails only GIN (Xu et al., 2019). We thus conclude that the</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">geometric scattering transform yields a rich set of invariant</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">statistical moments, which have nearly the same capacity as</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">the current state of the art in graph neural networks.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">4.2. Classification with low training-data availability</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Many modern deep learning methods require large amounts</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">of training data to generate representative features. On</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">the contrary, geometric scattering features are based on</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">each graph without any training processes. In this section,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">we demonstrate the performance of the GS-SVM under</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">low training-data availability and show that the scattering</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">features can embed enough graph information that even</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">under extreme conditions (e.g. only 20% training data), they</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">can still maintain relatively good classification results.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We use the normalized scattering moments for classification, these two input signals, they are concatenated to form a single vector. This scattering feature vector is a consistent Euclidean representation of the graph, which is independent of the original graph sizes (i.e., number of vertices or edges), and thus we can apply any traditional classifier to it. In particular, we use here the standard SVM classifier with an RBF kernel, which is popular and effective in many applications and also performs well in this case.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank the anonymous reviewers for their helpful comments, and Michael Perlmutter for developing related theory (i.e., geometric scattering on manifolds) leading to this work. This research was partially funded by: grant P42 ES004911 through the National Institute of Environmental Health Sciences of the NIH, supporting F.G.; IVADO (l'institut de valorisation des données) [G.W.]; the Alfred P. Sloan Fellowship (grant FG-2016-6607), the DARPA Young Faculty Award (grant D16AP00117), and NSF grant 1620216 [M.H.].</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep scattering spectrum</title>
		<author>
			<persName><forename type="first">J</forename><surname>Andén</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="4114" to="4128" />
			<date type="published" when="2014-08">August 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Andén</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lostanlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.08869</idno>
		<title level="m">Classification with joint time-frequency scattering</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generative networks as inverse problems with scattering transforms</title>
		<author>
			<persName><forename type="first">T</forename><surname>Angles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1993" to="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schönauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Geometric deep learning: Going beyond Euclidean data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Steerable wavelet scattering for 3D atomic systems with application to Li-Si energy prediction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Brumwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hirn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02320</idno>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop on Machine Learning for Molecules and Materials</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Classification with scattering operators</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1561" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Audio texture synthesis with scattering moments</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.0407</idno>
		<imprint>
			<date type="published" when="2013">2013a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Invariant scattering convolution networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1872" to="1886" />
			<date type="published" when="2013-08">August 2013b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02013</idno>
		<title level="m">Multiscale sparse microcanonical models</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Diffusion wavelets. Applied and Computational Harmonic Analysis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maggioni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="53" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The classification and evolution of enzyme function</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Cuesta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Furnham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Thornton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biophysical Journal</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1082" to="1086" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Solid harmonic wavelet scattering: Predicting quantum molecular energy from invariant descriptors of 3D electronic densities</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eickenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Exarchakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hirn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6540" to="6549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Solid harmonic wavelet scattering for predictions of molecule properties</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eickenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Exarchakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hirn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Thiry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page">241732</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Diffusion scattering transforms on graphs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08829</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Texture synthesis using convolutional neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Wavelets on graphs via spectral graph theory</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="129" to="150" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Wavelet scattering regression of quantum chemical energies. Multiscale Modeling and Simulation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hirn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Poilvert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.04654</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="827" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Covariant compositional networks for learning graphs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Trivedi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02144</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On valid optimal assignment kernels and applications to graph classification</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-L</forename><surname>Giscard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Wavelet scattering on the pitch spiral</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lostanlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Digital Audio Effects</title>
				<meeting>the 18th International Conference on Digital Audio Effects</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="429" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Group invariant scattering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1331" to="1398" />
			<date type="published" when="2012-10">October 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><surname>Wavelets</surname></persName>
		</author>
		<author>
			<persName><surname>Operators</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1717" to="1724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep roto-translation scattering for object classification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Oyallon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.8659</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Geometric scattering on manifolds</title>
		<author>
			<persName><forename type="first">M</forename><surname>Perlmutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hirn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.06968</idno>
		<ptr target="https://pygsp.readthedocs.io/en/stable/index.html" />
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop on Integration of Deep Learning Theories</title>
				<imprint>
			<date type="published" when="2018-09">2018. September 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Artificial Intelligence and Statistics</title>
				<editor>
			<persName><forename type="first">D</forename><surname>Van Dyk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting>the 12th International Conference on Artificial Intelligence and Statistics<address><addrLine>Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Weisfeiler-Lehman graph kernels</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J V</forename><surname>Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011-09">Sep. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Combined scattering for rotation invariant texture analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ESANN 2012 conference</title>
				<meeting>the ESANN 2012 conference</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Rigid-motion scattering for texture classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1403.1687</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning graph representations with recurrent neural network autoencoders</title>
		<author>
			<persName><forename type="first">A</forename><surname>Taheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berger-Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD Deep Learning Day</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Classifying graphs as images with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Tixier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.-P</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vazirgiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02218</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graph capsule convolutional neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08090</idno>
	</analytic>
	<monogr>
		<title level="m">Joint ICML and IJCAI Workshop on Computational Biology</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An endto-end deep learning architecture for graph classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4438" to="4445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Graph convolutional neural networks via scattering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lerman</surname></persName>
		</author>
		<idno>arXiv:1804:00099</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
