<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C1B6502FC49EFF75C7A675E0912C7C77</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TSG.2019.2892595, IEEE Transactions on Smart Grid</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Energy theft detection</term>
					<term>machine learning</term>
					<term>gradient boosting</term>
					<term>XGBoost</term>
					<term>CatBoost</term>
					<term>LightGBM</term>
					<term>SVM</term>
					<term>feature engineering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For the smart grid energy theft identification, this letter introduces a gradient boosting theft detector (GBTD) based on the three latest gradient boosting classifiers (GBCs): extreme gradient boosting (XGBoost), categorical boosting (CatBoost), and light gradient boosting method (LightGBM). While most of existing ML algorithms just focus on fine the hyperparameters of the classifiers, our ML algorithm, GBTD, focuses on the feature engineering-based preprocessing to improve detection performance as well as time-complexity. GBTD improves both detection rate (DR) and false positive rate (FPR) of those GBCs by generating stochastic features like standard deviation, mean, minimum, and maximum value of daily electricity usage. GBTD also reduces the classifier complexity with weighted feature-importance (WFI) based extraction techniques. Emphasis been laid upon the practical application of the proposed ML for theft detection by minimizing FPR and reducing data storage space and improving time-complexity of the GBTD classifiers. Additionally, this letter proposes an updated version of the existing six theft cases to mimic real world theft patterns and applies them to the dataset for numerical evaluation of the algorithm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The objective of electricity theft detection is to detect unusual activities in the electricity usage of a smart grid (SG) meter (or simply smart meter). Theft can be detected by checking for abnormalities in the user's electricity consumption patterns. Analyzing user behavior from historical data is the fundamental basis of a data science approach like machine learning (ML). In this letter, we implement a supervised ML-based theft detection model that identifies whether an abnormal/fraudulent usage pattern has occurred in the SG. Several studies have investigated different ML approaches that develop a computational model identifying the incidence of theft. Well-known such techniques in the literature include artificial neural networks, autoregressive integrated moving average (ARIMA) time series approaches, and support vector machines (SVM) (as seen in <ref type="bibr" target="#b0">[1]</ref>). Very recently, the authors of <ref type="bibr" target="#b1">[2]</ref> have shown the superiority of XGBoost, a gradient boosting classifier (GBC), over other ML algorithms for nontechnical loss (NTL) detection.</p><p>In this letter, we aim to provide a thorough comparison of the three latest GBCs including extreme gradient boosting (XGBoost), categorical boosting (CatBoost), and light gradient boosting method (LightGBM), and to propose a gradient boosting theft detector (GBTD) based on these GBCs that has a feature engineering-based preprocessing module to improve detection rate (DR), false positive rate (FPR), and time-complexity. In the GBTD classifiers, the preprocessing module has a stochastic feature generating function which improves FPR as well as DR by utilizing combinations of daily electricity usage values as features. The preprocessing module is also equipped with feature extraction function using weighted featureimportance (WFI) that greatly reduces the training time-complexity by discarding irrelevant features (noise) from the customer's dataset. This also helps in reduced storage space usage for customer data in SG.</p><p>For a given customer in SG, we can easily obtain his historical realusage data. However, the theft samples may rarely exist or be completely absent. We compensate the lack of theft cases in our original dataset by manipulating the real usage of the customer based on mathematical formulae. In <ref type="bibr" target="#b0">[1]</ref>, the authors propose the mathematical theft formulae keeping in mind that "the goal of theft is to report a lower consumption than the true amount used by consumer, or to shift the high usage to low-tariff periods". We agree with those points and propose a revision of the six theft cases <ref type="bibr" target="#b0">[1]</ref> to closely mimic real-world theft patterns with discontinuous reporting and apply these to the dataset for practical evaluation and numerical comparison.</p><p>In contrast of <ref type="bibr" target="#b1">[2]</ref> focusing on the detection of (mostly nonintentional) fraud and non-fraud anomalies, this letter focuses solely on (completely intentional) fraud/theft detection. Also, while the NTL samples in the paper <ref type="bibr" target="#b1">[2]</ref> were just 5.38 to 8.37 % of the dataset, while our dataset contains 50% NTL samples that are generated at the basis of an already studied dataset of <ref type="bibr" target="#b0">[1]</ref>. Hence the proposed approach allows fair and reliable comparison of the classifiers in terms of detection performance.</p><p>In Section II, we introduced the proposed GBTD algorithm using trees-based gradient boosting classifiers (GBCs) and its advantages, continued to improve the system performance in Section III by tweaking the training dataset and using feature engineering techniques, and finally addressed our conclusion in Section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROPOSED ALGORITHM AND ITS ADVANTAGES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Preliminary introduction of proposed methodologies</head><p>Our approach was inspired by existing research <ref type="bibr" target="#b0">[1]</ref> on AMI theft detection via user pattern recognition, which uses a consumption pattern-based electricity theft detector (CPBETD) as the main classifier algorithm. The CPBETD algorithm is based on SVM, which is a powerful classifier with the "rbf" kernel. In our algorithm, we chose GBCs as the energy theft classifiers. The tree ensemble model is a set of classification and regression trees (CART) and it is the basis of the GBCs. It is called gradient boosting because it uses a gradient descent algorithm to minimize loss when adding new trees. This approach supports both regression and classification predictive Korea, South Korea (email: rajiv.punmiya@gmail.com and schoe@catholic.ac.kr).</p><p>Energy theft detection using gradient boosting theft detector with feature engineering-based preprocessing modeling problems. XGBoost, CatBoost and LightGBM are more regularized and improved versions of GBCs. XGBoost's <ref type="bibr" target="#b2">[3]</ref> objective function is the sum of a specific loss function evaluated over all predictions and the sum of a regularization term for all predictors (K trees) based on pre-sort-based algorithm. On the other hand, CatBoost uses a special type of depth-first expansion called oblivious trees. The key idea is that it uses a vectorized representation of the tree (binarized splitting criteria within each level), which can be evaluated faster <ref type="bibr" target="#b3">[4]</ref>. Finally, LightGBM uses histogram-based algorithms <ref type="bibr" target="#b4">[5]</ref>, which bucket continuous feature (attribute) values into discrete bins. This speeds up training and reduces memory usage. Also, each of the GBTD classifiers is equipped with a "feature_importance" module which helps us in improving the classification. However, the SVM classifier used in in CPBETD does not include feature importance module. Hence, the GBTD classifiers allowing WFI-based preprocessing (see the next section) improve detection performance further compared to SVM and it is another novel implementation in our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proposed algorithm and numerical evaluation using old theft cases</head><p>Hereinafter, for fair comparison with other methods, we used the same Irish smart energy dataset (SEAI <ref type="bibr" target="#b5">[6]</ref>). Classifiers consists of XGBoost, CatBoost and LightGBM (collectively referred as gradient boosting classifiers (GBCs)) and the proposed method uses the following procedure for our simulations: 1) The dataset <ref type="bibr" target="#b5">[6]</ref> contains half-hourly reported usage of each customer (in kWh) for approximately 420 days, of which 361 and 59 days were used in the training set and testing set, respectively. That means we have a vector of 48 features to describe the customer's daily usage. Figure <ref type="figure">1</ref> shows an example of the typical daily usage of a customer (t being feature (or sample) index).</p><p>Fig. <ref type="figure">1</ref> Example of a single day usage (sampling interval of 30 min)</p><p>2) We used the same existing (old) six theft cases from reference <ref type="bibr" target="#b0">[1]</ref> to generate our malicious (or majority class) samples.</p><p>3) The minority class, i.e., benign class, was oversampled to balance out the six theft cases generated for each day using the synthetic minority oversampling technique (SMOT). 4) Then, we trained the classifiers with the training set and compared the results with the test set (for a single customer as well as an average of 5000 customers). In Table <ref type="table" target="#tab_1">1</ref>, when assuming old theft cases, we compared the average classification performance of 5000 customers between the different classifiers, i.e., CPBETD and proposed boosting algorithms (GBTD), in terms of detection rate (DR) and false positive rate (FPR). It shows that GBTD is superior to the existing CPBETD, mainly in terms of FPR (11% for CPBETD vs 7 to 8% for GBTD). In terms of commercial SG theft detection, due to huge number of customers, a lower FPR is highly preferable. Hence, we established that GBTD classifiers (i.e., XGBoost, CatBoost, LightGBM) are better than CPBETD classifier (i.e., SVM). In the next section, we discuss new theft cases and data preprocessing to further improve the GBTD classifiers' performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. INTRODUCTION OF NEW THEFT CASES AND IMPROVED PROPOSED ALGORITHMS USING FEATURE ENGINEERING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Generation of new theft cases</head><p>Based on reference <ref type="bibr" target="#b0">[1]</ref>, we generated the updated (new) six theft cases. The fundamental idea here is to generate more practical realtime malicious behavioral patterns and label them for the use of supervised ML algorithms. If the real usage of the customer is ùë• ùë° , then the following are the revised theft patterns (ùë° ‚àà <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">48]</ref> The authors of <ref type="bibr" target="#b0">[1]</ref> generated theft cases 1 and 2 by multiplying the actual readings with a random number between 0.1 and 0.8. We change the upper limit of 0.8 to 0.9 for theft case 1. For theft case 2, we argue that theft might not occur continuously and there may be some discontinuous reporting of "theft" or "manipulated" values. That is why we took random numbers between (and including) 0.1 and 1.0 as our multipliers for theft case 2. In theft case 3, the consumer either sends the real reading for that time window or just sends zero usage. Theft cases 4 and 5 were generated using the mean values of the readings, while the multiplier in the fourth case was changed from (0.1, 0.8) to (0.1, 1.0) to mimic real-world theft. Theft case 6 is when the consumer reports the reverse of the actual usage for that day. (Although theft case 6 is slightly controversial, we still consider it for the sake of consistency. An improved theft case 6 based on the price of energy consumption will be applied in future work.) Figure <ref type="figure">2</ref> shows the six different types of new theft cases generated using the real usage of a customer for a random day.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation of proposed algorithm with new theft cases</head><p>We followed the procedure of Section II.B except we used the new theft cases instead the old theft cases in step 2 to generate malicious samples. We evaluated the proposed GBTD algorithm with the new theft cases and compared it with the existing CPBETD algorithm. For the new theft cases, Table <ref type="table" target="#tab_1">1</ref> confirms that in terms of average DR (94 to 97% for GBTD vs 88% for CPBETD) and FPR (5 to 7% for GBTD vs 15% for CPBETD), the former is more dominant than the latter. It also implies the new theft cases are harder to detect for the SVM-based CPBETD algorithm.</p><p>1949-3053 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. The synthesized features that are derived from existing features of a given dataset could be helpful to improve the performance of the ML algorithm <ref type="bibr" target="#b6">[7]</ref>. Thus, we tested the proposed GBTD algorithm with some synthesized features and confirmed that the following four stochastic features-the standard deviation ('Std'; œÉ), mean ('Mean'; ùë•ÃÖ ùë° ), maximum ('Max'; ùë• ùë°,ùëöùëéùë• ), and minimum values of daily usage ('Min'; ùë• ùë°,ùëöùëñùëõ ) could improve the detection performance of the proposed algorithm further.</p><p>Following the same steps as in Section II.B, we numerically evaluated the three GBTD classifiers with default features (total of 48 features; w/o synth) or with default plus each of new synthesized features (total of 49 features; w/ Mean, w/ Std, w/ Min, or w/ Max) or with default plus all four new synthesized features (total of 52 features; w/All 4) while using the revised six theft cases and compared their performances each other in Table <ref type="table" target="#tab_2">2</ref>. While minor improvements were observed in DR, major improvements (i.e., large reduction compared to CPBETD) were observed in FPR; the lowering of FPR implies a better commercial usage of GBTD.  Feature extraction is a preprocessing function of GBTD that creates a subset of the given features to reduce noise and improve the GBTD classifiers' performance. The "feature_importance" module gives us a numerical weighted feature-importance (WFI) metric for each of the features' contributions in the decision trees. Depending on the classifier used, the same feature may or may not be as important. Via simulation, we confirmed that the 4 stochastic features in Table <ref type="table" target="#tab_2">2</ref> usually were among topmost important features of all the GBCs. To verify the significance of feature extraction, for 100 randomly chosen customers from the dataset, we recursively ran the GBCs with important feature subsets selected based on a WFI threshold value. At each iteration, we increased the threshold value, reduced the number of selected important features (ùëÅ ùëì ), re-trained and re-tested the GBCs. Figure <ref type="figure" target="#fig_3">3</ref> shows the general average trend of the 100 customers' theft DR, FPR, and evaluation time (training and testing time) vs ùëÅ ùëì while using the proposed feature extraction method. We observed slight improvement (or maintenance) of DR and FPR as ùëÅ ùëì reduces, until finally the performance is degraded when ùëÅ ùëì are too low (&lt; 10). Figure <ref type="figure" target="#fig_3">3</ref> also shows that lower number of features equate to lower timecomplexity. The average behavior of 100 customers from Table <ref type="table" target="#tab_2">2</ref> can be verified when ùëÅ ùëì = 52 in Figure <ref type="figure" target="#fig_3">3</ref>. Although all the three classifiers depict high DR and low FPR when all 52 features are used, we can just create a subset of top 10-20 important features depending on the customer's usage pattern and the classifier used, and yet maintain very high DR and lower FPR. This validates the significance of using feature extraction. In commercial use, during the initial training of a customer's usage pattern history, the proposed GBTD algorithm performs this feature extraction function that significantly reduces the processing time as well as storage space of subsequent retraining while maintaining higher performance in classifications. This letter presented a SG theft detection algorithm, called GBTD, based on the three GBCs (XGBoost, CATBoost, LightGBM). Out of the three GBCs, in terms of DR, both LightGBM and CATBoost outperformed XGBoost. However, LightGBM appeared to be the fastest classifier with highest FPR while CATBoost performed the slowest with lowest FPR. We also numerically proved that GBTD with feature engineering not only minimizes FPR but also reduces customer data storage space as well as processing time. Choice of the GBC depends on the availability of computation resources and acceptable FPR. As such the proposed algorithm would be beneficial for commercial use.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>): 1 .Fig. 2</head><label>12</label><figDesc>Fig. 2 Data generation and visualization of new theft cases</figDesc><graphic coords="2,322.00,302.01,234.34,90.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TSG.2019.2892595, IEEE Transactions on Smart Grid C. Preprocessing with synthetic feature generation and weighted feature importance (WFI) based feature extraction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3 DR, FPR and evaluation time vs the no. of features selected for average of 100 random customers IV. CONCLUSION</figDesc><graphic coords="3,319.95,205.69,243.14,76.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 AVERAGE</head><label>1</label><figDesc>DETECTION PERFORMANCE COMPARISON OF 5000 CUSTOMERS BETWEEN CPBETD AND GBTD CLASSIFIERS, WHERE OLD OR NEW (REVISED) THEFT CASES ARE USED (SEPARATED BY A SLASH '/')</figDesc><table><row><cell>(Old/New)</cell><cell>CPBETD classifier</cell><cell></cell><cell cols="2">GBTD classifiers</cell></row><row><cell></cell><cell>SVM</cell><cell cols="2">XGBoost CatBoost</cell><cell>LightGBM</cell></row><row><cell>DR (%)</cell><cell>94 / 88</cell><cell>95 / 94</cell><cell>96 / 97</cell><cell>96 / 97</cell></row><row><cell>FPR (%)</cell><cell>11 / 15</cell><cell>7 / 6</cell><cell>8 / 5</cell><cell>7 / 7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 PERFORMANCE</head><label>2</label><figDesc>COMPARISON WITHOUT OR WITH NEW FEATURE(S) (AVERAGE OF 100 RANDOM CUSTOMERS), WHERE REVISED THEFT CASES ARE USED</figDesc><table><row><cell>XGBoost</cell><cell>w/o synth</cell><cell>w/ Mean</cell><cell>w/ Std</cell><cell>w/ Min</cell><cell>w/ Max</cell><cell>w/ All 4</cell></row><row><cell>DR (%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FPR (%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work has been submitted on [DATE]. This work was supported by the Korean government (MSIT) (2017R1A2B4005840) and by the Catholic University of Korea, Research Fund, 2018. Rajiv Punmiya and Sangho Choe are with the Department of Information, Communications, and Electronics Engineering, at The Catholic University of</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Electricity theft detection in AMI using customers&apos; consumption patterns</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jokar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Arianpoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C M</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Smart Grid</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="216" to="226" />
			<date type="published" when="2016-01">Jan. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Detection of Non-Technical Losses Using Smart Meter Data and Supervised Learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Buzau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Exp√≥sito</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSG.2018.2807925</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Smart Grid</title>
		<imprint>
			<date type="published" when="2018-02">Feb. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">XGBoost: A Scalable Tree Boosting System</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939785</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08">Aug. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">CatBoost: gradient boosting with categorical features support</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dorogush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ershov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on ML Systems at Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">LightGBM: A Highly Efficient Gradient Boosting Decision Tree</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qi Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31 st Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<ptr target="http://www.ucd.ie/issda/data/commissionforenergyregulationcer/" />
	</analytic>
	<monogr>
		<title level="j">Irish Social Science Data Archive</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An empirical analysis of feature engineering for predictive modelling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heaton</surname></persName>
		</author>
		<idno type="DOI">10.1109/SECON.2016.7506650</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE SoutheastCon</title>
		<meeting>IEEE SoutheastCon<address><addrLine>Norfolk, VA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-04">April 2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
