<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention-Passing Models for Robust and Data-Efficient End-to-End Speech Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Matthias</forename><surname>Sperber</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
							<email>gneubig@cs.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jan</forename><surname>Niehues</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alex</forename><surname>Waibel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Attention-Passing Models for Robust and Data-Efficient End-to-End Speech Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Speech translation has traditionally been approached through cascaded models consisting of a speech recognizer trained on a corpus of transcribed speech, and a machine translation system trained on parallel texts. Several recent works have shown the feasibility of collapsing the cascade into a single, direct model that can be trained in an end-to-end fashion on a corpus of translated speech. However, experiments are inconclusive on whether the cascade or the direct model is stronger, and have only been conducted under the unrealistic assumption that both are trained on equal amounts of data, ignoring other available speech recognition and machine translation corpora.</p><p>In this paper, we demonstrate that direct speech translation models require more data to perform well than cascaded models, and although they allow including auxiliary data through multi-task training, they are poor at exploiting such data, putting them at a severe disadvantage. As a remedy, we propose the use of endto-end trainable models with two attention mechanisms, the first establishing source speech to source text alignments, the second modeling source to target text alignment. We show that such models naturally decompose into multitask-trainable recognition and translation tasks and propose an attention-passing technique that alleviates error propagation issues in a previous formulation of a model with two attention stages. Our proposed model outperforms all examined baselines and is able to exploit auxiliary training data much more effectively than direct attentional models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Speech translation takes audio signals of speech as input and produces text translations as output. Although traditionally realized by cascading an automatic speech recognition (ASR) and a machine translation (MT) component, recent work has shown that it is feasible to use a single sequence-to-sequence model instead <ref type="bibr" target="#b9">(Duong et al., 2016;</ref><ref type="bibr" target="#b24">Weiss et al., 2017;</ref><ref type="bibr" target="#b6">Bérard et al., 2018;</ref><ref type="bibr" target="#b2">Anastasopoulos and Chiang, 2018</ref>). An appealing property of such direct models is that we no longer suffer from propagation of errors, where the speech recognizer passes an erroneous source text to the machine translation component, potentially leading to compounding follow-up errors. Another advantage is the ability to train all model parameters jointly.</p><p>Despite these obvious advantages, two problems persist: (1) Reports on whether direct models outperform cascaded models (Fig. <ref type="figure" target="#fig_0">1a,d</ref>) are inconclusive, with some work in favor of direct models <ref type="bibr" target="#b24">(Weiss et al., 2017)</ref>, some work in favor of cascaded models <ref type="bibr" target="#b10">(Kano et al., 2017;</ref><ref type="bibr" target="#b6">Bérard et al., 2018)</ref>, and one work in favor of direct models for two out of the three examined language pairs <ref type="bibr" target="#b2">(Anastasopoulos and Chiang, 2018)</ref>. (2) Cascaded and direct models have been compared under identical data situations, but this is an unrealistic assumption: In practice, cascaded models can be trained on much more abundant independent ASR and MT corpora, whereas end-to-end models require hard-to-acquire end-to-end corpora of speech utterances paired with textual translations.</p><p>Our first contribution is a closer investigation of these two issues. Regarding the question of whether direct models or cascaded models are generally stronger, we hypothesize that direct models require more data to work well, due to the more complex mapping between inputs (source speech) and outputs (target text). This would imply that direct models outperform cascades when enough data are available, but underperform in low-data scenarios. We conduct experiments and present empirical evidence in favor of this hypothesis. Next, for a more realistic comparison with regard to data conditions, we train a direct speech translation model using more auxiliary ASR and MT training data than end-to-end data. This can be implemented through multi-task training <ref type="bibr" target="#b24">(Weiss et al., 2017;</ref><ref type="bibr" target="#b6">Bérard et al., 2018)</ref>. Our results show that the auxiliary data are beneficial only to a limited extent, and that direct multitask models are still heavily dependent on the end-to-end data.</p><p>As our second contribution, we apply a two-stage model <ref type="bibr" target="#b23">(Tu et al., 2017;</ref><ref type="bibr" target="#b10">Kano et al., 2017)</ref> as an alternative solution to our problem, hoping that such models may overcome the data efficiency shortcoming of the direct model. Twostage models consist of a first-stage attentional sequence-to-sequence model that performs speech recognition and then passes the decoder states as input to a second attentional model that performs translation (Fig. <ref type="figure" target="#fig_0">1b</ref>). This architecture is closer to cascaded translation while maintaining end-toend trainability. Introducing supervision from the source-side transcripts midway through the model creates inductive bias that guides the complex transformation between source speech and target text through a reasonable intermediate representation closely tied to the source text. The architecture has been proposed by <ref type="bibr" target="#b23">Tu et al. (2017)</ref> to realize a reconstruction objective, and a similar model was also applied to speech translation <ref type="bibr" target="#b10">(Kano et al., 2017)</ref> to ease trainability, although no experiments under varying data conditions have been conducted. We hypothesize that such a model may help to address the identified data efficiency issue: Unlike multi-task training for the direct model that trains auxiliary models on additional data but then discards many of the additionally learned parameters, the two-stage model uses all parameters of sub-models in the final end-to-end model. Empirical results confirm that the twostage model is indeed successful at improving data efficiency, but suffers from some degradation in translation accuracy under high data conditions compared with the direct model. One reason for this degradation is that this model re-introduces the problem of error propagation, because the second stage of the model depends on the decoder states of the first model stage which often contain errors.</p><p>Our third contribution, therefore, is an attentionpassing variant of the two-stage model that, rather than passing on possibly erroneous decoder states from the first to the second stage, passes on only the computed attention context vectors (Fig. <ref type="figure" target="#fig_0">1c</ref>). We can view this approach as replacing the early decision on a source-side transcript by an early decision only on the attention scores needed to compute the same transcript, where the attention scores are expectedly more robust to errors in source text decoding. We explore several variants of this model and show that it outperforms both the direct model and the vanilla two-stage model, while maintaining the improved data efficiency of the latter. Through an analysis, we further observe a trade-off between sensitivity to error propagation and data efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Baseline Models</head><p>This section introduces two types of end-to-end trainable models for speech translation, along with a cascaded approach, which will serve as our baselines. All models are based on the attentional encoder-decoder architecture of <ref type="bibr" target="#b3">Bahdanau et al. (2015)</ref> with character-level outputs, and use the architecture described in §2.1 as audio encoders. The end-to-end trainable models include a direct model and a two-stage model. Both are limited 1 by the fact that they can only be trained on end-to-end data, which is much harder to obtain than ASR or MT data used to train traditional cascades. 2 §3 will introduce multi-task training as a way to overcome this limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Audio Encoder</head><p>Sequence-to-sequence models can be adopted for audio inputs by directly feeding speech features (here, Mel filterbank features) instead of word embeddings as encoder inputs <ref type="bibr" target="#b7">(Chorowski et al., 2015;</ref><ref type="bibr" target="#b6">Chan et al., 2016)</ref>. Such an encoder transforms M feature vectors x 1:M into L encoded vectors e 1:L , performing downsampling such that L&lt;M . We use an encoder architecture that follows one of the variants described by <ref type="bibr" target="#b26">Zhang et al. (2017)</ref>: We stack two blocks, each consisting of a bidirectional long short-term memory (LSTM), a network-in-network (NiN) projection that downsamples by factor two, and batch normalization. After the second block, we add a final bidirectional LSTM layer. NiN denotes a simple linear projection applied at every time step, performing downsampling by concatenating pairs of adjacent projection inputs. Because of space constraints, we do not present detailed equations, but refer interested readers to <ref type="bibr" target="#b26">Zhang et al. (2017)</ref> as well as to our provided code for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Direct Model</head><p>The sequence-to-sequence model with audio inputs outlined above can be trained as a direct speech translation model by using speech data as input and the corresponding translations as outputs. Such a model does not rely on intermediate ASR output and is therefore not subject to error propagation. However, the transformation from source speech inputs to target text outputs is much more complex than that of an ASR or MT system taken individually, which may cause the model to require more data to perform well.</p><p>To make matters precise, given L audio encoder states e 1:L computed by the audio encoder as and MT corpora exist and for which it is more realistic to obtain good speech translation accuracy.</p><p>2 As a case in point, the largest available speech translation corpora <ref type="bibr" target="#b18">(Post et al., 2013;</ref><ref type="bibr">Kocabiyikoglu et al., 2018)</ref> are an order of magnitude smaller than the largest speech recognition corpora <ref type="bibr" target="#b8">(Cieri et al., 2004;</ref><ref type="bibr" target="#b17">Panayotov et al., 2015)</ref> (∼ 200 hours vs 2000 hours) and several orders of magnitude smaller than the largest machine translation corpora, e.g., those provided by the Conference on Machine Translation (WMT). described in §2.1, the direct model is computed as</p><formula xml:id="formula_0">s i = LSTM ([W e y i−1 ; c i−1 ] , s i−1 ; θ lstm ) (1) c i = Attention (s i , e 1:L ; θ att ) (2) si = tanh (W s [s i ; c i ] + b s ) (3) p (y i | y &lt;i , e 1:L ) = SoftmaxOut (s i ; θ out ) . (4)</formula><p>Here, W * , b * , and θ * are the trainable parameters, y i are output characters, and SoftmaxOut denotes an affine projection followed by a softmax operation. s i are decoder states with s 0 initialized to the last encoder state, and c i are attentional context vectors with c 0 =0. In Equation <ref type="formula">2</ref>, we compute Attention(•)= L j=1 α ij e j with weights α ij conditioned on e j and s i , parameterized by θ att , and normalized via a softmax operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Two-Stage Model</head><p>As an alternative to the direct model, the two-stage model uses a cascaded information flow while maintaining end-to-end trainability. Our main motivation for using this model is the potentially improved data efficiency when adding auxiliary ASR and MT training data ( §3). This model is similar to the architecture first described by <ref type="bibr" target="#b23">Tu et al. (2017)</ref>. It combines two encoder-decoder models in a cascade-like fashion, with the decoder of the first stage and the encoder of the second stage being shared (Fig. <ref type="figure" target="#fig_1">2</ref>). In other words, while a cascade would use the source-text outputs of the first stage as inputs into the second stage, in this model the second stage directly computes attentional context vectors over the decoder states of the first stage. The inputs of the two-stage model are speech frames, the outputs of the first stage are transcribed characters in the source language, and the outputs of the second stage are translated characters in the target language.</p><p>Again assuming L audio encoder states e 1:L , the first stage outputs of length N are computed identically to equations 1-4, except that input feeding (conditioning the decoding step on the previous context vector) is not used in the first </p><formula xml:id="formula_1">s src i = LSTM (W src e y src i−1 , s src i−1 ; θ src lstm ) (5) c src i = Attention (s src i , e 1:L ; θ src att ) (6) ssrc i = tanh (W src s [s src i ; c src i ] + b src s ) (7) p (y src i | y &lt;i , e 1:L ) = SoftmaxOut (s src i ; θ src out )<label>(8)</label></formula><p>Next, the second stage proceeds similarly but uses the stage 1 decoder states as input:</p><formula xml:id="formula_2">s trg j = LSTM W trg e y trg i−1 ; c trg j−1 , s trg j−1 ; θ trg lstm (9)</formula><p>c trg j = Attention s trg j , s src 1:N ; θ trg att (10)</p><formula xml:id="formula_3">strg j = tanh W trg s s trg j ; c trg j + b trg s (11) p y trg j | y &lt;j , s src 1:N = SoftmaxOut strg j ; θ trg out (12)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Cascaded Model</head><p>We finally utilize a traditional cascaded model as a baseline, whose architecture is kept as similar to the above models as possible in order to facilitate meaningful comparisons. The cascade consists of an ASR component and an MT component, which are both attentional sequence-to-sequence models according to equations 1-4, trained on the appropriate data. The ASR component uses the acoustic encoder of §2.1, and the MT model uses a bidirectional LSTM with 2 layers as encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Incorporating Auxiliary Data</head><p>The models described in §2.2 and §2.3 are trained only on speech utterances paired with translations (and transcripts in the case of §2.3), which is a severe limitation. To incorporate auxiliary ASR and MT data into the training, we make use of a multi-task training strategy. Such a strategy trains auxiliary ASR and MT models that share certain parameters with the main speech translation model. We implement multi-task training by drawing several minibatches, one minibatch for each task, and performing an update based on the accumulated gradients across tasks. Note that this results in a balanced contribution of each task.<ref type="foot" target="#foot_2">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-Task Training for the Direct Model</head><p>Multi-task training for direct speech translation models has previously been used by <ref type="bibr" target="#b24">Weiss et al. (2017)</ref> and <ref type="bibr" target="#b6">Bérard et al. (2018)</ref>, although not for the purpose of adding additional training utterances that are not shown to the main speech translation task. <ref type="foot" target="#foot_3">4</ref> We distinguish five model components: a source speech encoder, a source text encoder (a two-layer bidirectional LSTM working on character level), a source text decoder, a target text decoder, and an attention mechanism that we opt to share across all tasks. There are four ways in which these components can be combined into a complete sequence-to-sequence model (see Figure <ref type="figure" target="#fig_2">3</ref>), corresponding to the following four tasks:</p><p>ASR: Combines source speech encoder, generalpurpose attention, source text decoder. This is similar to the auxiliary ASR task used by <ref type="bibr" target="#b24">Weiss et al. (2017)</ref> and can be trained on common ASR data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MT:</head><p>Combines source text encoder, generalpurpose-attention, target text decoder. The addition of an MT task has been mentioned by <ref type="bibr" target="#b6">Bérard et al. (2018)</ref> and allows training on common MT data.</p><p>ST: Combines source speech encoder, generalpurpose-attention, target text decoder. This is our main task and requires end-to-end data for training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Auto-encoder (AE):</head><p>Combines source text encoder, general-purpose attention, source text decoder. The AE task can be trained on monolingual corpora in the source language and may serve to tighten the coupling between components and potentially improves the parameters of the general-purpose attention model. We have observed slight improvements by adding the AE task in preliminary experiments and will therefore use it throughout this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-Task Training for the Two-Stage Model</head><p>Including an auxiliary ASR task is straightforward with the two-stage model by simply computing the cross-entropy loss with respect to the softmax output of the first stage, and dropping the second stage. The auxiliary MT task computes only the second stage, replacing the inputs s src 1:N by states e asr 1:N computed as:</p><formula xml:id="formula_4">e asr i = LSTM W src e y transcr i , e src i−1 ; θ src lstm . (<label>13</label></formula><formula xml:id="formula_5">)</formula><p>That is, instead of computing the secondstage inputs using the first stage, we compute these inputs through a conventional encoder that encodes the reference transcript y transcr 1:N and uses the same embeddings matrix and unidirectional LSTM as the first stage decoder. Note that there is no equivalent to the auxiliary auto-encoder task of the direct multi-task model here.</p><p>Why might this architecture help to make better use of auxiliary ASR and MT data? Note that in the direct model only roughly half of the model parameters are shared between the main task and the ASR task, and likewise for main and MT tasks ( §3.1). Additional data would therefore only have a rather indirect impact on the main task. In contrast, in the two-stage model all parameters of the auxiliary tasks are shared with the main task and therefore have a more direct impact, potentially leading to better data efficiency.</p><p>Note that somewhat related to our multi-task strategy, <ref type="bibr" target="#b10">Kano et al. (2017)</ref> have decomposed their two-stage model in a similar way to perform pretraining for the individual stages, although not with the goal of incorporating additional auxiliary data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Attention-Passing Model</head><p>We have so far described a direct model that has the appealing property of avoiding error propagation in a principled way but that may not be particularly data-efficient, and have described a two-stage model that addresses the latter disadvantage. Unfortunately, the two-stage model re-introduces the error propagation problem into end-to-end modeling, because the second stage heavily depends on the potentially erroneous decoder states of the first stage. We therefore propose an improved attention-passing model in this section that is less impacted by error propagation issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Preventing Error Propagation</head><p>The main idea behind the attention-passing model is to not feed the erroneous first-stage decoder states to the second stage, but instead to pass on only the context vectors that summarize the relevant encoded audio at each decoding step. The first stage decoder is unfolded as usual by using discrete source-text representations, but the only information exposed to the translation stage are the per-timestep context vectors created as a byproduct of the decoding. Figure <ref type="figure" target="#fig_3">4</ref> illustrates this idea. Intuitively, we expect this to help because we no longer make an early decision on the identity of the source-language text, but only on the corresponding attentions. This is motivated by our observation that speech recognition attentions are sufficiently robust against decoding errors ( §5.7).</p><p>Formally, the first stage remains unchanged from equations 5-8. The context vectors c src i then form the input to the second stage: </p><formula xml:id="formula_6">x trg i = LSTM c src i , x trg i−1 ; θ src lstm (<label>14</label></formula><formula xml:id="formula_7">)</formula><formula xml:id="formula_8">s trg j = LSTM W trg e y trg i−1 ; c trg j−1 , s trg j−1 ; θ trg lstm (<label>15</label></formula><formula xml:id="formula_9">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Decoder State Drop-Out</head><p>Along with the modifications described in §4.1, we introduce an additional block drop-out operation <ref type="bibr">(Ammar et al., 2016)</ref> on the decoder states, replacing equation 7 by</p><formula xml:id="formula_10">ssrc i = tanh (W src s [BDrop {s src i } ; c src i ] + b src s ) .</formula><p>The block drop-out operation, denoted as BDrop, replaces the whole vector by zero with a certain probability (here: 0.5). This results in the context vectors c src i becoming the only information available to the output layer whenever the decoder states are dropped out. The motivation for this is to force the model to maximize the informativeness of the context vectors, which are later relied upon as sole inputs to the second stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Multi-Task Training</head><p>Similar to the basic two-stage model, the attentionpassing model as a whole is trained on speech-transcript-translation triplets, but can be decomposed into two sub-models that correspond to ASR and MT tasks. In fact, the ASR task is unchanged with the exception of the new block dropout operation. The MT task is obtained by replacing equation 14 by x trg i = LSTM W e y src i , x trg i−1 ; θ src lstm -that is, by using the transcript character embeddings as inputs instead of the context vectors used when training the main task. Note that the LSTMs in equations 5 and 14 are shared in order to have a match between stage 1 decoder and stage 2 encoder as with the basic model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Cross Connections</head><p>As a further extension to the attention-passing model of §4.1, we can introduce cross connections that concatenate the dropped-out first stage hidden decoder states to the second-stage inputs encoder. This causes equation 14 to be replaced by</p><formula xml:id="formula_11">x trg i = (19) LSTM Affine [c src i ; BDrop {s src i }] , x trg i−1 ; θ src lstm</formula><p>This extension moves the model closer to the basic two-stage model, and the inclusion of the context vectors and the block drop-out operation on the hidden decoder states ensures that the second stage decoder does not rely too strongly on the first stage outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Additional Loss</head><p>We further experiment with introducing an additional loss aimed at making the LSTM inputs between first stage decoder and second stage encoder RNN more similarly. Recall that in our attention-passing model, both RNNs share parameters (equations 5 and 14), so that similar inputs at both times is desirable. The loss is defined as follows:</p><formula xml:id="formula_12">L add = ||c src i − W e y src i || 2 .</formula><p>If combined with the cross connections ( §4.4), the formula is adjusted to</p><formula xml:id="formula_13">L add = ||Affine [c src i ; BDrop {s src i }] − W e y src i || 2 .</formula><p>We did not find it beneficial to apply a scaling factor when adding this loss to the main cross-entropy loss in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We conduct experiments on the Fisher and Callhome Spanish-English Speech Translation Corpus <ref type="bibr" target="#b18">(Post et al., 2013)</ref>, a corpus of Spanish telephone conversations that includes audio, transcriptions, and translations into English. We use the Fisher portion that consists of telephone conversations between strangers. The training data size is 138,819 sentences, corresponding to 162 hours of speech. ASR word error rates on this dataset are usually relatively high because of the spontaneous speaking style and challenging acoustics. From a translation viewpoint, the data can be considered as relatively easy with regard to both the topical domain and particular language pair.</p><p>Our implementation is based on the xnmt toolkit. <ref type="foot" target="#foot_4">5</ref> We use the speech recognition recipe as a starting point, which has previously been shown to achieve competitive ASR results <ref type="bibr" target="#b14">(Neubig et al., 2018)</ref>. <ref type="foot" target="#foot_5">6</ref>The vocabulary consists of the common characters appearing in English and Spanish, apostrophe, whitespace, and special start-of-sequence and unknown-character tokens. The same vocabulary is used on both encoder (for the MT auxiliary task) and decoder sides. We set the batch size dynamically depending on the input sequence size such that the average batch size is 24 sentences. We use Adam (Kingma and Ba, 2014) with initial learning rate of 0.0005, decayed by 0.5 when the validation BLEU score did not improve over 10 check points initially and 5 check points after the first decay. We initialize attention-passing models using weights from a basic two-stage model trained on the same data.</p><p>Following <ref type="bibr" target="#b24">Weiss et al. (2017)</ref>, we lowercase texts and remove punctuation. As speech features, we use 40-dimensional Mel filter bank features with per-speaker mean and variance normalization. We exclude a small number of utterances longer than 1500 frames from training to avoid running out of memory. The encoder-decoder attention is MLP-based, and the decoder uses a single LSTM layer. <ref type="foot" target="#foot_6">7</ref>   <ref type="bibr">Gal and Ghahramani, 2016)</ref>. We apply label smoothing <ref type="bibr" target="#b20">(Szegedy et al., 2016)</ref> and fix the target embedding norm to 1 (Nguyen and Chiang, 2018). We use beam search with beam size 15 and polynomial length normalization with exponent 1.5. <ref type="foot" target="#foot_7">8</ref>All BLEU scores are computed on Fisher/Test against 4 references.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Cascaded vs. Direct Models</head><p>We first wish to shed light on the question of whether cascaded or direct models can be expected to perform better. This question has been investigated previously <ref type="bibr" target="#b24">(Weiss et al., 2017;</ref><ref type="bibr" target="#b10">Kano et al., 2017;</ref><ref type="bibr" target="#b6">Bérard et al., 2018;</ref><ref type="bibr" target="#b2">Anastasopoulos and Chiang, 2018)</ref>, but with contradictory findings. We hypothesize that the increased complexity of the direct mapping from speech to translation increases the data requirements such models. provides evidence in favor of our hypothesis and indicates that direct end-to-end models should be expected to perform strongly only in a case where enough training data is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Two-Stage Models</head><p>Next, we investigate the performance of the twostage models, for both the basic variant ( §3.2) and our proposed attention-passing model ( §4). Again, all models are trained in a multi-task fashion by including auxiliary ASR and MT tasks based on the same data. Table <ref type="table" target="#tab_2">2</ref> shows the results. The basic two-stage model performs in between the direct and models from §5.1. APM, the attention-passing model of §4.1 which is designed to circumvent the negative effects of error propagation, outperforms the basic variant and performs similarly to the direct model. The APM extensions ( §4.4, §4.5) further improved the results, with the best model outperforming the direct model by 1.40 BLEU points and the basic two-stage model by 2.34 BLEU points absolute. The last row in the table confirms that the block dropout operation contributed to the gains: Removing it led to a drop by 0.66 BLEU points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Data Efficiency: Direct Model</head><p>Having established results in favor of our proposed model on the full data, we now examine the data efficiency of the different models. Our experimental strategy is to compare model performance (1) when trained on the full data, (2) when trained on partial data, and (3) when trained on partial speech-to-translation data but full auxiliary (ASR+MT) data. 9</p><p>9 An alternative experimental strategy is to train on the full data and then add auxiliary data from other domains to the Figure <ref type="figure" target="#fig_4">5</ref> shows the results, comparing the cascaded model against the direct model trained under conditions (1), (2), and (3).<ref type="foot" target="#foot_8">10</ref> Unsurprisingly, the performance of the direct model trained on partial data declines sharply as the amount of data is reduced. Adding auxiliary data through multi-task training improves performance in all cases. For instance, in the case of 69k speechto-translation instances, adding the full auxiliary data helps to reach the accuracy of the cascaded model. However, this is already somewhat disappointing because the end-to-end data, which is not available to the cascaded model, no longer yields an advantage. Moreover, reducing the endto-end data further reveals that multi-task training is not able to close the gap to the cascade. In the scenario with 35k end-to-end instances and full auxiliary data, the direct model underperforms the cascade by 9.14 BLEU points (32.50 vs. 23.36), despite being trained on more data. The unsatisfactory data efficiency in this controlled ablation study strongly indicates that the direct model will also fall behind a cascade that is trained on large amounts of external data. This claim is verified in §5.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Data Efficiency: Two-Stage Models</head><p>We showed that the direct model is poor at integrating auxiliary data and heavily depends on sufficient amounts of end-to-end training data.</p><p>training. We pursue this strategy in §5.5 as a more realistic scenario, but point out several problems that lead us to not use this as our main approach: Adding external auxiliary data (1) leads to side-effects through domain mismatch and</p><p>(2) severely limits the number of experiments that we can conduct because of the considerably increased training time. How do two-stage models behave with regard to this data efficiency issue? Figure <ref type="figure" target="#fig_5">6</ref> shows that both the basic two-stage model and the best APM perform reasonably well even when having seen much less end-to-end data. We can explain this by noticing that these models can be naturally decomposed into an ASR sub-model and an MT sub-model, while the direct model needs to add auxiliary sub-models to support multi-task training. Interestingly, the attention-passing model without cross-connections does better than the direct model with regard to data efficiency, but falls behind the basic and best proposed two-stage models. This indicates that access to ASR labels in some form contributes to favorable data efficiency of speech translation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Adding External Data</head><p>Our approach for evaluating data efficiency so far has been to assume that end-to-end data are available for only a subset of the available auxiliary data. In practice, we can often train ASR and MT tasks on abundant external data. We therefore run experiments in which we use the full Fisher training data for all tasks as before, and add OpenSubtitle 11 data for the auxiliary MT task. We clean and normalize the Spanish-English OpenSubtitle 2018 data <ref type="bibr" target="#b13">(Lison and Tiedemann, 2016)</ref> to be consistent with the employed Fisher training data by lowercasing and removing punctuation. We apply a basic length filter and obtain 61 million sentences. During training, we include the same number of sentences from in-domain and out-of-domain MT tasks in each minibatch in order to prevent degradation due to domain mismatch. Our models converged before a full pass over the OpenSubtitle data, but needed between two and three times more steps than the in-domain model to converge.</p><p>Table <ref type="table" target="#tab_3">3</ref> shows that all models were able to benefit from the added data. However, when examining the relative gains we can see that both the cascaded model and the models with two attention stages benefitted about twice as much from the external data as the direct model. In fact, the basic two-stage model now slightly surpasses the direct model, and the best APM is ahead of the basic two-stage model by almost the same absolute difference as before (2.36 BLEU points). The superior relative gains show that our findings from §5.3 and §5.4, namely, that two-stage models are much more efficient at exploiting auxiliary training data, generalizes to the setting in which large amounts of out-of-domain data are added to the MT task. Out-of-domain data are often much easier to obtain, and we can therefore conclude that the proposed approach is preferable in many practically relevant situations. Because these experiments are very expensive to conduct, we leave experiments with external ASR data for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Error Propagation</head><p>To better understand the impact of error propagation, we analyze how improved or degraded ASR labels impact the translation results. This experiment is applicable to APM, the two-stage model and the cascade, but not to the direct model which does not compute intermediate ASR outputs. We analyze three different settings: using the standard decoded ASR labels, replacing these labels with the gold labels, or artificially degrading the decoded labels by randomly introducing 10% of substitution, insertion, and deletion noise (Sperber  <ref type="bibr">et al., 2017)</ref>. Intuitively, models that suffer from error propagation issues are expected to rely most heavily on these intermediate labels and would therefore be most impacted by both degraded and improved labels.</p><p>Table <ref type="table" target="#tab_4">4</ref> shows the results. Unsurprisingly, the cascade responds most strongly to improved or degraded noise, confirming that it is severely impacted by error propagation. The APM, which does not directly expose the labels to the translation sub-model, is much less impacted. However, the impact is still more significant than perhaps expected, suggesting that improved attention models that are more robust to decoding errors <ref type="bibr" target="#b7">(Chorowski et al., 2015;</ref><ref type="bibr" target="#b21">Tjandra et al., 2017)</ref> may serve to further improve our model in the future. Note that the APM benefits poorly from gold ASR labels, which is expected because gold labels only improve the ASR alignments and by extension the passed context vectors, but these are quite robust against decoding errors in the first place.</p><p>The basic two-stage model is impacted significantly, although less strongly than the cascade, in line with our claim that such models are subject to error propagation despite being end-to-end trainable. Note that it falls behind the cascade for gold labels, despite both models being seemingly identical under this condition. This can beexplained by the cascaded model's use of beam search and greater number of encoder layers.</p><p>Somewhat contrary to our expectations, APM with cross connections appears equally subject to error propagation despite the block dropout on these connections, displaying the same accuracy gains across the three different settings. This suggests future explorations toward model variants with an even better trade-off between overall accu-  racy, data efficiency, and amount of degradation due to error propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Robustness of ASR Attentions</head><p>The attention-passing model was motivated by the assumption that attention scores are relatively robust against recognition errors. We perform a qualitative analysis to validate this assumption. Figure <ref type="figure" target="#fig_6">7</ref> shows the first-stage attention matrix when force-decoding the reference transcript, and Figure <ref type="figure" target="#fig_7">8</ref> shows the same regular decoding, which for this utterance produced significant errors. Despite the errors, we can see that the attention matrices are very similar. We manually inspected the first 100 test attention matrices and confirm that this behavior occurs very consistently. Further quantitative evidence is given in §5.6, which showed that the attention-passing model is more resistent to error propagation than the other models. Similarly to the discussed multi-task direct model, this approach discards many of the learned parameters when used on the main task and consequently may also suffer from data efficiency issues.</p><p>Direct end-to-end speech translation models were first used by <ref type="bibr" target="#b9">Duong et al. (2016)</ref>, although the authors did not actually evaluate translation performance. <ref type="bibr" target="#b24">Weiss et al. (2017)</ref> extended this model into a multi-task model and report excellent translation results. Our baselines do not match their results, despite considerable efforts. We note that other research groups have encountered similar replicability issues <ref type="bibr" target="#b4">(Bansal et al., 2018)</ref>, explanations include the lack of a large GPU cluster to perform ASGD training, as well as to explore an ideal number of training schedules and other hyper-parameter settings. <ref type="bibr" target="#b6">Bérard et al. (2018)</ref> explored the translation of audio books with direct models and report reasonable results, but do not outperform a cascaded baseline. <ref type="bibr" target="#b10">Kano et al. (2017)</ref> have first used a basic two-stage model for speech translation. They use a pretraining strategy for the individual sub-models, related to our multi-task approach, but do not attempt to integrate auxiliary data. Moreover, the authors only evaluated the translation of synthesized speech, which greatly simplifies training and may not lead to generalizable conclusions, as indicated by the fact that they were actually able to outperform a translation model that used the gold transcripts as input. <ref type="bibr" target="#b2">Anastasopoulos and Chiang (2018)</ref> conducted experiments on low-resource speech translation and used a triangle model that can be seen as a combination of a direct model and a two-stage model, but is not easily trainable in a multi-task fashion. It is therefore not a suitable choice for exploiting auxiliary data in or-der to compete with cascaded models under well-resourced data conditions. Finally, contemporaneous work explores transferring knowledge from high-resource to low-resource languages <ref type="bibr" target="#b5">(Bansal et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This work explored direct and two-stage models for speech translation with the aim of obtaining models that are strong, not only in favorable data conditions, but are also efficient at exploiting auxiliary data. We started by demonstrating that direct models do outperform cascaded models, but only when enough data is available, shedding light on inconclusive results from prior work. We further showed that these models are poor at exploiting auxiliary data, making them a poor choice in realistic situations. We were motivated to use two-stage models by their ability to overcome this shortcoming of the direct models, and found that two-stage models are in fact more data-efficient, but suffer from error propagation issues. We addressed this by introducing a novel attention-passing model that alleviates error propagation issues, as well as several model variants. The best proposed model outperforms all other tested models and is much more data efficient than the direct model, allowing this model to compete with cascaded models even under realistic assumptions with auxiliary data available. Analysis showed that there seems to be a trade-off between data efficiency and error propagation. Avenues for future work include testing better ASR attention models; adding other types of external data such as ASR data, unlabeled speech, or monolingual texts; and exploring further model variants.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Conceptual diagrams for various speech translation approaches. Cascade (a) uses separate machine translation and speech recognition models. The direct model (d) is a standard attentional encoder-decoder model. The basic 2-stage model (b) uses two attention stages and passes source-text decoder states to the translation component. Our proposed attention-passing model (c) applies two attention stages, but passes context vectors to the translation component for improved robustness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Basic two-stage model. Decoder states of the first stage double as encoder states for the second stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Direct multi-task model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Attention-passing model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Data efficiency for the direct (multi-task) model, compared against cascade on full auxiliary data.</figDesc><graphic url="image-353.png" coords="8,302.41,57.14,228.00,136.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Data efficiency across model types. All models use full auxiliary data through multi-task training.</figDesc><graphic url="image-354.png" coords="9,67.13,56.70,228.00,132.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: ASR attentions when force-decoding the oracle transcripts.</figDesc><graphic url="image-360.png" coords="10,314.33,239.16,87.40,77.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: ASR attentions after regular decoding.</figDesc><graphic url="image-363.png" coords="10,314.33,316.70,145.53,51.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>6</head><label></label><figDesc>Prior Work Model architectures similar to what we have referred to as the basic two-stage model have first been used by Tu et al. (2017) for a reconstruction task, where the first stage performs translation and the second stage attempts to reconstruct the original inputs based on the outputs of the first stage. A second variant of a similar architecture are Xia et al. (2017)'s deliberation networks, where the second stage refines or polishes the outputs of the first stage. For our purposes, the first stage performs speech recognition, a natural intermediate representation for the speech translation task, corresponding to the second stage output. Toshniwal et al. (2017) explore a different way of lower-level supervision during training of an attentional speech recognizer by jointly training an auxiliary phoneme recognizer based on a lower layer in the acoustic encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>compares the direct multi-task model</cell></row><row><cell>( §3.1) against a cascaded model with identical</cell></row><row><cell>architecture to the respective ASR and MT sub-</cell></row><row><cell>models of the multi-task model. The direct model</cell></row><row><cell>is trained with multi-task training on the auxiliary</cell></row><row><cell>ASR, MT, and AE tasks on the same data that</cell></row><row><cell>outperformed single-task training considerably in</cell></row><row><cell>preliminary experiments. As can be seen, the direct</cell></row><row><cell>model outperforms the traditional cascaded setup</cell></row><row><cell>only when both are trained on the full data, but not</cell></row><row><cell>when using only parts of the training data. This</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results for cascaded and multi-task models under full training data conditions.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Adding auxiliary OpenSubtitles MT data to the training. The two-stage models benefit much more strongly than the direct model, with our proposed model yielding the strongest overall results.</figDesc><table><row><cell>Model</cell><cell>Fisher Fisher+OpenSub</cell></row><row><cell>Cascade</cell><cell>32.45 34.58 (+6.2% rel.)</cell></row><row><cell>Direct model</cell><cell>35.30 36.45 (+3.2% rel.)</cell></row><row><cell cols="2">Basic two-stage 34.36 36.91 (+6.9% rel.)</cell></row><row><cell>Best APM</cell><cell>36.70 38.81 (+5.4% rel.)</cell></row></table><note>11 http://www.opensubtitles.org/.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Effect of altering the ASR labels for different models as a measure for robustness against error propagation. We compare results for the cascade, the basic two-stage model (B2S), and APM without and with cross connections. Percentages are relative to the results for unaltered (decoded) ASR labels.</figDesc><table><row><cell>Labels</cell><cell>Gold</cell><cell>Decod.</cell><cell>Perturbed</cell></row><row><cell cols="4">Cascade 58.15 (+44%) 32.45 25.67 (-26%)</cell></row><row><cell>B2S</cell><cell cols="3">56.60 (+39%) 34.36 28.81 (-19%)</cell></row><row><cell>APM</cell><cell cols="3">40.70 (+13%) 35.31 31.96 (-10%)</cell></row><row><cell>+ cross</cell><cell cols="3">58.29 (+37%) 36.70 30.48 (-20%)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Prior work noted that in severe low-resource situations it may actually be easier to collect speech paired with translations than transcriptions(Duong et al.,  </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2016" xml:id="foot_1">). However, we focus on well-resourced languages for which ASR</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">We also experimented with a final fine-tuning phase on only the main task<ref type="bibr" target="#b16">(Niehues and Cho, 2017)</ref>, but discarded this strategy for lack of consistent gains.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">Note that<ref type="bibr" target="#b5">Bansal et al. (2019)</ref> do experiment with additional speech recognition data, although, differently from our work, for purposes of cross-lingual transfer learning.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">https://github.com/neulab/xnmt.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">Code and configuration files can be found at http:// www.msperber.com/research/tacl-attentionpassing/.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">  7 Weiss et al. (2017)  report improvements from deeper decoders, but we encountered stability issues and therefore restricted the decoder to a single layer.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7">For two-stage and attention-passing models, we apply beam search only for the second stage decoder. We do not use the two-phase beam search of<ref type="bibr" target="#b23">Tu et al. (2017)</ref> because of its prohibitive memory requirements.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8">Note that the above hyper-parameters were selected for best full-data performance and are not re-tuned here.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Adam Lopez, Stefan Constantin, and the anonymous reviewers for their helpful comments. The work leading to these results has received funding from the European Union under grant agreement no. 825460.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Action Editor: Adam Lopez</title>
		<imprint>
			<date type="published" when="2018">2019. 2018. 2019. 2019</date>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>Revision batch: 2/. Distributed under a CC-BY 4.0 license. References Waleed Ammar, George Mulcaire, Miguel Ballesteros, Chris Dyer, and Noah A. Smith</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Many languages, one parser</title>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics (TACL)</title>
				<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="431" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tied multitask learning for neural speech translation</title>
		<author>
			<persName><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter</title>
				<meeting><address><addrLine>New Orleans, USA</addrLine></address></meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Representation Learning (ICLR)</title>
				<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Low-resource speech-to-text translation</title>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herman</forename><surname>Kamper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the International Speech Communication Association</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>InterSpeech</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pre-training on high-resource speech recognition improves low-resource speech-to-text translation</title>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herman</forename><surname>Kamper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter</title>
				<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Bérard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><forename type="middle">Can</forename><surname>Kocabiyikoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Icassp)</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calgary</forename><forename type="middle">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2016">2018. 2016</date>
		</imprint>
	</monogr>
	<note>Acoustics, Speech and Signal Processing. ICASSP</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">K</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Fisher corpus: A resource for the next generations of speech-to-text</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language Resources and Evaluation (LREC)</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="69" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
				<meeting><address><addrLine>San Diego, CA; Barcelona</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
	<note>Neural Information Processing Systems Conference (NIPS)</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Structured-based curriculum Learning for end-to-end English-Japanese Speech translation</title>
		<author>
			<persName><forename type="first">Takatomo</forename><surname>Kano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sakriani</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the International Speech Communication Association (InterSpeech)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2630" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><forename type="middle">L</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<meeting><address><addrLine>Banff</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Augmenting Librispeech with French translations: A multimodal corpus for direct speech translation evaluation</title>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Ali Can Kocabiyikoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName><surname>Kraif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language Resources and Evaluation (LREC)</title>
				<meeting><address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Open-Subtitles2016: Extracting large parallel corpora from movie and TV subtitles</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Lison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Language Resources and Evaluation (LREC)</title>
				<imprint>
			<publisher>Portorož</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="923" to="929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">XNMT: The eXtensible Neural Machine Translation toolkit</title>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Sperber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarguna</forename><surname>Padmanabhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Singh Sachan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachid</forename><surname>Riad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liming</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the Association for Machine Translation in the Americas (AMTA) Open Source Software Showcase</title>
				<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving lexical choice in neural machine translation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Toan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter</title>
				<meeting><address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploiting linguistic resources for neural machine translation using multi-task learning</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunah</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Machine Translation (WMT)</title>
				<meeting><address><addrLine>Copenhagen</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="80" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Librispeech: An ASR corpus based on public domain audio books</title>
		<author>
			<persName><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
				<meeting><address><addrLine>Brisbane</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved speech-totext translation with the Fisher and Callhome Spanish-English speech translation corpus</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damianos</forename><surname>Karakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Spoken Language Translation (IWSLT)</title>
				<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Toward robust neural machine translation for noisy input sequences</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Sperber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Spoken Language Translation (IWSLT)</title>
				<meeting><address><addrLine>Tokyo</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting><address><addrLine>Las Vegas, NV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Local monotonic attention mechanism for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">Andros</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sakriani</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Natural Language Processing (IJCNLP)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="431" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multitask learning with low-level auxiliary tasks for encoder-decoder based speech recognition</title>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the International Speech Communication Association (InterSpeech)</title>
				<meeting><address><addrLine>Stockholm</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural machine translation with reconstruction</title>
		<author>
			<persName><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence models can directly transcribe foreign speech</title>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the International Speech Communication Association (InterSpeech)</title>
				<meeting><address><addrLine>Stockholm</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deliberation networks: sequence generation</title>
		<author>
			<persName><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems Conference (NIPS)</title>
				<meeting><address><addrLine>Long Beach, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Very deep convolutional Networks for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
