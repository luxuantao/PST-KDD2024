<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contrastive Learning under Heterophily</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-03-11">11 Mar 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles Baharan Mirzasoleiman</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Contrastive Learning under Heterophily</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-03-11">11 Mar 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2303.06344v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph Neural Networks</term>
					<term>Contrastive Learning</term>
					<term>Graph Filters</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks are powerful tools for learning node representations when task-specific node labels are available. However, obtaining labels for graphs is expensive in many applications. This is particularly the case for large graphs. To address this, there has been a body of work to learn node representations in a self-supervised manner without labels. Contrastive learning (CL), has been particularly popular to learn representations in a self-supervised manner. In general, CL methods work by maximizing the similarity between representations of augmented views of the same example, and minimizing the similarity between augmented views of different examples. However, existing graph CL methods cannot learn highquality representations under heterophily, where connected nodes tend to belong to different classes. This is because under heterophily, augmentations of the same example may not be similar to each other. In this work, we address the above problem by proposing the first graph CL method, HLCL, for learning node representations, under heterophily. HLCL uses a high-pass and a low-pass graph filter to generate different views of the same node. Then, it contrasts the two filtered views to learn the final node representations. Effectively, the high-pass filter captures the dissimilarity between nodes in a neighborhood and the low-pass filter captures the similarity between neighboring nodes. Contrasting the two filtered views allows HLCL to learn rich node representations for graphs, under heterophily and homophily. Empirically, HLCL outperforms stateof-the-art graph CL methods on benchmark heterophily datasets and large-scale real-world datasets by up to 10%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Computing methodologies ? Neural networks; ? Mathematics of computing ? Graph Algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph neural networks (GNNs) are powerful tools for learning graph-structured data in various domains, including social networks, biological compound structures, and citation networks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22]</ref>. In general, GNNs leverage the graph's adjacency matrix to update the node representations by aggregating information from their neighbors. This can be seen as a low-pass filter that smooths the graph signals and produces similar node representations <ref type="bibr" target="#b13">[14]</ref>. GNNs have achieved great success in supervised and semi-supervised learning, where task-specific labels are available. However, obtaining high-quality labels is very expensive in many domains, specially as graphs grow larger in size. This has motivated a recent body of work on self-supervised learning on graphs that learn the representations in an unsupervised manner <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>Among the self-supervised methods, Contrastive Learning (CL) has shown a great success <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b32">33]</ref>. Contrastive learning obtains representations by maximizing the agreement between different augmented views of the same example, and minimizing agreement between differently augmented views of different examples. CL has been recently applied to graphs to learn node and graph representations <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30]</ref>. Graph CL methods work by first explicitly augmenting the input graph by altering the node features or the graph topology. Then, they learn representations by contrasting the augmented views of the graph encoded with a GNN-based encoder. Despite being successful on graphs with homophily, where neighboring nodes tend to share the same label, existing graph CL methods cannot learn high-quality representations for graphs with heterophily, where connected nodes often belong to different classes and have dissimilar features <ref type="bibr" target="#b29">[30]</ref>.</p><p>Learning self-supervised node representations under heterophily is indeed very challenging. When labels are available, they can be leveraged to learn how the neighborhood information can be aggregated to achieve a superior classification performance <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29]</ref>. However, without the label information it is not clear how the neighborhood information should be aggregated. Besides, explicit graph augmentation methods, such as topology and feature augmentation, allow better capturing the node similarities under homophily. In doing so, they can effectively boost the performance of graph CL under homophily. However, under heterophily, it is crucial to also leverage dissimilarities of nodes with their neighborhood to learn rich representations. Explicit graph augmentations cannot help capturing such dissimilarities.</p><p>In this work, we address the above challenges by proposing the first graph CL method under heterophily. The key idea of our proposed method, HLCL, is to leverage a high-pass graph filter in a typical GNN encoder to capture dissimilarities of nodes with their neighborhood and generate non-smooth node representations. The non-smooth representations are then contrasted with their smooth counterparts generated by the same encoder. This results in learning rich representations based on a combination of smooth and non-smooth components. More specifically, we use the normalized Lapcalian matrix as the high-pass filter in the GNN encoder to aggregate the node representations. The Laplacian matrix magnifies the differences in the node features in a neighborhood and makes the representations distinct. In contrast, a typical GNN encoder uses the normalized adjacency matrix as a low-pass filter to aggregate the node features with those of their neighbors, and enforce similar representations for the nodes in the same neighborhood. Maximizing the agreement between the high-pass and low-pass filtered representations enables incorporating both smooth and non-smooth components while learning the final representations. HLCL obtains state-of-the-art performance under heterophily without the need for explicit augmentation. At the same time, it can leverage explicit topology and feature augmentations to also obtain a superior performance on graphs with homophily.</p><p>We show the effectiveness of our HLCL framework through extensive experiments on graphs with heterophily and homophily for unsupervised representations learning, under the linear evaluation protocol. Our results demonstrate that, on five popular public benchmark datasets, our framework can outperform existing graph CL methods by up to 10% for representation learning on graphs with heterophily, while ensuring a comparable performance to state-ofthe-art graph CL methods on graphs with homophily. We also show that HLCL can easily scale to large real-world graphs, including Penn94, twitch-gamers, pokec, and genius, and outperform existing graph CL methods by up to 6%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>Notations. We denote by G = (V, E) an undirected graph, where V = {? 1 , ? 2 , . . . , ? ? } represents the node set, and E ? V ? V represents the edge set. We denote by ? ? ? ? {0, 1} ? ?? the symmetric adjacency matrix of graph G. That is, ? ? ? ? ? = 1 if and only if (? ? , ? ? ) ? E, and ? ? ? ? ? = 0 otherwise. We also denote the feature matrix by ? ? ? , where ? ? ? ?. ? R ? is the feature vector of the ? ?? node, and ? ? ? ? R ? is a column of the matrix and represents a graph signal. ? ? ? is the degree matrix of the graph, with ? ? ? ?? = ? ? ? ? ? ? , and N ? = { ? : ? ? ? ? ? = 1} is the neighborhood of node ?. ? ? ? is the Laplacian matrix of the graph, defined as ? ? ? = ? ? ? -? ? ?. The normalized Laplacian matrix is denoted by ? ? ? ??? = ? ? ? -1 2 ?? ?? ?? -1 2 , and the normalized adjacency matrix is defined in a similar fashion: ? ? ? ??? = ? ? ? -1 2 ?? ?? ?? - </p><formula xml:id="formula_0">???? = D-1 2 ? D-1 2 .</formula><p>Similarly, the renormalized Laplacian matrix is defined as L L L??? = ? ? ? -? ? ???? . L L L??? is a real symmetric matrix, with orthonormal eigenvectors {? ? ? ? } ? ?=1 ? R ? , and corresponding eigenvalues ? ? ? [0, 2) <ref type="bibr" target="#b5">[6]</ref>. For ? ? ???? we have ? ? ( ? ? ???? ) ? (-1, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph CL under Homophily</head><p>State-of-the-art graph CL methods work by explicitly augmenting the input graph using feature or topology augmentations, encoding the augmented graphs using a GNN-based encoder, and contrasting the encoded node representations <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>. Below, we discuss each of these steps in more detail.</p><p>Graph Augmentation. First, the input graph is explicitly augmented, by altering its topology or node features. Topology augmentation methods remove or add nodes or edges, and feature augmentation methods alter the node features by masking particular columns, dropping features at random, or randomly shuffling the node features <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>. Explicit graph augmentation methods help to better capture the similarity between the nodes. Effectively, if two nodes have similar augmentations, their representations are pulled together by the contrastive loss.</p><p>GNN Encoder. The augmented graphs are then passed through a GNN-based encoder to obtain the augmented node views. The GNN encoder produces node representations by aggregating the node features in a neighborhood as follows:</p><formula xml:id="formula_1">? ? ? ? = ? ( ? ? ???? ? ? ? ?-1 ? ? ? ?-1 ), ? ? ? 0 = ? ? ?,<label>(1)</label></formula><p>where ? ? ? ? ? is the node representations at layer ? of the encoder, ? ? ? ? ? R ? ? ?? ? -1 is the weight matrix in layer ? of the encoder, and ? is the activation function. Crucially, the Adjacency matrix ? ? ???? aggregates every node's features with the features of nodes in its immediate neighborhood. For a multi-layer graph encoder, it iteratively aggregates features in a multi-hop neighborhood of every node to learn its representation. Hence, it smooths out the node representations and produces similar representations for the nodes within the same multi-hop neighborhood.</p><p>Contrastive Loss. Finally, the contrastive loss distinguishes the representations of the same node in two different augmented views from other node representations. For example the commonly used InfoNCE loss <ref type="bibr" target="#b14">[15]</ref> is:</p><formula xml:id="formula_2">? (? ? ? ? , ? ? ? ? ) = -log ? sim(? ? ? ? ,? ? ? ? )/? ? sim(? ? ? ? ,? ? ? ? )/? + ??? ? sim(? ? ? ? ,? ? ? ? )/? + ??? ? sim(? ? ? ? ,? ? ? ? )/? ,<label>(2)</label></formula><p>where ? ? ? ? , ? ? ? ? are representations of two different augmented views of node ?, sim(? ? ? ? , ? ? ? ? ) = ? (?(? ? ?), ?(? ? ?)) where ? (., .) is the cosine similarity and ?(.) is a nonlinear projection to enhance the expression power and ? is a temperature parameter.</p><p>Intuitively, existing graph CL methods capture the similarity between nodes in a graph. Under homophily, nodes in a neighborhood have similar labels and features. Such nodes have similar augmented views. Therefore, the contrastive loss pulls the representations of the nodes in the same class together and distinguishes them from other classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graphs under Heterophily</head><p>Under heterophily, connected nodes may have different class labels and dissimilar features. For example, in a dating network, most people tend to connect with people of the opposite gender, while in protein structures, different amino acid types are more likely to connect to each other <ref type="bibr" target="#b29">[30]</ref>.</p><p>Homophily Ratio. To quantify how likely nodes with similar labels are connected in the graph, homophily ratio, ?" is defined as follows <ref type="bibr" target="#b15">[16]</ref>:</p><formula xml:id="formula_3">? = 1 |? | ?? ? ??</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of ?'s neighbors who have the same label as ?</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of ?'s neighbors</head><p>For larger values of ?, it is more likely that nodes with the same labels are connected together.</p><p>Existing graph CL methods have a very poor performance under heterophily, or low homophily ratio, and cannot learn high-quality representations, as we will discuss next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GRAPH CL UNDER HETEROPHILY</head><p>In this section, we first discuss the challenges of graph CL under heterophily or low homophily. Then, we present our approach to overcome these challenges and learn high-quality representations.</p><p>Challenges. Learning high-quality representations under heterophily, without having access to labels, is very challenging. First, under heterophily, nodes in a neighborhood have different class labels and dissimilar features. In this setting, aggregating features of nodes in a neighborhood using a GNN encoder fades out the dissimilarity between representations of nodes in different classes and makes them indistinguishable. Indeed, under heterophily, it is crucial to leverage both similarities and dissimilarities of a node to its neighborhood to obtain high-quality representations. However, without having access to the labels, it is not clear how the node features should be aggregated. In fact, even the homophily ratio cannot be calculated without having access to the labels. Second, under heterophily, explicit graph augmentations leveraged by existing graph CL methods may not be helpful in learning high-quality representations. In particular, topology augmentations such as edge drop that are most effective under homophily <ref type="bibr" target="#b31">[32]</ref>, may drastically harm the performance under heterophily (see Table <ref type="table" target="#tab_0">1</ref>). This is because nodes in a neighborhood have similar labels under homophily, but may have different labels under heterophily. Hence, while the two augmented versions of the same node with dropped edges are similar under homophily, augmented versions of the same node may not be similar under heterophily as the dropped edges may be connected to different labels. In this setting, maximizing the similarity between representations of the dissimilar augmentations results in a poor performance.</p><p>Due to the above reasons, existing graph CL methods that contrast different explicitly augmented views of a node obtained using a GNN encoder cannot yield separable representations for different classes, under heterophily. This yields a poor generalization performance for the downstream classifier, as we confirm by our experiments. Next, we discuss how we overcome the above challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph CL via Graph Filters</head><p>As discussed, under heterophily, it is crucial to leverage both similarities and dissimilarity between the node features in a neighborhood to be able to learn high-quality representations. However, without having access to the labels, it is not clear how the node features in a neighborhood should be aggregated. To address the above challenges, our key idea is to generate two filtered representations for every node using the same encoder. In particular, the first (nonsmooth) representation is generated using a high-pass filter which captures the dissimilarity of the node feature to its neighborhood. The second (smooth) representation is generated using a low-pass filter which captures the similarity of the node feature to its neighborhood. We note that the smooth representation is similar to the representation learned by a typical GNN-based encoder. Then, we learn the final node representations by contrasting the smooth and non-smooth representations. In doing so, we can learn rich node representation based on a combination of the smooth and non-smooth components.</p><p>Next, we introduce high-pass and low-pass graph filters, and discuss how they can be leveraged to learn smooth and non-smooth node representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">High-pass and Low-pass graph filters</head><p>The adjacency and Laplacian matrices can be leveraged to filter the smooth and non-smooth graph components, and capture similarity and dissimilarity of node features to their neighborhoods.</p><p>Specifically, multiplication of Laplacian with a graph signal L L L??? ? ? ? = ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?, acts as a filtering operation over ? ? ?, adjusting the scale of the components of ? ? ? in the frequency domain. The entries of every eigenvector, ? ? ? ? aligns with a cluster of connected nodes in the graph. For the Laplacian matrix, a smaller eigenvalue ? ? corresponds to a smoother eigenvector ? ? ? ? , and corresponds to a larger cluster of connected nodes. On the other hand, a larger ? ? corresponds to non-smooth eigenvectors ? ? ? ? , which identify smaller clusters of closely connected nodes in the graph. A Laplacian filter magnifies the part of the signal that aligns well with basis functions corresponding to large eigenvalues ? ? ? (1, 2) and suppresses the part the signal that aligns with basis functions corresponding to small eigenvalues ? ? ? [0, 1]. That means, for the small clusters of nodes that have a large alignment with ? ? ? ? corresponding to ? ? &gt; 1, the projection ? ? ? ? ? ? ? ? ? ? ? ? ? ? amplifies ? ? ? within the cluster and consequently magnifies the difference in ? ? ? among the nodes within that cluster. On the other hand, for the larger clusters that align well with ? ? ? ? corresponding to ? ? &lt; 1, the projection ? ? ? ? ? ? ? ? ? ? ? ? ? ? suppresses ? ? ? within the cluster and reduces the differences in ? ? ? among the nodes within that cluster. Hence the Laplacian matrices can be generally regarded as high-pass filters <ref type="bibr" target="#b7">[8]</ref>, that enlarge the differences in node features over small clusters, and smooths out the differences over larger clusters in the graph. On the other hand, affinity matrices, such as the normalized adjacency matrix, can be treated as low-pass filters <ref type="bibr" target="#b13">[14]</ref>, which suppress and filter out non-smooth components of the signals. This is because all of the eigenvalues of the affinity matrices are smaller than 1, i.e., ? ? ? (-1, 1].</p><p>On the node level, left multiplying L L L??? and ? ? ???? filters with ? ? ? can be understood as diversification and aggregation operations, respectively <ref type="bibr" target="#b12">[13]</ref>. In particular, a typical GNN filters smooth graph frequencies by aggregating the node representations with those of their neighbors, using the adjacency matrix, i.e.</p><formula xml:id="formula_4">( ? ? ???? ? ? ?) ? = ?? ? ?N ? 1 ? ? ? ?? ? ? ? ? .<label>(3)</label></formula><p>Hence, it results in similar representations for the nodes in a neighborhood. In contrast, the high-pass filter only preserves the highpass frequencies, using the Laplacian matrix, i.e.</p><formula xml:id="formula_5">( L L L??? ? ? ?) ? = ?? ? ?N ? 1 ? ? ? ?? (? ? ? ? -? ? ? ? ).<label>(4)</label></formula><p>In doing so, it magnifies the dissimilarities between the nodes and make the representations of nodes in a neighborhood distinguishable.</p><p>Since L L L??? + ? ? ???? = ? ? ? , both filters capture complementary information and their combination using a contrastive loss allow learning richer representations, as we discuss next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generating Graph Views via Graph Filters</head><p>As discussed, the key idea of our HLCL framework is to leverages a high-pass filter to generate diverse and distinguishable node representations, and contrast them with smooth and similar node representations generated by a low-pass filter. To do so, we leverage the normalized Laplacian matrix L L L??? and adjacency matrix ? ? ???? as the diversification and aggregation operations in Eq. ( <ref type="formula" target="#formula_4">3</ref>), (4). We note that other types of high-pass and low-pass filters can be used in a similar way in our framework.</p><p>More specifically, we input the graph ? ? ? into a graph encoder, and apply a high-pass filter ? ? ? ? ? = L L L??? and a low-pass filter ? ? ? ?? = ? ? ???? to the encoder to generate high-pass node representations ? ? ? ? and low-pass node representations ? ? ? ? as follows:</p><formula xml:id="formula_6">? ? ? ? ? = ? (? ? ? ? ? ? ? ? ?-1 ? ? ? ? ?-1 ),<label>(5)</label></formula><formula xml:id="formula_7">? ? ? ? ? = ? (? ? ? ?? ? ? ? ?-1 ? ? ? ? ?-1 ),<label>(6)</label></formula><p>where ? ? ? ? ? , and ? ? ? ? ? are the low-pass filtered and high-pass filtered representations at layer ? of the encoder, ? ? ? ? ? R ? ? ?? ? -1 is the weight matrix in layer ? of the encoder, ? is the activation function, and we have ? ? ? 0 ? = ? ? ? 0 ? = ? ? ? .</p><p>The high-pass filter ? ? ? ? filters out the low-frequency signals and preserves the high-frequency signals. In doing so, it captures the difference between features of every node with the features of the nodes in its neighborhood. Using a high-pass encoder within a multi-layer encoder iteratively captures the difference between features of the nodes in a multi-hop neighborhood of a node. Hence, it makes the representations of nodes that have different features from their neighbors distinct in their multi-hop neighborhood.</p><p>On the other hand, the low-pass filter, ? ? ? ? , resembles a typical GNN, which only preserves the low-frequency signals by aggregating every node's features with the features of nodes in its immediate neighborhood. Using the low-pass filter within a multi-layer graph encoder, it iteratively aggregates features in a multi-hop neighborhood of every node to learn its representation. Hence, it smooths out the node representations and produces similar representations for the nodes within the same multi-hop neighborhood.</p><p>While the low-pass filter is essential for learning good representations in smooth graphs, it cannot produce distinguished representations for graphs that are non-smooth, especially under heterophily. For such graphs, the high-pass filter is crucial to provide distinct representations. The combinations of both filters provide complementary information and allow learning both smooth and non-smooth components of the graphs simultaneously. Hence, it enables learning richer representations, particularly under heterophily.</p><p>Scalability via Message Passing. The high-pass and low-pass filtered representations can be obtained through message passing in an inductive manner, according to Eq. ( <ref type="formula" target="#formula_4">3</ref>), ( <ref type="formula" target="#formula_5">4</ref>), without the need to explicitly calculate the Laplacian matrix. In particular, the high-pass filtered representations can be obtained by iteratively differentiating the representations of a node and those of its neighbors, and the low-pass filtered representations can be obtained by aggregating the node's representation with those of its neighbors. Formally: ? ? ??</p><formula xml:id="formula_8">? = ? (? ? ? ?-1 ? ? ? ?-1 ? ),<label>(7)</label></formula><p>(? ? ? ? ? ) ? = ? ? ? {N ? ?{? } } ( ? ? ?? ? + ? ? ?? ? ),</p><p>(? ? ? ? ? ) ? = ? ? ? {N ? ?{? } } ( ? ? ?? ? -? ? ?? ? ).</p><p>This is the same approach used to train common GNNs on large graphs. Hence, our method will have the same complexity as conducting a normal GNN message passing with an additional message being passed. This makes our method scalable to large graphs, as we also confirm in our experiments.</p><p>Explicit graph augmentation. We note that the graph ? ? ? can be optionally explicitly augmented before entering the encoder. While explicit augmentation do not provide a considerable benefit and can even be harmful under heterophily, it is crucial for graphs with homophily. We will study in details the effect of explicit augmentation under homophily and heterophily, in our experiments.</p><p>Our HLCL framework is demonstrated in Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The Contrastive Learning Framework</head><p>After generating the low-pass and high-pass filtered graph views, we pass them through a shared non-linear projection head and Algorithm 1 Contrastive Learning with Graph Filters (HLCL)</p><p>1: for epoch=1,2,3,... do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2:</head><p>Input graph G into the shared graph Encoder ? (?)</p><p>3:</p><p>Generate high-frequency-signal node embeddings: ? ? ? ? ? = ? (? ? ? ? ? ? ? ? ?-1  ? ? ? ? ?-1 )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Generate low-frequency-signal node embeddings: ? ? ? ? ? = ? (? ? ? ?? ? ? ? ?-1  ? ? ? ? ?-1 )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Compute the contrastive objective L with Eq. (</p><p>Update parameters by applying stochastic gradient ascent to minimize L 7: end for employ a contrastive objective that enforces the filtered representations of each node in the two views to agree with each other.</p><p>Non-linear projection head. Before contrasting the high-pass and low-pass filtered representations, we pass them to a non-linear projection head ?(?), which maps the filtered views to another latent space where the contrastive loss is calculated. This has been shown to improve the performance <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref>, and we also confirm its effect in our experiment. In our setting, we use a two-layer perceptron network to obtain ? ? ? ? =?(? ? ? ? ) and ? ? ? ? =?(? ? ? ? ). We use the projection head only during the contrastive training, and use the low-pass filtered representations ? ? ? ? in the downstream task.</p><p>Contrastive loss. For every node ?, its projected representation generated by the low-pass filter, ? ? ? ? ? , is treated as the anchor, and the one generated by the high-pass filter, ? ? ? ? ? , is treated as the corresponding positive sample. Formally, we have:</p><formula xml:id="formula_12">? (? ? ? ? ? ,? ? ? ? ? ) = sim(? ? ? ? ? ,? ? ? ? ? )/?,<label>(10)</label></formula><p>where sim(? ? ? ? ? ,? ? ? ? ) is the cosine similarity between ? ? ? ? ? and ? ? ? ? ? , and ? is a temperature parameter.</p><p>Notably, the high-pass filter alleviates the need for using negative pairs in the contrastive loss, by automatically producing dissimilar representations for different nodes. Empirically, we also observed no improvement in the performance by incorporating the negative pairs, in presence of the high-pass filter.</p><p>Since two views are symmetric, the loss for another view, ? (? ? ? ? ? ,? ? ? ? ? ), is defined in a similar fashion. The overall objective to be maximized is then defined as the average over all positive pairs. Formally, we minimize:</p><formula xml:id="formula_13">L = - 1 2? ? ?? ?=1 [? (? ? ? ? ? ,? ? ? ? ? ) + ? (? ? ? ? ? ,? ? ? ? ? )].<label>(11)</label></formula><p>Effectively, maximizing the agreement between the low-pass and high-pass views pulls away the representation of nodes with different features from their neighborhood, and allows them to be distinguished from their neighbors.</p><p>Final representations. After minimizing the contrastive loss in Eq. ( <ref type="formula" target="#formula_11">11</ref>), we use the output of the low-pass filter (the typical GNN) as the final representation. Note that the parameters of the encoder are learned by contrasting the low-pass representation with highpass representations. Hence, the output of the low-pass encoder is different than that of existing graph CL methods that only use GNNbased (low-pass) encoders during contrastive learning. Indeed, we do not need to rely on the high-pass filter during the inference. The pseudocode is illustrated in Alg. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Effect of HP and LP Filters under Different Homophily Ratios</head><p>In general, contrasting the high-pass filtered view with its low-pass counterpart pushes away the representations of nodes that have dissimilar features to the other nodes in their neighborhood, and allows them to be distinguished better. In particular, the high-pass filter is essential for separating the classes under heterophily, and the low-pass filter is essential for learning high-quality representations under homophily. Nevertheless, real-world networks often have neighborhoods with medium homophily ratio (see Table <ref type="table" target="#tab_1">2</ref>, 3). In such cases, both filters are essential for learning rich representations. Here, we discuss the effect of high-pass and low-pass filters in neighborhoods with different homophily ratios, and explain why contrasting the filtered representations does not result in a significant harm in high-or low-homophily neighborhoods.</p><p>Heterophily. Under medium or low homophily ratio, a fraction of nodes in the neighborhood have different labels and dissimilar features to the rest of the nodes in the same neighborhood. As long as homophily ratio is not nearly zero, both filters are essential to obtain rich and separable representations. In particular, the low-pass filter is required to aggregate features of similar nodes, and the highpass filter is required to distinguish features of dissimilar nodes in the same neighborhood. By contrasting the high-pass and low-pass filtered features, HLCL is able to learn rich and distinguishable class representations under heterophily. Note that structural augmentation can drastically harm the performance under heterophily, as it reduces the effectiveness of the graph filters (see Table <ref type="table" target="#tab_3">4</ref>). Additional feature augmentation, in particular on the low-pass filter can slightly boost the performance.</p><p>Homophily. Under high homophily ratio, the low-pass filter aggregates the node features in a neighborhood and makes their representations similar to each other. On the other hand, the high-pass filter magnify the dissimilarity between node features in a neighborhood. In this case, explicit structural graph augmentations, in particular dropping edges or nodes, effectively alleviate the adverse effect of the high-pass filter and considerably boost the performance, as can be seen in Table <ref type="table" target="#tab_3">4</ref>. Additional features and structural augmentation on the low-pass filter can further improve the quality of the learned representations, as also confirmed by prior work <ref type="bibr" target="#b31">[32]</ref>. Fig. <ref type="figure" target="#fig_1">2</ref> compares t-SNE visualization of the learned representations with HLCL compare to GRACE, which is a state-of-the-art Graph CL method. We see that under heterophily (Chameleon), HLCL representations of every class form smaller compact clusters, which allows the downstream linear model to separate the classes. In contrast, GRACE representations are spread out and classes are not easily separable. In addition, under homophily (Cora), the high-pass filter combined with explicit graph augmentations allows HLCL to obtain separable high-quality class representations. Effectively, HLCL achieves nearly the same downstream performance as GRACE under homophily, as we confirm by our experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we evaluate the node representations learned with HLCL, under linear probe. We compare HLCL with existing graph CL methods, and conduct an extensive ablation study to evaluate the effect of each of HLCL's components.</p><p>Datasets. We conduct our experiments on five widely-used public benchmark datasets with different levels of homophily ratios, ?. We show the statistics of the datasets including ?, the number of nodes, edges, and classes, in table <ref type="table" target="#tab_1">2</ref>. For graphs with homophily, we use the citation networks including Cora and Citeseer <ref type="bibr" target="#b26">[27]</ref>. For graphs with heterophily, we use the Wikipedia network and the web page networks including Chameleon, Squirrel, and Texas <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20]</ref>. In addition, to illustrate the scalability of HLCL, we apply it to largescale real-world datasets recently provided by <ref type="bibr" target="#b11">[12]</ref>. The statistics of the datasets are shown in Table <ref type="table" target="#tab_2">3</ref>. We repeat the experiments 10 times for smaller benchmark datasets, and 3 times for large real-world datasets, using early-stopping, and report the average accuracy as the final result. For each run, following CPGNN <ref type="bibr" target="#b28">[29]</ref> and GRACE <ref type="bibr" target="#b32">[33]</ref>, we randomly select 10% nodes for training, 10% nodes for validation, and 80% nodes for testing.</p><p>HLCL setup. We employ a two-layer GNN <ref type="bibr" target="#b10">[11]</ref> as our low-pass encoder, and consider an additional high-pass channel with L L L??? as the message passing filter to capture the high-frequency signals from the graphs. The high-pass channel generates augmented node representations to be contrasted with those encoded by the GNN. The high-pass channel shares the same weight parameters with the GNN, but generate representations with different filters. The final representations is generated by the GNN encoder.</p><p>Linear Probe Evaluation. We follow the evaluation protocol used in <ref type="bibr" target="#b32">[33]</ref>. Models are first trained in a self-supervised manner without labels. Then, we fed the final node embeddings into a ? 2 -regularized logistic regression classifier to fit the labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Graph CL under Heterophily</head><p>HLCL vs Explicit Augmentation. First, we compare HLCL with a state-of-the-art graph CL method, namely GRACE <ref type="bibr" target="#b32">[33]</ref> with different explicit graph augmentations, under heterophily. We consider topology augmentation methods including Edge Removing (ER),  Node Dropping (ND), Edge Adding (EA), Subgraph with Random Walk (RWS), and Personalized PageRank (PPR), as well as feature augmentation method including Feature Masking (FM) and Feature Dropping (FD) for generating different graph augmentations. We compare these methods with HLCL. Except for the proposed highpass filter, we do not use any other explicit graph augmentation. Table <ref type="table" target="#tab_0">1</ref> shows that our method achieves a remarkable boost of up to 10% compared to the best graph augmentation method. This confirms the necessity of utilizing the non-smooth graph components under heterophily. Note that even though topology and feature augmentation methods benefit the homophily graphs, they are highly ineffective for graph CL under heterophily. HLCL vs Self-supervised Baselines. Next, we compare HLCL with existing baselines for self-supervised representation learning. We consider traditional representation learning methods like DeepWalk <ref type="bibr" target="#b17">[18]</ref>, deep learning methods including DGI <ref type="bibr" target="#b22">[23]</ref>, BGRL <ref type="bibr" target="#b20">[21]</ref>, and GRACE <ref type="bibr" target="#b32">[33]</ref>, and other popular representation learning baselines like Raw Features and DeepWalk + Raw Features as baselines. We also include MixHop <ref type="bibr" target="#b0">[1]</ref> in our experiments as the supervised learning baseline. Table <ref type="table" target="#tab_1">2</ref>  of HLCL when used on its own, HLCL (no-aug), and when it is applied to the graphs that are augmented with Edge Removing (ER) on the graphs with homophily and Feature Masking (FM) on the graphs with heterophily. We see that HLCL shows a significant boost on graphs with heterophily and a comparable performance on the graphs with homophily. Even without explicit augmentation, HLCL surpasses other baselines on graphs with heterophily, which confirms the effectiveness of the high-pass channel. Importantly, applying HLCL to graphs that are explicitly augmented achieves a comparable performance under homophily. Thus, the high-pass filter combined with explicit augmentations does not harm the performance on such graphs. Compared to supervised learning model such as MixHop trained in an end-to-end manner, HLCL achieves a comparable or superior performances. This, demonstrates the effectiveness of our design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>compares the performance</head><p>Large-scale Experiments Next, to confirm effectiveness and scalability of HLCL, we apply it to large-scale real-world datasets. Table <ref type="table" target="#tab_2">3</ref> shows that HLCL achieves up to 6% higher accuracy on large-scale datasets compared to baselines, and easily scales to large graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Studies</head><p>HLCL with Explicit Augmentation Next, we study the effect of explicit graph augmentation methods applied to both high-pass and the typical low-pass GNN channels of HLCL, under different homophily ratios. We include Edge Removing (ER), Node Dropping (ND), Edge Adding (EA), Feature Masking (FM), and Feature Dropout (FD) as graph augmentation methods. Table <ref type="table" target="#tab_3">4</ref> shows that combined with HLCL, feature and topology augmentations considerably benefit the performance on graphs with homophily. Topology augmentation, however, may not be helpful under heterophily.</p><p>Output Encoder with Different Graph Filters In our experiments, we use low-pass graph filter in our GNN encoder as the final representation encoder. To illustrate the effectiveness of our structural design, we present an ablation study on using different   filters or combinations of filters in our final encoder. The result is shown in table <ref type="table" target="#tab_4">5</ref>. We see that on most datasets low-pass encoder achieves the highest accuracy with a large gap, and variation involving the high-pass encoder has worse performances across all datasets except Texas, which has a nearly zero homophily ratio. This demonstrates the ineffectiveness of using high-pass filter directly in the representation encoder.</p><p>Existing Graph CL methods with high-pass encoders Finally, we illustrate the importance of our contrastive structure. We show that the main accuracy gains on heterophily datasets are from contrasting the low-and high-pass filtered graph views, instead of the introduction of the high-pass filter alone. Indeed, the high-pass filter alone cannot provide high-quality representations under heterophily, as different combinations of low-pass and high-pass filters are required to provide good representations for different graphs. Table <ref type="table" target="#tab_5">6</ref> shows that by replacing low-pass filter with high-pass filter in other graph CL methods, such methods are able to achieve a better accuracy on some heterophily datasets like Squirrel and Texas. However, their performances deteriorate significantly under homophily cases, where aggregation among neighborhoods are desired. Indeed, when using high-pass or low-pass alone, the model cannot achieve good performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>(Semi-)supervised learning on graphs. In recent years, GNNs have become one of the most prominent tools for processing graphstructured data. In general, GNNs utilize the adjacency matrix to learn the node representations, by aggregating information within every node's neighborhood <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref>. variants, including GraphSAGE <ref type="bibr" target="#b8">[9]</ref>, Graph Attention (GAT) <ref type="bibr" target="#b21">[22]</ref>, MixHop <ref type="bibr" target="#b0">[1]</ref>, SGC <ref type="bibr" target="#b13">[14]</ref>, GAT <ref type="bibr" target="#b22">[23]</ref>, and GIN <ref type="bibr" target="#b24">[25]</ref>, learn a more general class of neighborhood mixing relationships, by aggregating weighted information within a multi-hop neighborhood of every node. GNNs can be generally seen as applying a fix, or a parametric and learnable (e.g. GAT) low-pass graph filter to graph signals. Those with trainable parameters can adapt to a wider range of frequency levels on different graphs. However, they still have a higher emphasis on lower-frequency signals and discard the high-frequency signals in a graph. While the aggregation operation makes GNNs powerful tools for semi-supervised learning, it can make the learned node representations indistinguishable in a neighborhood <ref type="bibr" target="#b13">[14]</ref>. As a result, typical GNNs and their variants have been long criticized for their poor generalization performance under heterophily <ref type="bibr" target="#b1">[2]</ref>.</p><p>(Semi-)supervised learning under heterophily. To address over-smoothing issue of GNNs, recent methods propose to use other types of aggregation that better fit graphs with heterophily. Geom-GCN uses geometric aggregation in place of the typical aggregation <ref type="bibr" target="#b15">[16]</ref>, H 2 GCN uses several special model designs including separate aggregation and higher-neighborhood aggregation to train the model for handling graphs with heterophily, and CPGNN trains a compatibility matrix to model the heterophily level <ref type="bibr" target="#b28">[29]</ref>. More recently, <ref type="bibr" target="#b23">[24]</ref> proposed to learn an aggregation filter for every graph from a set of based filters designed based on different ways of normalizing the adjacency matrix. Most recently, GGCN introduced degree corrections and signed message passing on GCN to address both oversmoothing problems and the model's poor performances on heterophily graphs <ref type="bibr" target="#b25">[26]</ref>. <ref type="bibr" target="#b30">[31]</ref> analyzed and designed a uniform framework for GNNs propagations and proposed GNN-LF and GNN-HF that preserve information of different frequency separately by using different filtering kernels with learnable weights. FAGCN <ref type="bibr" target="#b3">[4]</ref> and FBGNN <ref type="bibr" target="#b12">[13]</ref> train two separate encoders to capture the high-pass and low-pass graph signals separately. Then they rely on labels to learn relatively complex mechanisms to combine the outputs of the encoders. However, learning how to combine the encoder outputs is highly sensitive to having high-quality labels. This makes such methods highly impractical for unsupervised contrastive learning, where the label information is not available. Unlike the above supervised methods, we leverage the highpass filter in the same encoder to generate augmented graph views that are contrasted with their low-pass counterparts to learn unsupervised representations. This is in contrast to learning the best combination of filtered signals of different encoders based on labels.</p><p>Contrastive learning on graphs. For contrastive learning on graphs, global graph-level and local node-level data are augmented and contrasted in different ways. DGI <ref type="bibr" target="#b22">[23]</ref> and GMI <ref type="bibr" target="#b16">[17]</ref> contrast graph and node representations within one augmented view of the original graph. More recent methods contrast global and local representations in two augmented views. GraphCL generates graph augmentations by subgraph sampling, node dropping, and edge perturbation and contrasts the augmented graph representations. GCC samples and contrasts subgraphs of the original graph <ref type="bibr" target="#b18">[19]</ref>. MVGRL leverages node diffusion to augment the graph and contrasts the node representations <ref type="bibr" target="#b9">[10]</ref>. More recently, contrasting the local node representations has been shown to achieve state-of-the-art. GRACE contrasts the node representations in two graph views augmented with feature masking and edge removal <ref type="bibr" target="#b32">[33]</ref>. GCA extends this by dropping the less important edges and features, based on node centrality and feature importance metrics <ref type="bibr" target="#b33">[34]</ref>. A thorough empirical study on the combinatorial effect of different augmentations has been conducted by <ref type="bibr" target="#b31">[32]</ref>. Due to the complexity of collecting negative samples in graph data, negative-samples-free contrastive objectives have been also studied. Among existing methods, BGRL that uses the Bootstrapping Latent loss <ref type="bibr" target="#b20">[21]</ref>, and GBT uses Barlow Twins loss <ref type="bibr" target="#b2">[3]</ref> are the most successful. Existing graph CL methods explicitly augment the input graph and contrast the augmented graph representations obtained with low-pass GNN-based encoders. In doing so, they only capture the similarity of nodes in a neighborhood. Hence, they perform poorly on graphs with heterophily. In contrast, we leverage low-pass and high-pass graph filters in the same GNN-based encoder to capture and contrast similarity and dissimilarity of nodes with their neighborhood. This allows achieving state-of-the-art under heterophily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We proposed HLCL, a contrastive learning framework that leverages a high-pass graph filter to generate non-smooth augmented representations to be contrasted with their smooth counterparts, generated using the same encoder. This is particularly beneficial for graphs with heterophily, where existing graph CL methods are highly ineffective. Through extensive experiments, we demonstrated that our proposed framework achieves up to 10% boost in the performance for representation learning under the linear probe, for various graphs under heterophily. At the same time, it provides a comparable performance on graphs with homophily. We believe our work provides an important direction for future work on contrastive learning under heterophily.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Our proposed HLCL framework leverages a high-pass graph filter in the encoder to generate a non-smooth graph view, and contrast it with its smooth counterparts generated by the same encoder.</figDesc><graphic url="image-1.png" coords="2,93.97,77.87,420.85,179.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 2 .</head><label>2</label><figDesc>Here, we use the renormalized version of the adjacency matrix ? ? ???? = D D D-1 2 ? ? ? D D D-1 2 as introduced in [11], where ? ? ? = ? ? ? + ? ? ? , D D D = ? ? ? + ? ? ? , and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: t-SNE visualization of final representations of chameleon and cora generated by HLCL and GRACE.</figDesc><graphic url="image-4.png" coords="6,77.61,223.28,239.59,136.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Existing graph augmentation methods are highly ineffective for graph CL under heterophily. HLCL achieves up to 10% improvement, over the best graph augmentation.</figDesc><table><row><cell></cell><cell></cell><cell>Chameleon</cell><cell>Squirrel</cell><cell>Texas</cell></row><row><cell></cell><cell cols="4">HLCL (no-aug) 48.03 ? 3.1 33.44 ? 1.7 64.21 ? 11.2</cell></row><row><cell></cell><cell>ER</cell><cell>34.23 ? 3.8</cell><cell>25.14 ? 2.3</cell><cell>52.63 ? 10.7</cell></row><row><cell></cell><cell>ND</cell><cell>36.02 ? 1.9</cell><cell>26.70 ? 1.8</cell><cell>50.53 ? 12.2</cell></row><row><cell>Topo</cell><cell>EA</cell><cell>38.20 ? 3.1</cell><cell>26.92 ? 1.6</cell><cell>54.73 ? 9.7</cell></row><row><cell></cell><cell>RWS</cell><cell cols="3">35.63 ? 3.3 24.59 ? 13.5 56.84 ? 11.7</cell></row><row><cell></cell><cell>PPR</cell><cell>34.14 ? 3.5</cell><cell>24.43 ? 2.9</cell><cell>50.00 ? 6.7</cell></row><row><cell>Feat</cell><cell>FM FD</cell><cell>37.55 ? 3.6 36.81 ? 3.9</cell><cell cols="2">25.58 ? 1.1 58.42 ? 14.21 25.75 ? 2.7 50.52 ? 6.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>HLCL achieves state-of-the-art under heterophily, and a comparable performance under homophily when combined with explicit graph augmentation.</figDesc><table><row><cell></cell><cell cols="2">Homophily</cell><cell></cell><cell>Heterophily</cell><cell></cell></row><row><cell></cell><cell>Cora</cell><cell>CiteSeer</cell><cell>Chameleon</cell><cell>Squirrel</cell><cell>Texas</cell></row><row><cell>Hom.(?)</cell><cell cols="4">0.83 ? .086 0.71 ? .16 0.25 ? .052 .22 ? .038</cell><cell>.06 ? .032</cell></row><row><cell>Nodes</cell><cell>2708</cell><cell>3327</cell><cell>2277</cell><cell>5201</cell><cell>183</cell></row><row><cell>Edges</cell><cell>5278</cell><cell>4676</cell><cell>31421</cell><cell>198493</cell><cell>295</cell></row><row><cell>Classes</cell><cell>6</cell><cell>7</cell><cell>5</cell><cell>5</cell><cell>5</cell></row><row><cell>HLCL</cell><cell cols="5">82.34 ? 2.7 71.35 ? 1.4 49.78 ? 1.8 34.11 ? 1.9 65.26 ? 7.8</cell></row><row><cell>HLCL (no-aug)</cell><cell>77.09 ? 1.7</cell><cell>64.34 ? 1.5</cell><cell cols="3">48.04 ? 3.1 33.44 ? 1.7 64.21 ? 11.2</cell></row><row><cell>Raw feature</cell><cell>64.8</cell><cell>64.6</cell><cell>31.61</cell><cell>24.43</cell><cell>52.63</cell></row><row><cell>DeepWalk</cell><cell>75.7 ? 1.7</cell><cell>50.5 ? 1.6</cell><cell>27.51 ? 3.1</cell><cell cols="2">25.47 ? 2.4 40.00 ? 15.9</cell></row><row><cell>DeepWalk + features</cell><cell>73.1 ? 2.3</cell><cell>47.6 ? 1.5</cell><cell>36.21 ? 4.6</cell><cell cols="2">26.23 ? 1.8 53.68 ? 10.2</cell></row><row><cell>DGI</cell><cell>82.60 ? 0.4</cell><cell>68.80 ? 0.7</cell><cell>40.48 ? 1.9</cell><cell>27.95 ? 0.9</cell><cell>60.00 ?10.8</cell></row><row><cell>BGRL</cell><cell>74.67 ? 0.6</cell><cell>64.25 ? 2.4</cell><cell>38.20 ? 1.4</cell><cell cols="2">26.03 ? 0.9 61.05 ? 11.1</cell></row><row><cell>GRACE</cell><cell cols="2">83.30 ? 0.4 72.10 ? 0.5</cell><cell>37.37 ? 0.6</cell><cell>28.67 ? 0.9</cell><cell>62.6 ? 7.2</cell></row><row><cell>MixHop</cell><cell cols="2">85.34 ? 0.4 73.23 ? 0.5</cell><cell cols="2">46.84 ? 3.5 36.42 ? 3.4</cell><cell>62.15 ? 2.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>HLCL achieves a noticeably higher accuracy on realistic large-scale datasets compared to other GCL methods. HLCL outperforms SOTA Graph CL methods by up to 6%.</figDesc><table><row><cell></cell><cell>Penn94</cell><cell>Twitch-Gamers</cell><cell>Pokec</cell><cell>Genius</cell></row><row><cell cols="2">Hom.(?) 0.48 ? .042</cell><cell>0.55 ? .040</cell><cell>0.42 ? .10</cell><cell>0.48 ? .21</cell></row><row><cell>Nodes</cell><cell>41,554</cell><cell>168,114</cell><cell>1,632,803</cell><cell>421,961</cell></row><row><cell>Edges</cell><cell>1,362,229</cell><cell>6,797,557</cell><cell>1,632,803</cell><cell>984,979</cell></row><row><cell>Classes</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell></row><row><cell>HLCL</cell><cell>68.12 ? 3.5</cell><cell>66.96 ? 0.9</cell><cell cols="2">56.50 ? 1.7 85.39 ? 2.4</cell></row><row><cell>DGI</cell><cell>62.86 ? 0.37</cell><cell>62.47 ? 2.8</cell><cell>54.56 ? 0.6</cell><cell>83.45 ? 1.8</cell></row><row><cell>BGRL</cell><cell>58.82 ? 0.6</cell><cell>55.09 ? 2.3</cell><cell>53.53 ? 1.2</cell><cell>76.38 ? 2.6</cell></row><row><cell cols="2">GRACE 62.52 ? 0.15</cell><cell>57.08 ? 0.11</cell><cell cols="2">53.61 ? 0.59 79.58 ? 2.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The result on combining HLCL with different graph view augmentations. HP represents augmentation on the highpass channel only, LP represents augmentation on the low-pass channel only, HL represents augmentation on both channels. Both topology and feature augmentations are included in the experiments. The worst results are highlighted in gray.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Homophily</cell><cell></cell><cell>Heterophily</cell></row><row><cell></cell><cell></cell><cell>Cora</cell><cell>CiteSeer</cell><cell>Chameleon</cell><cell>Squirrel</cell><cell>Texas</cell></row><row><cell></cell><cell cols="3">HLCL (no-aug) 77.09 ? 1.7 64.34 ? 1.5</cell><cell>48.03 ? 3.1</cell><cell>33.44 ? 1.7</cell><cell>64.21 ? 11.2</cell></row><row><cell>HP Topo</cell><cell>ER ND</cell><cell cols="2">80.80 ? 3.0 71.20 ? 1.3 78.42 ? 2.0 70.24 ? 2.0</cell><cell>36.11 ? 4.1 35.80 ? 2.7</cell><cell>26.26 ? 2.3 26.79 ? 1.6</cell><cell>57.36 ? 8.90 61.05 ? 12.0</cell></row><row><cell></cell><cell>EA</cell><cell cols="2">74.04 ? 2.9 65.75 ? 3.2</cell><cell>35.81 ? 3.3</cell><cell>26.79 ? 2.0</cell><cell>60.53 ? 7.90</cell></row><row><cell>HP Feat</cell><cell>FM FD</cell><cell cols="2">77.13 ? 2.9 64.67 ? 1.3 77.94 ? 2.6 66.05 ? 1.8</cell><cell>49.08 ? 3.8 47.07 ? 4.0</cell><cell>33.80 ? 1.7 32.71 ? 2.0</cell><cell>62.63 ? 9.8 61.57 ? 6.2</cell></row><row><cell>LP Topo</cell><cell>ER ND</cell><cell cols="2">77.94 ? 3.1 65.18 ? 1.9 75.66 ? 1.6 61.28 ? 2.6</cell><cell>37.07 ? 3.4 36.15 ? 2.0</cell><cell>25.36 ? 2.0 26.53 ? 1.6</cell><cell>57.37 ? 7.60 59.47 ? 8.50</cell></row><row><cell></cell><cell>EA</cell><cell cols="2">80.73 ? 2.2 68.35 ? 2.0</cell><cell>39.08 ? 2.0</cell><cell>26.75 ? 1.5</cell><cell>61.05 ? 11.8</cell></row><row><cell>LP Feat</cell><cell>FM FD</cell><cell cols="2">78.46 ? 2.4 66.68 ? 1.7 78.42 ? 2.7 67.75 ? 1.7</cell><cell>49.78 ? 1.8 47.46 ? 3.2</cell><cell>34.11 ?1.9 33.65 ? 1.7</cell><cell>65.26 ? 7.8 63.15 ? 7.4</cell></row><row><cell>HL Topo</cell><cell>ER ND</cell><cell cols="2">82.34 ? 2.7 71.35 ? 1.4 76.10 ? 2.6 65.90 ? 2.6</cell><cell>37.29 ? 2.6 35.81 ? 1.8</cell><cell>25.60 ? 1.6 25.39 ? 1.5</cell><cell>57.37 ? 8.3 61.58 ? 8.8</cell></row><row><cell></cell><cell>EA</cell><cell cols="2">81.02 ? 2.5 70.02 ? 3.6</cell><cell>37.46 ? 1.8</cell><cell>26.12 ? 1.7</cell><cell>60.53 ? 7.9</cell></row><row><cell>HL Feat</cell><cell>FM FD</cell><cell cols="2">78.60 ? 2.3 66.08 ? 2.0 77.50 ? 3.0 66.59 ? 1.7</cell><cell>49.52 ? 3.6 48.42 ? 4.0</cell><cell>33.90 ? 1.5 33.49 ? 1.8</cell><cell>63.68 ? 8.9 63.68 ? 6.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>HLCL with different graph filters in the final encoder. HP Concatenate LP represents using both high-pass filters and concatenating the output representations; HP Aggregate LP represents using both high-pass filters and aggregating the output representations.</figDesc><table><row><cell></cell><cell cols="2">Homophily</cell><cell></cell><cell>Heterophily</cell></row><row><cell></cell><cell>Cora</cell><cell>CiteSeer</cell><cell>Chameleon</cell><cell>Squirrel</cell><cell>Texas</cell></row><row><cell>LP</cell><cell cols="5">77.09 ? 1.7 64.34 ? 1.5 48.04 ? 3.1 33.44 ? 1.7 64.21 ? 11.2</cell></row><row><cell>HP</cell><cell cols="2">51.94 ? 2.9 36.22 ? 2.5</cell><cell cols="3">35.58 ? 3.1 29.83 ? 1.6 65.26 ? 10.3</cell></row><row><cell cols="3">HP Concatenate LP 66.47 ? 3.6 55.96 ? 2.6</cell><cell cols="2">41.09 ? 3.6 30.86 ? 1.9</cell><cell>65.21 ? 9.4</cell></row><row><cell>HP Aggregate LP</cell><cell cols="2">65.33 ? 2.8 53.47 ? 3.1</cell><cell cols="3">38.77 ? 3.4 28.00 ? 1.5 63.15 ? 11.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Using high-pass filter in existing Graph CL methods. Significant performance drops are highlighted in gray.</figDesc><table><row><cell></cell><cell cols="2">Homophily</cell><cell></cell><cell>Heterophily</cell></row><row><cell></cell><cell>Cora</cell><cell>CiteSeer</cell><cell>Chameleon</cell><cell>Squirrel</cell><cell>Texas</cell></row><row><cell>HLCL</cell><cell>82.34 ? 2.7</cell><cell>71.35 ? 1.4</cell><cell cols="3">49.78 ? 1.8 34.11 ? 1.9 65.26 ? 7.8</cell></row><row><cell>DGI:low</cell><cell>82.60 ? 0.4</cell><cell>68.80 ? 0.7</cell><cell cols="3">40.48 ? 1.9 27.95 ? 0.9 60.00 ?10.8</cell></row><row><cell>DGI:high</cell><cell>31.95 ? 2.8</cell><cell cols="4">30.54 ? 1.7 38.38 ? 3.5 29.57 ? 1.2 62.11 ? 10.2</cell></row><row><cell>BGRL:low</cell><cell>74.67 ? 0.6</cell><cell>64.25 ? 2.4</cell><cell cols="3">38.20 ? 1.4 26.03 ? 0.9 61.05 ? 11.1</cell></row><row><cell>BGRL:high</cell><cell>29.63 ? 2.8</cell><cell cols="3">24.99 ? 3.1 35.41 ? 3.4 29.57 ? 1.1</cell><cell>65.26 ? 8.2</cell></row><row><cell>GRACE:low</cell><cell>83.30 ? 0.4</cell><cell>72.10 ? 0.5</cell><cell cols="2">37.37 ? 0.6 28.67 ? 0.9</cell><cell>62.6 ? 7.2</cell></row><row><cell>GRACE:high</cell><cell>32.46 ? 2.0</cell><cell cols="4">26.55 ? 3.1 39.65 ? 2.9 33.05 ? 2.1 72.63 ? 9.3</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analyzing the expressive power of graph neural networks in a spectral perspective</title>
		<author>
			<persName><forename type="first">Muhammet</forename><surname>Balcilar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Renton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>H?roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Ga?z?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S?bastien</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Honeine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Graph Barlow Twins: A self-supervised representation learning framework for graphs</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bielak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Kajdanowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh V</forename><surname>Chawla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02466</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Beyond low-frequency information in graph convolutional networks</title>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3950" to="3957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Fan</forename><surname>Rk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename></persName>
		</author>
		<title level="m">Spectral graph theory</title>
		<imprint>
			<publisher>American Mathematical Soc</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Graph-structured data viewed through a Fourier lens</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Venkatesan Nallampatti Ekambaram</publisher>
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Contrastive multi-view representation learning on graphs</title>
		<author>
			<persName><forename type="first">Kaveh</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hosein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khasahmadi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4116" to="4126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods</title>
		<author>
			<persName><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hohne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><surname>Sijia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishnavi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omkar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><surname>Bhalerao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nam</forename><surname>Ser</surname></persName>
		</author>
		<author>
			<persName><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="20887" to="20902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Complete the missing half: Augmenting aggregation filtering with diversification for graph convolutional networks</title>
		<author>
			<persName><forename type="first">Sitao</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingde</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenqing</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08844</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Revisiting graph neural networks: All we have is low-pass filters</title>
		<author>
			<persName><forename type="first">Hoang</forename><surname>Nt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09550</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05287</idno>
		<title level="m">Geom-gcn: Geometric graph convolutional networks</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Graph representation learning via graphical mutual information maximization</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minnan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
		<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="259" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gcc: Graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-scale attributed node embedding</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Benedek Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rik</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complex Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Largescale representation learning on graphs via bootstrapping</title>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Thakoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Azabou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><forename type="middle">L</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06514</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Graph attention networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deep Graph Infomax. ICLR (Poster)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Demystifying graph neural network via graph filter assessment</title>
		<author>
			<persName><forename type="first">Yewen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusong</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks? arXiv preprint</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Two sides of the same coin: Heterophily and oversmoothing in graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milad</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06462</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Revisiting semisupervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5812" to="5823" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anup</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tung</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nedim</forename><surname>Lipka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nesreen K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13566</idno>
		<title level="m">Graph neural networks with heterophily</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="11168" to="11176" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7793" to="7804" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Interpreting and unifying graph neural networks with an optimization framework</title>
		<author>
			<persName><forename type="first">Meiqi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1215" to="1226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">An empirical study of graph contrastive learning</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01116</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04131</idno>
		<title level="m">Deep graph contrastive representation learning</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with adaptive augmentation</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2069" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
