<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Toward Machine Emotional Intelligence: Analysis of Affective Physiological State</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Rosalind</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
							<email>picard@media.mit.edu..</email>
							<affiliation key="aff0">
								<orgName type="laboratory">MIT Media Lab</orgName>
								<address>
									<addrLine>20 Ames Street</addrLine>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<addrLine>12 Ermou Road</addrLine>
									<postCode>16671</postCode>
									<settlement>Vouliagmeni, Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Elias</forename><surname>Vyzas</surname></persName>
							<email>evyzas@media.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">MIT Media Lab</orgName>
								<address>
									<addrLine>20 Ames Street</addrLine>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jennifer</forename><surname>Healey</surname></persName>
							<email>jahealey@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">MIT Media Lab</orgName>
								<address>
									<addrLine>20 Ames Street</addrLine>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="middle">J</forename><surname>Healey</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">IBM T.J. Watson Research Center</orgName>
								<address>
									<postBox>PO Box 218</postBox>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Toward Machine Emotional Intelligence: Analysis of Affective Physiological State</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9FEE7FC19B729DC36016600FA0EC0943</idno>
					<note type="submission">received 29 Dec. 1999; revised 3 Aug. 2000; accepted 5 Mar. 2001.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index TermsÐEmotion recognition</term>
					<term>physiological patterns</term>
					<term>feature selection</term>
					<term>Fisher Projection</term>
					<term>affective computing</term>
					<term>emotional intelligence</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>AbstractÐThe ability to recognize emotion is one of the hallmarks of emotional intelligence, an aspect of human intelligence that has been argued to be even more important than mathematical and verbal intelligences. This paper proposes that machine intelligence needs to include emotional intelligence and demonstrates results toward this goal: developing a machine's ability to recognize human affective state given four physiological signals. We describe difficult issues unique to obtaining reliable affective data and collect a large set of data from a subject trying to elicit and experience each of eight emotional states, daily, over multiple weeks. This paper presents and compares multiple algorithms for feature-based recognition of emotional state from this data. We analyze four physiological signals that exhibit problematic day-to-day variations: The features of different emotions on the same day tend to cluster more tightly than do the features of the same emotion on different days. To handle the daily variations, we propose new features and algorithms and compare their performance. We find that the technique of seeding a Fisher Projection with the results of Sequential Floating Forward Search improves the performance of the Fisher Projection and provides the highest recognition rates reported to date for classification of affect from physiology: 81 percent recognition accuracy on eight classes of emotion, including neutral.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>I T is easy to think of emotion as a luxury, something that is unnecessary for basic intelligent functioning and difficult to encode in a computer program; therefore, why bother giving emotional abilities to machines? Recently, a constellation of findings, from neuroscience, psychology, and cognitive science, suggests that emotion plays surprising critical roles in rational and intelligent behavior. Most people already know that too much emotion is bad for rational thinking; much less well-known is that neuroscience studies of patients who essentially have their emotions disconnected reveal that those patients have strong impairments in intelligent day-to-day functioning, suggesting that too little emotion can impair rational thinking and behavior <ref type="bibr" target="#b0">[1]</ref>. Apparently, emotion interacts with thinking in ways that are nonobvious but important for intelligent functioning. Emotion-processing brain regions have also been found to perform pattern recognition before the incoming signals arrive at the cortex: A rat can be taught to fear a tone even when its auditory cortex is removed, and similar emotion-oriented processing is believed to take place in human vision and audition <ref type="bibr" target="#b1">[2]</ref>.</p><p>Scientists have amassed evidence that emotional skills are a basic component of intelligence, especially for learning preferences and adapting to what is important <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. With increasing deployment of adaptive computer systems, e.g., software agents and video retrieval systems that learn from users, the ability to sense and respond appropriately to user affective feedback is of growing importance. Emotional intelligence consists of the ability to recognize, express, and have emotions, coupled with the ability to regulate these emotions, harness them for constructive purposes, and skillfully handle the emotions of others. The skills of emotional intelligence have been argued to be a better predictor than IQ for measuring aspects of success in life <ref type="bibr" target="#b3">[4]</ref>.</p><p>Machines may never need all of the emotional skills that people need; however, there is evidence that machines will require at least some of these skills to appear intelligent when interacting with people. A relevant theory is that of Reeves and Nass at Stanford: Human-computer interaction is inherently natural and social, following the basics of human-human interaction <ref type="bibr" target="#b4">[5]</ref>. For example, if a piece of technology talks to you but never listens to you, then it is likely to annoy you, analogous to the situation where a human talks to you but never listens to you. Nass and Reeves have conducted dozens of experiments of classical human-human interaction, taking out one of the humans and putting in a computer, and finding that the basic human-human results still hold.</p><p>Recognizing affective feedback is important for intelligent human-computer interaction. Consider a machine learning algorithm that has to decide when to interrupt the user. A human learns this by watching how you respond when you are interrupted in different situations: Did you receive the interruption showing a neutral, positive, or negative response? Without such regard for your response, the human may be seen as disrespectful, irritating, and unintelligent. One can predict a similar response toward computers that interrupt users oblivious to their positive or negative expressions. The computer could potentially appear more intelligent by recognizing and appropriately adapting to the user's emotion response.</p><p>Although not all computers will need emotional skills, those that interact with and adapt to humans in real-time are likely to be perceived as more intelligent if given such skills. The rest of this paper focuses on giving a machine one of the key skills of emotional intelligence: the ability to recognize emotional information expressed by a person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Human Emotion Recognition</head><p>Human newborns show signs of recognizing affective expressions such as approval and disapproval long before they acquire language. Affect recognition is believed to play an important role in learning and in developing the ability to attend to what is important and is likely a key part of the difference between normal child development and development of autistic children, who typically have impaired affect recognition <ref type="bibr" target="#b5">[6]</ref>. For example, instead of attending to the parent's speech with exaggerated inflection, the autistic child might tune in to an unimportant sound, missing the guidance provided by the affective cues.</p><p>Emotion modulates almost all modes of human commu-nicationÐword choice, tone of voice, facial expression, gestural behaviors, posture, skin temperature and clamminess, respiration, muscle tension, and more. Emotions can significantly change the message: sometimes it is not what was said that was most important, but how it was said. Faces tend to be the most visible form of emotion communication, but they are also the most easily controlled in response to different social situations when compared to the voice and other modes of expression. Affect recognition is most likely to be accurate when it combines 1) multiple kinds of signals from the user with 2) information about the user's context, situation, goals, and preferences. A combination of lowlevel pattern recognition, high-level reasoning, and natural language processing is likely to provide the best emotion inference <ref type="bibr" target="#b6">[7]</ref>.</p><p>How well will a computer have to recognize human emotional state to appear intelligent? Note that no human can perfectly recognize your innermost emotions, and sometimes people cannot even recognize their own emotions. No known mode of affect communication is lossless; some aspects of internal feelings remain private, especially if you wish them to be that way or if you sufficiently disguise them. What is available to an external recognizer is what can be observed and reasoned about, and this always comes with some uncertainty. Nonetheless, people recognize each other's emotions well-enough to communicate useful feedback. Our aim is to give computers recognition abilities similar to those that people have.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Importance of Physiological Emotion Recognition</head><p>When designing intelligent machine interfaces, why not focus on facial and vocal communicationÐaren't these the modes that people rely upon? There are cases where such modes will be preferable, as well as other behavior-based modes, such as gestural activity or time to complete a task. However, it is a mistake to think of physiology as something that people do not naturally recognize. A stranger shaking your hand can feel its clamminess (related to skin conductivity); a friend leaning next to you may sense your heart pounding; students can hear changes in a professor's respiration that give clues to stress; ultimately, it is muscle tension in the face that gives rise to facial expressions. People read many physiological signals of emotion. Physiological pattern recognition of emotion has important applications in medicine, entertainment, and humancomputer interaction. Affective states of depression, anxiety, and chronic anger have been shown to impede the work of the immune system, making people more vulnerable to viral infections, and slowing healing from surgery or disease <ref type="bibr">([4, chapter 11]</ref>). Physiological pattern recognition can potentially aid in assessing and quantifying stress, anger, and other emotions that influence health. Certain human physiological patterns show characteristic responses to music and other forms of entertainment, e.g., skin conductivity tends to climb as a piece of music ªpeps you upº and fall as it ªcalms you down.º This principle was utilized in a wearable ªaffective DJº to allow more personalized music selections than the one-size-fits-all approach of a disc jockey <ref type="bibr" target="#b7">[8]</ref>. Changes in physiological signals can also be examined for signs of stress arising while users interact with technology, helping detect where the product causes unnecessary irritation or frustration, without having to interrupt the user or record her appearance. This is a new area for pattern recognition research: detecting when products cause user stress or aggravation, thereby helping developers target areas for redesign and improvement.</p><p>Physiological sensing is sometimes considered invasive because it involves physical contact with the person. However, not only is technology improving with conductive rubber and fabric electrodes that are wearable, washable, and able to be incorporated in clothes and accessories <ref type="bibr" target="#b8">[9]</ref> but also there are new forms of noncontact physiological sensing on the horizon. In some cases, physiological sensors are perceived as less invasive than alternatives, such as video. Video almost always communicates identity, appearance, and behavior, on top of emotional information. Students engaged in distance learning may wish to communicate to the lecturer that they are furrowing their brow in confusion or puzzlement but not have the lecturer know their identity. They might not object to having a small amount of muscle tension anonymously transmitted, whereas they may object to having their appearance communicated.</p><p>New wearable computers facilitate different forms of sensing than traditional computers. Wearables often afford natural contact with the surface of the skin; however, they do not easily afford having a camera pointed at the user's face. (It can be done with a snugly fitted hat and stiff brim to mount the camera for viewing the wearer's face, a form factor that can be awkward both physically and socially.) In wearable systems, physiological sensing may be set up so that it involves no visible or heavy awkward supporting mechanisms; in this case, the physiological sensors may be less cumbersome than video sensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Related Research</head><p>The affect recognition problem is a hard one when you look at the few benchmarks which exist. In general, people can recognize an emotional expression in neutral-content speech with about 60 percent accuracy, choosing from among about six different affective labels <ref type="bibr" target="#b9">[10]</ref>. Computer algorithms match or slightly beat this accuracy, e.g., <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Note that computer speech recognition that works at about 90 percent accuracy on neutrally-spoken speech tends to drop to 50-60 percent accuracy on emotional speech <ref type="bibr" target="#b12">[13]</ref>. Improved handling of emotion in speech is important for recognizing what is said, as well as how it was said.</p><p>Facial expression recognition is easier for people, e.g., 70-98 percent accurate on six categories of facial expressions exhibited by actors <ref type="bibr" target="#b13">[14]</ref> and the rates computers obtain range from 80-98 percent accuracy when recognizing 5-7 classes of emotional expression on groups of 8-32 people <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Other research has focused not so much on recognizing a few categories of emotional expressions but on recognizing specific facial actionsÐthe fundamental muscle movements that comprise Paul Ekman's Facial Action Coding SystemÐwhich can be combined to describe all facial expressions. Recognizers have already been built for a handful of the facial actions <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, and the automated recognizers have been shown to perform comparably to humans trained in recognizing facial actions <ref type="bibr" target="#b17">[18]</ref>. These facial actions are essentially facial phonemes, which can be assembled to form facial expressions. There are also recent efforts that indicate that combining audio and video signals for emotion recognition can give improved results <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>.</p><p>Although the progress in facial, vocal, and combined facial/vocal expression recognition is promising, all of the results above are on presegmented data of a small set of sometimes exaggerated expressions or on a small subset of hand-marked singly-occurring facial actions. The state-ofthe-art in affect recognition is similar to that of speech recognition several decades ago when the computer could classify the carefully articulated digits, ªHY IY PY F F F Y W,º spoken with pauses in between, but could not accurately detect these digits in the many ways they are spoken in larger continuous conversations.</p><p>Emotion recognition research is also hard because understanding emotion is hard; after over a century of research, emotion theorists still do not agree upon what emotions are and how they are communicated. One of the big questions in emotion theory is whether distinct physiological patterns accompany each emotion <ref type="bibr" target="#b23">[24]</ref>. The physiological muscle movements comprising what looks to an outsider to be a facial expression may not always correspond to a real underlying emotional state. Emotion consists of more than its outward physical expression; it also consists of internal feelings and thoughts, as well as other internal processes of which the person having the emotion may not be aware.</p><p>The relation between internal bodily feelings and externally observable expression is still an open research area, with a history of controversy. Historically, James was the major proponent of emotion as an experience of bodily changes, such as your heart pounding or your hands perspiring <ref type="bibr" target="#b24">[25]</ref>. This view was challenged by Cannon <ref type="bibr" target="#b25">[26]</ref> and again by Schachter and Singer who argued that the experience of physiological changes was not sufficient to discriminate emotions. Schachter and Singer's experiments showed that, if a bodily arousal state was induced, then subjects could be put into two distinct moods simply by being put in two different situations. They argued that physiological responses such as sweaty palms and a rapid heart beat inform our brain that we are aroused and then the brain must appraise the situation we are in before it can label the state with an emotion such as fear or love <ref type="bibr" target="#b26">[27]</ref>.</p><p>Since the classic work of Schachter and Singer, there has been a debate about whether or not emotions are accompanied by specific physiological changes other than simply arousal level. Ekman et al. <ref type="bibr" target="#b27">[28]</ref> and Winton et al. <ref type="bibr" target="#b28">[29]</ref> provided some of the first findings showing significant differences in autonomic nervous system signals according to a small number of emotional categories or dimensions, but there was no exploration of automated classification. Fridlund and Izard <ref type="bibr" target="#b29">[30]</ref> appear to have been the first to apply pattern recognition (linear discriminants) to classification of emotion from physiological features, attaining rates of 38-51 percent accuracy (via cross-validation) on subject-dependent classification of four different facial expressions (happy, sad, anger, fear) given four facial electromyogram signals. Although there are over a dozen published efforts aimed at finding physiological correlates when examining small sets of emotions (from 2-7 emotions according to a recent overview <ref type="bibr" target="#b30">[31]</ref>), most have focused on t-test or analysis of variance comparisons, combining data over many subjects, where each was measured for a relatively small amount of time (seconds or minutes). Relatively few of the studies have included neutral control states where the subject relaxed and passed time feeling no specific emotion, and none to our knowledge have collected data from a person repeatedly, over many weeks, where disparate sources of noise enter the data. Few efforts beyond Fridlund's have employed linear discriminants, and we know of none that have applied more sophisticated pattern recognition to physiological features.</p><p>The work in this paper is novel in trying to classify physiological patterns for a set of eight emotions (including neutral), by applying pattern recognition techniques beyond that of simple discriminants to the problem (we use new features, feature selection, spatial transformations of features, and combinations of these methods) and by focusing on ªfeltº emotions of a single subject gathered over sessions spanning many weeks. The results we obtain are also independent of psychological debates on the universality of emotion categories <ref type="bibr" target="#b31">[32]</ref>, focusing instead on user-defined emotion categories.</p><p>The contributions of this paper include not only a new means for pattern analysis of affective states from physiology, but also the finding of significant classification rates from physiological patterns corresponding to eight affective states measured from a subject over many weeks of data. Our results also reveal significant discrimination among both most commonly described dimensions of emotion: valence and arousal. We show that the day-today variations in physiological signals are large, even when the same emotion is expressed, and this effect undermines recognition accuracy if it is not appropriately handled. This paper proposes and compares techniques for handling day-to-day variation and presents new results in affect recognition based on physiology. The results lie between the rates obtained for expression recognition from vocal and facial features and are the highest reported to date for classifying eight emotional states given physiological patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">GATHERING GOOD AFFECTIVE DATA</head><p>In computer vision or speech recognition, it has become easy to gather meaningful data; frame-grabbers, microphones, cameras, and digitizers are reliable, easy to use, and the integrity of the data can be seen or heard by nonspecialists; however, nonspecialists do not usually know what comprises a good physiological signal. Although people recognize emotional information from physiology, it is not natural to do so by looking at 1D signal waveforms. Not only does it take effort to learn what a good signal is, but the sensing systems (A/D converters and bufferingdata capture systems) for physiology do not seem to be as reliable as those for video and audio. Factors such as whether or not the subject just washed her hands, how much gel she applied under an electrode, motion artifacts, and precisely where the sensor was placed, all affect the readings. These are some of the technical factors that contribute to the difficulty in gathering accurate physiological data.</p><p>Although dealing with the recording devices can be tricky, a much harder problem is that of obtaining the ground truth of the data, or getting data that genuinely corresponds to a particular emotional state. In vision or speech research, the subject matter is often objective: scene depth, words spoken, etc. In those cases, the ground-truth labels for the data are easily obtained. Easy-to-label data is sometimes obtained in emotion research when a singular strong emotion is captured, such as an episode of rage. However, more often, the ground truthÐwhich emotion was presentÐis difficult to establish.</p><p>Consider a task where a person uses the computer to retrieve images. Suppose our job is to analyze physiological signals of the user as he or she encounters pleasing or irritating features or content within the system. We would like to label the data according to the emotional state of the user (ground truth). Here, the problem is complicated because there is little way of knowing whether the person was truly pleased or irritated when encountering the stimuli intended to induce these states. We may have tried to please, but the user was irritated because of something she remembered unrelated to the task. We may have tried to irritate and not succeeded. If we ask the person how she felt, her answer can vary according to her awareness of her feelings, her comfort in talking about feelings, her rapport with the administrator(s) of the experiment, and more. When you ask is also importantÐsoon and often is likely to be more accurate, but also more irritating, thereby changing the emotional state. Thus, measurement of ground truth disturbs the state of that truth. When it comes to faces or voices, we can see if the person was smiling or hear if her voice sounded cheerful, but that still does not mean that she was happy. With physiology, little is known about how emotions make their impact, but the signals are also potentially more sincere expressions of the user's state since they tend to be less mediated by cognitive and social influences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Five Factors in Eliciting Emotion</head><p>In beginning this research, we thought it would be simple to ask somebody to feel and express an emotion and to record data during such episodes. Indeed, this is the approach taken in most facial and vocal expression studies to date: turn the camera and microphone on, ask the subject to express joy, anger, etc., record it, and label it by what was requested. However, obtaining high quality physiological data for affect analysis requires attention to experimental design issues not traditionally required in pattern recognition research. Here, we outline five (not necessarily independent) factors that influence data collection, to serve as a useful guide for researchers trying to obtain affect data. We summarize the factors by listing their extreme conditions, but there are also in-between conditions:</p><p>1. Subject-elicited versus event-elicited: Does subject purposefully elicit emotion or is it elicited by a stimulus or situation outside the subject's efforts? 2. Lab setting versus real-world: Is subject in a lab or in a special room that is not their usual environment? 3. Expression versus feeling: Is the emphasis on external expression or on internal feeling? 4. Open-recording versus hidden-recording: Does subject know that anything is being recorded? 1 5. Emotion-purpose versus other-purpose: Does subject know that the experiment is about emotion? The most natural setup for gathering genuine emotions is opportunistic: The subject's emotion occurs as a consequence of personally significant circumstances (eventelicited); it occurs while they are in some natural location for them (real-world); the subject feels the emotion internally (feeling); subject behavior, including expression, is not influenced by knowledge of being in an experiment or being recorded (hidden-recording, other-purpose). Such data sets are usually impossible to get because of privacy and ethics concerns, but as recording devices are increasingly prevalent, people may cease to be aware of them, and the data captured by these devices can be as if the devices were hidden. Researchers may try to create such opportunistic situations; for example, showing an emotion-eliciting film in a theater without telling subjects the true purpose of the showing, and without telling them that they have been videotaped until afterward. Even so, an emotion-inducing 1. When the presence of the camera or other recording device is hidden, it is ethically necessary (via the guidelines of the MIT Committee on the Use of Humans as Experimental Subjects) to debrief the subject, let them know why the secrecy was necessary, tell them what signals have been recorded, and obtain their permission for data analysis, destroying the data if the subject withholds permission. Unlike with video, it is not yet possible to record a collection of physiological signals without the subject being aware of the sensors in some form (although this is changing with new technology.) However, subjects can be led to believe that their physiology is being sensed for some reason other than emotions, which is often an important deception since a subject who knows you are trying to make them frustrated may, therefore, not get frustrated. We have found that such deception is acceptable to almost all subjects when properly conducted. Nonetheless, we believe deception should not be used unless it is necessary and then only in accord with ethical guidelines. movie scene will affect some viewers more than others and some maybe not at all.</p><p>The opportunistic situation contrasts with the experiments typically used to gather data for facial or vocal expression recognition systems. The common setup is one in which data of a subject-elicited external expression in a lab setting, in front of a visible open-recording camera or microphone, and with knowledge that the data will be used for analysis of emotion (emotion-purpose) are gathered. Such expressions, made with or without corresponding internal feelings, are ªrealº and important to recognize, even if not accompanied by true feelings. Social communication involves both unfelt emotions (emphasizing expression) and more genuine ones (emphasizing feeling.) A protocol for gathering data may emphasize external expression or internal feeling; both are important. Moreover, there is considerable overlap since physical expression can help induce the internal feeling, and vice-versa.</p><p>In this paper, we gathered real data following a subjectelicited, close to real-world (subject's comfortable usual workplace), feeling, open-recording, and emotion-purpose methodology. The key one of these factors that makes our data unique is that the subject tried to elicit an internal feeling of each emotion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Single-Subject Multiple-Day Data Collection</head><p>The data we gather is from a single subject over many weeks of time, standing in contrast to efforts that examine many subjects over a short recording interval (usually single session on only one day). Although the scope is limited to one subject, the amount of data for this subject encompasses a larger set than has traditionally been used in affect recognition studies involving multiple subjects. The data are potentially useful for many kinds of analysis and will be made available for research purposes.</p><p>There are many reasons to focus on person-dependent recognition in the early stages of affect recognition, even though some forms of emotion communication are not only person-independent, but have been argued, namely, by Paul Ekman, to be basic across different cultures. Ekman and colleagues acknowledge that even simply labeled emotions like ªjoyº and ªangerº can have different interpretations across individuals within the same culture; this complicates the quest to see if subjects elicit similar physiological patterns for the same emotion. When lots of subjects have been examined over a short amount of time, researchers have had difficulty finding significant physiological patterns, which may be in part because physiology can vary subtly with how each individual interprets each emotion. By using one subject, who tried to focus on the same personal interpretation of the emotion in each session, we hoped to maximize the chance of getting consistent interpretations for each emotion. This also means that the expressive data can be expected to differ for another subject since the way another subject interprets and reacts to the emotions may differ. Hence, a weakness of this approach is that the precise features and recognition results we obtain with this data may not be the same for other subjects. However, the methodology for gathering and analyzing the data in this paper is not dependent on the subject; the approach described in this paper is general.</p><p>Emotion recognition is in an early stage of research, similar to early research on speech recognition, where it is valuable to develop person-dependent methods. For personal computing applications, we desire the machine to learn an individual's patterns and not just some average response formed across a group, which may not apply to the individual.</p><p>The subject in our experiments was a healthy graduate student with two years acting experience plus training in visualization, who was willing to devote over six weeks to data collection. The subject sat in her quiet workspace early each day, at roughly the same time of day, and tried to experience eight affective states with the aid of a computer controlled prompting system, the ªSentograph,º developed by Clynes <ref type="bibr" target="#b32">[33]</ref> and a set of personally-significant imagery she developed to help elicit the emotional state.</p><p>The Clynes protocol for eliciting emotion has three features that contribute to helping the subject feel the emotions and that make it appropriate for physiological data collection: 1) It sequences eight emotions in a way that supposedly makes it easier for many people to transition from emotion to emotion. 2) It engages physical expressionÐasking the subject to push a finger against a button with a dual axis pressure sensor in an expressive wayÐbut in a way that limits motion artifacts being introduced to the physiological signals. Physical expression gives somatosensory feedback to the subject, a process that can help focus and strengthen the feeling of the emotion <ref type="bibr" target="#b33">[34]</ref>. 3) It prompts the subject to repeatedly express the same emotion during an approximately three minute interval, at a rate dependent on the emotion in order to try to intensify the emotional experience <ref type="bibr" target="#b32">[33]</ref>.</p><p>The order of expression of the eight states: no emotion, anger, hate, grief, platonic love, romantic love, joy, and reverence was found by Clynes to help subjects reliably feel each emotion; for example, it would probably be harder on most subjects to have to oscillate between the positive and negative states, e.g., joy, hate, platonic love, grief, etc. However, because interpretations for each of the emotions may vary with each individual, we do not expect this order to be optimal for everyone.</p><p>Descriptive guidelines on the meaning of each emotion were developed by the subject before the experiment. The subject reported the images she used to induce each state, the degree to which she found each experience arousing (exciting, distressing, disturbing, tranquil), and the degree to which she felt the emotion was positive or negative (valence) (See Table <ref type="table">1</ref>). Daily ratings varied in intensityboth up and down, with no particular trend, but the overall character of each state was consistent over the weeks of data collection.</p><p>The eight emotions in this study differ from those typically explored. Although there is no widespread agreement on the definition and existence of ªbasicº emotions and which names would comprise such a list, researchers on facial expressions tend to focus on anger, fear, joy, and sadness with disgust and surprise often examined as well as contempt, acceptance, and anticipation (e.g., <ref type="bibr" target="#b31">[32]</ref>). Theorists still do not agree on what an emotion is and many of them do not consider love and surprise to be emotions.</p><p>Our work is less concerned with finding a set of ªbasicº emotions and more concerned with giving computers the ability to recognize whatever affective states might be relevant in a personalized human-computer interaction. The ideal states for a computer to recognize will depend on the application. For example, in a learning-tutor application, detecting expressions of curiosity, boredom, and frustration may be more relevant than detecting emotions on the theorists' ªbasicº lists.</p><p>Clynes' set of eight was motivated by considering emotions that have been communicated through centuries of musical performance on several continents. We started with his set not because we think it is the best for computerhuman interaction (such a set is likely to vary with computer applicationsÐentertainment, business, socializing, etc.), but rather because this set together with its method for elicitation had shown an ability to help subjects reliably feel the emotions and had shown repeatable signs of physical differentiation in how subjects' finger pressure applied to a finger rest differs with each emotion <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b34">[35]</ref>, a measurable outcome that suggests different states were being achieved. It was important to our investment in longterm data collection that we have a reliable method of helping the user repeatedly generate distinct emotional states.</p><p>For the purposes of this research, the specific emotions and their definitions are not as important as the fact that 1) the subject could relate to the named emotion in a consistent, specific, and personal way and 2) the emotion categories span a range of high and low arousal and positive and negative valence. These two dimensions are believed to be the most important dimensions for categorizing emotions <ref type="bibr" target="#b35">[36]</ref> and continue to be used for describing emotions that arise in many contexts, including recent efforts to categorize emotions arising when people look at imagery <ref type="bibr" target="#b36">[37]</ref>. The arousal axis ranges from calm and peaceful to active and excited, while the valence axis ranges from negative (displeasing) to positive (pleasing). The left hand was held still throughout data collection and the subject was seated and relatively motionless except for small pressure changes she applied with her right hand to the finger rest. Sensors and sampling were provided by the Thought Technologies ProComp unit, chosen because the unit is small enough to attach to a wearable computer and offers eight optically isolated channels for recording.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Experimental Method and</head><p>Signals were sampled at 20 Hz. <ref type="foot" target="#foot_1">2</ref> The ProComp automatically computed the heart rate (r) as a function of the interbeat intervals of the blood volume pressure, f. More details on this system and on our methodology are available <ref type="bibr" target="#b37">[38]</ref>. Each day's session lasted around 25 minutes, resulting in around 28 to 33 thousand samples per physiological signal, with each different emotion segment being around two to five thousand samples long, due to the variation built into the Clynes method of eliciting the emotional states <ref type="bibr" target="#b32">[33]</ref>. Eight signal segments of the raw data (2,000 samples each) from Data Set I are shown in Fig. <ref type="figure">1</ref>. On roughly a third of the 30 days for which we collected data, either one or more sensors failed during some portion of the 25-minute experiment because an electrode came loose or one or more channels failed to sample and save some of the data properly. From the complete or nearly-complete sessions, we constructed two overlapping Data Sets.</p><p>Data Set I was assembled before the 30 days were over, and was formed as follows: Data segments of 2,000 samples (100 seconds) in length were taken from each of the signals iY fY q, and for each of the eight emotions, on each of 19 days where there were no failures in these segments of data collection. The 2,000 samples were taken from the end of each emotion segment to avoid the transitional onset where the subject was prompted to move to the next emotion. A 20th day's data set was created out of a combination of partial records in which some of the sensors had failed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FEATURE EXTRACTION, SELECTION, AND TRANSFORMATION</head><p>Because the signals involved have different and complex sources, because there are not yet good models to describe them, and because of an interest in seeing how some classical methods perform before an extensive modeling effort is launched, we choose in this paper to explore a feature-based approach to classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Proposed Feature Sets</head><p>The psychophysiology and emotion literature contains several efforts to identify features of bodily changes (facial muscle movements, heartrate variations, etc.) that might correlate with having an emotion (e.g., <ref type="bibr" target="#b30">[31]</ref>). We gather a variety of features, some from the literature and some that we propose. Several that we propose are physically motivated, intended to capture the underlying nature of specific signals, such as the way respiration is quasiperiodic, while others are simple statistics and nonlinear combinations thereof. We do not expect the best classifier to require all the features proposed below or even to require such a huge number of features. Our effort is to advance the state-of-the-art in pattern recognition of affect from physiology by proposing a large space of reasonable features and systematically evaluating subsets of it and transformations thereof. Below, six statistical features are presented, followed by 10 more physically-motivated features aimed at compensating for day-to-day variations. Which features were found to be most useful (as a function of each of the classifiers) will be summarized later in Table <ref type="table">9</ref>.</p><p>The six statistical features can be computed for each of the signals as follows: Let the signal fiY fY qY Y rg from any one of the eight emotion segments be designated by . The signal is gathered for eight different emotions each day, for 20 days. Let n represent the value of the nth sample of the raw signal, where n IY F F F Y x, with x PY HHH for Data Set I, and with x in the range of 2,000 to 5,000 for Data Set II. Let n refer to the normalized signal (zero mean, unit variance):</p><formula xml:id="formula_0">n n À " ' i IY F F F Y RY</formula><p>where " and ' are the means and standard deviations of as explained below. Following are six statistical features we investigated: The features (1), ( <ref type="formula">2</ref>), ( <ref type="formula">3</ref>), ( <ref type="formula">4</ref>), <ref type="bibr" target="#b4">(5)</ref>, and (6) were chosen to cover and extend a range of typically measured statistics in the emotion physiology literature <ref type="bibr" target="#b38">[39]</ref>. (Means and variances are already commonly computed; the first difference approximates a gradient.) Note that not all the features are independent; in particular, and are nonlinear combinations of other features. Also, the heart rate signal, r, is derived from the blood volume pressure signal, f, by a nonlinear transformation performed automatically by the ProComp sensing system. It did not require an additional sensor and, so, was dependent upon f. The dependencies are not linear; consequently, they are not obtainable by the linear combination methods used later to reduce the dimensionality of the feature space and can potentially be of use in finding a good transformation for separating the classes. The comparisons below will verify this.</p><p>One advantage of the features in (1), ( <ref type="formula">2</ref>), ( <ref type="formula">3</ref>), ( <ref type="formula">4</ref>), ( <ref type="formula">5</ref>), and ( <ref type="formula">6</ref>) is that they can easily be computed in an online way <ref type="bibr" target="#b39">[40]</ref>, which makes them advantageous for real-time recognition systems. However, the statistical features do not exploit knowledge we may have about the physical sources of the signals, and provide no special normalization for day-today variations in the signals. Factors such as hand washing, gel application, and sensor placement can easily affect the statistics. These influences combine with the subject's daily mood and with other cognitive and bodily influences in presently unknown ways, making them hard to model.</p><p>In an effort to compensate for some of the nonemotionrelated variations of the signals and to include more physically-motivated knowledge about the signal (such as underlying periodic excitations), we also compute and evaluate another set of 10 physiology-dependent features, f I À f IH , described below.</p><p>From the interbeat intervals of the blood volume pressure waveform, the Procomp computes the heart rate, r, which is approximately Iathe interbeat intervals. We applied a 500-point (25 sec) Hanning window, h, <ref type="bibr" target="#b40">[41]</ref> to form a smoothed heartbeat rate, r Ã h, then took the mean:</p><formula xml:id="formula_1">f I I x x nI n X U</formula><p>The average acceleration or deceleration of the heart beat rate was calculated by taking the mean of the first difference:</p><formula xml:id="formula_2">f P I x À I xÀI nI nI À n I x À I x À I X V</formula><p>The skin conductivity signal, contains high frequency fluctuations that may be noise; these fluctuations are reduced by convolving with h, a 25 second Hanning window, to form s Ã h. We also use a form of contrast normalization to account for baseline fluctuations; this measure was proposed by Rose <ref type="bibr" target="#b41">[42]</ref> and found to be valuable over years of psychophysiology <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, where mxg and ming are, with respect to the whole day's data (for all emotions that day):</p><formula xml:id="formula_3">f Q s mins mxs À mins X W</formula><p>The mean of the first difference of the smoothed skin conductivity is also proposed:</p><formula xml:id="formula_4">f R " s nI Às n I x À I s x À s I X IH</formula><p>The respiration sensor measured expansion and contraction of the chest cavity using a Hall effect sensor attached around the chest with a velcro band. Let x d be the number of samples collected that day. To account for variations in the initial tightness of the sensor placement from day to day, we formed the mean of the whole day's respiration data:</p><formula xml:id="formula_5">" Ydy I x d x d nI n Y II</formula><p>and then subtracted this to get r À " Ydy . Two respiration features were then formed as:</p><formula xml:id="formula_6">f S I x x nI r n IP and f T I x À I x nI r n À " Ydy À Á P X IQ</formula><p>The four features f U -f IH represent frequency information in the respiration signal. Each was computed using the power spectral density function (PSD command) of Matlab, which uses Welch's averaged periodogram. The features, illustrated in Fig. <ref type="figure">2</ref>, represent the average energy in each of the first four 0.1 Hz bands of the power spectral density range 0.0-0.4 Hz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Selecting and Transforming Features</head><p>We first compare two techniques that have appeared in the literature to establish a benchmark: Sequential Floating Forward Search (SFFS) and Fisher Projection (FP). Next, we propose and compare a new combination of these, which we label SFFS-FP. This combination is motivated by the dual roles of the two methods: SFFS selects from a set of features, while Fisher Projection linearly transforms a set of features. Since feature selection is nonlinear, the cascade of these two methods should provide a more powerful combination than either alone. This was confirmed in our experiments.</p><p>The Sequential Floating Forward Search (SFFS) method <ref type="bibr" target="#b44">[45]</ref> is chosen because of its consistent success in previous evaluations of feature selection algorithms, where it has been shown to outperform methods such as Sequential Forward Search (SFS), Sequential Backward Search (SBS), Generalized SFS and SBS, and Max-Min in comparison studies <ref type="bibr" target="#b45">[46]</ref>. Of course the performance of SFFS is data dependent and the data set here is new and different; SFFS may not be the best method to use. Nonetheless, because of its well-documented success in other pattern recognition problems, it will help establish a benchmark for this new application area. The SFFS method takes as input the values of n features. It then does a nonexhaustive search on the feature space by iteratively including and omitting features. It outputs one subset of m features for each m, P m n, together with its classification rate. The algorithm is described in detail in <ref type="bibr" target="#b46">[47]</ref>.</p><p>Fisher Projection (FP) <ref type="bibr" target="#b47">[48]</ref> is a well-known method of reducing dimensionality by finding a linear projection of the data to a space of fewer dimensions where the classes are well-separated. Due to the nature of the Fisher projection method, the data can only be projected down to À I (or fewer if one wants) dimensions, assuming that originally there are more than À I dimensions and is the number of classes. If the amount of training data is inadequate, or if the quality of some of the features is questionable, then some of the dimensions of the Fisher projection may be a result of noise rather than a result of differences among the classes. In this case, Fisher might find a meaningless projection which reduces the error in the training data but performs poorly in the testing data. For this reason, we not only separate training and testing data, but we also evaluate projections down to fewer than À I dimensions. Note that if the number of features n is smaller than the number of classes , the Fisher projection is meaningful only up to at most n À I dimensions. Therefore, the number of Fisher projection dimensions d is I d minnY À I, e.g., when 24 features are used on all eight classes, all d IY U are tried, and when four features are used on eight classes, all d IY Q are tried.</p><p>A Hybrid SFFS with Fisher Projection (SFFS-FP) method is proposed, implemented, and evaluated here for comparison. As mentioned above, the SFFS algorithm proposes one subset of m features for each m, P m n. It selects, but does not transform the features. Instead of feeding the Fisher algorithm with all possible features, we use the subsets that the SFFS algorithm proposes as input to the Fisher Algorithm. Note that the SFFS method is used here as a preprocessor for reducing the number of features fed into the Fisher algorithm, and not as a classification method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CLASSIFICATION</head><p>This section describes and compares the results of a set of classification experiments leading up to the best results (81 percent) shown in Tables <ref type="table" target="#tab_3">6</ref> and<ref type="table" target="#tab_5">7</ref>.</p><p>The SFFS software employs a k-nearest-neighbor (k-NN) classifier <ref type="bibr" target="#b48">[49]</ref>, so that it not only outputs the best set of features according to this classifier, but their classification accuracy as well. We used the k-NN classifier for benchmarking the SFFS method, following the methodology of Jain and Zongker <ref type="bibr" target="#b45">[46]</ref>. For FP and SFFS-FP, we used a MAP classifier, with details below.</p><p>In all three comparisons, SFFS, FP, and SFFS-FP, we used the leave-one-out method for cross-validation because of the relatively small amount of data available and the high dimensional feature spaces. In each case, the data point (vector of features for one day's data for one emotion) to be classified was excluded from the data set before the SFFS, FP, or SFFS-FP was run. The best set of features or best transform (determined from the training set) was then applied to the test data point to determine classification accuracy.</p><p>For each k, where we varied I k PH, the SFFS algorithm output one set of m features for each P m n. For SFFS-FP, we computed all possible Fisher projections for each of these feature sets. For both the FP and SFFS-FP methods, we then </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Initial ResultsÐData Set I</head><p>Our first comparison was made on Data Set I using (1), ( <ref type="formula">2</ref>), ( <ref type="formula">3</ref>), ( <ref type="formula">4</ref>), ( <ref type="formula">5</ref>), and ( <ref type="formula">6</ref>) computed on the four raw physiological signals, yielding 24 features: " , ' , , , , , P iY fY qY . The results of SFFS, Fisher, and SFFS-FP are shown in Table <ref type="table">2</ref>. Table <ref type="table">2</ref> shows the results applied to all eight emotions, as well as the results applied to all sets of g emotions, where g QY RY S. The best-recognized emotions were (N)eutral, (A)nger, (G)rief, (J)oy, and (R)everence. The states of (H)ate and the states of love, (P)latonic Love and Romantic (L)ove, were not well-discriminated by any of the classifiers.</p><p>How good are these resultsÐare the differences between the classifiers significant? For each pair of results here and through the rest of the paper, we computed the probability that the lower error rate really is lower, treating error rate over the 160 (or fewer) trials as a Bernoulli random variable. For example, (last row of Table <ref type="table">2</ref>) we compared the 20 errors made by Fisher to the 10 errors made by SFFS-FP, over the 60 trials for the three emotions (AJR). The confidence that the performance was improved was 98 percent. Although the performance improvements of SFFS-FP over Fisher in the other rows range from 5 to over 8 percentage points, the confidences range from 77 percent to 87 percent; thus, the performance improvement of SFFS-FP cannot be said to be statistically significant in these cases, given that 90-95 percent confidence is usually required for that statement. Nonetheless, all the classifiers in this table provide statistically significant improved performance over a random classifier (confidence b WWXWW percent).</p><p>Table <ref type="table" target="#tab_0">3</ref> shows the number of dimensions and the number of features, respectively, that gave the best results. From looking at these numbers, we can conclude that performance was sometimes improved by projecting down to fewer than À I dimensions for a class problem. We can also see that a broad range of numbers of features led to the best results, but, in no case, were 20 or more features useful for constructing the best classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Problem of Day-Dependence</head><p>In visually examining several projections into 2D, we noticed that the features of different emotions from the same day often clustered more closely than did features for the same emotions on different days. To try to quantify this ªday dependence,º we ran a side experiment: How hard would it be to classify what day each emotion came from, as opposed to which emotion was being expressed? Because classifying days is not the main interest, we only ran one comparison: Fisher Projection applied to the 24 features " , ' , , , , , P iY fY qY computed on Data Set I. Leave-one-out cross-validation was applied to every point, and MAP classification was used on the classes (days or emotions), as described above. When the classes were PH days (versus the V emotions), then the recognition accuracy jumped to 83 percent, which is significantly better than random guessing (5 percent) and significantly better than the 40 percent found for emotion classification using these features.</p><p>The day dependence is likely due to three factors: 1) skinsensor interface influences-including hand washing, application of slightly different amounts of gel, and slight changes in positioning of the sensors; 2) variations in physiology that may be caused by caffeine, sugar, sleep, hormones, and other nonemotional factors; 3) variations in physiology that are mood and emotion dependentÐsuch as an inability to build up an intense experience of joy if the subject felt a strong baseline mood of sadness that day. The   <ref type="table">2</ref> The denominator in the dimensions columns is the maximum number of dimensions that could have been used; half of the time the results were better using a number less than this maximum (the numerator). When a range is shown for the number of features, m, then the performance was the same for the whole range.</p><p>first factor is slightly more controllable by using disposable electrodes that come from the manufacturer containing a premeasured gel, and always having the subject wash her hands in the same way at the same time before each session, steps we did not impose. However, we made an effort to place the sensors as similarly from day to day as manually possible and to manually apply the same amount of gel each day. Nonetheless, many of these sources of variation are natural and cannot be controlled in realistic long-term measuring applications. Algorithms and features that can compensate for day-to-day variations are needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Day Matrix for Handling Day-Dependence</head><p>The 24 statistical features extracted from the signals are dependent on the day the experiment was held. We now augment the PH Â PR matrix of the 20 days' 24 features with a PH Â IW day matrix, which appends a vector of length 19 to each vector of length 24. The vector is the same for all emotions recorded the same day and differs among days.</p><p>(The vectors were constructed by generating 20 equidistant points in a 19-dim space.) Let us briefly explain the principle with an illustration in Fig. <ref type="figure" target="#fig_6">3</ref>. Consider when the data come from two different days and only one feature is extracted. (This is the simplest way to visualize, but it trivially extends to more features). Although the feature values of one class are always related to the values of the other classes in the same way, e.g., the mean electromyogram signal for anger may always be higher than the mean electromyogram for joy, the actual values may be highly day-dependent (Fig. <ref type="figure" target="#fig_6">3a</ref>). To alleviate this problem, an extra dimension can be added before the features are input into the Fisher Algorithm (Fig. <ref type="figure" target="#fig_6">3b</ref>). If the data came from three different days, two extra dimensions are added rather than one (Fig. <ref type="figure" target="#fig_6">3c</ref>), etc. In the general case, h À I extra dimensions are needed for data coming from h different days; hence, we use 19 extra dimensions. The above can be also seen as using the minimum number of dimensions so that each of h points can be at equal distance from all others. Therefore, the h À I dimensional vector contains the coordinates of one such point for each day.</p><p>The effect of the day matrix on classification can be seen in Table <ref type="table" target="#tab_1">4</ref>, where we run Fisher and SFFS-FP with and without the day matrix, and compare it to SFFS, running with the same 24 features as above. Note that it is meaningless to apply SFFS to the day matrix, so that comparison is omitted. The use of the day matrix improves the classification by 3.1, 4.3, 6.9, and 9.4 percent; although only the highest two of these improvements are significant at b WH percent and b WS percent (the other confidences are 71-78 percent).</p><p>Table <ref type="table" target="#tab_1">4</ref> also reveals that all the methods perform better on Data Set II than on Data Set I. The improvements range from 5 to 13 percentage points, with confidences 81 percent, 94 percent, 97 percent, 98 percent, and 99 percent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Baseline Matrix for Handling Day-Dependence</head><p>We propose and evaluate another approach: use of a baseline matrix where the Neutral (no emotion) features of each day  These results were obtained with 24 statistical features, " , ' , , , , , P iY fY qY for both data sets. The day matrix adds are used as a baseline for (subtracted from) the respective features of the remaining seven emotions of the same day. This gives an additional PH Â PR matrix for each of the seven nonneutral emotions. The resulting classification results on seven emotions, run on Data Set I, are in Table <ref type="table" target="#tab_2">5</ref>, together with an additional trial of the day matrix for this case. All the results are significantly higher than random guessing (IRXQ percent) with confidence b WWXWW percent. Comparing among the various ways of handling day-dependence, we find the most significant improvement to be SFFS-FP using the baseline matrix, which at 54.3 percent is an improvement over 45 percent (confidence 94 percent). The performance improvements of SFFS-FP over that of Fisher in the last two rows of the table are also statistically significant (confidence 99 percent). Combining all the features (original 24 + baseline + day) appears to result in a decrease in performance, suggesting the limitations of Fisher with too many bad dimensions, where it cannot select out features, but only transform them. However, the decreases (from 54.3 to 49.3 and 40.7 to 35.0) have 80 percent and 84 percent confidence, so are not considered significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Better Features for Handling Day-Dependence</head><p>The comparisons above were all conducted with the original six statistical features (1), ( <ref type="formula">2</ref>), ( <ref type="formula">3</ref>), ( <ref type="formula">4</ref>), <ref type="bibr" target="#b4">(5)</ref>, and (6); we would now like to see how the features f I À f IH influence classification. We compare four spaces of features from which the algorithms can select and transform subsets: the original six, " , ' , , , , , for P iY fY qY for a total of 24 features; these same 24 features plus the same six statistics for r for a total of 30 features; features f I À f IH plus " i which were shown to be useful in our earlier investigation <ref type="bibr" target="#b49">[50]</ref>, and the combination of all 40 of these.</p><p>The results are in Table <ref type="table" target="#tab_3">6</ref>. First, we consider the case where all 40 features are available (the last row). Comparing each entry in the last row of this table with the same column entry in the first row, we find that all the results are significantly improved (confidence b WWXU percent).</p><p>Next, we look at the results with the day matrix. It appears to improve upon the performance of the statistical features in the first two rows; however, the improvements are only a few percentage points and are not significant (confidences 63-89.5 percent). The additional features of the day matrix can become a liability in the presence of the f I À f IH features, which compensate much better for the day-to-day variations (the decrease from 70 percent to 61.3 percent is significant at 95 percent confidence.) Without the day matrix, the f I À f IH features alone perform significantly better than the statistical features (confidence b WQ percent in all six comparisons.)</p><p>The best performance occurs when the methods have all forty features at their disposal and the individual best of these occurs with SFFS-FP. The overall best rate of 81.25 percent is significantly higher (b WS percent confidence) than 16 of the cases shown in Table <ref type="table" target="#tab_3">6</ref>. The three exceptions are in the last rowÐcompared to rates of 78.8 percent and to 77.5 percent the confidences are only 71 percent and 79 percent that 81.25 percent is a genuine improvement.</p><p>Table <ref type="table" target="#tab_5">7</ref> affords a closer look at breaking points of the best-performing case: the SFFS-FP algorithm operating on the full set of 40 features. Summing columns, one sees that the greatest number of false classifications lie in the categories of (P)latonic love and (J)oy. It is possible that this reflects an underlying predilection on the part of this subject toward being in these two categories, even when asked to experience a different emotion; however, one should be careful not to read more into these numbers than is justified.</p><p>Because of the longstanding debate regarding whether physiological patterns reveal valence differences (pleasingdispleasing aspects) of emotion, or only arousal differences, we consider here an alternate view of the results in Table <ref type="table" target="#tab_5">7</ref>. Table <ref type="table">8</ref> rearranges the rows and columns into groups based on similar arousal rating or similar valence rating. From this, we see that both arousal and valence are discriminated at rates significantly higher than random (confidence b WWXWW percent), and that the rate of discrimination based on valence (87 percent) versus the rate obtained based on arousal (84 percent) do not differ in a statistically significant way. The day matrix adds 19 features to the data input to the Fisher algorithm, offering no improvement when f I À f IH are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Finding Robust Features</head><p>A feature-based approach presents not only a virtually unlimited space of possible features that researchers can propose, but an intractable search of all possible subsets of these to find the best features. When people hand-select features, they may do so with an intuitive feel for which are most important; however, hand-selection is rarely as thorough as machine selection and tends to overlook nonintuitive combinations that may outperform intuitive ones. Machine selection is therefore preferable, even when it is suboptimal (nonexhaustive). Here, we analyze which features the machine searches selected repeatedly. The results of automatic feature selection from twelve experiments involving SFFS (either SFFS as in Jain and Zongker's code, or SFFS-FP, or SFFS-FP with the Day Matrix) are summarized in Table <ref type="table">9</ref>.</p><p>From Table <ref type="table">9</ref>, we see that features such as the means of the heart rate, skin conductivity, and respiration were never selected by any of the classifiers running SFFS, so that they need not have even been computed for these classifiers. At the same time, features such as the mean absolute normalized first difference of the heart rate ( r ), the first difference of the smoothed skin conductivity (f R ), and the three higher frequency bands of the respiration signal (f V À f IH ), were always found to contribute to the best results. Features that were repeatedly selected by classifiers yielding good classification rates can be considered more robust than those that were rarely selected, but only within the context of these experiments.</p><p>There were surprises in that some features physiologists have suggested to be important such as f Q were not found to contribute to good discrimination. Although the results here will be of interest to psychophysiologists, they must clearly be interpreted in the context of the recognition experiment here and not as some definitive statement about a physical correlate of emotion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>This paper has suggested that machine intelligence should include skills of emotional intelligence, based on recent scientific findings about the role of emotional abilities in human intelligence, and on the way human-machine interaction largely imitates human-human interaction. This is a shift in thinking from machine intelligence as one of primarily mathematical, verbal, and perceptual abilities. Emotion is believed to interact with all of these aspects of intelligence in the human brain. Emotions, largely overlooked in early efforts to develop machine intelligence, are increasingly regarded as an area for important research.</p><p>One of the key skills of emotional intelligence for adaptive learning systems is the ability to recognize the emotional communication of others. Even dogs can recognize their owner's affective expressions of pleasure or displeasureÐan  important piece of feedback. One of the difficulties researchers face in this area is the sheer difficulty of getting data corresponding to real emotional states; we have found that efforts in this area are more demanding than traditional efforts to get pattern recognition data. We presented five factors to aid researchers trying to gather good affect data.</p><p>The data gathered in this paper contrasts with that gathered for most other efforts at affect pattern recognition not so much in the use of physiology versus video or audio, but in its focus on having the subject try to generate a feeling versus having the subject try to generate an outward expression. One weakness of both data-gathering methods is that the subject elicited the emotion, versus a situation or stimulus outside the subject eliciting it. Our group at MIT has recently designed and built environments that focus on emotions not generated deliberately by the subject, e.g., collecting affective data from users driving automobiles in city and highway conditions <ref type="bibr" target="#b37">[38]</ref> and from users placed in frustrating computer situations <ref type="bibr" target="#b50">[51]</ref>; both of these areas aim at data generation in an event-elicited, close to real-world, feeling, open-recording, other-purpose experiment, with effort to make the open recording so comfortable that it is effectively ignored. Nonetheless, much work remains to be done in gathering and analyzing affect data; attempts to create situations that induce emotion in subjects remain subject to uncertainty.</p><p>One facet of emotion recognition is developed here for the first time: classification of emotional state from physiological data gathered from one subject over many weeks of data collection. The corpus of person-dependent affect data is larger than any previously reported, and the methodology we developed for its analysis is subject-independent. In future work, the patterns of many individuals may be clustered into groups, according to similarity of the physiological patterns, and these results leveraged to potentially provide person-independent recognition.</p><p>Prior to our efforts, researchers have not reported the way in which physiological features for different emotions from the same day tend to be more similar than features for the same emotions on different days. This side-finding of our work may help explain why so many conflicting results exist on prior attempts to identify emotion-dependent physiological correlates. The day-dependent variations that we found and the ways we developed of handling them may potentially be useful in handling across-subject variations. We proposed methods of normalizing features and baselining, showing that the normalizing features gave the best results, but have the drawback of not being easily implemented in an online way because of the requirement of having session-long summarizing information.</p><p>We found that the Fisher Projection applied to a subset of features preselected by SFFS always outperformed Fisher Projection applied to the full set of features when assessing percentage of errors made by the classifiers. However, only a few of the improvements of SFFS-FP over FP were significant at b WS percent; the rest had confidences ranging from 58-87 percent. From the significant improvements, we surmize that the Fisher Projection's ability to transform the feature space may work better when poorer-performing features are omitted up front. The combination is synergistic: fracturization followed by feature transformation, and may well apply in other domains.</p><p>Forty features were proposed and systematically evaluated by multiple algorithms. The best and worst features have been identified for this subject. These are of interest in the ongoing search for good features for affect recognition and may aid in trying to understand the differential effects of emotions on physiology.</p><p>Although the precise rates found here can only be claimed to apply to one subject, the methodology developed in this paper can be used for any subject. The method is general for finding the features that work best for a given subject and for assessing classification accuracies for that subject.</p><p>The results of 81 percent recognition accuracy on eight categories of emotion are the only results we know of for such a classification and are better than machine recognition rates of a similar number of categories of affect from speech (around 60-70 percent) and almost as good as automated recognition of facial expressions (around 80-98 percent).</p><p>Because there were doubts in the literature that physiological information shows any differentiation other than arousal level, these results are exciting. However, these results do not imply that a computer can detect and recognize your emotions with 81 percent accuracy because of the limited set of emotions examined, because of the optimistic estimates of error given by the leave-one-out method because of the fact that this experiment, like others in the literature, has only looked at forced choice among presegmented data because of the use of only one subject's long term data and because of the nature of true emotion, which consists of more than externally measurable signals. We expect that joint pattern analysis of signals from face, voice, body, and the surrounding situation is likely to give the most interesting emotion recognition results, especially since people read all these signals jointly.</p><p>The greater than six-times-chance recognition accuracy achieved in this pattern recognition research is nonetheless a significant finding; it informs long-debated questions by emotion theorists as to whether there is any physiological differentiation among emotions. Additionally, the finding of significant classification rates when the emotion labels are grouped either by arousal or by valence lends evidence against the belief that physiological signals only differentiate with respect to arousal; both valence and arousal were differentiated essentially equally by the method we developed.</p><p>A new question might be stated, ªWhat is the precise bodily nature of the differentiation that exists among different emotions and upon what other factors does this differentiation depend?º We expect that physiological patterning, in combination with facial, vocal, and other behavioral cues, will lead to significant improvements in machine recognition of user emotion over the coming decade, and that this recognition will be critical for giving machines the intelligence to adapt their behavior to interact more smoothly and respectfully with people. However, this ªrecognitionº will be on measurable signals, which we do not expect to include one's innermost thoughts. Much work remains before emotion interpretation can occur at the level of human abilities. F For more information on this or any other computing topic, please visit our Digital Library at http://computer.org/publications/dlib.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Construction of Data SetsData were gathered from four sensors: a triode electromyogram (i) measuring facial muscle tension along the masseter (with Ag-AgCl electrodes of size 11mm each and 10-20 high-conductivity gel), a photoplethysmyograph measuring blood volume pressure (f) placed on the tip of the ring finger of the left hand, a skin conductance () sensor measuring electrodermal activity from the middle of the three segments of the index and middle fingers on the palm-side of the left hand (with 11mm Ag-AgCl electrodes and K-Y Jelly used for low-conductivity gel), and a Hall effect respiration sensor () placed around the diaphragm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>s Descriptions of Imagery and Emotional Character Used for Each of the Eight Emotions Data Set II is a larger data set comprised of 20 days in which the sensors did not fail during any part of the experiment for the five signals: iY fY qY , and r. These 20 days included 16 of the original days from Data Set I, but, for all 20 days, we used all of the samples available for each emotion, thereby including the transitional regions. Because different emotions lasted for different lengths of time, the 2,000 samples in Data Set I were at times closer or farther away from the beginning of an emotion segment. To avoid this bias and to maximize data available for training and testing, Data Set II includes all the samples for all the emotions and signals over 20 days, roughly 2,000 to 5,000 samples per emotion per signal per day. With an average of twice the number of samples (or minutes of data) as Data Set I, Data Set II resulted in an average 10 percent gain in performance when we compared it to Data Set I across all the methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 .Fig. 1 . 2 . 3 . 4 . 5 . 6 .</head><label>1123456</label><figDesc>Fig. 1. Examples of physiological signals measured from a user while she intentionally expressed anger (left) and grief (right). From top to bottom: electromyogram (microvolts), blood volume pressure (percent reflectance), skin conductivity (microSiemens), and respiration (percent maximum expansion). Each box shows 100 seconds of response. The segments shown here are visibly different for the two emotions, which was not true in general.</figDesc><graphic coords="7,138.84,69.17,288.79,228.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 . 5 .</head><label>25</label><figDesc>Fig. 2. Power spectral density from 0.0-0.4 Hz in the respiration signal for eight emotions. The heights of the four bins shown here were used as features f U À f IH .</figDesc><graphic coords="9,31.07,69.17,241.29,201.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Three Methods Applied to Data Set I, Starting with the 24 Features " , ' , , , , , P iY fY qYThe best-recognized emotions are denoted by their first initial: (N)eutral, (A)nger, (G)rief, (J)oy, (R)everence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of a highly day-dependent feature for two emotions from two different days. (a) The feature values for (A)nger and (J)oy from two different days. (b) Addition of an extra dimension allows for a line to separate Anger from Joy. The data can be projected down to line , so the addition of the new dimension did not increase the final number of features. (c) In the case of data from three different days, the addition of two extra dimensions allows for a plane p to separate Anger from Joy. The data can again be projected down to line , not increasing the final number of features.</figDesc><graphic coords="11,64.80,69.17,436.88,164.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Rosalind W. Picard earned the bachelors degree in electrical engineering with highest honors from the Georgia Institute of Technology in 1984. She was named a National Science Foundation Graduate Fellow and worked as a member of the technical staff at AT&amp;T Bell Laboratories from 1984-1987, designing VLSI chips for digital signal processing and developing new methods of adaptive image compression. She earned the masters and doctorate degrees, both in electrical engineering and computer science, from the Massachusetts Institute of Technology (MIT) in 1986 and 1991, respectively. In 1991, she joined the MIT Media Laboratory as an assistant professor. She was promoted to associate professor in 1995 and awarded tenure at MIT in 1998. She is author or co-author of more than 80 peer reviewed scientific articles in pattern recognition, multidimensional signal modeling, computer vision, and human-computer interaction. She is a co-recipient with Tom Minka of a best paper prize (1998) from the Pattern Recognition Society for work on machine learning with multiple models. Dr. Picard guest edited the IEEE Transactions on Pattern Analysis and Machine Intelligence special issue on Digital Libraries: Representation and Retrieval, and edited the proceedings of the First IEEE International Workshop on Content-Based Access of Image and Video Libraries, for which she served as chair. Her award-winning book, Affective Computing, (MIT Press, 1997) lays the groundwork for giving machines skills of emotional intelligence. She is a senior member of the IEEE and a member of the IEEE Computer Society. Elias Vyzas received the BEngr degree in mechanical engineering from Imperial College, London, England in June 1994, and two MS degrees, one in mechanical engineering and one in electrical engineering and computer science, from MIT in January 1997. He received the masters degree in mechanical engineering from MIT in June 1999 and is currently completing service in the Greek Army. His research interests include affect and stress recognition from physiology, and assessment of mental workload and performance. Jennifer Healey received the BS (1993), the MS (1995) and the PhD (2000) degrees from MIT in electrical engineering and computer science. Her Master's thesis work in the field of optics and opto-electronics was completed at the Charles Stark Draper Laboratory. In 1995, she joined the MIT Media Laboratory and helped launch the Affective Computing research project. She held a postdoctoral position at IBM's Zurich Research Laboratory in artifical intelligence and knowledge management. She is currently a research staff member in the Human Language Technologies Group at IBM's T.J. Watson Research Center.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 3</head><label>3</label><figDesc>Number of Dimensions and Number of Features m Used in the Fisher Projections and Feature Selection Algorithms that Gave the Results of Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 4 Classification</head><label>4</label><figDesc>Accuracy for All Eight Emotions for Data Set I and Data Set II Improves with the Use of the Day Matrix</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 5 Data</head><label>5</label><figDesc>Set I, 7-Emotion (All but the Neutral State) Classification Results, Comparing Three Methods for Incorporating the Day Information</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 6 Comparative</head><label>6</label><figDesc>Classification Rates for Eight Emotions for Data Set II</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 8 Rearrangements of Table 7 ,</head><label>8of7</label><figDesc>Showing Confusion Matrices for Emotions Having Similar Valence Ratings (IQWaITH VU Percent) and Similar Arousal Ratings (IQSaITH VR Percent)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 7 Confusion</head><label>7</label><figDesc>Matrix for the Method that Gave the Best Performance Classifying Eight Emotions with Data Set II (81.25 Percent) An entry's row label is the true class, the column label is what it was classified as.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 23, NO. 10, OCTOBER 2001</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The electromyogram is the only signal for which this sampling rate should have caused aliasing. However, our investigation of the signal showed that it registered a clear response when the jaw was clenched versus relaxed; thus, it was satisfactory for gathering coarse muscle tension information.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank Rob and Dan Gruhl for help with hardware for data collection, the MIT Shakespeare Ensemble for acting support, Anil Jain and Doug Zongker for providing us their SFFS code, Tom Minka for help with its use, and the anonymous reviewers for helping improve the paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr">TABLE 9</ref> <p>Summary (Along Rows) of when Each Feature Was Chosen in the SFFS-Based Methods of Table <ref type="table">6</ref>: The Standard SFFS (S), SFFS Followed by Fisher Projection (SF), and SFFS Followed by Fisher Projection Using the Day Matrix (SFD)</p><p>The features listed in the left-most column are grouped by signal for easier physiological interpretation: electromyogram (i), blood-volume pressure (f), heart rate (r), skin conductivity (), and respiration (). The totals at the right, when high, suggest that the feature may be a robust one regardless of the classification method.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Error: Emotion, Reason, and the Human Brain</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Damasio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Descartes</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Gosset/Putnam Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The Emotional Brain</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ledoux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Simon &amp; Schuster</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<author>
			<persName><forename type="first">P</forename><surname>Salovey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ªEmotional Intelligence,º Imagination</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="185" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Goleman</surname></persName>
		</author>
		<title level="m">Emotional Intelligence</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Bantam Books</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Reeves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nass</surname></persName>
		</author>
		<title level="m">Center for the Study of Language and Information</title>
		<imprint>
			<publisher>Cambridge Univ. Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
	<note>The Media Equation</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Children with Autism: A Developmental Perspective</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Capps</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Harvard Univ. Press</publisher>
			<pubPlace>Cambridge, Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Affective Computing</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ªA New Affect-Perceiving Interface and Its Application to Personalized Music Selection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Healey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dabek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop Perceptual User Interfaces</title>
		<meeting>Workshop Perceptual User Interfaces</meeting>
		<imprint>
			<date type="published" when="1998-11">Nov. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Healey</surname></persName>
		</author>
		<author>
			<persName><surname>Wearables</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personal Technologies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="231" to="240" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">ªSpeech and Emotional States,º Speech Evaluation in Psychiatry</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<editor>J.K. Darby</editor>
		<imprint>
			<date type="published" when="1981">1981</date>
			<publisher>Grune and Stratton, Inc</publisher>
			<biblScope unit="page" from="189" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ªAcoustic Profiles in Vocal Emotion Expression</title>
		<author>
			<persName><forename type="first">R</forename><surname>Banse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="614" to="636" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Polzin</surname></persName>
		</author>
		<title level="m">ªDetecting Verbal and Non-Verbal Cues in the Communication of Emotions</title>
		<imprint>
			<date type="published" when="2000-06">June 2000</date>
		</imprint>
		<respStmt>
			<orgName>School of Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">during ICASSP &apos;99 Panel on Speech Under Stress</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><surname>Comm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Acoustics, Speech, and Signal Processing &apos;99</title>
		<meeting>Int&apos;l Conf. Acoustics, Speech, and Signal essing &apos;99</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ªEmotion Recognition: The Role of Facial Movement and the Relative Importance of Upper and Lower Areas of the Face</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Bassili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2049" to="2058" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ªRecognizing Human Facial Expressions from Log Image Sequences Using Optical Flow</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yacoob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="636" to="642" />
			<date type="published" when="1996-06">June 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gardner</surname></persName>
		</author>
		<title level="m">ªProsody Analysis for Speaker Affect Determination,º Proc. Workshop Perceptual User Interfaces &apos;97</title>
		<imprint>
			<date type="published" when="1997-10">Oct. 1997</date>
			<biblScope unit="page" from="45" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interpretation and Synthesis of Facial Expressions</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName><surname>ªanalysis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mass. Inst. of Technology</title>
		<imprint>
			<date type="published" when="1995-02">Feb. 1995</date>
			<pubPlace>Media Lab, Cambridge, Mass</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Zlochower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<title level="m">ªAutomated Face Analysis by Feature Point Tracking has High Concurrent Validity with Manual FACS Coding,º Psychophysiology</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="35" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ªMeasuring Facial Expressions by Computer Image Analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="253" to="263" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ªClassifying Facial Actions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="974" to="989" />
			<date type="published" when="1999-10">Oct. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Desilva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Miyasato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakatsu</surname></persName>
		</author>
		<title level="m">ªFacial Emotion Recognition Using Multi-Modal Information,º Proc. IEEE Int&apos;l Conf. Information, Comm., and Signal Processing</title>
		<imprint>
			<date type="published" when="1997-09">Sept. 1997</date>
			<biblScope unit="page" from="397" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
		<title level="m">ªBimodal Emotion Recognition by Man and Machine,º Proc. ATR Workshop Virtual Communication Environments</title>
		<imprint>
			<date type="published" when="1998-04">Apr. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Miyasato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakatsu</surname></persName>
		</author>
		<title level="m">ªMultimodal Human Emotion/Expression Recognition,º Proc. Third Int&apos;l Conf. Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="1998-04">Apr. 1998</date>
			<biblScope unit="page" from="366" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ªInferring Psychological Significance from Physiological Signals</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Cacioppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Tassinary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. Psychologist</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="16" to="28" />
			<date type="published" when="1990-01">Jan. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<author>
			<persName><forename type="first">W</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>James</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Writings 1878-1899</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page">1890</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lange Theory of Emotions: A Critical Examination and an Alternative Theory,º</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Cannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Psychology</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="106" to="124" />
			<date type="published" when="1927">1927</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">ªThe Interaction of Cognitive and Physiological Determinants of Emotional State,º Advances in Experimental Psychology</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schachter</surname></persName>
		</author>
		<editor>L. Berkowitz</editor>
		<imprint>
			<date type="published" when="1964">1964</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="49" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Levenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<title level="m">ªAutonomic Nervous System Activity Distinguishes Among Emotions,º Science</title>
		<imprint>
			<date type="published" when="1983-09">Sept. 1983</date>
			<biblScope unit="volume">221</biblScope>
			<biblScope unit="page" from="1208" to="1210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ªFacial and Autonomic Manifestations of the Dimensional Structure of Emotion</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Winton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Putnam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="195" to="216" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">ªElectromyographic Studies of Facial Expressions of Emotions and Patterns of Emotions,º Social Psychophysiology: A Sourcebook</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Fridlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Izard</surname></persName>
		</author>
		<editor>J.T. Cacioppo and R.E. Petty</editor>
		<imprint>
			<date type="published" when="1983">1983</date>
			<biblScope unit="page" from="243" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">ªThe Psychophysiology of Emotion,º Handbook of Emotions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Cacioppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Berntson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Poehlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Ito</surname></persName>
		</author>
		<editor>M. Lewis and J.M. Haviland-Jones</editor>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="173" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">ªThe Psychophysiology of Emotion,º Handbook of Emotions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Keltner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<editor>M. Lewis and J.M. Haviland-Jones</editor>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="236" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Clynes</surname></persName>
		</author>
		<title level="m">Sentics: The Touch of the Emotions</title>
		<imprint>
			<publisher>Anchor Press/ Doubleday</publisher>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Izard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ªFour Systems for Emotion Activation: Cognitive and Noncognitive Processes</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="68" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">ªFinger-Pressure Waveforms Measured on Clynes&apos; Sentograph Distinguished Among Emotions,º Perceptual and Motor Skills</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsuda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="371" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ªThree Dimensions of Emotion</title>
		<author>
			<persName><forename type="first">H</forename><surname>Schlosberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Rev</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="81" to="88" />
			<date type="published" when="1954-03">Mar. 1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ªThe Emotion Probe: Studies of Motivation and Attention</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="372" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">ªWearable and Automotive Systems for Affect Recognition from Physiology</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Healey</surname></persName>
		</author>
		<idno>526</idno>
	</analytic>
	<monogr>
		<title level="j">Mass. Inst. Technology</title>
		<imprint>
			<date type="published" when="2000-05">May 2000</date>
			<pubPlace>Cambridge, Mass</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Vyzas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
		<title level="m">ªAffective Pattern Classification,º Proc. AAAI 1998 Fall Symp., Emotional and Intelligent: The Tangled Knot of Cognition</title>
		<imprint>
			<date type="published" when="1998-10">Oct. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">ªOffline and Online Recognition of Emotion Expression from Physiological Data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vyzas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop Emotion-Based Agent Architectures, Third Int&apos;l Conf. Autonomous Agents</title>
		<meeting>Workshop Emotion-Based Agent Architectures, Third Int&apos;l Conf. Autonomous Agents</meeting>
		<imprint>
			<date type="published" when="1999-05">May 1999</date>
			<biblScope unit="page" from="135" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Discrete-Time Signal Processing</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Oppenheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Schafer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, N.J.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">ªCorrecting Psychophysiological Measures for Individual Differences in Range</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Lyyken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Luther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychophysiological Bulletin</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="481" to="484" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Lyyken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Venables</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ªDirect Measurement of Skin Conductance: A Proposal for Standardization</title>
		<imprint>
			<date type="published" when="1971">1971</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="656" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">ªThe Electrodermal System,º Principles of Psychophysiology: Physical, Social, and Inferential Elements</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Dawson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Schell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Filion</surname></persName>
		</author>
		<editor>J.T. Cacioppo and L.G. Tassinary</editor>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="295" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Pudil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Novovicova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<title level="m">ªFloating Search Methods in Feature Selection,º Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="1994-11">Nov. 1994</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1119" to="1125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">ªFeature Selection: Evaluation, Application, and Small Sample Performance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zongker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Patern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="163" />
			<date type="published" when="1997-02">Feb. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">ªSimultaneous Learning of Decision Rules and Important Attributes for Classification Problems in Image Analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pudil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Novovicova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="193" to="198" />
			<date type="published" when="1994-04">Apr. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<title level="m">Pattern Classification and Scene Analysis</title>
		<imprint>
			<publisher>Wiley-Interscience</publisher>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ªNearest Neighbor Pattern Classification</title>
		<imprint>
			<date type="published" when="1967-01">Jan. 1967</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="21" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Healey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
		<title level="m">ªDigital Processing of Affective Signals,º Proc. IEEE Int&apos;l Conf. Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
		<title level="m">ªFrustrating the User on Purpose: A Step Toward Building an Affective Computer,º Interaction with Computers</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
