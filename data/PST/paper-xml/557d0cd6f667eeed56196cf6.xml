<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Overview of the SPHINX Speech Recognition System</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>MEMBER, IEEE, HSIAO-WUEN</roleName><forename type="first">Kai-Fu</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University. Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">A</forename><forename type="middle">N D</forename><surname>Hon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University. Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>FELLOW. IEEE</roleName><forename type="first">Raj</forename><surname>Reddy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University. Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Overview of the SPHINX Speech Recognition System</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1DECF91EE96240FF15408D3F58E208BF</idno>
					<note type="submission">received July 5. 1988; revised March 2 2 . 1989. &apos; Processing, Apr. 1987.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Speaker independence, continuous speech, and large vocabularies pose three of the greatest challenges in automatic speech recognition. Previously, accurate speech recognizers avoided dealing simultaneously with all three problems. This paper describes SPHINX, a system that demonstrates the feasibility of accurate, large-vocabulary speaker-independent, continuous speech recognition.</p><p>SPHINX is based on discrete hidden Markov models (HMM's) with LPC-derived parameters. To provide speaker independence, we added knowledge to these HMM's in several ways: multiple codebooks of fixed-width parameters, and an enhanced recogniar with carefully designed models and word duration modeling. To deal with coarticulation in continuous speech, yet still adequately represent a large vocabulary, we introduce two new subword speech units-function-worddependent phone models and generaliied triphone models. With grammars of perplexity 997, 60, and 20, SPHINX attained word accuracies of 71, 94, and 96 percent on a 997-word task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION ONSIDERABLE progress has been made in speech</head><p>C recognition in the past 15 years. Many successful systems [ 11-[7] have emerged. Each of these systems has attained very impressive accuracy. However, they owe their success to one or more of the constraints they impose. This paper describe SPHINX, a system that tries to overcome three of these constraints: 1) speaker dependence, 2) isolated words, and 3) small vocabulary.' Speaker independence has been viewed as the most difficult constraint to overcome. This is because most parametric representations of speech are highly speaker dependent, and a set of reference patterns suitable for one speaker may perform poorly for another speaker. Researchers have found that errors increased by 300-500 percent when a speaker-dependent system is trained and tested in speaker-independent mode [SI,<ref type="bibr" target="#b8">[9]</ref>. Because of these difficulties, most speech recognition systems are speaker dependent. In other words, they require a speaker to "train" the system before reasonable performance can Manuscript</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>be expected. This training phase typically requires several hundred sentences. While speaker-trained systems are useful for some applications, they are inconvenient, less robust, more wasteful, and simply unusable for some applications. Speaker-independent systems must train on less appropriate training data. However, many more data can be acquired, which may compensate for the less appropriate training material.</p><p>Continuous speech recognition is significantly more difficult than isolated word recognition. Its complexity is a result of three innate properties of continuous speech. First, word boundaries are difficult to locate. Second, courticulutory effects are much stronger in continuous speech, causing the same sound to appear differently in various contexts. Third, content words (nouns, verbs, adjectives, etc.) are often emphasized, whilefincrion words (articles, prepositions, pronouns, short verbs, etc.) are poorly articulated. Error rates increase drastically from isolated-word to continuous speech. For example, Bahl et al. <ref type="bibr">[lo]</ref> reported a 280 percent error rate increase from isolated-word to continuous speech recognition. However, in spite of these problems and degradations, we believe that it is important to work on continuous speech research. Only with continuous speech can we achieve the desired speed and naturalness of man-machine communications.</p><p>Large vocubulury typically implies a vocabulary of about 1000 words or more. As vocabulary size increases, so does the number of confusable words. Also, larger vocabularies require the use of subword models, because it is difficult to train whole word models. Unfortunately, subword units usually lead to degraded performance because they cannot capture coarticulatory (interunit) effects as well as word models can. Error rate increased by 200-1000 percent in several studies <ref type="bibr">[ I l l -[ 131.</ref> In spite of these problems, large vocabulary systems are still needed for many versatile applications, such as dictation, dialog systems, and speech translation systems.</p><p>In this paper, we describe SPHINX, a large-vocabulary speaker-independent, continuous speech recognition system. SPHINX employs discrete hidden Markov models (HMM's) with LPC-derived parameters. To deal with speaker independence, we added knowledge to these HMM's in several ways. We represented additional knowledge through the use of multiple vector quantized codebooks. We also enhanced the recognizer with carefully designed models and word duration modeling. To 0096-3518/90/0100-0035$01 .OO 0 1990 IEEE deal with coarticulation in continuous speech, yet adequately represent a large vocabulary, we introduced two new speech units-function-word-dependent phone models and generalized triphone models. With these techniques, SPHINX achieved speaker-independent word recognition accuracies of 7 1, 94, and 96 percent on the 997word DARPA resource management task [ 141 with grammars of perplexity 997, 60, and 20.</p><p>In this paper, we first describe the task and database used for evaluating SPHINX in the following section. Section I11 then describes a baseline implementation of SPHINX. Enhancements to SPHINX using additional human knowledge and improved subword models are described in Sections IV and V. Section VI summarizes the results with SPHINX, and Section VI1 concludes with some final remarks. A full description of the SPHINX System can be found in [ 151 and [ 161.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">TASK A N D DATABASE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Resource Management Task</head><p>SPHINX was evaluated on the DARPA resource management task [ 141. This task, containing a vocabulary of 997 words, was designed for database query of naval resources. As such, there are a large number of long words, such as Apalachicola, Chattahoochee, and ECG041. These words are relatively easy to recognize. On the other hand, it also contains many confusable pairs, such as what/what 's, whut/was, thela, four/fourth, are/were, any/ many, etc. Also, there are many function words (such as a , and, of, the, t o ) , which are articulated very poorly and are hard to recognize or even locate. In particular, the and a are the most frequent words, but are optional according to the grammar.</p><p>The original grammar designed for the resource management task was a finite state grammar. This grammar had a perplexity of only about 9, which was too simple. Instead, we used three more difficult grammars with SPHINX: 1) null grammar (perplexity 997), where any word can follow any other word, 2) word-pair grammar (perplexity 60), a simple grammar that specifies a list of words that can legally follow any given word, and 3 ) bigram grammar (perplexity 20), a word-pair grammar that uses word-category transitions probabilities estimated from the grammar. It should be noted that the training and testing sentences were generated from the finite state grammar, which may reduce acoustic confusability <ref type="bibr">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The TIRM Database</head><p>Texas Instruments supplied Carnegie Mellon with a large speech database for the resource management task described in the previous section. The TIRM database contains 80 "training" speakers, 40 "development test" speakers, and 40 "evaluation speakers." At the time of this writing, only the 80 training speakers and the 40 development test speakers are available. Of these speakers, 85 are male and 35 are female, with each speaker reading 40 sentences generated by the sentence pattern grammar.</p><p>These sentences were recorded using a Sennheiser HMD-4 14-6, close-talking noise-cancelling headset-boom microphone in a sound-treated room. All speakers were untrained and instructed to read a list of sentences in a natural continuous fashion. The speech was sampled at 20 kHz at TI, downsampled to 16 kHz at the National Institute of Standards and Technology and saved on magnetic tapes.</p><p>In this study, all 80 training speakers, as well as 25 of the development test speakers, were used as training material. This gave us a total of 4200 training sentences. The remaining 15 development test speakers were set aside as testing speakers. Ten sentences were taken from each speaker, for a total of 150 test sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">THE BASELINE SPHINX SYSTEM</head><p>To establish a performance benchmark using standard HMM techniques on the resource management task, we began with a baseline HMM system. This system uses standard HMM techniques employed by many other systems [ 181-[20]. We will show that, using these techniques alone, we can already attain reasonable, albeit mediocre, accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Speech Processing</head><p>The speech is sampled at 16 kHz, and preemphasized with a filter whose transform function is 1-0.97z-'. The waveform is then blocked into frames. Each frame spans 20 ms, or 320 speech samples. Consecutive frames overlap by 10 ms, or 160 speech samples. Each frame is multiplied by a Hamming window with a width of 20 ms and applied every 10 ms.</p><p>From these smoothed speech samples, we computed the LPC coefficients using the autocorrelation method [2 11. LPC analysis was performed with order 14. Finally, a set of 12 LPC-derived cepstral coefficients was computed from the LPC coefficients. This representation is very similar to that used by <ref type="bibr">Shikano et al. [22]</ref> and <ref type="bibr">Rabiner et al. [23]</ref>.</p><p>The 12 LPC cepstrum coefficients for each frame were then vector quantized into one of 256 prototype vectors.</p><p>These vectors were generated by a variant of the Linde-Buzo-Gray algorithm 1241, [22] using Euclidean distance. We used 150 00 frames of nonoverlapped 20-ms coefficients extracted from 4000 sentences to generate the 256-vector codebook.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Phonetic Hidden Markov Models</head><p>Hidden Markov Models (HMM) were first described by Baum <ref type="bibr" target="#b22">[25]</ref>. Shortly afterwards, they were independently extended to automatic speech recognition by <ref type="bibr">Baker [26]</ref> and <ref type="bibr">Jelinek [27]</ref>. However, only in the past few years have HMM's become the predominant approach to speech recognition.</p><p>HMM's are parametric models particularly suitable for describing speech events. The success of HMM's is largely due to the forward-backward reestimation algorithm [ 191, which is a special case of the EM algorithm 1251. Every iteration of the algorithm modifies the parameters to increase the probability of the training data until a local maximum has been reached.</p><p>Because the resource management task is a large-vocabulary one, we cannot adequately train a model for each word. Thus, we have chosen to use phonetic HMM's, where each HMM represents a phone. There are a total of 45 phones, each characterized by { s }-a set of states including an initial state S, and a final state SF, { a,,}-a set of transitions where U,, is the probability of taking a transition from state i to statej, { b,, (k)}-the output probability matrix: the probability of emitting symbol k when taking a transition from state i to statej, k corresponds to one of the 256 VQ codes.</p><p>Each phonetic HMM has the topology shown in Fig. <ref type="figure">1</ref>. The three self-loops model three parts of a phone, and the lower transitions explicitly model durations of one, two, or three frames. Instead of assigning a unique output pdf to each transition, each phone is assigned three distributions, representing the beginning, middle, and end of the phone. Each of these three distributions is shared by several transitions. This model is almost identical to that used by IBM <ref type="bibr">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training</head><p>To initialize our phone model parameters, we used hand-segmented and hand-labeled segments from 2240 TIMIT 1291 sentences. We ran one iteration of fonvardbackward on these hand-labeled phone segments, and produced a model for each phone. This set of 45 phone models was used to initialize the parameters in the actual training.</p><p>After this initialization, we ran the forward-backward algorithm on the resource management (TIRM) training sentences. For each of the 4200 sentences, we created a sentence model from word models, which were in turn concatenated from phone models. To determine the phonetic spelling of a word, we used a pronunciation dictionary adopted from the baseform of the ANGEL System 1301, where each word is mapped to a single linear sequence of phones. Then, to create a sentence model from word models, we accounted for possible between-word silences by inserting a mandatory silence model at the beginning and at the end of the sentence. Between-word silences were also allowed, but were optional. This sentence model represents the expected pronunciation of the sentence. It was trained against the actual input speech using the forward-backward algorithm [ 191.</p><p>Two iterations of forward-backward training were then run. Most other HMM systems run more iterations, but we found that with our appropriate initialization, two it- erations were sufficient. The trained transition probabilities were used directly in recognition. The output probabilities, however, were smoothed with a uniform distribution to avoid probabilities that were too small.</p><p>The SPHINX recognition search is a standard time-synchronous Viterbi beam search <ref type="bibr">[19]</ref>, <ref type="bibr">[20]</ref>. The search processes input speech time synchronously, completely updating all accessible states for a time frame t -1 before moving on to frame t. The update for time t consists of two stages. First, for each within-word transition between states sf,. <ref type="figure">,,,,,</ref> and<ref type="figure" target="#fig_0">S,,, if P(S/,.~,,,~, t -1</ref>). P(trunsition) -P(output) is greater than P ( s r o , t ) , then P ( s r o , t ) is updated. Second, for the final state of every word, all legal word successors are tried, using P (transition ) derived from the language model.</p><p>In the Viterbi beam search, a hypothesis is pruned if its log probability is less than that of the best hypothesis by more than a preset threshold. We found it is possible to prune 80-90 percent of the hypotheses without any loss in accuracy. After the search is completed, a backtrace is performed to recover the best path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results</head><p>The results with the baseline SPHINX system, using 15 new speakers with 10 sentences each for evaluation, are shown in Table <ref type="table" target="#tab_0">I</ref>. To determine the recognition accuracy, we first align the recognized word string against the correct word string using a string match algorithm supplied by the National Institute of Standards and Technology [3 11. This alignment determines WordsCorrect, Substitutions, Deletions, Insertions. Finally, Percentcorrect and WordAccuracy are computed by</p><formula xml:id="formula_0">Percent Correct Words Correct loo ' Correct Length - - Word Accuracy Correct Length-Subs-Dels-Ins Correct Length = 100 .<label>( 2 )</label></formula><p>Confusions between homonyms (such as ship's and ships, or two and t o o ) are not counted for the null language model, and are counted for the word pair and the bigram language model. The results of this system are mediocre at best. Since</p><p>Word-Pair the bigram grammar already imposes tight constraints, we concluded that our baseline system was inadequate for any realistic large-vocabulary applications. In the subsequent sections, we describe our steps to improve the baseline SPHINX by incorporating knowledge and contextual modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. ADDING KNOWLEDGE TO SPHINX A . Fixed-Width Speech Parameters</head><p>The easiest way to add knowledge to HMM's is to introduce additional fixed-width parameters, or parameters than can be computed for every fixed-size frame. All we have to do is to devise a way of incorporating these parameters into the output pdf of the HMM's. In this section, we consider several types of frame-based parameters, and discuss possible ways of integrating them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I ) Bilinear Transform on the Cepstrum Coeficients:</head><p>The human ear's ability to discriminate between frequencies is approximated by a logarithmic function of the frequency, or a bark scale <ref type="bibr" target="#b19">[32]</ref>. Furthermore, Davis and Mermelstein 1331 have shown these logarithmically scaled coefficients yield superior recognition accuracy compared to linearly scaled ones. Therefore, there is strong motivation for transforming the LPC cepstrum coefficients into a mel-frequency scale.</p><p>Shikano <ref type="bibr" target="#b30">[34]</ref> reported significant improvement from using a bilinear transform 1351 on the LPC cepstral coefficients. Bilinear transform is a technique that transforms a linear frequency axis into a warped one using the allpass filter U,,,,,. = w + 2 tan-' ::</p><formula xml:id="formula_1">::sw) (4)</formula><p>where w is the sampling frequency expressed by the normalized angular frequency, a,,,,. is the converted frequency, and a is a frequency warping parameter. A positive a converts the frequency axis into a low-frequency weighted one. When a takes on values between 0.4 and 0.8, the frequency warping by a bilinear transform is comparable to that of the me1 or Bark scales. In this work, we use a value of 0.6 for a .</p><p>2) Differenced Cepstrum Coeflcients: Temporal changes in the spectra play an important role in human perception 1361. This is particularly true for speaker-independent recognition, where formant slopes are more reliable than absolute formant locations. Thus, it would be desirable to incorporate "slope" measurements into recognizers. Moreover, since HMM's assume each frame is independent of the past, it would be desirable to broaden the scope of a frame.</p><p>We use a simple slope measure, differenced LPC cepstrum coeflcients <ref type="bibr" target="#b30">[34]</ref>. The difference coefficients for frame n are the difference between the coefficients of frame n + 6 and n -6. In our current implementation, a differenced coefficient is computed every frame, with 6 = 2 frames, giving a 40 ms difference. In a preliminary experiment, we found this measure to be as good as the regression coeficienfs used in 1371 and <ref type="bibr">[7]</ref>.</p><p>3) Power and Differenced Power: Although LPCbased parameters perferm well in speech recognition, they do not contain sufficient information about power. For example, Coefficients in silence or noise regions are not very meaningful. Therefore, it is desirable to incorporate power into our recognizer. <ref type="bibr">Rabiner et al. [23]</ref> obtained significant improvement by adding power into the distance metric in vector quantization, and Shikano <ref type="bibr" target="#b30">[34]</ref> reported similar results. Finally, in a detailed study of prosody in speech recognition, <ref type="bibr">Waibel [38]</ref> found power to be the most important prosodic cue.</p><p>Since raw power may vary widely from speaker to speaker, we normalized power by subtracting the maximum power value in the sentence from each power value in the sentence. In our real-time system, we used an automatic gain control algorithm with a 250-ms look-ahead to predict the maximum power in a sentence.</p><p>Another important source of information is diferenced power, which is computed the same way as dtfferenced LPC cepstrum coeficients. Differenced power provides information about relative changes in amplitude or loudness. Indeed, our preliminary experiments indicated that differenced power is more useful than power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Integrating Fixed-Width Parameters in Multiple</head><p>Codebooks: There are many ways to integrate the above coefficients into the framework of a discrete HMM recognizer. We considered several possibilities [ 151, and decided to use multiple-codebook integration <ref type="bibr">[39]</ref>. Using this technique, coefficients are divided into sets, and each set is quantized into a separate codebook. We created three codebooks, each with 256 codes. These codebooks were generated from 1) bilinear-transformed LPC cepstrum coefficients, 2) differenced bilinear-transformed LPC cepstrum coefficients, and 3) a weighted combination of power and differenced power.</p><p>For each frame of speech, not one but several VQ codes are used to replace the input vector. Since each input frame is no longer a single symbol, but rather a vector of symbols, the discrete HMM algorithms must be modified to produce multiple symbols at each time frame. By assuming that the multiple output observations are independent, the output probability of emitting multiple symbols can then be computed as the product of the probability of producing each symbol.</p><p>The multiple-codebook approach has a distinct advantage over single-codebook approaches-namely , reduced used in VQ, ~ the distortion will be very large, which means the observed vectors will match their corresponding prototype vectors poorly. Multiple codebooks reduce the distortion by partitioning the feature space into several smaller subspaces. Table <ref type="table" target="#tab_0">I1</ref> clearly illustrates this point with the comparison of one-codebook distortion and three-codebook distortion.</p><p>Another advantage of multiple codebooks is the large increase in the dynamic range and precision of the resulting parameters. With three codebooks, there are 256' possible parameter combinations using just 256 x 3 parameters. With such an increase in precision comes the ability to make finer distinctions.</p><p>However, the independence assumption with multiple codebooks is inaccurate. Also, more memory and time are needed with multiple codebooks. But we felt that these disadvantages were well compensated by the advantages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Lexica UPhon olog ica 1 Improvements</head><p>Our next set of improvements involved the modification of the set of phones and the pronunciation dictionary. These changes lead to more accurate assumptions about how words are articulated, without changing our assumption that each word has a single pronunciation.</p><p>The first step we took was to replace the baseform pronunciation with the most likely pronunciation. For example, the first vowel of the word delete will appear as /iy/ in most dictionaries, but it is actually pronounced as /ih/ most of the time. This correction process modified about 40 percent of all the.baseforms.</p><p>With our linear representation of pronunciation, it is difficult to model the deletions of phonetic events. For example, the first /dl of the word did is always released, while the last /dl may be unreleased. Also, closures before stops are optional. We model these two types of deletions implicitly in the HMM parameters. We created separate models for the released stops and optional stops. We also merged closure-stop pairs as a single phone. These changes enabled the modeling of deletions within linear HMM's.</p><p>Although the English phonemes are well defined, there are actually many frequently used sounds that are not phonemic. For example, stop-fricative pairs such as /ks/, Ips/, Its/, /bz/, /dz/, or /gz/ are actually quite different from the concatenated phoneme pairs. They appear more like different affricates. Thus, it is sensible to model them as special phones. In this study, we only model Its/ in this fashion due to the lack of training data for the other nonphonemic affricates.</p><p>In order to improve the appropriateness of the word pronunciation dictionary, a small set of rules was created to 1) modify closure-stop pairs into optional compound phones when appropriate, 2) modify /t/'s and /d/'s into /dx/ when appropriate, 3) reduce nasal /t/'s when appropriate, and 4) perform other mappings such as It s/ to Its/.</p><p>Finally, there is the issue of what HMM topology is optimal for phones in general, and what topology is optimal for each phone. We found that although the choice of model was not critical for continuous speech recognition, the model shown in Fig. <ref type="figure">1</ref> led to the best results. In addition, we experimented with different ways of labeling the transitions, i.e., which output pdf should be tied to each transition. Each phone was assigned an appropriate set of tied transitions. The improvements in this section led to the set of phones enumerated in Table <ref type="table" target="#tab_3">111</ref>. These improvements have increased the number of phones from 45 to 48. Table <ref type="table" target="#tab_5">IV</ref> shows a section of our final phonetic pronunciation dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Word Duration Modeling</head><p>HMM's model duration of events with transition probabilities, which lead to a geometric distribution for the duration of state residence, for states with self-loops:</p><formula xml:id="formula_2">P l ( d ) = ( 1 -all) a:</formula><p>( 5 )</p><p>where P , ( d ) is the probability of taking the self-loop at state i for exactly d times. Several researchers have argued that this is an inadequate distribution for speech events, and proposed alternatives for duration modeling</p><p>We incorporated word duration into SPHINX as a part of the Viterbi search. The duration of a word is modeled by a univariate Gaussian distribution, with the mean and variance estimated from a supervised Viterbi segmentation of the training set. By precomputing the duration score for various durations, this duration model has essentially no overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results</head><p>We have presented various strategies for adding knowledge to SPHINX. The results of these strategies are shown in Table <ref type="table" target="#tab_5">V</ref>. The version abbreviations are defined in Table VI.</p><p>Consistent with earlier results [33], <ref type="bibr" target="#b30">[34]</ref>, we found that bilinear transformed coefficients improved the recognition rates, An even greater improvement came from the use of differential coefficients, power, and differenced power in three separate codebooks. Next, we enhanced the dictionary and the phone set-a step that led to an appreciable improvement.</p><p>[401, [411, 171.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Version Description</head><p>After adding four feature sets and three codebooks</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phonology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Duration</head><p>After all the dictionary and phonological improvements, plus implicit insenioddeletion modeling.</p><p>After integration of word duration probabilities into the Viterbi Search.</p><p>Finally, the addition of durational information significantly improved SPHINX'S accuracy when no grammar was used, but was not helpful with a grammar. With no grammar, the recognizer must consider many word hypotheses, and word duration modeling can filter out many hypotheses with implausible word durations. On the other hand, when a grammar is used, much more constraint is applied, sharply decreasing the utility of duration. Therefore, in subsequent versions, duration modeling is used only without grammar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONTEXT MODELING I N SPHINX</head><p>Given that we will use hidden Markov models to model speech, one important question is: what unit of speech should an HMM represent? In the previous sections, we have used phones as the fundamental unit of speech. An even more natural unit is words. In this section, we will discuss the strengths and weaknesses of word and phone models, as well as a number of other units proposed by earlier work. Then, we shall propose two new units that will substantially improve the performance of speaker-independent continuous speech recognizers. Finally, we will present comparative results of different variations of these units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A . Previously Proposed Units of Speech</head><p>Words are the most natural units of speech because they are exactly what we want to recognize. Word models are able to capture within-word contextual effects, so by modeling words as units, phonological variations can be assimilated. Therefore, when there are sufficient data, word models will usually yield the best performance. However, using word models in large-vocabulary recognition introduces several grave problems. Since training data cannot be shared between words, each word has to be trained individually. For a large-vocabulary task, this imposes too great a demand for training data and memory. Also, for many tasks, it would be convenient to provide the user with the option of adding new words to the vocabulary. If word models were used, the user would have to produce many repetitions of the word, which would be extremely inconvenient. Therefore, while word models are natural and model contexts well, because of the lack of sharing across words, they are not practical for large-vocabulary speech recognition.</p><p>In order to improve trainability, some subword unit has to be used. The most commonly used subword units are the phones of English. The implementation of SPHINX we have described thus far is based on phone models. With only about 50 phones in the English language, they can be sufficiently trained with just a few hundred sentences. We have seen that the earlier implementations of SPHINX yielded reasonably accurate results. However, studies [42], [ 131 have shown that well-trained word models outperform well-trained phone models. This is because phone models assume a phone in any context is equivalent to the same phone in any other context. However, phones are not produced independently, because our articulators cannot move instantaneously from one position to another. Thus, the realization of a phone is strongly affected by its immediate neighboring phones. Another problem with using phone models is that phones in function words, such as a , rhe, in, me, are often articulated poorly, and are not representative instances of the phones. Thus, while word models lack generality, phone models overgeneralize.</p><p>Word-dependent phones [12] are a compromise between word modeling and phone modeling. The parameters of a word-dependent phone model depend on the word in which the phone occurs. Like word models, word-dependent phone models can model word-dependent, phonological variations, but they also require considerable training and storage. However, with word-dependent phones, if a word has not been observed frequently, its parameters can be interpolated (or averaged) with those of context-independent phone models. This obviates the need of observing every word in training, and facilitates the addition of new words.</p><p>Another alternative-context-dependent phones [20], [ 121-i~ similar to word-dependent phones; instead of modeling phone-in-word, they model phone-in-context. The most commonly used context-dependent model is the triphone model. A triphone model is a phone-size model that takes into consideration the left and the right neighboring phones. Triphone modeling is powerful because it models the most important coarticulatory effects, and is much more sensitive than phone modeling. However, the large number of triphones causes them to be poorly trained, in spite of some robustness provided by interpolating with phones. Moreover, some phonetic contexts are quite similar, and triphones cannot take advantage of that.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Function-Word Dependent Phones</head><p>Function words are typically prepositions, conjunctions, pronouns, articles, and short verbs, such as the, a , in, are. Function words are particularly problematic in continuous speech recognition because they are typically unstressed. Moreover, the phones in function words are distorted in many ways. They may be shortened, omitted, or seriously affected by neighboring contexts. Since these effects are specific to the individual function words, explicit modeling of phones in these function words should lead to a much better representation. Function words have caused considerable problems in SPHINX. Function words take up only 4 percent of the vocabulary, or about 30 percent if weighed by frequency, yet they are accountable for almost SO percent of the errors.</p><p>In view of the above analysis, we propose a new speech unit: function-word-dependent phones. Function-worddependent phones are the same as word-dependent phones, except they are only used for function words. This strategy improves the modeling of the most difficult subset of words. Because function words occur frequently in any large-vocabulary task, function-word-dependent phones are readily trainable.</p><p>We selected a set of 42 function words (shown in Table <ref type="table" target="#tab_6">VII</ref>), for which we felt there were significant word-dependent coarticulatory effects, as well as adequate training data. A few of these words are not usually considered function words, but were appropriate for this task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Generalized Triphones</head><p>Although triphones model the most important coarticulatory effects, they are sparsely trained and consume substantial memory. We now describe a technique to deal with these problems by combining similar triphones. This approach is justified by the fact that some phones have the same effect on neighboring phones [ 151. By merging similar triphones, we both improve the trainability and reduce the memory usage.</p><p>We created generalized triphones by merging contexts with an agglomerative clustering procedure <ref type="bibr" target="#b39">[43]</ref>. where D ( a , b ) is the distance between two models of the same phone in context a and b. P,, ( i ) is the output probability of codeword i in model a , and N l f ( i ) is the forward-backward count of codeword i in model a . m is the merged model obtained by adding N,, and Nh. In measuring the distance between the two models, we only consider the output probabilities and ignore the transition probabilities, which are of secondary importance.</p><p>Equation ( 6 ) measures the ratio between the probability that the individual distributions generated the training data and the probability that the combined distribution generated the training data. This ratio is consistent with the maximum-likelihood criterion used in the forwardbackward algorithm. This distance metric is equivalent to, and was motivated by, entropy clustering used by <ref type="bibr">[44]</ref> and <ref type="bibr">[28]</ref>.</p><p>This context generalization algorithm provides the ideal means for finding the equilibrium between trainability and +Fnwd-dep.</p><p>153 62.9% (57.0%) sensitivity. Given a fixed amount of training data, it is possible to find the largest number of trainable detailed models. Armed with this technique, we could attack any problem and find the "right" number of models that are as sensitive and trainable as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Smoothing Detailed Models</head><p>While the detailed models introduced in the previous sections are more accurate models of acoustics-phonetics, they are less robust because many output probabilities will be zeros, which can be disastrous to recognition. We could intelligently replace the zeros with nonzero probabilities by combining these detailed models with other more robust ones. For example, we could combine the function-word-dependent phone models or the generalized triphone models with the robust context-independent phone models.</p><p>An ideal solution for weighting different estimates of the same event is deleted interpolated estimation 1451. Deleted interpolation weighs each distribution according to its ability to predict unseen data. By equating these weights to transition probabilities on parallel transitions, the interpolation problem is transformed into an HMM problem, and the weights are learned by the fonvardbackward algorithm.</p><p>In our implementation for training detailed (functionword-dependent and generalized triphone) models, we first initialized the detailed models with the general (context-independent) models. Two iterations of the normal fonvard-backward algorithm were run using detailed modeling. During the last iteration, we divided the data into two blocks, and maintained separate output and transition counts for each block. After the end of the last iteration, 100 iterations of deleted interpolation were run to combine: a detailed model (function-word-dependent or generalized triphone), a general model (context-independent phone models-the counts for a general model are the sum of the counts in all the detailed models that correspond to the general model), Thus, this procedure not only combined detailed (but less robust) models with robust (but less detailed) models, but also smoothed the distribution using the uniform distribution. The summary of the entire training procedure is illustrated in Fig. <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Results</head><p>Table <ref type="table" target="#tab_9">VI11</ref> shows that the direct modeling of phones in function words substantially reduced errors.    As indicated in Table <ref type="table" target="#tab_6">VIII</ref>, generalized triphone modeling led to another substantial improvement. We ran the agglomerative clustering algorithm to reduce 238 1 triphones to 1000 generalized triphones. Combined with function-word-dependent phones, there were a total of 1076 models.</p><p>More detailed descriptions and results on contextual modeling can be found in [ 1-51 and [46].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. SUMMARY OF RESULTS</head><p>Fig. <ref type="figure">3</ref> shows improvements from all versions of SPHINX described in this paper. The six versions in Fig. <ref type="figure">3</ref> correspond to the following descriptions with incremental improvements:</p><p>1) the baseline system, which uses only LPC cepstral parameters in one codebook;</p><p>2) the addition of differenced LPC cepstral coeficients, power, and differenced power in one codebook;</p><p>3) all four feature sets were used in three separate codebooks (this version was reported in <ref type="bibr">[47]</ref>, the first description of the SPHINX System; 4) tuning of phone models and the pronunciation dictionary, and the use of word duration modeling; 5 ) function word dependent phone modeling (this version was reported in <ref type="bibr" target="#b43">[48]</ref>); and 6 ) generalized triphone modeling (this version was reported in [ 151 and <ref type="bibr" target="#b44">[49]</ref>.</p><p>Table <ref type="table" target="#tab_10">X</ref> shows the word accuracy, gender, and geographical distribution of the 15 testing speakers. Although the performance appears to vary from speaker to speaker, this variability is not predictable from the speaker's gender or dialect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>We have described SPHINX-a hidden Markov modelbased system for large-vocabulary speaker-independent continuous speech recognition. On the one hand, HMM's perform better with detailed models. On the other hand, HMM's need considerable training. This need is accentuated in large-vocabulary speaker-independence, and discrete HMM's. However, given a fixed amount of training, model specificity and model trainability pose two incompatible goals. More specificity usually reduces trainability, and increased trainability usually results in over generality.</p><p>Thus, our work can be viewed as finding an equilibrium between specificity and trainability . To improve trainability, we used one of the largest speaker-independent To improve specificity, we used multiple codebooks of various LPC-derived features, and integrated external knowledge sources into the system. We also improved the phone set to include multiple representations of some phones, and introduced the use of function-word-dependent phone modeling and generalized triphone modeling.</p><p>Through these techniques we have demonstrated that large-vocabulary speaker-independent continuous speech recognition is feasible. We believe that with a powerful learning paradigm, the performance of a system can always be improved with more training data, subject to our ability to make the models more sophisticated. The SO- phisticated modeling techniques introduced in this paper reduced the error rate of our baseline system by as much as 85 percent, resulting in accuracies of 71, 94, and 96 percent for a 997-word vocabulary with grammars of perplexity 997, 60, and 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>, The authors wish to thank the members of the Carnegie Mellon Speech Group. In particular, we would like to acknowledge Hwang, R. Bisiani, and J. Polifroni. We would also like to thank P. Brown and R. Schwartz for helpful discussions, suggestions, and critique. Finally, we thank R. Taylor for reading drafts of this paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. I .</head><label>I</label><figDesc>Fig. I . The phone HMM used in baseline SPHINX. The label on a transition represents the output pdf to which the transition is tied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2</head><label></label><figDesc>LEE cr al.: SPHINX SPEECH RECOGNITION SYSTEM quantization error. If too many features are</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>t s / IEEE TRANSACTIONS ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING. VOL. 38. NO. I . J A N U A R Y 1990</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>1) Generate an HMM for every triphone context. 2) Create clusters of triphones, with each cluster con-3 ) Find the most similar pair of clusters that represents 4) For each pair of clusters, consider moving every i) Move the element if the resulting configuraii) Repeat until no such moves are left. sisting of one triphone initially. the same phone, and merge them together.element from one to the other.tion is an improvement.5 ) Until some convergence criterion is met, go to stepTo determine the similarity between two models, we 2. use the following distance metric:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>I-ROM ~U N C T l O N -~O R D -~I -, l ' I -" T PHONE MOL)t:LIYG ANI) G E N E R A L I Z ~I ) TRlPHONri M O D E L I V G . RESULI'S SHOWN ARE PERCIN I -CORRECT (WORD-ACCURACY) Version IModelsI Nogrammar I Word pair 1 Bigram Context-ind.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>I</head><label></label><figDesc>48 155.1% (49.6%) 186.8% (84.4%) 191.2% (90.6%) 01-FUNCTION WORD ERRORS AND N O ~F F U N C I ION-'YORI) E R R O R S WITH AND WITHOUT FUNCTloN-WORD-Dt:Pl~Nl)l:.N I_ PHONE MODELING. CONTEXT-INDFPENDENT MODELS WERt. USlil) WITHOUT G R A M</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>20. 0 Fig. 3 .</head><label>03</label><figDesc>Fig. 3. Results of five versions of SPHINX.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I BASELINE</head><label>I</label><figDesc>SPHINX RESULTS. EVALUATED ON 150 SENTENCES FROM 15</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>SPEAKERS 60 61.8% I 58.1% Grammar I Perplexity I Percent Correct Word Accuracy Sone</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>I 991 I</cell><cell>31.1%</cell><cell>I</cell><cell>2s.xc</cell></row><row><cell>Bigram</cell><cell>20</cell><cell>76.1%</cell><cell></cell><cell>74.8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 11 QUANTIZATION ERROR OF A SINC1.F CODEBOOK VFRSlJS THF TOTAL QUANTILATION ERROR IN THREE CODEBOOKS Codebook I 1-codebook 1 3-codebook Size distortion distortion 1 .oo</head><label>11</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>0.48</cell></row><row><cell></cell><cell>0.83</cell><cell>0.39</cell></row><row><cell>128 256</cell><cell cols="2">0.72 I 0.61 I 0.25 0.31</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 111 LIST OF IHE IMPROVED SETOF PHONES IN SPHINX</head><label>111</label><figDesc>Example I Phone I Example 1 Phone I Example 1</figDesc><table /><note><p>Phone I</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 1V A SECTION OF THE SPHINX DICTIONARY WITH WORD, ORIGINAL BAS[:FORM, AND THE PRONUNCIATION AFTER RULE APPLICATION Word ADDED ADDING AFFECT AFTER AGAIN AJAX ALASKA ALERT ALERTS</head><label>1V</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V THE SPHINX RESULTS WITH KNOWLEDGE ENHANCEMENTS. RESULTS SHOWN ARE PERCENT-CORRECT (WORD-ACCURACY)</head><label>V</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI THE DEFINITION OF THE VERSION ABBREVIATIONS USED IN TABLE v</head><label>VI</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V11 THE LIST OF 42 FUNCTION WORDS THAT SPHINX MOIIE1.S S t P A K A T E L Y A ALL AND ANY</head><label>V11</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ARE</cell><cell>AT</cell><cell>BE</cell></row><row><cell>BEEN</cell><cell>BY</cell><cell>DID</cell><cell>FIND</cell><cell>FOR</cell><cell>FROM</cell><cell>GET</cell></row><row><cell>GIVE</cell><cell>HAS</cell><cell>aAVE</cell><cell>HOW</cell><cell>IN</cell><cell>IS</cell><cell>IT</cell></row><row><cell>LIST</cell><cell>MANY</cell><cell>MORE</cell><cell>OF</cell><cell>ON</cell><cell>ONE</cell><cell>OR</cell></row><row><cell>SHOW</cell><cell>THAN</cell><cell>THAT</cell><cell>THE</cell><cell cols="2">THEIR TO</cell><cell>USE</cell></row><row><cell>WAS</cell><cell>WERE</cell><cell>WHAT</cell><cell>WHY</cell><cell>WILL</cell><cell>WITH</cell><cell>WOULD</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Table IX gives the number of errors (substitutions + deletions + The training procedure in SPHINX</figDesc><table><row><cell>Dictionary</cell><cell cols="2">Forward-Backward</cell><cell>Labeled TIMIT Training</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Speech</cell></row><row><cell></cell><cell>Phone Models</cell><cell></cell></row><row><cell></cell><cell>1 Trained C-Ind. Phone Models</cell><cell cols="2">Unlabeled Task--D-in Training</cell></row><row><cell></cell><cell cols="2">Forward-</cell><cell>Speech</cell></row><row><cell></cell><cell cols="2">Backward</cell></row><row><cell>Context-</cell><cell>ParamS.</cell><cell></cell></row><row><cell>Dictionary</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Deleted</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Distri-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>bution</cell></row><row><cell></cell><cell cols="2">Cantext-dep. Phone models</cell></row><row><cell cols="2">ffien. Triphones 1076 174.2% (70.6%) Fig. 2 . Context-ind.</cell><cell cols="2">90.6% (87.9%) 93.8% (93.0%) 94.7% (93.7%) 96.2% (95.8%)</cell></row><row><cell>a uniform distribution.</cell><cell></cell><cell></cell></row><row><cell>insertions) made by SPHINX (context-independent models,</cell><cell></cell><cell></cell></row><row><cell>no grammar) with and without the use of function-word-</cell><cell></cell><cell></cell></row><row><cell>dependent phone models. With function-word-dependent</cell><cell></cell><cell></cell></row><row><cell>phone modeling, function word errors are cut by 27 per-</cell><cell></cell><cell></cell></row><row><cell>cent, which accounts for almost all of the improvement</cell><cell></cell><cell></cell></row><row><cell>from 45.3 to 53.4 percent accuracy.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI11 IMPROVtMtN</head><label>VI11</label><figDesc>I_</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE X SPEAKER</head><label>X</label><figDesc>GREW UP I N MORE T H A N ONE REGION. RESULTS SHOWN A R F SPHINX WORD ACCURACY BY SPEAKERS. "MOVED" MEANS THAT 'THFTo facilitate sharing between models, we used deleted interpolation to combine robust models with detailed ones. By combining poorly trained (contextdependent, generalized context, function-word-dependent speaker-dependent) models with well-trained (context-independent speaker-independent, uniform) models, we improved trainability through sharing.</figDesc><table><row><cell>WORD ACCURACY</cell></row><row><cell>lmk</cell></row><row><cell>awf</cell></row><row><cell>speech databases.</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by a National Science Foundation Graduate Fellowship, and by Defense Advanced Research Projects Agency Cwtract N00039-85-C-0163. The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies, either expressed or implied. of the National Science Foundation, the Defense Advanced Research Projects Agency, or the U . S . Government.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Kai-Fu Lee (S'8S-M'88) was born in Taipei. Taiwan, in 1961 He received the A B degree signal processing (summa cum laude) in  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The HARPY speech recognition aystcm</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Lowerre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Sci. Dep</title>
		<imprint>
			<date type="published" when="1976-04">Apr. 1976</date>
			<publisher>Carnegie Mellon Univ</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Speaker-independent isolated word recognition using a 129-word airline vocabulary</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Wilpon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bergh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J . Acoust. Soc. Atnrr</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="390" to="396" />
			<date type="published" when="1982-08">Aug. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Feature-based speaker independent recognition of English letters</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Stem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S S M</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="middle">P</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Specker</surname></persName>
		</author>
		<author>
			<persName><surname>Pilant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">IEEE Int. Conf. Acoust.. Speech. Signal Processing</title>
		<imprint>
			<date type="published" when="1983-10">Oct. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A real-time, isolated-word, speech recognition system for dictation transcription</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. fEEE fnt. Con$ Acoust., Speech, Signal Processing</title>
		<meeting>fEEE fnt. Con$ Acoust., Speech, Signal essing</meeting>
		<imprint>
			<date type="published" when="1985-03">Mar. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust HMM-based techniques for recognition of speech produced under stress and in noise</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lippmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Weinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Tech</title>
		<imprint>
			<date type="published" when="1986-04">Apr. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BYBLOS: The BBN continuous speech recognition system</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">0</forename><surname>Dunham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">A</forename><surname>Kimball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Krasner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Kubala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Makhoul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roucos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE fnt. Con$ Acoust., Speech, Signal Processing</title>
		<meeting>IEEE fnt. Con$ Acoust., Speech, Signal essing</meeting>
		<imprint>
			<date type="published" when="1987-04">Apr. 1987</date>
			<biblScope unit="page" from="89" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">High performance connected digit recognition using hidden Markov models</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Wilpon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">K</forename><surname>Soong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">IEEE Int. Conf. Acoust.. Speech, Signal Processing</title>
		<imprint>
			<date type="published" when="1988-04">Apr. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Evaluation of a word recognition system using syntax analysis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E A E</forename><surname>Levinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><surname>Flanagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Acoust., Speech, Signal Processing</title>
		<imprint>
			<date type="published" when="1977-04">Apr. 1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic speaker adaptation in the Harpy speech recognition system</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Lowerre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Acoust., Speech, Signal Processing</title>
		<imprint>
			<date type="published" when="1977-04">Apr. 1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Speech recognition of a natural text read as isolated words</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Bahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Acoust., Speech, Signal Processing</title>
		<imprint>
			<date type="published" when="1981-04">Apr. 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The role of worddependent coarticulatory effects in a phoneme-based speech recognition system</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E L R</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wilpon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><surname>Roucos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kimball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kubala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dunham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krasner</surname></persName>
		</author>
		<author>
			<persName><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">IEEE Int. Conf. Acoust.. Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="713" to="726" />
			<date type="published" when="1983-06">June 1983. Apr. 1986</date>
		</imprint>
	</monogr>
	<note>fEEE Trans. Acoust., Speech, Signal Processing</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Speaker stress-resistant continuous speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Acoust., Speech. Signal Processing</title>
		<imprint>
			<date type="published" when="1988-04">Apr. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A database for continuous speech recognition in a 1000-word domain</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pallett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Acoust., Speech, Signal Processing</title>
		<imprint>
			<date type="published" when="1988-04">Apr. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-vocabulary speaker-independent continuous speech recognition: The SPHINX system</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ph.D. dissertation, Comput. Sci. Dep.. Carnegie Mellon Univ</title>
		<imprint>
			<date type="published" when="1988-04">Apr. 1988</date>
		</imprint>
	</monogr>
	<note>Speech Recognition: The Development of the SPHfNX System</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recognition results with several experimental acoustic processors</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Bahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Acoust., Speech, Signal Processing</title>
		<imprint>
			<date type="published" when="1979-04">Apr. 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the application of vector quantization and hidden Markov models to speakerindependent, isolated word recognition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Levinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Sondhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1075" to="1105" />
			<date type="published" when="1983-04">Apr. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A maximum likelihood approach to continuous speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Bahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">lEEE Trans. Patrern A n d . Machine Intell.</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="179" to="190" />
			<date type="published" when="1983-03">Mar. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Context-dependent modeling for acoustic-phonetic recognition of continuous speech</title>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kimball</surname></persName>
		</author>
		<author>
			<persName><forename type="middle">M</forename><surname>Roucos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krasner</surname></persName>
		</author>
		<author>
			<persName><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Acoust., Speech, Signal Processing</title>
		<imprint>
			<date type="published" when="1985-04">Apr. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Markel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Gray</surname></persName>
		</author>
		<title level="m">Linear Prediction of Speech</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Speaker adaptation through vector quantization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shikano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Reddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Acoust., Speech, Signal Processing</title>
		<imprint>
			<date type="published" when="1986-04">Apr. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the performance of isolated word speech recognizers using vector quantization and temporal energy contours</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">K</forename><surname>Soong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AT&amp;TBellLub. Tech. 1</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1245" to="1260" />
			<date type="published" when="1984-09">Sept. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An algorithm for vector quantizer design</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Linde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="84" to="95" />
			<date type="published" when="1980-01">Jan. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An inequality and associated maximization technique in ststistical estimation of probabilistic functions of Markov pro-Boston, MA: Kluwer Academic, 1989. cesses</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Baum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inequalities</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The DRAGON system-An overview</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="24" to="29" />
			<date type="published" when="1975-02">Feb. 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Continuous speech recognition by statistical methods</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1976-04">Apr. 1976</date>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="532" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The acoustic-modeling problem in automatic speech recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Sci. Dep</title>
		<imprint>
			<date type="published" when="1987-05">May 1987</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">An acousticphonetic data base</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pallett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987-05">May 1987</date>
			<publisher>Acoust. Soc. Amer</publisher>
		</imprint>
	</monogr>
	<note>presented at the 113th Meet</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The lexical access component of the CMU continuous speech recognition system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rudnicky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baumeister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Degraaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Acoust., Speech, Signal (311 D. Pallett</title>
		<imprint>
			<date type="published" when="1987-03">Mar. 1987</date>
			<biblScope unit="page" from="75" to="78" />
		</imprint>
	</monogr>
	<note>Proc. DARPA Speech Recog. Workshop</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Subdivision of the audible frequency range into critical bands (Frequenzgruppen)</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J . Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">248</biblScope>
			<date type="published" when="1961-02">Feb. 1961</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Comparison of parametric representations of monosyllabic word recognition in continuously spoken sentences</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mermelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">E E E Trurrs. Acoust., Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="357" to="366" />
			<date type="published" when="1980-08">Aug. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Evaluation of LPC spectral matching measures for phonetic unit recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shikano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep.. Comput. Sci. Dep</title>
		<imprint>
			<date type="published" when="1985-05">May 1985</date>
			<publisher>Carnegie Mellon Univ</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Discrete representation of signals</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Oppenheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1972-06">June 1972</date>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="681" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Auditory perception and its application to computer analysis of specch</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ruske</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Computer Aiitrlysis cnid Perception. Auditory Signals</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">De</forename><surname>Mori</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="1982">1982</date>
			<publisher>CRC Press</publisher>
			<pubPlace>Boca Raton. FL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Speaker-independent isolated word recognition using dynamic features of speech spectrum</title>
		<author>
			<persName><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">fEEE Trans. Acoust., Speed?. Sigiicrl Processing</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="52" to="59" />
			<date type="published" when="1986-02">Feb. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Prosody and speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Sci. Dep</title>
		<imprint>
			<date type="published" when="1986-10">Oct. 1986</date>
			<publisher>Carnegie Mellon Univ</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Integration ofacoustic information in a large vocabulary word recognizer</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N M</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lennig</surname></persName>
		</author>
		<author>
			<persName><surname>Mermelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. f E E E frit. Con$ Acoust. , Speech. Sigriul Processing</title>
		<meeting>f E E E frit. Con$ Acoust. , Speech. Sigriul essing</meeting>
		<imprint>
			<date type="published" when="1987-04">Apr. 1987</date>
			<biblScope unit="page" from="697" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Explicit modeling of state occupancy in hidden Markov models for automatic speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Russel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE frit. Con$ Acortst. , Speech. Sipial Proc.rssing</title>
		<meeting>IEEE frit. Con$ Acortst. , Speech. Sipial .rssing</meeting>
		<imprint>
			<date type="published" when="1985-04">Apr. 1985</date>
			<biblScope unit="page" from="5" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Continuously variable duration hidden Markov models for automatic speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Levinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Speedi Larlguage</title>
		<imprint>
			<biblScope unit="page" from="29" to="45" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Acoustic Markov models used in the Tangord speech recognition system</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Bahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F P V De</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">IEEE Int. Conf. Acoust.. Speech, Signal Processing</title>
		<imprint>
			<date type="published" when="1988-04">Apr. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">0</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<title level="m">Putterti Cla.ssifcation and Scene Annlysis</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An information theoretic approach to the automatic determination of Phonemic baseforms</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Lucassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE h t . Cor$ Acousr., Speech, Signal Processitig</title>
		<meeting>IEEE h t . Cor$ Acousr., Speech, Signal essitig</meeting>
		<imprint>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Interpolated estimation of Markov source parameters from sparse data</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition in</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Practice</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Gelsema</surname></persName>
		</editor>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>North-Holland</publisher>
			<date type="published" when="1980">1980</date>
			<biblScope unit="page" from="381" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Context-dependent phonetic hidden Markov models for continuous speech recognition</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">submitted to the fEEE Trtrns. Acousr., Speech, Signal Processing. 1471</title>
		<imprint>
			<date type="published" when="1987-07">1987. July 1987</date>
		</imprint>
	</monogr>
	<note>Towards speaker-independent continuous speech recognition. presented at the</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Large-vocabulary speaker-independent continuous speech recognition</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE fnt. Con$ Acoust., Speech, Signal Processing</title>
		<meeting>IEEE fnt. Con$ Acoust., Speech, Signal essing</meeting>
		<imprint>
			<date type="published" when="1988-04">Apr. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On large-vocabulary speaker-independent fcontinuous speech recognition</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J . Euro. Assoc. Signal Processing (Speech Communications)</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="375" to="379" />
			<date type="published" when="1988-12">Dec. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
