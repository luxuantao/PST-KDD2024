<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-taught dimensionality reduction on the high-dimensional small-sized data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012-08-04">4 August 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaofeng</forename><surname>Zhu</surname></persName>
							<email>zhux@itee.uq.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technology and Electrical Engineering</orgName>
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
							<email>huang@itee.uq.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technology and Electrical Engineering</orgName>
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
							<email>yang.yang@itee.uq.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technology and Electrical Engineering</orgName>
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Heng</surname></persName>
						</author>
						<author>
							<persName><surname>Shen</surname></persName>
							<email>shenht@itee.uq.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Technology and Electrical Engineering</orgName>
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
							<email>csxu@nlpr.ia.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
							<email>jluo@cs.rochester.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Rochester</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-taught dimensionality reduction on the high-dimensional small-sized data</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2012-08-04">4 August 2012</date>
						</imprint>
					</monogr>
					<idno type="MD5">D67671AC6DC03179D0ECCB5BCBDD9A94</idno>
					<idno type="DOI">10.1016/j.patcog.2012.07.018</idno>
					<note type="submission">Received 16 February 2012 Received in revised form 28 June 2012 Accepted 21 July 2012</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Dimensionality reduction Self-taught learning Joint sparse coding Manifold learning Unsupervised learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To build an effective dimensionality reduction model usually requires sufficient data. Otherwise, traditional dimensionality reduction methods might be less effective. However, sufficient data cannot always be guaranteed in real applications. In this paper we focus on performing unsupervised dimensionality reduction on the high-dimensional and small-sized data, in which the dimensionality of target data is high and the number of target data is small. To handle the problem, we propose a novel Self-taught Dimensionality Reduction (STDR) approach, which is able to transfer external knowledge (or information) from freely available external (or auxiliary) data to the high-dimensional and small-sized target data. The proposed STDR consists of three steps: First, the bases are learnt from sufficient external data, which might come from the same ''type'' or ''modality'' of target data. The bases are the common part between external data and target data, i.e., the external knowledge (or information). Second, target data are reconstructed by the learnt bases by proposing a novel joint graph sparse coding model, which not only provides robust reconstruction ability but also preserves the local structures amongst target data in the original space. This process transfers the external knowledge (i.e., the learnt bases) to target data. Moreover, the proposed solver to the proposed model is theoretically guaranteed that the objective function of the proposed model converges to the global optimum. After this, target data are mapped into the learnt basis space, and are sparsely represented by the bases, i.e., represented by parts of the bases. Third, the sparse features (that is, the rows with zero (or small) values) of the new representations of target data are deleted for achieving the effectiveness and the efficiency. That is, this step performs feature selection on the new representations of target data. Finally, experimental results at various types of datasets show the proposed STDR outperforms the state-of-the-art algorithms in terms of k-means clustering performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Many real applications (such as text categorization, computer vision, image retrieval, microarray technology and visual recognition) involve high-dimensional data <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b20">21]</ref>. In practice, although high-dimensional data can be analyzed with highperformance contemporary computers, to deal with high-dimensional data often leads to some problems, such as the explosion in execution time, the curse of dimensionality, ignoring the impact of the noise and redundancy, and so on. However, it has been proven that the ''intrinsic'' dimensionality of high-dimensional data is typically small <ref type="bibr" target="#b41">[42]</ref>. Therefore, it is necessary to develop efficient and effective approaches to search for such ''intrinsic'' dimensionality. Dimensionality reduction techniques have been broadly used to address this via reducing the number of features. Moreover, dimensionality reduction techniques help to decrease time and space complexity as well as to make data more comprehensible <ref type="bibr" target="#b48">[49]</ref>.</p><p>Traditional dimensionality reduction methods have been demonstrated to be effective when sufficient data are provided. However, in real applications, these models often suffer from the problem of lacking sufficient data so that it is difficult to build effective models. For instance, a personal album on Flickr 1 usually contains only a small number of photos. In such a case, if we want to perform automatic tagging on these personal photos via visual content analysis, the learning models will probably not work effectively. To handle this problem, a straight-forward solution can be designed to convey external knowledge (or information) to enhance the effectiveness of the model. Transfer learning <ref type="bibr" target="#b31">[32]</ref> is capable of transferring external data (that is, external data can come from a different but related task) to strengthen learning tasks. As illustrated in Fig. <ref type="figure" target="#fig_6">1(a)</ref>, the learning process of transfer learning requires external data to be relevant to target data (e.g., target data: ''car'' and ''motorcycles'' vs. external data: ''bus'', ''tractor'' and ''aircraft''). Such an assumption makes transfer learning less flexible and effective in case not enough relevant external data can be obtained. Recently, self-taught learning <ref type="bibr" target="#b34">[35]</ref> was proposed to relax such a strong assumption to allow us leverage more possible external data in the learning task. That is, self-taught learning only requires that external data are with similar ''modality'' or ''type'' to target data, which can be obtained easily in real applications. For example, as shown in Fig. <ref type="figure" target="#fig_6">1(b)</ref>, natural scene images can be used to categorize motor vehicle images by using self-taught learning. Due to placing significantly fewer restrictions on external data, self-taught learning is much easier to be applied than transfer learning, and has been applied in many practical applications <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25]</ref>, such as image, audio, text classification, among others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contents lists available at</head><note type="other">SciVerse ScienceDirect</note><p>Above analysis motivates us to develop a new unsupervised dimensionality reduction approach for dealing with the highdimensional and small-sized data via self-taught learning. In this paper we model the dimensionality reduction model as a reconstruction process, by proposing a Self-Taught Dimension Reduction (STDR) approach. The proposed STDR consists of three steps: (1) A set of the bases is learnt from external data by employing existing dictionary learning models, such as online dictionary learning <ref type="bibr" target="#b28">[29]</ref>. The learnt bases are the common part on both external data and target data. In real application, the external data can easily be obtained. Thus sufficient external data ensure to obtain effective bases. <ref type="bibr" target="#b1">(2)</ref> Target data are represented (or reconstructed) by the learnt bases by designing a robust joint sparse coding model. The proposed model not only provides robust reconstruction ability but also preserves the local structures amongst target data. After the reconstruction process, target data are sparsely represented by the learnt bases. Moreover, the unimportant features (or the noise features) are represented with small values or zeros. <ref type="bibr" target="#b2">(3)</ref> We delete these unimportant features to perform feature selection on the new representations of target data.</p><p>The contributions of the proposed solution for performing dimensionality reduction on the high-dimensional and small-size data are presented as follows:</p><p>The STDR is devised to handle the high-dimensional and small-sized data by employing sufficient external data, which only need to be with similar ''type'' or ''modality'' of target data. To the best of our knowledge, this is the first work that focuses on employing such external data in a reconstruction framework to perform dimensionality reduction on the highdimensional and small-sized data.</p><p>The proposed objective function in the STDR is a robust linear reconstruction model. That is, the ' 2,1 -norm loss function (a.k.a., robust loss function) is designed to eliminate the impact of the outliers as well as to achieve the minimal reconstruction error; The Laplacian prior is used to preserve local structures of target data. The Laplacian prior considers the correlations among target data so that it makes the learning process more effective <ref type="bibr" target="#b3">[4]</ref>; The ' 2,1 -norm regularization is designed to avoid the over-fitting issue and to sparsely select a subset of the learnt bases to represent target data. Moreover, we design a novel solution to the resulted optimization problem via a simple algorithm which is theoretically guaranteed to converge to the global optimum.</p><p>Extensive experiments are conducted on various types of datasets to illustrate the effectiveness of the proposed STDR method. The results show that the proposed STDR outperforms several state-of-the-art and baseline dimensionality reduction methods in term of k-means clustering performance.</p><p>The remainder of the paper is organized as below: Related work is briefly reviewed in Section 2, followed by the proposed STDR approach in Section 3. The experimental results are reported and analyzed in Section 4 while Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>To deal with high-dimensional data is very challengeable to all kinds of learning tasks (or models) in the domain of data mining and machine learning. With a lot of features, the hypothesis feature space becomes huge. This leads the learning models to easily be over-fitting. Thus the performance of the learning models will be degenerated. Moreover, the built models become computationally inefficient and difficult to be interpreted. Dimension reduction methods are designed to address these problems.</p><p>Different methods on dimensionality reduction usually fall into two categories, i.e., feature selection and feature extraction respectively. Given a dataset with a large number of features, if some of them are irrelevant, feature selection performs dimensionality reduction by removing the irrelevant features, then outputting the relevant original features. Feature extraction tries to reduce the dimensionality of original data by combining the original features. The key difference between feature selection and feature extraction is that feature selection keeps relevant original features, while feature extraction generates new features by combining the original features. Different applications require different dimensionality reduction methods. For example, feature selection is usually applied for the cases in which the original features need to be kept for the interpretation. Feature extraction is more preferable for the cases in which the users expect to obtain better performance rather than to interpret the derived results.</p><p>Feature selection algorithms <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b22">23]</ref> are broadly categorized into three groups: filter model, wrapper model and embedded model. Filter model is usually designed to first analyze the general characteristics of the data, and then to evaluate the features without involving a learning algorithm. In the existing methods on filter model, feature selection is decided by the predefined criteria, such as, mutual information <ref type="bibr" target="#b14">[15]</ref>, variable ranking <ref type="bibr" target="#b7">[8]</ref>, among others. For example, Laplacian score method <ref type="bibr" target="#b17">[18]</ref> ranks the features by evaluating the power of locality preservation of each feature. In real applications, filtering model is (relatively) robust against the issue of over-fitting, but may fail to select the most ''useful'' features. Wrapper model ''wraps'' the selection process to identify relevant features while requiring a predetermined learning algorithm <ref type="bibr" target="#b16">[17]</ref>. For example, the method in <ref type="bibr" target="#b29">[30]</ref> was designed to find a subset of all the features by maximizing the performance of the SVM classifier. In real applications, wrapper model can in principle find the most ''useful'' features, so it often outperforms filter model. However, wrapper model is with high computation cost and prone to the issue of over-fitting. Embedded model (e.g., <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b43">44]</ref>) performs feature selection during the process of model building. Thus embedded model usually regards feature selection as a part of the training process, in which useful features are obtained by optimizing the objective function of the learning model. Recent embedded model receives increasing interests due to its superior performance. For example, the method <ref type="bibr" target="#b42">[43]</ref> added a ' 0 -norm constraint into the proposed objective function to achieve sparse solution for effectively and efficiently performing feature selection. Both the method in <ref type="bibr" target="#b30">[31]</ref> employing a ' 1 -norm regularization and the method in <ref type="bibr" target="#b26">[27]</ref> employing a ' 2,1 -norm regularization were designed to achieve the similar objectives. Embedded model is similar to wrapper model, but is with less computation cost and less prone to the issue of over-fitting.</p><p>Feature extraction reduces the dimensionality of original data by combining original features under the pre-set constraints. Feature extraction usually falls into two categories <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b46">47]</ref>, i.e., projective methods and manifold methods respectively. Projective methods attempt to find the low-dimensional projections containing the most information of original data, by maximizing the pre-defined objective functions. Moreover, projective methods can find the explicit transformation between the original data matrix and a low-dimensional space, i.e., finding a transformation matrix V and expressing the reduced data Y of original data X as Y ¼ V T X. For example, principal component analysis (PCA) <ref type="bibr" target="#b4">[5]</ref> was designed to search for the new feature space (that is, the subspace) via maximizing the variance of the data. Independent component analysis (ICA) <ref type="bibr" target="#b33">[34]</ref> tries to find the projections by considering the probability distributions of original data into the objective function. Manifold methods first assume that original data lie on a low-dimensional manifold, then attempt to search for them by satisfying some suitable objective functions. But manifold methods do not form explicit projections <ref type="bibr" target="#b46">[47]</ref>. For example, multidimensional scaling (MDS) <ref type="bibr" target="#b11">[12]</ref> finds the top rank projections for preserving the inter-point distance (that is, dissimilarity); ISOMAP <ref type="bibr" target="#b39">[40]</ref> preserves the isometric properties of the original space in the subspace; Both locally linear embedding (LLE) <ref type="bibr" target="#b35">[36]</ref> and Laplacian eigenmap <ref type="bibr" target="#b2">[3]</ref> focus on preserving the local neighbor structures of original data; The method in <ref type="bibr" target="#b9">[10]</ref> preserves the separability of the data by using the weighted displacement vectors; The method in <ref type="bibr" target="#b44">[45]</ref> was uniquely designed to preserve the feature of global coordinates by a compatible mapping. It has been shown (e.g., <ref type="bibr" target="#b21">[22]</ref>) that projective methods and manifold methods can be brought together while employing the kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Self-taught dimensionality reduction</head><p>In this section, we first describe the used notations in this paper, and then give an overall description of the proposed STDR. Finally, we present the detailed elaboration of each component in the STDR.</p><p>To elaborately describe the details of the STDR, we first depict how to learn the bases from external data by using the methods on dictionary learning. Second, a robust sparse coding model is proposed to reconstruct target data based on the learnt bases. Third, feature selection is conducted on the new representations of target data. Fourth, we give the pseudocode of the STDR. Finally, we give the theoretical analysis on the proposed robust joint graph sparse coding model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Notation</head><p>For clarity, we summarize the notations used in this paper in Table <ref type="table" target="#tab_0">1</ref>. Besides, the ' p -norm of a vector v A R n is defined as</p><formula xml:id="formula_0">JvJ p ¼ P n i ¼ 1 9v i 9 p 1=p</formula><p>, where v i is the ith element of v. If p¼0 then we get the ''pseudo norm'' (a.k.a., ' 0 -norm) which is defined as the number of non-zero elements in v. The ' r,p -norm over a matrix MA R nÂm is defined as JMJ r,p ¼</p><formula xml:id="formula_1">P n i ¼ 1 P m j ¼ 1 Jm ij J r p=r 1=p ,</formula><p>where m ij is the element of the ith row and jth column.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Framework</head><p>Before elaborating the technical details of the proposed STDR, we briefly introduce its framework. The proposed framework presented in Fig. <ref type="figure" target="#fig_2">2</ref> includes three steps.</p><p>In the first step, the STDR learns the common parts (that is, the bases) from a large amount of external data. External data can be with different data distribution but with same ''type'' or ''modality'' (or mildly related) to target data <ref type="bibr" target="#b24">[25]</ref>. According to the example on image analysis in <ref type="bibr" target="#b10">[11]</ref>, different types of objects (such as, diamond, ring, platinum and titanium) may share common features. For example, diamond and ring perhaps share similar features about ''diamond''; ring and platinum share same ''modality'' about ''platinum''; platinum and titanium share same ''type'' about ''metal''. In such a situation, external (or auxiliary) data can easily be obtained as well as benefit for obtaining better data representation for limited target data. Moreover, the fact on the different datasets with different distribution but sharing common features has been found in all kinds of real applications,  <ref type="bibr" target="#b37">[38]</ref>.With sufficient external data, the model for learning the bases can effectively be built. The learnt bases are regarded as a ''bridge'' between external data and target data. With this, useful knowledge (e.g., the learnt bases) in external data are transferred to target data. This is reasonable because: First, the bases are the common part between external data and target data, thus it is feasible for using the learnt bases to represent target data. This enables the learning task on the limited target data more effectively. Second, to present target data by the bases (i.e., the high-level representations) has been showed to be more effective than those representing target data with the traditional methods, such as pixel raw features <ref type="bibr" target="#b34">[35]</ref>.</p><p>Third, the learnt bases can be reused for various learning tasks, such as for limited target data, for the current external data, and the others. Last but not least, the dimensionality of the bases can be larger or smaller than the ones of target data. We will give a further discussion on this in Section 4.6.1.</p><p>In the second step, the STDR reconstructs target data into the new feature space (that is, the basis space), via the proposed Algorithm 2. The outputs of this step are the new presentations of target data. Meanwhile, the redundant features of the new representations are assigned with small values or shrunk to zero.</p><p>In the third step, the STDR performs feature selection on the derived new representations of target data. According to above analysis, we know that the first step of the STDR is designed to deal with the issue of small-sized data via employing external data. Both the second step and the third step of the STDR are designed for dealing with the issue of high-dimensional data via performing feature selection on the new representations of target data. The objective is to search for the ''intrinsic'' dimensionality of high-dimensional data. Hence, the STDR can simultaneously handle the high-dimensional and small-sized data.</p><p>Finally, after performing dimensionality reduction on the highdimensional and small-sized data, the reduced data are fed into the k-means algorithm. The clustering performance is used to evaluate the effect of the dimensionality reduction algorithm.</p><p>Comparing the proposed STDR method with feature selection and feature extraction, the STDR first generates the new representations of target data from external data, and then performs feature selection on the derived representations of target data. It is similar to embedded model of feature selection, such as <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b45">46]</ref>. Therefore, the STDR can be categorized into feature selection rather than feature extraction. Comparing the STDR with the existing methods on embedded model of feature selection, such as the algorithm UDFS <ref type="bibr" target="#b45">[46]</ref> and the algorithm RFS <ref type="bibr" target="#b30">[31]</ref>. The UDFS performs feature selection by combining discriminative ability of the data and local structures of target data. And the UDFS was designed for unsupervised learning and did not employ external data. The RFS conducts feature selection under the assumption of supervised learning and does not employ external data.</p><p>Comparing the STDR with self-taught learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35]</ref>, both of them belong to sparse learning models, but they have difference. First, the objective of self-taught learning in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35]</ref> is to utilize the freely unlabeled data to improve the performance of supervised learning task, even if the unlabeled data used cannot be associated with the labels of the task. The STDR learns the bases from external data (including unlabeled data and labeled data) to reconstruct target data. The objective of the STDR is designed to use a large number of external data to achieve the better performance of dimensionality reduction in unsupervised learning. Second, to improve the performance of the learning task, the STDR takes more constraints into account than self-taught learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35]</ref> does during the reconstruction process. For example, the robust loss function is designed for more effective avoiding the high impact of outliers in target data; The constraint on graph Laplacian regularization is used to ensure that the similar data points in the original space are still similar in the basis space; The ' 2,1 -norm regularization is designed to generate the row sparsity, i.e., the sparsity through the whole feature (or row). However, self-taught learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35]</ref> only employs the standard sparse coding model (such as in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24]</ref>, which consists of a least square loss function and a ' 1 -norm regularization), to reconstruct the labeled data. The literature in <ref type="bibr" target="#b30">[31]</ref> show that the ' 2,1 -norm regularization is better for preforming feature selection than the ' 1 -norm regularization. Third, the objective function of the STDR is solved by proposing a novel solution. Moreover, the proposed solver to the objective function is theoretically guaranteed that the objective function of the STDE converges to the global optimum. The proposed solver is efficient since it optimizes the objective matrix by regarding it as a whole. Thus it can generate the sparse codes (i.e., the new representations) of target data efficiently. However, the objective function of self-taught learning is solved by generating the new representation of a data point (or a sample) once. Thus it requires solving the Lasso problem <ref type="bibr" target="#b13">[14]</ref> d times, where d is the number of the dimensionality of target data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning bases from external data</head><p>In order to transfer knowledge from external data to target data, we need to find a proper bridge to connect them first. The literature in <ref type="bibr" target="#b34">[35]</ref> has showed that diverse types of images may contain common basic visual patterns. This motivates us to build such a ''bridge'' by extracting certain common elements (i.e., the bases) from sufficient external data first, and then to use the extracted bases to reconstruct target data.</p><p>Dictionary learning <ref type="bibr" target="#b28">[29]</ref> has been proven to be quite effective for learning the bases in various applications, such as image processing, audio recognition, and so on. In this paper, we employ the existing methods on dictionary learning to search for a ''bridge''. Given external data matrix</p><formula xml:id="formula_2">X o ¼ ½X o 1 ,X o 2 , . . . ,X o</formula><p>no , dictionary learning can usually be formulated as follows:</p><formula xml:id="formula_3">min fB,S o g JX o ÀBS o J 2 F þl X no i ¼ 1 Js o i J 1 s:t: JB j J 2 r1,j ¼ 1,2, . . . ,m,<label>ð1Þ</label></formula><p>where J Á J F is defined as Frobenius norm, B ¼ ½B 1 ,B 2 , . . . ,B m denotes m bases learnt from X o , and S o ¼ ½s o 1 ,s o 2 , . . . ,s o no is the matrix of the sparse codes. The constraint on each base JB j J 2 r1 (j ¼ 1,2, . . . ,mÞ is used to prevent B from having arbitrarily large values, which might lead to very small values of S o .</p><p>To optimize the above objective function, a series of algorithms have been designed, such as online dictionary learning <ref type="bibr" target="#b28">[29]</ref> and gradient decent with iterative projection method <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b47">48]</ref>, and so on. In this paper, we employ online dictionary learning method <ref type="bibr" target="#b28">[29]</ref> because it is efficient to handle large-scale data. In the implementation process, we utilize SPAMS <ref type="bibr" target="#b28">[29]</ref> toolkits to solve the objective function in Eq. (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Reconstructing target data</head><p>In this part we focus on devising a robust and effective model to reconstruct target data from the learnt bases. To this end, we take three facets into account, i.e., robust reconstruction, local structure preservation and irrelevant features elimination.</p><p>Given a set of n target data points X ¼ ½x 1 ,x 2 , . . . ,x n A R dÂn and the bases B A R dÂm learnt from external data, the reconstruction process of X using B can be achieved by all kinds of loss functions, including least square loss function, logistic loss function, squared hinge loss function, and so on. The literatures (e.g., <ref type="bibr" target="#b12">[13]</ref>) has shown that the ' 2,1 -norm loss function (i.e., the robust loss function) is robust for avoiding the adverse impact of outliers. In this paper, we employ robust loss function as the loss function in the proposed objective function to robustly reconstruct target data. The robust loss function is defined as in Eq. (2). min</p><formula xml:id="formula_4">S X n i ¼ 1 Jx i ÀBs i J 2 ,<label>ð2Þ</label></formula><p>where S ¼ ½s 1 ,s 2 , . . . ,s n are the sparse codes corresponding to X.Given a loss function in the optimization issue, it is feasible for us to add a regularization into the optimization issue in real applications since the regularization can be designed to avoid the issue of over-fitting or to meet some requirements, such as the sparsity. In this paper, to perform feature selection, we should choose a regularization leading to distinguishing the features, i.e., distinguishing the important features and the unimportant features for the learning task. Motivated by the characteristics of the sparsity in sparse coding, we expect the important features are represented by non-zero and the unimportant features are represented by zero after the reconstruction process. Then we give up the unimportant features (i.e., the features with zero values) and use the important features to perform the learning process. In real applications, the ' 1 -norm regularization is able to achieve the sparsity for each individual data point. It is proposed to obtain an approximate result to the ' 0 -norm regularization under practical conditions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28]</ref>. The ' 1 -norm regularization is often used in separable spare coding models, such as graph sparse coding <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b47">48]</ref> and Lasso <ref type="bibr" target="#b13">[14]</ref> , and so on. However, the ' 1 -norm regularization does not guarantee that all the data points are sparse in the same features. Fortunately, the ' 2,1 -norm regularization can satisfy our expectation and is defined as</p><formula xml:id="formula_5">JSJ 2,1 ¼ X m j ¼ 1 JðsÞ j J 2 ,<label>ð3Þ</label></formula><p>where ðsÞ j is the jth row of S, which indicates the effect of the jth feature to all the data points. The ' 2,1 -norm regularization can penalize all coefficients in one row as it regards each single feature as a whole. Therefore, it achieves the row-level sparsity for all data points at the same time. The ' 2,1 -norm is also rotational invariant for rows: Given any rotational matrix R,</p><formula xml:id="formula_6">JMRJ 2,1 ¼ JMJ 2,1</formula><p>. Therefore, we choose the ' 2,1 -norm as the regularization term to achieve feature selection min</p><formula xml:id="formula_7">S X n i ¼ 1 Jx i ÀBs i J 2 þl X m j ¼ 1 JðsÞ j J 2 ,<label>ð4Þ</label></formula><p>where l40 is a tuning parameter.</p><p>Furthermore, one might expect that local structures of target data in the original space can be well preserved in the new feature space <ref type="bibr" target="#b3">[4]</ref>, that is, the close data points in the original space should be also close in the intrinsic geometry (i.e., the basis space). In this paper, the STDR builds a k-nearest-neighbor graph for each data point in target data to achieve this objective.</p><p>More specifically, following the idea in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref>, we use a heat kernel w ij ¼ e ÀJx i Àx j J 2 =s (s is a tuning parameter, we set s ¼ 1 in this paper) to build a weight matrix W. The value of w ij is used to measure the closeness of two points x i and x j , and we set w ii ¼ 0 to avoid the problem of scale in this paper. Given a weight matrix W, we use the Euclidean distance to measure the smoothness between s i and s j (where s i (or s j ) is the projections of x i (or x j ) respectively in the basis space), that is,</p><formula xml:id="formula_8">1 2 X i,j Js i Às j J 2 w ij ¼ X i s i D ii s T i À X i,j s i s T j w ij ¼ trðSDS T ÞÀtrðSWS T Þ ¼ trðSLS T Þ:<label>ð5Þ</label></formula><p>We denote D as a diagonal matrix. The ith diagonal element of D is computed as the sum of the ith column of W, that is, D ii ¼ P j w ij . Obviously, L ¼ DÀW is a Laplacian matrix. By integrating the Laplacian prior in Eq. ( <ref type="formula" target="#formula_8">5</ref>) into Eq. ( <ref type="formula" target="#formula_7">4</ref>), we obtain the final objective function for reconstructing target data as follows:</p><formula xml:id="formula_9">min S X n i ¼ 1 Jx i ÀBs i J 2 þa trðSLS T ÞþlJSJ 2,1 ,<label>ð6Þ</label></formula><p>where aZ0 and l40 are the tuning parameters.</p><p>By setting X ¼ ½x 1 , . . . ,x n and S ¼ ½s 1 , . . . ,s n , Eq. ( <ref type="formula" target="#formula_9">6</ref>) can be changed into min</p><formula xml:id="formula_10">S JXÀBSJ 2 þ a trðSLS T ÞþlJSJ 2,1 ,<label>ð7Þ</label></formula><p>where aZ0 and l40 are the tuning parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Feature selection</head><p>After solving Eq. ( <ref type="formula" target="#formula_9">6</ref>) (or Eq. ( <ref type="formula" target="#formula_10">7</ref>)), whose details can be found in Section 3.7 and Algorithm 2, we obtain the new representations S (S A R mÂn ) of original target data X (X A R dÂn ). Due to the ' 2,1norm regularization, most of the rows in S shrink to zero, which implies that the corresponding features of these zero rows are not important to the new representations. To achieve efficiency in future learning tasks, we may remove them, i.e., we are able to perform dimensionality reduction on original target data. More specifically, we first rank the rows in the S in descending order according to the ' 2 -norm values of each individual row JðsÞ j J 2 ,j ¼ 1,2, . . . ,m, and then select top ranked rows as the results of dimensionality reduction.</p><p>Actually, the bases learnt from external data can be applied to various kinds of data, so it may contain redundancy and noise for certain data. Fortunately, the regularization items (i.e., both the Laplacian prior item and the ' 2,1 -norm regularization item) in Eq. ( <ref type="formula" target="#formula_9">6</ref>) can detect them by enforcing the rows with small values. Hence, our proposed model can help to achieve an effective and efficient learning by preserving the locality and deleting the redundancy and noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Pseudocode of the STDR approach</head><p>In summary, we summarize the proposed STDR method as follows:</p><p>Algorithm 1. Pseudocode of the proposal STDR approach. Now we give a toy example to describe the process of the proposed STDR. We generate the toy dataset (including 200 data points which are represented by 4 dimensions) by following Gaussian distribution. This toy dataset includes two clusters (or classes) where one cluster contains 101 data points and another contains 99 data points. We plot the first two dimensions of the toy data in the left sub-figure of Fig. <ref type="figure" target="#fig_4">3</ref>, and the last two dimensions of the toy data in middle sub-figure of Fig. <ref type="figure" target="#fig_4">3</ref>. We perform k-means clustering algorithm on the toy data and obtain clustering performance (i.e., clustering accuracy (ACC for short) defined in Section 4.1.4) as 80.5%. We also perform k-means clustering algorithm on six sub-datasets (each sub-dataset consists of two-dimension original toy data), and obtain the best ACC as 73.5% and the worse one as 50.50% among six results.</p><p>In our STDR, we first generate external data (including 3000 data points which are represented by 4 dimensions) by following exponential distribution. Second, we obtain the bases (with the matrix B A R 4Â4 ) by conducting the first step of Algorithm 1 on the external data. Third, with the same parameters' setting as in Section 4, we obtain the new representations S (S A R 4Â200 ) by conducting Algorithm 2. Fourth, we implement the third step of Algorithm 1 to obtain the reduced dataset S 0 A R 2Â200 . Fifth, we perform k-means clustering algorithm on S 0 and obtain the value of ACC 94.5%. Moreover, we plot all data points of S 0 in the right sub-figure of Fig. <ref type="figure" target="#fig_4">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Optimization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.1.">The proposed solver</head><p>As can be seen, the objective function in Eq. ( <ref type="formula" target="#formula_9">6</ref>) (or Eq. ( <ref type="formula" target="#formula_10">7</ref>)) is convex, so it has the global optimum. However, the optimization in Eq. ( <ref type="formula" target="#formula_9">6</ref>) (or Eq. ( <ref type="formula" target="#formula_10">7</ref>)) is very challengeable since the ' 2,1 -norm regularization is non-smooth. To efficiently minimize the value of the objective function in Eq. ( <ref type="formula" target="#formula_9">6</ref>), we describe our solution as follows.</p><p>By setting the derivative of the objective function in Eq. ( <ref type="formula" target="#formula_9">6</ref>) (or Eq. ( <ref type="formula" target="#formula_10">7</ref>)) with respect to S to zero, we obtain</p><formula xml:id="formula_11">ðB T D l B þlD r ÞS þ SðaLÞ ¼ B T D l X,<label>ð8Þ</label></formula><p>where the derivative of the first term in Eq. ( <ref type="formula" target="#formula_9">6</ref>) is equivalent to B T D l BSÀB T D l X and the derivative of the third term in Eq. ( <ref type="formula" target="#formula_9">6</ref>) is equivalent to lD r S. D r and D l are diagonal matrices with their ith</p><formula xml:id="formula_12">diagonal element calculated as d r i,i ¼ 1=2JðSÞ i J 2 (or d l i,i ¼ 1=2JðXÀBSÞ i J 2 )</formula><p>, and here ðMÞ j is used to denote the ith row of a matrix M.</p><p>Since both D r and D l depend on the value of S, it is impractical to compute S directly. In this part we design a novel iterative algorithm to optimize Eq. ( <ref type="formula" target="#formula_9">6</ref>) by alternatively computing S, D r and D l . We first summarize the details in Algorithm 2 and then prove that in each iteration the updated S, D r and D l make the value of the objective function in Eq. ( <ref type="formula" target="#formula_9">6</ref>) decrease.</p><p>As seen in Algorithm 2, in each iteration, given fixed D r and D l , S can be first calculated by Eq. ( <ref type="formula" target="#formula_11">8</ref>) via the solution of the Sylvester equation. <ref type="foot" target="#foot_1">2</ref> Then D r and D l are updated by d r i,i ¼ 1=2JðSÞ i J 2 and d l i,i ¼ 1=2JðXÀBSÞ i J 2 respectively. The iteration process is repeated until there is no change to the value of the objective function. Algorithm 2. An iterative algorithm for solving Eq. ( <ref type="formula" target="#formula_9">6</ref>). </p><formula xml:id="formula_13">Input: X A R dÂn ,B A R dÂm ,L A R nÂn ,</formula><formula xml:id="formula_14">Update D t þ 1 r by computing ith diagonal element d r i,i ¼ 1 2JðS t þ 1 Þ i J 2 ; Update D t þ 1 l by computing ith diagonal element d l i,i ¼ 1 2JðXÀBS t þ 1 Þ i J 2 ; t ¼ t þ 1;</formula><p>9 until there is no change to the value of the objective function in Eq. (6);</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.2.">Convergence</head><p>In this part we introduce Theorem 1 that guarantee the value of the objective function in Eq. ( <ref type="formula" target="#formula_9">6</ref>) is monotonically decreased in each iteration of Algorithm 2.</p><p>First we describe the follow lemma similar to in <ref type="bibr" target="#b30">[31]</ref>:</p><p>Lemma 1. For any positive values a i and b i , i ¼ 1, . . . ,m, the following always holds:</p><formula xml:id="formula_15">2 ffiffiffiffiffi ffi ab p rða þ bÞ ) 2 ffiffiffiffiffi ffi ab p Àa r 2bÀb ) ffiffiffi a p À a 2 ffiffiffi b p r ffiffiffi b p À b 2 ffiffiffi b p :<label>ð9Þ</label></formula><p>Theorem 1. In each iteration, Algorithm 2 monotonically decreases the value of the objective function in Eq. (6).</p><p>Proof. In order to prove Theorem 1, we first introduce an auxiliary optimization problem with respect to S as follows:</p><formula xml:id="formula_16">min S t þ 1 trðXÀBS t þ 1 Þ T ðXÀBS t þ 1 Þ D l þ l trðS T t þ 1 S t þ 1 Þ D r þ a trðS t þ 1 LS T t þ 1 Þ,<label>ð10Þ</label></formula><p>where D r and D l are calculated according to S t in the tth iteration. By denoting S t þ 1 as the variable of the objective function in Eq. ( <ref type="formula" target="#formula_16">10</ref>), and S, D r , D l as the optimal results obtained in the tth iteration, we have:</p><formula xml:id="formula_17">S n t þ 1 ¼ argmin St þ 1 trðXÀBS t þ 1 Þ T ðXÀBS t þ 1 Þ D l þ l trðS T t þ 1 S t þ 1 Þ D r þ a trðS t þ 1 LS T t þ 1 Þ,<label>ð11Þ</label></formula><formula xml:id="formula_18">which indicates that trðXÀBS n t þ 1 Þ T ðXÀBS n t þ 1 Þ D l þ l trðS n t þ 1 TS n t þ 1 Þ D r þa trðS n t þ 1 LS n t þ 1 TÞ r trðXÀBSÞ T ðXÀBSÞ D l þ l trðS T SÞ D r þ a trðSLS T Þ:<label>ð12Þ</label></formula><p>By changing the trace form into the form of summation, we have</p><formula xml:id="formula_19">X m i ¼ 1 JðXÀBS n t þ 1 Þ i J 2 2 2JðXÀBSÞ i J 2 þ l X m i ¼ 1 JðS n t þ 1 Þ i J 2 2 2JðSÞ i J 2 þ a trðS n t þ 1 LS n t þ 1 TÞ r X m i ¼ 1 JðXÀBSÞ i J 2 2 2JðXÀBSÞ i J 2 þ l X m i ¼ 1 JðSÞ i J 2 2 2JðSÞ i J 2 þ a trðSLS T Þ,<label>ð13Þ</label></formula><p>where ðMÞ i denotes the ith row of a matrix M. After performing a simple mathematical transformation, we have</p><formula xml:id="formula_20">X m i ¼ 1 JðXÀBS n t þ 1 Þ i J 2 À X m i ¼ 1 JðXÀBS n t þ 1 Þ i J 2 À X m i ¼ 1 JðXÀBS n t þ 1 Þ i J 2 2 2JðXÀBSÞ i 2 ! þl X m i ¼ 1 JðS n t þ 1 Þ i J 2 Àl X m i ¼ 1 JðS n t þ 1 Þ i J 2 À X m i ¼ 1 JðS n t þ 1 Þ i J 2 2 2JðSÞ i J 2 ! þa trðS n t þ 1 LS n t þ 1 TÞ r X m i ¼ 1 JðXÀBSÞ i J 2 À X m i ¼ 1 JðXÀBSÞ i J 2 À X m i ¼ 1 JðXÀBSÞ i J 2 2 2JðXÀBSÞ i J 2 ! þl X m i ¼ 1 JðSÞ i J 2 Àl X m i ¼ 1 JðSÞ i J 2 À X m i ¼ 1 JðSÞ i J 2 2 2JðSÞ i J 2 ! þ a trðSLS T Þ:<label>ð14Þ</label></formula><p>By substituting a and b in Eq. ( <ref type="formula" target="#formula_15">9</ref>) with</p><formula xml:id="formula_21">JðXÀBS n t þ 1 Þ i J 2 2 (or JðS n t þ 1 Þ i J 2 2</formula><p>) and JðXÀBSÞ i J 2 2 (or JðSÞ i J 2 2 ) in Eq. ( <ref type="formula" target="#formula_20">14</ref>) respectively, and then summing the results over i ¼ 1,2, . . . ,m, and them summing the results over i ¼ 1, . . . ,m, we have</p><formula xml:id="formula_22">X m i ¼ 1 JðXÀBS n t þ 1 Þ i J 2 þl X m i ¼ 1 JðS n t þ 1 Þ i J 2 þ a trðS n t þ 1 LS n t þ 1 TÞ r X m i ¼ 1 JðXÀBSÞ i J 2 þ l X m i ¼ 1 JðSÞ i J 2 þa trðSLS T Þ:<label>ð15Þ</label></formula><p>This indicates that the value of the objective function in Eq. ( <ref type="formula" target="#formula_9">6</ref>) monotonically decreases in each iteration of Algorithm 2. Therefore, due to the convexity of Eq. ( <ref type="formula" target="#formula_9">6</ref>), Algorithm 2 is able to enable the objective function in Eq. ( <ref type="formula" target="#formula_9">6</ref>) converge to the global optimum. &amp;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head><p>In order to evaluate the effectiveness of the proposed STDR method, we apply it into two real applications, i.e., image analysis and document analysis. For the domain of image analysis, we use three image datasets, including USPS, <ref type="foot" target="#foot_2">3</ref> Letter, <ref type="foot" target="#foot_3">4</ref> and MNIST. <ref type="foot" target="#foot_4">5</ref> For the domain of document analysis, we use three textual datasets, including Reuters21578,<ref type="foot" target="#foot_5">6</ref> 20Newsgroups<ref type="foot" target="#foot_6">7</ref> and TDT2.   <ref type="table" target="#tab_2">2</ref>. We then describe how to build self-taught learning tasks according to the existing literatures <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>Both USPS and MNIST are handwritten digit databases. USPS contains 9298 images in 10 categories. MNIST contains 10,000 samples in 10 categories. Dataset Letter has 20,000 unique stimuli for representing 26 capital letters on English alphabet. In the first two experiments of Section 4.2, we take USPS and MNIST as target data respectively while dataset Letter is external data. In the third experiment of Section 4.2, we take dataset Letter as target data while regarding USPS as external data.</p><p>The Reuters21578 corpus is a set of documents appeared on the Reuters newswire in 1987. In our experiments, we use 7285 documents in total to learn the bases of target data, that is, TDT2. The original TDT2 corpus contains news stories collected from 6 sources, including 2 newswires (APW, NYT), 2 radio programs (VOA, PRI) and 2 television programs (CNN, ABC). In our fourth experiment of Section 4.2, we delete the newswires sources (that is, APW and NYT), which may be relevant to Reuters21578, from dataset TDT2. The left data contain 10,733 stories in 94 categories are regarded as target data.</p><p>20Newsgroups dataset was organized into 20 different newsgroups, in which each groups was corresponded to a different topic. Some of the newsgroups are very closely related to each other, while others are highly unrelated (e.g. misc.forsale vs. the others). In our fifth experiment of Section 4.2, we use one group (that is, 964 documents for the group misc.forsale) to learn the bases for target data, that is, the left 19 groups with 17,810 documents in total.</p><p>In this paper, both target data and external data are represented in a standard way, such as raw pixel intensities for images and the bag-of-words (with vocabulary size 500) representation for text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Comparison algorithms</head><p>We compare the proposed STDR approach with the following algorithms:</p><p>Original: all original features are used to conduct k-means clustering. We want to know whether or not the dimensionality reduction algorithms can improve the clustering performance on the high-dimensional and small-sized data.</p><p>LScore: Laplacian Score <ref type="bibr" target="#b17">[18]</ref> belongs to filter model of feature selection. The Laplacian score of a feature is evaluated by its locality preserving power. The feature will have the high score if data points in the same topic are close to each other. MCFS: Multi-Cluster Feature Selection <ref type="bibr" target="#b6">[7]</ref> selects features using spectral regression with the ' 1 -norm regularization.</p><p>That is, the MCFS sequentially performs the LPP <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref> and least square regression. UDFS: Unsupervised Discriminative Feature Selection <ref type="bibr" target="#b45">[46]</ref> conducts feature selection by combining discriminative analysis with the local structures of target data. LPP: Locality Preserving Projections (LPP) <ref type="bibr" target="#b17">[18]</ref>, as one of the methods on feature extraction, does not take least square regression into account. JFS: Joint Feature Selection (JFS) considers minimal reconstructor error but not considers the local structures of target data. We change the supervised feature selection algorithm RFS in <ref type="bibr" target="#b30">[31]</ref> to generate the JFS in our experiments. The proposed STDR combines the LPP with the JFS together, so both the JFS and the LPP are the baselines of the proposed STDR. JGFS: Joint Graph Feature Selection can be regarded as an extension of the proposed STDR approach. Different from the STDR, the JGFS builds the dimensionality reduction model without using external data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Experimental setting</head><p>In our experiments, we set the parameters for the comparison algorithms by following the instructions in their papers. For example, the parameter l for controlling the weight between the discriminative ability and the similarity matrix in the UDFS is set as 1000 according to the setting in <ref type="bibr" target="#b45">[46]</ref>. The parameter of reduced dimensionality on the MCFS is set as the number of the classes in the original target data, same as the setting in <ref type="bibr" target="#b6">[7]</ref>. In both the JGFS and the STDR, the parameter a is set as f0:01,0:1,1,10g, the parameter l is set as f0:001,0:1,10,1000g. In the JFS, the parameter l is set as f0:001,0:1,10,1000g. For the algorithms (such as the LScore, the LPP, the MCFS, the UDFS, the JGFS and the STDR), which need to build a k-nearest-neighbor graph, we set the distance metric as Euclidean distance, the value of k as 5, the weight mode as a heat kernel whose width is set as 1.</p><p>Since the algorithms (such as the ''Original'', the LScore, the MCFS, the UDFS, the LPP and the JGFS) cannot use external data, we first perform dimensionality reduction with these algorithms on target data, and then perform k-means algorithm on the reduced data. In both the JFS and the STDR, we first use all the data in external data to learn the bases, and then reconstruct target data to the learnt basis space. Then we perform feature selection on the new representations of target data to generate the reduced data. Finally, we perform k-means clustering algorithm on the reduced data.</p><p>In all algorithms, first, to generate small-sized target data, we randomly sample (but with evenly ratio for each class) original target data to generate our target data. The sample size are set as 500, 1000 and 2000 respectively. Of each sample size, we generate 10 target datasets. For example, in the first two experiments of Section 4.2, for original target data USPS, we generate 30 our target datasets, where the size of ten datsets is 500, 1000 and 2000 respectively. Second, the left dimensionality after performing dimensionality reduction on our target datasets is kept as {200, 400, 600} for dataset USPS, Letter and MNIST, and {100, 300, 400} for document datasets. That is, each our target dataset is generated three reduced datasets via each dimensionality reduction method. Third, we perform k-means clustering algorithm on each reduced dataset, and then use the clustering performance to evaluate the effectiveness of all dimensionality reduction algorithms. In k-means clustering algorithm, the number of clusters is set as the number of the classes of original target dataset. We perform k-means algorithm 20 runs with random initializations on each reduced dataset. The average result of the outcome in 20 runs as the result on each reduced dataset. The best result among three different reduced datasets is regarded as the final results of one target datasets. We report the average result of 10 target datasets as the final result of one sample size.</p><p>In Section 4.2, we compare the clustering performance on the reduced data derived by all dimensionality reduction algorithms, for evaluating the effectiveness of the proposed STDR. In Section 4.3, we test the parameters sensitivity of the proposed method according to the variation of the parameters, such as a and l in Eq. ( <ref type="formula" target="#formula_9">6</ref>) (or Eq. ( <ref type="formula" target="#formula_10">7</ref>)), aiming at achieving the best performance of the proposed STDR. We use different external data to learn the bases for same target data, aiming at analyzing the effects of different external data to the proposed STDR in Section 4.4. We evaluate the convergence rate of the proposed Algorithm 2 on all five datasets, for evaluating the efficiency of our optimization algorithm, in terms of the objective function value in each iteration in Section 4.5. In Section 4.6, we give a brief discussion on setting the number of the bases as well as the complexity for solving Sylvester equation in the proposed STDR. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4.">Evaluation metrics</head><p>We evaluate the clustering results by comparing the derived cluster labels (via the dimensionality reduction algorithms) with the truth labels (provided by original datasets). Two standard clustering metrics (i.e., accuracy (referred to as ACC) and normalized mutual information (NMI)) are used to measure the clustering performance. Given a data point x i , denote y i and y n i as the cluster labels and the truth labels respectively. The evaluation term ACC is defined as follows:</p><formula xml:id="formula_23">ACC ¼ P n i ¼ 1 dðy i ,mapðy n i ÞÞ n ,<label>ð16Þ</label></formula><p>where n is the size of samples and dðx,yÞ is the delta function, dðx,yÞ ¼ 1 if x ¼y, and dðx,yÞ ¼ 0, otherwise. Actually, the clustering algorithm can tell us which data are in the same cluster, but cannot indicate the exactly cluster labels as the truth ones. Hence, mapðy n i Þ is the optimal mapping function that permutes cluster labels to match the truth labels. In our experiments, we employ the Kuhn-Munkres algorithm <ref type="bibr" target="#b32">[33]</ref> as the optimal mapping function. According to Eq. ( <ref type="formula" target="#formula_23">16</ref>), the maximal value of ACC is 1 in an arbitrary case. And the minimal value is 1=k, when there are k clusters and every cluster contains equivalent number of the data.</p><p>Following the paper <ref type="bibr" target="#b38">[39]</ref>, the evaluation of NMI is defined as follows:</p><formula xml:id="formula_24">NMI ¼ P k i ¼ 1 P k j ¼ 1 n i,j log nn i,j ñi n j ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi P k i ¼ 1 ñi log ñi n P k j ¼ 1 n j log n j n s ,<label>ð17Þ</label></formula><p>where n is the number of all data. ñi (or n j ) is the number of data in the ith cluster in the cluster labels (or the number of data in the jth cluster in the truth labels). n i,j is the number of data in the intersection between the ith cluster in the cluster labels and the jth cluster in the truth labels. Obviously, the value of NMI equals to 1 if the cluster labels are identical with the truth labels, and 0 if they are independent. Note that, ACC and NMI are two independent indicators to measure clustering performance. It is possible that ACC is bigger while NMI is smaller, or ACC is smaller while NMI is bigger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results of ACC and NMI on all algorithms</head><p>The clustering results (that is, ACC and NMI) on all algorithms are presented from Tables <ref type="table" target="#tab_3">3</ref><ref type="table">4</ref><ref type="table" target="#tab_4">5</ref><ref type="table" target="#tab_5">6</ref><ref type="table">7</ref><ref type="table">8</ref>. 9 The experimental results show that the STDR achieves the best performance and the JGFS is the second best.</p><p>As can be seen from the experimental result, we can make the following conclusions as:</p><p>1. The results of the ''Original'' are better than those of the JFS, and worse than the others. However, the JFS is more efficient due to significantly reducing the dimensionality of target data. 2. The methods on dimensionality reduction (except the JFS) take the local structures of target data into account, and outperform the ''Original''. Therefore, to preserve the local structures in the dimensionality reduction model is crucial. This is consistent with the conclusion in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b45">46]</ref>. 3. According to the experimental results, it is necessary to conduct dimensionality reduction, even with the high-dimensional and small-sized data.</p><p>We understand that the ' 2,1 -norm can be used to remove redundancy and noise. However, our results showed that the JFS performs worse than the ''Original'' on the clustering results. One possible reason is that some useful features could be also removed, depending on the degree of dimensionality reduction.</p><p>The main advantage of using the ' 2,1 -norm in our experiments lies in the improvement of efficiency. Therefore, we have to also combine it with the Laplacian prior (i.e., the LPP), to further improve the effectiveness. By doing so, both efficiency and effectiveness can be achieved.</p><p>Both the STDR and the JGFS build the dimensionality reduction model by simultaneously taking the Laplacian prior and the minimal reconstruction error into account. However, the algorithms (such as the JFS, the LScore and the LPP) only consider one of constraints. The more constraints enable to build more effective models of dimensionality reduction, so the clustering performance of the algorithms (such as the JFS, the LScore and the LPP) are worse than those of the STDR (or the JGFS).</p><p>Both the MCFS and the UDFS consider two constraints, but they do not outperform either the STDR or the JGFS. This can be explained as follows. First, the MCFS separately performs the process of the Laplacian prior and the regression process. Thus it does not consider the correlations among target data. Second, the UDFS outperforms the other comparison algorithms (such as the LScore, the JFS, the LPP and the MCFS) since it simultaneously takes the discriminative ability and the Laplacian prior into account. This is same as the conclusion in <ref type="bibr" target="#b45">[46]</ref> on the case with sufficient target data. However,according to the illustrated experimental results, the UDFS is a data-driven method, i.e., very sensitive to the choice of the datasets on handling the highdimensional and small-sized data. For example, the UDFS does not obtain good clustering performance on the first experiment, i.e., ''Letter vs. MNIST'', from Tables <ref type="table" target="#tab_3">3</ref><ref type="table">4</ref><ref type="table" target="#tab_4">5</ref><ref type="table" target="#tab_5">6</ref><ref type="table">7</ref><ref type="table">8</ref>.</p><p>Both the STDR and the JGFS build the dimensionality reduction model by simultaneously taking the Laplacian prior and the minimal reconstruction error into account. The key difference between them is that the STDR employs external data for learning the bases, but the JGFS learns the bases with the given target data. As can be seen from Table <ref type="table" target="#tab_6">9</ref>, the difference on the STDR over the JFGS is the maximum, i.e., 2.3% for ACC and 1.68% for NMI while the sample size is 500, and the difference is the minimum, i.e., 1.32% for ACC and 0.18% for NMI while the sample size is 2000. According to the results in Table <ref type="table" target="#tab_6">9</ref>, we can make the conclusions as follows:</p><p>1. With more information (i.e., sufficient external data) in the process for learning the bases, the STDR can build a more effective model to learn the bases than the JGFS, which learns the bases by using limited target data. However, with the increase of the size of target data, the JGFS can also build efficient models to learn the bases with sufficient target data, the function of external data will decrease. For example, when the size of target data is 2000, the results of the STDR are a little better than those of then JGFS. 2. In the process of building learning models, to introduce external data into the learning model can increase the useful information, or add the probability of introducing noise, or even degenerate the performance of the learning model. As can be seen from the experimental results, when the size of target data is small, such as 500, to introduce external data for 9 Since target data are sampled from a large number of dataset as well as are with small sample size, the data points in the dataset with 500 samples can be totally different from those in the dataset with 1000 samples. Therefore, it is normally and reasonable for the case, in which the clustering performance in the datasets with 500 samples are better than those in the same datasets with 1000 samples, such as the clustering performance in the fourth row (i.e., the results on ''USPS vs. Letter'') of Table <ref type="table" target="#tab_3">3</ref> are higher than those in the fourth row (i.e., the results on ''USPS vs. Letter'') of Table <ref type="table" target="#tab_4">5</ref>.</p><p>learning limited target data more effectively improves the performance of the dimensionality reduction model than those of the cases with large sample size, such as 1000 or 2000. However, even in the case with enough target data, the STDR still outperforms the comparison algorithms. According to the experimental results, we can make the conclusion: When the number of target data increases, the function of external data to target data decreases but not degenerates.Hence, external data are always effective to target data in the proposed STDR approach, even if with sufficient target data. Actually, in real applications, we do not employ external data while we have with sufficient target data since sufficient target data can ensure us to effectively build learning models. In such a case, to employ external data maybe increase the change for introducing the noise and the redundancy in external data into the learning model. 3. According to the above analysis, it is very feasible for us to perform dimensionality reduction by using the proposed STDR to handle the high-dimensional and small-sized data.</p><p>Finally, we also find the clustering performance of the STDR with less external data (e.g., the fifth experiment, that is, using part of data in dataset 20Newsgroup to learn the bases for the left ones) are similar to those of the JGFS with sufficient external data. This conclusion is consistent to that in <ref type="bibr" target="#b34">[35]</ref>, in which only 10 outdoors images are used to effectively perform self-taught classification on Caltech 101 dataset. Hence, less external data can also improve the performance of dimensionality reduction of the STDR, but sufficient external data are preferable. This is feasible in real applications because there are a larger number of available external data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Effect on different external data</head><p>In this section, we evaluate the effects of different external data to target data in the dimensionality reduction model. In our experiments, we fix target data, and utilize different external data to learn the bases of target data. More specifically, we set external data as MNIST and USPS respectively for target data Letter, and present the results in Table <ref type="table" target="#tab_0">10</ref>. We also set external data as the revised dataset TDT2 (in which there are only 2 newswires, that is, APW and NYT) and Reuters respectively for target data TDT2 (without those two sources). The experimental results are presented in Table <ref type="table" target="#tab_0">11</ref>.</p><p>In Tables <ref type="table" target="#tab_0">10</ref> and<ref type="table" target="#tab_0">11</ref>, the results of the JGFS are presented in the second row, and those of the STDR are listed in the last two rows. The results of ACC on different sample size are listed from second column to fourth column. The results of NMI on different sample size are listed in the last three columns.</p><p>As can be seen, the results of the STDR are still better than those of the JGFS in terms of different external data. Moreover, the maximal difference of the results between the STDR and the JGFS is in the case while sample size is 500, for two different external data in our experiments. Hence, different external data in the STDR can obtained similar performance of dimensionality reduction. Actually, we always expect to know which kind of external data can more improve the performance in self-taught learning, or want to know which kinds of data are with common visual patterns. The interesting issues, similar to the analysis in both transfer learning <ref type="bibr" target="#b31">[32]</ref> and self-taught learning <ref type="bibr" target="#b34">[35]</ref>, will be the issue of our future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Parameters' sensitivity</head><p>In this section, we study the clustering performance of the STDR with respect to the variations of different parameters' setting, that is, l and a in Eq. ( <ref type="formula" target="#formula_9">6</ref>). Due to the similar results in terms of different sample size, we only report one of them. That is, we list the results of ACC and NMI on the case with 500 target data, the left dimensionality after performing dimensionality reduction was kept as 600 for image datasets and 400 for document datasets respectively. The experimental results are presented in Figs. <ref type="figure">4</ref> and<ref type="figure">5</ref>.</p><p>As can be seen, the STDR can achieve better clustering performance with larger sparsity (that is, the larger value of l)</p><p>and smaller weight on the Laplacian prior regularization (that is, between 0 and 1). Actually, the larger sparsity leads to less run costs. However, according to the experimental results, the better values of the l are various in different datasets. The conclusion on small weight on a is consistent with those in previous work on graph sparse coding in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b47">48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Convergence rate</head><p>We solve Eq. ( <ref type="formula" target="#formula_10">7</ref>) by the proposed Algorithm 2. In this experiment, we want to know the convergence rate of Algorithm 2. Here we report some of the results in Figs. <ref type="figure" target="#fig_8">6</ref> and<ref type="figure" target="#fig_11">7</ref> due to lack of space. Fig. <ref type="figure" target="#fig_8">6</ref> shows the results on the objective function value while fixing the value of a (i.e., a ¼ 1) and varying l. Fig. <ref type="figure" target="#fig_11">7</ref> shows the results on the objective function value while fixing the value of l (i.e., l ¼ 0:1) and varying l. In both Figs. <ref type="figure" target="#fig_8">6</ref> and<ref type="figure" target="#fig_11">7</ref>, the x-axis and yaxis denotes the number of iterations and the objective function value respectively.</p><p>We can observe from both Figs. <ref type="figure" target="#fig_8">6</ref> and<ref type="figure" target="#fig_11">7:</ref> (1) the objective function value rapidly decreases at the first few iterations; and (2) the objective function value becomes stable after about 20 iterations (or even less than 20 in many cases) on all datasets. This confirms the fast convergence rate of Algorithm 2 to solve the proposed optimization problem in Eq. <ref type="bibr" target="#b6">(7)</ref>. Similar results are observed for other a and l values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1.">The dimensionality of the basis</head><p>Actually, in the proposed STDR approach, we also need to tune the value of m, that is, the number of the bases. The proposed STDR belongs to one of models on sparse coding, so the value of m can be larger or smaller than the dimensionality d of original data, according to the real applications <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b28">29]</ref>. The case m Z d means that we first map original data into a high-dimensional space, in which we reconstruct target data as well as perform feature selection. This is similar to some methods (such as, kernel methods in machine learning) for dealing with high-dimensional  data. That is, sometimes the learning model can obtain better performance in the higher-dimensional feature space than the original feature space. The case m r d indicates that the STDR first performs dimensionality reduction, and then reconstructs target data to form their new representations, in which we perform dimension reduction again by deleting the redundancy in the new representations. For simplification, we always set m ¼d in our experiments, same as the literatures <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b47">48]</ref>.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2.">The solution of Sylvester equation</head><p>The solution of Sylvester equation is really time expensive, such as dnnnðmaxðd,nÞÞ, where d is the dimensionality of the bases and n is the size of target data in this paper. However, it is reasonable for us to solve Sylvester equation via MATLAB function lyap since the proposed STDR is designed to handle the highdimensional small-size data. First, the number of the instances in the high-dimensional small-size data is usually small, such as less than 2000 in this paper. Second, the dimensionality of the basis can be large or small according to the pre-setting. As can be seen in Section 4.6.1, while the dimensionality of original data (e.g., 10,000) is high, we can learn to obtain the basis with small dimensionality (e.g., 500, the maximal dimensionality is 784 in this paper). According to our implementation, we find that the efficiency is acceptable for solving Sylvester equation via MATLAB function lyap on all our cases. For example, in our experiments, the maximal size of the matrix in Sylvester equation is with 2000*2000. To solve such Sylvester equation (i.e., the fifth step of Algorithm 2) will cost about 10 s in a normal modern PC. Due to fast convergence rate of the STDR shown in Section 4.5, i.e., usually less than 30 iterations, to finish Algorithm 2 only takes less than 5 min in our experiments. Moreover, as a pre-preprocessing phase, feature selection is often off-line. And its efficiency is not the most important thing. Finally, package LAPACK was designed to solve the large-scale Sylvester equation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel Self-Taught Dimensionality Reduction (STDR) approach for dealing with the high-dimensional and small-sized data. The proposed STDR approach first learns the bases from external data. Then the STDR reconstructs limited target data via the learnt bases, by proposing a novel objective function, which uses a ' 2,1 -norm loss function for avoiding the outliers to make the adverse impact, a ' 2,1 -norm regularization term for detecting the redundancy of the new representations of target data, and a Laplacian prior regularization term for preserving the local structures of target data. After this, the new representations of target data are performed feature selection by deleting their redundancy. The experimental results showed that the proposed STDR can effective utilize external data for coming over the drawback of limited target data.</p><p>In the future, we will extend the proposed method into its kernel edition for performing dimensionality reduction on the high-dimensional and small-sized data. We will also focus on the topics, such as which kind of external data can more improve the performance in self-taught learning, or which kinds of data are with common visual patterns, and so on.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>journal homepage: www.elsevier.com/locate/pr Pattern Recognition 0031-3203/$ -see front matter &amp; 2012 Elsevier Ltd. All rights reserved. http://dx.doi.org/10.1016/j.patcog.2012.07.018</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Examples of learning models of transfer learning and self-taught learning, adapted by [11,25,35]. (a) Transfer learning. (b) Self-taught learning.</figDesc><graphic coords="2,128.96,75.47,331.99,135.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The framework of the proposed STDR approach.</figDesc><graphic coords="4,74.72,61.19,476.39,138.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Input: X A R dÂn : target data, X o A R dÂn o : external data, and m: the number of selected features; Output: Reduced representation S 0 ; 1 Learn bases B from external data X o ; /* Section 3.3 */; 2 Generate new representations S of X by performing Algorithm 2; /* Section 3.4 */; 3 Generate reduced feature representation S 0 by performing feature selection on S; /* Section 3.5 */;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. An toy dataset example on the proposed STDR. For better viewing, see original color pdf file. Note that, the blue dot represents the points in the first cluster, the red star represents the points in the second cluster. (a) Original toy data. (b) Reduced data derived by the STDR. (For interpretation of the references to color in this figure caption, the reader is referred to the web version of this paper.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>8</head><label>8</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>4. 1 .</head><label>1</label><figDesc>Experiments setup 4.1.1. Dataset Details of the used datasets in the our experiments are presented in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. The results of ACC on the different datasets at different parameters' setting. (a) Letter_USPS. (b) Letter_MNIST. (c) USPS_Letter. (d) Reuters_TDT2. (e) 20News_20News.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. An illustration on convergence rate of Algorithm 2 for solving the proposed objective function with fixed a, i.e., a ¼ 1. (a) Letter_USPS. (b) Letter_MNIST. (c) USPS_Letter. (d) Reuters_TDT2. (e) 20News_20News.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. An illustration on convergence rate of Algorithm 2 for solving the proposed objective function with fixed l, i.e., l ¼ 0:1. (a) Letter_USPS. (b) Letter_MNIST. (c) USPS_Letter. (d) Reuters_TDT2. (e) 20News_20News.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Notations used in this paper.</figDesc><table><row><cell>Notation</cell><cell>Description</cell></row><row><cell>Feature space</cell><cell>Uppercase italic</cell></row><row><cell>Scalar</cell><cell>Lowercase italic</cell></row><row><cell>Column vector</cell><cell>Lowercase bold</cell></row><row><cell>Matrix</cell><cell>Uppercase bold</cell></row><row><cell>Transpose</cell><cell>Superscript T</cell></row><row><cell>Inverse of matrix</cell><cell>Superscript À 1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>a,l; Output: S A R mÂn ; 1 Initialize t ¼0; 2 Initialize D 0 r as a m Â m identity matrix; 3 Initialize D 0 l as a d Â d identity matrix; Update S t þ 1 by solving the Sylvester function in Eq: ð8Þ;</figDesc><table><row><cell>4 repeat</cell></row><row><cell>5</cell></row><row><cell>6</cell></row><row><cell>7</cell></row><row><cell>8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Details of self-taught dimensionality reduction applications evaluated in the experiments.</figDesc><table><row><cell>Domain</cell><cell>External data</cell><cell>Target data</cell><cell>#(class)</cell><cell>Raw features of target data</cell></row><row><cell>Image analysis</cell><cell>Letter</cell><cell>USPS</cell><cell>10</cell><cell>Intensities in 28 Â 28 pixel digit image</cell></row><row><cell>Image analysis</cell><cell>Letter</cell><cell>MNIST</cell><cell>10</cell><cell>Intensities in 28 Â 28 pixel digit image</cell></row><row><cell>Image analysis</cell><cell>USPS</cell><cell>Letter</cell><cell>26</cell><cell>Intensities in 28 Â 28 pixel character image</cell></row><row><cell>Documents analysis</cell><cell>Reuters newswire</cell><cell>TDT2</cell><cell>94</cell><cell>Bag-of-words with 500 words</cell></row><row><cell>Documents analysis</cell><cell>964 newsgroup documents</cell><cell>17,810 newsgroup documents</cell><cell>19</cell><cell>Bag-of-words with 500 words</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Clustering results (ACC% (std%)) of different algorithms with sample size 500.</figDesc><table><row><cell>External vs. objective</cell><cell>Original</cell><cell>LScore</cell><cell>MCFS</cell><cell>UDFS</cell><cell>LPP</cell><cell>JFS</cell><cell>JGFS</cell><cell>STDR</cell></row><row><cell>Letter vs. MNIST</cell><cell>41.9(3.7)</cell><cell>44.6(2.9)</cell><cell>43.7(3.2)</cell><cell>37.6(2.0)</cell><cell>46.3(2.4)</cell><cell>41.7(1.3)</cell><cell>46.7(2.7)</cell><cell>52.5(2.1)</cell></row><row><cell>Letter vs. USPS</cell><cell>47.8(2.2)</cell><cell>44.6(1.5)</cell><cell>48.2(1.5)</cell><cell>49.4(1.8)</cell><cell>52.3(1.6)</cell><cell>49.2(2.2)</cell><cell>52.4(2.2)</cell><cell>55.6(1.9)</cell></row><row><cell>USPS vs. Letter</cell><cell>32.0(1.4)</cell><cell>31.2(1.5)</cell><cell>30.9(1.7)</cell><cell>32.1(1.4)</cell><cell>31.3(1.7)</cell><cell>30.9(1.5)</cell><cell>32.3(1.6)</cell><cell>32.2(1.8)</cell></row><row><cell>Reuters vs. TDT2</cell><cell>14.8(0.7)</cell><cell>15.0(0.7)</cell><cell>15.1(1.0)</cell><cell>17.1(1.3)</cell><cell>17.0(0.6)</cell><cell>13.6(0.6)</cell><cell>17.1(0.3)</cell><cell>17.3(0.1)</cell></row><row><cell>20News vs. 20News</cell><cell>17.4(0.6)</cell><cell>18.0(0.5)</cell><cell>17.2(0.5)</cell><cell>17.7(0.7)</cell><cell>18.4(0.8)</cell><cell>17.1(0.6)</cell><cell>18.6(0.8)</cell><cell>21.0(1.0)</cell></row><row><cell>Table 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Clustering results (NMI% (std%)) of different algorithms with sample size 500.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>External vs. objective</cell><cell>Original</cell><cell>LScore</cell><cell>MCFS</cell><cell>UDFS</cell><cell>LPP</cell><cell>JFS</cell><cell>JGFS</cell><cell>STDR</cell></row><row><cell>Letter vs. MNIST</cell><cell>45.2(2.3)</cell><cell>46.8(2.4)</cell><cell>45.2(2.2)</cell><cell>33.9(2.5)</cell><cell>52.1(1.8)</cell><cell>40.4(2.0)</cell><cell>52.3(1.9)</cell><cell>54.4(1.9)</cell></row><row><cell>Letter vs. USPS</cell><cell>44.7(2.1)</cell><cell>39.4(1.8)</cell><cell>45.1(2.0)</cell><cell>46.3(2.1)</cell><cell>48.5(2.1)</cell><cell>44.8(1.8)</cell><cell>48.8(2.0)</cell><cell>50.9(2.1)</cell></row><row><cell>USPS vs. Letter</cell><cell>44.8(1.3)</cell><cell>44.1(1.2)</cell><cell>43.6(1.2)</cell><cell>45.0(1.3)</cell><cell>43.6(1.2)</cell><cell>43.2(1.3)</cell><cell>45.4(1.2)</cell><cell>45.5(1.3)</cell></row><row><cell>Reuters vs. TDT2</cell><cell>34.2(1.0)</cell><cell>34.2(0.9)</cell><cell>34.2(1.1)</cell><cell>35.1(0.5)</cell><cell>36.9(1.7)</cell><cell>33.3(1.2)</cell><cell>36.9(0.6)</cell><cell>37.9(0.9)</cell></row><row><cell>20News vs. 20News</cell><cell>4.9(0.2)</cell><cell>7.0(0.8)</cell><cell>4.7(0.2)</cell><cell>7.2(0.6)</cell><cell>6.0(0.5)</cell><cell>4.1(0.4)</cell><cell>7.5(0.6)</cell><cell>10.6(1.0)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>Clustering results (ACC% (std%)) of different algorithms with sample size 1000.</figDesc><table><row><cell>External vs. objective</cell><cell>Original</cell><cell>LScore</cell><cell>MCFS</cell><cell>UDFS</cell><cell>LPP</cell><cell>JFS</cell><cell>JGFS</cell><cell>STDR</cell></row><row><cell>Letter vs. MNIST</cell><cell>42.4(2.6)</cell><cell>45.5(1.6)</cell><cell>43.6(3.0)</cell><cell>37.9(2.3)</cell><cell>41.3(4.1)</cell><cell>42.5(1.2)</cell><cell>49.6(2.1)</cell><cell>55.6(1.5)</cell></row><row><cell>Letter vs. USPS</cell><cell>49.8(1.7)</cell><cell>45.1(1.1)</cell><cell>49.9(1.5)</cell><cell>50.8(2.2)</cell><cell>53.0(1.5)</cell><cell>49.6(1.9)</cell><cell>57.8(1.7)</cell><cell>57.9(1.8)</cell></row><row><cell>USPS vs. Letter</cell><cell>30.1(0.8)</cell><cell>28.9(0.7)</cell><cell>28.3(0.9)</cell><cell>30.3(1.0)</cell><cell>29.0(0.7)</cell><cell>28.6(0.6)</cell><cell>30.5(1.0)</cell><cell>30.7(1.1)</cell></row><row><cell>Reuters vs. TDT2</cell><cell>15.9(0.4)</cell><cell>15.4(0.8)</cell><cell>15.7(0.7)</cell><cell>17.0(1.1)</cell><cell>16.8(0.6)</cell><cell>14.2(0.7)</cell><cell>17.6(0.9)</cell><cell>17.4(1.0)</cell></row><row><cell>20News vs. 20News</cell><cell>18.8(0.9)</cell><cell>19.7(1.2)</cell><cell>18.5(0.9)</cell><cell>19.1(0.9)</cell><cell>20.7(2.1)</cell><cell>18.2(0.9)</cell><cell>20.5(1.1)</cell><cell>21.4(1.3)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>Clustering results (NMIC% (std%)) of different algorithms with sample size 1000.</figDesc><table><row><cell>External vs. objective</cell><cell>Original</cell><cell>LScore</cell><cell>MCFS</cell><cell>UDFS</cell><cell>LPP</cell><cell>JFS</cell><cell>JGFS</cell><cell>STDR</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9</head><label>9</label><figDesc>Average difference of the clustering performance on the STDR over the JFGS in five experiments with different sample size.</figDesc><table><row><cell>Sample size</cell><cell>ACC (%)</cell><cell>NMI (%)</cell></row><row><cell>500</cell><cell>2.3</cell><cell>1.68</cell></row><row><cell>1000</cell><cell>1.4</cell><cell>0.72</cell></row><row><cell>2000</cell><cell>1.32</cell><cell>0.18</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>X. Zhu et al. / Pattern Recognition 46 (2013) 215-229</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Given the known Dr and D l , Eq. (8) becomes the standard form of the Sylvester equation<ref type="bibr" target="#b1">[2]</ref>, which can be solved by Matlab function lyap or software LAPACK<ref type="bibr" target="#b0">[1]</ref>. More specifically, given known matrix A, B and C, the Sylvester equation was designed to predict the unknown X via the equation AX þ XB ¼ C. For example, in Eq. (8), given known matrix ðB T D l B þ lDrÞ, aL and B T D l X, Matlab function lyap was used to predict the unknown S. X. Zhu et al. / Pattern Recognition 46 (2013) 215-229</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/multiclass.html/usps.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>http://archive.ics.uci.edu/ml/datasets/Letter þ Recognition.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>http://www.uk.research.att.com/facedatabase.html.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>http://www.daviddlewis.com/resources/testcollections/reuters21578/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>http://people.csail.mit.edu/jrennie/20Newsgroups/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>http://www.itl.nist.gov/iad/mig/tests/tdt/1998/.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J D</forename><surname>Croz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Greenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hammarling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ostrouchov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mckenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Sorensen</surname></persName>
		</author>
		<title level="m">Lapack Users&apos; Guide</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>third ed.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Solution of the matrix equation ax þ xb ¼c [f4]</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Bartels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="820" to="826" />
			<date type="published" when="1972-09">September. 1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Manifold regularization: a geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised feature selection for principal components analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Boutsidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Drineas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Special Interest Group on Knowledge Discovery and Data Mining</title>
		<imprint>
			<biblScope unit="page" from="61" to="69" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised feature selection for multi-cluster data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Special Interest Group on Knowledge Discovery and Data Mining</title>
		<imprint>
			<biblScope unit="page" from="333" to="342" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning linear discriminant projections for dimensionality reduction of image descriptors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="338" to="352" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Benefitting from the variables that variable selection discards</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>De Sa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1245" to="1264" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sparse multinominal logistic regression via Bayesian l1 regularisation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Cawley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L C</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Girolami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Margin-based discriminant dimensionality reduction for visual recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cevikalp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Polikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Self-taught clustering</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="200" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multidimensional scaling using majorization: SMACOF in R</title>
		<author>
			<persName><forename type="first">J</forename><surname>De Leeuw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">R1-pca: rotational invariant l1-norm principal component analysis for robust subspace factorization</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H Q</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="281" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Johnstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Least angle regression</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="407" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Estimating optimal feature subsets using mutual information feature selector and rough sets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Foitong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rojanavasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Attachoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pinngern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="973" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Local features are not lonely-Laplacian sparse coding for image classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-T</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3555" to="3561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gene selection for cancer classification using support vector machines</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Barnhill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="389" to="422" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Laplacian score for feature selection</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Locality preserving projections</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="197" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stable local dimensionality reduction approaches</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2054" to="2066" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Locality condensation: a new dimensionality reduction method for image retrieval</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Üger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>ACM Multimedia</publisher>
			<biblScope unit="page" from="219" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Trace optimization and eigenproblems in dimension reduction methods</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kokiopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Saad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numerical Linear Algebra with Applications</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="565" to="602" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature selection for machine learning classification problems: a recent overview</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kotsiantis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient sparse coding algorithms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="801" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exponential family sparse coding with applications to self-taught learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Teichman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1113" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">General cost models for evaluating dimensionality reduction in high-dimensional spaces</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1447" to="1460" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-task feature learning via efficient l2,1-norm minimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards structural sparsity: an explicit l2/l0 approach</title>
		<author>
			<persName><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="344" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Online learning for matrix factorization and sparse coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="19" to="60" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A wrapper method for feature selection using support vector machines</title>
		<author>
			<persName><forename type="first">S</forename><surname>Maldonado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Science Journal</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="page" from="2208" to="2217" />
			<date type="published" when="2009-06">June. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient and robust feature selection via joint l2,1-norms minimization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Combinatorial Optimization: Algorithms and Complexity</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Steiglitz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Dover Publications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A nonnegative pca algorithm for independent component analysis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="66" to="76" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-taught learning: transfer learning from unlabeled data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="759" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An adaptive and dynamic dimensionality reduction method for high-dimensional indexing</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB Journal</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="219" to="234" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bregman divergence-based regularization for transfer subspace learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="929" to="942" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cluster ensembles-a knowledge reuse framework for combining multiple partitions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="583" to="617" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Angford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Jaap van den Herik</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Postma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TiCC TR 2009C005</title>
		<meeting><address><addrLine>Tilburg University</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning a kernel matrix for nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Use of the zero norm with linear models and kernel methods</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch Ölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tipping</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1439" to="1461" />
			<date type="published" when="2003-03">March. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Feature selection for unsupervised and supervised inference: the emergence of sparsity in a weight-based approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1855" to="1887" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction with local spline embedding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1285" to="1298" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">L21-norm regularized discriminative feature selection for unsupervised learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hug</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conferences on Artificial Intelligence</title>
		<meeting>the International Joint Conferences on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1589" to="1594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Harmonizing hierarchical manifolds for multimedia document semantics understanding and cross-media retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="437" to="446" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Graph regularized sparse coding for image representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1327" to="1336" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Manifold elastic net: a unified framework for sparse dimension reduction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="340" to="371" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by mixed kernel canonical correlation analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3003" to="3016" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m">Xiaofeng Zhu is a Ph.D. candidate in of Information Technology &amp; Electrical Engineering, The University of Queensland. His research topics include feature selection and analysis, pattern recognition and data mining</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m">Computer Science from School of ITEE, The University of Queensland. Dr. Huang&apos;s research interests include multimedia search and knowledge discovery</title>
		<meeting><address><addrLine>China</addrLine></address></meeting>
		<imprint/>
		<respStmt>
			<orgName>The University of Queensland. She received her B.Sc. degree from Department of Computer Science, Tsinghua University</orgName>
		</respStmt>
	</monogr>
	<note>Zi Huang is a Lecturer and an Australian Postdoctoral Fellow in School of Information Technology &amp; Electrical Engineering</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m">Yang Yang is a Ph.D. candidate in of Information Technology &amp; Electrical Engineering, The University of Queensland. His research topics include content understanding and pattern recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">He obtained his B.Sc. (with 1st class Honours) and Ph</title>
	</analytic>
	<monogr>
		<title level="m">His research interests include Multimedia/Mobile/Web Search, Database Management, Dimensionality Reduction, etc. Heng Tao has extensively published and served on program committees in most prestigious international publication venues in Multimedia and Database societies. He is also the winner of Chris Wallace Award for outstanding Research Contribution in 2010 from CORE Australasia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000">2000. 2004</date>
		</imprint>
		<respStmt>
			<orgName>Heng Tao Shen is a Professor of Computer Science in School of Information Technology &amp; Electrical Engineering, The University of Queensland ; D. from Department of Computer Science, National University of Singapore</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">He is a Senior Member of IEEE and Member of ACM. Jiebo Luo is a Professor in Department of Computer Science in the University of Rochester. His research spans image processing, computer vision, machine learning, data mining, medical imaging, and ubiquitous computing. He has been an advocate for contextual inference in semantic understanding of visual data, and continues to push the frontiers in this area by incorporating geo-location context and social context. He has published extensively with over 180 papers and 60 US patents. He has been involved in numerous technical conferences, including serving as the program co-chair of ACM Multimedia 2010 and IEEE CVPR 2012</title>
	</analytic>
	<monogr>
		<title level="m">Chief of the Journal of Multimedia, and has served on the editorial boards of the IEEE Transactions on Pattern Analysis and Machine Intelligence, IEEE Transactions on Multimedia, IEEE Transactions on Circuits and Systems for Video Technology</title>
		<meeting><address><addrLine>China; Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996. 1998 to 2008</date>
		</imprint>
		<respStmt>
			<orgName>Changsheng Xu is a Professor in National Lab of pattern Recognition, Institute of Automation, Chinese Academy of Sciences. He is also Executive Director of China-Singapore Institute of Digital media. He received the Ph.D. degree from Tsinghua University</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include Multimedia Content Analysis, Image Processing. He is a Fellow of the SPIE, IEEE, and IAPR</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
