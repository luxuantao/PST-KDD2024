<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GNN-LM: LANGUAGE MODELING BASED ON GLOBAL CONTEXTS VIA GNN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-05-04">4 May 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
							<email>yuxian_meng@shannonai.com</email>
						</author>
						<author>
							<persName><forename type="first">Shi</forename><surname>Zong</surname></persName>
							<email>szong@nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
							<email>xiaoya_li@shannonai.com</email>
						</author>
						<author>
							<persName><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
							<email>xiaofei_sun@shannonai.com</email>
						</author>
						<author>
							<persName><forename type="first">Tianwei</forename><surname>Zhang</surname></persName>
							<email>tianwei.zhang@ntu.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
							<email>wufei@zju.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
							<email>jiwei_li@shannonai.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shannon</forename><surname>Ai</surname></persName>
						</author>
						<title level="a" type="main">GNN-LM: LANGUAGE MODELING BASED ON GLOBAL CONTEXTS VIA GNN</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-05-04">4 May 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2110.08743v5[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Inspired by the notion that "to copy is easier than to memorize", in this work, we introduce GNN-LM, which extends vanilla neural language model (LM) by allowing to reference similar contexts in the entire training corpus. We build a directed heterogeneous graph between an input context and its semantically related neighbors selected from the training corpus, where nodes are tokens in the input context and retrieved neighbor contexts, and edges represent connections between nodes. Graph neural networks (GNNs) are constructed upon the graph to aggregate information from similar contexts to decode the token. This learning paradigm provides direct access to the reference contexts and helps improve a model's generalization ability. We conduct comprehensive experiments to validate the effectiveness of the GNN-LM: GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a 3.9 point improvement over its counterpart of the vanilla LM model), and shows substantial improvement on One Billion Word and Enwiki8 datasets against strong baselines. In-depth ablation studies are performed to understand the mechanics of GNN-LM. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Language modeling (LM) is a basic and long-standing task in natural language processing <ref type="bibr" target="#b49">(Shannon, 2001;</ref><ref type="bibr" target="#b2">Bahl et al., 1983;</ref><ref type="bibr" target="#b6">Chen &amp; Goodman, 1999;</ref><ref type="bibr" target="#b40">Mikolov et al., 2012;</ref><ref type="bibr" target="#b61">Xie et al., 2017)</ref>. It aims at predicting the upcoming token given the sequence of previous context consisting of a sequence of tokens. A common practice to train a language model is to enforce the model to maximize the probability of the upcoming ground-truth token at training time. At test time, the next token to predict could be the one with the highest probability (via greedy search) or the one that maximizes a window of tokens through the beam search strategy. This form of training-test procedure can be viewed as a process of memorization, or doing a close-book examination, if we compare the training data to a book and inference to doing an examination: The process of iterating N epochs over the training data is comparable to reviewing the book N times and the model needs to memorize what is the most likely to appear given specific context based on the training data. At test time, the book needs to be closed, i.e., the model does not have means to refer to the training data at test time, and the model has to invoke related memory to predict the next token during inference.</p><p>There are two limitations to this close-book examination strategy: (1) the memorization-based language models are usually hard to memorize the knowledge of hard examples (e.g., long-tail cases in the training set); (2) memory required to memorize the whole training data is usually intensive. The difficulty of resolving these two problems can be substantially alleviated if the model can be provided with related contexts from the training set so that the model can reference them for decisions. This process can be viewed as a strategy different from memorization or close-book examinationcopy, or in other words, open-book examination. For example, given a prefix "J. K. Rowling is best known for writing" and we want to predict the upcoming token, a language model will more easily generate token "Harry" if it can refer to the context "J. K. Rowling wrote the Harry Potter fantasy series". Motivated by the observation that "to copy is easier than to memorize", or "an open-book exam is easier than to a close-book exam", in this work, we introduce a new language modeling scheme  </p><formula xml:id="formula_0">c t = (w 1 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , w t‚àí1</formula><p>) (here the context is "The movie is"), a base LM model encodes it into a high-dimensional representation h t , which is then used to query the training datastore to retrieve the nearest contexts along with the visited tokens (marked in red). Right: The tokens in the input context and the retrieved tokens comprise a graph and are viewed as two types of nodes: nodes from the original text and nodes from the neighbor text. Intra-context edges link tokens within the same input, and inter-context edges link tokens from the retrieved contexts to the original context. After modeling the graph as a whole with GNNs, we use the updated representation of w t‚àí1 (token "is" in this example) to compute the likelihood of the next token.</p><p>-GNN-LM, which provides an LM model with the ability to reference similar contexts from the entire training corpus as cues for prediction. The similar contexts, defined as the k neighbors of the input in the training corpus, are served as additional references for the model to predict the next token. To integrate retrieved neighbors with the input, we build a directed heterogeneous graph on top of the input and the extracted contexts, where nodes are the tokens and edges represent the connections between them. We define two types of nodes -the original node from the input context and the neighbor node from the extracted contexts, and two types of edges -the inter-context edge and the intra-context edge that respectively associate inter (i.e., between retrieved contexts and input contexti.e., context within the input) and intra (i.e., context within the inputi.e., context in the retrieved sentences) contexts. A graph neural network (GNN) is employed to aggregate information from both inter-context and intra-context, which is used to generate the target token. We observe that the proposed scheme retrieves the related contexts as references, making it significantly easier for the model to predict upcoming words in the LM task.</p><p>We further combine GNN-LM with kNN-LM <ref type="bibr" target="#b23">(Khandelwal et al., 2019)</ref>, an orthogonal technique enhancing language models, to improve the overall performance of our model. We carry out experiments on three widely used language modeling benchmarks: WikiText-103, One Billion Word and Enwik8. Experimental results show that our proposed framework outperforms the strong baseline on all three benchmarks. Specifically, applying the GNN-LM framework to a strong base LM leads to substantial performance boost (-1.9 perplexity) on WikiText-103, and combining with kNN-LM achieves a new state-of-the-art perplexity of 14.8 -a 3.9 point improvement over the base LM. We perform comprehensive analyses including complexity analysis and the effects of different components to better understand the mechanics of GNN-LM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">GNN-LM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">OVERALL PIPELINE</head><p>We present the overall pipeline of our model in Figure <ref type="figure" target="#fig_0">1</ref>. At each time step t, a neural language model (LM) f (‚Ä¢) first encodes a sequence of context tokens c t = (w 1 , w 2 , ..., w t‚àí1 ) to a high-dimensional representation h t = f (c t ) ‚àà R d , where d is the dimension of hidden states. Then a transformation matrix W ‚àà R V √ód is used to estimate the probability of the t-th token p(w t |c t ) = softmax(W h t ), where V is the size of the vocabulary. We augment the vanilla neural language model by allowing it to reference samples in the training set that are similar to the current decoded sequence. Concretely, we leverage a novel self-attention augmented Graph Neural Network (GNN) on top of the vanilla LM to enable message passing between the context c and retrieved reference tokens from the training set, updating the representation h t generated by the vanilla LM. The updated representation, which aggregates additional information from reference tokens, is then used to estimate p LM (w t |c t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">GRAPH CONSTRUCTION</head><p>The first step of our proposed framework is to build a graph capturing the connections between the context tokens c t = (w 1 , w 2 , ..., w t‚àí1 ) and those similar to c t in the training set. To this end, we construct a directed heterogeneous graph, where the nodes are tokens from c t or the tokens from the neighbor contexts retrieved from the training set, and the edges represent different relationships between the nodes to be discussed below.</p><p>Formally, we define a graph as G = (V, E, A, R, œÑ, œÜ), where V is a collection of nodes v and E is a collection of edges e. We define two types of nodes A = {a o , a n }, where a o means that the node is within the input c t . a n means the node is in N (c t ), the set of extracted contexts within the neighborhood of c t . We also define two types of edges R = {r inter , r intra }, where r inter means inter-context connection (from a n nodes to a o nodes) and r intra means intra-context connection (between two nodes of same type). Each token within the input is a node of type a o , and edges of type r intra are constructed from node w i to w j (i ‚â§ j), which can be viewed as a graph interpretation of the transformer structure. Both nodes and edges are associated with their respective type mapping functions œÑ (v) : V ‚Üí A and œÜ(e) : E ‚Üí R.</p><p>For an input context c t , we retrieve k nearest neighbors</p><formula xml:id="formula_1">N (c t ) = {c (1) t1 , ..., c<label>(k)</label></formula><p>t k } of c t from the training set as follows: we first use h t to query the cached representations of all tokens for training samples, where the cached representations are obtained by a pretrained LM. The distance is measured by the cosine similarity<ref type="foot" target="#foot_1">2</ref> , and we retrieve the top K tokens denoted by {w (i) j }. The superscript (i)  denotes the i-th training sample and the subscript j denotes the j-th time step. w (i) j thus means that the j-th time step of the i-th training sample is retrieved as one of the nearest neighbors to h t . w  j+1 , which is the token right after the retrieved token w (i) j , to directly augment the output probability, we explicitly take advantage of all contextual tokens near w (i) ti as additional information in the form of graph nodes. In this way, the model is able to reference similar contexts in the training set and leverage the corresponding ground-truth target tokens via the heterogeneous graph built on both the original input tokens and the context reference tokens.</p><p>For the neighbor context window size l and r, we set l = r = 1 in all experiments. During experiments, we find that using shallow (i.e. 3) GNN layers and adding r intra edges between adjacent tokens can alleviate overfitting. Since a 3-layer GNN only aggregates information from 3-hop nodes in the graph, using larger l and r have no influence on GNN representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">GNN ON THE CONSTRUCTED GRAPH</head><p>We now use graph neural networks (GNNs) to aggregate and percolate the token information based on the graph constructed in Section 2.2. In this work, to accommodate the modeling of r intra from node w i to w j (i ‚â§ j) within c t , where Transformer with self-attention is usually adopted, we extend the self-attention mechanism to r inter , and construct a self-attention augmented GNN.</p><p>Specifically, the l-th layer representation of node n is computed by (here we use the superscript [l] to represent the l-th layer):</p><formula xml:id="formula_2">h [l] n = Aggregate ‚àÄs‚ààN (n) (Attention(s, e, n) ‚Ä¢ Feature(s, e, n)) + h [l‚àí1] n .<label>(1)</label></formula><p>Attention(s, e, n) estimates the importance of the source node s on target node n with relationship e, Feature(s, e, n) is the information feature that s should pass to n, and Aggregate(‚Ä¢) aggregates the neighborhood message with the attention weights. To draw on the information in the heterogeneous graph, we use different sets of parameters for different node types œÑ (‚Ä¢) and different edge types œÜ(‚Ä¢) akin to <ref type="bibr" target="#b16">Hu et al. (2020)</ref>.</p><p>Attention Similar to the multi-head attention mechanism of Transformer <ref type="bibr" target="#b58">(Vaswani et al., 2017)</ref>, the Attention(‚Ä¢, ‚Ä¢, ‚Ä¢) operator in our model consists of h heads, which compute attention weights independently, followed by concatenation to get the final output. For simplicity, we only describe the single-head situation below. For each edge (s, e, n), the representation of target node n is mapped to a query vector Q(n), and the representation of source node s is mapped to a key vector K(s). The scaled inner-production is then used to compute the attention weight between Q(n) and K(s), which is further normalized over all edges that have the same edge type:</p><formula xml:id="formula_3">K(s) = W k œÑ (s) h [l‚àí1] s , Q(n) = W q œÑ (n) h [l‚àí1] n , Attention(s, e, n) = 1 Z exp K(s)W ATT œÜ(e) Q(n) ‚Ä¢ ¬µ œÑ (s),œÜ(e),œÑ (n) ‚àö d , Z = s ‚ààN (n),e ‚ààœÜ(e)</formula><p>Attention(s , e , n),</p><p>(</p><formula xml:id="formula_4">)<label>2</label></formula><p>where d is the hidden dimensionality, and</p><formula xml:id="formula_5">W q œÑ (s) ‚àà R d√ód , W k œÑ (n) ‚àà R d√ód , W ATT œÜ(e) ‚àà R d√ód , ¬µ ‚àà R |A|√ó|R|√ó|A| are learnable model parameters.</formula><p>Feature Parallel to the calculation of attention weights, we propagate information from source node s to target node n. The single-head feature is defined by:</p><formula xml:id="formula_6">Feature(s, e, n) = W v œÑ (s) h [l‚àí1] s W FEA œÜ(e) ,<label>(3)</label></formula><p>where W v œÑ (s) ‚àà R d√ód and W FEA œÜ(e) ‚àà R d√ód are learnable model parameters.</p><p>Aggregate Aggregate(‚Ä¢) weight-sums the feature Message(s, e, n) within the vicinity using Attention(s, e, n), and the result is then linearly projected into a d-dimensional representation:</p><formula xml:id="formula_7">Aggregate(‚Ä¢) = W o œÑ (n) ‚äï ‚àÄs‚ààN (n) (Attention(s, e, n) ‚Ä¢ Feature(s, e, n)) (4)</formula><p>where ‚äï is element-wise addition and W o œÑ (n) ‚àà R d√ód is model parameter. The representation of token w t‚àí1 from the last layer is used to compute the language model probability p LM (w t |c t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">kNN BASED PROBABILITY FOR NEXT TOKEN</head><p>We further incorporate the proposed model with kNN <ref type="bibr" target="#b23">(Khandelwal et al., 2019;</ref><ref type="bibr">2020;</ref><ref type="bibr" target="#b37">Meng et al., 2021)</ref>, a related but orthogonal technique, to improve the performance of our model. It extends a vanilla LM by linearly interpolating it with a k-nearest neighbors (kNN) model. Concretely, for each input context c t = (w 1 , w 2 , ..., w t‚àí1 ), we retrieve the k nearest neighbors</p><formula xml:id="formula_8">N (c t ) = {c (1) t1 , ..., c (k) t k },</formula><p>and compute the kNN based probability for the next token by:</p><formula xml:id="formula_9">p(w t |c t ) = Œªp kNN (w t |c t ) + (1 ‚àí Œª)p LM (w t |c t ), p kNN (w t |c t ) = 1 Z k i=1 1 wt=w (i) t i exp cos(f (c t ), f (c (i) ti ))/T ,<label>(5)</label></formula><p>with Z being the normalization factor, f (‚Ä¢) is the neural language model encoding contexts to high dimensional representations, cos(‚Ä¢, ‚Ä¢) is cosine similarity, and Œª and T are hyperparameters.<ref type="foot" target="#foot_2">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>We conduct experiments on three widely-used language modeling datasets: WikiText-103 <ref type="bibr" target="#b38">(Merity et al., 2016)</ref>, One Billion Word <ref type="bibr" target="#b5">(Chelba et al., 2013)</ref> and Enwik8 <ref type="bibr" target="#b35">(Mahoney, 2011)</ref>. For all experiments, we add a 3-layer self-attention augmented GNN on top of the pretrained base LM, and use the same hidden dimension and number of heads as our base LM. We retrieve k = 1, 024 nearest neighbors for each source token, among them the top 128 neighbors are used in graph, and all of them are used in computing the kNN-based probability p kNN (w t |c t ). For the neighbor context window size l and r in Section 2.2, we set l = 1 and r = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">TRAINING DETAILS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KNN Retrieval</head><p>In order to reduce memory usage and time complexity, in practice we use FAISS <ref type="bibr" target="#b19">(Johnson et al., 2019)</ref> for fast approximate kNN search. Concretely, we quantized each dense vector to q bytes, followed with a clustering of all vectors to C clusters. During retrieval, we only search in 32 clusters whose centroids are nearest to query vector. For WikiText-103 and Enwik8 datasets, which contain approximately 100M tokens, we set q = 128 and c = 4, 096. For One Billion Word dataset, we set q = 64 and c = 1, 048, 576 (2 20 ) for faster search.</p><p>Data Leakage Prevention When searching for the k nearest neighbors of c t = (w 1 , w 2 , ..., w t‚àí1 ), we need to make sure each reference neighbor token does not leak information for w t . Specifically, we should not retrieve c t+1 = (w 1 , w 2 , ..., w t ) as reference, otherwise the model prediction is trivial to optimize since the information of target token is already included in the graph. Let T be the maximum sequence length and L be the number of layers. Practically, the representation of each token is dependent on previous T and T √ó L tokens for Transformer and Transformer-XL, respectively. Therefore we ignore all the neighboring nodes within this interval in graph construction during training. During inference, we do not impose this constraint.</p><p>Feature Quantization The input node representations of the graph neural network H [0] are generated by a pretrained neural language model. To accelerate training and inference, we wish to cache all token representations of the entire training set. However, frequently accessing Terabytes of data is prohibitively slow. To address this issue, we followed <ref type="bibr" target="#b37">Meng et al. (2021)</ref> to use product quantization (PQ) <ref type="bibr" target="#b17">(Jegou et al., 2010;</ref><ref type="bibr" target="#b12">Ge et al., 2013)</ref> to compress the high-dimensional representation of each token. In our experiments, quantizing representations from 1,024-dimension floating-point dense vectors to 128 bytes reduces the memory consumption from 2.3TB to 96GB for the One Billion Word dataset, thus making the end-to-end model training feasible.  <ref type="bibr" target="#b59">(Wu et al., 2018)</ref> 0.34B 26.7 Mesh-Tensorflow <ref type="bibr" target="#b51">(Shazeer et al., 2018)</ref> 4.9B 24.0 Evolved Transformer <ref type="bibr" target="#b51">(Shazeer et al., 2018)</ref> -28.6 Transformer-XL <ref type="bibr" target="#b8">(Dai et al., 2019)</ref> 0.8B 21.8 Adaptive inputs (base) <ref type="bibr">(Baevski &amp; Auli, 2018)</ref> 0.36B 25.2 Adaptive inputs (large) <ref type="bibr">(Baevski &amp; Auli, 2018)</ref> 0.46B 23.9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MAIN RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WikiText</head><p>base LM <ref type="bibr">(Baevski &amp; Auli, 2018</ref>   shows the comparison between base LM and GNN-LM in speed in WikiText-103. We observe that the speed of GNN-LM is approximately 8 to 20 times slower than the base LM <ref type="bibr">(Baevski &amp; Auli, 2018)</ref> with respect to different k.</p><p>It is worth noting that the overhead of the proposed model comes from kNN retrieval, which can be done in advance and thus does not result in time overhead when running the model. Specifically, the time overhead for retrieval comes from two processes: 1) building data indexes with token representations in the train set; 2) collecting nearest neighbors by querying the data indexes. For WikiText-103, building data indexes takes approximately 24 hours on a CPU machine with 64 cores. And querying data indexes for all tokens in train set takes approximately 30 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ABLATION STUDY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Neighbors per Token</head><p>The number of neighbors per source token (i.e., k) significantly influences how much information could be retrieved from the training set. Figure <ref type="figure" target="#fig_3">2</ref>(c) shows that test perplexity monotonically decreases when k increases from 8 to 128. This trend implies that even larger improvements can be achieved with a larger value of k.</p><p>Neighbor Quality We evaluate the quality of kNN retrieval by examining whether the target token to predict (i.e., w t ) is the same as the token that comes right after the retrieved nearest sequence using Published as a conference paper at ICLR 2022 kNN recall range <ref type="bibr">[0, 4) [4, 27) [27, 137) [137, 463) [463, 1024]</ref> base LM -7. </p><p>t k }, the quality of kNN retrieval is defined by</p><formula xml:id="formula_11">R(c t ) = k i=1 1 w t = w (i) ti ,<label>(6)</label></formula><p>where w t is the target token to predict at time step t, and w</p><formula xml:id="formula_12">(i)</formula><p>ti is the token that comes right after the i-th retrieved neighbor. We calculate and then divide all samples in the WikiText-103 test set by the recall value into 5 buckets, with each bucket containing around 50k tokens. Results are reported in Table <ref type="table" target="#tab_4">4</ref>. We observe that GNN-kNN-LM gains more relative improvements to base LM when the quality of kNN retrieval reaches a relatively high level.</p><p>Representation in kNN We finally study the effect of using different representations in the kNN scoring function in Section 2.4. We experiment with two types of representations: (1) from the last layer of Transformer, which is the default setting, and (2) from the last layer of GNN. The model performances with different choices for query and key are reported in Table <ref type="table" target="#tab_5">5</ref>. Results show that using GNN representations for both query and key leads to the best performance. It suggests that GNN learns better representations for context similarity. We also observe that the performance is marginally worse when both query and key are using Transformer representations. Considering that building an additional datastore for GNN representations is computationally intensive, in practice we can directly use Transformer representations (the default setting). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Language Modeling Traditional methods for language modeling use n-gram statistics to compute the probability of the next token given the (n ‚àí 1)-gram context <ref type="bibr" target="#b2">(Bahl et al., 1983;</ref><ref type="bibr" target="#b41">Nadas, 1984;</ref><ref type="bibr" target="#b6">Chen &amp; Goodman, 1999)</ref>. With the development of neural language models (NLMs) <ref type="bibr" target="#b40">(Mikolov et al., 2012)</ref>, deep learning based methods begin to dominate the learning paradigm of language modeling. For example, <ref type="bibr" target="#b21">Jozefowicz et al. (2016)</ref> built a strong language model by combining the LSTM <ref type="bibr" target="#b48">(Schuster &amp; Paliwal, 1997)</ref>   number of model parameters. On top of Transformer <ref type="bibr" target="#b58">(Vaswani et al., 2017)</ref>, considerable efforts have been devoted to building stronger and more efficient language models <ref type="bibr" target="#b51">(Shazeer et al., 2018;</ref><ref type="bibr" target="#b8">Dai et al., 2019;</ref><ref type="bibr" target="#b4">Beltagy et al., 2020;</ref><ref type="bibr" target="#b43">Press et al., 2020b;</ref><ref type="bibr">a)</ref>. BERT <ref type="bibr" target="#b10">(Devlin et al., 2018)</ref> proposed the Masked Language Modeling (MLM) pretraining paradigm to train a deep bidirectional Transformer model; RoBERTa <ref type="bibr" target="#b34">(Liu et al., 2019)</ref> removed the Next Sentence Prediction (NSP) task in BERT; XLNet <ref type="bibr" target="#b63">(Yang et al., 2019)</ref> generalized BERT pretraining to the autoregressive manner; Span-level BERTs <ref type="bibr" target="#b28">(Lewis et al., 2019;</ref><ref type="bibr" target="#b52">Song et al., 2019;</ref><ref type="bibr" target="#b20">Joshi et al., 2020)</ref> introduced span-level masks rather than just relying on token-level masks. ELECTRA <ref type="bibr" target="#b7">(Clark et al., 2020)</ref> proposed to detect token replacement as opposed to token generation, improving both the efficiency and effectiveness of pretraining. <ref type="bibr" target="#b55">Sun et al. (2021)</ref> extends BERT to accommodate glyph information.</p><p>Graph Neural Networks Graph neural networks (GNNs) capture the dependencies and relations between nodes connected with edges, which propagate features across nodes layer by layer <ref type="bibr" target="#b47">(Scarselli et al., 2008;</ref><ref type="bibr" target="#b25">Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b15">Hamilton et al., 2017)</ref>. GNNs have demonstrated effectiveness in a wide variety of tasks in natural language processing such as text classification <ref type="bibr" target="#b64">(Yao et al., 2019;</ref><ref type="bibr" target="#b33">Lin et al., 2021)</ref>, machine translation <ref type="bibr" target="#b3">(Bastings et al., 2017)</ref>, question answering <ref type="bibr" target="#b53">(Song et al., 2018;</ref><ref type="bibr" target="#b9">De Cao et al., 2018)</ref>, recommendation <ref type="bibr" target="#b60">(Wu et al., 2019)</ref> and information extraction <ref type="bibr" target="#b31">(Li et al., 2020a)</ref>. For example, <ref type="bibr" target="#b13">Guo et al. (2019)</ref> proposed Star Transformer, a Transformer backbone but replaces the fully-connected structure in self-attention with a star-like topology. <ref type="bibr" target="#b65">Ye et al. (2019)</ref> adopted a fine-to-coarse attention mechanism on multi-scale spans via binary partitioning (BP). <ref type="bibr" target="#b32">Li et al. (2020b)</ref> proposed to learn word connections specific to the input via reinforcement learning.</p><p>Retrieval-augmented Models Retrieving contexts from another corpus as additional information improves the model's robustness towards infrequent data points. A typical application of retrievalaugmented models is open-domain question answering, which solicits related passages from a large open-domain database to answer a given question. The dominant approach is to cache dense representations of the passages and retrieve the closest ones to the input during inference <ref type="bibr" target="#b30">(Lewis et al., 2020b;</ref><ref type="bibr" target="#b22">Karpukhin et al., 2020;</ref><ref type="bibr" target="#b62">Xiong et al., 2020;</ref><ref type="bibr" target="#b62">Lee et al., 2020;</ref><ref type="bibr" target="#b32">Li et al., 2020b)</ref>. <ref type="bibr" target="#b29">Lewis et al. (2020a)</ref> proposed to first extract a set of related texts and condition on them to generate the target text. allowing for strong zero-shot performance. Besides open-domain QA, other tasks such as language modeling <ref type="bibr" target="#b23">(Khandelwal et al., 2019;</ref><ref type="bibr" target="#b14">Guu et al., 2020)</ref>, machine translation <ref type="bibr" target="#b66">(Zhang et al., 2018;</ref><ref type="bibr" target="#b57">Tu et al., 2018;</ref><ref type="bibr" target="#b18">Jitao et al., 2020</ref>), text classification <ref type="bibr" target="#b33">(Lin et al., 2021)</ref>, and task-oriented dialog generation <ref type="bibr" target="#b11">(Fan et al., 2020;</ref><ref type="bibr" target="#b56">Thulke et al., 2021</ref>) also benefit from the additionally retrieved information. For example, <ref type="bibr" target="#b23">Khandelwal et al. (2019)</ref> retrieved k nearest neighbors from a large-scale unannotated corpus and interpolates with the decoded sentence for language modeling. <ref type="bibr" target="#b24">Khandelwal et al. (2020)</ref>; <ref type="bibr" target="#b37">Meng et al. (2021)</ref> retrieved kNNs from the parallel translation corpus to augment the machine translation outputs. However, these methods retrieve related texts independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>In this work, we propose GNN-LM, a new paradigm for language modeling that extends vanilla neural language model by allowing to reference similar contexts in the entire training corpus. High dimensional token representations are used to retrieve k nearest neighbors of the input context as reference. We build a directed heterogeneous graph for each input context, where nodes are tokens from either the input context or the retrieved neighbor contexts, and edges represent connections between tokens. Graph neural networks are then leveraged to aggregate information from the retrieved contexts to decode the next token. Experimental results show that our proposed method outperforms strong baselines in standard benchmark datasets, and by combining with kNN LM, we are able to achieve state-of-the-art results on WikiText-103. In future work, we will consider improving efficiency for building the graph and retrieving nearest neighbors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An overview of the proposed GNN-LM model pipeline. Left: Given an input contextc t = (w 1 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , w t‚àí1) (here the context is "The movie is"), a base LM model encodes it into a high-dimensional representation h t , which is then used to query the training datastore to retrieve the nearest contexts along with the visited tokens (marked in red). Right: The tokens in the input context and the retrieved tokens comprise a graph and are viewed as two types of nodes: nodes from the original text and nodes from the neighbor text. Intra-context edges link tokens within the same input, and inter-context edges link tokens from the retrieved contexts to the original context. After modeling the graph as a whole with GNNs, we use the updated representation of w t‚àí1 (token "is" in this example) to compute the likelihood of the next token.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, where l and r respectively denote the left and right window size. The corresponding representations {h (i) j+p } r p=‚àíl are used as the initialized node embeddings Different from kNN-LM (Khandelwal et al., 2019) that uses w (i)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparisons between base LM and GNN-LM on WikiText-103 with respect to different k. (a) GPU memory usage. (b) Speed (word per second). (c) Test perplexity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>This movie is great . ùëê (2) : Those movies are bad . ùëê (3) : This movie is what I like.</figDesc><table><row><cell cols="2">ùëê (1) : Neighbor Contexts</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Graph Construction (ùíç = ùíì = ùüè) ùëé ùëõ ùëé ùëõ ùëé ùëõ movie is great</cell><cell>ùëé ùëú</cell><cell>Inter-context edge Intra-context edge Node from the original context</cell></row><row><cell></cell><cell>Training Datastore</cell><cell>ùíâ ùë°</cell><cell></cell><cell>Input query</cell><cell>The</cell><cell>movie</cell><cell>is</cell><cell></cell><cell>ùëé ùëõ</cell><cell>Node from the neighbor context</cell></row><row><cell>Input Context</cell><cell></cell><cell>Base LM</cell><cell></cell><cell>ùíÑ ùíï</cell><cell>ùëé ùëú</cell><cell>ùëé ùëú</cell><cell>ùëé ùëú</cell><cell></cell><cell></cell></row><row><cell>ùíÑ ùíï</cell><cell>The</cell><cell>movie</cell><cell>is</cell><cell cols="2">movies ùëé ùëõ</cell><cell>are ùëé ùëõ</cell><cell>bad ùëé ùëõ</cell><cell>movies ùëé ùëõ</cell><cell>is ùëé ùëõ</cell><cell>what ùëé ùëõ</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>-103 WikiText-103 is the largest available word-level language modeling benchmark with long-term dependency. It contains 103M training tokens from 28K articles, and has a vocabulary of around 260K. We use the base version of deep Transformer language model with adaptive embeddings (Baevski &amp; Auli, 2018) as our base LM. This model has 16 decoder layers. Test perplexity on WikiText-103 dataset.</figDesc><table><row><cell>The dimensionality of</cell></row></table><note>One Billion Word One Billion Word is a large-scale word-level language modeling dataset of short-term dependency. It does not preserve the order of sentences, contains around 768M training tokens and has a vocabulary of around 800k. We adopt the very large version of Transformer model inBaevski &amp; Auli (2018)  as our base LM. Results in Table2show that GNN-kNN-LM helps base LM reduce 0.5 perplexity with only 27M additional parameters. For comparison,Baevski &amp; Auli  (2018)  use 560M additional parameters to reduce perplexity from 23.9 to 23.0.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Test perplexity on One Billion Word dataset.</figDesc><table><row><cell>)</cell><cell>1.03B</cell><cell>23.0</cell></row><row><cell>+kNN</cell><cell>1.02B</cell><cell>22.8</cell></row><row><cell>+GNN</cell><cell>1.05B</cell><cell>22.7</cell></row><row><cell>+GNN+kNN</cell><cell>1.05B</cell><cell>22.5</cell></row><row><cell cols="4">Enwik8 Enwik8 is a character-level language modeling benchmark that consists of 100M characters</cell></row><row><cell cols="4">from English Wikipedia articles, and has a vocabulary of 208. For base LM, we use Transformer-</cell></row><row><cell cols="4">XL (Dai et al., 2019) with 12 layers, 8 heads, 512 dimensional embedding and 2,048 dimensional</cell></row><row><cell cols="4">inner feed forward layer. Table 3 shows that GNN-kNN-LM outperforms base LM by 0.03 Bit per</cell></row><row><cell cols="4">Character (BPC), achieving 1.03 BPC with only 48M parameters, comparable to 18L Transformer-XL</cell></row><row><cell>with 88M parameters.</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell cols="2"># Param BPC (‚Üì)</cell></row><row><cell>64L Transformer (Al-Rfou et al., 2019)</cell><cell></cell><cell>235M</cell><cell>1.06</cell></row><row><cell>18L Transformer-XL (Dai et al., 2019)</cell><cell></cell><cell>88M</cell><cell>1.03</cell></row><row><cell>24L Transformer-XL (Dai et al., 2019)</cell><cell></cell><cell>277M</cell><cell>0.99</cell></row><row><cell cols="2">24L Transformer-XL + Dynamic Eval (Krause et al., 2019)</cell><cell>277M</cell><cell>0.94</cell></row><row><cell>Longformer (Beltagy et al., 2020)</cell><cell></cell><cell>102M</cell><cell>0.99</cell></row><row><cell>Adaptive Transformer (Sukhbaatar et al., 2019)</cell><cell></cell><cell>209M</cell><cell>0.98</cell></row><row><cell>Compressive Transformer (Rae et al., 2019)</cell><cell></cell><cell>277M</cell><cell>0.97</cell></row><row><cell>Sandwich Transformer (Press et al., 2020a)</cell><cell></cell><cell>209M</cell><cell>0.97</cell></row><row><cell>12L Transformer-XL (Dai et al., 2019)</cell><cell></cell><cell>41M</cell><cell>1.06</cell></row><row><cell>+kNN</cell><cell></cell><cell>41M</cell><cell>1.04</cell></row><row><cell>+GNN</cell><cell></cell><cell>48M</cell><cell>1.04</cell></row><row><cell>+GNN+kNN</cell><cell></cell><cell>48M</cell><cell>1.03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Bit per Character on the Enwik8 dataset.Space Complexity In our model, we consider k nearest neighbors for each token c i in context; the number of nodes in the graph is k times larger than vanilla LM during training. Accordingly, training GNN requires approximately k times larger memory than vanilla LM, since we have to maintain hidden representations of each node for backward propagation. We propose two strategies to alleviate the space issue: (1) For all datasets, we first train with a smaller k = 32, then further finetune the model with a larger k = 128; and (2) For datasets with extremely long dependency (e.g., WikiText-103), we truncate the context to a smaller length (e.g., 128) instead of the original longer context (e.g., 3,072) used by vanilla Transformer(Baevski &amp; Auli, 2018). Note that we build GNN model on top of the vanilla Transformer, and the parameters of Transformer are fixed when GNN parameters are being trained. Hence, the GNN could exploit long dependency information learned by Transformer without having to build a large graph with long context. Figure2(a)shows the comparison of base LM and GNN-LM on GPU memory usage with variant k inWikiText-103. 4   </figDesc><table><row><cell>4 ANALYSIS</cell></row><row><cell>4.1 COMPLEXITY ANALYSIS</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of base LM and GNN-LM in different kNN recall buckets. We report average log probabilities within each bucket, and compute the absolute and relative improvement. the recall metric. Given a sample c t = (w 1 , w 2 , ..., w t‚àí1 ) and its kNN N (c t ) = {c</figDesc><table><row><cell></cell><cell>14</cell><cell>-3.84</cell><cell>-2.21</cell><cell>-1.19</cell><cell>-0.30</cell></row><row><cell>+GNN+kNN</cell><cell>-7.15</cell><cell>-3.46</cell><cell>-1.71</cell><cell>-0.80</cell><cell>-0.21</cell></row><row><cell cols="2">absolute improvement -0.01</cell><cell>0.38</cell><cell>0.50</cell><cell>0.39</cell><cell>0.09</cell></row><row><cell>relative improvement</cell><cell>-0.0%</cell><cell>10%</cell><cell>23%</cell><cell>33%</cell><cell>32%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(1) t1 , ..., c</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Test perplexity on WikiText-103 with different representations as query and key.4.3 EXAMPLESTable6presents two examples showing the input and the corresponding extracted three neighbor contexts. The two examples demonstrate that the extracted contexts have a strong connection in semantics to the input, and thus leveraging the neighboring information will benefit model predictions.</figDesc><table><row><cell cols="3">Query Repres. Key Repres. Test ppl (‚Üì)</cell></row><row><cell>Transformer</cell><cell>Transformer</cell><cell>14.82</cell></row><row><cell>Transformer</cell><cell>GNN</cell><cell>15.16</cell></row><row><cell>GNN</cell><cell>Transformer</cell><cell>14.97</cell></row><row><cell>GNN</cell><cell>GNN</cell><cell>14.76</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>model and the CNN structure;<ref type="bibr" target="#b36">Melis et al. (2017)</ref>;<ref type="bibr" target="#b39">Merity et al. (2017)</ref> applied a variety of regularizations to LSTMs and achieved state-of-the-art results;Baevski &amp; Auli (2018)   proposed adaptive input embeddings, which can improve performance while drastically reducing the Input: In 2000 Boulter had a guest @-@ starring Extracted 1: In 2009 , Beghe had a guest @-@ starring role on the television show Californication . Extracted 2: had previously worked on Hack , for a guest @-@ starring episode arc on the show . Extracted 3: and because of Patrick Stewart 's hilarious guest @-@ starring role as " Number One . "Input: Tourism is a vital industry in Manila , and Extracted 1: a large audience in Mogadishu , and was widely sold prior to the civil war . Extracted 2: industry is well established , with Mumbai Port being one of the oldest and most Extracted 3: transportation has become a large business in Newark , accounting for more than 17</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Two examples showing the input context and the corresponding extracted three neighbors. The bold token is the gold token to predict, and the underlined are the extracted context tokens.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The code can be found at https://github.com/ShannonAI/GNN-LM</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">In practice, we use FAISS<ref type="bibr" target="#b19">(Johnson et al., 2019)</ref> for fast approximate kNN search.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">The original version of kNN-LM<ref type="bibr" target="#b23">(Khandelwal et al., 2019)</ref> uses negative L2 distance as vector similarity, and does not have hyperparameter T . We followed<ref type="bibr" target="#b24">Khandelwal et al. (2020)</ref> to add hyperparameter T and followed<ref type="bibr" target="#b37">Meng et al. (2021)</ref> to use cosine similarity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">We note base LM uses a context length of 3,072, while the context length of GNN-LM is 128. We scale up the value of GNN-LM 24 times for fair comparison.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Character-level language modeling with deeper self-attention</title>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dokook</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandy</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3159" to="3166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive input representations for neural language modeling</title>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Segatron: Segment-aware transformer for language modeling and understanding</title>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Lalit R Bahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName><surname>Mercer ; He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luchen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="1983">1983. 2021</date>
			<biblScope unit="page" from="179" to="190" />
		</imprint>
	</monogr>
	<note>A maximum likelihood approach to continuous speech recognition</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graph convolutional encoders for syntax-aware neural machine translation</title>
		<author>
			<persName><forename type="first">Jasmijn</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalil</forename><surname>Sima'an</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09">September 2017</date>
			<biblScope unit="page" from="1957" to="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Robinson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.3005</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="359" to="394" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Question answering by reasoning across documents with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09920</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Augmenting transformers with knn-based composite memory for dialogue</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Braud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12744</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Optimized product quantization</title>
		<author>
			<persName><forename type="first">Tiezheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifa</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="744" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09113</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Startransformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08909</idno>
		<title level="m">Realm: Retrievalaugmented language model pre-training</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Heterogeneous graph transformer</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="2704" to="2710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Product quantization for nearest neighbor search</title>
		<author>
			<persName><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Boosting neural machine translation with similar translations</title>
		<author>
			<persName><forename type="first">Josep</forename><forename type="middle">M</forename><surname>Xu Jitao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Crego</surname></persName>
		</author>
		<author>
			<persName><surname>Senellart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1580" to="1590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with gpus</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv√©</forename><surname>J√©gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Daniel S Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04906</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00172</idno>
		<title level="m">Generalization through memorization: Nearest neighbor language models</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00710</idno>
		<title level="m">Nearest neighbor machine translation</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Dynamic evaluation of transformer language models</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Kahembwe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08378</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning dense representations of phrases at scale</title>
		<author>
			<persName><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mujeen</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12624</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>Bart</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15020</idno>
		<title level="m">Pre-training via paraphrasing</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Retrieval-augmented generation for knowledge-intensive nlp tasks</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandara</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>K√ºttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rockt√§schel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.11401</idno>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph enhanced dual attention network for document-level relation extraction</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhonghao</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
				<meeting>the 28th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020a</date>
			<biblScope unit="page" from="1551" to="1560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Sac: Accelerating and structuring self-attention via sparse adaptive connection</title>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.09833</idno>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Bertgcn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05727</idno>
		<title level="m">Transductive text classification by combining gcn and bert</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Large text compression benchmark</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Mahoney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">On the state of the art of evaluation in neural language models</title>
		<author>
			<persName><forename type="first">G√°bor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05589</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiayu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.14528</idno>
		<title level="m">Fast nearest neighbor machine translation</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843</idno>
		<title level="m">Pointer sentinel mixture models</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Regularizing and optimizing lstm language models</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02182</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Statistical language models based on neural networks</title>
		<author>
			<persName><forename type="first">Tom√°≈°</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2nd April, 80. 2012</date>
			<biblScope unit="page">26</biblScope>
			<pubPlace>Mountain View</pubPlace>
		</imprint>
	</monogr>
	<note>Presentation at Google</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Estimation of probabilities in the language model of the ibm speech recognition system</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Nadas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="859" to="861" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improving transformer models by reordering their sublayers</title>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Ofir Press</publisher>
			<date type="published" when="2020">2020a</date>
			<biblScope unit="page" from="2996" to="3005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Shortformer: Better language modeling using shorter inputs</title>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15832</idno>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fast parametric learning with activation memorization</title>
		<author>
			<persName><forename type="first">Jack</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4228" to="4237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Jack W Rae</surname></persName>
		</author>
		<author>
			<persName><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="53" to="68" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuldip K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName><forename type="first">Claude</forename><surname>Elwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shannon</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMOBILE mobile computing and communications review</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="55" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<title level="m">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Mesh-tensorflow: Deep learning for supercomputers</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Penporn</forename><surname>Koanantakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Mass: Masked sequence to sequence pre-training for language generation</title>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5926" to="5936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Exploring graph-structured passage representation for multi-hop reading comprehension with graph neural networks</title>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02040</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Adaptive attention span in transformers</title>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">√âdouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="331" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Zijun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Chinesebert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.16038</idno>
		<title level="m">Chinese pretraining enhanced by glyph and pinyin information</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Efficient retrieval augmented generation from unstructured knowledge for task-oriented dialog</title>
		<author>
			<persName><forename type="first">David</forename><surname>Thulke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nico</forename><surname>Daheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Dugast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04643</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning to remember translation history with a continuous cache</title>
		<author>
			<persName><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="407" to="420" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">≈Åukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Session-based recommendation with graph neural networks</title>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyuan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="346" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Ziang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aiming</forename><surname>L√©vy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02573</idno>
		<title level="m">Data noising as smoothing in neural network language models</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Approximate nearest neighbor negative contrastive learning for dense text retrieval</title>
		<author>
			<persName><forename type="first">Lee</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwok-Fung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junaid</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnold</forename><surname>Overwijk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00808</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
				<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7370" to="7377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04070</idno>
		<title level="m">Bp-transformer: Modelling long-range context via binary partitioning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Guiding neural machine translation with retrieved translation pieces</title>
		<author>
			<persName><forename type="first">Jingyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eiichro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02559</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
