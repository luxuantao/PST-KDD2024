<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Binarized Graph Representations with Multi-faceted antization Reinforcement for Top-K Recommendation</title>
				<funder ref="#_pPSRTQM">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
				<funder ref="#_JKqwV2x">
					<orgName type="full">Research Grants Council of the Hong Kong Special Administrative Region, China</orgName>
				</funder>
				<funder ref="#_jjvRVES">
					<orgName type="full">Research Impact Fund</orgName>
				</funder>
				<funder ref="#_kCjwU47">
					<orgName type="full">CUHK</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-06-05">5 Jun 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yankai</forename><surname>Chen</surname></persName>
							<email>ykchen@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
							<email>huifeng.guo@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yingxue</forename><surname>Zhang</surname></persName>
							<email>yingxue.zhang@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chen</forename><surname>Ma</surname></persName>
							<email>chenma@cityu.edu.hk</email>
							<affiliation key="aff2">
								<orgName type="institution">City University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">City University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
							<email>tangruiming@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingjie</forename><surname>Li</surname></persName>
							<email>lijingjie1@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
							<email>king@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">KDD &apos;22</orgName>
								<address>
									<addrLine>August 14-18</addrLine>
									<postCode>2022</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Binarized Graph Representations with Multi-faceted antization Reinforcement for Top-K Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-06-05">5 Jun 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3534678.3539452</idno>
					<idno type="arXiv">arXiv:2206.02115v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Recommender system</term>
					<term>Quantization-based</term>
					<term>Binarization</term>
					<term>Graph Convolutional Network</term>
					<term>Graph Representation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning vectorized embeddings is at the core of various recommender systems for user-item matching. To perform efficient online inference, representation quantization, aiming to embed the latent features by a compact sequence of discrete numbers, recently shows the promising potentiality in optimizing both memory and computation overheads. However, existing work merely focuses on numerical quantization whilst ignoring the concomitant information loss issue, which, consequently, leads to conspicuous performance degradation. In this paper, we propose a novel quantization framework to learn Binarized Graph Representations for Top-K Recommendation (BiGeaR). BiGeaR introduces multi-faceted quantization reinforcement at the pre-, mid-, and post-stage of binarized representation learning, which substantially retains the representation informativeness against embedding binarization. In addition to saving the memory footprint, BiGeaR further develops solid online inference acceleration with bitwise operations, providing alternative flexibility for the realistic deployment. The empirical results over five large real-world benchmarks show that BiGeaR achieves about 22%?40% performance improvement over the state-of-theart quantization-based recommender system, and recovers about 95%?102% of the performance capability of the best full-precision counterpart with over 8? time and space reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS Concepts</head><p>? Information systems ? Recommender systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recommender systems, aiming to perform personalized information filtering <ref type="bibr" target="#b64">[65]</ref>, are versatile to many Internet applications. Learning vectorized user-item representations (i.e., embeddings), as the core of various recommender models, is the prerequisite for online inference of user-item interactions <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b55">56]</ref>. With the explosive data expansion (e.g., Amazon owns over 150M users and 350M products <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b51">52]</ref>), one major existing challenge however is to perform inference efficiently. This usually requires large memory and computation consumption (e.g., for Amazon 500M-scaled fullprecision<ref type="foot" target="#foot_0">1</ref> embedding table) on certain data centers <ref type="bibr" target="#b48">[49]</ref>, and therefore hinders the deployment to devices with limited resources <ref type="bibr" target="#b48">[49]</ref>.</p><p>To tackle this issue, representation quantization recently provides the promising feasibility. Generally, it learns to quantize latent features of users and items via converting the continuous full-precision representations into discrete low-bit ones. The quantized representations thus are conducive to model size reduction and inference speed-up with low-bit arithmetic on devices where CPUs are typically more affordable than expensive GPUs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Technically, quantization can be categorized into multi-bit, 2-bit (i.e., ternarized), and 1-bit (i.e., binarized) quantization. With only one bitwidth, representation binarization for recommendation takes the most advantage of representation quantization and therefore draws the growing attention recently <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>Despite the promising potentiality, it is still challenging to develop realistic deployment mainly because of the large performance degradation in Top-K recommendation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b49">50]</ref>. The crux of the matter is the threefold information loss:</p><p>? Limited expressivity of latent features. Because of the discrete constraints, mapping full-precision embeddings into compact binary codes with equal expressivity is NP-hard <ref type="bibr" target="#b18">[19]</ref>. Thus, instead of proposing complex and deep neural structures for quantization <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b69">70]</ref>, sign(?) function is widely adopted to achieve <ref type="bibr" target="#b0">(1)</ref> embedding binarization <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b54">55]</ref>. However, this only guarantees the sign (+/-) correlation for each embedding entry.</p><p>Compared to the original full-precision embeddings, binarized targets produced from sign(?) are naturally less informative.</p><p>? Degraded ranking capability. Ranking capability, as the essential measurement of Top-K recommendation, is the main objective to work on. Apart from the inevitable feature loss in numerical quantization, previous work further ignores the discrepancy of hidden knowledge that is inferred by full-precision and binarized embeddings <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b49">50]</ref>. However, such hidden knowledge is vital to reveal users' preference towards different items, losing of which may thus draw degraded ranking capability and suboptimal model learning accordingly. ? Inaccurate gradient estimation. Due to the non-differentiability of quantization function sign(?), Straight-Through Estimator (STE) <ref type="bibr" target="#b3">[4]</ref> is widely adopted to assume all propagated gradients as 1 in backpropagation <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b49">50]</ref>. Intuitively, the integral of 1 is a certain linear function other than sign(?), whereas this may lead to inaccurate gradient estimation and produce inconsistent optimization directions in the model training.</p><p>To address aforementioned problems, we propose a novel quantization framework, namely BiGeaR, to learn the Binarized Graph Representations for Top-K Recommendation. Based on the natural bipartite graph topology of user-item interactions, we implement BiGeaR with the inspiration from graph-based models, i.e., Graph Convolutional Networks (GCNs) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30]</ref>. With the emphasis on deepening the exploration of multi-hop subgraph structures, GCNbased recommender models capture the high-order interactive relations and well simulate Collaborative Filtering for recommendation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b64">65]</ref>. Specifically, BiGeaR sketches graph nodes (i.e., users and items) with binarized representations, which facilitates nearly one bit-width representation compression. Furthermore, our model BiGeaR decomposes the prediction formula (i.e., inner product) into bitwise operations (i.e., Popcount and XNOR). This dramatically reduces the number of floating-point operations (#FLOP) and thus introduces theoretically solid acceleration for online inference. To avoid large information loss, as shown in Figure <ref type="figure" target="#fig_0">1</ref>(a), BiGeaR technically consists of multi-faceted quantization reinforcement at the pre-, mid-, and post-stage of binarized representation learning:</p><p>(1) At the pre-stage of model learning, we propose the graph layerwise quantization (from low-to high-order interactions) to consecutively binarize the user-item features with different semantics. Our analysis indicates that such layer-wise quantization can actually achieve the magnification effect of feature uniqueness, which significantly compensates for the limited expressivity of embeddings binarization. The empirical study also justifies that, this is more effective to enrich the quantization informativeness, rather than simply increasing the embedding sizes in the conventional manner <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b49">50]</ref>. (2) During the mid-stage of embedding quantization, BiGeaR introduces the self-supervised inference distillation to develop the low-loss ranking capability inheritance. Technically, it firstly extracts several pseudo-positive training samples that are inferred by full-precision embeddings of BiGeaR. Then these samples serve as the direct regularization target to the quantized embeddings, such that they can distill the ranking knowledge from full-precision ones to have similar inference results.</p><p>Different from the conventional knowledge distillation, our approach is tailored specifically for Top-K recommendation and therefore boosts the performance with acceptable training costs. (3) As for the post-stage backpropagation of quantization optimization, we propose to utilize the approximation of Dirac delta function (i.e., function) <ref type="bibr" target="#b5">[6]</ref> for more accurate gradient estimation. In contrast to STE, our gradient estimator provides the consistent optimization direction with sign(?) in both forward and backward propagation. The empirical study demonstrates its superiority over other gradient estimators. Empirical Results. The extensive experiments over five real-world benchmarks show that, BiGeaR significantly outperforms the stateof-the-art quantization-based recommender model by 25.77%?40.12% and 22.08%?39.52% w.r.t. Recall and NDCG metrics. Furthermore, it attains 95.29%?100.32% and 95.32%?101.94% recommendation capability compared to the best-performing full-precision model, with over 8? inference acceleration and space compression. Discussion. It is worthwhile mentioning that BiGeaR is related to hashing-based models (i.e., learning to hash) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, as, conceptually, binary hashing can be viewed as 1-bit quantization. However, as shown in Figure <ref type="figure" target="#fig_0">1</ref>(b), they have different motivations. Hashingbased models are usually designed for fast candidate generation, followed by full-precision re-ranking algorithms for accurate prediction. Meanwhile, BiGeaR is end-to-end that aims to make predictions within the proposed architecture. Hence, we believe BiGeaR is technically related but motivationally orthogonal to them. Organization. We present BiGeaR methodology and model analysis in ? 2 and ? 3. Then we report the experiments and review the related work in ? 4 and ? 5 with the conclusion in ? 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BiGeaR Methodology</head><p>In this section, we formally introduce: (1) graph layer-wise quantization for feature magnification; (2) inference distillation for ranking capability inheritance; (3) gradient estimation for better model optimization. BiGeaR framework is illustrated in Figure <ref type="figure" target="#fig_3">2(a)</ref>. The notation table and pseudo-codes are attached in Appendix A and B.</p><p>Preliminaries: graph convolution. Its general idea is to learn node representations by iteratively propagating and aggregating latent features of neighbors via the graph topology <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b57">58]</ref>. We adopt the graph convolution paradigm working on the continuous space from LightGCN <ref type="bibr" target="#b20">[21]</ref> that recently shows good recommendation performance. Let ( ) , ( ) ? R denote the continuous feature embeddings of user and item computed after layers of information propagation. N ( ) represents 's neighbor set. They can be iteratively updated by utilizing information from the ( -1)-th layer:</p><formula xml:id="formula_0">( ) = ?N ( ) 1 | N ( ) | ? | N ( ) | ( -1) , ( ) = ?N ( ) 1 | N ( ) | ? | N ( ) | ( -1) . (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Layer-wise Quantization</head><p>We propose the graph layer-wise quantization mainly by computing quantized embeddings and embedding scalers: (1) these quantized embeddings sketch the full-precision embeddings with -dimensional binarized codes (i.e., {-1, 1} ); and (2) each embedding scaler reveals the value range of original embedding entries. Specifically, during the graph convolution at each layer, we track the intermediate information (e.g., ( ) ) and perform the layer-wise   1-bit quantization in parallel as:</p><formula xml:id="formula_1">+ + -+ - + --+ - wl wl ? (l) u ? (l) i q (l) u q (l) i 1 1 0 1 0 1 0 0 1 0 q(l) i q(l) u XNOR 1 0 1 1 1 w 2 l ? (l) u ? (l) i Popcount 4 ?2 -d 3w 2 l ? (l) u ? (l) i w 2 l ? (l) u ? (l) i Inner Product wl wl ? (l) u ? (l) i Top-R inference Top-R inference Top-R inference Input! l = 1 l = L l = 0 (a)</formula><formula xml:id="formula_2">( ) = sign ( ) , ( ) = sign ( ) ,<label>(2)</label></formula><p>where embedding segments ( ) , ( ) ? {-1, 1} retain the node latent features directly from ( ) and ( ) . To equip with the layerwise quantized embeddings, we further include a layer-wise positive embedding scaler for each node (e.g., ( ) ? R + ), such that ( ) ( ) ( ) . Then for each entry in ( ) ( ) , it is still binarized by</p><formula xml:id="formula_3">{-( ) , ( ) }.</formula><p>In this work, we compute the mean of L1-norm as:</p><formula xml:id="formula_4">( ) = 1 ? | | ( ) | | 1 , ( ) = 1 ? | | ( ) | | 1 .<label>(3)</label></formula><p>Instead of setting ( ) and ( ) as learnable, such deterministic computation is simple yet effective to provide the scaling functionality whilst substantially pruning the parameter search space. The experimental demonstration is in Appendix F.2. After layers of quantization and scaling, we have built the following binarized embedding table for each graph node as:</p><formula xml:id="formula_5">A = { (0) , (1) , ? ? ? , ( ) }, Q = { (0) , (1) , ? ? ? , ( ) }.<label>(4)</label></formula><p>From the technical perspective, BiGeaR binarizes the intermediate semantics at different layers of the receptive field <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b58">59]</ref> for each node. This, essentially, achieves the magnification effect of feature uniqueness to enrich user-item representations via the interaction graph exploration. We leave the analysis in ? 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Prediction Acceleration</head><p>Model Prediction. Based on the learned embedding table, we predict the matching scores by adopting the inner product:</p><formula xml:id="formula_6">, = ( A , Q ), ( A , Q ) ,<label>(5)</label></formula><p>where function (?, ?) in this work is implemented as:</p><formula xml:id="formula_7">( A , Q ) = =0 ( ) ( ) , ( A , Q ) = =0 ( ) ( ) .<label>(6)</label></formula><p>Here represents concatenation of binarized embedding segments, in which weight measures the contribution of each segment in prediction.</p><p>can be defined as a hyper-parameter or a learnable variable (e.g., with attention mechanism <ref type="bibr" target="#b52">[53]</ref>). In this work, we set ? to linearly increase for segments from lower-layers to higher-layers, mainly for its computational simplicity and stability.</p><p>Computation Acceleration. Notice that for each segment of ( A , Q ), e.g., ( ) ( ) , entries are binarized by two values (i.e., -( ) or ( ) ). Thus, we can achieve the prediction acceleration by decomposing Equation ( <ref type="formula" target="#formula_6">5</ref>) with bitwise operations. Concretely, in practice, ( ) and ( ) will be firstly encoded into basic -bits binary codes, denoted by ( ) , ( ) ? {0, 1} . Then we replace Equation ( <ref type="formula" target="#formula_6">5</ref>) by introducing the following formula:</p><formula xml:id="formula_8">, = =0 2 ( ) ( ) ? 2Popcount XNOR( ( ) , ( ) ) -.<label>(7)</label></formula><p>Compared to the original computation approach in Equation ( <ref type="formula" target="#formula_6">5</ref>), our bitwise-operation-supported prediction in Equation ( <ref type="formula" target="#formula_8">7</ref>) reduces the number of floating-point operations (#FLOP) with Popcount and XNOR. We illustrate an example in Figure <ref type="figure" target="#fig_3">2</ref>(b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Self-supervised Inference Distillation</head><p>To alleviate the asymmetric inference capability issue between full-precision representations and binarized ones, we introduce the self-supervised inference distillation such that binarized embeddings can well inherit the inference knowledge from full-precision ones. Generally, we treat our full-precision intermediate embeddings (e.g., ( ) ) as the teacher embeddings and the quantized segments as the student embeddings. Given both teacher and student embeddings, we can obtain their prediction scores as ? , and , . For Top-K recommendation, then our target is to reduce their discrepancy as:</p><formula xml:id="formula_9">argmin D ( ? , , , ).<label>(8)</label></formula><p>A straightforward implementation of function D from the conventional knowledge distillation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25]</ref> is to minimize their Kullback-Leibler divergence (KLD) or mean squared error (MSE). Despite their effectiveness in classification tasks (e.g., visual recognition <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b59">60]</ref>), they may not be appropriate for Top-K recommendation as:</p><p>? Firstly, both KLD and MSE in D encourage the student logits (e.g., , ) to be similarly distributed with the teacher logits in a macro view. But for ranking tasks, they may not well learn the relative order of user preferences towards items, which, however, is the key to improving Top-K recommendation capability. ? Secondly, they both develop the distillation over the whole item corpus, which may be computational excessive for realistic model training. As the item popularity usually follows the Long-tail distribution <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b50">51]</ref>, learning the relative order of those frequently interacted items located at the tops of ranking lists actually contributes more to the Top-K recommendation performance.</p><p>To develop effective inference distillation, we propose to extract additional pseudo-positive training samples from teacher embeddings to regularize the targeted embeddings on each convolutional layer. Concretely, let represent the activation function (e.g., Sigmoid). We first pre-train the teacher embeddings to minimize the Bayesian Personalized Ranking (BPR) loss <ref type="bibr" target="#b45">[46]</ref>:</p><formula xml:id="formula_10">L ? = - ?U ?N ( ) ?N ( ) ln ( ? , - ? , ),<label>(9)</label></formula><p>where L ? forces the prediction of an observed interaction to be higher than its unobserved counterparts, and the teacher score ? , is computed as</p><formula xml:id="formula_11">? , = =0 ( ) , =0 ( ) .</formula><p>Please notice that we only disable binarization and its associated gradient estimation in pre-training. After it is well-trained, for each user , we retrieve the layer-wise teacher inference towards all items I:</p><formula xml:id="formula_12">?,( ) = ( ) , ( ) ?I .<label>(10)</label></formula><p>Based on the segment scores ?,( ) at the -th layer, we first sort out Top-R items with the highest matching scores, denoted by ( )</p><formula xml:id="formula_13">? ( ).</formula><p>And hyper-parameter R ? I. Inspired by <ref type="bibr" target="#b50">[51]</ref>, then we propose our layer-wise inference distillation as follows:</p><formula xml:id="formula_14">L ( ) = =0 L ( ) ( ,( ) , ( ) ? ( )) = - 1 =0 =1 ? ln ( ,<label>( ) , ( ) ? ( , )</label></formula><p>), <ref type="bibr" target="#b10">(11)</ref> where student scores ,( ) is computed similarly to Equation ( <ref type="formula" target="#formula_12">10</ref>) and ( )  ? ( , ) returns the -th high-scored item from the pseudopositive samples.</p><p>is the ranking-aware weight presenting two major effects: (1) since samples in ( )  ? ( ) are not necessarily all ground-truth positive, thus balances their contribution to the overall loss; (2) it dynamically adjusts the weight importance for different ranking positions in ( )  ? ( ). To achieve these, can be developed by following the parameterized geometric distribution for approximating the tailed item popularity <ref type="bibr" target="#b44">[45]</ref>:</p><formula xml:id="formula_15">= 1 exp(-2 ? ),<label>(12)</label></formula><p>where 1 and 2 control the loss contribution level and sharpness of the distribution. Intuitively, L encourages highly-recommended items from full-precision embeddings to more frequently appear in the student's inference list. Moreover, our distillation approach regularizes the embedding quantization in a layer-wise manner as well; this will effectively narrow their inference discrepancy for a more correlated recommendation capability. Objective Function. Combining L that calculates BPR loss (similar to Equation ( <ref type="formula" target="#formula_10">9</ref>)) with the student predictions from Equation <ref type="bibr" target="#b4">(5)</ref> and L for all training samples, our final objective function for learning embedding binarization is defined as:</p><formula xml:id="formula_16">L = L + L + | |? | | 2 2 ,<label>(13)</label></formula><p>where</p><formula xml:id="formula_17">| |? | | 2 2</formula><p>is the 2-regularizer of node embeddings parameterized by hyper-parameter to avoid over-fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Gradient Estimation</head><p>Although Straight-Through Estimator (STE) <ref type="bibr" target="#b3">[4]</ref> enables an executable gradient flow for backpropagation, it however may cause the issue of inconsistent optimization direction in forward and backward propagation: as the integral of the constant 1 in STE is a linear function, other than sign(?) function. To furnish more accurate gradient estimation, in this paper, we utilize the approximation of Dirac delta function <ref type="bibr" target="#b5">[6]</ref> for gradient estimation.</p><p>Concretely, let ( ) denote the unit-step function, a.k.a., Heaviside step function <ref type="bibr" target="#b13">[14]</ref>, where ( ) = 1 for &gt; 0 and ( ) = 0 otherwise. Obviously, we can take a translation from step function to sign(?) by sign( ) = 2 ( ) -1, and thus theoretically sign( ) = 2 ( ) . As for ( ) , it has been proved <ref type="bibr" target="#b5">[6]</ref> that, ( ) = 0 if ? 0, and ( ) = ? otherwise, which exactly is the Dirac delta function, a.k.a., the unit impulse function ( ?) <ref type="bibr" target="#b5">[6]</ref> shown in Figure <ref type="figure" target="#fig_4">3</ref>(a). However, it is still intractable to directly use ( ?) for gradient estimation. A feasible solution is to approximate ( ?) by introducing zero-centered Gaussian probability density as follows:</p><formula xml:id="formula_18">( ) = lim ?? | | ? exp(-( ) 2 ),<label>(14)</label></formula><p>which imlies that:</p><formula xml:id="formula_19">sign( ) 2 ? exp(-( ) 2 ).<label>(15)</label></formula><p>As shown in Figure <ref type="figure" target="#fig_4">3</ref>(b)-(c), hyper-parameter determines the sharpness of the derivative curve for approximation to sign(?). Intuitively, our proposed gradient estimator follows the main direction of factual gradients with sign(?) in model optimization. This will produce a coordinated value quantization from continuous embeddings to quantized ones, and thus a series of evolving gradients can be estimated for the inputs with diverse value ranges. As we will show in ? 4.6 of experiments, our gradient estimator can work better than other recent estimators <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b61">62]</ref>.</p><p>3 Model Analysis</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Magnification of Feature Uniqueness</head><p>We take user as an example for illustration and the following analysis can be popularized to other nodes without loss of generality. Similar to sensitivity analysis in statistics <ref type="bibr" target="#b30">[31]</ref> and influence diffusion in social networks <ref type="bibr" target="#b60">[61]</ref>, we measure how the latent feature of a distant node finally affects 's representation segments before binarization (e.g., ( ) ), supposing is a multi-hop neighbor of . We denote the feature enrichment ratio E ( ) ? as the L1-norm of Jacobian matrix ( ) / (0) , by detecting the absolute influence of all fluctuation in entries of (0) to ( ) , i.e., E ( )</p><formula xml:id="formula_20">? = ( ) / (0) 1 .</formula><p>Focusing on a -length path ? connected by the node sequence: ? , -1</p><formula xml:id="formula_21">? , ? ? ? , 1 ? , 0</formula><p>? , where ? = and 0 ? = , we follow the chain rule to develop the derivative decomposition as:</p><formula xml:id="formula_22">( ) (0) = ?=1 ? ? ? ? ? ? ( ) ? (0) 0 ? ? ? ? ? ? ? ? = ?=1 1 = 1 |N ( ? ) | ? 1 |N ( -1 ? ) | ? = |N ( ) | |N ( ) | ?=1 =1 1 |N ( ? ) | ? ,<label>(16)</label></formula><p>where is the number of paths between and in total. Since all factors in the computation chain are positive, then:</p><formula xml:id="formula_23">E ( ) ? = ( ) (0) 1 = ? |N ( ) | |N ( ) | ? ?=1 =1 1 |N ( ? ) | . (<label>17</label></formula><formula xml:id="formula_24">)</formula><p>Note that here the term ?=1 =1 1/ | N ( ? ) | is exactly the probability of the -length random walk starting at that finally arrives at , which can be interpreted as:</p><formula xml:id="formula_25">E ( ) ? ? 1 | N ( ) | ? ( -step random walk from arrives at ). (<label>18</label></formula><formula xml:id="formula_26">)</formula><p>Magnification Effect of Feature Uniqueness. Equation ( <ref type="formula" target="#formula_25">18</ref>) implies that, with the equal probability to visit adjacent neighbors, distant nodes with fewer degrees (i.e., | N ( ) |) will contribute more feature influence to user . But most importantly, in practice, these -hop neighbors of user usually represent certain esoteric and unique objects with less popularity. By collecting the intermediate information in different depth of the graph convolution, we can achieve the feature magnification effect for all unique nodes that live within hops of graph exploration, which finally enrich 's semantics in all embedding segments for quantization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Complexity Analysis</head><p>To discuss the feasibility for realistic deployment, we compare BiGeaR with the best full-precision model LightGCN <ref type="bibr" target="#b20">[21]</ref>, as they are end-to-end with offline model training and online prediction.</p><p>Training Time Complexity. , , and represent the number of users, items, and edges; and are the epoch number and batch size. We use BiGeaR ? and BiGeaR to denote the pretraining version and binarized one, respectively. As we can observe from Table <ref type="table" target="#tab_0">1</ref>, (1) both BiGeaR ? and BiGeaR takes asymptotically similar complexity of graph convolution with LightGCN (where BiGeaR takes additional (2 ( +1) ) complexity for binarization). (2) For L computation, to prevent over-smoothing issue <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">35]</ref>, usually ? 4; compare to the convolution operation, the complexity of L is acceptable. (3) For L preparation, after the training of BiGeaR ? , we firstly obtain the layerwise prediction scores with ( ( + 1)) time complexity and then sort out the Top-R pseudo-positive samples for each user with ( + ln ). For BiGeaR , it takes a layer-wise inference distillation from BiGeaR ? with ( +1) . ( <ref type="formula" target="#formula_5">4</ref>) To estimate the gradients for BiGeaR , it takes (2 ( + 1) ) for all training samples. </p><formula xml:id="formula_27">L Loss - ( + 1) ( + ln ) ( + 1) Gradient Estimation - - 2 ( + 1)</formula><p>Memory overhead and Prediction Acceleration. We measure the memory footprint of embedding tables for online prediction. As we can observe from the results in Table <ref type="table" target="#tab_1">2</ref>: (1) Theoretically, the embedding size ratio of our model over LightGCN is 32 ( +1) (32+ ) . Normally, ? 4 and ? 64, thus our model achieves at least 4? space cost compression. (2) As for the prediction time cost, we compare the number of binary operations (#BOP) and floatingpoint operations (#FLOP) between our model and LightGCN. We find that BiGeaR replaces most of floating-point arithmetics (e.g., multiplication) with bitwise operations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>We evaluate our model on Top-K recommendation task with the aim of answering the following research questions: ? RQ1. How does BiGeaR perform compared to state-of-the-art full-precision and quantization-based models? ? RQ2. How is the practical resource consumption of BiGeaR? ? RQ3. How do proposed components affect the performance?  Evaluation Metric. In Top-K recommendation evaluation, we select two widely-used evaluation protocols Recall@K and NDCG@K to evaluate Top-K recommendation capability.</p><p>Competing Methods. We include the following recommender models: (1) 1-bit quantization-based methods including graph-based (GumbelRec <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b67">68]</ref>, HashGNN <ref type="bibr" target="#b49">[50]</ref>) and general model-based (LSH <ref type="bibr" target="#b15">[16]</ref>, HashNet <ref type="bibr" target="#b6">[7]</ref>, CIGAR <ref type="bibr" target="#b27">[28]</ref>), and (2) full-precision models including neural-network-based (NeurCF <ref type="bibr" target="#b22">[23]</ref>) and graph-based (NGCF <ref type="bibr" target="#b55">[56]</ref>, DGCF <ref type="bibr" target="#b56">[57]</ref>, LightGCN <ref type="bibr" target="#b20">[21]</ref>). Detailed introduction of these methods are attached in Appendix D. We exclude early quantization-based recommendation baselines, e.g., CH <ref type="bibr" target="#b38">[39]</ref>, DiscreteCF <ref type="bibr" target="#b65">[66]</ref>, DPR <ref type="bibr" target="#b66">[67]</ref>, and full-precision solutions, e.g., GC-MC <ref type="bibr" target="#b4">[5]</ref>, PinSage <ref type="bibr" target="#b64">[65]</ref>, mainly because the above competing models <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b55">56]</ref> have validated the superiority.</p><p>Experiment Settings. Our model is implemented by Python 3.7 and PyTorch 1.14.0 with non-distributed training. The experiments are run on a Linux machine with 1 NVIDIA V100 GPU, 4 Intel Core i7-8700 CPUs, 32 GB of RAM with 3.20GHz. For all the baselines, we follow the official reported hyper-parameter settings, and for methods lacking recommended settings, we apply a grid search for hyper-parameters. The embedding dimension is searched in {32, 64, 128, 256, 512, 1024}. The learning rate is tuned within {10 -<ref type="foot" target="#foot_3">4</ref> , 10 -<ref type="foot" target="#foot_2">3</ref> , 10 -<ref type="foot" target="#foot_1">2</ref> } and the coefficient of 2 normalization is tuned among {10 -<ref type="foot" target="#foot_5">6</ref> , 10 -<ref type="foot" target="#foot_4">5</ref> , 10 -4 , 10 -3 }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4: Performance comparison (waveline and underline represent the best performing full-precision and quantization-based models).</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>MovieLens (%) Gowalla (%) Pinterest (%) Yelp2018 (%) Amazon-Book (%) Recall@20 NDCG@20 Recall@20 NDCG@20 Recall@20 NDCG@20 Recall@20 NDCG@20 Recall@20 NDCG@20 We initialize and optimize all models with default normal initializer and Adam optimizer <ref type="bibr" target="#b28">[29]</ref>. We report all hyper-parameters in Appendix E for reproducibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Analysis (RQ1)</head><p>We evaluate Top-K recommendation by varying K in {20, 40, 60, 80, 100}. We summarize the Top@20 results in Table <ref type="table">4</ref> for detailed comparison and plot the Top-K recommendation curves in Appendix F.1. From Table <ref type="table">4</ref>, we have the following observations:</p><p>? Our model offers a competitive recommendation capability to state-of-the-art full-precision recommender models.</p><p>(1) BiGeaR generally outperforms most of full-precision recommender models excluding LightGCN over five benchmarks. The main reason is that our model and LightGCN take similar graph convolution methodology with network simplification <ref type="bibr" target="#b20">[21]</ref>, e.g., removing self-connection and feature transformation, which is proved to be effective for Top-K ranking and recommendation. Moreover, BiGeaR collects the different levels of interactive information in multi depths of graph exploration, which significantly enriches semantics to user-item representations for binarization.</p><p>(2) Compared to the state-of-the-art method Light-GCN, our model develops about 95%?102% of performance capability w.r.t. Recall@20 and NDCG@20 throughout all datasets. This shows that our proposed model designs are effective to narrow the performance gap with full-precision model LightGCN.</p><p>Although the binarized embeddings learned by BiGeaR may not achieve the exact expressivity parity with the full-precision ones learned by LightGCN, considering the advantages of space compression and inference acceleration that we will present later, we argue that such performance capability is acceptable, especially for those resource-limited deployment scenarios. ? Compared to all binarization-based recommender models, BiGeaR presents the empirically remarkable and statistically significant performance improvement. (1) Two conventional methods (LSH, HashNet) for general item retrieval tasks underperform CIGAR, HashGNN and BiGeaR, showing that a direct model adaptation may be too trivial for Top-K recommendation.</p><p>(2) Compared to CIGAR, graph-based models generally work better. This is mainly because, CIGAR combines general neural networks with learning to hash techniques for fast candidate generation; on the contrary, graph-based models are more capable of exploring multi-hop interaction subgraphs to directly simulate the high-order collaborative filtering process for model learning.</p><p>(3) Our model further outperforms HashGNN by about 26%?40% and 22%?40% w.r.t. Recall@20 and NDCG@20, proving the effectiveness of our proposed multi-faceted optimization components in embedding binarization. (4) Moreover, the significance test in which -value &lt; 0.05 indicates that the improvements over all five benchmarks are statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Resource Consumption Analysis (RQ2)</head><p>We analyze the resource consumption in training, online inference, and memory footprint by comparing to the best two competing models, i.e., LightGCN and HashGNN. Due to the page limits, we report the empirical results of MovieLens dataset in Table <ref type="table" target="#tab_5">5</ref>.</p><p>(</p><p>: we set batch size = 2048 and dimension size = 256 for all models. We find that HashGNN is fairly time-consuming than LightGCN and BiGeaR. This is because HashGNN adopts the early GCN framework <ref type="bibr" target="#b17">[18]</ref> as the backbone; LightGCN and BiGeaR utilize more simplified graph convolution architecture in which operations such as self-connection, feature transformation, and nonlinear activation are all removed <ref type="bibr" target="#b20">[21]</ref>. Furthermore, BiGeaR needs 5.1s and 6.2s per epoch for pretraining and quantization, both of which take slightly more yet asymptotically similar time cost with LightGCN, basically following the complexity analysis in ? 3.2.</p><p>(2)</p><p>: we randomly generate 1,000 queries for online prediction and conduct experiments with the vanilla NumPy<ref type="foot" target="#foot_6">7</ref> on CPUs.  We observe that HashGNN takes a similar time cost with Light-GCN. This is because, while HashGNN ? purely binarizes the continuous embeddings, its relaxed version HashGNN adopts a Bernoulli random variable to provide the probability of replacing the quantized digits with original real values <ref type="bibr" target="#b49">[50]</ref>. Thus, although HashGNN ? can use Hamming distance for prediction acceleration, HashGNN with the improved recommendation accuracy can only be computed by floating-point arithmetics. As for BiGeaR, thanks to its bitwise-operation-supported capability, it runs about 8? faster than LightGCN whilst maintaining the similar performance on MovieLens dataset.</p><p>: we only store the embedding tables that are necessary for online inference. As we just explain, HashGNN interprets embeddings by randomly selected real values, which, however, leads to the expansion of space consumption. In contrast to HashGNN , BiGeaR can separately store the binarized embeddings and corresponding scalers, making a balanced trade-off between recommendation accuracy and space overhead.  <ref type="figure" target="#fig_6">4</ref> by denoting Recall@20 and NDCG@20 in red and blue, respectively, and vary color brightness for different variants. From these results, we have the following explanations.</p><p>? Firstly, BiGeaR / discards the layer-wise quantization and adopts the conventional manner by quantizing the last outputs from convolution iterations. We fix dimension = 256 and vary layer number for BiGeaR, and only vary dimension for variant BiGeaR / with fixed = 2. (1) Even by continuously increasing the dimension size from 64 to 1024, BiGeaR / slowly improves both Recall@20 and NDCG@20 performance.</p><p>(2) By contrast, our layer-wise quantization presents a more efficient capability in improving performance by increasing from 0 to 3. When = 4, BiGeaR usually exhibits a conspicuous performance decay, mainly because of the common over-smoothing issue in graph-based models <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">35]</ref>. Thus, with a moderate size = 256 and convolution number ? 3, BiGeaR can attain better performance with acceptable computational complexity.</p><p>? Secondly, BiGeaR / omits the feature magnification effect by adopting the way used in HashGNN <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b49">50]</ref> as:</p><formula xml:id="formula_30">( ) = ?N ( ) 1 |N ( ) | ( -1) .</formula><p>(19) Similar to the analysis in ? 3.1, such modification will finally disable the "magnification term" in Equation <ref type="bibr" target="#b17">(18)</ref> and simplify it to the vanilla random walk for graph exploration. Although BiGeaR / presents similar curve trends with BiGeaR when increases, the general performance throughout all five datasets is unsatisfactory compared to BiGeaR. This validates the effectiveness of BiGeaR's effort in magnifying unique latent features, which enriches user-item representations and boosts Top-K recommendation performance accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Study of Inference Distillation (RQ3.B)</head><p>4.5.1 Effect of Layer-wise Distillation. We study the effectiveness of our inference distillation by proposing two ablation variants, namely noID and endID. While noID totally removes our information distillation in model training, endID downgrades the original layer-wise distillation to only distill information from the last layer of graph convolution. As shown in Table <ref type="table" target="#tab_6">6</ref>, both noID and en-dID draw notable performance degradation. Furthermore, the performance gap between endID and BiGeaR shows that it is efficacious to conduct our inference distillation in a layer-wise manner for further performance enhancement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variant</head><p>MovieLens Gowalla Pinterest Yelp2018 Amazon-book R@20 N@20 R@20 N@20 R@20 N@20 R@20 N@20 R@20 N@20 noID 24. <ref type="bibr" target="#b39">40</ref>  4.5.2 Conventional Knowledge Distillation. To compare with the conventional approach, we modify BiGeaR by applying KL divergence for layer-wise teacher and student logits, i.e., ?,( ) v.s. ,( ) . We denote this variant as KLD. As we can observe from Table <ref type="table" target="#tab_6">6</ref>, using conventional knowledge distillation with KL divergence leads to suboptimal performance. This is because KL divergence encourages the teacher and student training objects to have a similar logit distribution, but users' relative order of item preference can not be well learned from this process. Compared to the conventional approach, our proposed layer-wise Inference distillation is thus more effective for ranking information distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Study of Gradient Estimation (RQ3.C)</head><p>We compare our gradient estimation with several recently studied estimators, such as Tanh-like <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b42">43]</ref>, SSwish <ref type="bibr" target="#b11">[12]</ref>, Sigmoid <ref type="bibr" target="#b61">[62]</ref>, and projected-based estimator <ref type="bibr" target="#b37">[38]</ref> (denoted as PBE), by implementing them in BiGeaR. We report their Recall@20 in Figure <ref type="figure" target="#fig_7">5</ref> and the performance gain of our over these estimators accordingly. We have two main observations:</p><p>(1) Our proposed approach shows the consistent superiority over all other gradient estimators. These estimators usually use visimilar functions, e.g., tanh(?), to approximate sign(?). However, these functions are not necessarily theoretically relevant to sign(?). This may lead to inaccurate gradient estimation. On the contrary, as we explain in ? 2.4, we transfer the unit-step function (?) to sign(?) by sign(?) = 2 (?) -1. Then we can further estimate the gradients of sign(?) with the approximated derivatives of (?). In other words, our approach follows the main optimization direction of factual gradients with sign(?); and different from previous solutions, this guarantees the coordination in both forward and backward propagation.</p><p>(2) Furthermore, compared to the last four datasets, MovieLens dataset confronts a larger performance disparity between our approach and others. This is because MovieLens dataset is much denser than the other datasets, i.e., # # ?# = 0.0419 ? {0.00084, 0.00267, 0.0013, 0.00062}, which means that users tend to have more item interactions and complicated preferences towards different items. Consequently, this posts a higher requirement for the gradient estimation capability in learning ranking information. Fortunately, the empirical results in Figure 5 demonstrate that our solution well fulfills these requirements, especially for dense interaction graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Full-precision recommender models. (1) Collaborative Filtering (CF) is a prevalent methodology in modern recommender systems <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b64">65]</ref>. Earlier CF methods, e.g., Matrix Factorization <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b45">46]</ref>, reconstruct historical interactions to learn user-item embeddings. Recent neural-network-based models, e.g., NeurCF <ref type="bibr" target="#b22">[23]</ref> and attention-based models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref>, further boost performance with neural networks. (2) Graph-based methods focus on studying the interaction graph topology for recommendation. Graph convolutional networks (GCNs) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30]</ref> inspire early work, e.g., GC-MC <ref type="bibr" target="#b4">[5]</ref>, PinSage <ref type="bibr" target="#b64">[65]</ref>, and recent models, e.g., NGCF <ref type="bibr" target="#b55">[56]</ref>, DGCF <ref type="bibr" target="#b56">[57]</ref>, and LightGCN <ref type="bibr" target="#b20">[21]</ref>, mainly because they can well simulate the highorder CF signals among high-hop neighbors for recommendation.</p><p>Learning to hash. Hashing-based methods map dense floatingpoint embeddings into binary spaces for Approximate Nearest Neighbor (ANN) search acceleration. A representative model LSH <ref type="bibr" target="#b15">[16]</ref> has inspired a series of work for various tasks, e.g., fast retrieval of images <ref type="bibr" target="#b6">[7]</ref>, documents <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b68">69]</ref>, and categorical information <ref type="bibr" target="#b26">[27]</ref>.</p><p>For Top-K recommendation, models like DCF <ref type="bibr" target="#b65">[66]</ref>, DPR <ref type="bibr" target="#b66">[67]</ref> include neural network architectures. A recent work CIGAR <ref type="bibr" target="#b27">[28]</ref> proposes adaptive model designs for fast candidate generation. To investigate the graph structure of user-item interactions, model HashGNN <ref type="bibr" target="#b49">[50]</ref> applies hashing techniques into graph neural networks for recommendation. However, one major issue is that solely using learned binary codes for prediction usually draws a large performance decay. Thus, to alleviate the issue, CIGAR further equips with additional full-precision recommender models (e.g., BPR-MF <ref type="bibr" target="#b45">[46]</ref>) for fine-grained re-ranking; HashGNN proposes relaxed version by mixing full-precision and binary embedding codes.</p><p>Quantization-based models. Quantization-based models share similar techniques with hashing-based methods, e.g., sign(?) is usually adopted mainly because of its simplicity. However, quantizationbased models do not pursue extreme encoding compression, and thus they develop multi-bit, 2-bit, and 1-bit quantization for performance adaptation. Recently, there is growing attention to quantize graph-based models, such as Bi-GCN <ref type="bibr" target="#b53">[54]</ref> and BGCN <ref type="bibr" target="#b1">[2]</ref> However, these two models are mainly designed for geometric classification tasks, but their capability in product recommendation is unclear. Thus, in this paper, we propose BiGeaR to learn 1-bit user-item representation quantization for Top-K recommendation. Different from binary hashing-based methods, BiGeaR aims to make predictions within its own framework, making a balanced trade-off between efficiency and performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we present BiGeaR to learn binarized graph representations for recommendation with multi-faceted binarization techniques. The extensive experiments not only validate the performance superiority over competing binarization-based recommender systems, but also justify the effectiveness of all proposed model components. In the future, we plan to investigate two major possible problems. (1) It is worth developing binarization techniques for model-agnostic recommender systems with diverse learning settings <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b62">63]</ref>. (2) Instead of using sign(?) for quantization, developing compact multi-bit quantization methods with similarity-preserving is promising to improve ranking accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of BiGeaR.</figDesc><graphic url="image-1.png" coords="1,343.25,176.72,118.53,81.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Forward propagation of BiGeaR. !! (b) Prediction acceleration. (c)Backward propagation of BiGeaR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: BiGeaR first pre-trains the full-precision embeddings and then triggers the (1) graph layer-wise quantization, (2) inference distillation, and (3) accurate gradient estimation to learn the binarized representations (Best view in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Gradient estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>BiGeaRw/o F U Recall@20 BiGeaRw/o LW NDCG@20 BiGeaRw/o LW NDCG@20 BiGeaRw/o F U NDCG@20 BiGeaR</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Study of graph layer-wise quantization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Gradient estimator comparison w.r.t. Recall@20.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Traing time complexity.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">LightGCN</cell><cell cols="2">BiGeaR ?</cell><cell cols="2">BiGeaR</cell></row><row><cell cols="2">Graph Normalization</cell><cell>2</cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell>-</cell></row><row><cell cols="2">Conv. and Quant.</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2 (</cell><cell>2 + ( + 1) )</cell></row><row><cell>L</cell><cell>Loss</cell><cell>2</cell><cell></cell><cell cols="2">2 ( + 1)</cell><cell cols="2">2 ( + 1)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Complexity of space cost and online prediction.</figDesc><table><row><cell></cell><cell>Embedding size</cell><cell cols="2">#FLOP</cell><cell></cell><cell>#BOP</cell></row><row><cell>LightGCN</cell><cell>32( + )</cell><cell>2</cell><cell></cell><cell></cell><cell>-</cell></row><row><cell>BiGeaR</cell><cell>( + ) ( + 1) (32 + )</cell><cell>4</cell><cell>( + 1)</cell><cell>2</cell><cell>( + 1)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>To guarantee the fair comparison, we directly use five widely evaluated datasets (including the training/test splits) from: MovieLens 2 [9, 10, 24, 50], Gowalla 3 [21, 50, 56, 57], Pinterest 4 [15, 50], Yelp2018 5 [21, 56, 57], Amazon-Book 6 [21, 56, 57]. Dataset statistics and descriptions are reported in Table 3 and Appendix C.</figDesc><table><row><cell>4.1 Experiment Setup</cell></row><row><cell>Datasets.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The statistics of datasets.</figDesc><table><row><cell></cell><cell cols="5">MovieLens Gowalla Pinterest Yelp2018 Amazon-Book</cell></row><row><cell>#Users</cell><cell>6,040</cell><cell>29,858</cell><cell>55,186</cell><cell>31,668</cell><cell>52,643</cell></row><row><cell>#Items</cell><cell>3,952</cell><cell>40,981</cell><cell>9,916</cell><cell>38,048</cell><cell>91,599</cell></row><row><cell cols="5">#Interactions 1,000,209 1,027,370 1,463,556 1,561,406</cell><cell>2,984,108</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>NeurCF 21.40 ? 1.51 37.91 ? 1.14 14.64 ? 1.75 23.17 ? 1.52 12.28 ? 1.88 13.41 ? 1.13 4.28 ? 0.71 7.24 ? 0.53 3.49 ? 0.75 6.71 ? 0.72 NGCF 24.69 ? 1.67 39.56 ? 1.26 16.22 ? 0.90 24.18 ? 1.23 14.67 ? 0.56 13.92 ? 0.44 5.89 ? 0.35 9.38 ? 0.52 3.65 ? 0.73 6.90 ? 0.65 DGCF 25.28 ? 0.39 45.98 ? 0.58 18.64 ? 0.30 25.20 ? 0.41 BiGeaR 25.57 ? 0.33 45.56 ? 0.31 18.36 ? 0.14 24.96 ? 0.17 15.57 ? 0.22 16.83 ? 0.46 6.47 ? 0.14 11.60 ? 0.18 4.68 ? 0.11 8.12 ? 0.12 ? 1.23 14.87 ? 0.76 8.14 ? 0.98 12.19 ? 0.86 7.88 ? 1.21 9.84 ? 0.90 2.91 ? 0.51 5.06 ? 0.67 2.41 ? 0.95 4.39 ? 1.16 HashNet 15.43 ? 1.73 24.78 ? 1.50 11.38 ? 1.25 16.50 ? 1.42 10.27 ? 1.48 11.64 ? 0.91 3.37 ? 0.78 7.31 ? 1.16 2.86 ? 1.51 4.75 ? 1.33 CIGAR 14.84 ? 1.44 24.63 ? 1.77 11.57 ? 1.01 16.77 ? 1.29 10.34 ? 0.97 11.87 ? 1.20 3.65 ? 0.90 7.87 ? 1.03 3.05 ? 1.32 4.98 ? 1.24 GumbelRec 16.62 ? 2.17 29.36 ? 2.53 12.26 ? 1.58 17.49 ? 1.08 10.53 ? 0.79 11.86 ? 0.86 3.85 ? 1.39 7.97 ? 1.59 2.69 ? 0.55 4.32 ? 0.47 HashGNN ? 14.21 ? 1.67 24.39 ? 1.87 11.63 ? 1.47 16.82 ? 1.35 10.15 ? 1.43 11.96 ? 1.58 3.77 ? 1.02 7.75 ? 1.39 3.09 ? 1.29 5.19 ? 1.03 HashGNN 19.87 ? 0.93 37.32 ? 0.81 13.45 ? 0.65 19.12 ? 0.68 12.38 ? 0.86 13.63 ? 0.75 4.86 ? 0.36 8.83 ? 0.27 3.34 ? 0.25 5.82 ? 0.24 BiGeaR 25.57 ? 0.33 45.56 ? 0.31 18.36 ? 0.14 24.96 ? 0.17 15.57 ? 0.22 16.83 ? 0.46 6.47 ? 0.14 11.60 ? 0.18 4.68 ? 0.11 8.12 ? 0.12</figDesc><table><row><cell>LightGCN</cell><cell>??? 26.28 ? 0.20</cell><cell>??? 46.04 ? 0.18</cell><cell>??? 19.02 ? 0.19</cell><cell cols="7">??? 15.52 ? 0.42 25.71 ? 0.25 15.33 ? 0.28 16.29 ? 0.24 ??? 16.51 ? 0.56 6.37 ? 0.55 11.08 ? 0.48 4.32 ? 0.34 7.73 ? 0.27 ??? ??? 6.79 ? 0.31 ??? 12.17 ? 0.27 ??? 4.84 ? 0.09 ??? 8.11 ? 0.11</cell></row><row><cell>Capability</cell><cell>97.30%</cell><cell>98.96%</cell><cell>96.53%</cell><cell>97.08%</cell><cell>100.32%</cell><cell>101.94%</cell><cell>95.29%</cell><cell>95.32%</cell><cell>96.69%</cell><cell>100.12%</cell></row><row><cell cols="2">LSH 11.38 Gain 28.69%</cell><cell>22.08%</cell><cell>36.51%</cell><cell>30.54%</cell><cell>25.77%</cell><cell>23.48%</cell><cell>33.13%</cell><cell>31.37%</cell><cell>40.12%</cell><cell>39.52%</cell></row><row><cell>-value</cell><cell>5.57e-7</cell><cell>2.64e-8</cell><cell>2.21e-7</cell><cell>7.69e-8</cell><cell>2.5e-5</cell><cell>3.51e-5</cell><cell>3.27e-6</cell><cell>5.30e-8</cell><cell>3.49e-6</cell><cell>7.14e-8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Resource consumption on MovieLens dataset.</figDesc><table><row><cell></cell><cell>LightGCN</cell><cell cols="2">HashGNN ?</cell><cell>HashGNN</cell><cell>BiGeaR</cell></row><row><cell>/#epcoch</cell><cell>4.91s</cell><cell>186.23s</cell><cell></cell><cell>204.53s</cell><cell>(5.16+6.22)s</cell></row><row><cell>/#query</cell><cell>32.54ms</cell><cell>2.45ms</cell><cell></cell><cell>31.76ms</cell><cell>3.94ms</cell></row><row><cell></cell><cell>9.79MB</cell><cell cols="2">0.34MB</cell><cell>9.78MB</cell><cell>1.08MB</cell></row><row><cell>Recall@20</cell><cell>26.28%</cell><cell>14.21%</cell><cell></cell><cell>19.87%</cell><cell>25.57%</cell></row><row><cell cols="6">4.4 Study of Layer-wise Quantization (RQ3.A)</cell></row><row><cell cols="6">To verify the magnification of feature uniqueness in layer-wise</cell></row><row><cell cols="6">quantization, we modify BiGeaR and propose two variants, denoted</cell></row><row><cell>as BiGeaR / ure</cell><cell cols="2">and BiGeaR /</cell><cell cols="3">. We report the results in Fig-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Learning inference distillation.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>It could be single-precision or double-precision; we use float32 as the default for explanation and conducting experiments throughout this paper.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://grouplens.org/datasets/movielens/1m/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/gusye1234/LightGCN-PyTorch/tree/master/data/gowalla</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://sites.google.com/site/xueatalphabeta/dataset-1/pinterest_iccv</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://github.com/gusye1234/LightGCN-PyTorch/tree/master/data/yelp2018</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://github.com/gusye1234/LightGCN-PyTorch/tree/master/data/amazon-book</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>https://www.lfd.uci.edu/~gohlke/pythonlibs/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The work described in this paper was partially supported by the <rs type="funder">National Key Research and Development Program of China</rs> (No. <rs type="grantNumber">2018AAA0100204</rs>), the <rs type="funder">Research Grants Council of the Hong Kong Special Administrative Region, China</rs> (<rs type="grantNumber">CUHK 2410021</rs>, <rs type="funder">Research Impact Fund</rs>, No. <rs type="grantNumber">R5034-18</rs>), and the <rs type="funder">CUHK</rs> <rs type="grantName">Direct Grant</rs> (<rs type="grantNumber">4055147</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_pPSRTQM">
					<idno type="grant-number">2018AAA0100204</idno>
				</org>
				<org type="funding" xml:id="_JKqwV2x">
					<idno type="grant-number">CUHK 2410021</idno>
				</org>
				<org type="funding" xml:id="_jjvRVES">
					<idno type="grant-number">R5034-18</idno>
				</org>
				<org type="funding" xml:id="_kCjwU47">
					<idno type="grant-number">4055147</idno>
					<orgName type="grant-name">Direct Grant</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>U, I</head><p>Collection of users and items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N ( )</head><p>Neighbors of node .</p><p>( )</p><p>Full-precision embedding of node at -th convolution.</p><p>( )</p><p>Binarized embedding of of node at -th quantization.</p><p>( )</p><p>-th embedding scaler of node . A and Q</p><p>Binarized embedding table of learned by BiGeaR.</p><p>-th weight in predicting matching score. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Notation Table</head><p>We list key notations in Table <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Pseudo-codes of BiGeaR</head><p>The pseudo-codes of BiGeaR are attached in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Datasets</head><p>? MovieLens <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b49">50]</ref> is a widely adopted benchmark for movie recommendation. Similar to the setting in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b49">50]</ref>, , = 1 if user has an explicit rating score towards item , otherwise , = 0. In this paper, we use the MovieLens-1M data split. ? Gowalla <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57]</ref> is the check-in dataset <ref type="bibr" target="#b35">[36]</ref> collected from Gowalla, where users share their locations by check-in. To guarantee the quality of the dataset, we extract users and items with no less than 10 interactions similar to <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57]</ref>. ? Pinterest <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b49">50]</ref> is an implicit feedback dataset for image recommendation <ref type="bibr" target="#b14">[15]</ref>. Users and images are modeled in a graph. </p><p>11 if with inference distillation then 12 (0) ? sign (0) , (0) ? sign (0) ,</p><p>21 Optimize BiGeaR with regularization;</p><p>Edges represent the pins over images initiated by users. In this dataset, each user has at least 20 edges. ? Yelp2018 <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57]</ref> is collected from Yelp Challenge 2018 Edition, where local businesses such as restaurants are treated as items. We retain users and items with over 10 interactions similar to <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57</ref>]. ? Amazon-Book <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57]</ref> is organized from the book collection of Amazon-review for product recommendation <ref type="bibr" target="#b19">[20]</ref>. Similarly to <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57]</ref>, we use the 10-core setting to graph nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Competing Methods</head><p>? LSH <ref type="bibr" target="#b15">[16]</ref> is a representative hashing method to approximate the similarity search for massive high-dimensional data. We follow the adaptation in <ref type="bibr" target="#b49">[50]</ref> to it for Top-K recommendation. ? HashNet <ref type="bibr" target="#b6">[7]</ref> is a state-of-the-art deep hashing method that is originally proposed for multimedia retrieval tasks. We use the same adaptation strategy in <ref type="bibr" target="#b49">[50]</ref> to it for recommendation.</p><p>? CIGAR <ref type="bibr" target="#b27">[28]</ref> is a hashing-based method for fast item candidate generation, followed by complex full-precision re-ranking algorithms. We use its quantization part for fair comparison. ? GumbelRec is a variant of our model with the implementation of Gumbel-softmax for categorical variable quantization <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b67">68]</ref>. GumbelRec utilizes the Gumbel-softmax trick to replace sign(?) function for embedding binarization. ? HashGNN <ref type="bibr" target="#b49">[50]</ref> is the state-of-the-art end-to-end 1-bit quantization recommender system. HashGNN ? denotes its vanilla hard encoding version; and HashGNN is the relaxed version of replacing several quantized digits with the full-precision ones. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Hyper-parameter Settings</head><p>We report all hyper-parameter settings in Table <ref type="table">2</ref>. 1 ? 10 -3 5 ? 10 -4 5 ? 10 -4 5 ? 10 -4 1 ? 10 -4 5 ? 10 -5 1 ? 10 -4 1 ? 10 -4 1 ? 10 -6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Additional Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Top-K Recommendation Curve</head><p>We curve the Top-K recommendation by varying K from 20 to 100 and compare BiGeaR with several selected models. As shown in Figure <ref type="figure">1</ref>, BiGeaR consistently presents the performance superiority over HashGNN, and shows the competitive recommendation accuracy with DGCF and LightGCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Implementation of Embedding Scaler ( )</head><p>We set the embedding scaler to learnable (denoted by LB) and show the results in Table <ref type="table">3</ref>. We observe that, the design of learnable embedding scaler does not achieve the expected performance. This is probably because there is no direct mathematical constraint to it and thus the parameter search space is too large to find the optimum by stochastic optimization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MovieLens</head><p>Gowalla Pinterest Yelp2018 Amazon-book R@20 N@20 R@20 N@20 R@20 N@20 R@20 N@20 R@20 N@20 F.3 Implementation of .</p><p>We try the following three additional implementation of and report the results in Tables 4.</p><p>(1) = 1 +1 equally contributes for all embedding segments.</p><p>(2) = 1 +1-is positively correlated to the value, so as to highlight higher-order structures of the interaction graph.</p><p>(3) = 2 -( +1-) is positively correlated to with exponentiation. The experimental results show that implementation (2) performs fairly well compared to the others, demonstrating the importance of highlighting higher-order graph information. This corroborates the design of our implementation in BiGeaR, i.e., ? , which however is simpler and effective with better recommendation accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MovieLens</head><p>Gowalla Pinterest Yelp2018 Amazon-Book R@20 N@20 R@20 N@20 R@20 N@20 R@20 N@20 R@20 N@20 (1) 22.75 41. <ref type="bibr" target="#b12">13</ref>  F.4 Implementation of .</p><p>We further evaluate different : <ref type="bibr" target="#b0">(1)</ref> =is negatively correlated to the ranking position .</p><p>= 1 is inversely proportional to position .</p><p>= 2 -is exponential to the value of -. We observe from Table <ref type="table">5</ref> that the implementation (3) works slightly worse than Equation <ref type="bibr" target="#b11">(12)</ref> but generally better than the other two methods. This show that the exponential modeling is more effective to depict the importance contribution of items for approximating the tailed item popularity <ref type="bibr" target="#b44">[45]</ref>. Moreover, Equation ( <ref type="formula">12</ref>) introduces hyper-parameters to provide the flexibility of adjusting the function properties for different datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MovieLens</head><p>Gowalla Pinterest Yelp2018 Amazon-Book R@20 N@20 R@20 N@20 R@20 N@20 R@20 N@20 R@20 N@20 (1) 24.97 44. <ref type="bibr" target="#b32">33</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Large scale distributed neural network training through online distillation</title>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Ormandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Binary Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ga?tan</forename><surname>Bahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9492" to="9501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scalable methods for 8-bit training of neural networks</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Banner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>L?onard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno>arXiv</idno>
		<title level="m">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02263</idno>
		<title level="m">Graph convolutional matrix completion</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Newbold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bracewell</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">N</forename><surname>Bracewell</surname></persName>
		</author>
		<title level="m">The Fourier transform and its applications</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">31999</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hashnet: Deep learning to hash by continuation</title>
		<author>
			<persName><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5608" to="5617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attentive collaborative filtering: Multimedia recommendation with item-and component-level attention</title>
		<author>
			<persName><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="335" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingxue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengchen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqiao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianye</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<title level="m">Modeling Scale-free Graphs with Hyperbolic Geometry for Knowledge-aware Recommendation</title>
		<imprint>
			<publisher>WSDM</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Attentive Knowledge-aware Graph Convolutional Networks with Collaborative Guidance for Personalized Recommendation</title>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangchen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>ICDE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep neural networks for youtube recommendations</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recsys</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Sajad</forename><surname>Darabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mouloud</forename><surname>Belbahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vahid</forename><surname>Partovi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nia</forename></persName>
		</author>
		<idno>arXiv</idno>
		<title level="m">Bnn+: Improved binary network training</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep hashing for compact binary codes learning</title>
		<author>
			<persName><forename type="first">Erin</forename><surname>Venice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Moulin</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2475" to="2483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<ptr target="https://en.wikipedia.org/wiki/Heaviside_step_function" />
		<title level="m">Step function</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning image and user features for recommendation in social networks</title>
		<author>
			<persName><forename type="first">Xue</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwen</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Similarity search in high dimensions via hashing</title>
		<author>
			<persName><forename type="first">Aristides</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="518" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Ruihao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenghu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazhen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<title level="m">Differentiable soft quantization: Bridging full-precision and low-bit neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4852" to="4861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Some optimal inapproximability results</title>
		<author>
			<persName><forename type="first">Johan</forename><surname>H?stad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="798" to="859" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling the visual evolution of fashion trends with one-class collaborative filtering</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lightgcn: Simplifying and powering graph convolution network for recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Nais: Neural attentive item similarity model for recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhankui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2354" to="2366" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast matrix factorization for recommendation with implicit feedback</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In 5th ICLR</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning to embed categorical features without embedding tables for recommendation</title>
		<author>
			<persName><forename type="first">Wang-Cheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">Zhiyuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiansheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>SIGKDD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Candidate generation with binary codes for large-scale top-n recommendation</title>
		<author>
			<persName><forename type="first">Wang-Cheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1523" to="1532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In 5th ICLR</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Understanding black-box predictions via influence functions</title>
		<author>
			<persName><forename type="first">Pang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koh</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1885" to="1894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Two-Stage Hashing for Fast Document Retrieval</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="495" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Modeling user exposure in recommendation</title>
		<author>
			<persName><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Mcinerney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="951" to="961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Towards accurate binary convolutional neural network</title>
		<author>
			<persName><forename type="first">Xiaofan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Pan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Chunlei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenrui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Guo</surname></persName>
		</author>
		<title level="m">RBCN: Rectified binary convolutional networks for enhancing the performance of 1-bit DCNNs</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Collaborative hashing</title>
		<author>
			<persName><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2139" to="2146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Chris J Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><surname>Teh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In 5th ICLR</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The long tail of recommender systems and how to leverage it</title>
		<author>
			<persName><forename type="first">Yoon-Joo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Tuzhilin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Amazon product statistics</title>
		<ptr target="https://www.retailtouchpoints.com/resources/how-many-products-does-amazon-carry" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Forward and backward information retention for accurate binary neural networks</title>
		<author>
			<persName><forename type="first">Ruihao</forename><surname>Haotong Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianglong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziran</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingkuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2250" to="2259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Xnor-net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="525" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Improving pairwise learning for item recommendation from implicit feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="273" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<idno>arXiv</idno>
		<title level="m">Bayesian personalized ranking from implicit feedback</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Semi-supervised Multi-label Learning for Graph-structured Data</title>
		<author>
			<persName><forename type="first">Zixing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqiao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1723" to="1733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Graph-based semisupervised learning: A comprehensive review</title>
		<author>
			<persName><forename type="first">Zixing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangli</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zenglin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>TNNLS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Degreequant: Quantization-aware training for graph neural networks</title>
		<author>
			<persName><forename type="first">Javier</forename><surname>Shyam A Tailor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">D</forename><surname>Fernandez-Marques</surname></persName>
		</author>
		<author>
			<persName><surname>Lane</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>9th ICLR</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Learning to hash with GNNs for recommender systems</title>
		<author>
			<persName><forename type="first">Qiaoyu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ninghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<idno>WWW. 1988- 1998</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning compact ranking models with high performance for recommender system</title>
		<author>
			<persName><forename type="first">Jiaxi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2289" to="2298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<ptr target="https://backlinko.com/amazon-prime-users" />
		<title level="m">Amazon user statistics</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Graph attention networks. ICLR</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Bi-gcn: Binary graph convolutional network</title>
		<author>
			<persName><forename type="first">Junfu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanfang</forename><surname>Guo</surname></persName>
		</author>
		<idno>CVPR. 1561-1570</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A survey on learning to hash</title>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><surname>Heng Tao Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="769" to="790" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Neural graph collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Disentangled graph collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongye</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1001" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TNNLS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7308" to="7316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Discrete-time Temporal Network Embedding via Implicit Hierarchical Learning in Hyperbolic Space</title>
		<author>
			<persName><forename type="first">Menglin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Kalander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="1975">2021. 1975-1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">HRCF: Enhancing collaborative filtering via hyperbolic geometric regularization</title>
		<author>
			<persName><forename type="first">Menglin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WebConf</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2462" to="2471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Discrete collaborative filtering</title>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fumin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="325" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Discrete personalized ranking for fast collaborative filtering from implicit feedback</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guowu</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Doc2hash: Learning discrete latent variables for documents retrieval</title>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2235" to="2240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Discrete Wasserstein Autoencoders for Document Retrieval</title>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<editor>ICASSP. IEEE</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8159" to="8163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Deep hashing network for efficient similarity retrieval</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
