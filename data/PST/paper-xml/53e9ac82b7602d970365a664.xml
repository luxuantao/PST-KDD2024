<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">X-Stream: Edge-centric Graph Processing using Streaming Partitions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Amitabha</forename><surname>Roy</surname></persName>
							<email>amitabha.roy@epfl.ch</email>
						</author>
						<author>
							<persName><forename type="first">Ivo</forename><surname>Mihailovic</surname></persName>
							<email>ivo.mihailovic@epfl.ch</email>
						</author>
						<author>
							<persName><forename type="first">Willy</forename><surname>Zwaenepoel</surname></persName>
							<email>willy.zwaenepoel@epfl.ch</email>
						</author>
						<title level="a" type="main">X-Stream: Edge-centric Graph Processing using Streaming Partitions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">11A3F45DF522BDBEE7DCFB0B69B2F116</idno>
					<idno type="DOI">10.1145/2517349.2522740</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>X-Stream is a system for processing both in-memory and out-of-core graphs on a single shared-memory machine. While retaining the scatter-gather programming model with state stored in the vertices, X-Stream is novel in (i) using an edge-centric rather than a vertex-centric implementation of this model, and (ii) streaming completely unordered edge lists rather than performing random access. This design is motivated by the fact that sequential bandwidth for all storage media (main memory, SSD, and magnetic disk) is substantially larger than random access bandwidth.</p><p>We demonstrate that a large number of graph algorithms can be expressed using the edge-centric scatter-gather model. The resulting implementations scale well in terms of number of cores, in terms of number of I/O devices, and across different storage media. X-Stream competes favorably with existing systems for graph processing. Besides sequential access, we identify as one of the main contributors to better performance the fact that X-Stream does not need to sort edge lists during preprocessing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Analytics over large graphs is an application that is beginning to attract significant attention in the research community. Part of the reason for this upsurge of interest is the great variety of information that is naturally encoded as graphs. Graph processing poses an interest-Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the Owner/Author. Copyright is held by the owner/author(s). SOSP <ref type="bibr">'13</ref> Figure <ref type="figure">1</ref>: Vertex-centric Scatter-Gather ing systems challenge: the lack of access locality when traversing edges makes obtaining good performance difficult <ref type="bibr" target="#b28">[39]</ref>.</p><p>This paper presents X-Stream, a system for scale-up graph processing on a single shared-memory machine.</p><p>Similar to systems such as Pregel <ref type="bibr" target="#b29">[40]</ref> and Powergraph <ref type="bibr" target="#b20">[31]</ref>, X-Stream maintains state in the vertices, and exposes a scatter-gather programming model. The computation is structured as a loop, each iteration of which consists of a scatter phase followed by a gather phase. Figure <ref type="figure">1</ref> illustrates the common vertex-centric implementation of the scatter-gather programming model. Both the scatter and the gather phase iterate over all vertices. The user provides a scatter function to propagate vertex state to neighbors and a gather function to accumulate updates from neighbors to recompute the vertex state. This simple programming model is sufficient for a variety of graph algorithms <ref type="bibr" target="#b29">[40,</ref><ref type="bibr" target="#b20">31,</ref><ref type="bibr" target="#b43">54]</ref> ranging from computing shortest paths to ranking web pages in a search engine, and hence is a popular interface for graph processing systems.</p><p>The accepted (and intuitive) approach to scale-up graph processing, for both in-memory <ref type="bibr" target="#b22">[33]</ref> and out-ofcore <ref type="bibr" target="#b32">[43]</ref> graphs, is to sort the edges of the graph by originating vertex and build an index over the sorted edge list. The execution then involves random access through the index to locate edges connected to a vertex. Implicit in this design is a tradeoff between sequential and random access, favoring a small number of random accesses through an index in order to locate edges connected to an edge_scatter(edge e) send update over e update_gather(update u) apply update u to u.destination while not done for all edges e edge_scatter(e) for all updates u update_gather(u)</p><p>Figure <ref type="figure">2</ref>: Edge-centric Scatter-Gather active vertex, over streaming a large number of (potentially) unrelated edges and picking up those connected to active vertices. It is this tradeoff that is revisited in this paper.</p><p>Random access to any storage medium delivers less bandwidth than sequential access. For instance, on our testbed (16 core/64 GB 1U server, 7200 RPM 3TB magnetic disk and 200 GB PCIe SSD), bandwidth for sequential reads compared to random reads is 500 times higher for disks and 30 times higher for SSDs. Even for main memory, as a result of hardware prefetching, sequential bandwidth outstrips random access bandwidth by a factor of 4.6 for a single core and by a factor of 1.8 for 16 cores. See §5.1 for more details.</p><p>We demonstrate in this paper that the larger bandwidth of sequential access can be exploited to build a graph processing system based purely on the principle of streaming data from storage. We show that this design leads to an efficient graph processing system for both in-memory graphs and out-of-core graphs that, in a surprising number of cases, equals or outperforms systems built around random access through an index.</p><p>To achieve this level of performance, X-Stream introduces an edge-centric approach to scatter-gather processing, shown in Figure <ref type="figure">2</ref>: the scatter and gather phase iterate over edges and updates on edges rather than over vertices. This edge-centric approach altogether avoids random access into the set of edges, instead streaming them from storage. For graphs with the common property that the edge set is much larger than the vertex set, access to edges and updates dominates the processing cost, and therefore streaming the edges is often advantageous compared to accessing them randomly. Doing so comes, however, at the cost of random access into the set of vertices. We mitigate this cost using streaming partitions: we partition the set of vertices such that each partition fits in high-speed memory (the CPU cache for in-memory graphs and main memory for out-of-core graphs). Furthermore, we partition the set of edges such that edges appear in the same partition as their source vertex. We then process the graph one partition at a time, first reading in its vertex set and then streaming its edge set from storage. A positive consequence of this approach is that we do not need to sort the edge list, thereby not incurring the pre-processing delays in other systems <ref type="bibr" target="#b22">[33,</ref><ref type="bibr" target="#b26">37]</ref>.</p><p>Graphchi <ref type="bibr" target="#b26">[37]</ref> was the first system to explore the idea of avoiding random access to edges. Graphchi uses a novel out-of-core data structure that consists of partitions of the graph called 'shards'. Unlike streaming partitions, shards have to be pre-sorted by source vertex, a significant pre-processing cost, especially if the graph is not used repeatedly. Graphchi also continues to use the vertex-centric implementation in Figure <ref type="figure">1</ref>. This requires the entire shard -vertices and all of their incoming and outgoing edges -to be present in memory at the same time, leading to a larger number of shards than streaming partitions, the latter requiring only the vertex state to be in memory. Streaming partitions therefore take better advantage of sequential streaming bandwidth. Shards also require a re-sort of the edges by destination vertex in order to direct updates to vertices in the gather step.</p><p>This paper makes the following contributions through the design, implementation and evaluation of X-Stream:</p><p>• We introduce edge-centric processing as a new model for graph computation and show that it can be applied to a variety of graph algorithms.</p><p>• We show how the edge-centric processing model can be implemented using streaming partitions both for in-memory and out-of-core graphs, merely by using different partition sizes for different media.</p><p>• We demonstrate that X-Stream scales well in terms of number of cores, I/O devices and across different storage media. For instance, X-Stream identifies weakly connected components in graphs with up to 512 million edges in memory within 28 seconds, with up to 4 billion edges from SSD in 33 minutes, and with up to 64 billion edges from magnetic disk in under 26 hours.</p><p>• We compare X-Stream to alternative graph processing systems, and show that it equals or outperforms vertex-centric and index-based systems on a number of graph algorithms for both in-memory and out-of-core graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The X-Stream Processing Model</head><p>X-Stream presents to the user a graph computation model in which the mutable state of the computation is stored in the vertices, more precisely in the data field of each vertex. The input to X-Stream is an unordered set of directed edges. Undirected graphs are represented using a pair of directed edges, one in each direction.</p><p>X-Stream provides two principal API methods for expressing graph computations. Edge-centric scatter takes as input an edge, and computes, based on the data field of its source vertex, whether an update value needs to be sent to its destination vertex, and, if so, the value of that update. Edge-centric gather takes as input an update, and uses its value to recompute the data field of its destination vertex.</p><p>The overall computation is structured as a loop, terminating when some application-specific termination criterion is met. Each loop iteration consists of a scatter phase followed by a gather phase. The scatter phase iterates over all edges and applies the scatter method to each edge. The gather phase iterates over all updates produced in the scatter phase and applies the gather method to each update. Hence, X-Stream's edge-centric scatter gather is synchronous and guarantees that all updates from a previous scatter phase are seen only after the scatter is completed and before the next scatter phase is begun. In this sense it is similar to distributed graph processing systems such as Pregel <ref type="bibr" target="#b29">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Streams</head><p>X-Stream uses streaming to implement the graph computation model described above. An input stream has one method, namely read the next item from the stream. An input stream is read in its entirety, one item at a time. An output stream also has one method, namely append an item to the stream.</p><p>The scatter phase of the computation takes the edges as the input stream, and produces an output stream of updates. In each iteration it reads an edge, reads the data field of its source vertex, and, if needed, appends an update to the output stream. The gather phase takes the updates produced in the scatter phase as its input stream. It does not produce any output stream. For each update in the input stream, it updates the data value of its destination vertex.</p><p>The idea of using streams for graph computation applies both to in-memory and out-of-core graphs. To unify the presentation, we use the following terminology. We refer to caches in the case of in-memory graphs and to main memory in the case of out-of-core graphs as Fast Storage. We refer to main memory in the case of inmemory graphs and to SSD or disks in the case of outof-core graphs as Slow Storage.</p><p>Figure <ref type="figure">3</ref> shows how memory is accessed in the edge streaming model. The appeal of the streaming approach </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Streaming Partitions</head><p>A streaming partition consists of a vertex set, an edge list, and an update list. The vertex set of a streaming partition is a subset of the vertex set of the graph. The vertex sets of different streaming partitions are mutually disjoint, and their union equals the vertex set of the entire graph. The edge list of a streaming partition consists of all edges whose source vertex is in the partition's vertex set. The update list of a streaming partition consists of all updates whose destination vertex is in the partition's vertex set.</p><p>The number of streaming partitions stays fixed throughout the computation. During initialization, the vertex set of the entire graph is partitioned into vertex sets for the different partitions, and the edge list of each partition is computed. These vertex sets and edge lists also remain fixed during the entire computation. The update list of a partition, however, varies over time: it is recomputed before every gather phase, as described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Scatter-Gather with Partitions</head><p>With streaming partitions, the scatter phase iterates over all streaming partitions, rather than over all edges, as described before. Similarly, the gather phase also iterates over all streaming partitions, rather than over all  For each streaming partition, the scatter phase reads its vertex set, streams in its edge list, and produces an output stream of updates. This output stream is appended to a list Uout. These updates need to be re-arranged such that each update appears in the update list of the streaming partition containing its destination vertex. We call this the shuffle phase. The shuffle takes as its input stream the updates produced in the scatter phase, and moves each update to the update list Uin(p), where p is the streaming partition containing the destination vertex of the update. After the shuffle phase is completed, the gather phase can start. For each streaming partition, we read its vertex set, stream in its update list, and compute new values for the data fields of the vertices as we read updates from the update list.</p><p>Streaming partitions are a natural unit of parallelism for both the scatter and the gather phase. We will show how to take advantage of this parallelism in §4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Size and Number of Partitions</head><p>Choosing the correct number of streaming partitions is critical to performance. On the one hand, in order to produce fast random access to the vertices, all vertices of a streaming partition must fit in Fast Storage. On the other hand, in order to maximize the sequential nature of access to Slow Storage to load the edge lists and the update lists of a streaming partition, their number must be kept as small as possible. We restrict the vertex sets of streaming partitions to be of equal size. As a result, we choose the number of the streaming partitions such that, allowing for buffers and other auxiliary data structures, the vertex set of each streaming partition fills up Fast Storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">API Limitations and Extensions</head><p>Unlike vertex-centric graph processing APIs, there are no means to iterate over the edges or updates belonging to a vertex in X-Stream's edge-centric API. However, in addition to allowing the user to specify edge scatter and gather functions, X-Stream also supports vertex iteration, which simply iterates over all vertices in the graph, applying a user-specified function on each vertex. This is useful for initialization and for various aggregation operations.</p><p>X-Stream also supports interfaces other than edgecentric scatter-gather. For example, X-Stream supports the semi-streaming model for graphs <ref type="bibr" target="#b15">[26]</ref> or graph algorithms that are built on top of the W-Stream model <ref type="bibr" target="#b3">[14]</ref>. Although these models allow a richer set of graph algorithms to be expressed, we focus on the more familiar scatter-gather mode of operation in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Out-of-core Streaming Engine</head><p>The input to the out-of-core graph processing engine is a file containing the unordered edge list of the graph. In addition, we store three disk files for each streaming partition: a file each for the vertices, edges and updates.</p><p>As we saw in Section 2, with the edge-centric scattergather model, sequential access is easy to achieve for the scatter and gather phases. The hard part is to achieve sequential access for the shuffle phase. To do so for out-of-core graphs, we slightly modify the computation structure suggested in Figure <ref type="figure" target="#fig_0">4</ref>. Instead of a strict sequence of scatter, shuffle and gather phases, we fold the shuffle phase into the scatter phase. In particular, we run the scatter phase, appending updates to an in-memory buffer. Whenever that buffer becomes full, we run an in-memory shuffle, which partitions the list of updates in the in-memory buffer into (in-memory) lists of updates for vertices in the different partitions, and then appends those lists to the disk files for the updates of each partition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">In-memory Data Structures</head><p>Besides the vertex array used to store the vertices of a streaming partition, we need in-memory data structures to hold input from disk (edges during the scatter phase and updates during the gather phase), the input and the output of the in-memory shuffle phase, and output to disk (updates after each in-memory shuffle phase). In order to avoid the overhead of dynamic memory allocation, we designed a statically sized and statically allocated data structure, the stream buffer, to store these variable-sized data items. A stream buffer consists of a  (large) array of bytes called the chunk array, and an index array with K entries for K streaming partitions (see Figure <ref type="figure">5</ref>). The i-th entry in the index array describes the chunk of the chunk array with data relating to the i-th partition.</p><p>Using two streaming buffers, the in-memory shuffle becomes straightforward. One streaming buffer is used to store the updates resulting from the scatter phase. A second streaming buffer is used to store the result of the in-memory shuffle. We make one pass over the input buffer counting the updates destined for each partition. We then fill in the index array of the second streaming buffer. Finally, we copy the updates from the first buffer to the appropriate location in the chunk array of the second buffer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Operation</head><p>The disk engine begins by partitioning the input edge list into different streaming partitions. Interestingly, this can be done efficiently by using the in-memory shuffle. The successive parts of the input edge list are read in from disk to a streaming buffer, shuffled into another streaming buffer, and then written to the disk files containing the edge lists of the streaming partitions.</p><p>After this pre-processing step, the graph processing engine then enters the main processing loop, depicted in Figure <ref type="figure" target="#fig_1">6</ref>.</p><p>Finally, we implemented a couple of optimizations. First, if the entire vertex set fits into memory, the vertex array need not be written out to disk at the end of the gather phase. Second, if all updates for the entire scatter phase fit into a streaming buffer, then the updates are not written back to disk after the in-memory shuffle. Instead, the gather phase simply reuses the in-memory output of the shuffle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Disk I/O</head><p>X-Stream performs asynchronous direct I/O to and from the stream buffer, bypassing the operating system's page cache. We use an additional 4K page per streaming partition to keep I/O aligned, regardless of the starting point of a chunk in a streaming buffer.</p><p>We actively prefetch from a stream, exploiting the sequentiality of access. As soon as a read into one input stream buffer is completed, we start the next read into a second input stream buffer. Similarly, the writes to disk of the chunks in one output buffer are overlapped with computing the updates of the scatter phase into another output buffer. This requires allocating an extra stream buffer for input and an extra one for output. We found this prefetch distance of one, both on input and output, sufficient to keep the disks 100% busy in our experiments. A deeper prefetch can effectively be achieved with a larger chunk array, if necessary. X-Stream transparently exploits RAID architectures. The sequential writes stripe the files across disks, and the sequential reads of these striped files achieve a multiplication of bandwidth compared to single disks. We can also exploit parallelism between the input and output files, which can be placed on different disks. X-Stream does asynchronous I/O using dedicated I/O threads and spawns one thread for each disk. One can therefore put the edges and updates on different disks, doing I/O to them in parallel.</p><p>X-Stream's I/O design is also well suited for use with SSDs. All X-Stream writes are sequential and therefore avoid the problem of write amplification <ref type="bibr" target="#b23">[34]</ref>, in a manner similar to log-structured filesystems <ref type="bibr" target="#b41">[52]</ref> and memory allocators built specifically for flash devices <ref type="bibr" target="#b5">[16]</ref>. Many modern flash translation layers in fact implement a log-structured filesystem in firmware. X-Stream's sequential writes can be a considered a best case for such firmware. In addition, we always truncate files when the streams they contain are destroyed. On most operating systems, truncation automatically translates into a TRIM command sent to the SSD, freeing up blocks and thereby reducing pressure on the SSD garbage collector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Number of Partitions</head><p>Given a fixed amount of memory, the need to size stream buffers properly creates additional requirements on the number of streaming partitions beyond simply the need to fit the vertex set of the streaming partition in memory. We need to issue large enough units of I/O to approach streaming bandwidth to disk. Assuming a uniform distribution of updates across streaming partitions, and assuming we need to issue I/O request of S bytes to achieve maximum I/O bandwidth, we need to size the chunk array to at least S * K bytes for K streaming partitions. Our design requires two stream buffers each for the input and output streams in order to support prefetching. In addition we need a stream buffer for shuffling: a total of 5. If we assume that N is total space taken by the vertices and M is the total amount of main memory available then our requirements translate to: N K + 5SK ≤ M. Viable solutions to this inequality exist even for very large graphs. The left hand side reaches a minimum at</p><formula xml:id="formula_0">K = N</formula><p>5S at which point the minimum amount of memory needed is 2 √ 5NS. For an I/O unit of S = 16MB (justified in §5) the minimum amount of main memory required for a graph with total vertex data size as large as N = 1TB is therefore only M = 17GB with under K = 120 streaming partitions. This ignores the overhead of the index data which would have come to an additional 5KB in our design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">In-memory Streaming Engine</head><p>The in-memory engine is designed for processing graphs whose vertices, edges and updates fit in memory. Our main concern in designing the in-memory streaming engine is parallelism. We need all available cores in a system to reach peak streaming bandwidth to memory. In addition, parallelism was important in order to use all available computational resources (such as floating point units). We therefore discuss the important building blocks for parallelism in the in-memory streaming engine. A second concern was that the in-memory engine must deal with a larger number of partitions than the out-of-core streaming engine. This necessitates the use of a multi-stage shuffler, described in §4.2.</p><p>The in-memory engine considers the CPU caches when choosing the number of streaming partitions and aims to fit the vertex data corresponding to each partition in the CPU cache. Unlike disks, we do not have direct control on block allocation in CPU caches. The in-memory streaming engine must also consider additional data that must be brought in without displacing the vertices. Observing that an edge and an update must refer to a vertex The engine needs exactly three stream buffers, one to hold the edges of the graph, one to hold generated updates and one more to be used during shuffling. We start by loading the edges into the input stream buffer and shuffling them into a chunk of edges for each streaming partition. We then process streaming partitions one by one generating updates from the streaming partitions in the scatter phase. All the generated updates are appended into a single output stream buffer. The output stream buffer is then shuffled into chunks of updates per streaming partition, which are then absorbed in the gather phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Parallel Scatter-Gather</head><p>Our approach to parallelizing the scatter and gather phases rests on the observation that the streaming operation can be done independently for different streaming partitions. Supporting parallel scatter-gather requires awareness of shared caches when calculating the number of streaming partitions. We assume underlying cores receive equal shares of a shared cache.</p><p>The threads executing different streaming partitions are still required to append their updates to the same chunk array. Each thread first writes to a private buffer (of size 8K), which is flushed to the shared output chunk array, by first atomically reserving space at the end and then appending the contents of the private buffer.</p><p>Executing streaming partitions in parallel can lead to significant workload imbalance as the partitions can have different numbers of edges assigned to them. We therefore implemented work stealing in X-Stream, allowing threads to steal streaming partitions from each other. This lets us avoid the work imbalance problem that requires specialized solutions for in-memory <ref type="bibr" target="#b33">[44]</ref> or scale out <ref type="bibr" target="#b20">[31]</ref> graph processing systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Parallel Multistage Shuffler</head><p>The in-memory engine must deal with a larger number of partitions than the out-of-core engine as CPU caches are small in size with respect to main memory and current architectural trends indicate that they are unlikely to grow further <ref type="bibr" target="#b16">[27]</ref>. Repeating the analysis in § 3.4, for a graph with 1TB of vertex data (on a system with more than 1TB RAM) and a 1MB CPU cache we need at least 1M partitions to ensure that the randomly accessed vertex data for each partition fits in CPU cache (even excluding the rest of the vertex footprint). Shuffling into a large number of partitions leads to a significant challenge in maintaining our design goal of exploiting sequential access bandwidth to memory.</p><p>Sequential access from the CPU core provides higher bandwidth to memory for two reasons. The first is that microprocessors (such as the current generation x86 one used in our experiments) usually come with hardware prefetchers that can track multiple streams. Having the prefetchers track the input and output streams is beneficial in hiding the latency of access to memory (the primary source of higher bandwidth for sequential accesses). Increasing the number of partitions beyond a point means that we lose the benefit of hardware prefetchers. The second benefit of sequential accesses is due to maximum spatial locality as each cacheline is fully used before being evicted. This is only possible if we can fit a cacheline from the input stream and all output streams in the cache. With a larger number of partitions this is no longer the case (SRAM caches closest to the core typically fit only 512 to 1024 64-byte cachelines).</p><p>Inspired by solutions to similar problems in cacheconscious sorting <ref type="bibr" target="#b40">[51]</ref> and in systems such as Phoenix <ref type="bibr" target="#b34">[45]</ref> or tiled map-reduce <ref type="bibr" target="#b13">[24]</ref>, we implemented a multi-stage shuffler for the in-memory engine in X-Stream. We group partitions together into a tree hierarchy, with a branching factor (we term this the fanout) of F. This is done in X-Stream by enforcing that the number of streaming partitions for the in-memory engine is a power of two and also setting the fanout of the tree to a power of two. The tree is then implicitly maintained by using the most significant b bits of the partition ID to choose between between groupings at a tree level with 2 b nodes. We then do one shuffle step for each level in this tree. The input consists of a stream buffer with as many chunks as nodes at that level in the tree. Each input chunk is shuffled into F output chunks. Given a target of K partitions, the multi-stage shuffler can therefore shuffle the input into K chunks in⌈log F K⌉ steps down the tree. We use exactly two stream buffers in the shuffle process, alternating them between the input and output roles.</p><p>The fanout F can be set keeping in mind the constraints described above. For the experiments in this paper, we bounded it to the number of cachelines available in the CPU cache. In most cases, however, we are also within the limit of the number of tracked streams in the hardware prefetcher, which is a micro-architectural detail we did not specifically tune for.</p><p>We observe that a stream buffer typically carries many more objects than partitions. Our experiments ( §5) on graphs result in in-memory streams with over a billion objects but never require more than 1K partitions. This causes shuffling to be cheaper than sorting even with multiple stages, a point we return to in the evaluation.</p><p>In order to enable parallelism in the multi-stage shuffler, we required the ability to have threads work in parallel on stream buffers, without needing synchronization. Our solution is to assign X-Stream threads disjoint equally sized slices of the stream buffer. Each thread receives exactly one slice. Figure <ref type="figure" target="#fig_2">7</ref> shows how a stream buffer is sliced across threads. Each thread has an independent index array describing chunks in its slice of the stream buffer. A thread is only allowed to access its own slice during shuffling. We parallelize the shuffle step by assigning each thread to shuffle its own slice. Since the number of target partitions is the same across all the threads, they all end up with the final slice in the same output stream buffer, at which point they synchronize.</p><p>The chunk corresponding to a streaming partition is the union of the corresponding chunks from all the slices. A thread can therefore recover a chunk during the scatter and gather steps from the sliced stream buffer using sequential accesses plus at most P random accesses, where P is the number of threads. Usually the number of threads P is far smaller than the number of objects in the chunk, rendering the random access negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Layering over Disk Streaming</head><p>The in-memory engine is logically layered above the out-of-core engine. We do so by allowing the disk engine to independently choose its count of streaming partitions. For each iteration of the streaming loop in Figure <ref type="figure" target="#fig_1">6</ref>, the loaded input chunk is processed using the inmemory engine that then independently chooses a further in-memory partitioning for the current disk partition.</p><p>This allows us to ensure that we maximize usage of main memory bandwidth and computational resources with the out-of-core streaming engine. The slower disk continues to remain a bottleneck in all cases, even when using faster disks such as SSDs and using floating-point computation in some graph algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Environment</head><p>Our testbed is an AMD Opteron (6272, 2.1Ghz), dualsocket, 32-core system. The CPU uses a clustered core micro-architecture, with a pair of cores sharing the instruction cache, floating point units and L2 cache. The system is equipped with 64GB of main memory; two 200GB PCI Express SSDs arranged into a software RAID-0 configuration and two 3 TB SATA-II magnetic disks also arranged into a software RAID-0 configuration.</p><p>We first measured the streaming bandwidth available from main memory. We used a microbenchmark, in which each thread read from or wrote to a thread-private buffer of size 256 MB (well beyond the capacity of the L3 cache and TLBs). The results in Figure <ref type="figure">8</ref> show that for reads memory bandwidth saturates with 16 cores at approximately 25GB/s. Adding 16 more cores increases the available bandwidth by only 5%. We therefore use only 16 cores of the system, as memory bandwidth, the critical resource for graph processing, is already saturated with just 16 cores. We run a single thread on one core of each pair of clustered cores, leaving the other core of each pair unused. When determining the number of partitions for in-memory graphs, we assume that each core has exclusive access to its 2MB shared L2 cache We used the fio [1] tool to benchmark the streaming bandwidth of the RAID-0 SSD and disk pairs in our testbed. Our workload issues a single synchronous request at a time, and varies the size of the request. The results are shown in Figure <ref type="figure">9</ref>. An interesting aspect there is that SSD write bandwidth shows a temporary drop with a 512K request size. This is likely due to the flash translation layer not being able to keep up at this write request size. Bandwidth for both SSD and disk shows a sharp increase at 1M request sizes. The RAID stripe unit is 512K, and hence past 1M the request is striped across the SSDs or disks in the RAID-0 pair, explaining the increase in bandwidth. Figure <ref type="figure">9</ref> also shows that for reads both the SSDs and the disks are saturated with sequential requests of size 16MB. We therefore chose 16MB as our preferred I/O unit size for the out-of-core engine.</p><p>We also measured random access bandwidth by accessing entirely a randomly chosen cacheline from an inmemory buffer or by doing synchronous 4K transfers from an out-of-core file. Figure <ref type="figure">11</ref> shows that sequential access beats random access for every medium, with an increasing gap as we move to slower media. For main memory, it is necessary to use all available cores to saturate memory bandwidth. Random write performance is better than random read performance. For main memory, this is due to the write-coalescing buffers present in the AMD 6272 micro-architecture. For the out-of-core case, this is due to the write cache on the disk absorbing the writes, allowing the next write to be issued while the previous one is outstanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Algorithms and Graphs</head><p>We evaluate X-Stream using the following algorithms:</p><p>• Weakly Connected Components (WCC).</p><p>• Strongly Connected Components (SCC), using <ref type="bibr" target="#b36">[47]</ref>. Requires a directed graph.</p><p>• Single-Source Shortest Paths (SSSP).</p><p>• Minimum Cost Spanning Tree (MCST) using the GHS <ref type="bibr" target="#b19">[30]</ref> algorithm.</p><p>• Maximal Independent Set (MIS).</p><p>• Conductance <ref type="bibr" target="#b9">[20]</ref>.</p><p>• SpMV: Multiply the sparse adjacency matrix of a directed graph with a vector of values, one per vertex.</p><p>• Pagerank <ref type="bibr" target="#b31">[42]</ref> (5 iterations).</p><p>• Alternating Least Squares (ALS) <ref type="bibr" target="#b44">[55]</ref> (5 iterations). Requires a bipartite graph.</p><p>• Bayesian Belief Propagation (BP) <ref type="bibr" target="#b24">[35]</ref> (5 iterations).</p><p>We used both synthetic and real-world (Figure <ref type="figure">10</ref>) graph datasets to evaluate X-Stream. We generated synthetic undirected graphs using the RMAT generator <ref type="bibr" target="#b12">[23]</ref> and used them to study the scaling properties and evaluate configuration choices of X-Stream. RMAT graphs have a scale-free property that is a feature of many real-world graphs <ref type="bibr" target="#b6">[17]</ref>. We use RMAT graphs with an average degree of 16 (as recommended by the Graph500 benchmark [9]). We use the term scale n to refer to a synthetic graph with 2 n vertices and 2 n+4 edges. For SCC, we assigned a random edge direction to the synthetic RMAT and Friendster graphs.</p><p>All the graphs are provided to X-Stream as unordered lists of edges. For inputs without an edge weight, we added a random edge weight (a pseudo-random floating point number in the range [0 1)). The footprint of vertex data varied from a single byte in the case of MIS (a boolean variable) to almost 250 bytes in the case of ALS.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Applicability</head><p>We demonstrate that X-Stream's API can be used to express a large number of graph algorithms with good performance, in spite of the fact that the API does not allow direct access to the edges associated with a vertex. Figure <ref type="figure">12a</ref> shows how X-Stream performs on a variety of graph algorithms, real-world datasets and storage media. The yahoo-web graph did not fit onto our SSD, so it is absent from SSD results.</p><p>The execution time on SSD is roughly half of that on magnetic disk reflecting the fact that the SSD delivers twice the sequential bandwidth of the magnetic disk (Fig <ref type="figure">9</ref>), although at a considerably greater cost per byte.</p><p>X-Stream performs well on all algorithms and data sets, with the exception of traversal algorithms (WCC, SCC, MIS, MSCT and SSSP) for DIMACS and the Yahoo webgraph. DIMACS traversals take a long time relative to the size of the graph, and the Yahoo webgraph did not finish in a reasonable amount of time. We hypothesized that the problem lies in the structure of these graphs, and to confirm this, we implemented HyperANF <ref type="bibr" target="#b10">[21]</ref> in X-Stream to measure the neighborhood function of graphs.</p><p>The neighborhood function N G (t) is defined as the number of pairs of vertices reachable within t steps in the undirected version of the graph. Figure <ref type="figure">13</ref> shows the number of steps needed to converge to a constant value for the neighborhood function, which is equal to the diameter of graph. As Figure <ref type="figure">13</ref> shows, the Yahoo webgraph and DIMACS have a diameter much larger than other comparable graphs in our dataset. A high diameter results in graphs with an 'elongated' structure, causing X-Stream to execute a very large number of scattergather iterations, each of which requires streaming the entire edge list but doing little work.</p><p>In Figure <ref type="figure">12b</ref> we report some additional information on the execution of WCC on the various graphs, including 1) the number of scatter-gather steps, 2) the ratio of total execution time to streaming time, and 3) the percentage of edges that were streamed and along which no updates were sent. For DIMACS we see, as discussed above, the very large number of scatter-gather steps. The ratio of total execution time to streaming time is approximately 1 for out-of-core graphs, confirming that the execution time is governed by the bandwidth of secondary storage. For in-memory graphs, the ratio ranges roughly between 2 and 3, indicating that here too streaming takes up an important fraction of the execution time, but computation starts playing a role as well. The 'wasted' edges, i.e., edges that are streamed in but produce no updates, are a direct consequence of the tradeoff underlying X-Stream. As Figure <ref type="figure">12b</ref> shows, X-Stream does waste considerable sequential bandwidth for some algorithms. Exploring generic stream compression algorithms as well as those specific to graphs <ref type="bibr" target="#b0">[11]</ref>, or performing extra passes to eliminate those edges that are no longer needed are important avenues of exploration we are pursuing to reduce wastage in X-Stream.</p><p>We conclude that X-Stream is an efficient way to execute a variety of algorithms on real-world graphs, its only limitation being graphs whose structure requires a large number of iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Scalability</head><p>We study the scalability of X-Stream from two different angles. First, we look at the improvement in performance for a given graph size as more resources are added. Second, we show that as more storage is added, larger graphs can be handled. For these experiments, we select two traversal algorithms (BFS and WCC) and two sparse matrix multiplication algorithms (Pagerank and SpMW).</p><p>Figure <ref type="figure" target="#fig_0">14</ref> (with both axes in log scale) shows how X-Stream's performance scales with increasing thread  For all algorithms, performance improves linearly as more threads are added. X-Stream is able to take advantage of more memory bandwidth with increasing thread count (Figure <ref type="figure">8</ref>) and does not incur any synchronization overhead.</p><p>Figure <ref type="figure">15</ref> shows how X-Stream's performance scales with an increasing number of I/O devices. We compare three different configurations: with one disk/SSD, with separate disks/SSDs for reading and writing, and with two disks/SSDs arranged in RAID-0 fashion (our baseline configuration). In the case of magnetic disk, we use an RMAT scale 30 graph, and in the case of the SSD, we use an RMAT scale 27 graph. Putting the edges and updates on different disks/SSDs reduces runtime by up to 30%, compared to using one disk/SSD. RAID-0 reduces runtime up to 50-60% of the runtime of using disk/SSD. Clearly, X-Stream's sequential disk access pattern allows it to fully take advantage of additional I/O devices.</p><p>The scalability of X-Stream in terms of graph size is purely a function of the available storage on the machine: the design principle of streaming is equally applicable to all three of main memory, SSD and magnetic disk. We limit the available memory to X-Stream to 16GB. Figure <ref type="figure" target="#fig_1">16</ref> (with both axes in log scale) illustrates that X-Stream scales almost seamlessly across available devices as graph size doubles, with the 'bumps' in runtime occurring as we move to slower storage devices.</p><p>The fact that X-Stream starts from an unordered edge list means that it can easily handle growing graphs -similar to distributed systems such as Kineograph <ref type="bibr" target="#b14">[25]</ref>. We used X-Stream to add 330M edges at a time from the Twitter dataset <ref type="bibr" target="#b25">[36]</ref> to an initially empty graph. After each addition we recomputed weakly connected components on the graph taking into account the new edges. Figure <ref type="figure" target="#fig_6">17</ref> shows the recomputation time as the graph grows in size. X-Stream is limited to only use 16GB of main memory, forcing the graph to go to SSD. Each batch of ingested edges is partitioned and appended to files on the SSD and weakly connected components are recomputed. The time required for this grows with the size of the accumulated graph as component labels need to be propagated across a larger accumulated graph. However, even when the last batch of 330M added edges is ingested before the graph reaches its peak size (1.9 billion edges) recomputation takes less than 7 minutes. In contrast, the full graph takes around 20 minutes (Figure <ref type="figure">12a</ref>). In summary, X-Stream can absorb new edges with little overhead because it supports efficient recomputation on graphs with the newly added edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Comparison with Other Systems</head><p>In-memory We begin by considering the performance of the in-memory engine as compared to other systems that process graphs in memory. We study the effect of the design decision made in X-Stream to stream edges and updates rather than doing random access through an index into their sorted version.</p><p>First, we consider the costs of actually producing sorted formats for input graphs such as compressed sparse row.   This requires sorting the edge list and producing an index over the sorted list. Figure <ref type="figure">18</ref> compares the time required for sorting the edge list of various RMAT graphs to the time required to compute results in X-Stream from the unsorted graph. We use both quicksort (from the C library) and counting sort (since the keyspace is known) to sort the graph. Both of these are single-threaded, and therefore we compare them with a single-threaded run of X-Stream. Sorting does not scale well with increasing graph size, and as a consequence X-Stream ends up completing all the graph benchmarks faster than either version of sorting at the largest graph size. This provides evidence that, where pre-processing times are a concern, streaming is a winning proposition in comparison to sorting and random access. For all comparisons with other systems in this paper, we allow the other systems to start with a sorted and indexed edge list, while also reporting pre-processing times where available. X-Stream uses the unordered edge list as input.</p><p>We next compare the performance of X-Stream to optimized in-memory implementations of breadth-first search that do random access through an index over the edges. The first method (local queue), due to Agarwal et. al. <ref type="bibr" target="#b1">[12]</ref>, uses a per-core queue of vertices to be visited and heavily optimized synchronization. The second method (hybrid), due to Hong et. al. <ref type="bibr" target="#b22">[33]</ref>, includes significant enhancements to the other (older) piece of work. In the comparisons we use the exact same graph as used in Hong et. al. <ref type="bibr" target="#b22">[33]</ref>.</p><p>Figure <ref type="figure" target="#fig_7">19</ref> shows the performance of X-Stream, local queue, and hybrid. The figure includes 99% confidence intervals on the runtime that are too small to be visible due to negligible variation in runtime. X-Stream performs better than both methods for all thread counts, albeit with a closing gap towards higher thread counts. The reason for this closing gap in runtime is that the gap between random and sequential memory access bandwidth is also closing with increasing thread count, from 4.6X at 1 core to 1.8X at 16 cores (Figure <ref type="figure">11</ref>). At the same time, X-Stream sends updates on only about 35% of the streamed edges, thereby wasting 65% of the available sequential bandwidth (a tradeoff we described in §1).</p><p>Random access, however, enables highly effective algorithm-specific optimizations. Beamer et al. <ref type="bibr" target="#b7">[18]</ref> demonstrated that for scale-free graphs large speedups can be obtained in the later execution stages of BFS by iterating over the set of target vertices rather than the set of source vertices. At these later stages, the set of discovered vertices has grown to cover a large portion of the graph, and therefore a number of updates go to vertices that are already part of the BFS tree. It is then cheaper to pull updates by scanning neighbors from the remaining vertices, rather than push updates by iterating over the BFS horizon. Ligra <ref type="bibr" target="#b37">[48]</ref> is a recent main-memory graph processing system that implements this observation. We compare the performance of X-Stream with Ligra on a subset of the Twitter graph <ref type="bibr" target="#b25">[36]</ref>. Comparing with Ligra is, unfortunately, not a strict apples-to-apples comparison. Ligra runs on the Cilk runtime <ref type="bibr" target="#b18">[29]</ref> and is compiled with the Intel compiler, while X-Stream is built on top of Linux pthreads and is compiled with gcc.</p><p>We list the runtimes for the two systems in Figure <ref type="figure">20</ref> for BFS and Pagerank, separating the overall runtime in preprocessing time and runtime for the computation proper. For BFS, when considering only the computation proper, Ligra is much faster than X-Stream (10X -20X), but this performance comes at a significant pre-processing cost. Using direction reversal requires pre-processing to produce an inverted edge list, in turn requiring random access to a large data structure in order to switch edges from a list sorted by source to one sorted by destination. This pre-processing dominates the overall runtime, and is about 7X-8X that of the overall running time for X-Stream. This pre-processing time in Ligra could be improved using counting sort instead of quicksort, or by storing the reversed and sorted edge list in order to amortize the pre-processing cost. For Pagerank, X-Stream is faster than Ligra at all thread counts. Pagerank's uniform communication pattern makes direction reversal ineffective.</p><p>Finally, we analyzed the impact of the memory access patterns on instruction throughput, comparing X-Stream to the other in-memory graph processing solutions discussed above. Figure <ref type="figure" target="#fig_8">21</ref> shows the average count of instructions per cycle (IPC) and the total number of memory references for BFS. X-Stream shows a far higher IPC than the other implementations. In general, a higher IPC results either from a smaller number of main memory references (misses in the last level cache) or from a lower average latency to resolve memory references. For BFS, the improved IPC cannot be explained by reduced memory references alone. In the case of Ligra, Figure <ref type="figure" target="#fig_8">21</ref> shows that X-Stream makes more memory references and yet demonstrates a higher IPC. We therefore conclude that the higher IPC in X-Stream is due to lower latencies for resolving memory access, a result of the fact that sequential access allows the prefetcher to hide some of the memory access latency.</p><p>In summary, X-Stream's in-memory engine demonstrates that sequential access can be a winning proposition even given the relatively small gap between random and sequential access bandwidth to main memory and even when compared to specialized multi-threaded implementations of graph algorithms. We also underlined an important property of X-Stream that makes it attractive for in-memory graph processing: the fact that it can return results immediately from unordered edge lists.</p><p>Out-Of-Core We now compare the performance of X-Stream to Graphchi <ref type="bibr" target="#b26">[37]</ref>. Like X-Stream, Graphchi is  a scale-up system that can process large graphs from secondary storage. Graphchi uses the traditional vertexcentric approach to graph processing, but it uses an innovative out-of-core data structure, called parallel sliding windows, to reduce the amount of random access to disk. We used the same algorithms and graphs as reported by Graphchi <ref type="bibr" target="#b26">[37]</ref>, constraining both systems to 8 GB of memory and using the SSD for storage (as done in that work).</p><p>Figure <ref type="figure">22</ref> shows the results in terms of execution time.</p><p>Graphchi needs time to pre-sort the graph into shards before beginning execution. For three out of four algorithms we used to compare against Graphchi, X-Stream finishes execution on the same unsorted graph before Graphchi finishes sorting it into shards. This result extends the observations about sorting time for the inmemory case in the previous section to the out-of-core case. Moving further, we found that X-Stream finished execution of the graph algorithm faster than Graphchi, even excluding pre-processing time. We attribute X-Stream's shorter runtimes to two factors.</p><p>The first factor is the vertex-centric approach used in Graphchi, in which updates are absorbed by vertices by executing a loop over their in-edges. This requires Graphchi to re-sort the edges in the shard by destination vertex after loading the shard into memory. The creation of this reverse-sorted in-memory data structure consumes a significant amount of time, reported as re-sort in Figure <ref type="figure">22</ref>.</p><p>The second contributor to Graphchi's longer runtime is its incomplete usage of available streaming bandwidth from the SSD. For the graphs used in this experiment, of the fact that it only needs to fit the vertex data for the partition into memory. In contrast, Graphchi needs many shards, because it also needs to fit all edges of a shard into memory. This leads to more fragmented reads and writes that are distributed over many shards. Figure <ref type="figure" target="#fig_9">23</ref> illustrates this phenomenon with an I/O bandwidth report from the iostat tool. The report depicts a 4-minute interval, after the shard creation phase for Graphchi, of the execution of Pagerank on the Twitter graph. X-Stream aggregate bandwidth use is much higher than that of Graphchi. For X-Stream, the scatter phase exhibits a regular pattern, alternating between a burst of reads (from the edge file) and a burst of writes (to the update files). The gather phase exhibits a long burst of reads without any writes. In contrast, Graphchi's SSD accesses are far more bursty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Design Decisions</head><p>First, we consider the effect of varying the number of partitions. The number of partitions needs to be large enough such that the vertex set of each streaming partition fits in the CPU cache for in-memory graphs or in main memory for out-of-core graphs. Too large a number of partitions, however, leads to excessive partitioning overhead and more random accesses. Figure <ref type="figure" target="#fig_11">24</ref> shows the effect of the number of partitions on the in-memory execution time for four algorithms using the RMAT-25 graph. The execution time is relatively stable for a large range of number of partitions, but increases substantially when too small or too large a number is chosen.</p><p>Second, Figure <ref type="figure">25</ref> shows the effect of varying the number of shuffler stages for the RMAT-25 graph with 1M (2 20 ) partitions. The figure shows total runtime for the same four algorithms, normalized to the runtime for a single-stage shuffler. Using a one-stage shuffler is clearly sub-optimal due to the reasons explained in §4.2.</p><p>Using too many stages leads to unnecessary copying. The optimal choice in this case is a two-stage shuffle, X-Stream automatically picks the number of streaming partitions for in-memory and out-of-core graphs, using the amount of main memory and the cache size as inputs. It also automatically picks the shuffler fanout for in-memory graphs, using the number of cache lines as input. Space constraints prevent us from presenting the algorithm for doing so, but in all cases that we have been able to verify, X-Stream succeeds in choosing optimal or near-optimal values.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Generalization</head><p>To express X-Stream's capabilities and limitations in more general terms, we examined using the theoretical I/O model <ref type="bibr" target="#b2">[13]</ref> the cost of propagating a message from (any) source vertex to all other (assumed reachable) vertices in a graph G = (V, E), modeling label propagation in the graph. The maximum number of edge-centric scatter phases to complete this task is the diameter of the graph (D). The I/O model uses a "memory" of M words backed by an infinitely sized disk from which transfers are made in aligned units of B words. Fewer I/Os implies a faster algorithm. The results in Figure <ref type="figure" target="#fig_12">26</ref> confirm that X-Stream does well on low diameter graphs where it scales better than solutions that involve first sorting the graph. It also shows that for dense graphs, X-Stream uses fewer partitions than Graphchi uses shards and that it scales better than Graphchi on I/Os regardless of graph diameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Exploiting sequential access bandwidth has been a longstanding theme in the algorithms community with cacheoblivious data structures <ref type="bibr" target="#b8">[19,</ref><ref type="bibr" target="#b17">28,</ref><ref type="bibr" target="#b39">50]</ref>. X-Stream is not cache-oblivious, but aspires to the same goal of sequential scans over data. Another closely related area of work is stream processing. Stream processing aims to analyze unbounded information flows using only a constant amount of buffering. Specific to graphs, there has been some success on stream processing using small [O(V polylog(V ))] space in the semi-streaming model <ref type="bibr" target="#b30">[41]</ref>, streaming just the edges, and more recently with the W-Stream model <ref type="bibr" target="#b3">[14]</ref>. This is a good fit to streaming partitions, and we expect to implement semistreaming and W-Stream algorithms on X-Stream as research on them progresses. X-Stream's shuffling phase also draws inspiration from work on sorting algorithms, such as polyphase merging <ref type="bibr" target="#b40">[51]</ref>.</p><p>From the perspective of disks, sequential access has been a constant theme for both magnetic disks and SSDs <ref type="bibr" target="#b5">[16,</ref><ref type="bibr" target="#b35">46,</ref><ref type="bibr" target="#b38">49]</ref>. The latency of servicing random requests in SSDs can be hidden by concurrency in servicing them, a feature not available in magnetic disks. This has become a viable route to graph processing from SSDs <ref type="bibr" target="#b21">[32,</ref><ref type="bibr" target="#b42">53]</ref>.</p><p>A number of distributed graph processing systems [10, <ref type="bibr" target="#b20">31,</ref><ref type="bibr" target="#b29">40]</ref> provide scale-out solutions for graph processing. X-Stream offers the alternative of using easier-tomanage single servers. It is competitive to Graphchi, which itself is competitive to many of these distributed systems <ref type="bibr" target="#b26">[37]</ref>.</p><p>X-Stream is best suited to graphs with low diameter in relation to size. There is evidence that the diameter of real world graphs often grows only sub-logarithmically (O( log(V ) loglog(V ) )) with the number of vertices <ref type="bibr" target="#b11">[22]</ref> or even demonstrates densification <ref type="bibr" target="#b27">[38]</ref>, where the diameter shrinks with new vertices joining the network. On a similar note, Backstrom et. al. <ref type="bibr" target="#b4">[15]</ref> report that the average path length between two people in the 721 million strong Facebook social network is smaller than 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>X-Stream is an edge-centric approach to the scattergather model. X-Stream uses streaming partitions to utilize the sequential streaming bandwidth of the storage medium for graph processing, scaling seamlessly across graphs stored in main memory, on SSD and on magnetic disk. We have demonstrated that X-Stream's approach is in many cases a winning proposition when compared against the traditional approach of indexing the edge list and performing random access through the index. A release of X-Stream is available at: http://labos.epfl.ch/x-stream</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Edge-Centric Scatter-Gather with Streaming Partitions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Disk Streaming Loop</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Slicing a Streaming Buffer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 8: Memory bandwidth</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 12 :Figure 13 :</head><label>1213</label><figDesc>Figure 12: Different Algorithms on Real World Graphs: (a) Runtimes; (b) Number of scatter-gather iterations, ratio of runtime to streaming time, and percentage of wasted edges for WCC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure 14: Strong Scaling</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 17</head><label>17</label><figDesc>Figure 17: Re-computing WCC</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: In-memory BFS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: Instructions per Cycle and Total Number of Memory References for BFS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 23 :</head><label>23</label><figDesc>Figure 23: Disk Bandwidth</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 24 :</head><label>24</label><figDesc>Figure 24: Effect of the Number of Partitions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 26 :</head><label>26</label><figDesc>Figure 26: Big-O Bounds in the I/O Model (U is the set of updates)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>inbound edges of v while not done for all vertices v that need to scatter updates vertex_scatter(v) for all vertices v that have updates vertex_gather(v)</head><label></label><figDesc>, Nov 03-06 2013, Farmington, PA, USA ACM 978-1-4503-2388-8/13/11. http://dx.doi.org/10.1145/2517349.2522740</figDesc><table><row><cell>vertex_scatter(vertex v)</cell></row><row><cell>send updates over outgoing edges of v</cell></row><row><cell>vertex_gather(vertex v)</cell></row><row><cell>apply updates from</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>scatter phase: for each streaming_partition p read in vertex set of p for each edge e in edge list of p edge_scatter(e): append update to Uout shuffle phase: for each update u in Uout let p = partition containing target of u append u to Uin(p) destroy Uout gather phase: for each streaming_partition p read in vertex set of p for each update u in Uin(p) edge_gather(u) destroy Uin(p)</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>merged scatter/shuffle phase: for each streaming partition s while edges left in s load next chunk of edges into input buffer for each edge e in memory edge_scatter(e) appending to output buffer if output buffer is full or no more edges in-memory shuffle output buffer for each streaming partition p append chunk p to update file for p gather phase: for each streaming_partition p read in vertex set of p while updates left in p load next chunk of updates into input buffer for each update u in input buffer edge_gather(u) write vertex set of p</head><label></label><figDesc></figDesc><table><row><cell>Index Array (K entries)</cell></row><row><cell>Chunk</cell></row><row><cell>Chunk Array</cell></row><row><cell>Figure 5: Stream Buffer (K: number of partitions)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>2 ... K 1</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>... K 1 2 ... K</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: We would like to thank Aapo Kyrola and Julian Shun for their help in benchmarking Graphchi and Ligra respectively. We would also like to thank our reviewers and shepherd as well as Ed Bugnion, Stanko Novakovic, Eiko Yoneki and Florin Dinu for their feedback that helped improve the paper. X-Stream was supported by a grant from the Hasler Foundation.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pre-Sort (s)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runtime (s)</head><p>Re-sort (s) Twitter pagerank X-Stream (1) none 397.57 ± 1.83 -Graphchi <ref type="bibr" target="#b21">(32)</ref> 752.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards compressing web graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitzenmacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Data Compression Conference</title>
		<meeting>the Data Compression Conference</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="203" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scalable graph exploration on multicore processors</title>
		<author>
			<persName><forename type="first">V</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Petrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pasetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The input/output complexity of sorting and related problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Vitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1116" to="1127" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the streaming model augmented with a sorting primitive</title>
		<author>
			<persName><forename type="first">G</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ruhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium on Foundations of Computer Science</title>
		<meeting>the IEEE Symposium on Foundations of Computer Science</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="540" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Four degrees of separation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Backstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Boldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ugander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vigna</surname></persName>
		</author>
		<idno>CoRR, abs/1111.4570</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SSDAlloc: hybrid SSD/RAM memory management made easy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Badam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Pai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USENIX conference on Networked Systems Design and Implementation</title>
		<meeting>the USENIX conference on Networked Systems Design and Implementation</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Emergence of scaling in random networks</title>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Barabási</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Albert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="issue">5439</biblScope>
			<biblScope unit="page" from="509" to="512" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Direction-optimizing breadth-first search</title>
		<author>
			<persName><forename type="first">S</forename><surname>Beamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis</title>
		<meeting>the International Conference on High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optimal sparse matrix dense vector multiplication in the I/O-model</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Brodal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fagerberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vicari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM symposium on Parallel Algorithms and Architectures</title>
		<meeting>the ACM symposium on Parallel Algorithms and Architectures</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="61" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Algebraic Graph Theory</title>
		<author>
			<persName><forename type="first">N</forename><surname>Biggs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">HyperANF: approximating the neighbourhood function of very large graphs on a budget</title>
		<author>
			<persName><forename type="first">P</forename><surname>Boldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vigna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International conference on World Wide Web</title>
		<meeting>the International conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="625" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The diameter of a scale-free random graph</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bollobas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Riordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="34" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">R-MAT: A recursive model for graph mining</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIAM International Conference on Data Mining</title>
		<meeting>the SIAM International Conference on Data Mining</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tiled-MapReduce: optimizing resource usages of data-parallel applications on multicore with tiling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International conference on Parallel Architectures and Compilation Techniques</title>
		<meeting>the International conference on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="523" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Kineograph: taking the pulse of a fast-changing and connected world</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM European conference on Computer Systems</title>
		<meeting>the ACM European conference on Computer Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="85" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On graph problems in a semi-streaming model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feigenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mcgregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International colloquium on Automata, Languages and Programming</title>
		<meeting>the International colloquium on Automata, Languages and Programming</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="531" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Clearing the clouds: a study of emerging scale-out workloads on modern hardware</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kocberber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alisafaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the International conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cache-Oblivious algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Frigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Prokop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramachandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual symposium on Foundations of Computer Science</title>
		<meeting>the Annual symposium on Foundations of Computer Science</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="285" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The implementation of the Cilk-5 multithreaded language</title>
		<author>
			<persName><forename type="first">M</forename><surname>Frigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Randall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Programming Language Design and Implementation</title>
		<meeting>the conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="212" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A distributed algorithm for minimum-weight spanning trees</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Gallager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Humblet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Spira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Programming Languages and Systems</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="66" to="77" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Powergraph: distributed graphparallel computation on natural graphs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Operating Systems Design and Implementation</title>
		<meeting>the conference on Operating Systems Design and Implementation</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="17" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Turbograph: a fast parallel graph engine handling billion-scale graphs in a single PC</title>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International conference on Knowledge discovery and data mining</title>
		<meeting>the International conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="77" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient parallel graph exploration on multi-core CPU and GPU</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Oguntebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Olukotun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International conference on Parallel Architectures and Compilation Techniques</title>
		<meeting>the International conference on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="78" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Write amplification analysis in flashbased solid state drives</title>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Eleftheriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Iliadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pletka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Israeli Experimental Systems Conference</title>
		<meeting>the Israeli Experimental Systems Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Inference of beliefs on billion-scale graphs</title>
		<author>
			<persName><forename type="first">U</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Large-scale Data Mining: Theory and Applications</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">What is Twitter, a social network or a news media?</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International conference on World Wide Web</title>
		<meeting>the International conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="591" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graphchi: Large-scale graph computation on just a PC</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Blelloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Operating Systems Design and Implementation</title>
		<meeting>the conference on Operating Systems Design and Implementation</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graph evolution: Densification and shrinking diameters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Challenges in parallel graph processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lumsdaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hendrickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Processing Letters</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="20" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pregel: a system for large-scale graph processing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Malewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Austern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Bik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Dehnert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Leiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Czajkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Management of Data</title>
		<meeting>the International Conference on Management of Data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Data streams: algorithms and applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Discrete Algorithms</title>
		<meeting>the Symposium on Discrete Algorithms</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">413</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The PageRank citation ranking: Bringing order to the web</title>
		<author>
			<persName><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multithreaded asynchronous graph traversal for inmemory and semi-external memory</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gokhale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Amato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International conference for High Performance Computing, Networking, Storage and Analysis</title>
		<meeting>the International conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Managing large graphs on multi-cores with graph awareness</title>
		<author>
			<persName><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haridasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Technical Conference</title>
		<meeting>the Annual Technical Conference</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Evaluating MapReduce for multi-core and multiprocessor systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ranger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raghuraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Penmetsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International symposium on High Performance Computer Architecture</title>
		<meeting>the International symposium on High Performance Computer Architecture</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The design and implementation of a log-structured file system</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Ousterhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="52" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Computing strongly connected components in Pregel-like systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Salihoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Widom</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ligra: a lightweight graph processing framework for shared memory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Blelloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Principles and Practice of Parallel Programming</title>
		<meeting>the Symposium on Principles and Practice of Parallel Programming</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An updateaware storage system for low-locality updateintensive workloads</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Simha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-C</forename><surname>Chiueh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the International conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="375" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Stratified B-trees and versioned dictionaries</title>
		<author>
			<persName><forename type="first">A</forename><surname>Twigg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Byde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Milos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moreton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wilkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wilkie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Hot topics in Storage and File Systems</title>
		<meeting>the Conference on Hot topics in Storage and File Systems</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Algorithms and Data Structures for External Memory</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Vitter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">JFFS : the Journalling Flash File System</title>
		<author>
			<persName><forename type="first">D</forename><surname>Woodhouse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Ottawa Linux Symposium</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Scale-up graph processing: a storage-centric view</title>
		<author>
			<persName><forename type="first">E</forename><surname>Yoneki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on Graph Data Management Experiences and Systems</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A scalable distributed parallel breadth-first search algorithm on bluegene/l</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Mclendon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hendrickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Catalyurek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 ACM/IEEE conference on Supercomputing</title>
		<meeting>the 2005 ACM/IEEE conference on Supercomputing</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Large-scale parallel collaborative filtering for the netflix prize</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wilkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International conference on Algorithmic Aspects in Information and Management</title>
		<meeting>the International conference on Algorithmic Aspects in Information and Management</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="337" to="348" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
