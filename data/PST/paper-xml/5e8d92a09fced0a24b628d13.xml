<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DiffNet++: A Neural Influence and Interest Diffusion Network for Social Recommendation</title>
				<funder ref="#_SJCAN8m">
					<orgName type="full">Foundation of Key Laboratory of Cognitive Intelligence, iFLYTEK, P.R., Chia</orgName>
				</funder>
				<funder>
					<orgName type="full">CAAI-Huawei MindSpore Open Fund</orgName>
				</funder>
				<funder ref="#_PQGWgNz #_2GUXsWH #_yEcQRRn #_yTgDurC #_Y8N7h5Z #_QBJn6Qk #_bECYNYd">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-01-05">5 Jan 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Le</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Junwei</forename><forename type="middle">?</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Peijie</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Richang</forename><surname>Hong</surname></persName>
						</author>
						<author role="corresp">
							<persName><roleName>Member, IEEE</roleName><forename type="first">Yong</forename><surname>Ge</surname></persName>
							<email>yongge@email.arizona.edu</email>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Meng</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Information Engineering</orgName>
								<orgName type="department" key="dep2">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<postCode>230009</postCode>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Hefei Comprehensive National Science Center</orgName>
								<address>
									<postCode>230088</postCode>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Computer Science and Information Engineering</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<postCode>230009</postCode>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Management Information Systems Department</orgName>
								<orgName type="institution">The Univer-sity of Arizona</orgName>
								<address>
									<settlement>Tucson</settlement>
									<region>Arizona</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DiffNet++: A Neural Influence and Interest Diffusion Network for Social Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-01-05">5 Jan 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2002.00844v4[cs.SI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>recommender systems</term>
					<term>graph neural network</term>
					<term>social recommendation</term>
					<term>influence diffusion</term>
					<term>interest diffusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Social recommendation has emerged to leverage social connections among users for predicting users' unknown preferences, which could alleviate the data sparsity issue in collaborative filtering based recommendation. Early approaches relied on utilizing each user's first-order social neighbors' interests for better user modeling, and failed to model the social influence diffusion process from the global social network structure. Recently, we propose a preliminary work of a neural influence Diff usion Network (i.e., DiffNet) for social recommendation <ref type="bibr" target="#b42">[43]</ref>. DiffNet models the recursive social diffusion process for each user, such that the influence diffusion hidden in the higher-order social network is captured in the user embedding process. Despite the superior performance of DiffNet, we argue that, as users play a central role in both user-user social network and user-item interest network, only modeling the influence diffusion process in the social network would neglect the latent collaborative interests of users hidden in the user-item interest network. To this end, in this paper, we propose DiffNet++, an improved algorithm of DiffNet that models the neural influence diffusion and interest diffusion in a unified framework. By reformulating the social recommendation as a heterogeneous graph with social network and interest network as input, DiffNet++ advances DiffNet by injecting both the higher-order user latent interest reflected in the user-item graph and higher-order user influence reflected in the user-user graph for user embedding learning. This is achieved by iteratively aggregating each user's embedding from three aspects: the user's previous embedding, the influence aggregation of social neighbors from the social network, and the interest aggregation of item neighbors from the user-item interest network. Furthermore, we design a multi-level attention network that learns how to attentively aggregate user embeddings from these three aspects. Finally, extensive experimental results on four real-world datasets clearly show the effectiveness of our proposed model. We release the source code at https://github.com/PeiJieSun/diffnet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Collaborative Filtering (CF) based recommender systems learn user and item embeddings by utilizing user-item interest behavior data, and have attracted attention from both the academia and industry <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b31">[32]</ref>. However, as most users have limited behavior data, CF suffers from the data sparsity issue <ref type="bibr" target="#b0">[1]</ref>. With the development of social networks, users build social relationships and share their item preferences on these platforms. As well supported by the social influence theory, users in a social network would influence each other, leading to similar preferences <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Therefore, social recommendation has emerged, which focuses on exploiting social relations among users to alleviate data sparsity and enhancing recommendation performance <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b42">[43]</ref>.</p><p>In fact, as users play a central role in social platforms with user-user social behavior and user-item interest behavior, the key to social recommendation relies on learning user embeddings with these two kinds of behaviors. For a long time, by treating the user-item interest network as a user-item matrix, CF based models resort to matrix factorization to project both users and items into a low latent space <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b35">[36]</ref>. Most social based recommender systems advance these CF models by leveraging the useruser matrix to enhance each user's embedding learning with social neighbors' records, or regularizing the user embedding learning process with social neighbors <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b14">[15]</ref>. For example, SocialMF <ref type="bibr" target="#b18">[19]</ref> and SR <ref type="bibr" target="#b29">[30]</ref> added social regularization terms based on social neighbors in the optimization function, and TrustSVD incorporated influences of social neighbors' decisions as additional terms for modeling a user's embedding <ref type="bibr" target="#b14">[15]</ref>. In summary, these models leveraged the first-order social neighbors for recommendation, and partially alleviated the data sparsity issue in CF.</p><p>Despite the performance improvement of these social recommendation models, we argue that the current social recommendation models are still far from satisfactory. In fact, as shown in Fig. <ref type="figure" target="#fig_2">1</ref>, users play a central role in two kinds of behavior networks: the user-user social network and the user-item interest network. On one hand, users naturally form a social graph with a global recursive social diffusion process. Each user is not only influenced by the direct first-order social neighbors, but also the higher-order ego-centric social network structure. E.g., though user u1 does not follow u5, u1 may be largely influenced by u5 in the social recommendation process as there are two second-order paths: u1 ? u2 ? u5 and u1 ? u4 ? u5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rating Matrix</head><p>Interest diffusion with higher-order user-item graph modeling Influence diffusion with higher-order user-user graph modeling Fig. <ref type="figure" target="#fig_2">1</ref>. An overall illustration of social recommendation. The second column shows how traditional models treat this problem with matrix representations of users' two kinds of behaviors. In this paper, we try to model both the influence diffusion and interest diffusion with graph representation of users' two kinds of behaviors.</p><p>Simply reducing the social network structure to the firstorder social neighbors would not well capture these higherorder social influence effect in the recommendation process. On the other hand, given the user-item bipartite interest graph, CF relies on the assumption that "similar users show similar item interests". Therefore, each user's latent collaborative interests are not only reflected by her rated items but also influenced by similar users' interests from items. E.g, though u1 does not show interests for v3 with a direct edge connection, the similar user u2 (as they have common item interests of v1) shows item interest for v3 as: u1 ? v1 ? u2 ? v3. Therefore, v3 is also useful for learning u1's embedding to ensure the collaborative signals hidden in the user-item graph are injected for user embedding learning. To summarize, previous CF and social recommendation models only considered the observed firstorder structure of the two graphs, leaving the higher-order structures of users under explored.</p><p>To this end, we reformulate users' two kinds of behaviors as a heterogeneous network with two graphs, i.e, a user-user social graph and a user-item interest graph, and propose how to explore the heterogeneous graph structure for social recommendation. In fact, Graph Convolutional Networks (GCNs) have shown huge success for learning graph structures with theoretical elegance, practical flexibility and high performance <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b21">[22]</ref>. GCNs perform node feature propagation in the graph, which recursively propagate node features by iteratively convolutional aggregations from neighborhood nodes, such that the up to K-th order graph structure is captured with K iterations <ref type="bibr" target="#b46">[47]</ref>. By treating user-item interactions as a bipartite interest graph and user-user social network as a social graph, some works have applied GCNs separately on these two kinds of graphs <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b42">[43]</ref>. On one hand, given the useritem interest graph, NGCF is proposed to directly encode the collaborative information of users by exploring the higher-order connectivity patterns with embedding propagation <ref type="bibr" target="#b40">[41]</ref>. On the other hand, in our previous work, we propose a Diff usion neural Network (DiffNet) to model the recursive social diffusion process in the social network, such that the higher-order social structure is directly modeled in the recursive user embedding process <ref type="bibr" target="#b42">[43]</ref>. These graph based models showed superior performance compared to the previous non-graph based recommendation models by modeling either graph structure. Nevertheless, how to design a unified model for better user modeling of these two graphs remains under explored.</p><p>In this paper, we propose to advance our preliminary DiffNet structure, and jointly model the two graph structure (user-item graph and user-user graph) for social recommendation. While it seems intuitive to perform message passing on both each user's social network and interest network, it is not well designed in practice as these two kinds of graphs serve as different sources to reflect each user's latent preferences. Besides, different users may have different preferences in balancing these two graphs, with some users are likely to be swayed by social neighbors, while others prefer to remain their own tastes. To this end, we propose DiffNet++, an improved algorithm of DiffNet that models the neural influence diffusion and interest diffusion in a unified framework. Furthermore, we design a multi-level attention network structure that learns how to attentively aggregate user embeddings from different nodes in a graph, and then from different graphs. In summary, our main contributions are listed as follows:</p><p>? Compared to our previous work of DiffNet <ref type="bibr" target="#b42">[43]</ref>, we revisit the social recommendation problem as predicting the missing edges in the user-item interest graph by taking both user-item interest graph and user-user social graph as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We propose DiffNet++ that models both the higherorder social influence diffusion in the social network and interest diffusion in the interest network in a unified model. Besides, we carefully design a multilevel attention network to attentively learn how the users prefer different graph sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>Extensive experimental results on two real-world datasets clearly show the effectiveness of our proposed DiffNet++ model. Compared to the baseline with the best performance, DiffNet++ outperforms it about 14% on Yelp , 21% on Flickr, 12% on Epinions, and 4% on Dianping for top-10 recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM DEFINITION AND RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Definition</head><p>In a social recommender system, there are two sets of entities: a user set U (|U | = M ), and an item set V (|V | = N ). Users form two kinds of behaviors in the social platforms: making social connections with other users and showing item interests. These two kinds of behaviors could be defined as two matrices: a user-user social connection matrix S ? R M ?M , and a user-item interaction matrix R ? R M ?N . In the social matrix S, if user a trusts or follows user b, s ba = 1, otherwise it equals 0. We use S a to represent the user set that user a follows, i.e., S a = [b|s ba <ref type="bibr">= 1]</ref>. The user-item matrix R shows users' rating preferences and interests to items. As some implicit feedbacks (e.g., watching movies, purchasing items, listening to songs ) are more common in real-world applications, we also consider the recommendation scenario with implicit feedback <ref type="bibr" target="#b36">[37]</ref>. Let R denote users' implicit feedback based rating matrix, with r ai = 1 if user a is interested in item i, otherwise it equals 0. We use R a represents the item set that user a has consumed, i.e., R a = [i|r ai = 1], and R i denotes the user set which consumed the item i, i.e., R i = [a|r ia <ref type="bibr">= 1]</ref>.</p><p>Given the two kinds of users' behaviors, the user-user social network is denoted as a user-user directed graph: GS =&lt; U, S &gt;, where U is the nodes of all users in the social network. If the social network is undirected, then user a connects to user b denotes a follows b, and b also follows a, i.e., s ab = 1? s ba = 1. The user interest network denotes users' interests for items, which could be constructed from the user-item rating matrix R as an undirected bipartite network:</p><formula xml:id="formula_0">GI =&lt; U ? V, R &gt;.</formula><p>Besides, each user a is associated with real-valued attributes (e.g, user profile), denoted as x a in the user attribute matrix X ? R d1?M . Also, each item i has an attribute vector y i (e.g., item text representation, item visual representation) in item attribute matrix Y ? R d2?N . We formulate the graph based social recommendation problem as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1 (Graph Based Social Recommendation).</head><p>Given the user social network G S and user interest network GI , these two networks could be formulated as a heterogeneous graph that combines GS and GI as:</p><formula xml:id="formula_1">G = GS ? GI =&lt; U ? V, X, Y, R, S &gt;.</formula><p>Then, the graph based social recommendation asks that: given graph G in the social network, our goal is to predict users' unknown preferences to items , i.e, the missing links in the graph based social recommendation as :</p><formula xml:id="formula_2">R = f (G) = f (U ? V, X, Y, R, S)</formula><p>, where R ? R M ?N denotes the predicted preferences of users to items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Preliminaries and Related Work</head><p>In this subsection, we summarize the related works for social recommendation into three categories: classical social recommendation models, the recent graph based recommendation models, and attention modeling in the recommendation domain.</p><p>Classical Social Recommendation Models. By formulating users' historical behavior as a user-item interaction matrix R, most classical CF models embed both users and items in a low dimension latent space, such that each user's predicted preference to an unknown item turns to the inner product between the corresponding user and item embeddings as <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b35">[36]</ref>:</p><formula xml:id="formula_3">rai = v T i ua,<label>(1)</label></formula><p>where ua is the embedding of user a, which is the a-th column of the user embedding matrix U. Similarly, vi represents item i's embedding in the i-th column of item embedding matrix</p><formula xml:id="formula_4">V.</formula><p>In fact, as various specialized matrix factorization models have been proposed for specific tasks, factorization machines is proposed as a general approach to mimic most factorization models with simple feature engineering <ref type="bibr" target="#b35">[36]</ref>. Recently, some deep learning based models have been proposed to tackle the CF problem <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b27">[28]</ref>. These approaches advanced previous works by modeling the non-linear complex interactions between users, or the complex interactions between sparse feature input.</p><p>The social influence and social correlation among users' interests are the foundation for building social recommender systems <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Therefore, the social network among users could be leveraged to alleviate the sparsity in CF and enhance recommendation performance <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Due to the superiority of embedding based models for recommendation, most social recommendation models are also built on these embedding models. These social embedding models could be summarized into the following two categories: the social regularization based approaches <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b43">[44]</ref> and the user behavior enhancement based approaches <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Specifically, the social regularization based approaches assumed that connected users would show similar embeddings under the social influence diffusion. As such, besides the classical CF pair-wised loss function in BPR <ref type="bibr" target="#b36">[37]</ref>, an additional social regularization term is incorporated in the overall optimization function as:</p><formula xml:id="formula_5">M i=1 M j=1 sij||ui -uj|| 2 F = U(D -S)U T ,<label>(2)</label></formula><p>where D is a diagonal matrix with d aa = M b=1 s ab . Instead of the social regularization term, some researchers argued that the social network provides valuable information to enhance each user's behavior <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b14">[15]</ref>. TrustSVD is such a representative model that shows stateof-the-art performance <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. By assuming the implicit feedbacks of a user's social neighbors' on items could be regarded as the auxiliary feedback of this user, TrustSVD modeled the predicted preference as:</p><formula xml:id="formula_6">rai = v T i (u a + |R a | -1 2 i?Ra y i + |S a | -1 2 b?Sa u b )<label>(3)</label></formula><p>where Ra = [i|rai = 1] is the itemset that a shows implicit feedback, and y i is an implicit factor vector. Therefore, these first two terms compose SVD++ model that explicitly builds each user's liked items in the user embedding learning process <ref type="bibr" target="#b22">[23]</ref>. In the third term, u b denotes the latent embedding of user b, who is trusted by a. As such, a's latent embedding is enhanced by considering the influence of her trusted users' latent embeddings in the social network.</p><p>As items are associated with attribute information (e.g., item description, item visual information), ContextMF is proposed to combine social context and social network under a collective matrix factorization framework with carefully designed regularization terms <ref type="bibr" target="#b20">[21]</ref>. Social recommendation has also been extended with social circles <ref type="bibr" target="#b33">[34]</ref>, temporal context <ref type="bibr" target="#b37">[38]</ref>, rich contextual information <ref type="bibr" target="#b41">[42]</ref>, user role in the social network <ref type="bibr" target="#b38">[39]</ref>, and efficient training models without negative sampling <ref type="bibr" target="#b5">[6]</ref>. All these previous works focused on how to explore the social neighbors, i.e., the observed links in the social network. Recently, CNSR is proposed to leverage the global social network in the recommendation process <ref type="bibr" target="#b43">[44]</ref>. In CNSR, each user's latent embedding is composed of two parts: a free latent embedding (classical CF models), and a social network embedding that captures the global social network structure. Despite the relative improvement of CNSR, we argue that CNSR is still suboptimal as the global social network embedding process is modeled for the network based optimization tasks instead of user preference learning. In contrast to CNSR, our work explicitly models the recursive social diffusion process in the global social network for optimizing the recommendation task. Researchers proposed to generate social sequences based on random walks on user-user and user-item graph, and further leveraged the sequence embedding techniques for social recommendation <ref type="bibr" target="#b10">[11]</ref>. This model could better capture the higher-order social network structure. However, the performance heavily relies on the choice of random walk strategy, including switching between user-item graph and user-user graph, and the length of random walk, which is both time-consuming and labor-consuming.</p><p>Graph Convolutional Networks and Applications in Recommendation. GCNs generalize the convolutional operations from the regular Euclidean domains to non-Euclidean graph and have empirically shown great success in graph representation learning <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Specifically, GCNs recursively perform message passing by applying convolutional operations to aggregate the neighborhood information, such that the K-th order graph structure is captured with K iterations <ref type="bibr" target="#b21">[22]</ref>. By treating the user-item interaction as a graph structure, GCNs have been applied for recommendation <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b50">[51]</ref>. Earlier works relied on spectral GCNs, and suffered from huge time complexity <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b50">[51]</ref>. Therefore, many recent works focus on the spatial based GCNs for recommendation <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b40">[41]</ref>. PinSage is a GCN based content recommendation model by propagating item features in the item-item correlation graph <ref type="bibr" target="#b47">[48]</ref>. GC-MC applied graph neural network for CF, with the first order neighborhood is directly modeled in the process <ref type="bibr" target="#b3">[4]</ref>. NGCF extended GC-MC with multiple layers, such that the higher-order collaborative signals between users and items can be modeled in the user and item embedding learning process <ref type="bibr" target="#b40">[41]</ref>.</p><p>As the social structure among users could be naturally formulated as a user-user graph, recently we propose a preliminary graph based social recommendation model, DiffNet, for modeling the social diffusion process in recommendation <ref type="bibr" target="#b42">[43]</ref>. DiffNet advances classical embedding based models with carefully designed influence diffusion layers, such that how users are influenced by the recursive influence diffusion process in the social network could be well modeled. Given each user a, the user embedding u a is sent to the influence diffusion layers. Specifically, let K denote the depth of the influence diffusion layers and h k a is the user representation at the k-th layer of the influence diffusion part. For each user a, her updated embedding h k+1 a is performed by social diffusion of the embeddings at the k-th layer with two steps: aggregation from her social neighbors at the k-th layer (Eq.( <ref type="formula" target="#formula_7">4</ref>)), and combination of her own latent embedding h k a at k-th layer and neighbors:</p><formula xml:id="formula_7">h k+1 Sa = P ool(h k b |b ? S a ),<label>(4)</label></formula><formula xml:id="formula_8">h k+1 a = s (k+1) (W k ? [h k+1 Sa , h k a ]),<label>(5)</label></formula><p>where the first equation is a pooling operation that transforms all the social trusted users' influences into a fixed length vector h k+1 Sa , s(x) is a transformation function and we use s (k+1) to denote the transformation function for (k +1)-th layer. As such, with a diffusion depth K, DiffNet could automatically models how users are influenced by the K-th order social neighbors in a social network for social recommendation. When K = 0, the social diffusion layers disappear and DiffNet degenerates to classical CF models.</p><p>In summary, all these previous GCN based models either considered the higher-order social network or the higherorder user interest network for recommendation. There are some recently works that also leverage the graph neural networks for social recommendation <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b44">[45]</ref>. Specifically, GraphRec is designed to learn user representations by fusing first order social and first-order item neighbors with non-linear neural networks <ref type="bibr" target="#b9">[10]</ref>. Researchers also proposed deep learning techniques to model the complex interaction of dynamic and static patterns reflected from users' social behavior and item preferences <ref type="bibr" target="#b44">[45]</ref>. Although these works relied on deep learning based models with users' two kinds of behaviors, they only modeled the first order structure of the social graph and interest graph. We differ from these works as we simultaneously fuse the higher-order social and interest network structure for better social recommendation.</p><p>Attention Models and Applications. As a powerful and common technique, attention mechanism is often adopted when multiple elements in a sequence or set would have an impact of the following output, such that attentive weights are learned with deep neural networks to distinguish important elements <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b45">[46]</ref>. Given a user's rated item history, NAIS is proposed to learn the neural attentive weights for item similarity in item based collaborative filtering <ref type="bibr" target="#b15">[16]</ref>. For graph structure data, researchers proposed graph attention networks to attentively learn weights of each neighbor node in the graph convolutional process <ref type="bibr" target="#b39">[40]</ref>. In social recommendation, many attention models have been proposed to learn the social influence strength <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b9">[10]</ref>. E.g., with each user's direct item neighbors and social neighbors, GraphRec leverages attention modeling to learn the attentive weights for each social neighbor and each rated item for user modeling <ref type="bibr" target="#b9">[10]</ref>. In social contextual recommender systems, users' preferences are influenced by various social contextual aspect, and an attention network was proposed to learn the attention weight of each social contextual aspect in the user decision process. Our work is also inspired by the applications of attention modeling, and apply it to fuse the social network and interest network for social recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE PROPOSED MODEL</head><p>In this section, we first show the overall architecture of our proposed model DiffNet++, followed by each component. After that, we will introduce the learning process of DiffNet++. Finally, we give a detailed discussion of the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Architecture</head><p>As shown in the related work part, our preliminary work of DiffNet adopts the recursive influence diffusion process for iterative user embedding learning, such that the up to K-th order social network structure is injected into the social recommendation process <ref type="bibr" target="#b42">[43]</ref>. In this part, we propose DiffNet++, an enhanced model of DiffNet that fuses both influence diffusion in the social network G S and interest diffusion in the interest network G I for social recommendation. We show the overall neural architecture of DiffNet++ in Fig. <ref type="figure" target="#fig_1">2</ref>. The architecture of DiffNet++ contains four main parts: an embedding layer, a fusion layer, the influence and interest diffusion layers, and a rating prediction layer. Specifically, by taking related inputs, the embedding layer outputs free embeddings of users and items, and the fusion layer fuses both the content features and free embeddings. In the influence and interest diffusion layers, we carefully design a multi-level attention structure that could effectively diffuse higher-order social and interest networks. After the diffusion process reaches stable, the output layer predicts the preference score of each unobserved user-item pair.</p><p>Embedding Layer. It encodes users and items with corresponding free vector representations. Let P ? R M ?D and Q ? R N ?D represent the free latent embedding matrices of users and items with D dimensions. Given the one hot representations of user a, the embedding layer performs an index selection and outputs the free user latent embedding p a , i.e., the transpose of a-th row from user free embedding matrix P. Similarly, item i's embedding q i is the transpose of i-th row of item free embedding matrix Q.</p><p>Fusion Layer. For each user a, the fusion layer takes p a and her associated feature vector x a as input, and outputs a user fusion embedding u 0 a that captures the user's initial interests from different kinds of input data. We model the fusion layer as:</p><formula xml:id="formula_9">u 0 a = g(W1 ? [pa, xa, ]),<label>(6)</label></formula><p>where W 1 is a transformation matrix, and g(x) is a transformation function. Without confusion, we omit the bias term. This fusion layer could generalize many typical fusion operations, such as the concatenation operation u 0 a = [p a , x a ] by setting W 1 as an identity matrix and g(x) an identity function.</p><p>Similarly, for each item i, the fusion layer models the item embedding v 0 i as a function between its free latent vector q i and its feature vector y i as:</p><formula xml:id="formula_10">v 0 i = g(W 2 ? [q i , y i ]).<label>(7)</label></formula><p>Influence and Interest Diffusion Layers. By feeding the output of each user a's fused embedding u 0 a and each item i's fused embedding v 0 i into the influence and interest diffusion layers, these layers recursively model the dynamics of this user's latent preference and the item's latent preference propagation in the graph G with layer-wise convolutions. In detail, at each layer k + 1, by taking user a's embedding u k a and item i's embedding v k i from previous layer k as input, these layers recursively output the updated embeddings of v k+1 i and u k+1 a with diffusion operations. This iteration step starts at k = 0 and stops when the recursive process reaches a pre-defined depth K. As each item only appears in the user-item interest graph G I , in the following, we would first introduce how to update item embeddings, followed by the user embedding with influence and interest diffusions.</p><p>For each item i, given its k-th layer embedding v k i , we model the updated item embedding v k+1 i at the (k + 1)-th layer from G I as:</p><formula xml:id="formula_11">?k+1 i = AGGu(u k a , ?a ? Ri) = a?R i ? k+1 ia u k a ,<label>(8)</label></formula><formula xml:id="formula_12">v k+1 i = ?k+1 i + v k i ,<label>(9)</label></formula><p>where</p><formula xml:id="formula_13">R i = [a|r ia = 1] is the userset that rates item i. u k a</formula><p>is the k-th layer embedding of user a. ?k+1 i is the item i's aggregated embedding from its neighbor users in the useritem interest graph G I , with ? k+1 ia denotes the aggregation weight. After obtaining the aggregated embedding ?k+1 i from the k-th layer, each item's updated embedding v k+1 i is a fusion of the aggregated neighbors' embeddings and the item's emebedding at previous layer k. In fact, we try different kinds of fusion functions, including the concatenation and the addition, and find the addition always shows the best performance. Therefore, we use the addition as the fusion function in Eq.( <ref type="formula" target="#formula_12">9</ref>).</p><p>In the item neighbor aggregation function, Eq.( <ref type="formula" target="#formula_11">8</ref>) shows the weight of user a to item i. A naive idea is to aggregate the embeddings from i's neighbor users with mean pooling operation, i.e., ?k+1 i = a?Ri 1 |Ri| u k a . However, it neglects the different interest weights from users, as the importance values of different users vary in item representation. Therefore, we use an attention network to learn the attentive weight ? k+1 ia in Eq.( <ref type="formula" target="#formula_11">8</ref>) as:</p><formula xml:id="formula_14">? k+1 ia = M LP1([v k i , u k a ]),<label>(10)</label></formula><p>where a MultiLayer Perceptrion (MLP) is used to learn the node attention weights with the related user and item embeddings at the k-th layer. After that, we normalize the attention weights with:</p><formula xml:id="formula_15">? k+1 ia = exp(? k+1 ia ) b?R i exp(? k+1 ib ) . (<label>11</label></formula><formula xml:id="formula_16">)</formula><p>Specifically, the exponential function is used to ensure each attention weight is larger than 0.</p><p>For each user a, let u k a denote her latent embedding at the k-th layer. As users play a central role in both the social network G S and the interest network G I , besides </p><formula xml:id="formula_17">pk+1 a = b?Sa ? k+1 ab u k b ,<label>(12)</label></formula><formula xml:id="formula_18">qk+1 a = i?Ra ? k+1 ai v k i ,<label>(13)</label></formula><p>where Eq.( <ref type="formula" target="#formula_17">12</ref>) shows how each user updates her latent embedding by fusing the influence diffusion aggregation pk+1 a and interest diffusion aggregation qk+1 a , as well as her own embedding u k a at previous layer. Since each user appears in both the social graph and interest graph, Eq.( <ref type="formula" target="#formula_18">13</ref>) and Eq.( <ref type="formula" target="#formula_19">14</ref>) model the influence diffusion aggregation and interest diffusion aggregation from the two graphs respectively. Specifically, ? k+1 ab denotes the social influence of user b to a at the (k + 1)-th layer in the social network, and ? k+1 ai denotes the attraction of item i to user a at the (k + 1)-th layer in the interest network.</p><p>In addition to the user and item embeddings, there are three groups of weights in the above three equations. A naive idea is to directly set equal values of each kind of weights, i.e., ? |Ra| . However, this simple idea could not well model the different kinds of weights in the user decision process. In fact, these three groups of weights naturally present a two-layer multi-level structure. Specifically, the social influence strengths and the interest strengths could be seen as node-level weights, which model how each user bal-ances different neighboring nodes in each graph. By sending the aggregations of node level attention into Eq.( <ref type="formula" target="#formula_17">12</ref>), ? k+1 al is the graph level weight that learns to fuse and aggregate information from different graphs. Specifically, the graph layer weights are important as they model how each user balances the social influences and her historical records for user embedding. Different users vary, with some users are more likely to be swayed by the social network while the interests of others are quite stable. Therefore, the weights in the graph attention layer for eah user also need to be personally adapted.</p><formula xml:id="formula_20">(k+1) a1 =? (k+1) a2 = 1 2 , ?<label>(k+1)</label></formula><p>As the three groups of weights represent a multi-level structure, we therefore use a multi-level attention network to model the attentive weights. Specifically, the graph attention network is designed to learn the contribution weight of each aspect when updating a's embedding with different graphs, i.e., pk+1 a and qk+1 a in Eq.( <ref type="formula" target="#formula_17">12</ref>), and the node attention networks are designed to learn the attentive weights in each social graph and each interest graph respectively. Specifically, the social influence score ? k+1 ab is calculated as follows:</p><formula xml:id="formula_21">? k+1 ab = M LP2([u k a , u k b ]). (<label>15</label></formula><formula xml:id="formula_22">)</formula><p>In the above equation, the social influence strength ? k+1 ab takes the related two users' embeddings at the k-th layer as input, and sending these features into a MLP to learn the complex relationship between features for social influence strength learning. Without confusion, we omit the normalization step of all attention modeling in the following, as all of them share the similar form as shown in Eq. <ref type="bibr" target="#b10">(11)</ref>. Similarly, we calculate the interest influence score ? k+1 ai by taking related user embedding and item embedding as input:</p><formula xml:id="formula_23">? k+1 ai = M LP3([u k a , v k i ])<label>(16)</label></formula><p>After obtaining the two groups of the node attentive weights, the output of the node attention weights are sent to the graph attention network, and we could model the graph attention weights of ? k+1 al (l = 1, 2) as:</p><formula xml:id="formula_24">? k+1 a1 = M LP4([u k a , pk a ])<label>(17)</label></formula><formula xml:id="formula_25">? k+1 a2 = M LP4([u k a , qk a ])<label>(18)</label></formula><p>In the above equation, for each user a, the graph attention layer scores not only rely on the user's embedding (u k a ), but also the weighted representations that are learnt from the node attention network. For example, as shown in Eq.( <ref type="formula" target="#formula_17">12</ref>), ? (k+1) a1 denotes the influence diffusion weight for contributing to users' depth (k + 1) embedding , with additional input of the learned attentive combination of the influence diffusion aggregation in Eq.( <ref type="formula" target="#formula_18">13</ref>). Similarly, ? (k+1) a2 denotes the interest diffusion weight for contributing to users' depth (k+1) embedding , with additional input of the learned attentive combination of the interest diffusion aggregation in Eq.( <ref type="formula" target="#formula_19">14</ref>). As ? k+1 a1 + ? k+1 a2 = 1, larger ? k+1 a1 denotes higher influence diffusion effect with less interest diffusion effect. Therefore, the learned aspect importance scores are tailored to each user, which distinguish the importance of the influence diffusion effect and interest diffusion effect during the user's embedding updating process. Prediction Layer. After the iterative K-layer diffusion process, we obtain the embedding set of u and i with u k a and v k i for k = [0, 1, 2, ..., K]. Then, for each user a, her final embedding is denoted as: u * a = [u 0 a ||u 1 a ||...||u K a ] that concatenates her embedding at each layer. Similarly, each item i's final embedding is :</p><formula xml:id="formula_26">v * i = [v 0 i ||v 1 i ||...||v K i ].</formula><p>After that, the predicted rating is modeled as the inner product between the final user and item embeddings <ref type="bibr" target="#b6">[7]</ref>:</p><formula xml:id="formula_27">rai = [u 0 a ||u 1 a ||...||u K a ] T [v 0 i ||v 1 i ||...||v K i ].<label>(19)</label></formula><p>Please note that, some previous works directly use the K-th layer embedding for prediction layer as rai = [u K a ] T V K i . Recently, researchers found that if we use the K-th layer embedding, GCN based approaches are proven to over-smoothing issue as K increases <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b51">[52]</ref>. In this paper, to tackle the over-smoothing problem, we adopt the prediction layer as the LR-GCCF model, which receives state-of-the-art performance with user-item bipartite graph structure <ref type="bibr" target="#b6">[7]</ref>. In LR-GCCF, Chen et al. carefully analyzed the simple concatenation of entity embedding at each layer is equivalent to residual preference learning, and why this simple operation could alleviate the over-smoothing issue <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Training</head><p>We use a pair-wise ranking based loss function for optimization, which is widely used for implicit feedback <ref type="bibr" target="#b36">[37]</ref>:</p><formula xml:id="formula_28">L = min ? (a,i)?R + ?(a,j)?R - -ln?(r ai -raj ) + ?||?|| 2 . (<label>20</label></formula><formula xml:id="formula_29">)</formula><p>where R + denotes the set of positive samples (observed user-item pairs), and R -denotes the set of negative samples (unobserved user-item pairs that randomly sampled from R). ?(x) is sigmoid function. For all the trainable parameters, we initialize them with the Gaussian distribution with a mean value of 0 and a standard deviation of 0.01. Besides, we do not deliberately adjust the dimensions of each embedding size in the convolutional layer, all of them keep the same size. As for the several MLPs in the multi-level attention network, we use two-layer structure. In the experiment part, we will give more detail descriptions about the parameter setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Matrix Formulation of DiffNet++</head><p>The key idea of our proposed DiffNet++ model is the well designed interest and influence diffusion layers. In fact, this part could be calculated in matrix forms. In the following, we would like to show how to update user and item embedding from the k-th layer to the (k + 1)-th layer with matrix operations. Let H (k+1) = [? k+1 ia ] ? R N ?M denote the matrix representation of attentive item aggregation weigth in Eq.( <ref type="formula" target="#formula_14">10</ref>), we have:</p><formula xml:id="formula_30">H = M LP1(U k , V k ).<label>(21)</label></formula><p>At the user side, given Eq.( <ref type="formula" target="#formula_17">12</ref>) ,let</p><formula xml:id="formula_31">A (k+1) = [? k+1 ab ] ? R M ?M , B (k+1) = [? k+1</formula><p>ia ] ? R M ?N denote the attentive weight matrices of social network (Eq.( <ref type="formula" target="#formula_18">13</ref>)) and interest network(Eq.( <ref type="formula" target="#formula_19">14</ref>)), i.e., the outputs of the node attention layer. We use ? (k+1) = [? k+1 al ] ? R M ?2 to denote the attentive weight matrix of the multi-level networks in Eq.( <ref type="formula" target="#formula_24">17</ref>) and Eq. <ref type="bibr" target="#b17">(18)</ref>. All these three attention matrices can be calculated similarly as shown above.</p><p>After learning the attention matrices, we could update user and item embeddings at the (k + 1)-th layer as:</p><formula xml:id="formula_32">U (k+1) V (k+1) = I 1 R. * B. * rm(?(:, 2), N ) R T . * H I 2 U (k) V (k)<label>(22)</label></formula><formula xml:id="formula_33">+ S. * A. * rm(?(:, 1), M ) 0 0 0 U (k) V (k)<label>(23)</label></formula><formula xml:id="formula_34">= I 1 + S. * A. * rm(?(:, 1), M ) R. * B. * rm(?(:, 2), N ) R T . * H I 2 U (k) V (k) , (<label>24</label></formula><formula xml:id="formula_35">)</formula><p>where I1 is an identity matrix with M rows, and I2 is an identity matrix with N rows. Moreover, ?(:, 1) and ?(:, 2) represent the first column and second column of matrix ?, . * denotes the dot product and rm(A, r 1 ) denotes an array that containing r 1 copies of A in the column dimensions. Based on the above matrix operations of the social and influence diffusion layers, DiffNet++ is easily implemented by current deep learning frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussion</head><p>Space complexity. As shown in Eq.( <ref type="formula" target="#formula_28">20</ref>), the model parameters are composed of two parts: the user and item free embeddings ?1=[P, Q], and the parameter set in the fusion layer and the attention modeling, i.e., ?2=[W1, W2, [M LPi]i=1,2,3,4]. Since most embedding based models (e.g., BPR <ref type="bibr" target="#b36">[37]</ref>, FM <ref type="bibr" target="#b35">[36]</ref>) need to store the embeddings of each user and each item, the space complexity of ? 1 is the same as classical embedding based models and grows linearly with users and items. For parameters in ?2, they are shared among all users and items, with the dimension of each parameter is far less than the number of users and items. In practice, we empirically find the two-layer MLP achieve the best performance. As such, this additional storage cost is a small constant that could be neglected. Therefore, the space complexity of DiffNet++ is the same as classical embedding models.</p><p>Time complexity. Compared to the classical matrix factorization based models, the additional time cost lies in the influence and interest diffusion layers. Given M users, N items and diffusion depth K, suppose each user directly connects to L s users and L i items on average, and each item directly connects to L u users. At each influence and interest diffusion layer, we need to first calculate the twolevel attention weight matrices as shown in Eq.( <ref type="formula" target="#formula_30">21</ref>), and then update user and item embeddings. Since in practice, MLP layers are very small (e.g., two layers), the time cost for attention modeling is about O(M (L s +L i )D+N L u D). After that, as shown in Eq.( <ref type="formula" target="#formula_34">24</ref>), the user and item update step also costs O(M (L s +L i )D +N L u D). Since there are K diffusion layers, the total additional time complexity for influence and interest diffusion layers are O(K(M (L s + L i ) + L u )D). In practice, as L s , L i , L u min{M, N }, the additional time is linear with users and items, and grows linearly with diffusion depth K. Therefore, the total time complexity is acceptable in practice.</p><p>Model Generalization. The proposed DiffNet++ model is designed under the problem setting with the input of user feature matrix X, item feature matrix Y, and the social network S. Specifically, the fusion layer takes users' (items') feature matrix for user (item) representation learning. The layer-wise diffusion layer utilizes the social network structure S and the interest network structure R to model how users' latent preferences are dynamically influenced from the recursive influence and interest diffusion process. Next, we would show that our proposed model is generally applicable when different kinds of data input are not available.</p><p>When the user (item) features are not available, the fusion layer disappears. In other words, as shown in Eq.( <ref type="formula" target="#formula_10">7</ref>), each item's latent embedding v 0 i degenerates to q i . Similarly, each user's initial layer-0 latent embedding u 0 = p a (Eq.( <ref type="formula" target="#formula_9">6</ref>)). Similarly, when either the user attributes or the item attributes do not exist, the corresponding fusion layer of user or item degenerates. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>Datasets. We conduct experiments on four real-world datasets: Yelp, Flickr, Epinions and Dianping.</p><p>Yelp is a well-known online location based social network, where users could make friends with others and review restaurants. We use the Yelp dataset that is publicly available 2 . Flickr 3 is an online image based social sharing platform for users to follow others and share image preferences. In this paper, we use the social image recommendation dataset that is crawled and published by authors in <ref type="bibr" target="#b41">[42]</ref>, with both the social network structure and users' rating records of images. Epinions is a social based product review platform and the dataset is introduced in <ref type="bibr" target="#b30">[31]</ref> and is publicly available 4 . Dianping is the largest Chinese location based social network, and we use this dataset that is crawled by authors in <ref type="bibr" target="#b25">[26]</ref>. This dataset is also publicly available 5 .</p><p>Among the four datasets, Yelp and Flickr are two datasets with user and item attributes, and are adopted as datasets of our previously proposed DiffNet model <ref type="bibr" target="#b42">[43]</ref>. The remaining two datasets of Epinions and Dianping do not contain user and item attributes. We use the same preprocessing steps of the four datasets. Specifically, as the original ratings are presented with detailed values, we transform the original scores to binary values. If the rating value is larger than 3, we transform it into 1, otherwise it equals 0. For both datasets, we filter out users that have less than 2 rating records and 2 social links and remove items which have been rated less than 2 times. We randomly select 10% of the data for the test. In the remaining 90% data, to tune the parameters, we select 10% from the training data as the validation set. We show an overview of the characteristics of the four datasets in Table <ref type="table" target="#tab_1">1</ref>. In this table, the last line shows whether the additional user and item attributes are available on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 2</head><p>Comparison of the baselines, with "F" represents feature input and "S" denotes the social network input. For the modeling process, we use OI and OS to denote the observed first-order interest network and social network for user embedding learning. We use "HS" to denote the higher-order social information for embedding learning, and "HI" to denote higher-order interest information for embedding learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Model Input User Embedding Ability   Baselines and Evaluation Metrics. To illustrate the effectiveness of our method, we compare DiffNet++ with competitive baselines, including classical CF models (BPR <ref type="bibr" target="#b36">[37]</ref>, FM <ref type="bibr" target="#b35">[36]</ref>), social based recommendation model (SocialMF <ref type="bibr" target="#b18">[19]</ref>, TrustSVD <ref type="bibr" target="#b13">[14]</ref>, ContextMF <ref type="bibr" target="#b20">[21]</ref>, CNSR <ref type="bibr" target="#b43">[44]</ref>), as well as the graph based recommendation models of GraphRec <ref type="bibr" target="#b9">[10]</ref>, PinSage <ref type="bibr" target="#b47">[48]</ref>, NGCF <ref type="bibr" target="#b40">[41]</ref>. Please note that, in PinSage, we take the user-item graph with both user and item features as input, in order to transform this model for the recommendation task. For our proposed models of DiffNet <ref type="bibr" target="#b42">[43]</ref> and DiffNet++, since both models are flexible and could be reduced to simpler versions without user and item features, we use DiffNet-nf and DiffNet++-nf to represent reduced versions of DiffNet and DiffNet++ when removing user and item features. For better illustration, we list the main characteristics of all these  <ref type="table" target="#tab_3">4</ref>, with our proposed models are listed with italic letters. Please note that, as BPR learns free user and item embeddings with the observed user-item ratings. Therefore, the first-order interest network is not learned in the emebedding modeling process. As can be seen from this paper, our proposed DiffNet++-nf and DiffNet++ are the only two models that consider both the higher-order social influence and higher-order interest network for social recommendation.</p><formula xml:id="formula_36">F S OI OS HI HS Classical CF BPR [37] ? ? ? ? ? ? FM [36] ? ? ? ? ? ? Social recom- mendation SocialMF [19] ? ? ? ? ? ? TrustSVD [14] ? ? ? ? ? ? ContextMF [21] ? ? ? ? ? ? CNSR [44] ? ? ? ? ? ? Graph neural network based recom- mendation GraphRec [10] ? ? ? ? ? ? PinSage [48] ? ? ? ? ? ? NGCF [41]</formula><p>For the top-N ranking evaluation, we use two widely used metrics, Hit Ratio (HR) <ref type="bibr" target="#b8">[9]</ref> and Normalized Discounted Cummulative Gain (NDCG) <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b42">[43]</ref>. Specifically, HR measures the percentage of hit items in the top-N list, and NDCG puts more emphasis on the top ranked items. As we focus on the top-N ranking performance with large itemset, similar as many other works <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b42">[43]</ref>, to evaluate the performance, for each user, we randomly select 1000 unrated items that a user has not interacted with as negative samples. Then, we mix these pseudo negative samples and corresponding positive samples (in the test set) to select top-N potential candidates. To reduce the uncertainty in this process, we repeat this procedure 5 times and report the average results.</p><p>Parameter Setting. For the regularization parameter ? in Eq. <ref type="bibr" target="#b19">(20)</ref>, we empirically try it in the range of [0.0001, 0.001, 0.01, 0.1] and finally set ? = 0.01 to get the best performance. For the fusion layer in Eq.( <ref type="formula" target="#formula_9">6</ref>) and Eq.( <ref type="formula" target="#formula_10">7</ref>), we first transform the each user (item) feature vector to the same free embedding space, and calculate as: u 0 a = W 1 ? x a + p a , and v 0 a = W 2 ? y i + q a . For attention modeling, we resort to MLP with two layers. For our proposed model, we initialize all of them with a Gaussian distribution with a mean value of 0 and the standard deviation of 0.01. We use the Adam optimizer for with an initial learning rate of 0.001, and the training batch size is 512. In the training process, as there are much more unobserved items for each user, we randomly select 8 times pseudo negative samples for each user at each iteration. Since each iteration we change the pseudo negative samples, each unobserved item gives very weak signal. For all the baselines, we carefully tune the parameters to ensure the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overall Performance Comparison</head><p>We show the overall performance of all models for top-10 recommendation with different embedding size D from Table <ref type="table" target="#tab_2">3</ref> to Table6. In Table <ref type="table" target="#tab_2">3</ref>, we show the comparisons on Yelp and Flickr, with the node attribute values are available. In Table <ref type="table" target="#tab_3">4</ref>, we depict the results on Epinions and Dianping without attribute values. On Epinions and Dianping, we do not report models that need to take attribute data as input. We notice that besides BPR, nearly all models show better performance with the increase of dimension D. All models improve over BPR, which only leverages the observed user-item rating matrix for recommendation, and suffer the data sparsity issue in practice.</p><p>TrustSVD and SocialMF utilize social neighbors of each user as auxiliary information to alleviate this problem. GraphRec further improves over these traditional social recommendation models by jointly considering the first-order social neighbors and interest neighbors in the user embedding process. However, GraphRec only models the first-order relationships of two graphs for user embedding learning, with the higher-order graph structures are neglected. For GCN based models, PinSage and NGCF model the higherorder user-item graph structure, and DiffNet models the higher-order social structure. These graph neural models beat matrix based baselines by a large margin, showing the effectiveness in leveraging higher-order graph structure for recommendation. Our proposed DiffNet++ model always performs the best under any dimension D, indicating the effectiveness of modeling the recursive diffusion process in the social interest network. Besides, we observe DiffNet++ and DiffNet always show better performance compared to their counterparts that do not model the user and item features, showing the effectiveness of injecting both feature and latent embeddings in the fusion layer. We further compare the performance of different models with different top-N values in Table <ref type="table" target="#tab_5">5</ref> and Table <ref type="table" target="#tab_6">6</ref>, and the overall trend is the same as analyzed before. Therefore, we could empirically conclude the superiority of our proposed models. As nearly all models showed better performance at D = 64, we would use this setting to show the comparison of different models in the following analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Under Different Sparsity</head><p>In this part, we would like to investigate how different models perform under different rating sparsity. Specifically, we first group users into different interest groups based on the number of observed ratings of each user. E.g, <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16)</ref> means each user has at least 8 rating records and less than 16    <ref type="figure" target="#fig_4">3</ref>(a) and Fig. <ref type="figure" target="#fig_4">3</ref>(b) respectively. From both datasets, we observe that as users have more ratings, the overall performance increases among all models. This is quite reasonable as all models could have more user behavior data for user embedding modeling. Our proposed models consistently improve all baselines, and especially show larger improvements on sparser dataset. E.g., when users have less than 8 rating records, DiffNet++ improves 22.4% and 45.0% over the best baseline on Yelp and Flickr respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Detailed Model Analysis</head><p>Diffusion Depth K. The number of layer K is very important, as it determines the diffusion depth of different graphs. We show the results of different K values for two datasets with attributes in Table <ref type="table" target="#tab_7">7</ref>. The column of "Improve" shows the performance changes compared to the best setting, i.e., K = 2. When K increases from 0 to 1, the performance increases quickly (DiffNet++ degenerates to BPR when K = 0), and it achieves the best performance when K = 2. However, when we continue to increase the layer to 3, the performance drops. We empirically conclude that 2-hop higher-order social interest graph structure is enough for social recommendation. And adding more layers may introduce unnecessary neighbors in this process, leading to performance decrease. Other related studies have also empirically found similar trends <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b47">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Effects of Multi-level Attention.</head><p>A key characteristic of our proposed model is the multi-level attention modeling by fusing the social network and interest network for recommendation. In this subsection, we discuss the effects of different attention mechanisms. We show the results of different attention modeling combinations in in Table <ref type="table" target="#tab_8">8</ref>, with "AVG" means we directly set the equal attention weights without any attention learning process. As can be observed from this table, either the node level attention or the graph level attention modeling could improve the recommendation results, with the graph level attention shows better results. When combing both the node level attention and the graph level attention, the performance can be further improved. E.g., for the Flickr dataset, the graph level attention improves more than 4% compared to the results of the average attention, and combining the node level attention further improves about 2%. However, the improvement of attention modeling varies in different datasets, with the results of the Yelp dataset is not as significant as the Flickr dataset. This observation implies that the usefulness of considering the importance strength of different elements in the modeling process varies, and our proposed multilevel attention modeling could adapt to different datasets' requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Value Analysis.</head><p>For each user a at layer k, the graph level attention weights of ? k a1 and ? k a2 denote the social influence diffusion weight and the interest diffusion weight. A larger value of ? k a1 indicates the social influence diffusion process is more important to capture the user embedding learning with less influence from the interest network. In Table <ref type="table" target="#tab_9">9</ref>, we show the learned mean and variance of all users' attention weights at the graph level at each layer k. Since both datasets receive the best performance at K = 2, we show the attention weights at the first diffusion layer (k = 1) and the second diffusion layer k = 2. There are several interesting findings. First, we observe for both datasets, at the first diffusion layer with k = 1, the average value of the social influence strength ? 1 a1 are very high, indicating the first-order social neighbors play a very important role in representing each user's first layer representation. This is quite reasonable as users' rating behavior are very sparse, and leveraging the first order social neighbors could largely improve the recommendation performance. When k = 2, the average social influence strength ? 2 a1 varies among the two datasets, with the Yelp dataset shows a larger average social  Runtime. In Table <ref type="table" target="#tab_10">10</ref>, we show the runtime of each model on four datasets. Among the four datastes, Epinions and Dianping do not have any attribute information. For fair comparison, we perform experiments on a same server. The server has an Intel i9 CPU, 2 Titan RTX 24G, and 64G memory. The classical BPR model costs the least time, followed by the shallow latent factor based social recommendation models of SocailMF and TrustSVD. CNSR has longer runtime as it needs to update both the user embedding learned from user-item behavior, as well as the social embedding learned from user-user behavior. The graph based models cost more time than classical models. Specifically, NGCF and DiffNet have similar time complexity as they capture either the interest diffusion or influence diffusion. By injecting both the interest diffusion and influence diffusion process, DiffNet++ costs more time than these two neural graph models. GraphRec costs the most time on the two datasets without attributes. The reason is that, though GraphRec only considers one-hop graph structure, it adopts a deep neural architecture for modeling the complex interactions between users and items. As we need to use the deep neural architecture for each user-item rating record, GraphRec costs more time than the inner-product based prediction function in DiffNet++. On Yelp and Flickr, these two datasets have attribute information as input, and the DiffNet++ model needs the fusion layer to fuse attribute and free embeddings, while GraphRec does not have any attribute fusion. Therefore, DiffNet++ costs more time than GraphRec on the two datasets with attributes. The average training time of DiffNet++ on the largest Dianping dataset is about 25 seconds for one epoch, and it usually takes less than 100 epoches to reach convergence. Therefore, the total runtime of DiffNet++ is less than 1 hour on the largest dataset, which is also very time efficient. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS AND FUTURE WORK</head><p>In this paper, we presented a neural social and interest diffusion based model, i.e., DiffNet++, for social recommendation. We argued that, as users play a central role in social network and interest network, jointly modeling the higher-order structure of these two networks would mutually enhance each other. By formulating the social recommendation as a heterogeneous graph, we recursively learned the user embedding from convolutions on user social neighbors and interest neighbors, such that both the higher-order social structure and higher-order interest network are directly injected in the user modeling process. Furthermore, we designed a multi-level attention network to attentively aggregate the graph and node level representations for better user modeling. Experimental results on two real-world datasets clearly showed the effectiveness of our proposed model. In the future, we would like to explore the graph reasoning models to explain the paths for users' behaviors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The overall structure of the DiffNet++ model. As shown in the graph, we use Node ATT to denote the node level attention layer in each graph, and Graph ATT to denote the graph attention layer when fusing the interest graph representation and social graph representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>ab = 1</head><label>1</label><figDesc>|Sa| , and ? (k+1) ai = 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Yelp dataset. (b) Flickr dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Performance under different rating sparsity on two datasets.</figDesc><graphic url="image-145.png" coords="11,93.48,46.06,209.89,106.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>our model, with ?1 = [P, Q], and the parameter set in the fusion layer and the multi-level attention modeling, i.e., ?2 = [W1, W2, [M LPi]i=1,2,3,4]. All the parameters in the above loss function are differentiable.</figDesc><table /><note><p>? = [?1, ?2] is the regularization parameters in</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>The statistics of the four datasets after preprocessing.</figDesc><table><row><cell>Dataset</cell><cell>Yelp</cell><cell>Flickr</cell><cell cols="2">Epinions Dianping</cell></row><row><cell>Users</cell><cell>17,237</cell><cell>8,358</cell><cell>18,202</cell><cell>59,426</cell></row><row><cell>Items</cell><cell>38,342</cell><cell>82,120</cell><cell>47,449</cell><cell>10,224</cell></row><row><cell>Ratings</cell><cell cols="2">204,448 327,815</cell><cell>298,173</cell><cell>934,334</cell></row><row><cell>Links</cell><cell cols="2">143,765 187,273</cell><cell>381,559</cell><cell>813,331</cell></row><row><cell>Rating Density</cell><cell>0.03%</cell><cell>0.05%</cell><cell>0.03%</cell><cell>0.12%</cell></row><row><cell>Link Density Attributes</cell><cell>0.05% ?</cell><cell>0.27% ?</cell><cell>0.15% ?</cell><cell>0.02% ?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3</head><label>3</label><figDesc>Overall comparison with different dimension size D on Yelp and Flickr (attributes are available).</figDesc><table><row><cell></cell><cell>Yelp</cell><cell></cell><cell>Flickr</cell><cell></cell></row><row><cell>Model</cell><cell>HR</cell><cell>NDCG</cell><cell>HR</cell><cell>NDCG</cell></row><row><cell></cell><cell cols="4">D=16 D=32 D=64 D=16 D=32 D=64 D=16 D=32 D=64 D=16 D=32 D=64</cell></row><row><cell>BPR</cell><cell cols="4">0.2435 0.2616 0.2632 0.1468 0.1573 0.1554 0.0773 0.0812 0.0795 0.0611 0.0652 0.0628</cell></row><row><cell>FM</cell><cell cols="4">0.2768 0.2835 0.2825 0.1698 0.1720 0.1717 0.1115 0.1212 0.1233 0.0872 0.0968 0.0954</cell></row><row><cell>SocialMF</cell><cell cols="4">0.2571 0.2709 0.2785 0.1655 0.1695 0.1677 0.1001 0.1056 0.1174 0.0862 0.0910 0.0964</cell></row><row><cell>TrustSVD</cell><cell cols="4">0.2826 0.2854 0.2939 0.1683 0.1710 0.1749 0.1352 0.1341 0.1404 0.1056 0.1039 0.1083</cell></row><row><cell cols="5">ContextMF 0.2985 0.3011 0.3043 0.1758 0.1808 0.1818 0.1405 0.1382 0.1433 0.1085 0.1079 0.1102</cell></row><row><cell>CNSR</cell><cell cols="4">0.2702 0.2817 0.2904 0.1723 0.1745 0.1746 0.1146 0.1198 0.1229 0.0913 0.0942 0.0978</cell></row><row><cell>GraphRec</cell><cell cols="4">0.2873 0.2910 0.2912 0.1663 0.1677 0.1812 0.1195 0.1211 0.1231 0.0910 0.0924 0.0930</cell></row><row><cell>PinSage</cell><cell cols="4">0.2944 0.2966 0.3049 0.1753 0.1786 0.1855 0.1192 0.1234 0.1257 0.0937 0.0986 0.0998</cell></row><row><cell>NGCF</cell><cell cols="4">0.3050 0.3068 0.3042 0.1826 0.1844 0.1828 0.1110 0.1150 0.1189 0.0880 0.0895 0.0945</cell></row><row><cell>DiffNet-nf</cell><cell cols="4">0.3126 0.3156 0.3195 0.1854 0.1882 0.1928 0.1342 0.1317 0.1408 0.1040 0.1034 0.1089</cell></row><row><cell>DiffNet</cell><cell cols="4">0.3293 0.3437 0.3461 0.1982 0.2095 0.2118 0.1476 0.1588 0.1657 0.1121 0.1242 0.1271</cell></row><row><cell cols="5">DiffNet++-nf 0.3194 0.3199 0.3230 0.1914 0.1944 0.1942 0.1410 0.1480 0.1503 0.1100 0.1132 0.1169</cell></row><row><cell>DiffNet++</cell><cell cols="4">0.3406 0.3552 0.3694 0.2070 0.2158 0.2263 0.1562 0.1678 0.1832 0.1213 0.1286 0.1420</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4</head><label>4</label><figDesc>Overall comparison with different dimension size D on Epinions and Dianping (attributes are not available).</figDesc><table><row><cell></cell><cell>Epinions</cell><cell></cell><cell>Dianping</cell><cell></cell></row><row><cell>Model</cell><cell>HR</cell><cell>NDCG</cell><cell>HR</cell><cell>NDCG</cell></row><row><cell></cell><cell cols="4">D=16 D=32 D=64 D=16 D=32 D=64 D=16 D=32 D=64 D=16 D=32 D=64</cell></row><row><cell>BPR</cell><cell cols="4">0.2620 0.2732 0.2822 0.1702 0.1788 0.1812 0.2160 0.2302 0.2299 0.1286 0.1326 0.1319</cell></row><row><cell cols="5">SocialMF 0.2720 0.2842 0.2893 0.1732 0.1824 0.1857 0.2325 0.2345 0.2410 0.1360 0.1377 0.1416</cell></row><row><cell cols="5">TrustSVD 0.2726 0.2854 0.2884 0.1773 0.1839 0.1848 0.2364 0.2371 0.2341 0.1381 0.1401 0.1390</cell></row><row><cell>CNSR</cell><cell cols="4">0.2757 0.2874 0.2898 0.1748 0.1856 0.1876 0.2356 0.2377 0.2418 0.1394 0.1413 0.1435</cell></row><row><cell cols="5">GraphRec 0.3093 0.3117 0.3156 0.1994 0.2016 0.2051 0.2408 0.2541 0.2622 0.1412 0.1503 0.1556</cell></row><row><cell cols="5">PinSage 0.2980 0.3003 0.3073 0.1911 0.1933 0.1928 0.2353 0.2452 0.2552 0.1390 0.1434 0.1489</cell></row><row><cell>NGCF</cell><cell cols="4">0.3029 0.3065 0.3192 0.1977 0.2008 0.1958 0.2489 0.2586 0.2584 0.1470 0.1503 0.1534</cell></row><row><cell>DiffNet</cell><cell cols="4">0.3242 0.3281 0.3407 0.2007 0.2054 0.2191 0.2522 0.2600 0.2645 0.1483 0.1521 0.1555</cell></row><row><cell cols="2">DiffNet++ 0.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>3367 0.3434 0.3503 0.2158 0.2217 0.2288 0.2676 0.2682 0.2713 0.1593 0.1589 0.1605</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5</head><label>5</label><figDesc>Overall comparison with different top-N values (D=64) on Yelp and Flickr (attributes are available).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Yelp</cell><cell></cell><cell></cell><cell cols="2">Flickr</cell></row><row><cell>Model</cell><cell></cell><cell>HR</cell><cell></cell><cell>NDCG</cell><cell></cell><cell>HR</cell><cell></cell><cell>NDCG</cell></row><row><cell></cell><cell>N=5</cell><cell>N=10 N=15</cell><cell>N=5</cell><cell>N=10 N=15</cell><cell>N=5</cell><cell>N=10 N=15</cell><cell>N=5</cell><cell>N=10 N=15</cell></row><row><cell>BPR</cell><cell cols="8">0.1695 0.2632 0.3252 0.1231 0.1554 0.1758 0.0651 0.0795 0.1037 0.0603 0.0628 0.0732</cell></row><row><cell>FM</cell><cell cols="8">0.1855 0.2825 0.3440 0.1341 0.1717 0.1876 0.0989 0.1233 0.1473 0.0866 0.0954 0.1062</cell></row><row><cell>SocialMF</cell><cell cols="8">0.1739 0.2785 0.3365 0.1324 0.1677 0.1841 0.0813 0.1174 0.1300 0.0723 0.0964 0.1061</cell></row><row><cell>TrustSVD</cell><cell cols="8">0.1882 0.2939 0.3688 0.1368 0.1749 0.1981 0.1089 0.1404 0.1738 0.0978 0.1083 0.1203</cell></row><row><cell cols="9">ContextMF 0.2045 0.3043 0.3832 0.1484 0.1818 0.2081 0.1095 0.1433 0.1768 0.0920 0.1102 0.1131</cell></row><row><cell>CNSR</cell><cell cols="8">0.1877 0.2904 0.3458 0.1389 0.1746 0.1912 0.0920 0.1229 0.1445 0.0791 0.0978 0.1057</cell></row><row><cell>GraphRec</cell><cell cols="8">0.1915 0.2912 0.3623 0.1279 0.1812 0.1956 0.0931 0.1231 0.1482 0.0784 0.0930 0.0992</cell></row><row><cell>PinSage</cell><cell cols="8">0.2105 0.3049 0.3863 0.1539 0.1855 0.2137 0.0934 0.1257 0.1502 0.0844 0.0998 0.1046</cell></row><row><cell>NGCF</cell><cell cols="8">0.1992 0.3042 0.3753 0.1450 0.1828 0.2041 0.0891 0.1189 0.1399 0.0819 0.0945 0.0998</cell></row><row><cell>DiffNet-nf</cell><cell cols="8">0.2101 0.3195 0.3982 0.1535 0.1928 0.2164 0.1087 0.1408 0.1709 0.0979 0.1089 0.1192</cell></row><row><cell>DiffNet</cell><cell cols="8">0.2276 0.3461 0.4217 0.1679 0.2118 0.2307 0.1178 0.1657 0.1855 0.1072 0.1271 0.1301</cell></row><row><cell cols="9">DiffNet++-nf 0.2112 0.3230 0.3989 0.1551 0.1942 0.2176 0.1140 0.1503 0.1799 0.1021 0.1169 0.1256</cell></row><row><cell>DiffNet++</cell><cell cols="8">0.2503 0.3694 0.4493 0.1841 0.2263 0.2497 0.1412 0.1832 0.2203 0.1269 0.1420 0.1544</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 6</head><label>6</label><figDesc>Overall comparison with different top-N values (D=64)on Epinions and Dianping (attributes is not available).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Epinions</cell><cell></cell><cell></cell><cell cols="2">Dianping</cell></row><row><cell>Model</cell><cell></cell><cell>HR</cell><cell></cell><cell>NDCG</cell><cell></cell><cell>HR</cell><cell></cell><cell>NDCG</cell></row><row><cell></cell><cell>N=5</cell><cell>N=10 N=15</cell><cell>N=5</cell><cell>N=10 N=15</cell><cell>N=5</cell><cell>N=10 N=15</cell><cell>N=5</cell><cell>N=10 N=15</cell></row><row><cell>BPR</cell><cell cols="8">0.2005 0.2822 0.3256 0.1526 0.1812 0.1917 0.1412 0.2299 0.2864 0.1024 0.1319 0.1482</cell></row><row><cell cols="9">SocialMF 0.2098 0.2893 0.3431 0.1575 0.1857 0.2016 0.1546 0.2410 0.3063 0.1111 0.1416 0.1608</cell></row><row><cell cols="9">TrustSVD 0.2102 0.2884 0.3396 0.1574 0.1848 0.2001 0.1521 0.2341 0.2966 0.1100 0.1390 0.1574</cell></row><row><cell>CNSR</cell><cell cols="8">0.2151 0.2898 0.3444 0.1592 0.1876 0.2035 0.1564 0.2418 0.3077 0.1132 0.1435 0.1621</cell></row><row><cell cols="9">GraphRec 0.2335 0.3156 0.3620 0.1764 0.2051 0.2199 0.1725 0.2622 0.3300 0.1240 0.1556 0.1755</cell></row><row><cell cols="9">PinSage 0.2207 0.3073 0.3073 0.1589 0.1908 0.2008 0.1631 0.2552 0.3177 0.1141 0.1489 0.1664</cell></row><row><cell>NGCF</cell><cell cols="8">0.2308 0.3192 0.3777 0.1706 0.1958 0.2131 0.1695 0.2584 0.3263 0.1220 0.1534 0.1733</cell></row><row><cell>DiffNet</cell><cell cols="8">0.2457 0.3407 0.3967 0.1857 0.2191 0.2357 0.1734 0.2645 0.3302 0.1235 0.1555 0.1748</cell></row><row><cell cols="9">DiffNet++ 0.2602 0.3503 0.4051 0.1973 0.2288 0.2450 0.1798 0.2713 0.3375 0.1281 0.1605 0.1802</cell></row><row><cell>models in Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 7 HR</head><label>7</label><figDesc>@10 and NDCG@10 performance with different diffusion depth K (D = 64). Then, we calculate the average performance of each interest group. The sparsity analysis on Yelp dataset and Flickr dataset are shown in Fig.</figDesc><table><row><cell>Depth K</cell><cell>HR</cell><cell cols="3">Yelp Improve NDCG Improve</cell><cell>HR</cell><cell cols="3">Flickr Improve NDCG Improve</cell></row><row><cell>K = 2</cell><cell>0.3694</cell><cell>-</cell><cell>0.2263</cell><cell>-</cell><cell>0.1832</cell><cell>-</cell><cell>0.1420</cell><cell>-</cell></row><row><cell>K = 0</cell><cell cols="2">0.2632 -28.32%</cell><cell>0.1554</cell><cell cols="3">-30.81% 0.0795 -55.21%</cell><cell>0.0628</cell><cell>-53.86%</cell></row><row><cell>K = 1</cell><cell>0.3566</cell><cell>-2.89%</cell><cell>0.2159</cell><cell>-3.87%</cell><cell>0.1676</cell><cell>-5.58%</cell><cell>0.1283</cell><cell>-5.73%</cell></row><row><cell>K = 3</cell><cell>0.3626</cell><cell>-1.25%</cell><cell>0.2215</cell><cell>-1.38%</cell><cell>0.1743</cell><cell>-1.80%</cell><cell>0.1347</cell><cell>-1.03%</cell></row><row><cell>rating records.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 8 HR</head><label>8</label><figDesc>@10 and NDCG@10 performance with different attentional variants (D = 64).</figDesc><table><row><cell cols="2">Graph Attention Node Attention</cell><cell>HR</cell><cell cols="3">Yelp Improve NDCG Improve</cell><cell>HR</cell><cell cols="3">Flickr Improve NDCG Improve</cell></row><row><cell>AVG</cell><cell>AVG</cell><cell>0.3631</cell><cell>-</cell><cell>0.2224</cell><cell>-</cell><cell>0.1733</cell><cell>-</cell><cell>0.1329</cell><cell>-</cell></row><row><cell>AVG</cell><cell>ATT</cell><cell cols="8">0.3657 +0.72% 0.2235 +0.49% 0.1792 +3.40% 0.1368 +2.93%</cell></row><row><cell>ATT</cell><cell>AVG</cell><cell cols="8">0.3662 +0.85% 0.2249 +1.12% 0.1814 +4.67% 0.1387 +4.36%</cell></row><row><cell>ATT</cell><cell>ATT</cell><cell cols="8">0.3694 +1.74% 0.2263 +1.75% 0.1832 +5.71% 0.1420 +6.85%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 9</head><label>9</label><figDesc>Mean statistics of the graph level attention values (K = 2), with ? k a1 is the social influence weight and ? k a2 is the interest weight. while the Flickr dataset shows a larger interest influence weight with quite small value of average social influence weight. We guess a possible reason is that, as shown in Table1, Flickr dataset shows denser social links compared to the Yelp dataset, with a considerable amount of directed social links at the first diffusion layer, the average weight of the second layer social neighbors decreases.</figDesc><table><row><cell>Layer k</cell><cell>Social ? k a1</cell><cell>Yelp Interest ? k a2</cell><cell cols="2">Flickr a1 Social ? k Interest ? k a1</cell></row><row><cell>k=1</cell><cell>0.7309</cell><cell>0.2691</cell><cell>0.8381</cell><cell>0.1619</cell></row><row><cell>k=2</cell><cell>0.6888</cell><cell>0.3112</cell><cell>0.0727</cell><cell>0.9273</cell></row><row><cell cols="2">influence weight,</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 10</head><label>10</label><figDesc>Average one epoch runtime of each model on the two largest datasets (seconds).</figDesc><table><row><cell>Model</cell><cell cols="4">Yelp Flickr Epinions Dianping</cell></row><row><cell>BPR</cell><cell>1.51</cell><cell>1.75</cell><cell>2.34</cell><cell>3.86</cell></row><row><cell>FM</cell><cell>1.94</cell><cell>1.87</cell><cell>\</cell><cell>\</cell></row><row><cell>SocailMF</cell><cell>1.74</cell><cell>3.38</cell><cell>2.41</cell><cell>6.83</cell></row><row><cell>TrustSVD</cell><cell>1.90</cell><cell>3.44</cell><cell>2.60</cell><cell>8.10</cell></row><row><cell>ContextMF</cell><cell>1.83</cell><cell>3.51</cell><cell>\</cell><cell>\</cell></row><row><cell>CNSR</cell><cell>2.55</cell><cell>3.52</cell><cell>4.26</cell><cell>13.24</cell></row><row><cell>GraphRec</cell><cell>4.33</cell><cell>5.65</cell><cell>5.98</cell><cell>42.98</cell></row><row><cell>PinSage</cell><cell>4.07</cell><cell>3.58</cell><cell>3.48</cell><cell>19.28</cell></row><row><cell>NGCF</cell><cell>4.07</cell><cell>3.59</cell><cell>3.28</cell><cell>20.38</cell></row><row><cell>DiffNet</cell><cell>2.65</cell><cell>3.42</cell><cell>3.15</cell><cell>15.66</cell></row><row><cell>DiffNet++</cell><cell>7.72</cell><cell>7.21</cell><cell>4.69</cell><cell>25.62</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>This work was supported in part by grants from the <rs type="funder">National Natural Science Foundation of China</rs>( Grant No. <rs type="grantNumber">U19A2079</rs>, <rs type="grantNumber">U1936219</rs>, <rs type="grantNumber">91846201</rs>, <rs type="grantNumber">61725203</rs>, <rs type="grantNumber">61732008</rs>, <rs type="grantNumber">61722204</rs>, <rs type="grantNumber">61932009</rs>), and the <rs type="funder">Foundation of Key Laboratory of Cognitive Intelligence, iFLYTEK, P.R., Chia</rs>(Grant No. <rs type="grantNumber">COGOS-20190002</rs>), and <rs type="funder">CAAI-Huawei MindSpore Open Fund</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_PQGWgNz">
					<idno type="grant-number">U19A2079</idno>
				</org>
				<org type="funding" xml:id="_2GUXsWH">
					<idno type="grant-number">U1936219</idno>
				</org>
				<org type="funding" xml:id="_yEcQRRn">
					<idno type="grant-number">91846201</idno>
				</org>
				<org type="funding" xml:id="_yTgDurC">
					<idno type="grant-number">61725203</idno>
				</org>
				<org type="funding" xml:id="_Y8N7h5Z">
					<idno type="grant-number">61732008</idno>
				</org>
				<org type="funding" xml:id="_QBJn6Qk">
					<idno type="grant-number">61722204</idno>
				</org>
				<org type="funding" xml:id="_bECYNYd">
					<idno type="grant-number">61932009</idno>
				</org>
				<org type="funding" xml:id="_SJCAN8m">
					<idno type="grant-number">COGOS-20190002</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Adomavicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tuzhilin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="734" to="749" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Influence and correlation in social networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Anagnostopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahdian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="7" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graph convolutional matrix completion</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V D</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An efficient adaptive transfer neural network for social-aware recommendation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="225" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Revisiting graph based collaborative filtering: A linear residual graph convolutional network approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Item-based top-n recommendation algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOIS</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="143" to="177" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph neural networks for social recommendation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep social collaborative filtering</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recsys</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="305" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A structural theory of social influence</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Friedkin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adaptive edge features guided graph attention networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
		<idno>ArXiv, abs/1809.02709</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Trustsvd: collaborative filtering with both the explicit and implicit influence of user trust and of item ratings</title>
		<author>
			<persName><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yorke-Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="123" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<author>
			<persName><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yorke-Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A novel recommendation model regularized with user trust and item ratings</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1607" to="1620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nais: Neural attentive item similarity model for recommendation</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2354" to="2366" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A matrix factorization technique with trust propagation for recommendation in social networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jamali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recsys</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="135" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Social contextual recommendation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="45" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scalable recommendation with social contextual information</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2789" to="2802" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Factorization meets the neighborhood: a multifaceted collaborative filtering model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Experimental evidence of massive-scale emotional contagion through social networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Guillory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PNAS</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="8788" to="8790" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Social selection and peer influence in an online social network</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaufman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PNAS</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="68" to="72" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Overlapping community regularization for rating prediction in social recommender systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mamoulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recsys</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">xdeepfm: Combining explicit and implicit feature interactions for recommender systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1754" to="1763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Influence maximization over large-scale social networks: A bounded linear approach</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recommender systems with social regularization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Trust-aware recommender systems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Avesani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recsys</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Probabilistic matrix factorization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Geometric matrix completion with recurrent multi-graph neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3697" to="3707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Personalized recommendation combining user interest and social circle</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1763" to="1777" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deepinf: Social influence prediction with deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2110" to="2119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bpr: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attentive recurrent social recommendation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="185" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Exploiting local and global social context for recommendation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2712" to="2718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Neural graph collaborative filtering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A hierarchical attention model for social contextual image recommendation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1754" to="1867" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A neural influence diffusion model for social recommendation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="235" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Collaborative neural social recommendation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TSMC: Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Dual graph attention networks for deep latent representation of multifaceted social effects in recommender systems</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="2091" to="2102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">STAR-GCN: stacked and reconstructed graph convolutional networks for recommender systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4264" to="4270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Leveraging social connections to improve personalized ranking for collaborative filtering</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Spectral collaborative filtering</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recsys</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="311" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Le Wu is currently an associate professor at the Hefei University of Technology (HFUT), China. She received the Ph.D. degree from the University of Science and Technology of China (USTC)</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
	</analytic>
	<monogr>
		<title level="m">Graph neural networks: A review of methods and applications</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Her general area of research interests is data mining, recommender systems and social network analysis. She has published more than 40 papers in referred journals and conferences. Dr. Le Wu is the recipient of the Best of SDM 2015 Award, and the Distinguished Dissertation Award from China Association for Artificial Intelligence (CAAI) 2017</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
