<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Privacy-Preserving Collaborative Deep Learning with Unreliable Participants</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-10-25">25 Oct 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lingchen</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Qian</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Qin</forename><surname>Zou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Yanjiao</forename><surname>Chen</surname></persName>
						</author>
						<title level="a" type="main">Privacy-Preserving Collaborative Deep Learning with Unreliable Participants</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-10-25">25 Oct 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1812.10113v3[cs.CR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Collaborative learning</term>
					<term>deep learning</term>
					<term>privacy</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With powerful parallel computing GPUs and massive user data, neural-network-based deep learning can well exert its strong power in problem modeling and solving, and has archived great success in many applications such as image classification, speech recognition and machine translation etc. While deep learning has been increasingly popular, the problem of privacy leakage becomes more and more urgent. Given the fact that the training data may contain highly sensitive information, e.g., personal medical records, directly sharing them among the users (i.e., participants) or centrally storing them in one single location may pose a considerable threat to user privacy.</p><p>In this paper, we present a practical privacy-preserving collaborative deep learning system that allows users to cooperatively build a collective deep learning model with data of all participants, without direct data sharing and central data storage. In our system, each participant trains a local model with their own data and only shares model parameters with the others. To further avoid potential privacy leakage from sharing model parameters, we use functional mechanism to perturb the objective function of the neural network in the training process to achieve -differential privacy. In particular, for the first time, we consider the existence of unreliable participants, i.e., the participants with low-quality data, and propose a solution to reduce the impact of these participants while protecting their privacy. We evaluate the performance of our system on two well-known real-world datasets for regression and classification tasks. The results demonstrate that the proposed system is robust against unreliable participants, and achieves high accuracy close to the model trained in a traditional centralized manner while ensuring rigorous privacy protection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Qian Wang's research is supported in part by the NSFC under Grants 61822207 and U1636219, the Equipment Pre-Research Joint Fund of Ministry of Education of China (Youth Talent) under Grant 6141A02033327, the Outstanding Youth Foundation of Hubei Province under Grant 2017CFA047, and the Fundamental Research Funds for the Central Universities under Grant 2042019kf0210. Qin Zou's research is supported in part by the NSFC under Grants 61872277 and 41571437, the Natural Science Foundation of Hubei Province under Grant 2018CFB482. Yanjiao Chen's research is supported by the NSFC under Grant 61702380, the Natural Science Foundation of Hubei Province under Grant 2017CFB134, the Hubei Provincial Technological Innovation Special Funding Major Projects under Grant 2017AAA125. (Corresponding author: Qin <ref type="bibr">Zou)</ref> L. Zhao and Q. Wang are with the School of Cyber Science and Engineering, Wuhan University, Wuhan 430072, Hubei, China, and also with The State Key Laboratory of Cryptography, P.O. Box 5159, Beijing 100878, China. Email: {lczhaocs, qianwang}@whu.edu.cn.</p><p>Q. Zou and Y. Chen are with the School of Computer Science, Wuhan University, Wuhan 430072, Hubei, China. Email: {qzou, chenyan-jiao}@whu.edu.cn.</p><p>Y. Zhang is with the Huawei Technologies Co., Ltd, Shenzhen 518129, Guangdong, China. Email: zhangyan113@huawei.com. Work done primarily at the School of Computer Science, Wuhan University, China I N the past few years, deep learning has demonstrated largely improved performance over traditional machinelearning methods in various applications, e.g., image understanding <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, speech recognition <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, cancer analysis <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, and the game of Go <ref type="bibr" target="#b6">[7]</ref>. The great success of deep learning is owing to the development of powerful computing processor and the availability of massive data for training the neural networks. In general, the deep learning model will be more accurate if trained with more diverse data, and it motivates companies and institutions to collect as much data as possible from their users. These data are usually generated by sensors on users' personal devices, e.g., GPS, cameras, smartphones, and heart rate sensors <ref type="bibr" target="#b7">[8]</ref> etc. From the perspective of privacy, however, user-generated data is usually highly sensitive, e.g., location information, personal medical records, and social relationships etc. To gather these sensitive data at a centralized location will raise serious concerns about privacy leakage. A recent regulation of EU <ref type="bibr" target="#b8">[9]</ref> stipulates that companies should carefully collect and use users' personal data, and users have the right to require the company to permanently 'forget' their data. The bill also prohibits any automated individual decision-making (e.g., personal financial situation, personal health condition, and location prediction) based on the data, and it may greatly affect the machinelearning tasks performed by companies. In addition, in many sectors especially medical industry, sharing personal data is forbidden by laws or regulations.</p><p>To gain the benefit of machine learning while protecting the user privacy, there is a rising interest in designing privacyassured machine learning algorithms from both academia and industry. Existing solutions for traditional machine learning algorithms mainly exploit intrinsic features of the algorithms, e.g., strict convex objective functions. Privacy-preserving techniques such as secure multi-party computation or differential privacy have been applied to linear and logistic regression analysis <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, k-means clustering <ref type="bibr" target="#b11">[12]</ref>, support vector machines <ref type="bibr" target="#b12">[13]</ref>, and crowd machine learning <ref type="bibr" target="#b13">[14]</ref>. In recent years, privacy-preserving deep learning has received much attention from the research community. In <ref type="bibr" target="#b14">[15]</ref>, in order to hide the private data, only the intermediate representations obtained by a local neural network are published. However, this scheme did not provide a rigorous privacy guarantee since some sensitive information can be inferred from intermediate features. In <ref type="bibr" target="#b15">[16]</ref>, homomorphic encryption was first applied to convolutional neural networks (CNNs), where the model was trained in a centralized manner, and it required extensive computation resources. Subsequently, many other works tried to make inferences on encrypted data, e.g., <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b20">[21]</ref>, etc. However, although the recent schemes have improved the efficiency significantly, they often lead to much higher overheads as compared to computing on the original plaintext data.</p><p>To enhance the effectiveness and efficiency of privacypreserving machine learning approaches, a number of differential privacy based methods have been proposed. In <ref type="bibr" target="#b21">[22]</ref>, a differentially-private stochastic gradient descent algorithm together with a mechanism to accurately track the privacy loss during training were designed, which could train deep neural networks with a modest privacy budget and a manageable model quality. But the scheme still depends on the number of training epochs and some empirical parameters (e.g., the lot size, and the clipping bound). In <ref type="bibr" target="#b22">[23]</ref>, differential privacy was applied to a specific deep learning model-deep auto-encoder, and sensitivity analysis and noise insertion were conducted on data reconstruction and cross-entropy error objective functions. In <ref type="bibr" target="#b23">[24]</ref>, a framework called DSSGD was proposed to ensure differential privacy for distributed deep networks. In recent years, some attacks try to extract alternative information from the machine learning process, e.g., model inversion attacks <ref type="bibr" target="#b24">[25]</ref>, membership inference attacks <ref type="bibr" target="#b25">[26]</ref>, and model extraction attacks <ref type="bibr" target="#b26">[27]</ref>. In particular, by utilizing generative adversarial network (GAN), the authors in <ref type="bibr" target="#b27">[28]</ref> claimed that a distributed deep learning approach cannot protect the training sets of honest participants even if the model is trained in a privacy-preserving manner like <ref type="bibr" target="#b23">[24]</ref>. However, it was declared in <ref type="bibr" target="#b28">[29]</ref> that the scheme in <ref type="bibr" target="#b27">[28]</ref> cannot truly break the rigorous differential privacy.</p><p>In this paper, we investigate the problem of collaborative deep learning with strong privacy protection while maintaining a high data utility. In our model, users, i.e., participants, cooperatively learn a collective deep learning model that can benefit from the data of all users. Our work is most related to <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b29">[30]</ref>, but is quite different in several aspects. The proposed schemes in <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b22">[23]</ref> were not designed for the collaborative deep learning setting. In <ref type="bibr" target="#b23">[24]</ref>, participants only shared a subset of parameters with the others to reduce communication costs, and differential privacy is achieved by inserting noises to truncated weights. All the above solutions have their own limitations. The consumed privacy budget during the learning process is relatively high for every single parameter. The total privacy budget is proportional to the number of parameters, which may be in the tens of thousands in deep learning models. The parameter that tunes the fraction of uploaded gradients is used to quantify the privacy, but each pixel in the training data may be revealed by multiple gradients. Compared with <ref type="bibr" target="#b23">[24]</ref>, the scheme in <ref type="bibr" target="#b29">[30]</ref> considers providing client-level differential privacy which can hide the existence of participants, and uses the moment accountant technique in <ref type="bibr" target="#b21">[22]</ref> to track the privacy loss. However, both methods did not consider the scenario that the data quality of certain participants may be poor, which may degrade the performance of collaborative learning.</p><p>To address the above issues, in our design, each participant, e.g., a mobile user or medical institution, maintains a local neural network model and a local dataset that may be highly sensitive. Instead of sharing local data with the central server, the participant only uploads the updated parameters of the local model generated from the local dataset. The central server derives the global parameters for the collective model using the updates from all participants. Although parameter sharing can prevent direct exposure of the local data, the information of sensitive data may be indirectly disclosed. To solve this problem, we utilize differential privacy <ref type="bibr" target="#b30">[31]</ref> to obtain the sanitized parameters to minimize the privacy leakage. Unlike <ref type="bibr" target="#b23">[24]</ref> where noise is directly injected to the gradients, we apply functional mechanism <ref type="bibr" target="#b31">[32]</ref> to perturb the objective function of the neural network, and obtain the sanitized parameters by minimizing the perturbed objective function.</p><p>In collaborative learning, the quality of data contributed by different participants may be very diverse. Different terminal devices or people may have different capacities to generate the training data, and there may exist unpredictable random errors during data collection and storage. Participants with low quality data are referred to as unreliable participants (discussed in detail in Section II-A). To make the learning process fair and non-discriminative, we consider the 'data quality' as one of the privacy concerns of participants, which should not be inferred by other participants during the learning process. We adopt exponential mechanism to protect this privacy while effectively learning an accurate model. Our main contributions are summarized as follows.</p><p>• To the best of our knowledge, we are the first to investigate the problem of privacy-preserving collaborative deep learning by taking into account the existence of unreliable participants.</p><p>• We present a novel scheme called SecProbe, which allows participants to share model parameters, and deals with unreliable participants by utilizing exponential mechanism. SecProbe can protect the data privacy for each participant while effectively learning an accurate model. • We derive the approximate polynomial form of the objective function in a neural network with two different loss functions, and use functional mechanism to inject noises to the coefficients to achieve differential privacy without consuming too much privacy budget. We show that it is easy to extend and apply our method to networks with more layers. • We evaluate the performance of SecProbe on two wellknown real-world datasets for regression and classification tasks. The results demonstrate that SecProbe is robust to unreliable participants, and achieves high accuracy close to the solution obtained in the traditional centralizetrained model, while providing a rigorous privacy guarantee.</p><p>II. PROBLEM STATEMENT AND PRELIMINARIES In this section, we introduce the problem statement, some preliminary knowledge of deep learning, differential privacy and functional mechanism used in our design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Statement</head><p>In this paper, we consider the generic setting of privacypreserving distributed collaborative deep learning. As shown Server Server Unreliable Fig. <ref type="figure">1</ref>: A collaborative deep learning system with unreliable participants in Figure <ref type="figure">1</ref>, in our model, each participant may have its own sensitive data, and it would like to learn a model benefiting from both its own data and those of the others.</p><p>In particular, instead of making the assumption that all the participants are 'reliable', i.e., the data held by each participant is balanced and has the same or similar quality, we consider a more practical model where there may exist a small group of participants who are 'unreliable' during some phases of the whole learning process. That is, a portion of data held by unreliable participants is not always as accurate as data held by others, and thus their uploaded parameters may disturb the learning accuracy.</p><p>In our daily lives, unreliable participants are common in a collaborative learning system. Consider a typical scenario where several hospitals aim to learn a model together for cancer prediction for patients. There may exist non-negligible gaps in the quality of data among different hospitals since a rich-experienced chief physician with advanced medical devices in a high-rate hospital will be more likely to produce accurate data than an junior physician with low-end of devices in an ordinary hospital. Note that, it does not mean that the data in the ordinary hospital are all and/or always 'bad'. Actually, every participant might have some bad data in some phases of their training process when more and more data are being gathered into their local dataset, because there are so many possibilities to go wrong in the data generation and storage procedures. Consequently, the existence of unreliable participants will bring non-ignorable disturbance during the collaborative training process, which may finally result in an inaccurate or even useless model.</p><p>Therefore, the problem is how to design a privacypreserving collaborative deep learning system, which can reduce the impact of unreliable participants on the learned model, and avoid the leakage of participants' privacy from the training data and the trained models.</p><p>Threat Model. In our scheme, we assume that the server is honest-but-curious and non-colluding, i.e., it always follows the protocol, and will not reveal the information of participants or collude with any participant. The participants might be malicious, each participant (or a set of colluding participants) might try to infer other participants' local data and their data quality from the trained model, or upload false parameters/data deliberately into the system. In addition, to achieve stronger security, we will discuss how to deal with the untrusted server by introducing anonymity communication techniques in Section III-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep Learning</head><p>Broadly speaking, deep learning, based on artificial neural networks, aims to learn and extract high-level abstractions in data and build a network model to describe accurate relations between inputs and outputs. Common deep learning models are usually constructed by multi-layer networks, where nonlinear functions are embedded, so that more complicated underlying features and relations can be learned in different layers. Interested readers can refer to thorough surveys or reviews in <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>.</p><p>There are multiple forms of deep learning models, e.g., multi-layer perceptron (MLP), convolutional neural network (CNN), recurrent neural network (RNN) <ref type="bibr" target="#b34">[35]</ref>. Different models fit for different types of problems, and among all of those models, MLP is a very common and representative form of deep learning architecture. Specifically, MLP is a kind of feed-forward neural network, where each neuron receives the outputs of neurons from the previous layer. Figure <ref type="figure" target="#fig_1">2</ref> shows a typical MLP with multiple hidden layers. Each neuron has an activation function which is usually non-linear. As shown, for a neuron in a hidden layer, say j, the output of the neuron is calculated by h j = f (W (1) j x), where W</p><p>(1) j is the weight vector which determines the contribution of each input signal to the neuron j, x is the input of the model, and f is the activation function. The activation function is usually nonlinear, capturing the complicated non-linear relation between the output and input. Typical examples are sigmoid function f (x) = (1 + e −x ) −1 , ReLU function f (x) = max(0, x), and hyperbolic tangent f (x) = e 2x −1 e 2x +1 . In this work, we will focus on an MLP model, where ReLU function is applied.</p><p>Training a neural network, i.e., learning the parameters (weights) of the network, is a non-convex optimization problem. The typical algorithms used to solve the problem are different types of gradient descent methods <ref type="bibr" target="#b35">[36]</ref>. In this paper, we will consider a supervised learning task, e.g., regression analysis, and assume the output of the network is z. Suppose the data we use to train the network is a tuple (x i , y i ), where x i denotes the network input and y i is the label. Consequently, we can use loss (objective) function to measure the difference between the network output and the real training label, e.g.,</p><formula xml:id="formula_0">Error i = (z i − y i ) 2 .</formula><p>We then can use back propagation <ref type="bibr" target="#b36">[37]</ref> algorithm to propagate the error back to the neurons, compute the contribution of each neuron to this error, and adjust the weights accordingly to reduce the training error. The adjustment procedure for a weight, say w j , is w j = w j − l ∂Errori ∂wj , where l is the learning rate.</p><p>Among various gradient descent algorithms, stochastic gradient descent (SGD) <ref type="bibr" target="#b37">[38]</ref> is considered to be especially fit for optimizing highly non-convex problems for its high efficiency and effectiveness. This algorithm brings stochastic factors into the training process, which helps the model to escape from local optimum. For a large training dataset, SGD first randomly samples a small subset (mini-batch) of the whole dataset, then computes the gradients over the mini-batch and updates the weights, e.g., for weight w j . After one iteration on the minibatch, the new w j is computed by w j = w j − l ∂Error b ∂wj , where Error b is the loss function computed on the mini-batch b. In our work, we will apply SGD to each participant to train its local model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Differential Privacy</head><p>Differential privacy has become a de facto standard privacy model for statistics analysis with provable privacy guarantee, and has been widely used in data publishing <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref> and data analysis <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>. Intuitively, a mechanism satisfies differential privacy if its outputs are approximately the same even if a single record in the dataset is arbitrarily changed, so that an adversary infers no more information from the outputs about the record owner than from the dataset where the record is absent.</p><p>Definition 1 (Differential Privacy <ref type="bibr" target="#b30">[31]</ref>): A privacy mechanism M gives -differential privacy, where &gt; 0, if for any datasets D and D differing on at most one record, and for all sets S ⊆ Range(M),</p><formula xml:id="formula_1">Pr[M(D) ∈ S] ≤ exp( ) • Pr[M(D ) ∈ S],<label>(1)</label></formula><p>where is the privacy budget representing the privacy level the mechanism provides. Generally speaking, a smaller guarantees a stronger privacy level, but also requires a larger perturbation noise. Definition 2 (Sensitivity <ref type="bibr" target="#b42">[43]</ref>): For any function f :</p><formula xml:id="formula_2">D → R d , the sensitivity of f w.r.t. D is ∆(f ) = max D,D ∈D ||f (D) − f (D )|| 1<label>(2)</label></formula><p>for all D and D differing on at most one record.</p><p>Laplace mechanism is the most commonly used mechanism that satisfies -differential privacy. Its main idea is to add noise drawn from a Laplace distribution into the datasets to be published.</p><p>Theorem 1 (Laplace Mechanism <ref type="bibr" target="#b42">[43]</ref>): For any function</p><formula xml:id="formula_3">f : D → R d , the Laplace Mechanism M for any dataset D ∈ D, M(D) = f (D) + Lap(∆(f )/ ) d<label>(3)</label></formula><p>satisfies -differential privacy, where the noise Lap(∆(f )/ ) is drawn from a Laplace distribution with mean zero and scale ∆(f )/ . Obviously, the Laplace mechanism only fits for numeric query. Thus, for the query whose outputs are not numeric, Mcsherry et al. <ref type="bibr" target="#b43">[44]</ref> proposed Exponential mechanism that selects an output r from the output domain R.</p><p>Theorem 2 (Exponential Mechanism <ref type="bibr" target="#b43">[44]</ref>): Let ∆u be the sensitivity of the utility function u:</p><formula xml:id="formula_4">(D × R) → R, the mechanism M for any dataset D ∈ D, M(D, u) = choose r ∈ R with probability ∝ exp( u(D, r) 2∆u ) (4) gives -differential privacy.</formula><p>This theorem implies that the Exponential mechanism can make high utility outputs exponentially more likely at a rate that mainly depends on the utility score such that the final output would be approximately optimum with respect to u, and meanwhile provide rigorous privacy guarantee.</p><p>The composition properties of differential privacy provide privacy guarantee for a sequence of computations.</p><p>Theorem 3 (Sequential Composition <ref type="bibr" target="#b44">[45]</ref>):</p><formula xml:id="formula_5">Let M 1 , M 2 , • • • , M r</formula><p>be a set of mechanisms and each M i provides i -differential privacy. Let M be another mechanism that executes M 1 (D), • • • , M r (D) using independent randomness for each M i . Then M satisfies ( i i )-differential privacy.</p><p>Theorem 4 (Parallel Composition <ref type="bibr" target="#b44">[45]</ref>):</p><formula xml:id="formula_6">Let M i each provide i -differential privacy. A sequence of M i (D i )'s over disjoint datasets D i provide max( i )-differential privacy.</formula><p>These theorems allow us to distribute the privacy budget among r mechanisms to realize -differential privacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Functional Mechanism</head><p>Functional mechanism (FM) <ref type="bibr" target="#b31">[32]</ref> is a general framework for regression analysis with differential privacy. It can be seen as an extension of the Laplace mechanism which ensures privacy by perturbing the optimization goal of regression analysis instead of injecting noise directly into the regression results.</p><p>A typical regression analysis on dataset D returns a model parameter ẇ that minimizes the optimization (objective) function f D (w) = xi∈D f (x i , w). However, directly releasing ẇ would raise privacy concern, since the parameters reveal information about dataset D and function f D (w). In order to achieve differential privacy, we use FM to firstly perturb the objective function f D (w) (by exploiting the polynomial representation of f D (w)), and then release the parameter w that minimizes the perturbed objective function fD (w).</p><p>We assume w is a vector containing d values w 1 , . . . , w d . Let φ(w) denote the product of w 1 , . . . , w d , i.e., φ(w) = Algorithm 1 A high-level description of SecProbe 1: Build the models and initialize all parameters 2: for each communication round do 3:</p><p>for each participant i do 4:</p><p>for iteration j = 1 to I do The server chooses to accept Wi according to the computed utility score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10:</head><p>The server conducts model average to obtain W new and send it to each participant. 11: end for</p><formula xml:id="formula_7">w c1 1 • w c2 2 • • • w c d d , where c 1 , . . . , c d ∈ N . Let Φ j (j ∈ N ) denote the set of all products of w 1 , . . . , w d with degree j, i.e., Φ j = {w c1 1 • w c2 2 • • • w c d d | d l=1 c l = j}.</formula><p>By the Stone-Weierstrass Theorem <ref type="bibr" target="#b45">[46]</ref>, any continuous and differentiable function f (w) can always be written as a polynomial of w 1 , . . . , w d , i.e., f (x i , w) = J j=0 φ∈Φj λ φx i Φ(w), where λ φx i ∈ R denotes the coefficient of φ(w) in the polynomial, and J ∈ [0, ∞]. Similarly, we can derive the polynomial function of f D (w) as</p><formula xml:id="formula_8">f D (w) = J j=0 φ∈Φj xi∈D λ φx i Φ(w).<label>(5)</label></formula><p>Lemma 1: ( <ref type="bibr" target="#b31">[32]</ref>) Let D and D be any two neighboring databases. Let f D (w) and f D (w) be the objective functions of regression analysis on D and D , respectively. Then, we have the following inequality</p><formula xml:id="formula_9">∆ = J j=1 φ∈Φj xi∈D λ φx i − xi∈D λ φ x i 1 ≤ 2 max x J j=1 φ∈Φj λ φx 1 .</formula><p>To achieve -differential privacy, FM perturbs f D (w) by injecting Laplace noise into its polynomial coefficients. According to Lemma 1, f D (w) is perturbed by injecting Laplace noise with scale of Lap(∆/ ) into the polynomial coefficients λ(φ), where ∆ = 2 max x J j=1 φ∈Φj λ φx 1 . Then we can derive the model parameter w which minimizes the perturbed function fD (w). In this work, we propose to utilize FM in our design to protect the privacy of participants' local data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SECPROBE: PRIVACY-PRESERVING COLLABORATIVE DEEP LEARNING SYSTEM A. System Architecture</head><p>In Figure <ref type="figure">1</ref>, we assume there are N participants, and each of them has a sensitive dataset for local training. The participants aim to learn a common model, i.e., the architectures of the local models are identical, and the learning objectives are the same. There are many deficiencies and difficulties of collecting all the data from participants in advance and training on the entire dataset. Such complicated process of data collection usually incurs high communication overhead, and participants may not be willing to directly upload their data to a third party from the perspective of privacy or business consideration. Therefore, the participants only exchange the parameters (weights) with others, and a server (e.g., a cloud service provider) undertakes the job of communicating with participants, exchanging and storing parameters. In our model, we assume there exists a global model and an auxiliary validation dataset on the server. This dataset can be very small, and it is easy to be obtained in practice. For example, the data can be collected from participants who have already been expired with no privacy concern or publicly well-tested handclassified datasets (such as MNIST <ref type="bibr" target="#b46">[47]</ref>).</p><p>Algorithm 1 gives the high-level steps of SecProbe. The server and participants build their own models and initialize all the parameters before the learning starts. For each communication round, the participants locally train their own models using SGD in a differentially private way. After I times of iteration, the participants upload the perturbed parameters to the server. The server then uses the auxiliary validation data to compute a utility score for each participant, and then chooses to accept the parameters with certain probability. Next, the averaged model parameters are computed and distributed to each participant for the next round of local training. Note that the participant can terminate its training procedure and drop out from the system at any time if it believes its model is accurate enough, and meanwhile a new participant can also join into the system at anytime. We next describe the detailed procedures of SecProbe on the server side and the participant side, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. SecProbe: The Server Part</head><p>Algorithm 2 gives the pseudocode of SecProbe on the server side. The server first initializes the parameters and waits for the local training results from each participant. When the number of participants who upload their weights to the server reaches a pre-fixed threshold M, the server stops receiving the uploaded data and sends a stop signal to notify other participants that there is no need to upload weights (in step 2). The parameter M is used to control the number of participants that the server plans to utilize per round and meanwhile saves a lot of communication costs. Alternatively, this procedure can also be achieved by randomly assigning a set of M participants at the beginning of each round. These two approaches have their own advantages and both can be used in our design. The former can intrinsically deal with the occurrence of failed uploads, while the latter can save a lot of computation costs on the participant side. Without loss of generality, we adopt the first approach in the description of Algorithm 2.</p><p>As discussed above, the existence of unreliable participants indicates that the parameters uploaded by them may be disruptive, and it may reduce the accuracy of the global model. To reduce their effect on the model accuracy, we measure the data quality of these unreliable participants by calculating </p><formula xml:id="formula_10">u(G, D, m) = 1 d d i (1 − | max(z i , 3y i ) − y i y i |),<label>(6)</label></formula><p>where z i is the output of the model with parameter G(m), and y i is the real value from the auxiliary validation data. Without loss of generality, we assume that y i is in range [0, 1].</p><p>The scoring function calculates an accuracy score for each participant m.</p><p>If the server chooses the participants only according to the scoring function without any uncertainty, the participants could easily infer which participants hold the low-quality data by comparing their own parameters and the new parameters sent from the server. In SecProbe, we utilize the exponential mechanism to inject uncertainty into the sampling procedure against this kind of inference. The server samples K participants without replacement such that</p><formula xml:id="formula_11">Pr[Selecting participant m] ∝ exp( 2K∆u u(G, D, m)).<label>(7)</label></formula><p>However, the sensitivity of calculating 1 − | zi−yi yi | is unscalable since the proportion of z i to y i is infinite in theory. If z i ≤ 3y i , the value will be within [−1, 1]. We observe that this condition always holds in practice, as the predicted parameter z i does not deviate from the real value y i for more than three times 1 . To this end, we place a restriction on the output z i , i.e., replacing z i in Eq. <ref type="bibr" target="#b5">(6)</ref> with max(3y i , z i ). If z i ≤ 3y i , it will be clipped to avoid the unbounded sensitivity. By adding this restriction, the sensitivity can be bounded to 1. Experimental results show that this design will not affect the accuracy of the learned model. 1 We find that the average of</p><formula xml:id="formula_12">| z i −y i y i</formula><p>| is always less than 1 based on our experimental results in Section IV Moreover, for a classification task, the scoring function u can be defined as the correct prediction rate directly.</p><p>Lemma 2: The sensitivity of the prediction accuracy for classification is ∆u = 1  2 . Proof: Let m and n denote the number of correct predictions and the number of samples respectively. The sensitivity is ∆ = m+1 n+1 − m n = n−m n(n+1) . Since n ≥ 1 and n ≥ m, the maximum of ∆u is 1 2 when n = 1 and m = 0. Remarks. In this step, the goal is to make the participants in the collaborative learning process non-discriminative. To achieve this, we use the exponential mechanism to prevent adversaries from knowing the performances of uploaded models, i.e., the utility scores. The privacy of the participants' training data will be considered in the next step. As it is unfair to measure the performance of the participants' own data, a reliable dataset is required for evaluation, e.g., a validation dataset on the server. In our design, since the performance of the model is reflected by the prediction accuracy, we protect the prediction values on the validation dataset. Specifically, the server constructs a centralized virtual dataset which has the same number of records with the original validation dataset, and each record has M attributes which represent the prediction results (the clipped z i for regression, or the correctness for classification) on the M participants' models. Then, the neighboring dataset consists of two virtual datasets which are different in only one row. Suppose a query asks: "which model has the best quality?". This query can be regarded as "which model gets the highest utility score on the auxiliary validation dataset?". Obviously, the effect of removing or adding one record in the virtual dataset on the utility score is the sensitivity as above. For the regression task, the summation term in Eq. ( <ref type="formula" target="#formula_10">6</ref>) is in range [-1, 1]. So, changing one record in the virtual dataset could place an impact on the utility score no larger than 1. For the classification task, adding or removing one record will affect the number of correct predictions. As the neighboring dataset is non-empty, the maximum influence on the utility score is 1  2 . Theorem 5: The sampling procedure in Algorithm 2 (line 4) satisfies -differential privacy.</p><p>Proof: Proof sketch. Because sampling one participant consumes K budget and satisfies K -differential privacy according to Theorem 2, the sampling procedure which samples K participants will satisfy K K -differential privacy. Note that Theorem 5 only ensures that this sampling procedure satisfies -differential privacy at the current training iteration. Due to the composition properties of differential privacy, the privacy level provided by it may degrade during training. We will discuss in detail which privacy level our mechanism will provide during the whole training process in the privacy analysis of this section.</p><p>Remarks. The above procedure samples a set of participants at an exponential rate based on the scoring function while preventing the sampling procedure from leaking privacy. Therefore, the real quality of uploaded weights from a participant cannot be inferred by others since the new weights are computed on a set of privately-chosen participants, and the system can sample the approximately optimal weights and eliminate the disturbance of unreliable participants as much Run SGD with batch size |S| on the local dataset using the perturbed loss function 7: end for 8: Upload W to the server 9: Receive the new averaged weight W new from the server 10: Repeat steps 5-9 until an acceptable small test error is obtained 11: Drop out of the system as possible. It is easy to see that the time complexity of the sampling step is O(KM ). We can further significantly reduce the running time by implementing the sampling step on a static balanced binary tree as suggested in <ref type="bibr" target="#b47">[48]</ref>. The improved sampling step can run in time O(M + K ln(M )).</p><p>After choosing the final accepted weights, the server conducts a model average operation that sets the new global weights to be the average of all the accepted weights. The server finally sends the new weights to every participant, and waits for the next round of parameter uploading. We now briefly explain the reason why model average operation works. The average operation to some extent consistently inherits the procedure of SGD by randomly choosing a mini-batch of the training data to get the sum of errors on the minibatch and then computing the gradients on the error. The average operation acts as choosing a mini-batch of the data from all the accepted participants and computing the gradients on the overall error. Note that, our experiments show that this operation works well only if the parameters of each participant are randomly initialized by the same seed, which is easy to be implemented, e.g., the participants can download the same initialized parameters from the server to replace their own initializations at the very beginning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. SecProbe: The Participant Part</head><p>Algorithm 3 presents the pseudocode of SecProbe on the participant side. Each participant has its own local training dataset and conducts the standard SGD algorithm to train its local model. Let W i denote the network weights of participant i. To protect the privacy of the participant's sensitive data being disclosed by W i , the participant applies differential privacy onto the training algorithm to get sanitized weights Wi 's, and uploads them to the server.</p><p>To achieve differential privacy, Laplace mechanism was utilized in <ref type="bibr" target="#b23">[24]</ref> to directly inject noise to the weights. However, their scheme has to consume too much privacy budget for each weight per epoch in order to achieve acceptable results. Instead</p><formula xml:id="formula_13">x i z i W W</formula><p>Fig. <ref type="figure" target="#fig_4">3:</ref> A neural network with one hidden layer of directly injecting noise to the weights W , in our design we propose to utilize functional mechanism <ref type="bibr" target="#b31">[32]</ref> to perturb the objective function of the network, train the model on the perturbed objective function and finally compute the sanitized weights W . Since the structures of the neural networks may be varied and often depend on specific application scenarios, it is impossible to design a one-size-fits-all differentially-private solution for all deep learning models. In this paper, we focus on the most common neural network MLP. Specifically, we first consider a three-layer fully-connected neural network, design algorithms to train the model in a differentially-private manner, and then show that more hidden layers can be stacked easily by using our proposed scheme.</p><p>The regression problem usually uses mean square error (MSE) as the loss function. Suppose the training set D has a set of n tuples κ 1 , κ 2 , . . . , κ n . For each tuple κ i = (X i , y i ), X i contains d attributes (x i1 , x i2 , . . . , x id ) and y i is the label of κ i . Without loss of generality, we assume each attribute in X i and y i is in the range [0, 1], which is easy to be satisfied by data normalization. The MLP takes X i as input and outputs a prediction z i of y i as accurate as possible. Then, the objective function can be given by</p><formula xml:id="formula_14">(D, W ) = n i=1 (z i − y i ) 2 . (<label>8</label></formula><formula xml:id="formula_15">)</formula><p>Recall the calculations of MLP, we have z i = σ 1 (HW (2) ) and H = σ 2 (X T i W (1) ), where W (1) and W (2) are the weight matrixes of the network (as shown in Figure <ref type="figure" target="#fig_4">3</ref>), σ 1 is the sigmoid function, and σ 2 is the ReLU function. Note that we bound the ReLU function by [0, 1] to avoid introducing an unbounded global sensitivity.</p><p>Consequently, for a mini-batch S sampled from the training set D, the objective function can be written into the following form</p><formula xml:id="formula_16">(S, W ) = |S| i=1 (z i − y i ) 2 = |S| i=1 [y 2 i − 2y i [1 + e (−(ReLU(X T i W (1) )W (2) j )) ] −1 + [1 + e (−(ReLU(X T i W (1) )W (2) j )) ] −2 ].<label>(9)</label></formula><p>Recall that FM requires the objective function to be the polynomial representation of weights w, thus we need to approximate Eq. ( <ref type="formula" target="#formula_16">9</ref>) and rewrite it into a polynomial form. Since the first term of Eq. ( <ref type="formula" target="#formula_16">9</ref>) is already in the polynomial form, we only consider the other two terms. To utilize Taylor Expansion to help approximate the functions as suggested in <ref type="bibr" target="#b31">[32]</ref>, ∀j ∈ [1, d], we define four functions f 1j , f 2j , g 1j and g 2j as follows.</p><formula xml:id="formula_17">f 1 = −2y i [1 + exp(−z)] −1 ; f 2 = [1 + exp(−z)] −2 ; g 1 = ReLU(X T i W (1)</formula><p>)W (2) ; g 2 = ReLU(X T i W (1) )W (2) .</p><p>Then, we can rewrite Eq. ( <ref type="formula" target="#formula_16">9</ref>) into the following form</p><formula xml:id="formula_19">(S, W ) = |S| i=1 [y 2 i + f 1 (g 1 (κ i , W )) + f 2 (g 2 (κ i , W ))]. (<label>11</label></formula><formula xml:id="formula_20">)</formula><p>Given the above decomposition of the original loss function, we can then apply Taylor expansion in Eq. ( <ref type="formula" target="#formula_19">11</ref>) and obtain Eqs. ( <ref type="formula" target="#formula_34">12</ref>) and ( <ref type="formula" target="#formula_21">13</ref>),</p><formula xml:id="formula_21">˜ (S, W ) = |S| i=1 y 2 i + 2 l=1 ∞ k=0 f (k) l (γ l ) k! g l (κ i , W ) − γ l k ,<label>(13)</label></formula><p>where γ l is a real number and without loss of generality we set it to be zero for ease of analysis. As can be seen in Eq. ( <ref type="formula" target="#formula_21">13</ref>), the number of polynomial terms is infinite, which may result in an unacceptable large sensitivity. Thus, we propose to truncate Eq. ( <ref type="formula" target="#formula_21">13</ref>) by cutting off all polynomial terms with order larger than 2, i.e., we set k ∈ [0, 2]. Then we can obtain the final polynomial objective function used for training as Eq. ( <ref type="formula" target="#formula_34">12</ref>). The influence of truncating the polynomial and clipping the ReLU function is shown in Table <ref type="table" target="#tab_1">I</ref>. We can see that, both operations do not have obvious impacts on the training accuracy. Now we are ready to give the following lemma. Lemma 3: Let S and S be any two neighboring databases. Let ˆ (S, W ) and ˆ (S , W ) be the objective functions of MLP on S and S respectively. Then, the global sensitivity of the objective function ˆ over S and S is</p><formula xml:id="formula_22">∆ ≤ 1 2 b + 1 8 b 2 ,</formula><p>where b is the number of hidden units in the hidden layer.</p><p>Proof: Without loss of generality, we assume that S and S differ in the last tuple, κ |S| (κ |S| ). According to Lemma 1, we have</p><formula xml:id="formula_23">∆ ≤ 2 max κ 1 4 b p=1 h p + 1 16 b p=1,q=1 h p h q ≤ 2( 1 4 b + 1 16 b 2 ) = 1 2 b + 1 8 b 2 ,</formula><p>where h is the value of hidden neurons at the hidden layer.</p><p>As can be seen, the sensitivity of the objective function ˆ (S, W ) only depends on the model structure, which is independent with the cardinality of the dataset S. Finally, we inject Laplace noise with scale ∆ to the coefficients of ˆ (S, W ) and obtain the perturbed objective function ¯ (S, W ), which satisfies -differential privacy.</p><p>The classification problem usually adopts cross-entropy error as the loss function. We construct a CNN the same as <ref type="bibr" target="#b23">[24]</ref> which has two convolutional layers, two max pooling layers and one hidden layer with 128 neurons and sigmoid activation function as an example to solve the classification problem. Similar to the regression problem, the objective function is</p><formula xml:id="formula_24">(κ i , W ) = n i=1 −y i log z i . (<label>14</label></formula><formula xml:id="formula_25">)</formula><p>The Eq. ( <ref type="formula" target="#formula_24">14</ref>) can also be decomposed to three functions as follows</p><formula xml:id="formula_26">f = −y i log[1 + exp(−z)] −1 and</formula><formula xml:id="formula_27">g 1 = g 2 = Conv(κ i )W.</formula><p>Note that, Conv(κ i ) represents the output of the previous pooling layer, and W is the weight matrix of the fullyconnected layer. Then, the loss function can be rewritten as Eq. ( <ref type="formula" target="#formula_28">15</ref>),</p><formula xml:id="formula_28">(S, W ) = − |S| i=1 M j=1 f ((κ i , W )). (<label>15</label></formula><formula xml:id="formula_29">)</formula><p>The expansion form of Eq. ( <ref type="formula" target="#formula_28">15</ref>) is</p><formula xml:id="formula_30">˜ (S, W ) = − |S| i=1 M j=1 ∞ k=0 f (k) l (γ l ) k! g l (κ i , W ) − γ l k . (<label>16</label></formula><formula xml:id="formula_31">)</formula><p>The final polynomial objective function used for training is given in Eq. ( <ref type="formula" target="#formula_35">17</ref>). Lemma 4: Let S and S be any two neighboring databases. Let ˆ (S, W ) and ˆ (S , W ) be the objective functions of MLP on S and S respectively. The global sensitivity of the objective function ˆ over S and S is</p><formula xml:id="formula_32">∆M (≤ b + 1 4 b 2 ).</formula><p>Proof: Without loss of generality, we assume that S and S differ in the last tuple, κ |S| (κ |S| ). According to Lemma 1, we have</p><formula xml:id="formula_33">∆ ≤ 2M max κ 1 2 b p=1 h p + 1 8 b p=1,q=1 h p h q ≤ 2M ( 1 2 b + 1 8 b 2 ) = M (b + 1 4 b 2 ).</formula><p>We next revisit Algorithm 3. After obtaining the perturbed loss function ¯ (S, W ), the participant performs standard SGD algorithm with the batch size |S| and iterates for I times. Then it gets the sanitized weights W 's of the current round and uploads them to the server. The parameter I manages the communication cost of the system by controlling the frequency of updates between the participants and the server. Simply put, the increase of I will decrease the frequency of updates and thus reduce the communication cost. But it will also depress the benefits from collaborative learning at the same time since the 'collaboration' decreases. We will evaluate the effect of I through extensive experiments in the next section.  Scalability. In the above discussion, we focus on an MLP model with one hidden layer for regression analysis. Based on the above calculations, it is easy to stack more hidden layers with ReLU function into the model to address more complicated problems. For example, if an additional hidden layer with b neurons is added, the only change of Eq. ( <ref type="formula" target="#formula_16">9</ref>) is adding the layer matrix multiplication and activation function in the exponential. Because the output of the previous layer is bounded to [−1, 1], the sensitivity of the loss function will slightly change to 1  2 b + 1 8 b 2 . Moreover, the functional mechanism can also be applied to other types of loss functions (e.g., huber-loss function), other activation functions (e.g., hyperbolic tangent), and other types of networks (e.g., Auto-Encoder or RNN) with certain adaptations, which are beyond the scope of this paper.</p><formula xml:id="formula_34">ˆ (S, W ) = |S| i=1 y 2 i + 2 l=1 2 k=0 f (k) l (γ l ) k! g l (κ i , W )−γ l k = |S| i=1 y 2 i + 2 l=1 f (0) l (0) + 2 l=1 f (1) l (0) ReLU(X T i W (1) )W (2) + 2 l=1 f (2) l (0) 2 ReLU(X T i W (1) )W (2) 2 = |S| i=1 y 2 i − y i + 1 4 + 1 − 2y i 4 ReLU(X T i W (1) )W (2) + 1 16 ReLU(X T i W (1) )W (2) 2<label>(12)</label></formula><formula xml:id="formula_35">ˆ (S, W ) = − |S| i=1 M j=1 2 k=0 f (k) l (γ l ) k! g l (κ i , W ) − γ l k = |S| i=1 log 2 + ( 1 2 − y i )Conv(κ i )W + 1 8 [Conv(κ i )W ] 2<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Security Analysis</head><p>Let the privacy budget used in participant sampling and objective function perturbation be 1 and 2 respectively. Since the training procedure on each participant strictly follows FM, the parameters computed from the perturbed objective function satisfy -differential privacy in each training iteration. Let S i be the training batch of an iteration. Since all batches are disjoint from each other in a training epoch (e.g., S i and S i−1 contain different tuples sampled randomly from the training data), where an epoch is one full training process that consists of several iterations covering the whole training data, we can conclude that the training process at each participant ensures 2 -differential privacy in each epoch according to <ref type="bibr">Theorem 4.</ref> Recall that the sampling procedure in Algorithm 2 also ensures 1 -differential privacy at each sampling step. Since it can be seen that each step of sampling protects the privacy of the partial training data, we can conclude that the sampling procedure in Algorithm 2 satisfies 1 -differential privacy in each epoch.</p><p>Note that the two procedures above address two different privacy concerns respectively. In case of passive adversaries, the procedures executed by each participant aim to protect the privacy of the training data, focusing on each single record of the training data, while the procedures run by the server aim to protect the privacy of the data quality, which takes all the corresponding records as a whole. Therefore, we can finally conclude that SecProbe satisfies max( 1 , 2 )differential privacy in each training epoch.</p><p>We further consider the effects of different behaviors by adversaries. Firstly, if a set of participants are curious-buthonest, they may try to infer some information from others. But obviously, using differential privacy can prevent this leakage. If some participants are malicious, there may be two kinds of behaviors: 1) sending fake parameters to the server; 2) stealing the parameters from the communication process directly. For the first malicious behavior, thanks to the exponential mechanism that we have introduced, it is almost impossible for the fake parameters to significantly affect the model, thus there is no negative effect on the training process, and the adversaries cannot infer the data quality of other participants. For the second one, the adversary may eavesdrop on channels between honest participants and the server. Some effective cryptography tools can be used to encrypt the parameters (e.g., AES) and verify the received data (e.g., SHA-256) to ensure communication security. Therefore, our scheme is robust and secure when facing malicious participants.</p><p>Furthermore, in this paper, we assume the server is honestbut-curious and non-colluding, i.e., the server can know the data quality of each participant after uploading, but it will not reveal the qualities or collude with some participants. As discussed in <ref type="bibr" target="#b23">[24]</ref>, anonymous authentication and communication techniques can help to relax this assumption by preventing the server from linking uploaded parameters with the participants. At the beginning of the protocol, a participant can use the protocol in <ref type="bibr" target="#b48">[49]</ref> to obtain enough e-tokens (e.g., in our experimental settings, 500 is sufficient to make the learning process converge), and in each round of communication the participant can use an e-token to authenticate his identity and upload the parameters. Then, the approach in <ref type="bibr" target="#b49">[50]</ref> can be used to prevent tracking the identity while downloading the updated model. Obviously, the anonymity only relies on the security of the adopted anonymous communication protocol, the design of which is outside the scope of this paper.</p><p>However, even if the identities of participants are protected by the anonymity techniques, the use of differential privacy is still essential. Considering an extreme case that if only one participant is honest, and all the other N − 1 participants are collusive. Obviously, the parameters of the honest one are easy to be inferred. This is because the aggregation is only a simple average, and some sensitive information in the local dataset of the honest participant would be leaked from the parameters, even if the others do not know which party owns these information. In this scenario, differential privacy will work as differential privacy can help protect the record privacy even if the adversaries know all the other records in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL EVALUATION</head><p>In this section, we evaluate the performance of SecProbe on a real-world dataset. All experiments are conducted on a machine with an Intel E5-2660-v2 CPU, a Quadro K5200 GPU and 128GB RAM, running on Ubuntu 16.04.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>For the regression task, we use the dataset from Integrated Public Use Microdata Series <ref type="bibr" target="#b50">[51]</ref>, named US, which contains 600,000 census records collected in US. There are 15 attributes in the dataset, namely, Sex, Age, Race, Education, Filed of Degree, Marital Status, Family Size, Number of Children, Hours Work per Week, Ownership of Dwelling, Number of Children, Number of Rooms, Private Health Insurance, Living Difficulty and Annual Income. Among all these attributes, there are 6 attributes that are categorical, including Race, Education, Field of Degree, Marital Status, Private Health Insurance and Living Difficulty. For an attribute that can only be two possible values (e.g., male and female for sex), we set it to be 0 or 1. For the remainings, we follow the common practice in machine learning to transform these attributes by one-hot encoding. We then normalize the other numeric attributes into the scope of [0, 1]. Specifically, for the Annual Income, we apply log transformation before normalization to obtain a relatively stable distribution <ref type="bibr" target="#b51">[52]</ref>. After these transformations, our dataset now has 20 attributes.</p><p>We randomly sample 90,000 records to construct the test dataset, and 10,000 records to form the auxiliary validation dataset on the server. The remaining 500,000 records are randomly divided into N parts, where N is the number of participants. The data is already shuffled before training.</p><p>We focus on a regression task predicting the value of Annual Income by using the other attributes as the input. The accuracy of the model is measured by mean relative error (MRE),</p><formula xml:id="formula_36">MRE = 1 n n i=1 |z i − y i | y i ,<label>(18)</label></formula><p>where y i is the real value, z i is the predicted value produced by the network, and n is the number of tuples in the test dataset.  For the binomial classification task, like <ref type="bibr" target="#b23">[24]</ref>, we use the MNIST and SVHN datasets as benchmarks. The MNIST dataset consists of 28x28 images of handwritten digits with 60,000 training samples and 10,000 test samples. The SVHN dataset consists of 32x32 images of house numbers with 73,527 training samples, 26,032 test samples and 10 classes. We use the accuracy of classification to evaluate the performance of these two models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Setup</head><p>We use the popular neural network architectures: multi-layer perceptron (MLP) with three fully-connected layers. For the regression task, the activation functions of hidden layer and output layer are ReLU and sigmoid function respectively. The number of neurons in the hidden layer is 80. We use SGD as the learning algorithm. The learning rate and the mini-batch size are set to be 0.01 and 128, respectively. The weights of the models are randomly initialized by normal distribution (with mean 0 and standard deviation 1).</p><p>For both MNIST and SVHN datasets, we use a CNN with two convolutional layers, two max-pooling layers, and one hidden layer with 128 neurons. The activation of the output layer is the sigmoid function. The other hyperparameters are the same as the regression task.</p><p>Since the approaches proposed in <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b21">[22]</ref> are not specially designed for collaborative learning, we mainly compare all results with DSSGD in <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b29">[30]</ref>, and two baseline approaches. The first is the centralized training on the entire dataset, which is the basic approach that does not consider privacy concerns and ought to have the best performance of the model accuracy. The second is stand-alone training, which trains solely on the local dataset without collaboration. We call these two baselines Centralized and Stand-alone, respectively. For simplicity, we abbreviate the method in <ref type="bibr" target="#b29">[30]</ref> as CSDP in the rest of this paper. All schemes are implemented on TensorFlow. For SecProbe, we set the privacy budgets used in participants' sampling and perturbing objective functions to be the same. We fine tune the parameters in DSSGD according to <ref type="bibr" target="#b23">[24]</ref> and use the settings with the best performance (the parameter download ratio θ d = 1, gradient bound γ = 0.001, gradient selecting threshold τ = 0.0001).</p><p>To simulate the unreliable participants, we randomly choose half of the participants and replace P fraction of their data with random noise in the range [0,1]. We vary P to evaluate the robustness of SecProbe against unreliable participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results</head><p>The effect of I. Table <ref type="table" target="#tab_2">II</ref> shows the effect of parameter I. The results show that a larger I will speed up the training Training convergence. Figures 4, 5 and 6 show the training convergence of all schemes for regression and classification tasks respectively. We vary the number of participants N in SecProbe and DSSGD, and set M = N , K = M , = 1, and P = 0. The y-axis is the performance of trained model, and the x-axis denotes the number of communication rounds. Although Centralized achieves the highest accuracy in all the settings, SecProbe can achieve almost the same accuracy while providing a rigorous privacy guarantee. In US and MNIST, Centralized has lower convergence rate than the distributed schemes at the early stage. This may be because that more data samples are used for each iteration for the whole system, which can help accelerate the convergence in the early stage of training. Our scheme also has a better performance than DSSGD in terms of both convergence rate and model accuracy in the regression task, and it has almost the same performance in the classification task with more rigorous privacy guarantee. The main reason is that only perturbing the uploaded gradients is not enough to protect the training data, and DSSGD consumes too much privacy budget in perturbing all the gradient values. Moreover, the noise directly injected to each gradient independently may also make the training The effect of parallelism. The parameter M controls the number of participants that the server chooses per round, which can also be regarded as parallelism degree. We vary M to be (0.1N, 0.3N, 0.5N, 0.7N, 1.0N ) and set K = M , = 1, and P = 0. Note that the total size of training dataset does not change with different values of M . As can be seen in Figure <ref type="figure">7</ref>, the increase of parallelism will speed up the convergence of training, and it leads to a more accurate model while increasing the communication loads. On the contrary, the decrease of parallelism will reduce the number of accesses for each data point. Briefly speaking, the size of M affects the amount of data for training in each communication round and the convergence speed. It is good to see that, when M = 0.1N it can still achieve relatively accurate results. Based on the above results, we choose M = 0.5N for the following experiments to strike a good balance between efficiency and convergence rate.</p><p>The effect of auxiliary validation set. To show the effect of the auxiliary validation set on the system performance, we re-design the training set and the test set to simulate one or multiple special participants whose hypotheses are not included at the server. More specifically, we exclude all Fig. <ref type="figure">10</ref>: The influence of malicious participants samples whose one attribute is within a certain range from the original datasets, and put these excluded data to the special participants and a special test set. Here we choose the attribute Age due to its numeric form and its impact on the income. We remove all samples with Age ≤ 0.25 or Age ≥ 0.7 from the original datasets. We set the number of special participants as 1, 0.1N, and 0.2N respectively, and demonstrate the results in Figure <ref type="figure">8</ref>. We can see that the performance on the special test set is significantly worse than that of the reliable test set. With more special participants, the performance of the special test set will improve due to the increasing contribution of the special participants to the collaborative learning process. Therefore, a special participant with high-quality dataset that contains new findings can also play a role in improving the model, even if it is misjudged by the server.</p><p>Certainly, the lack of the auxiliary validation dataset which can provide good utility scores for all participants is a practical limitation. However, this can be mitigated by choosing more participants in each round, i.e., increasing the probability that special participants are chosen. It can be observed from Figure <ref type="figure">8</ref> that even if the special participants' models do not have good utility scores, the aggregated model can have the increased performance on the special test set while rising the proportion of special participants.</p><p>Robustness against unreliable participants. Figure <ref type="figure">9</ref> shows the results of the robustness against unreliable participants. We vary the proportion of unreliable participants R as (30%, 50%, 70%), and replace P proportion of their data with random noise, and set M = 0.5N , K = 0.5M , and = 1. We vary the total number of participants N and the proportion P of the noise data, which means that one half of the participants are unreliable with P fraction of their data to be random noise. We set N = (30, 60, 100) and P = (0.2, 0.4, 0.6). Correspondingly, for Centralized, we set P = (0.1, 0.2, 0.3). For the number of sampling participants K, we set it to be the half of M , which follows the assumption that the majority of the participants are reliable. As can be seen in Figure <ref type="figure">9</ref>, the prediction accuracies of all the other methods decrease quickly with the increase of noise, and more unreliable participants will increase this impact, since they are all lack of the strategy dealing with unreliable participants/data. For Centralized and Stand-alone, they are disturbed by the unreliable data directly. Specially, for DSSGD CSDP, they randomly sample parameters/participants to aggregate the model, under an equivalent sampling probability. As a result, the unreliable participants will significantly impact on accuracy. Meanwhile, our approach SecProbe achieves very high accuracy which is almost the same as the performance of the case with no unreliable participants, and it is also robust against the proportion of noise. The experimental results validate the effectiveness of our scheme. The influence of malicious participants. There may also exist some malicious participants who may destroy the usability of the aggregated model. We show the influence of malicious participants by setting N = 60, M = 30, and K = 15, and tuning the number of malicious participants from 10 to 50 at an interval of 10. The uploaded malicious parameters are randomly set within [0, 1], as the abnormal values, e.g., tens or hundreds, can be easily detected and dropped out. In Figure <ref type="figure">10</ref>, we can see that the existence of malicious participants disturbs the training process significantly, with a fast convergence rate at the early stage of training and a sudden stop followed. The potential reason is that even if only one malicious participant is chosen, the randomly generated parameters will disturb the learning process. Although the performance of the trained model reduces gradually with the increase of malicious participants, the prediction error is still less than 0.2. Similar to special participants, malicious participants can influence the performance owing to their contribution aggregating the model. Therefore, it can also be mitigated by tuning the number of chosen participants in each round to exclude the malicious participants as much as possible.</p><p>Accuracy vs. privacy. We evaluate the effect of different values of privacy budget on the accuracy of the neural network. Figure <ref type="figure" target="#fig_8">11</ref> shows the results compared with the competitors. The x-axis represents the privacy budget per training epoch (an epoch contains several iterations over all training samples). It is shown that, a larger value of results in high accuracy while providing lower privacy guarantee. SecProbe achieves almost the same results with Centralized and outperforms Stand-alone when ≥ 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper, we took the first step to investigate the problem of privacy-preserving collaborative deep learning system while considering the existence of unreliable participants, and presented a new scheme called SecProbe. SecProbe utilizes exponential mechanism and functional mechanism to protect both the privacy of the participants' data and the quality of their data, the two major privacy concerns in such a system. The experimental results demonstrated that SecProbe is robust to unreliable participants, and can achieve high-accuracy results which are close to the model trained in a traditional centralized manner, while providing rigorous privacy guarantee.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: A neural network with multiple hidden layers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 2 6 :</head><label>26</label><figDesc>SecProbe on the server side 1: Initialize parameters W 0 and send them to each participant 2: Wait for participants to upload their weights until there are already M participants' weights W1 , W2 , . . . , WM 3: Calculate the accuracy score u(G, D, m) for each uploading participant by running the model with weights from each of them over the auxiliary validation dataset 4: Sample K participants from M without replacement such that Pr[Selecting participant m] ∝ exp( 2K∆u u(G, D, m)) 5: Average the K weights and obtain W new = 1 Send the new averaged weights to all participants 7: Repeat steps 2-6 until there is no participant in the system a utility score for each participant. Specifically, the server runs the model on the auxiliary validation dataset D with the weights of each of M participants respectively and obtains a utility score for participant m. Let G = [ W1 , . . . , Wm ] denote the set of uploaded weights, where each item can be used to infer the data quality of each participant. For a regression task, suppose the dataset has d samples, we define a scoring function u(G, D, m) as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 3</head><label>3</label><figDesc>SecProbe on the participant side 1: Download the same initialized weights W 0 from the server 2: Set the mini-batch size |S| and the number of iterations I, and perform local SGD in each communication round 3: Decompose the loss function (S, W ) and derive an approximated polynomial form ˆ (S, W ) 4: Obtain the perturbed loss function ¯ (S, W ) by functional mechanism 5: for iteration j = 1 to I do 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 :Fig. 5 :Fig. 6 :</head><label>456</label><figDesc>Fig. 4: A comparison of training convergences of all schemes for the regression task</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :Fig. 8 :Fig. 9 :</head><label>789</label><figDesc>Fig. 7: The effect of the number of participants (M ) selected per round on the system performance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 :</head><label>11</label><figDesc>Fig. 11: Accuracy vs. privacy budget (N = 60, M = 30, K = 15, and P = 0.2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Accuracy of truncated functions</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>The effect of the number of iterations I which controls the frequency of updates. Each entry in the table gives the necessary number of communication rounds to achieve MRE 0.15 (N = 60, M = 30, K = 30, P = 0, and = 1).</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR&apos;16</title>
				<meeting>of CVPR&apos;16</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">DeepCrack: Learning hierarchical convolutional features for crack detection</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1498" to="1512" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP&apos;13</title>
				<meeting>of ICASSP&apos;13</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using deep learning to enhance cancer diagnosis and classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fakoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ladhak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML&apos;13</title>
				<meeting>of ICML&apos;13</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Integrative data analysis of multi-platform cancer data with a multimodal deep learning approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Computational Biology and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="928" to="937" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dolphin: Real-time hidden acoustic signal capture with smartphones</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koutsonikolas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Mobile Computing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="560" to="573" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">General data protection regulation</title>
		<ptr target="https://en.wikipedia.org/wiki/GeneralDataProtectionRegulation" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Privacy-preserving multivariate statistical analysis: Linear regression and classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SDM&apos;04</title>
				<meeting>of SDM&apos;04</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="222" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Privacy-preserving logistic regression</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Monteleoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS&apos;09</title>
				<meeting>of NIPS&apos;09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="289" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Privacy-preserving distributed kmeans clustering over arbitrarily partitioned data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Jagannathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of KDD&apos;05</title>
				<meeting>of KDD&apos;05</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="593" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning in a large function space: Privacy-preserving mechanisms for svm learning</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">I</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Taft</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0911.5708</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Crowdml: A privacy-preserving learning framework for a crowd of smart devices</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hamm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Champion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICDCS&apos;15</title>
				<meeting>of ICDCS&apos;15</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Privynet: A flexible framework for privacy-preserving deep neural network training</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Z</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06161</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gilad-Bachrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dowlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lauter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naehrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wernsing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML&apos;16</title>
				<meeting>of ICML&apos;16</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="201" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Oblivious neural network predictions via minionn transformations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Juuti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Asokan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CCS&apos;17</title>
				<meeting>of CCS&apos;17</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="619" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Secureml: A system for scalable privacypreserving machine learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mohassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of S&amp;P&apos;17</title>
				<meeting>of S&amp;P&apos;17</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="19" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Privacy-preserving collaborative model learning: The case of word vector training</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2381" to="2393" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Chameleon: A hybrid secure computation framework for machine learning applications</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Riazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weinert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tkachenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Songhori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Koushanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AsiaCCS&apos;18</title>
				<meeting>of AsiaCCS&apos;18</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="707" to="721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Xonn: Xnor-based oblivious deep neural network inference</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Riazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Samragh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Lauter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Koushanfar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1505" to="1518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning with differential privacy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CCS&apos;16</title>
				<meeting>of CCS&apos;16</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="308" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Differential privacy preservation for deep auto-encoders: an application of human behavior prediction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI&apos;16</title>
				<meeting>of AAAI&apos;16</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1309" to="1316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Privacy-preserving deep learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CCS&apos;15</title>
				<meeting>of CCS&apos;15</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1310" to="1321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Model inversion attacks that exploit confidence information and basic countermeasures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ristenpart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CCS&apos;15</title>
				<meeting>of CCS&apos;15</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1322" to="1333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Membership inference attacks against machine learning models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stronati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of S&amp;P&apos;17</title>
				<meeting>of S&amp;P&apos;17</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stealing machine learning models via prediction apis</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Juels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ristenpart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of USENIX Security&apos;16</title>
				<meeting>of USENIX Security&apos;16</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="601" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep models under the gan: information leakage from collaborative deep learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hitaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ateniese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pérez-Cruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CCS&apos;17</title>
				<meeting>of CCS&apos;17</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="603" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep learning and differential privacy</title>
		<ptr target="https://github.com/frankmcsherry/blog/blob/master/posts/2017-10-27.md" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Differentially private federated learning: A client level perspective</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nabi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.07557</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Differential privacy</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICALP&apos;06</title>
				<meeting>of ICALP&apos;06</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Functional mechanism: regression analysis under differential privacy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Winslett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of VLDB&apos;12</title>
				<meeting>of VLDB&apos;12</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1364" to="1375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A tutorial survey of architectures, algorithms, and applications for deep learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">APSIPA Transactions on Signal and Information Processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">e2</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deep learning based gait recognition using smartphones in the wild</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00338</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Nonlinear programming: analysis and methods</title>
		<author>
			<persName><forename type="first">M</forename><surname>Avriel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Courier Corporation</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive modeling</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Solving large scale linear prediction problems using stochastic gradient descent algorithms</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML&apos;04</title>
				<meeting>of ICML&apos;04</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">116</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Realtime and spatio-temporal crowd-sourced social network data publishing with differential privacy</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Dependable and Secure Computing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="591" to="606" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Differentially private highdimensional data publication via sampling-based inference</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of KDD&apos;15</title>
				<meeting>of KDD&apos;15</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="129" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bayesian differential privacy on correlated data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGMOD&apos;15</title>
				<meeting>of SIGMOD&apos;15</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="747" to="762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Calibrating data to sensitivity in private data analysis: a platform for differentially-private analysis of weighted datasets</title>
		<author>
			<persName><forename type="first">D</forename><surname>Proserpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of VLDB&apos;14</title>
				<meeting>of VLDB&apos;14</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="637" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Calibrating noise to sensitivity in private data analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of TCC&apos;06</title>
				<meeting>of TCC&apos;06</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="265" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mechanism design via differential privacy</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of FOCS&apos;07. IEEE</title>
				<meeting>of FOCS&apos;07. IEEE</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="94" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Privacy integrated queries: an extensible platform for privacy-preserving data analysis</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Mcsherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGMOD&apos;09</title>
				<meeting>of SIGMOD&apos;09</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="19" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Principles of mathematical analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Rudin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964">1964</date>
			<publisher>McGraw-Hill</publisher>
			<biblScope unit="volume">3</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
				<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Discovering frequent patterns in sensitive data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laxman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thakurta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of KDD&apos;10</title>
				<meeting>of KDD&apos;10</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="503" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">How to win the clonewars: efficient periodic n-times anonymous authentication</title>
		<author>
			<persName><forename type="first">J</forename><surname>Camenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hohenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kohlweiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lysyanskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meyerovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CCS&apos;06</title>
				<meeting>of CCS&apos;06</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="201" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dissent in numbers: Making strong anonymity scale</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Wolinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Corrigan-Gibbs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of OSDI&apos;12</title>
				<meeting>of OSDI&apos;12</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="179" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Minnesota population center. public use microdata series, international: Version 6.4. university of minnesota</title>
		<ptr target="https://international.ipums.org" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">He received the Ph.D. degree from Illinois Institute of Technology, USA. His research interests include AI security, data storage, search and computation outsourcing security and privacy, wireless system security, big data security and privacy, and applied cryptography etc</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE TrustCom&apos;16, WAIM&apos;14, and IEEE ICNP&apos;11 etc. He serves as Associate Editors for IEEE Transactions on Dependable and Secure Computing (TDSC) and IEEE Transactions on Information Forensics and Security (TIFS)</title>
				<meeting><address><addrLine>China; China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2018. 2016. 2018. 2004 and 2012. 2010 to 2011. 2015</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1136" to="1150" />
		</imprint>
		<respStmt>
			<orgName>Central South University ; Cyber Science and Engineering, Wuhan University ; Cyber Science and Engineering, Wuhan University ; sion from Wuhan University ; Computer Vision Lab, University of South Carolina, USA. Currently ; School of Computer Science, Wuhan University</orgName>
		</respStmt>
	</monogr>
	<note>His research activities involve computer vision, pattern recognition, and machine learning. He is a Senior Member of the IEEE and a Member of the ACM. Yan Zhang received the B.S. degree from China University of Geoscience in 2014 and the M.S. degree from Wuhan University in 2017. He is currently a software engineer at the Huawei Technologies Co., Ltd. His current research interests include differential privacy in data publishing, data mining and machine learning</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Her research interests include spectrum management for Femtocell networks, network economics, network security, and Quality of Experience (QoE) of multimedia delivery/distribution</title>
		<author>
			<persName><forename type="first">Yanjiao</forename><surname>Chen Received Her</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Electronic Engineering from Tsinghua University in 2010 and the Ph.D. degree in Computer Science and Engineering from Hong Kong University of Science and Technology in 2015. She is currently a Professor in Wuhan University</title>
				<meeting><address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
