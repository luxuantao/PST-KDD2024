<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large Scale Evolving Graphs with Burst Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yifeng</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiangwei</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">DAMO Academy, Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
							<email>yang.yhx@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">DAMO Academy, Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Le</forename><surname>Song</surname></persName>
							<email>le.song@antfin.com</email>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>jietang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Large Scale Evolving Graphs with Burst Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Analyzing large-scale evolving graphs are crucial for understanding the dynamic and evolutionary nature of social networks. Most existing works focus on discovering repeated and consistent temporal patterns, however, such patterns cannot fully explain the complexity observed in dynamic networks. For example, in recommendation scenarios, users sometimes purchase products on a whim during a window shopping. Thus, in this paper, we design and implement a novel framework called BurstGraph which can capture both recurrent and consistent patterns, and especially unexpected bursty network changes. The performance of the proposed algorithm is demonstrated on both a simulated dataset and a world-leading E-Commerce company dataset, showing that they are able to discriminate recurrent events from extremely bursty events in terms of action propensity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dynamic networks, where edges and vertices arrive over time, are ubiquitous in various scenarios, (e.g., social media, security, public health, computational biology and user-item purchase behaviors in the E-Commerce platform <ref type="bibr" target="#b1">[Akoglu and Faloutsos, 2013;</ref><ref type="bibr" target="#b1">Akoglu et al., 2015]</ref>), and have attracted significant research interests in recent years. An important problem over dynamic networks is burst detection -finding objects and relationships that are unlike the normal. There are many practical applications spanning numerous domains of burst detection, such as bursty interests of users in E-Commerce <ref type="bibr">[Parikh and Sundaresan, 2008]</ref>, cross-community relationships in social networks. Recently, the research community has focused on network embedding learning. One class of the network embedding methods represent nodes as single points in a low-dimensional latent space, which aims to preserve structural and content information of the network <ref type="bibr">[Perozzi et al., 2014;</ref><ref type="bibr" target="#b2">Grover and Leskovec, 2016]</ref>. Other classes include edge embedding and subgraph embedding <ref type="bibr" target="#b3">[Dong et al., 2017]</ref>. However, most existing network embedding methods mainly focus on the network structure, ignoring the bursty links appearing in the dynamic networks <ref type="bibr">[Perozzi et al., 2014;</ref><ref type="bibr" target="#b1">Dai et al., 2016;</ref><ref type="bibr" target="#b3">Qiu et al., 2018]</ref>.</p><p>In social network dynamics, users may generate consistent temporal patterns by buying consumable goods, such as food and papers, to satisfy their recurrent needs; or purchasing durable products, such as cell phones and cars, to satisfy their longtime needs. However, in the real world, bursty links are very common in network evolution. For instance, in a social network, people will meet new friends or discover new interests if they are in a new environment; in an E-Commerce network, customers often do window shopping when they are exploring recommendation sections. Figure <ref type="figure" target="#fig_0">1</ref> illustrates the interest evolution of the young lady during shopping. However, existing works on modeling dynamic networks mostly focus on repeated and consistent patterns <ref type="bibr">[Trivedi et al., 2017;</ref><ref type="bibr" target="#b3">Li et al., 2017;</ref><ref type="bibr" target="#b4">Zhou et al., 2018]</ref>, and cannot well capture bursty links due to their sparsity. Such important bursty information is commonly viewed as noisy data in the general machine learning algorithms and ignored in modeling <ref type="bibr" target="#b1">[Chandola et al., 2009]</ref>. Furthermore, these bursty dynamics are hidden in other complex network dynamics, including the addition/removal of edges and the update of edge weights. It is challenging to design a framework to account for all these changes. To tackle the aforementioned challenges, we propose a novel framework with contributions summarized as follows:</p><p>• Problem Formulation: we formally define the problem of evolving graphs with bursty links. The key idea is to detect bursty links in dynamic graphs during their onset.</p><p>• Algorithms: we propose a novel framework for modeling evolving graphs with bursty links, namely Burst-Graph. BurstGraph divides graph evolution into two parts: vanilla evolution and bursty links' occurrences.</p><p>For the sparsity of bursty links, a spike-and-slab distribution <ref type="bibr" target="#b3">[Mitchell and Beauchamp, 1988]</ref> is introduced as an approximation posterior distribution in the variational autoencoder (VAE) <ref type="bibr" target="#b3">[Kingma and Welling, 2013]</ref> framework, while vanilla evolution accepts the original framework of VAE. To fully exploit the dynamic information in graph evolution, we propose an RNN-based dynamic neural network by capturing graph structures at each time step.</p><p>The cell of RNN maintains information of both vanilla and bursty evolution, which is updated over time.</p><p>The rest of this paper is organized as follows: Section 2 briefly reviews related work. Section 3 introduces the problem statement of evolving graphs with bursty links and presents the proposed framework. Experiments on both simulated dataset and real datasets are presented in Section 4 with discussions. Finally, Section 5 concludes the paper and visions the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Static Network Embedding. Recently, learning representations for networks has attracted considerable research efforts. Inspired by <ref type="bibr">Word2Vec [Mikolov et al., 2013]</ref>, <ref type="bibr">[Perozzi et al., 2014;</ref><ref type="bibr" target="#b2">Grover and Leskovec, 2016</ref>] learn a node representation with its neighborhood contexts. As an adjacency matrix is used to represent the topology of a network, representative works, such as <ref type="bibr" target="#b3">[Qiu et al., 2018]</ref>, use matrix factorization to learn low-rank space for the adjacency matrix. Deep learning methods <ref type="bibr" target="#b4">[Wang et al., 2016]</ref> are proposed to introduce effective non-linear function learning in network embedding.</p><p>Dynamic Network Embedding. Actually, inductive static methods <ref type="bibr">[Perozzi et al., 2014;</ref><ref type="bibr" target="#b2">Hamilton et al., 2017a]</ref> can also handle dynamic networks by making inference of the new vertices. <ref type="bibr" target="#b2">[Du et al., 2018]</ref> extends the skip-gram methods to update the original vertices' embedding. <ref type="bibr" target="#b4">[Zhou et al., 2018]</ref> focuses on capturing the triadic structure properties for learning network embedding. Considering both the network structure and node attributes, <ref type="bibr" target="#b3">[Li et al., 2017]</ref> focuses on updating the top eigenvectors and eigenvalues for the streaming network.</p><p>Burst Detection. Traditionally, burst detection is to detect an unexpectedly large number of events occurring within some time duration. There are two typical types of burst detection approaches, i.e., threshold-based <ref type="bibr" target="#b3">[Heard et al., 2010]</ref> and statebased methods <ref type="bibr">[Kleinberg, 2003]</ref>. <ref type="bibr" target="#b3">[Heard et al., 2010]</ref> studies fast algorithms using self-similarity to model bursty time series. <ref type="bibr">[Kleinberg, 2003]</ref> uses infinite-state automaton to model the burstiness and extract structure from text streams. However, the link building of evolving graphs is usually sparse and slow, where unexpected links are rare to occur simultaneously. Therefore, our definition of a burst is simply an unexpected behavior within a time duration, which is a straightforward definition adopted by many applications in the real world. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Model</head><p>Existing dynamic network embedding methods mainly focus on the expansion of new vertices and new edges, but usually ignore the changes of vertices' attributes through time that lead to unexpected bursty network changes <ref type="bibr" target="#b1">[Angel et al., 2012]</ref>.</p><p>To tackle this problem, we propose to learn low-dimensional representations of vertices to capture both vanilla and bursty evolution. Given a dynamic network {G 1 , ..., G T }, the dynamic network embedding is to learn a series of functions , where each function f φt maps vertices in network G t to low-dimensional vectors: f φt (v i ) − → R d and φ t s are corresponding network parameters. We first summarize symbols and notations in Table <ref type="table" target="#tab_0">1</ref> and use bold uppercase for matrices (e.g., A), bold lowercase for vectors (e.g., a), normal lowercase for scalars (e.g., a). 1 denotes a vector whose elements are all 1 and I denotes the identity matrix. The framework of the proposed model is illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. It mainly consists of two components, namely the original VAE for vanilla evolution and the spike-and-slab model to detect bursty links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">VAE for Graph Evolution</head><p>After detecting the bursty links at each discrete time t, we split the adjacency matrix of graph A t into two parts: vanilla adjacency matrix A v,t and burst adjacency matrix A b,t . To capture information of these two parts, we extend the framework of VAE and introduce a spike-and-slab distribution to simulate the sparsity of burst adjacency matrix. The loss function of VAE at each time step t is written as:</p><formula xml:id="formula_0">L t = ln n i=1 p(A i,t |G i,t ) = n i=1 ln Z S p(A vi,t |z t ) p(A bi,t |s t )p(z t |G i,t )p(s t |G i,t )dz t ds t ,<label>(1)</label></formula><p>where G i,t denotes the graph structure of vertex v i at time step t. z t and s t are random variables in the VAE-based model for vanilla and bursty evolution respectively. The evidence lower bound (ELBO) of VAE <ref type="bibr" target="#b3">[Kingma and Welling, 2013</ref>] can be written as:</p><formula xml:id="formula_1">L t =E zt∼q φ (zt|Gi,t) [ln p θ (A vi,t |z t )p(z t ) q φ (z t |G i,t ) ] + λ • E st∼q φ (st|Gi,t) [ln p θ (A bi,t |s t )p(s t ) q φ (s t |G i,t ) ],<label>(2)</label></formula><p>where importance weight λ is a hyperparameter. θ and φ are parameters of the encoder and decoder networks respectively. The approximate posterior distribution of random variable z t follows a Gaussian distribution: </p><formula xml:id="formula_2">q φ (z t |G i,t ) ∼ N (µ 0 (G i,t ), Σ 0 (G i,t )).</formula><formula xml:id="formula_3">u i,t = GraphSAGE(G i,t ) µ 0 (G i,t ) = f µ0 (u i,t ) Σ 0 (G i,t ) = f Σ0 (u i,t ),<label>(3)</label></formula><p>where µ 0 (G i,t ) and Σ 0 (G i,t ) share the same GraphSAGE network to learn representation from topology structure and attributes of vertex v i . In our paper, f µ0 (•) and f Σ0 (•) are both two fully-connected layers where hidden layers are activated by RELU function. Similar to the original framework of VAE <ref type="bibr" target="#b3">[Kingma and Welling, 2013]</ref>, the prior of random variable z t follows a standard Gaussian distribution. This is no longer suitable in the case of rare and discrete bursty links. In our paper, the approximate posterior distribution of random variables s t for bursty links is set to follow a spike-and-slab distribution [Mitchell and <ref type="bibr" target="#b3">Beauchamp, 1988]</ref>:</p><formula xml:id="formula_4">c t |G i,t iid ∼ Bernoulli(ψ(G i,t )) r t,1 |G i,t ∼ N (0, 1 β Σ 1 (G i,t )) r t,2 |G i,t ∼ N (µ 1 (G i,t ), •Σ 1 (G i,t )) s t = (1 − c t ) r t,1 + c t r t,2 ,<label>(4)</label></formula><p>where µ 1 (•) and Σ 1 (•) are the encoder networks, with the same neural network structure settings as µ 0 (•) and Σ 0 (•) in Equation (3), respectively. ψ(•) is also an encoder network which is two fully-connected layers activated by sigmoid function.</p><p>The value for β &gt; 0 is predefined with usual setting β = 100 for more flexbilities <ref type="bibr" target="#b3">[Ishwaran et al., 2005]</ref>. Therefore, r t,1 follows a spike distribution while r t,2 is a slab distribution.</p><p>For easy implementation, the prior distribution p * (s t ) of s t is set to: p * (s t ) ∼ (1 − α) • N (0, 1 100 I) + α • N (0, I), where α = {α i } and each α i is drawn from a Bernoulli distribution:</p><formula xml:id="formula_5">{α i } iid ∼ Bernoulli( 1 2 ).</formula><p>The regularization loss of spike-and-slab variable s t in ELBO can be written as:</p><formula xml:id="formula_6">−D KL (q(s t |G i,t )||p(s t )) =E q(st|Gi,t) [ln p(c t ) q(c t |G i,t ) + ln p(r t,1 ) q(r t,1 |G i,t ) + ln p(r t,2 ) q(r t,2 |G i,t ) ] = − D KL (q(c t |G i,t )||p(c t )) − D KL (q(r t,1 |G i,t )||p(r t,1 )) − D KL (q(r t,2 |G i,t )||p(r t,2 )) = − ln 2 − ψ(G i,t ) ln ψ(G i,t ) − (1 − ψ(G i,t )) ln(1 − ψ(G i,t )) + 1 2 (1 + ln(Σ 1 (G i,t )) − Σ 1 (G i,t ) − β • µ 2 1 (G i,t )) + 1 2 (1 + ln Σ 1 (G i,t ) − Σ 1 (G i,t ) − µ 2 1 (G i,t )),</formula><p>(5) By using the reparameterization trick, the random variables like z t , c t , r t,1 and r t,2 can be rewritten as:</p><formula xml:id="formula_7">z t = µ 0 (G i,t ) + γ 0 • Σ 1 2 0 (G i,t ) c t = σ(ln − ln (1 − ) + ln ψ(G i,t ) + ln (1 − ψ(G i,t ))) r t,1 = γ 1 • ( 1 β • Σ 1 (G i,t )) 1 2 r t,2 = µ 1 (G i,t ) + γ 2 • Σ 1 2 1 (G i,t ),<label>(6)</label></formula><p>where follows the uniform distribution ∼ U(0, 1). γ 0 , γ 1 and γ 2 all follow the standard Gaussian distribution: γ 0 , γ 1 , γ 2 ∼ N (0, I). The decoder network f s (s t ) = σ(W s • s t ) is a transition function to reconstruct the bursty links A bi,t in time t. For vanilla evolution, the framework are similar with the original VAE, where the prior of random variable z t follows a standard Gaussian distribution and the decoder network f z (z t ) is a fully-connected network activated by sigmoid function to reconstruct the vanilla connection A vi,t at time t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning Representations For Dynamic Network</head><p>In the evolving network settings, the goal is to learn the evolving changes from a set of sequential graphs G = {G 1 , G 2 , ..., G T }. To take the temporal structure of the sequential graphs into account, we introduce an RNN structure into our framework as shown in Figure <ref type="figure" target="#fig_3">3</ref>. The prior of the latent random variables is not set to follow a standard distribution, but depends on the last hidden variable h t−1 : In the similar way, the approximate posterior distribution of these random variables will depend on both the current network snapshot G i,t and the last hidden variable h t−1 :</p><formula xml:id="formula_8">z t |h t−1 ∼ N (µ 0 (h t−1 ), Σ 0 (h t−1 )) r t,1 |h t−1 ∼ N (0, 1 β • Σ 1 (h t−1 )) r t,2 |h t−1 ∼ N (µ 1 (h t−1 ), Σ 1 (h t−1 )) c t |h t−1 iid ∼ Bernoulli(ψ(h t−1 )),<label>(7)</label></formula><formula xml:id="formula_9">z t |G i,t , h t−1 ∼ N (µ 0 (G i,t , h t−1 ), Σ 0 (G i,t , h t−1 )) r t,1 |G i,t , h t−1 ∼ N (0, 1 β • Σ 1 (G i,t , h t−1 )) r t,2 |G i,t , h t−1 ∼ N (µ 1 (G i,t , h t−1 ), Σ 1 (G i,t , h t−1 )) c t |G i,t , h t−1 iid ∼ Bernoulli(ψ(G i,t , h t−1 )),<label>(8)</label></formula><p>In the same way of RNN, the hidden variable h t is updated based on the previous hidden variable h t−1 , the random variables z t and s t :</p><formula xml:id="formula_10">h t = f h (h t−1 , z t , s t ),</formula><p>where f h (•) is also a fully-connected network to transform the hidden variables.</p><p>Training. We adopt Adam optimizer <ref type="bibr" target="#b3">[Kingma and Ba, 2014]</ref> to optimize the objective and also introduce dropout with weight penalties into our proposed model. As expected, we penalize L1-norm of weight W s to induce the sparsity of the output. It is worth to note that all the parameters of our proposed model are shared along dynamic graphs over time interval (1, T ). GraphSAGE in encoder network also shares a random features input for each vertice over time interval (1, T ). The edge evolution is always sparse and unbalanced, which brings trouble to identify the positive instances to achieve better performance. Instead of traditional sigmoid cross entropy loss function, we use inter-and-intra class balanced loss in our model:</p><formula xml:id="formula_11">L = 1 2 ( L nod pos n nod pos + L nod neg n nod neg ) inter−class loss + 1 2 ( L cls pos n cls pos + L cls neg n cls neg ) intra−class loss</formula><p>, where L pos and L neg are the cross entropy losses for positive and negative samples, respectively. Similarly, L nod and L cls are the cross entropy losses for labels in each node and class. Similar to the setting of loss, n pos and n neg define the number of positive and negative samples, while n nod and n cls define the number of labels in each node and class, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>In this section, we evaluate our model in the dynamic setting from the performance on the multi-class link prediction.</p><p>Dataset. We first use the simulated data to verify the effectiveness of the model, then we apply BurstGraph to a real challenging dataset from a world-leading E-Commerce company.</p><p>Table <ref type="table">3</ref>: Results(%) comparison of different embedding methods. We use bold to highlight winners.</p><p>• Simulated Dataset: We generate a sequence of synthetic bipartite graph snapshots of 21K vertices and 527K edges that share similar characteristics as real data. We assume that each vertex has two types (vanilla/burst) of representations. More specifically, we divide the generation process into two parts: the first part is to generate hyperparameters for each vertex to ensure the independence; the second part is to generate links by sampling two representations of vertices from Gaussian distributions with fixed hyperparameters. First, for each vertex, we sample the hyperparameters µ 0 , σ<ref type="foot" target="#foot_1">2</ref> 0 from the uniform distributions µ 0 ∼ U(0, 2.5) and σ 2 0 ∼ U(0, 0.01) for the vanilla evolution, and µ 1 , σ 2 1 from µ 1 ∼ U(−2.5, 0) and σ 2 1 ∼ U(0, 1) for the bursty evolution respectively. To make sure that bursty links are sparse, the variance σ 2 1 is set to be bigger than σ 2 0 . We then generate the links in each snapshot as follows: we first determine the link types of each vertex pair with probability drawn from Bernoulli(0.1) for bursty links and vanilla links otherwise. We then generate the links for each vertex pair. According to their link types, we resample the representations of these two vertices from corresponding Gaussian distributions: N (µ 0 , σ 2 0 ) or N (µ 1 , σ 2 1 ). A fixed weight W is employed to transform these two representations into a single value. In all simulated graph snapshots, we set a threshold to truncate around 4% of vertex pairs as links. Results are illustrated in Figure <ref type="figure" target="#fig_4">4</ref>.</p><p>• Alibaba Dataset: We collect this dataset from a worldleading E-Commerce company Alibaba with two types of nodes, user and item. The bursty link between user and item is defined as if the user has no interaction with the item or similar items in the same category during the last 15 days according to business needs. The other links are viewed as vanilla links. With dynamic graph setting, we split the whole graph into a sequence of graph snapshots with the same time interval (e.g., 1 day). The dataset with sampled items is denoted as Alibaba-S dataset. The statistics of the above datasets are shown in Table <ref type="table" target="#tab_1">2</ref>. In each dataset, graph snapshot G t consists of all the nodes and interactions that appear at time step t. In our experimental setting, we hide a set of edges from the original graph and train on the remaining graph. The test dataset contains 10% randomly selected vertices, while the negative links are also randomly selected with the same number of positive links for each link type (vanilla/burst). Baseline Methods We compare our model against the following network embedding algorithms.  <ref type="bibr">et al., 2013]</ref> to learn latent representations by treating walks as the equivalent of sentences.</p><p>• GraphSAGE: GraphSAGE 2 <ref type="bibr" target="#b2">[Hamilton et al., 2017b]</ref> is a general inductive framework of network embedding, which leverages topological structure and attribute information of vertices to efficiently generate vertex embedding. Besides, GraphSAGE can still maintain a good performance even with random features input.</p><p>• CTDNE: CTDNE <ref type="bibr" target="#b3">[Nguyen et al., 2018]</ref> is a continuoustime dynamic network embedding method. CTDNE generates temporal random walk as the context information of each vertex and uses Skip-gram algorithm to learn latent representations. In this method, the time of each edge is simply valued according to the number of its snapshot.</p><p>• TNE: TNE 3 [Zhu et al., 2016] is a dynamic network embedding algorithm based on matrix factorization. Besides, this method holds a temporal smoothness assumption to ensure the continuity of the embeddings in evolving graphs.</p><p>It is worthy to mention, DeepWalk and GraphSAGE are static network embedding methods. To facilitate the comparison between our method and these relevant baselines, these two methods are trained with the whole graph, which includes all graph snapshots. In the following, we compare the performance of these methods with BurstGraph 4 on three datasets in terms of Micro-F1 and Macro-F1.</p><p>Comparison Results. Table <ref type="table">3</ref> shows the overall performance of different methods on three datasets. Our model BurstGraph is able to consistently outperform all sorts of baselines in various datasets. Next we compare and analyze results on vanilla evolution and burst links, respectively. For the vanilla evolution, BurstGraph has a performance gain of +1.4% in terms of Micro-F1 and +3.5% in terms of Macro-F1 on average. For the bursty evolution, BurstGraph outperforms other methods +5.5% in terms of Micro-F1 and +5.3% in terms of Macro-F1 averagely. These results show that splitting evolving graphs into vanilla and bursty evolution not only benefits for the performance of the bursty evolution, but also benefits for that of the vanilla evolution. Moreover, compared to other baselines, the performance of BurstGraph is quite robust over the three datasets. Notice that DeepWalk, CTDNE and TNE perform poorly on the simulated dataset. One potential reason could be that the simulated generation process may be easier for message passing algorithms (e.g., Graph-SAGE) compared to matrix factorization based methods (e.g., DeepWalk, CTDNE or TNE).</p><p>Parameter Analysis. We investigate the sensitivity of different hyperparameters in BurstGraph including importance weight λ and random variable dimension d. Figure <ref type="figure" target="#fig_5">5</ref> shows the performance of BurstGraph on Alibaba-S dataset when altering the importance weight λ and variable dimension d, respectively. From part a), we can see that the performance of vanilla link prediction is stable when changing the importance weight λ. The performance of burst link prediction rises with the increase of importance weight λ and converges slowly when importance weight is larger than 1. From part b), we can conclude that the performance of BurstGraph is relatively stable within a large range of variable dimension, and the performance decreases when the variable dimension is either too small or too large.</p><p>Visualization of vertex representations. We visualize the embedding vectors of sampled vertices in the simulated dataset and Alibaba-S dataset learned by BurstGraph. We project the embedding vectors to a 2-dimensional space with t-SNE method. As shown in Figure <ref type="figure">6</ref>, embeddings from the vanilla evolution can be clearly separated from embeddings of the bursty evolution. More specifically, the embeddings of vertexes in the vanilla evolution evenly spread out in the space, while the embeddings of vertexes in the bursty evolution gather 3 https://github.com/linhongseba/Temporal-Network-Embedding 4 https://github.com/ericZhao93/BurstGraph in a relatively small area. The reason could be that the vertex embedding in the vanilla evolution is highly depended on its attributes, while the vertex embedding in the bursty evolution is another way around because of sparsity. Figure <ref type="figure">6</ref>: 2D visualization on embeddings (100 randomly selected vertices) for vanilla evolution and bursty evolution. This visualizes the embeddings from vanilla and bursty evolution on Simulated dataset (left) and Alibaba-S dataset (right). Embeddings from vanilla evolution are spread out while embeddings from bursty evolution concentrate in a relatively small area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we propose a novel approach for evolving graphs and assume that the evolving of edges in a sequence of graph snapshots can be split into two parts: vanilla and bursty evolution. In addition, these two parts utilize variational autoencoders based on two different prior distributions to reconstruct the graph evolution, respectively. The vanilla evolution follows a Gaussian distribution, when the burst evolution follows a spike-and-slab distribution. Experiment results on real-world datasets show the benefits of our model on bursty links prediction in evolving graphs. However, there still exist limitations in our model. First, only bursty links are considered in our framework. However, there exist other bursty objects, e.g., vertices and communities, which should also be taken into account. We plan to extend our approach to support these bursty objects in the future. Second, we plan to propose a new time series model that supports continuous inputs rather than discretized graph snapshots.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustrative example of the observed interest evolution of a young lady during shopping. The main interests of the lady are clothes and shoes, while there also exist burst interests, such as a mop.</figDesc><graphic url="image-1.png" coords="1,315.54,461.66,241.91,115.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illustration of the proposed framework BurstGraph. At time step t, the framework generates vanilla evolution and bursty evolution based on network structure Gt. Part a is an original VAE for vanilla evolution, where random variable zt follows a Gaussian distribution. Part b is an extended VAE for bursty evolution, where random variable st follows a spike-and-slab distribution because of the sparsity of bursty links. The encoder for these two random variables zt and st shares the same GraphSAGE to utilize the information from vertices and their neighbors.</figDesc><graphic url="image-2.png" coords="3,54.00,54.00,503.99,201.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>µ 0 (•) and Σ 0 (•) are the encoder networks, which can be any highly flexible functions such as neural networks [Kingma and Welling, 2013]. We use Graph-SAGE [Hamilton et al., 2017a], a representative framework of graph convolutional network, to generate representation from vertex attributes and its neighbours:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: An illustration of the RNN structure in BurstGraph, the hidden variable ht is updated by last hidden variable ht−1, random variables zt and st. h0 is set to a zero vector. The prior s t and z t depend on ht−1. The initial prior s 0 follows the distribution p * (st) mentioned before, while the initial prior z t follows a standard Gaussian distribution.</figDesc><graphic url="image-3.png" coords="3,315.55,333.31,241.91,161.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Adjacency matrix heatmap of sub-graphs with four time stamps of the simulated dataset. Each row represents a user, and each column represents an item. The red points represent the vanilla links between users and items, and the black points represent the burstyCompared to the bursty evolution, vanilla evolution is more regular and consistent over time.</figDesc><graphic url="image-7.png" coords="5,447.58,277.91,80.36,66.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The performance of BurstGraph on Alibaba-S dataset with increasing importance weight λ (left) or variable dimension d (right).</figDesc><graphic url="image-12.png" coords="6,321.30,289.65,230.40,129.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of Notation.</figDesc><table><row><cell>Notation</cell><cell>Description</cell></row><row><cell>G t</cell><cell>network snapshot at time step t</cell></row><row><cell>A t</cell><cell>adjacency matrix for network structure in G t</cell></row><row><cell>A v,t</cell><cell>adjacency matrix for vanilla evolution in G t</cell></row><row><cell>A vi,t</cell><cell>adjacency vector for vertex v i in the vanilla evolution in G t</cell></row><row><cell>A b,t</cell><cell>adjacency matrix for bursty evolution in G t</cell></row><row><cell>A bi,t</cell><cell>adjacency vector for vertex v i in the bursty evolution in G t</cell></row><row><cell>h t</cell><cell>hidden variable of RNN-based VAE at t</cell></row><row><cell>z t</cell><cell>random variable for vanilla evolution</cell></row><row><cell>s t</cell><cell>random variable for bursty evolution</cell></row><row><cell cols="2">c t , r t,1 , r t,2 composition variables of s t</cell></row><row><cell>λ, β</cell><cell>hyperparameters of our method</cell></row><row><cell>d</cell><cell>dimension of random variables</cell></row><row><cell>n</cell><cell>total number of nodes in G</cell></row><row><cell>T</cell><cell>number of time steps</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of datasets.</figDesc><table><row><cell>dataset</cell><cell>#vertices</cell><cell>#edges</cell><cell cols="2">#classes #time</cell></row><row><cell>Simulate</cell><cell>20,118</cell><cell>527,268</cell><cell>118</cell><cell>6</cell></row><row><cell>Alibaba-S</cell><cell>19,091</cell><cell>479,441</cell><cell>213</cell><cell>6</cell></row><row><cell>Alibaba-L</cell><cell>25,432</cell><cell>3,745,819</cell><cell>7,419</cell><cell>6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/phanein/deepwalk</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://github.com/williamleif/GraphSAGE</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Alibaba-L vanilla evolution bursty evolution vanilla evolution bursty evolution vanilla evolution bursty evolution Model Micro Macro Micro Macro Micro Macro Micro Macro Micro Macro Micro Macro</title>
		<author>
			<persName><forename type="first">-S</forename><surname>Alibaba</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dense subgraph maintenance under streaming edge weight updates for real-time story identification</title>
		<author>
			<persName><forename type="first">Faloutsos</forename><forename type="middle">;</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><surname>Akoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</title>
				<editor>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</editor>
		<meeting>the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009-07">2013. 2013. 2015. 2015. 2012. 2012. 2009. July 2009. 2016. 2016. 2017. 2017</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
	<note>Leman Akoglu, Hanghang Tong, and Danai Koutra</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dynamic network embedding: An extended approach for skip-gram based network embedding</title>
		<author>
			<persName><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2018. 2018. 2016. 2016. 2017a. 2017. 2017b. 2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nish Parikh and Neel Sundaresan. Scalable and near real-time burst detection from ecommerce queries</title>
		<author>
			<persName><surname>Heard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<idno>arXiv:1312.6114</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<editor>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</editor>
		<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1988">2010. 2010. 2005. 2005. 2014. 2014. 2013. 2013. 2003. 2003. 2017. 2017. 2013. 2013. 1988. 1988. 2018. 2018. 2018. 2008. 2008. 2014. 2018. 2018. 2017</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="3462" to="3471" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the 34th International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Junming Yin, Greg Ver Steeg, and Aram Galstyan. Scalable temporal latent space inference for link prediction in dynamic social networks</title>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016. 2016. 2018. 2018. 2016</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2765" to="2777" />
		</imprint>
	</monogr>
	<note>Dynamic network embedding by modeling triadic closure process</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
