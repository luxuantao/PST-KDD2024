<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BRIO: Bringing Order to Abstractive Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yixin</forename><surname>Liu</surname></persName>
							<email>yixin.liu@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
							<email>dragomir.radev@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
							<email>gneubig@cs.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BRIO: Bringing Order to Abstractive Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstractive summarization models are commonly trained using maximum likelihood estimation, which assumes a deterministic (onepoint) target distribution in which an ideal model will assign all the probability mass to the reference summary. This assumption may lead to performance degradation during inference, where the model needs to compare several system-generated (candidate) summaries that have deviated from the reference summary. To address this problem, we propose a novel training paradigm which assumes a non-deterministic distribution so that different candidate summaries are assigned probability mass according to their quality. Our method achieves a new state-of-the-art result on the CNN/DailyMail (47.78 ROUGE-1) and XSum (49.07 ROUGE-1) datasets. Further analysis also shows that our model can estimate probabilities of candidate summaries that are more correlated with their level of quality. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural methods for abstractive summarization <ref type="bibr" target="#b44">(Rush et al., 2015;</ref><ref type="bibr" target="#b34">Nallapati et al., 2016;</ref><ref type="bibr" target="#b6">Chopra et al., 2016;</ref><ref type="bibr" target="#b23">Lewis et al., 2020;</ref><ref type="bibr">Zhang et al., 2020)</ref> formulate summarization as a sequenceto-sequence (Seq2Seq) problem <ref type="bibr" target="#b51">(Sutskever et al., 2014)</ref>, learning to generate the summary in an autoregressive manner. Such models are commonly trained with maximum likelihood estimation (MLE), maximizing predictive probability of the reference output given the gold sub-sequence before it. However, during inference the model must also generate the output based on possibly erroneous previous steps. This can hurt model performance, a phenomenon often called exposure bias <ref type="bibr" target="#b1">(Bengio et al., 2015;</ref><ref type="bibr" target="#b43">Ranzato et al., 2016)</ref>. To maintain reasonable performance even in the case of a sub-sequence with errors, we argue that the model must accurately estimate relative quality of different generated outputs, since effective inference requires comparison among these candidates.</p><p>To understand whether existing models can accurately perform such relative comparisons, we conducted a preliminary study on pre-trained BART <ref type="bibr" target="#b23">(Lewis et al., 2020)</ref>, first generating two candidate summaries from the model and observing whether a higher probability is assigned to the candidate with a higher ROUGE <ref type="bibr" target="#b26">(Lin, 2004)</ref> score. As Tab. 1 shows, the accuracy is far from ideal. This is likely due to the fact that MLE training only encourages the model to assign high probability to the reference summary, and is agnostic about any relative comparison between non-reference summaries. However, we argue that it is also important for the order of model scores to be coordinated with the actual quality metrics by which the summaries will be evaluated -higher model scores should indicate better quality summaries. In the following we will refer to models that have such scores as "coordinated" for conciseness.</p><p>We introduce a training paradigm which requires the abstractive model to be able to be accurate with respect to predicting the tokens in the reference summaries and coordinated with respect to arXiv: <ref type="bibr">2203.16804v1 [cs.CL]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? ?</head><p>Figure <ref type="figure">1</ref>: Comparison of MLE loss (LMLE) and the contrastive loss (LCtr) in our method. MLE assumes a deterministic (one-point) distribution, in which the reference summary receives all the probability mass. Our method assumes a nondeterministic distribution in which system-generated summaries also receive probability mass according to their quality. The contrastive loss encourages the order of model-predicted probabilities of candidate summaries to be coordinated with the actual quality metric M by which the summaries will be evaluated. We assign the abstractive model a dual role -a single model could be used both as a generation model and a reference-free evaluation model.</p><p>the candidate summaries. In other words, we give the abstractive model a dual role: as a generation model, it generates the output summaries in an autoregressive way; as an evaluation model, it can be used to score the quality of candidate summaries by estimating a probability distribution over candidate outputs. The generation model is trained using the standard MLE loss, but to train the evaluation model we introduce a contrastive loss <ref type="bibr" target="#b13">(Hadsell et al., 2006)</ref> defined over different candidate summaries generated by pre-trained abstractive models (Fig. <ref type="figure">1</ref>), following previous work on ranking-based or contrastive learning <ref type="bibr" target="#b16">(Hopkins and May, 2011;</ref><ref type="bibr" target="#b67">Zhong et al., 2020;</ref><ref type="bibr">Liu et al., 2021b)</ref>.</p><p>Our main contribution is to change the target distribution of abstractive models from a one-point deterministic distribution assumed by MLE training to a non-deterministic distribution in which candidate summaries are also assigned probability mass according to their quality. The new SOTA performance on <ref type="bibr">CNN/DailyMail (Hermann et al., 2015)</ref> and XSum <ref type="bibr" target="#b35">(Narayan et al., 2018)</ref> datasets demonstrated the effectiveness of our method. Our in-depth analysis also found that the abstractive models trained using our method can estimate the candidate summary quality more accurately, in concert with the the objective of our training paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Abstractive Summarization</head><p>The goal of abstractive summarization is to create a function g that takes a source document D and generates an appropriate summary S S ? g(D)</p><p>(1)</p><p>Training Objective Neural abstractive summarization models aim to learn a neural model g that results in good summaries. Maximum likelihood estimation (MLE) is the standard training algorithm. It aims to maximize the likelihood of the reference summary S * , i.e.,</p><formula xml:id="formula_0">? * = argmax ? i log p g ? (S * (i) |D (i) ; ?) (2)</formula><p>where ? denotes the parameters of g and p g ? denotes the probability distribution entailed by these parameters. The summation is over the training set and {D (i) , S * (i) } is the i-th training sample. For a specific sample {D (i) , S * (i) }, Eq. 2 is equivalent to minimizing the sum of negative loglikelihoods of the tokens {s</p><formula xml:id="formula_1">* 1 , ? ? ? , s * j , ? ? ? , s * l }</formula><p>in the reference summary S * whose length is l, which is the cross-entropy loss:</p><formula xml:id="formula_2">Lxent = - l j=1 s ptrue(s|D, S * &lt;j ) log pg ? (s|D, S * &lt;j ; ?)<label>(3)</label></formula><p>where S * &lt;j denotes the partial reference sequence {s * 0 , ? ? ? , s * j-1 } and s * 0 is a pre-defined start token. p true is a one-hot distribution under the standard MLE framework:</p><formula xml:id="formula_3">ptrue(s|D, S * &lt;j ) = 1 s = s * j 0 s = s * j (4)</formula><p>In practice, label smoothing <ref type="bibr" target="#b52">(Szegedy et al., 2016)</ref> is a widely used and effective technique that modifies the target distribution in Eq. 4 to a "soft" label by assigning probability mass ? to other tokens:</p><formula xml:id="formula_4">ptrue(s|D, S * &lt;j ) = 1 -? s = s * j ? N -1 s = s * j (5)</formula><p>where N is the size of the dictionary. Inference and Exposure Bias During inference, the abstractive model g is used to generate the candidate summary in an autoregressive manner. It is intractable to enumerate all the possible candidate outputs, so in practice methods such as beam search are used to reduce the search space.</p><p>One important step in search is estimating the probability of the next word s t given the previous predicted sequence S &lt;t :</p><formula xml:id="formula_5">p g ? (s t |D, S &lt;t ; ?)<label>(6)</label></formula><p>Comparing Eq. 6 with Eq. 3, the major difference is that during inference the model makes new predictions based on its own previous predictions S &lt;t instead of the reference S * &lt;t . As a result, even if the generation model g achieves very high accuracy w.r.t. Eq. 3, once S &lt;t starts to deviate from S * , there is the risk that the performance of g will significantly degrade. This problem has been identified as the exposure bias <ref type="bibr" target="#b1">(Bengio et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Coordinating Abstractive Models</head><p>Eq. 6 implies that the abstractive model g should be able to assign higher estimated probability to the better candidate summary during inference. However, this intuition is not directly captured in the standard MLE objective used in training -a model obtaining zero MLE loss would assign zero probability to any candidate summary different from the reference. This is obviously improper for any task where multiple reasonable generations may exist <ref type="bibr" target="#b18">(Khayrallah et al., 2020)</ref>, and also does not say anything about the ordering of two imperfect references. We therefore advocate for making the alternative assumption that the probability of one candidate should be well-correlated with its quality as evaluated by an automatic metric M . Since it is intractable to enumerate all the possible candidate outputs, we only require our model to be able to accurately predict the ranking order of a set of the most probable candidate summaries ?, which are its own beam search results. In order to achieve this objective, we slightly modify the conditions of Eq. 5, maintaining the general functional form, but instead specifying the marginal probability of the non-reference candidates S to be ?, and encouraging coordination of probabilities and qualities among non-reference candidates as follows:</p><formula xml:id="formula_6">? ? ? ? ? ? ? ? ? p true ? (S|D) = 1 -? S = S * S?S p true ? (S|D) = ? S = S * p true ? (Si|D) &gt; p true ? (Sj|D) ?Si, Sj ? ?, M (Si) &gt; M (Sj)<label>(7)</label></formula><p>We next describe precisely how we encourage coordination through contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrastive Learning for Coordination</head><p>The candidate quality measure M can be defined in many ways. In this work we define it as the ROUGE <ref type="bibr" target="#b26">(Lin, 2004</ref>) score of a candidate summary S i given the reference summary S * . To coordinate a pre-trained abstractive model, we 1) use it to generate different candidate summaries with various levels of quality,<ref type="foot" target="#foot_1">2</ref> then 2) encourage the model to assign higher estimated probabilities to better candidates by fine-tuning the model with a contrastive loss, following the previous work <ref type="bibr" target="#b16">(Hopkins and May, 2011;</ref><ref type="bibr" target="#b67">Zhong et al., 2020)</ref>:</p><formula xml:id="formula_7">Lctr = i j&gt;i max(0, f (Sj) -f (Si) + ?ij)<label>(8)</label></formula><p>where S i and S j are two different candidate summaries and ROUGE(S i , S * ) &gt; ROUGE(S j , S * ), ?i, j, i &lt; j. ? ij is the margin multiplied by the difference in rank between the candidates, i.e.,</p><formula xml:id="formula_8">? ij = (j -i) * ?. f (S i ) is the length-normalized estimated log-probability 3 f (S) = l t=1 log p g ? (s t |D, S &lt;t ; ?) |S| ? (9)</formula><p>where ? is the length penalty hyperparameter. This loss gives the abstractive model a dual purpose, first as a reference-free evaluation model, which can be used in a two-stage summarization pipeline, where it is used to score the candidates generated by a pre-trained generation model and select the final output from them. However, since the autoregressive generation depends on both the token-level prediction accuracy and sequencelevel coordination, the model fine-tuned with the contrastive loss alone can no longer be used as a generation model. Multi-task Fine-tuning Following <ref type="bibr" target="#b9">Edunov et al. (2018)</ref>, we combine the contrastive (Eq. 8) and cross-entropy (Eq. 3) losses to preserve the generation ability of the pre-trained abstractive model:</p><formula xml:id="formula_9">L mul = L xent + ?L ctr (<label>10</label></formula><formula xml:id="formula_10">)</formula><p>where ? is the weight of the contrastive loss. We note that the contrastive and the cross-entropy loss can effectively complement each other -since the contrastive loss is defined on the sequence level, the token-level cross-entropy loss serves as a normalization to ensure that the model could assign balanced probability mass across the whole sequence.</p><p>Training Methods of Seq2Seq Models In order to align the training objective and evaluation metric, structured losses have been used for the Seq2Seq model training. Among them, marginbased losses <ref type="bibr" target="#b14">(Herbrich et al., 1999;</ref><ref type="bibr" target="#b53">Taskar et al., 2004;</ref><ref type="bibr" target="#b11">Gimpel and Smith, 2010)</ref>, which require the model to assign higher probability to the better output, are a major category. Many margin-based losses used in modern seq2seq models <ref type="bibr" target="#b59">(Wiseman and Rush, 2016;</ref><ref type="bibr" target="#b9">Edunov et al., 2018)</ref> assume a deterministic (one-point) distribution: a model can achieve zero loss if it can assign a much higher probability to the (pseudo)-reference, regardless of relative comparisons of other candidate summaries. By contrast, our method has a non-deterministic assumption (Eq. 7), which focuses on the pair-wise ranking of a set of candidate summaries.</p><p>One main challenge of directly optimizing a Seq2Seq model with quality scores of the output is that the discrete sampling process makes the loss non-differentiable. To circumvent this problem, reinforcement learning has been used to reformulate the conditional text generation tasks <ref type="bibr" target="#b43">(Ranzato et al., 2016;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2016;</ref><ref type="bibr" target="#b24">Li et al., 2016;</ref><ref type="bibr" target="#b41">Paulus et al., 2018;</ref><ref type="bibr" target="#b25">Li et al., 2019)</ref>. Compared to this school of methods, our method is based on supervised learning, and it is more stable and less sensitive to the design choices (e.g. reward shaping), which are well-known challenges of reinforcement learning methods. Minimum risk training <ref type="bibr" target="#b48">(Shen et al., 2016;</ref><ref type="bibr" target="#b58">Wieting et al., 2019)</ref> and other online sampling based methods <ref type="bibr" target="#b1">(Bengio et al., 2015;</ref><ref type="bibr" target="#b36">Norouzi et al., 2016;</ref><ref type="bibr" target="#b66">Zhang et al., 2019)</ref> belong to another school of methods used to circumvent the problem of non-differentiability. However, they also exhibit similar problems of stability as reinforcement learning. Contrastive Learning Recently, contrastive learning <ref type="bibr" target="#b13">(Hadsell et al., 2006)</ref> has been introduced into several conditional text generation tasks, such as machine translation <ref type="bibr" target="#b62">(Yang et al., 2019;</ref><ref type="bibr" target="#b39">Pan et al., 2021)</ref>, text summarization <ref type="bibr" target="#b2">(Cao and Wang, 2021;</ref><ref type="bibr" target="#b61">Xu et al., 2021;</ref><ref type="bibr" target="#b50">Sun and Li, 2021)</ref>, and other tasks <ref type="bibr" target="#b54">(Uehara et al., 2020;</ref><ref type="bibr" target="#b5">Cho et al., 2021;</ref><ref type="bibr">Lee et al., 2021b)</ref>. Among these application scenarios, most work deployed contrastive learning in the latent representation space, following the framework proposed in <ref type="bibr" target="#b3">Chen et al. (2020)</ref>. However, in this work we adopt contrastive learning over the discrete space of the generated texts.</p><p>Besides, instead of constructing the contrastive learning examples by rule-based methods (e.g. perturbing the reference output), we use the generation models to construct the examples, which makes the contrastive learning task closer to the generation task. Sun and Li (2021) also adopted contrastive learning on the generated texts. However, their formulation belongs to the margin-based losses. We have discussed the difference between our method and the margin-based losses in the previous paragraphs. Discriminative Reranking Discriminative reranking has been widely studied for conditional generation tasks <ref type="bibr" target="#b47">(Shen et al., 2004;</ref><ref type="bibr">Och et al., 2004;</ref><ref type="bibr" target="#b56">Wan et al., 2015;</ref><ref type="bibr" target="#b31">Mizumoto and Matsumoto, 2016)</ref>. Some recent works <ref type="bibr" target="#b30">(Liu and Liu, 2021;</ref><ref type="bibr">Lee et al., 2021a)</ref> have also explored discriminative reranking of candidates from neural natural language generation models, which adopt large pre-trained language models (e.g. BERT <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref>) as the reranker. In this work, we factorize the Seq2Seq model (e.g., BART) trained on the same dataset as the reranking model, which maximizes the parameter sharing across two stages. Besides, our approach contributes an instance of leveraging large pre-trained Seq2Seq models as a quality estimation model <ref type="bibr" target="#b63">(Yuan et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head><p>Datasets We mainly use three datasets in our experiments (statistics in Appendix A). CNNDM 4 (Hermann et al., 2015) is a large scale news dataset. Following <ref type="bibr" target="#b34">Nallapati et al. (2016)</ref>, we treat the news articles as the source documents and the associated highlights as the summaries. XSum<ref type="foot" target="#foot_4">5</ref>  <ref type="bibr" target="#b35">(Narayan et al., 2018</ref>) is a highly abstractive dataset of articles from the British Broadcasting Corporation (BBC). NYT<ref type="foot" target="#foot_5">6</ref>  <ref type="bibr" target="#b45">(Sandhaus, 2008)</ref> contains articles from the New York Times and the associated summaries. We follow <ref type="bibr" target="#b17">Kedzie et al. (2018)</ref> for data preprocessing and splitting, and use the associated archival abstracts as the summaries. Baselines We choose a variety of related models with strong performance as baselines. BART <ref type="bibr" target="#b23">(Lewis et al., 2020)</ref> and PEGASUS <ref type="bibr">(Zhang et al., 2020)</ref> are both large pre-trained Seq2Seq</p><p>LMs standard in the literature. GSum <ref type="bibr" target="#b8">(Dou et al., 2021)</ref> is built on BART, and improves performance by using additional guidance from an extractive summarizer. SimCLS <ref type="bibr" target="#b30">(Liu and Liu, 2021)</ref> introduces a two-stage framework where the pre-trained BART model is used to generate candidates and a pre-trained RoBERTa <ref type="bibr" target="#b28">(Liu et al., 2019)</ref> model is fine-tuned as an evaluation model to score the candidate summaries and select from them. It achieves state-of-the-art performance on both CNNDM and XSum. GOLD (Pang and He, 2021) uses offline reinforcement learning to train the BART model by treating the reference summaries as the demonstrations, a different formulation that can also improve the performance of the original BART. SeqCo <ref type="bibr" target="#b61">(Xu et al., 2021)</ref> and ConSum <ref type="bibr" target="#b50">(Sun and Li, 2021)</ref> are two recent methods that aim to leverage contrastive learning to improve the performance of the abstractive summarization model (BART). Implementation Details In the following experiments, we use either BART or PEGASUS as a backbone. We label our proposed methods BRIO, with two variants: (1) BRIO-Ctr is fine-tuned with the contrastive loss (Eq. 8) only; (2) BRIO-Mul is fine-tuned with the multi-task loss (Eq. 10). We use BRIO-Ctr as an evaluation model that scores different candidate summaries generated by a Seq2Seq abstractive model and selects the final output from them, and BRIO-Mul as a standard Seq2Seq model that takes the source documents as input and generates the output in an autoregressive manner. Further details are in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>The results are shown in Tab 2. For CNNDM and NYT we use BART as the backbone model while for XSum we use the pre-trained PEGASUS model as our base model since it achieves better performance than BART. We have the following observations:</p><p>(1) BRIO-Ctr outperforms SimCLS, its counterpart as an evaluation model in a two-stage summarization framework. Specifically, both BRIO-Ctr and SimCLS are used to score the candidate summaries generated by a Seq2Seq abstractive model (BART). The final outputs are selected based on those scores. We attribute BRIO-Ctr's superior performance to its use of the same model architecture (BART) for both candidate generation and scoring, while SimCLS uses RoBERTa as the evaluation model. As a result, BRIO-Ctr maximizes the parameter sharing between the two stages, and preserves the power of the Seq2Seq model pre-trained on the same dataset.</p><p>(2) BRIO-Mul is able to establish the new stare-of-the-art performance on CNNDM. Notably, the previous state-of-the-art model, GSum, takes additional guidance as input and needs a separate encoder to encode the guidance information, while BRIO-Mul uses the same parameterization of BART. Compared to other methods (ConSum, SeqCo, GOLD) that aim to improve upon BART, BRIO-Mul performs much better, showing the effectiveness of our training method.</p><p>(3) Since on XSum we use PEGASUS instead of BART as the base model, the result shows that our method is not restricted to the specific choice of the base model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis</head><p>We further perform some in-depth analyses from diverse perspectives on the CNNDM dataset to gain more insights into our proposed method.  <ref type="table">3</ref>: Model performance with different ? coefficients weighting the contrastive loss (Eq. 10) on CNNDM. BRIO-Ctr is trained with the contrastive loss only, which no longer preserves its generation ability. We report its performance when it is used as an evaluation model to select from candidate summaries. R-1/2/L are the ROUGE-1/2/L F1 scores.   Coefficients of the Multi-Task Loss The multitask loss (Eq. 10) used to train our model contains two parts: the cross-entropy loss and the contastive loss. As shown in Tab. 3, as the weight of the contrastive loss (?) increases, the model's performance improves. However, the cross-entropy loss is still necessary to preserve the model's ability as a generation model. We argue that this is because the token level accuracy is still important during the autoregressive generation process, where the individual tokens are predicted sequentially. In addition, we also found that the model tends to achieve the best performance (w.r.t the ROUGE scores on the development set) faster with a higher ?. Specifically, it requires less than one entire epoch to achieve the best performance on CNNDM, making our approach an efficient fine-tuning method.</p><formula xml:id="formula_11">Coefficient (?) R-1 R-2 R-L 0 (</formula><p>Generation-Finetuning as a Loop Since the fine-tuned model (BRIO-Mul) is still able to gen- Increasing the Beam Width While theoretically a larger beam width (i.e. the number of candidates maintained during beam search) would allow more candidates to be considered and therefore increase the upper bound of the performance, in practice model performance may be lower if the beam width is too large. The reason for this phenomenon is closely related to the low sequence-level coordination of the generator. Specifically, increasing the beam width may introduce candidates with lower quality <ref type="bibr" target="#b49">(Stahlberg and Byrne, 2019)</ref>, and the generator may not be able to differentiate them from high-quality candidates. In Tab. 5, we compare the performance of the pre-trained BART and our model (BRIO-Mul) with different beam widths used during inference. We observe that the performance of BART goes down as the beam width increases. On the other hand, our model is able to achieve better performance with a larger number of beams, demonstrating that our training method can improve the coordination of the model by encouraging the model to assign estimated probabilities to candidate summaries wellcorrelated with their quality. Training with Different Evaluation Metrics In the previous experiments, we used ROUGE as the evaluation metric to define the target ordering of the candidate summaries (Eq.7). To evaluate our method's performance beyond ROUGE, we use a model-based semantic similarity metric, BERTScore <ref type="bibr">(Zhang* et al., 2020)</ref>, 7 as the evaluation metric M in Eq.7 to compare the performance of different candidate summaries. Then, we trained another version of BRIO-Mul based on the order of candidate summaries calculated by BERTScore.</p><formula xml:id="formula_12">Beams BART BRIO-Mul R-1 R-2 R-1 R-2<label>4</label></formula><p>The results in Tab. 6 show that (1) Our model can significantly improve the model performance when either ROUGE or BERTScore is used as the target evaluation metric for ordering candidate summaries. This suggests that it is possible to use our method to optimize any specific target metric, making our method an alternative to reinforcement learning or minimum risk training. (2) Our model that is trained on one evaluation metric (e.g. BERTScore) also achieves improvement on another metric (e.g. ROUGE) compared with the baseline model, which indicates that the improvement made by our model is not from exploiting the potential weaknesses of individual metrics. Besides, this result also demonstrates a non-trivial degree of agreement between ROUGE and BERTScore. Novel n-grams We compare the ratio of novel n-grams in reference, BRIO-Mul's, and BART's summaries. As Tab. 7 shows, our model is more "abstractive" compared to BART, although reference summaries still contain more novel n-grams. This is likely due to the fact that our model is optimized at the sequence-level, allowing more freedom for paraphrasing and compression.</p><p>We further investigate the relation of the "abstractiveness" and model performance by com-7 https://github.com/Tiiiger/bert_score. We use its default version for English texts.  paring our model (BRIO-Mul) with the baseline model (BART) on different buckets of test examples grouped by the "novelty" of the reference summaries,<ref type="foot" target="#foot_6">8</ref> i.e.,</p><formula xml:id="formula_13">Novelty(D, S * ) = g?G S * 1(g / ? GD) |GS * |<label>(11)</label></formula><p>where D and S * are the source document and reference summary respectively, G D and G S * are the sets of bigrams in D and S * , 1 is the indicator function. The results in Fig. <ref type="figure" target="#fig_2">3</ref> show that when novelty is higher, (1) all models' performance decreases;</p><p>(2) our model achieves larger improvement over the baseline model.</p><p>Rank Correlation We computed the rank correlation between the estimated probabilities of the candidate summaries calculated by the generators and the quality scores of the candidate summaries. We use Eq. 9 to calculate the estimated probabilities<ref type="foot" target="#foot_7">9</ref> and we use ROUGE-1 as the quality score metric of the candidate summaries. We calculate  Spearman's rank correlation for each sample, and use the average score as the overall correlation, We investigated two specific settings: 1) ranking candidate summaries generated by a different model (PEGASUS); 2) ranking candidate summaries generated by themselves (BART &amp; BRIO-Mul). We use 16 candidates in total for calculation. As Tab. 8 shows, our model achieves better rank correlation on the candidate summaries generated by both itself and the independent model. This suggests that our model can better estimate the quality of candidate summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Token-level Calibration</head><p>Calibration requires that a model's confidence on its predictions is equal to the accuracy of these predictions <ref type="bibr" target="#b12">(Guo et al., 2017)</ref>. Previous work <ref type="bibr" target="#b32">(M?ller et al., 2019;</ref><ref type="bibr" target="#b20">Kumar and Sarawagi, 2019;</ref><ref type="bibr" target="#b57">Wang et al., 2020)</ref> has found that a more calibrated text generation model tends to have better performance, and techniques like label smoothing can improve both the token-level calibration and sequence-level accuracy (i.e. the ability of generating better results). One intuitive explanation of this phenomenon is to interpret the model's estimated probability of a generated summary as the product of the model's confidences on a series of tokenlevel predictions. Then, since a more calibrated model's confidence estimates better the accuracy of its predictions, the model's estimated probability of one sequence should be more indicative of the quality of this sequence, which is essential for the beam search during inference. However, the relation of token-level calibration and sequencelevel performance remains inconclusive <ref type="bibr" target="#b32">(M?ller et al., 2019)</ref>. <ref type="foot" target="#foot_8">10</ref> For example, a generator that always predicts a uniform distribution over all tokens would be perfectly calibrated, however, such a model would not generate high-quality outputs.</p><p>We investigate this relation from the opposite direction by evaluating whether our model (BRIO-Mul), which is trained to have better sequencelevel performance, would also be more calibrated at the token-level compared with the baseline models that are trained using MLE and label smoothing. We follow previous work by using the Expected Calibration Error <ref type="bibr" target="#b33">(Naeini et al., 2015)</ref> (ECE) as the evaluation metric of calibration:</p><formula xml:id="formula_14">ECE = M m=1 |B m | n |acc(B m ) -conf(B m )| (12)</formula><p>where the samples are grouped into M equal-width buckets by confidence (conf), B m denotes the m-th bucket, and n is the total number of samples. Following <ref type="bibr" target="#b57">Wang et al. (2020)</ref>, we evaluate model calibration on the system-generated summaries during inference and use the tercom toolkit<ref type="foot" target="#foot_9">11</ref> to assign labels (correct/incorrect) to the system-generated summaries based on the reference summaries.</p><p>The results in Tab. 9 show that BRIO-Mul is better calibrated compared to BART, suggesting that our method helps to improve the token-level calibration by explicitly encouraging the model to have more accurate sequence-level probability estimations. The reliability graph is shown in Fig. <ref type="figure" target="#fig_3">4</ref>. We found that (1) abstractive models are generally over-confident on their own predictions, (2) models are generally more calibrated on XSum than CNNDM. This is likely due to the fact that XSum has shorter summaries therefore it is less likely to be affected by the exposure bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Few-shot Fine-tuning</head><p>The training paradigm proposed in this paper may be extended to any Seq2Seq model. However, it can be a non-trivial overhead to generate the candidate summaries using large neural models on the entire training set. On the other hand, recent work <ref type="bibr" target="#b42">(Raffel et al., 2020;</ref><ref type="bibr">Zhang et al., 2020;</ref><ref type="bibr">Schick and Sch?tze, System Summary</ref> Reference chelsea forward tammy abraham nets first-half double for chelsea. dominic solanke adds a third late on as chelsea look set to win trophy. manchester city struggle without injured star thierry ambrose. read: mourinho warns his young chelsea players he can not play them all. click here to read our match report from man city 's academy stadium.</p><p>BART tammy abraham scored twice in the first half to give chelsea the lead. isaac buckley-ricketts levelled the game for manchester city. dominic solanke scored late on to put a gloss on the scoreline. click here to read sportsmail's player ratings from the youth cup final.</p><p>BRIO-Mul chelsea beat manchester city 3-1 in the youth cup final at the etihad stadium. tammy abraham scored twice in the first half to give chelsea the lead. dominic solanke scored late on to seal the win for the home side.</p><p>Reference alejandro valverde won ahead of julian alaphilippe and michael albasini. chris froome finished 123rd after a crash during the final 12 kilometres. team sky's sports director gabriel rasch praised froome for finishing. rasch said froome was 'banged up' but expects to ride tour de romandie.</p><p>BART movistar rider alejandro valverde won fleche wallonne on wednesday. team sky's chris froome fell in the final 12km but finished the race. philippe gilbert pulled out of the race after a bad crash 50km from the end. click here for more cycling news.  2021; <ref type="bibr" target="#b10">Fabbri et al., 2021)</ref> has shown that few-shot learning can be an effective fine-tuning method of pre-trained models for text generation tasks. Therefore, we investigate our model's performance in a few-shot setting. Specifically, we randomly sample 100/1000 examples from the training set of CNNDM/XSum, and fine-tune the models that are pre-trained using MLE loss on those examples. More training details can be found in Appendix C. The results are shown in Tab. 11. All experiments are repeated three times, and the reported results are the average performance. The results indicate that our model can achieve improvement over the baseline model under the few-shot learning setting with a small computational overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BRIO-Mul</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Case Study on CNNDM</head><p>Tab. 10 presents an interesting pattern we observed when comparing the results of BRIO-Mul and BART, which demonstrates that our method helps the abstractive model to filter out noise patterns in the original data. Specifically, some of the reference summaries (331/11490) in CNNDM contains the phrase "click here", pointing to a hyperlink, and 103 source documents also contain this phrase. BART picked up this pattern, and generates this phrase in 96 output summaries. On the contrary, our model learns to ignore this noise pattern and never generated it across the whole test set, likely because it identified that generated candidates with this pattern rarely achieve a high ROUGE score, and downweighted the probability accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this work, we presented a new training paradigm that assigns candidate outputs probability mass according to their quality using contrastive learning. While our method has achieved significant improvement on abstractive summarization, we note several directions for the future work to explore. First, since our method makes no assumptions specifically about the summarization task, it can be extended to other conditional text generation tasks such as machine translation. Second, it is possible to apply our method in a reinforcement learning setting, where the candidate summaries are dynamically generated. Finally, in experiments we only used diverse beam search to generate the candidate summaries, but it is likely that other candidate generation methods could yield further improvements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Loop of candidate generation and model finetuning.</figDesc><graphic url="image-1.png" coords="6,92.69,266.08,174.61,85.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>System</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance comparison (BART v.s. BRIO-Mul) w.r.t. reference summary novelty. The x-axis represents different buckets of test examples grouped by reference summary novelty (Eq. 11). Larger x-coordinates correspond to examples of which the reference summaries have higher novelty. The left figure shows the performance improvement of our model compared with the baseline model, while the right one shows model performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Reliability graphs on the CNNDM and XSum datasets. The accuracy of model's predictions is plotted against the model's confidence on these predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Accuracy of different abstractive summarization systems w.r.t ranking the quality of candidate summaries on CNNDM dataset. Acc. stands for the frequency of the model assigning higher probabilities to better candidate summaries. The candidate summaries are generated by a pre-trained model (BART), and we select the best and the worst candidates (w.r.t.</figDesc><table><row><cell>High</cell><cell>53.99</cell><cell>29.85</cell><cell>51.12</cell><cell>100.00</cell></row><row><cell>Low</cell><cell>33.48</cell><cell>10.85</cell><cell>30.45</cell><cell>0.00</cell></row><row><cell>BART</cell><cell>44.88</cell><cell>21.68</cell><cell>41.92</cell><cell>54.80</cell></row><row><cell>Ours</cell><cell>50.10</cell><cell>26.29</cell><cell>47.19</cell><cell>79.63</cell></row></table><note><p>ROUGE scores) for each of the samples. High and Low represent the average performance of the best and worst candidates respectively. R-1/2/L are the ROUGE-1/2/L scores. The original BART only achieves 54.80% accuracy.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>31 Mar 2022</figDesc><table><row><cell>Seq2Seq Generation Model</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Encoder</cell><cell></cell><cell>Decoder</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>? ? ? ? ? ? ? ?</cell><cell></cell><cell></cell></row><row><cell>Source Input</cell><cell></cell><cell>Reference Output</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>? ???</cell><cell></cell><cell></cell></row><row><cell>Reference-free Evaluation Model</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Decoder</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>? ? (?) ? ? (?) ? ? (?) ? ? (?)</cell><cell></cell><cell>?(?)</cell></row><row><cell>Encoder</cell><cell>? ???</cell><cell>Candidate Output ? Candidate Output ?</cell><cell>-</cell><cell>&gt;</cell></row><row><cell></cell><cell></cell><cell>? ? (?) ? ? (?) ? ? (?) ? ? (?)</cell><cell></cell><cell>?(?)</cell></row><row><cell>Source Input</cell><cell></cell><cell>Decoder</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Results on CNNDM with different beam widths (the number of beams) used in beam search. The default beam width is 4. R-1/2 are the ROUGE-1/2 F1 scores. erate, we can use it to generate a new set of candidates in the same way as we used the pre-trained BART model, and continue fine-tuning it on this newly created set of candidates<ref type="bibr" target="#b37">(Och, 2003)</ref>. Fig.2illustrates this iterative process. The results shown in Tab. 4 illustrate that this new model (BRIO-Loop) outperforms BRIO-Mul. Besides, the model reached the best performance very quickly, showing the potential of adopting our method in an online framework where the new candidates are dynamically generated from the current model. We leave this direction for future work.</figDesc><table><row><cell></cell><cell>44.29 21.17</cell><cell>47.78 23.55</cell></row><row><cell>10</cell><cell>43.83 20.76</cell><cell>47.98 23.81</cell></row><row><cell>20</cell><cell>43.53 20.49</cell><cell>48.07 23.92</cell></row><row><cell>50</cell><cell>43.06 20.05</cell><cell>48.18 24.01</cell></row><row><cell>100</cell><cell>42.79 19.76</cell><cell>48.23 24.09</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Results on CNNDM using different evaluation metrics as M in Eq.7. BRIO-Mul (R) is trained with candidate summaries ordered by ROUGE scores, while BRIO-Mul (B) is trained with candidate summaries ordered by BERTScore. R-1/2/L are ROUGE-1/2/L F1 scores. BS denotes BERTScore.</figDesc><table><row><cell>System</cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell><cell>BS</cell></row><row><cell>BART</cell><cell cols="4">44.29 21.17 41.09 27.38</cell></row><row><cell cols="5">BRIO-Mul (R) 47.78 23.55 44.57 32.11</cell></row><row><cell cols="5">BRIO-Mul (B) 47.53 23.22 44.37 32.59</cell></row><row><cell>System</cell><cell cols="3">Unigram Bigram</cell><cell></cell></row><row><cell>Reference</cell><cell></cell><cell>.1110</cell><cell>.4865</cell><cell></cell></row><row><cell>BART</cell><cell></cell><cell>.0101</cell><cell>.0924</cell><cell></cell></row><row><cell cols="2">BRIO-Mul</cell><cell>.0262</cell><cell>.2381</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Ratio of novel n-grams of different models on CNNDM. Novel n-grams are those that appear in the summaries but not in the source documents.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Rank Correlation between the model's estimated probabilities of the candidate summaries and the quality scores (ROUGE) of the candidate summaries on CNNDM. Own stands for the candidates generated by the models themselves, while PEGASUS stands for the candidates generated by the pretrained PEGASUS model.</figDesc><table /><note><p>?: significantly better than the baseline model (BART) (p &lt; 0.01).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Expected Calibration Error (ECE), accuracy (Acc) and confidence (Conf) on the test set of CNNDM and XSum.</figDesc><table><row><cell>Dataset</cell><cell>System</cell><cell>ECE</cell><cell>Acc</cell><cell>Conf</cell></row><row><cell>CNNDM</cell><cell cols="4">BART BRIO-Mul .2719 .4271 .6652 .4097 .3711 .7365</cell></row><row><cell>XSum</cell><cell cols="4">PEGASUS .2369 .4688 .6990 BRIO-Mul .1423 .4744 .5881</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>alejandro valverde defended his fleche wallonne title in belgium on wednesday. movistar rider finished ahead of julian alaphilippe and michael albasini. team sky's chris froome fell in the final 12km of the race but finished in 123rd. froome was involved in a crash but finished the race despite being 'banged up'Referencemanuel pellegrini won the premier league and capital one cup last season. city currently sit fourth in the league table -12 points behind chelsea. pellegrini's contract expires at the end of the 2015-16 season. city players have been impressed with vieira's work with the youth team. pep guardiola is city's first-choice to succeed pellegrini at the etihad.BART manuel pellegrini's future at manchester city is under scrutiny. patrick vieira is highly-respected among the city players. city's first-choice managerial option is bayern munich boss pep guardiola. click here for all the latest manchester city news. click here for more premier league news.BRIO-Mulmanchester city players have backed patrick vieira to replace manuel pellegrini as manager of the club. the frenchman is highly-respected among the players at the etihad stadium. pellegrini's future at the club is under scrutiny after a disappointing season. city's first-choice manager is current bayern munich boss pep guardiola.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Case Study on CNNDM. BRIO-Mul learns to ignore the noise pattern ("click here") while BART cannot.</figDesc><table><row><cell>Dataset</cell><cell>System</cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell></row><row><cell>CNNDM</cell><cell cols="4">BART BRIO-Few 45.81 21.91 42.61 44.29 21.17 41.09</cell></row><row><cell>XSum</cell><cell cols="4">PEGASUS 47.46 24.69 39.53 BRIO-Few 47.95 24.89 39.71</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Few-shot Fine-tuning. BRIO-Few is trained on only 100/1000 training examples on CNNDM and XSum respectively. R-1/2/L are ROUGE-1/2/L F1 scores.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We have made our code, results, and trained models publicly available at https://github.com/yixinL7/BRIO.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>This is achieved by using diverse beam search<ref type="bibr" target="#b55">(Vijayakumar et al., 2018)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>  3  We length-normalize as it is standard in comparing hypotheses in neural sequence generation<ref type="bibr" target="#b4">(Cho et al., 2014)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://cs.nyu.edu/~kcho/DMQA/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://github.com/EdinburghNLP/XSum</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://catalog.ldc.upenn.edu/LDC2008T19</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>The calculation is performed using ExplainaBoard(Liu  et al., 2021a). https://github.com/neulab/ExplainaBoard.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>We found the value of the length penalty factor ? in Eq. 9 by maximizing the rank correlation on the validation set.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8"><p>In general, better token-level calibration doesn't guarantee better sequence-level performance.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9"><p>11 http://cs.umd.edu/~snover/tercom/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_10"><p>The checkpoint is "facebook/bart-large-cnn", containing around 400M parameters.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_11"><p>The checkpoint is "google/pegasus-xsum"" containing around 568M parameters.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_12"><p>The checkpoint is "facebook/bart-large".</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_13"><p>https://github.com/summanlp/evaluation/tree/master/ ROUGE-RELEASE-1.5.5</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_14"><p>PTB tokenizer is used for tokenization. https: //nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ process/PTBTokenizer.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank the anonymous reviewers for valuable feedback and helpful suggestions.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details</head><p>We use diverse beam search <ref type="bibr" target="#b55">(Vijayakumar et al., 2018)</ref> to generate 16 candidates for each data sample. On CNNDM and XSum, we use the pre-trained BART 12 and PEGASUS 13 models from the Transformers <ref type="bibr" target="#b60">(Wolf et al., 2020)</ref>  </p><p>where warmup denotes the warmup steps, which is set to 10000, step is the number of updating steps, lr is the learning rate. We set the length penalty factor ? in the scoring function (Eq. 9) to the same value as used in the original beam search. We search the value of the margin ? in the contrastive loss (Eq. 8) within the range [1 ? 10 -5 , 1], and decide the value based on the model performance on the validation set. We also performed extensive search for the coefficient ? in Eq. 10. The specific hyper-parameter setting is reported in Tab. 13.</p><p>We use the standard ROUGE <ref type="bibr" target="#b26">(Lin, 2004</ref>) Perl package 15 for evaluation. The command line parameters are '-c 95 -r 1000 -n 2 -m'. Before the Datasets ? (Eq. 8) ? (Eq. 9) ? (Eq. 10) CNNDM 0.001 2.0 100 XSum 0.1 0.6 100 NYT 0.001 2.0 100 </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An actorcritic algorithm for sequence prediction</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philemon</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR, abs/1607.07086</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems -Volume 1, NIPS&apos;15</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems -Volume 1, NIPS&apos;15<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">CLIFF: Contrastive learning for improving faithfulness and factuality in abstractive summarization</title>
		<author>
			<persName><forename type="first">Shuyang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6633" to="6649" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/W14-4012</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<meeting>SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Contrastive multi-document question generation</title>
		<author>
			<persName><forename type="first">Sang</forename><surname>Woon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudha</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asli</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengdi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16</title>
		<meeting>the 16</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12" to="30" />
		</imprint>
	</monogr>
	<note>th Conference of the European Chapter</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Abstractive sentence summarization with attentive recurrent neural networks</title>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1012</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">GSum: A general framework for guided neural abstractive summarization</title>
		<author>
			<persName><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.384</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4830" to="4842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Classical structured prediction losses for sequence to sequence learning</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1033</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving zero and few-shot abstractive summarization with intermediate fine-tuning and data augmentation</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simeng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.57</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
		<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="704" to="717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Softmaxmargin CRFs: Training log-linear models with cost functions</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Los Angeles, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="733" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2006.100</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>USA. IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
	<note>CVPR &apos;06</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Support vector learning for ordinal regression</title>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Obermayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom??</forename><surname>Ko?isk?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tuning as ranking</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<publisher>UK. Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1352" to="1362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Content selection in deep learning models of summarization</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Kedzie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><surname>Daum?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1208</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1818" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simulated multiple reference training improves low-resource machine translation</title>
		<author>
			<persName><forename type="first">Huda</forename><surname>Khayrallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.7</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="82" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Calibration of encoder decoder models for neural machine translation</title>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<idno>CoRR, abs/1903.00802</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discriminative reranking for neural machine translation</title>
		<author>
			<persName><forename type="first">Ann</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.563</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7250" to="7264" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Contrastive learning with adversarial perturbations for conditional text generation</title>
		<author>
			<persName><forename type="first">Seanie</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Bok Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">Marjan Ghazvininejad,. 2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for dialogue generation</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1127</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1192" to="1202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with distributional semantic rewards for abstractive summarization</title>
		<author>
			<persName><forename type="first">Siyao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deren</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengda</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1623</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6038" to="6044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Zihuiwen Ye, and Graham Neubig. 2021a. ExplainaBoard: An explainable leaderboard for NLP</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuaichen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-demo.34</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="280" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">2021b. Ref-Sum: Refactoring neural summarization</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.113</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="1437" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SimCLS: A simple framework for contrastive learning of abstractive summarization</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-short.135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1065" to="1072" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Discriminative reranking for grammatical error correction with statistical machine translation</title>
		<author>
			<persName><forename type="first">Tomoya</forename><surname>Mizumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1133</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1133" to="1138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">When does label smoothing help?</title>
		<author>
			<persName><forename type="first">Rafael</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Obtaining well calibrated probabilities using bayesian binning</title>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">F</forename><surname>Mahdi Pakdaman Naeini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milos</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><surname>Hauskrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI&apos;15</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI&apos;15</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2901" to="2907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-to-sequence RNNs and beyond</title>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?aglar</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Gul?ehre</surname></persName>
		</author>
		<author>
			<persName><surname>Xiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K16-1028</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="280" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Don&apos;t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</title>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1206</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1797" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reward augmented maximum likelihood for neural structured prediction</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Zhifeng Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1723" to="1731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName><surname>Och</surname></persName>
		</author>
		<idno type="DOI">10.3115/1075096.1075117</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A smorgasbord of features for statistical machine translation</title>
		<author>
			<persName><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shankar</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Libin</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viren</forename><surname>Eng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004</title>
		<meeting>the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004<address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Contrastive learning for many-to-many multilingual neural machine translation</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.21</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="244" to="258" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Text generation by learning from demonstrations</title>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Richard Yuanzhe Pang and He He</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-totext transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Aurelio</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">In 4th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02">2016. 2016. May 2-4, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1044</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Evan</forename><surname>Sandhaus</surname></persName>
		</author>
		<title level="m">The New York Times Annotated Corpus. LDC corpora. Linguistic Data Consortium</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Few-shot text generation with natural language instructions</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.32</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="390" to="402" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Discriminative reranking for machine translation</title>
		<author>
			<persName><forename type="first">Libin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004</title>
		<meeting>the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004<address><addrLine>Boston, Massachusetts</addrLine></address></meeting>
		<imprint>
			<publisher>USA. Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Minimum risk training for neural machine translation</title>
		<author>
			<persName><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1159</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1683" to="1692" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">On NMT search errors and model errors: Cat got your tongue?</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Stahlberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Byrne</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1331</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3356" to="3362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Alleviating exposure bias via contrastive learning for abstractive text summarization</title>
		<author>
			<persName><forename type="first">Shichao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR, abs/2108.11846</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.308</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Max-margin markov networks</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning with contrastive examples for data-to-text generation</title>
		<author>
			<persName><forename type="first">Yui</forename><surname>Uehara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsuya</forename><surname>Ishigaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kasumi</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Noji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keiichi</forename><surname>Goshima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ichiro</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.213</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2352" to="2362" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Diverse beam search for improved description of complex scenes</title>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Vijayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramprasaath</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Multi-document summarization via discriminative summary reranking</title>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno>ArXiv, abs/1507.02062</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">On the inference calibration of neural machine translation</title>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.278</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3070" to="3079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Beyond BLEU:training neural machine translation with semantic similarity</title>
		<author>
			<persName><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1427</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4344" to="4355" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence learning as beam-search optimization</title>
		<author>
			<persName><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1137</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1296" to="1306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Sequence level contrastive learning for text summarization</title>
		<author>
			<persName><forename type="first">Shusheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno>CoRR, abs/2109.03481</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Reducing word omission errors in neural machine translation: A contrastive learning approach</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1623</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6191" to="6196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">BARTScore: Evaluating generated text as text generation</title>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Pegasus: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Liu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11328" to="11339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Bertscore: Evaluating text generation with bert</title>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Bridging the gap between training and inference for neural machine translation</title>
		<author>
			<persName><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1426</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4334" to="4343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Extractive summarization as text matching</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqing</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.552</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6197" to="6208" />
		</imprint>
	</monogr>
	<note>Xipeng Qiu, and Xuanjing Huang</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
