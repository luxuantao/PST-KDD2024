<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On Face Segmentation, Face Swapping, and Face Perception</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuval</forename><surname>Nirkin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Open University of Israel</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Robotics and Intelligent Systems</orgName>
								<orgName type="institution">USC</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anh</forename><surname>Tu Án Tr Àn</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Robotics and Intelligent Systems</orgName>
								<orgName type="institution">USC</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tal</forename><surname>Hassner</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Open University of Israel</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Information Sciences Institute</orgName>
								<orgName type="institution">USC</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gérard</forename><surname>Medioni</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Robotics and Intelligent Systems</orgName>
								<orgName type="institution">USC</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On Face Segmentation, Face Swapping, and Face Perception</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B3EEAE41F63B5E41C16334D7485394F1</idno>
					<idno type="DOI">10.1109/FG.2018.00024</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Face swapping</term>
					<term>segmentation</term>
					<term>recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure 1: Inter-subject swapping. LFW G.W. Bush photos swapped using our method onto very different subjects and images. Unlike previous work [4], [17], we do not select convenient targets for swapping.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Swapping faces means transferring a face from a source photo onto a face appearing in a target photo, attempting to generate realistic, unedited looking results. Although face swapping today is often associated with viral Internet memes <ref type="bibr" target="#b30">[31]</ref>, it is actually far more important than this practice may suggest: Face swapping can also be used for preserving privacy <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b29">[30]</ref>, digital forensics <ref type="bibr" target="#b30">[31]</ref> and as a potential face specific data augmentation method <ref type="bibr" target="#b28">[29]</ref> especially in applications where training data is scarce (e.g., facial emotion recognition <ref type="bibr" target="#b23">[24]</ref>).</p><p>Going beyond particular applications, face swapping is also an excellent opportunity to develop and test essential face processing capabilities: When faces are swapped between arbitrarily selected, unconstrained images, there is no guarantee on the similarity of viewpoints, expressions, 3D face shapes, genders or any other attribute that makes swapping easy <ref type="bibr" target="#b16">[17]</ref>. In such cases, swapping requires robust and effective methods for face alignment, segmentation, 3D shape estimation (though we will later challenge this assertion), expression estimation and more.</p><p>We describe a face swapping method and test it in settings where no control is assumed over the images or their pairings. We evaluate our method using extensive quantitative tests at a scale never before attempted by other face swapping methods. These tests allow us to measure the effect face swapping has on machine face recognition, providing insights from the perspectives of both security applications and face perception.</p><p>Technically, we focus on face segmentation and the design of a face swapping pipeline. Our contributions include:</p><p>• Semi-supervised labeling of face segmentation. We provide a novel means of generating a rich image set with face segmentation labels, by using using motion cues and 3D data augmentation. The data we collect is used to train a FCN to segment faces faster and more accurately than existing methods. • Face swapping pipeline. We describe a face swapping pipeline and show it to work well on images and image pairs of unprecedented difficulty. • Quantitative tests. Despite over a decade of work and contrary to other face processing tasks (e.g., recognition), face swapping methods were never quantitatively tested. We offer the first quantitative test protocols for intra-and inter-subject face swapping systems. Our qualitative results show that our swapped faces are as compelling as those produced by others, if not more. Our quantitative tests further show that our intra-subject face 98 2018 13th IEEE International Conference on Automatic Face &amp; Gesture Recognition swapping has little effect on face verification accuracy: our swapping does not introduce artifacts or otherwise changes these images in ways which affect subject identities.</p><p>We report inter-subject results on randomly selected pairs. These tests require facial appearance to change, sometimes substantially, in order to naturally blend source faces into their new surroundings. We show that this changes them, making them less recognizable. Though this perceptual phenomenon was described over two decades ago by Sinha and Poggio <ref type="bibr" target="#b34">[35]</ref> in their well-known Clinton-Gore illusion, we are unaware of previous quantitative reports on how this applies to machine face recognition.</p><p>For code and deep models, please see our project page. 1   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Face segmentation. To swap only faces, without their surrounding context or occlusions, we require per-pixel segmentation labels. Previous methods designed for this purpose include the work of <ref type="bibr" target="#b27">[28]</ref> which segment individual facial regions (e.g., eyes, mouth) but not the entire face. An example based method was proposed in <ref type="bibr" target="#b35">[36]</ref>. More recently, <ref type="bibr" target="#b8">[9]</ref> segmented faces by alternating between segmentation and landmark localization using deformable part models. They report state of the art performance on the Caltech Occluded Faces in the Wild (COFW) dataset <ref type="bibr" target="#b5">[6]</ref>. Two recent methods proposed to segment faces using deep neural networks. In <ref type="bibr" target="#b25">[26]</ref> a network was trained to simultaneously segment multiple facial regions, including the entire face. This method was used in the face swapping method of <ref type="bibr" target="#b16">[17]</ref>, but can be slow. The very recent method of <ref type="bibr" target="#b33">[34]</ref> recently outperformed <ref type="bibr" target="#b8">[9]</ref> on COFW as well as reported realtime processing speeds by using a deconvolutional neural network.</p><p>Face swapping. Methods for swapping faces were proposed as far back as 2004 <ref type="bibr" target="#b4">[5]</ref> with fully automatic techniques described nearly a decade ago <ref type="bibr" target="#b3">[4]</ref>. These methods were originally offered in response to privacy preservation concerns: Face swapping can be used to obfuscate identities of subjects appearing in publicly available photos, as a substitute to face pixelation or blurring <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Since then, however, many of their applications seem to come from recreation <ref type="bibr" target="#b16">[17]</ref> or entertainment (e.g., <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b37">[38]</ref>).</p><p>Regardless of the application, previous face swapping systems often share several key aspects. First, some methods restrict the target photos used for transfer. Given an input source face, they search through large face albums to choose ones that are easy targets for face swapping <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b16">[17]</ref>. Such targets are those which share similar appearance properties with the source, including facial tone, pose, expression and more. Though our method can be applied in similar settings, our tests focus on more extreme conditions, where 1 www.openu.ac.il/home/hassner/projects/faceswap the source and target images are arbitrarily selected and can be (often are) substantially different.</p><p>Second, most previous methods estimate the structure of the face. Some methods estimate 3D facial shapes <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b24">[25]</ref>, by fitting 3D Morphable Face Models (3DMM). Others instead estimate dense 2D active appearance models <ref type="bibr" target="#b39">[40]</ref>. This is presumably done in order to correctly map textures across different individual facial shapes.</p><p>Finally, deep learning was used to transfer faces <ref type="bibr" target="#b18">[19]</ref>, as if they were styles transfered between images. This method, however, requires the network to be trained for each source image and thus can be impractical in many applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SWAPPING FACES IN UNCONSTRAINED IMAGES</head><p>Fig. <ref type="figure" target="#fig_0">3</ref> summarizes our face swapping method. When swapping a face from a source image, I S , to a target image, I T , we treat both images the same, apart from the final stage (Fig. <ref type="figure" target="#fig_0">3(d)</ref>). Our method first localizes 2D facial landmarks in each image (Fig. <ref type="figure" target="#fig_0">3(b)</ref>). We use an off-the-shelf detector for this purpose <ref type="bibr" target="#b15">[16]</ref>. Using these landmarks, we compute 3D pose (viewpoint) and modify the 3D shape to account for expression. These steps are discussed in Sec. III-A.</p><p>We next segment faces from backgrounds and occlusions (Fig. <ref type="figure" target="#fig_0">3(c</ref>)) using a FCN trained to predict per-pixel face visibility (Sec. III-B). We describe how we generate rich labeled data to train our FCN. Finally, the source is efficiently warped onto the target using the two aligned 3D face shapes as proxies and blended onto the target (Sec. III-C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Fitting 3D face shapes</head><p>To enrich our set of examples for training the segmentation network (Sec. III-B) we explicitly model 3D face shapes. These 3D shapes are also used as proxies to transfer textures from one face onto another, when swapping faces (Sec. III-C). We experimented with two alternative methods of obtaining these 3D shapes.</p><p>The first, inspired by <ref type="bibr" target="#b11">[12]</ref> uses a generic 3D face, making no attempt to fit its shape to the face in the image aside from pose (viewpoint) alignment. We, however, also estimate facial expressions and modify the 3D face accordingly.</p><p>A second approach uses the recent state of the art, deep method for single image 3D face reconstruction <ref type="bibr" target="#b36">[37]</ref>. It was shown to work well on unconstrained photos such as those considered here. To our knowledge, this is the only method quantitatively shown to produce invariant, discriminative and accurate 3D shape estimations. The code they released regresses 3D Morphable face Models (3DMM) in neutral pose and expression. We extend it by aligning 3D shapes with input photos and modifying the 3D faces to account for facial expressions.</p><p>3D shape representation and estimation. Whether generic or regressed, we use the popular Basel Face Model (BFM) <ref type="bibr" target="#b31">[32]</ref> to represent faces and the 3DDFA Morphable Model <ref type="bibr" target="#b40">[41]</ref> for expressions. These are both publicly available 3DMM representations. More specifically, a 3-D face shape V ⊂ R 3 is modeled by combining the following independent generative models:</p><formula xml:id="formula_0">V = v + W S α + W E γ.</formula><p>(1)</p><p>Here, vector v is the mean face shape, computed over aligned facial 3D scans in the Basel Faces collection and represented by the concatenated 3D coordinates of their 3D points. When using a generic face shape, we use this average face. Matrices W S (shape) and W E (expression) are principle components obtained from the 3D face scans. Finally, α is a subject-specific 99D parameter vector estimated separately for each image and γ is a 29D parameter vector for expressions. To fit 3D shapes and expressions to an input image, we estimate these parameters along with camera matrices.</p><p>To estimate per-subject 3D face shapes, we regress α using the deep network of <ref type="bibr" target="#b36">[37]</ref>. They jointly estimate 198D parameters for face shape and texture. Dropping the texture components, we obtain α and back-project the regressed face by v + W S α, to get the estimated shape in 3D space. Pose and expression fitting. Given a 3D face shape (generic or regressed) we recover its pose and adjust its expression to match the face in the input image. We use the detected facial landmarks, p = {p i } ⊂ R 2 , for both purposes. Specifically, we begin by solving for the pose, ignoring expression. We approximate the positions in 3D of the detected 2D facial landmarks Ṽ = { Ṽi } by:</p><formula xml:id="formula_1">Ṽ ≈ f ( v) + f (W S ) α, (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where f (•) is a function selecting the landmark vertices on the 3D model. The vertices of all BFM faces are registered so that the same vertex index corresponds to the same facial feature in all faces. Hence, f need only be manually specified once, at preprocessing. From f we get 2D-3D correspondences, p i ↔ Ṽi , between detected facial features and their corresponding points on the 3D shape. Similarly to <ref type="bibr" target="#b10">[11]</ref>, we use these correspondences to estimate 3D pose, computing 3D face rotation, R ∈ R 3 , and translation vector t ∈ R 3 using the EPnP solver <ref type="bibr" target="#b22">[23]</ref>.</p><p>Following pose estimation, we estimate the expression parameters in vector γ by formulating expression estimation as a bounded linear problem:</p><formula xml:id="formula_3">δ R P (R, t) f ( v) + f (W S ) α + f (W E ) γ = δ R (p), with γ j ≤ 3 σ j ∀ j = {1 . . . 29}<label>(3)</label></formula><p>where δ R (•) is a visibility check that removes occluded points given the head rotation R; P (R, t) is the projection matrix, given the extrinsic parameters (R,t); and σ j is the standard deviation of the j-th expression component in γ. This problem can be solved using any constrained linear least-squares solver.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep face segmentation</head><p>Our method uses a FCN to segment the visible parts of faces from their context and occlusions. Other methods previously tailored novel network architectures for this task (e.g., <ref type="bibr" target="#b33">[34]</ref>). We show that excellent segmentation results can be obtained with a standard FCN, provided that it is trained on plenty of rich and varied examples.</p><p>Obtaining enough diverse images with ground truth segmentation labels can be hard: Saito et al. <ref type="bibr" target="#b33">[34]</ref>, for example, used manually segmented LFW faces and a semi-automatic segmentation method <ref type="bibr" target="#b6">[7]</ref> for this purpose. These labels were costly to produce and limited in their variability and number. We instead propose a novel means of generating numerous training examples with little manual effort and show that a standard FCN trained on these examples outperforms state of the art face segmentation results. Semi-supervised training data collection. We produce large quantities of segmentation labeled face images by using motion cues in unconstrained face videos. To this end, we process videos from the recent IARPA Janus CS2 dataset <ref type="bibr" target="#b17">[18]</ref>. These videos portray faces of different poses, ethnicities and ages, viewed under widely varying conditions. We used 1,275 videos of subjects not included in LFW, of the 2,042 CS2 videos (309 subjects out of 500).</p><p>Given a video, we produce a rough, initial segmentation using a method based on <ref type="bibr" target="#b9">[10]</ref>. Specifically, we keep a hierarchy of regions with stable region boundaries computed with dense optical flow. Though these regions may be overor under-segmented, they are computed with temporal coherence and so these segments are consistent across frames.</p><p>We use the method of <ref type="bibr" target="#b15">[16]</ref> to detect faces and facial landmarks in each of the frames. Facial landmarks were then used to extract the face contour and extend it to include the forehead. All the segmented regions generated above, that did not overlap with a face contour are then discarded. All intersecting segmented regions are further processed using a simple interface which allows browsing the entire video, selecting the partial segments of <ref type="bibr" target="#b9">[10]</ref> and adding or removing them from the face segmentation using simple mouse clicks. Fig. <ref type="figure" target="#fig_2">4(a)</ref> shows the interface used in the semisupervised labeling. A selected frame is typically processed in about five seconds. In total, we used this method to produce 9,818 segmented faces, choosing anywhere between one to five frames per video in a little over a day of work. Occlusion augmentation. This collection is further enriched by adding synthetic occlusions. To this end, we explicitly use 3D information estimated for our example faces. Specifically, we estimate 3D face shape for our segmented faces, using the method described in Sec. III-A. We then use computer graphic (CG) 3D models of various objects (e.g., sunglasses) to modify the faces. We project these CG models onto the image and record their image locations as synthetic occlusions. Each CG object added 9,500 face examples. The detector used in our system <ref type="bibr" target="#b15">[16]</ref> failed to accurately localize facial features on the remaining 318 faces, and so this augmentation was not applied to them.</p><p>Finally, an additional source of synthetic occlusions was supplied following <ref type="bibr" target="#b33">[34]</ref> by overlaying hand images at various positions on our example images. Hand images were taken from the egohands dataset of <ref type="bibr" target="#b2">[3]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Face swapping and blending</head><p>Face swapping from a source I S to target I T proceeds as follows. The 3D shape associated with the source, V S , is projected down onto I S using its estimated pose, P (R S , t S ) (Sec. III-A). We then sample the source image using bilinear interpolation, to assign 3D vertices projected onto the segmented face (Sec. III-B) with intensities sampled from the image at their projected coordinates.</p><p>The shapes for both source and target, V S and V T correspond in the indices of their vertices. We can therefore  Table <ref type="table">I</ref>: COFW <ref type="bibr" target="#b5">[6]</ref> segmentation results.</p><p>directly transfer these sampled intensities from all vertices v i ∈ V S to v i ∈ V T . This provides texture for the vertices corresponding to visible regions in I S on the target 3D shape. We now render V T onto I T , using the estimated target pose (R T , t T ), masking the rendered intensities using the target face segmentation (see Fig. <ref type="figure" target="#fig_0">3(d)</ref>). Finally, the rendered, source face is blended-in with the target context using an off the shelf method <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We performed comprehensive experiments in order to test our method, both qualitatively and quantitatively. Runtimes were all measured on an Intel Core i7 4820K computer with 32GB DDR4 RAM and an NVIDIA GeForce Titan X. Using the GPU, our system swaps faces at 1.3 fps. On the CPU, this is slightly slower, performing at 0.8 fps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Face segmentation results</head><p>Qualitative face segmentation results are provided in Fig. <ref type="figure" target="#fig_0">3</ref> and<ref type="figure" target="#fig_3">5</ref>, visualized following <ref type="bibr" target="#b33">[34]</ref> to show segmented regions (red) overlaying the aligned 3D face shapes, projected onto the faces (gray).</p><p>We provide also quantitative tests, comparing the accuracy of our segmentations to existing methods. We follow the evaluation procedure described by <ref type="bibr" target="#b8">[9]</ref>, testing the 507 face photos in the COFW dataset <ref type="bibr" target="#b5">[6]</ref>. Previous methods included the regional predictive power (RPP) estimation <ref type="bibr" target="#b38">[39]</ref>, Structured Forest <ref type="bibr" target="#b14">[15]</ref>, segmentation-aware part model (SAPM) <ref type="bibr" target="#b8">[9]</ref>, the deep method of <ref type="bibr" target="#b25">[26]</ref>, and <ref type="bibr" target="#b33">[34]</ref>. WE provide results also for our method, trained without out occlusion augmentation (Sec. III-B). Note that Structured  Forest <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b33">[34]</ref> used respectively 300 and 437 images for testing, without reporting which images were used. Result for <ref type="bibr" target="#b25">[26]</ref> was computed by us, using their code, out of the box, but optimizing for the segmentation threshold which provided the best accuracy.</p><p>Accuracy is measured using the standard intersection over union (IOU) metric, comparing predicted segmentations with manually annotated ground truth masks from <ref type="bibr" target="#b14">[15]</ref>, as well as two metrics from <ref type="bibr" target="#b14">[15]</ref>: global -overall percent of correctly labeled pixels -and ave(face), the average face pixel recall. Tab. I reports these results along with run times. Our method is the fastest yet achieves comparable result with the state of the art. Note that we use the same GPU model as <ref type="bibr" target="#b33">[34]</ref> and measure run time for <ref type="bibr" target="#b25">[26]</ref> ourselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Qualitative face-swapping results</head><p>We provide face swapping examples produced on unconstrained LFW images <ref type="bibr" target="#b13">[14]</ref> using randomly selected targets in Fig. <ref type="figure" target="#fig_6">1, 3,</ref> and<ref type="figure" target="#fig_4">6</ref>. We chose these examples to demonstrate a variety of challenging settings. In particular, these results used source and target faces of widely different poses, occlusions and facial expressions. To our knowledge, previous work never showed results for such challenging settings.</p><p>In addition, Fig. <ref type="figure">7</ref> shows a qualitative comparison with the very recent method <ref type="bibr" target="#b16">[17]</ref> using the same source-target pairs. We note that <ref type="bibr" target="#b16">[17]</ref> used the segmentation of <ref type="bibr" target="#b25">[26]</ref> which we show in Sec. IV-A to perform worst than our own. This is qualitatively evident in Fig. <ref type="figure">7</ref> by the face hairlines. Fig. <ref type="figure">7</ref> also provides results from the publicly available code of Kowalski <ref type="bibr" target="#b19">[20]</ref> and Hrastnik <ref type="bibr" target="#b12">[13]</ref>. In both cases, absence of a segmentation is clearly evident.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Quantitative tests</head><p>Similarly to previous work, we offer qualitative results to visualize the quality of our swapped faces (Sec. IV-B). Unlike others, however, we also offer extensive quantitative tests designed to measure the effect of swapping on the perceived identity of swapped faces. To this end we propose two test protocols, motivated by the following assumptions.</p><p>Assumption 1. Swapping faces between images of different subjects (i.e., inter-subject swapping) changes facial context (e.g., hair, skin tone, head shape). Effective swapping must therefore modify source faces, sometimes substantially, to blend them naturally into their new contexts thereby producing faces that look less like the source subjects.</p><p>Assumption 2. If a face is swapped from source to target photos of the same person (intra-subject swapping), the Figure <ref type="figure">7</ref>: Comparison with previous face swap methods <ref type="bibr" target="#b0">(1)</ref> Result published by Kemelmacher-Shlizerman <ref type="bibr" target="#b16">[17]</ref>. <ref type="bibr" target="#b1">(2)</ref><ref type="bibr" target="#b2">(3)</ref><ref type="bibr" target="#b3">(4)</ref> Results obtained by the public implementations of Kowalski <ref type="bibr" target="#b19">[20]</ref> and Hrastnik <ref type="bibr" target="#b12">[13]</ref>.</p><p>output of an effective swapping method should easily be recognizable as the person in the source photo as the two photos share the same context.</p><p>The first assumption is based on well-known trait of human visual perception: Face recognition requires both internal and external cues (faces and their context) to recognize faces. This idea was claimed by the seminal work of <ref type="bibr" target="#b34">[35]</ref> and extensively studied in the context of biological visual systems (e.g., <ref type="bibr" target="#b1">[2]</ref>). To our knowledge, it was never explored for machine recognition systems and never quantitatively. The robustness of our method allows us to do just that.</p><p>The second assumption is intended to verify that when the context remains the same (the same subject) swapping does not change facial appearances in a way which makes faces less recognizable. This ensures that the swapping method does not introduce artifacts or changes facial appearances.</p><p>To test these assumptions, we produce modified (face swapped) versions of the LFW benchmark <ref type="bibr" target="#b13">[14]</ref>. We estimate how recognizable faces appear after swapping by using a publicly available, state of the art face recognition system in lieu of a large scale human study. Though the recognition abilities of humans and machines may be different, modern systems already claim human or even super-human accuracy <ref type="bibr" target="#b26">[27]</ref>. We therefore see the use of a state of the art machine system as an adequate surrogate to human studies which often involve problems of their own <ref type="bibr" target="#b21">[22]</ref>.</p><p>Face verification. We use the ResFace101 <ref type="bibr" target="#b28">[29]</ref> face recognition system to test if faces remain recognizable after swapping. ResFace101 obtained near perfect verification results on LFW, yet it was not optimized for that benchmark and tested also on IJB-A <ref type="bibr" target="#b17">[18]</ref>. Moreover, it was trained on synthetic face views not unlike the ones produced by face swapping. For these reasons, we expect ResFace101 to be well suited for our purposes. Recognition is measured by 100%-EER (Equal Error Rate), accuracy (Acc.), and normalized Area Under the Curve (nAUC). Finally, we provide ROC curves for all our tests.</p><p>Inter-subject swapping verification protocols. We begin by measuring the effect of inter-subject face swapping on face verification accuracy. To this end, we process all faces in the LFW benchmark, swapping them onto photos of other, randomly selected subjects. We make no effort to verify the quality of the swapped results and if swapping failed, we treat the result as any other image.</p><p>We use the original LFW test protocol with its same/notsame subject pairs. Our images, however, present the original faces with possibly very different contexts. Specifically, let (I 1 i , I 2 i ) be the i-th LFW test image pair. We produce I 1 i , the swapped version of I 1 i , by randomly picking another LFW subject and image from that subject as a target, taking I 1 i as the source. We then do the same for I 2 i to obtain I 2 i . Matching pairs of swapped images, however, can obscure changes to both images which make the source faces equally unrecognizable: Such tests only reflect the similarity of swapped images to each other, not to their sources. We therefore test verification on benchmark pairs comparing original vs. swapped images. This is done twice, once on pairs ( I 1 i , I 2 i ), the other on pairs (I 1 i , I 2 i ). We then report the average results for both trials. We refer to these tests as face preserving tests.</p><p>We also performed context preserving tests: These use benchmark image pairs as targets rather than sources. Thus, they preserve the context of the original LFW images, not the faces. By doing so, we can measure the effect of context on recognition. This test setup is reminiscent of the inverse mask tests performed by <ref type="bibr" target="#b20">[21]</ref>. Their tests were designed to measure how well humans recognize LFW faces if the face was cropped out without being replaced, and showed this led to a drop in recognition. Unlike them, our images contain faces of other subjects swapped in place of the original faces, and so are more realistic.</p><p>Inter-subject swapping results. We provide verification results for both face preserving and context preserving intersubject face swapping in Tab. II and ROC curves for the various tests in Fig. <ref type="figure" target="#fig_5">8</ref>. Our results include ablation studies, showing accuracy with a generic face and no segmentation (Generic), with an estimated 3D face shape (Sec. III-A) and no segmentation (Est. 3D), with a generic face and segmentation (Seg.) and with an estimated 3D shape and face segmentation (Est. 3D+Seg.).</p><p>The face preserving results in Tab. II (bottom) are con- sistent with our Assumption 1: The more the source face is modified, by estimating 3D shape and better segmenting the face, the less it is recognizable as the original subject and the lower the verification results. Using a simple generic shape and no segmentation provides ∼8% better accuracy than using our the entire pipeline. Importantly, just by estimating 3D face shapes, accuracy drops by ∼3.5% compared to using a simple generic face shape. Unsurprisingly, the context preserving results in Tab. II (top) are substantially lower than the face preserving tests. Unlike the face preserving tests, however, the harder we work to blend the randomly selected source faces into their contexts, the better recognition becomes. By better blending the sources into the context, more of the context is retained and the easier it is to verify the two images based on their context without the face itself misleading the match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intra-subject swapping verification protocols and results.</head><p>To test our second assumption, we again process the LFW benchmark, this time swapping faces between different images of the same subjects (intra-subject face swapping). Of course, all same labeled test pairs, by definition, belong to subjects that have at least two images, and so this did not affect these pairs. Not-same pairs, however, sometimes include images from subjects which have only a single image. To address this, we replaced them with others for which more than one photo exists.</p><p>We again run our entire evaluation twice: once, swapping the first image in each test pairs keeping the second unchanged, and vice versa. Our results average these two trials. Results obtained using different components of our system are provided in Tab. III and Fig. <ref type="figure" target="#fig_5">8</ref>. These show that even under extremely different viewing conditions, In general, accuracy drops by ∼1%, with a similar nAUC compared to the use of original LFW images. This slight drop suggests that our swapping between different images of the same subject does not alter apparent facial identities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>We describe a simple face swapping method which is robust enough to allow for large scale, quantitative tests. From these tests, several key observations emerge. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Method overview. (a) Source (top) and target (bottom) input images. (b) Detected facial landmarks used to establish 3D pose and facial expression for a 3D face shape (Sec. III-A). (c) Our segmentation of Sec. III-B (red) overlaid on the projected 3D face (gray). (d) Source transfered onto target without blending, and the final results (e) after blending (Sec. III-C).</figDesc><graphic coords="3,83.64,68.82,426.27,147.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig 4(b) shows a synthetic hand augmentation and Fig 4(c) a sunglasses augmentation, along with their resulting segmentation labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a) Interface used for semi-supervised labeling. (b-c) Augmented examples and segmentation labels for occlusions due to (b) hands and (c) synthetic sunglasses.</figDesc><graphic coords="4,313.54,71.89,219.60,114.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Qualitative segmentation results from the COFW (1-4) and LFW (5-8) data sets.</figDesc><graphic coords="5,72.31,207.57,454.88,166.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Qualitative LFW inter-subject face swapping results. Examples were selected to represent extremely different poses (4), genders (1,2), expressions (1), ethnicities (1,3), ages (3,4) and occlusions (1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Inter-subject swapping ROC curves. Ablation study for the two experiments. Baseline shown in red. perceived subject identity remains unchanged, supporting our Assumption 2.In general, accuracy drops by ∼1%, with a similar nAUC compared to the use of original LFW images. This slight drop suggests that our swapping between different images of the same subject does not alter apparent facial identities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( 1 )</head><label>1</label><figDesc>State of the art face segmentation can be obtained with a standard segmentation network, provided that the network is trained on rich and diverse examples. (2) Collecting such examples is easy. (3) Both face and context play important roles in recognition. We offer quantitative support for the two decades old claim of Sinha and Poggio [35]. (4) Better swapping leads to more facial changes and a drop in recognition. Finally, (5), 3D face shape estimation better blends the two faces together and so produces less recognizable faces. ACKNOWLEDGMENTS This research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA 2014-14071600011. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purpose notwithstanding any copyright annotation thereon. TH was also partly funded by the Israeli Ministry of Science, Technology and Space</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table II :</head><label>II</label><figDesc>Inter-subject face swapping. Ablation study.</figDesc><table><row><cell>Method</cell><cell>100%-EER</cell><cell>Acc.</cell><cell>nAUC</cell></row><row><cell cols="3">Baseline (ResFace101) 98.10±0.90 98.12±0.80</cell><cell>99.71±0.24</cell></row><row><cell cols="3">Context preserving (face swapped out)</cell><cell></cell></row><row><cell>Generic</cell><cell cols="2">64.58±2.10 64.56±2.22</cell><cell>69.94±2.24</cell></row><row><cell>Est. 3D</cell><cell cols="2">69.00±1.43 68.93±1.19</cell><cell>75.58±2.20</cell></row><row><cell>Seg.</cell><cell cols="3">68.93±1.98 69.00±1.93 76.06± 2.15</cell></row><row><cell>Est. 3D+Seg.</cell><cell cols="2">73.17±1.59 72.94±1.39</cell><cell>80.77±2.22</cell></row><row><cell cols="3">Face preserving (face swapped in)</cell><cell></cell></row><row><cell>Generic</cell><cell cols="2">92.28±1.37 92.25±1.45</cell><cell>97.55±0.71</cell></row><row><cell>Est. 3D</cell><cell cols="2">88.77±1.50 88.53±1.25</cell><cell>95.53±0.99</cell></row><row><cell>Seg.</cell><cell cols="2">89.92±1.48 89.98±1.36</cell><cell>96.17±0.93</cell></row><row><cell>Est. 3D+Seg.</cell><cell cols="2">86.48±1.74 86.38±1.50</cell><cell>93.71±1.42</cell></row><row><cell>Method</cell><cell>100%-EER</cell><cell>Acc.</cell><cell>nAUC</cell></row><row><cell>Baseline (VGGFace)</cell><cell cols="2">97.23±0.88 97.35±0.77</cell><cell>99.54±0.30</cell></row><row><cell cols="2">Baseline (ResFace101) 98.10±0.90</cell><cell>98.12±0.80</cell><cell>99.71±0.24</cell></row><row><cell>Generic</cell><cell>97.02±0.98</cell><cell>97.02±0.97</cell><cell>99.53±0.31</cell></row><row><cell>Est. 3D</cell><cell>97.05±0.98</cell><cell>97.03±1.01</cell><cell>99.52±0.32</cell></row><row><cell>Seg.</cell><cell>97.12±1.09</cell><cell>97.08±1.07</cell><cell>99.53±0.31</cell></row><row><cell>Est. 3D+Seg.</cell><cell>97.12±1.09</cell><cell>97.12±0.99</cell><cell>99.52±0.31</cell></row><row><cell>Est. 3D+Seg.</cell><cell>96.65±0.85</cell><cell>96.63±0.92</cell><cell>99.45±0.29</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table III :</head><label>III</label><figDesc>Intra-subject face swapping. Ablation study.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Creating a photoreal digital actor: The digital emily project</title>
		<author>
			<persName><forename type="first">O</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lambeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Debevec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Visual Media Production</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="176" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">External facial features modify the representation of internal facial features in the fusiform face area</title>
		<author>
			<persName><forename type="first">V</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yovel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="720" to="725" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lending a hand: Detecting hands and recognizing activities in complex egocentric interactions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bambach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2015-12">December 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Face swapping: automatically replacing faces in photographs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bitouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Exchanging faces in images</title>
		<author>
			<persName><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scherbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Wiley Online Library</publisher>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="669" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">P</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1513" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Facewarehouse: A 3D facial expression database for visual computing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="413" to="425" />
			<date type="published" when="2014-03">March 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Poseshop: Human image database construction and personalized content synthesis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="824" to="837" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using segmentation to predict the absence of occluded parts</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Irvine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Mach. Vision Conf</title>
		<meeting>British Mach. Vision Conf</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="22" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph-based video segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Viewing real-world faces in 3D</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<ptr target="www.openu.ac.il/home/hassner/projects/poses" />
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3607" to="3614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Effective face frontalization in unconstrained images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Enbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Hrastnik</surname></persName>
		</author>
		<ptr target="https://github.com/hrastnik/FaceSwap" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007-10">October 2007</date>
			<pubPlace>UMass, Amherst</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Structured semisupervised forest for facial landmarks localization with face mask reasoning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Mach. Vision Conf</title>
		<meeting>British Mach. Vision Conf</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">94</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Transfiguring portraits</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pushing the frontiers of unconstrained face detection and recognition: IARPA Janus Benchmark-A</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Fast faceswap using convolutional neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Korshunova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dambre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09577</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Kowalski</surname></persName>
		</author>
		<ptr target="https://github.com/MarekKowalski/FaceSwap" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attribute and simile classifiers for face verification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2009 IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="365" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Labeled faces in the wild: A survey</title>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Face Detection and Facial Image Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="189" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Epnp: An accurate o (n) solution to the pnp problem</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">155</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Emotion recognition in the wild via convolutional neural networks and mapped binary patterns</title>
		<author>
			<persName><forename type="first">G</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Multimodal Interaction</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Face replacement with large-pose differences</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia Conf</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-objective convolutional learning for face labeling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3451" to="3459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Surpassing human-level face verification performance on lfw with gaussianface</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.3840</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical face parsing via deep learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2480" to="2487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Do We Really Need to Collect Millions of Faces for Effective Face Recognition?</title>
		<author>
			<persName><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Leksut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<ptr target="www.openu.ac.il/home/hassner/projects/augmentedfaces" />
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision, 2016. Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Photorealistic face de-identification by aggregating donors face components</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mosaddegh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conf. Comput. Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="159" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Manifold learning and spectral clustering for image phylogeny forests</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Oikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Rezende Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goldenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. on Inform. Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="18" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A 3D face model for pose and illumination invariant face recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Paysan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Romhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Advanced Video and Signal based Surveillance</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Poisson image editing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Prez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gangnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH Conf. Comput. Graphics</title>
		<meeting>ACM SIGGRAPH Conf. Comput. Graphics</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Real-time facial segmentation and performance capture from rgb input</title>
		<author>
			<persName><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="244" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">I think I know that face</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">384</biblScope>
			<biblScope unit="issue">404</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Exemplar-based face parsing</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3484" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Regressing robust and discriminative 3D morphable models with a very deep neural network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An eye for an eye: A single camera gaze-replacement method</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Robust face alignment under occlusion via regional predictive power estimation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2393" to="2403" />
			<date type="published" when="2015-08">Aug 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised face alignment by robust nonrigid mapping</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1265" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3D solution</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
