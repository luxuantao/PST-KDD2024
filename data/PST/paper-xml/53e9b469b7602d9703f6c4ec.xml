<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Vector Quantization with Complexity Costs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Joachim</forename><surname>Buhmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern Caliiomia</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country>H. Kiihnel was</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institut fiir Informatik 11</orgName>
								<orgName type="institution">Rheinische Friedrich-Wilhelms-Universit2t</orgName>
								<address>
									<addrLine>RomerstraBe 164</addrLine>
									<postCode>D-53117</postCode>
									<settlement>Bonn</settlement>
									<country>FRG</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">H</forename><surname>Kiihnel</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute for Medical Optics</orgName>
								<orgName type="institution">Ludwig Maximilian Universitiit Miinchen</orgName>
								<address>
									<addrLine>Theresian stra6e 37</addrLine>
									<postCode>D-80333</postCode>
									<settlement>Miinchen</settlement>
									<country>FRG</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Vector Quantization with Complexity Costs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">20B4ABE5BE68CF30DE794A1640ABCAD6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Absftact-Vector quantization is a data compression method where a set of data points is encoded by a reduced set of reference vectors, the codebook. A vector quantization strategy is discussed that jointly optimizes distortion errors and the codebook complexity, thereby deterqining the size of the codebook. A maximum entropy estimation of the cost function yields an optimpl number of reference vectors, their positions and their assignment probabilities. The dependence of the codebook density on the data density for differeqt complexity functions is investigrrted in the limit of asymptotic quantization levels. How different complexity measures influence the efficiency of vector quantizers is studied for the task of image compression, i.e., we quantize the wavelet coefficients of cgray-level images and measure the reconstruction error. Our approach establishes a unifying hmework for different quantization methods like Kmeans clustering and its fuzzy version, entropy constrained vector quantization or topological feature maps and competitive neural networks.</p><p>Index Term-Vector quantization, complexity costs, maximum entropy estimation, image compression, neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCITON</head><p>NFORMATION reduction represents an essential step in I many areas of information processing as for example in image and speech processing and pattem recognition in general. It is particularly important in pattem classification problems to uncover the structure of a given data set, to generalize over inessential details or to remove noise. However, in the absence of a parametric model for a data set, data compression techniques are required that preserve the original data as completely as possible. Vector quantization (VQ), a widely used method of lossy information compression, encodes a given set of &amp;dimensional data vectors { z i I z i E Bd; i = l , . . . , N } with a much smaller set of codebook vectors Y = {y, I y, E W; Q = I , -. -, K } ( K &lt;&lt; N ) . What strategy should be followed to design and optimize a vector quantization codebook? In this paper, we will primarily discuss design strategies to generate application adequate codebooks.'</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and Hans Kuhnel</head><p>The design of an optimal vector quantizer requires 1) finding a set of reference vectors that represent the data points with minimal residual distortion and 2) specifying the complexity of the codebook. Both design specifications are mutually dependent and cannot be optimized separately if we aim for an optimally efficient vector quantizer. This paper discusses the strategy to jointly optimize a distortwn measure and a codebook complexity function for the design of a vector quantizer. We focus on a Lagrangian variation principle to find optimal codebooks for a large class of distortion and complexity measures. The data assignments to reference vectors are estimated in the maximum entropy sense, a strategy first proposed by <ref type="bibr">Rose et al. [38]</ref>, <ref type="bibr">[39]</ref> for K-means clustering. The special case of jointly minimizing suitable distortion costs with constrained entropy of the codebook vectors has been discussed by Chou et al. <ref type="bibr">[9]</ref>.</p><p>The traditional VQ design strategy, as followed in the well-known K-means clustering algorithm <ref type="bibr">[35]</ref> or the LBG algorithm by Linde et al. <ref type="bibr">[32]</ref>, exclusively relies on the distortion errors for placing the reference vectors. The size of the codebook is fixed (K-means) or it is incrementally increased until the distortion error drops below a predefined threshold (LBG). Following the K-means clustering concept lossy data compression with K reference vectors can be mathematically formulated as an optimization of binary assignment variables Mi, E (0, 1) with the objective to minimize the sum of all distortion errors introduced by vector quantizing the data. The variables Mi, specify to which reference vector a data point i is assigned to, i.e., Mia. = 1 denotes that data point i is uniquely assigned to reference vector la.. The uniqueness of assignments implies the constraint E, Mi, = 1. The Kmeans cost function E K m for vector quantization is given by</p><formula xml:id="formula_0">N K i=la=l</formula><p>The distortion measure Di,(zi, y,) quantifies the quantization error for data point z i . A widely used distortion measure is the squared Euclidean distance D;,(zi, y,) llzigallr, r = 2. Various other distortion measures have been proposed in the literature <ref type="bibr">[24]</ref>, but generalizations of the squared Euclidean distortions (r # 2) are most common for many signal processing applications, e.g., T = 1 supposedly is in accordance with the sensitivity profile of the human visual system [15] and is claimed to be a natural choice for image processing applications. The codebook vectors J , are determined by the centroid condition 0018-9448/93$03.00 0 1993 IEEE which is optimal according to rate distortion theory (see <ref type="bibr">[ll]</ref>). The K equations (2) require that the distortion measure 'Dz, (zz, 3 , ) is differentiable in y,; for nondifferentiable distortions Di, (zi, a,) we replace (2) by the constraints</p><p>[32]). Variation of Mz, implicitly determines the reference vectors y, if we enforce (2) strictly. Provided, the optimum set of reference vectors has been found then each data point is represented by its closest reference vector. It should be mentioned that algorithms that minimize the previous cost function under the constraint of unique assignment are also known as "hard clustering" algorithms. In contrast, procedures that return continuous assignments Mi, E [0, 1 1 of a data point to several "cluster centers," are called "soft" or "fuzzy clustering" algorithms [4], <ref type="bibr">[17]</ref>.2</p><p>How large should the "magic" number K of reference vectors be? It is clear that the result of the optimization of cost function (1) depends on the number of codebook vectors K or, more generally speaking, on the complexity of the codebook. The trivial solution to declare all data points to be reference vectors, K = N and yi = zi, i = 1, . , N, minimizes (l), but does not achieve any data reduction. Algorithms like K-means clustering assume that the size of the codebook is determined a priori, i.e., that the number K of reference vectors with K &lt;&lt; N is prespecified. Other procedures like ISODHA [3] stop adding new reference vectors as soon as the residual distortion error falls below a fixed threshold. These approaches can result in suboptimal vector quantizers for particular signal processing tasks since optimization of distortion and intrinsic constraint limits on the codebook are related and do affect each other in general. For example, entropic costs for codebook design have been shown to yield superior quantization performance in speech compression [9], <ref type="bibr">[26]</ref>.</p><p>We, therefore, suggest extending the cost function (1) by an application dependent complexity measure which has to reflect the inherent costs of too complex vector quantizers. The complexity term limits the number of reference vectors.</p><p>The admissible class of distortion measures is generalized to include topology preserving vector quantization schemes, also known as source-channel-coding, which reduce the detrimental effect of channel noise on the quantized data set Section VI what gain in compression efficiency results from an entropic complexity measure for special data distributions with outliers. As a "real-world" application of the new vector quantization algorithm we discuss the entropy optimized compression of wavelet decomposed images in Section VI1 (see also <ref type="bibr">[7]</ref>). This quantization scheme preserves psychophysically important image features like edges more accurately than the K-means clustering algorithm. In Section VIII, we derive an online algorithm that asymptotically approaches the maximum entropy estimation of the centroids y, and the assignment probabilities p , .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">COMPLEXITY LIMITED VECTOR QUANTIZATION COSTS</head><p>Vector quantization is a compromise between precision and simplicity of a data representation. As outlined above, we propose to explicitly reflect this tradeoff in the cost function by adding a complexity term to the usual distortion measure and jointly optimizing both cost terms. This strategy to control the complexity of a codebook is more natural for many applications than to a priori set the number of reference vectors to K, in particular when a "too complex" vector quantizer still constitutes a valid, but inefficient solution as in data transmission and storage. The balance between distortion costs and complexity costs yields an optimal number of reference vectors. The number of clusters naturally depends on the data distribution and on a parameter X that weights the complexity kosts versus the distortion costs. The new vector quantization cost function</p><formula xml:id="formula_1">N K E K ( { &amp; z } ) = c c M i , ( ' D i , ( z i , yo) -k Xc(Pa)) (3)</formula><p>i=la=l only depends on the assignment variables {Mi,}. The reference vector y, are defined by the K equations</p><formula xml:id="formula_2">N K i = l v = l</formula><p>which are linear in the assignment variables Mi,. B,,,(zi, Y ) are a set of functions that depend on the codebook Y and the data point zi. Variation of {Mi,} implicitly determines the reference vector y, and the assignment probability . N</p><p>In the following, we carry out the calculations for general a E (1, -. , K}. Configurations with degenerate clusters, i.e., y, = yp for a # /? are inadmissible, since they give rise to an overestimation of the "true" complexity of the codebook. We, therefore, have to assure that each cluster index represents a separate cluster. Cluster degeneration does not occur for hard clustering but is a problem for fuzzy clustering [39].</p><p>An appropriate complexity measure C depends on the par-. ticular information processing application at hand, i.e., it is a function of the variables y, , p,. For reasons of simplicity, we limit our analysis to complexity measures depending only on the assignment probabilities p , and not on the refeqence vectors y,. A more general derivation is feasible, if necessary. In data compression, we want to encode the data set with minimal distortion error and compress the resulting index set optimally without further losses. In this context, a natural complexity measure for a particular cluster a is C(p,) = -log(p,), which results in an average complexity (C) = -E, p , log pa of the codebook. Note, that this is identical with the entropy of an information source emitting letters of a K-ary alphabet with probabilities p,, a = 1, . . . ,K. (C) sets a lower bound for the minimal possible average cbdeword length if the messages a are subject to data compaction algorithms like arithmetic coding or Huffman coding [ll]. Lagrangian optimization of a vector quantization cost function with entropic complexity costs is extensively discussed in [9]. We will apply entropy optimized quantization to the problem of image compression in Section VII.</p><p>Another strategy to limit the number of clusters is defined by complexity costs which penalize small, rarely used reference vectors, i.e., C , = l/&amp;, s = 1, 2,... . Such complexity measures favor equal assignment probabilities since they strongly diverge for p , = 0. The hardware aspect of a vector quantization solution, e.g., scarcity of processing elements or expensive chip area in a VLSI *plementation is better taken into account by C , = l/&amp; with its preference of load balan'ced codebooks than by the entropic complexity measure C(p,) = -log(p,). The special case s = 1 with complexity costs strictly proportional to the number of clusters (C, = l/p, + C , C , M ; , / p , = NK) corresponds to the K-means clusteriqg cost function [32]. For s = 1, all clusters have the same cost term independent of the number of data points assigned to them. We will use this case as a standard to compare with the entropy optimized vector quantizer in Section VI. The lhpit s + 00 produces codebooks that are perfectly load-balanced, i.e., all clusters have the same number of data points assigned to (1ims-,-p, = 1/K, Va). The choice of an appropriate distortion measure Di,(zi, y,) is another degree of freedom in the design of a vector quantization algorithm. Usually a generalization of the Euclidean distance is employed to measure quantization errors, i.e., % Y) = 1 1 2 -y r .</p><p>(6) Our formalism, however, is applicable to even more general difference distortion measures. The Minkowski l,-norms . Such schemes proved to be very sensitive to channel noise in data transmission. The following block diagram describes the communication process where quantized data are sent through a noisy communication channel:</p><p>-1</p><p>At the encoding stage data point 2; is assigned to cluster a. The index a, however, gets corrupted by channel noise and arrives as iqdex y at the receiver's side of the communication channel. U t us denote the transition probability from index a to index y by T , , with T , , = 1. The receiver, consequently, reconstructs the "incorrect" reference vector y, as a representation of zi instead of the reference vector y, . Note, that the centroid condition generalizes to Ci E, Mi, T,,(d/dy,)Db(zi, y,) = 0. It has to be emphasized that the resulting cost function for topological vector quantization as well as the generalized centroid condition are still linear in the assignment variables {Mi,} and, therefore, the minimization Ls tractable like the original optimization problem (3). The transition probability T, , has to be determined on the basis of the channel noise characteristics and on the basis of the chosen code for the reference vector indices. The special case of a tridiagonal transition matrix T ,</p><formula xml:id="formula_3">, = 1 -v; T,, = v/2;</formula><p>T, , = 0 V I ay I&gt; 1 defines a linear chain with nearest neighbor transitions. Topology preserving vector quantization reduces to nontopological clustering if we set T , , = 1 for a = y and T, , = 0, otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">RE-ESTIWON EQUATIONS OF {y,} AND { p a }</head><p>Our main objective is to jointly minimize the distortion and complexity costs in (3) with respect to the variables Mi,.</p><p>We can interpret the minimization of the vector quantization cost function (3) as a search for a set of assignment variables offer another alternative to quantify distortion errors [5].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{Mi,} that yield the quantization costs E K ( { M ~, } )</head><p>= ( E K ) .</p><p>The inference principle which selects the most stable set of assignment variables with respect to random fluctuations in the assignment process is the maximum entropy principle supports this design philosophy for optimization algorithms.</p><p>Another substantial advantage of maximum entropy inference is the inherent parallelism of the method which facilitates the mapping of resulting algorithms to parallel hardware or VLSI implementations on chips.</p><p>In this section we will calculate the maximum entropy distribution of the assignment variables { M,,} for quantization costs Ey({Mi,}) = ( E K ) . Subsequent minimization of (EK) yields the global minimum of the vector quantization costs t ? ~( { M i , } ) . As a consequence of the maximum entropy principle the distributip function of the assignment variables is the Gibbs distribution</p><p>The parameter p, often referred to as the inverse of the "computational temperature" T, is a Lagrange multiplier, chosen such that the average quantization costs amount to ( E K ) . The factor e x p ( p 3 K ) normalizes the Gibbs distribution. 3 K is called the free energy in statistical physics and it is given by</p><formula xml:id="formula_4">I K -N</formula><p>Assuming that Vi, and GaU are differentiable functions we derive the order parameter equations with the abbreviation Ei, being defined in (11).</p><p>Equations ( <ref type="formula">12</ref>)-( <ref type="formula">15</ref>) can be simplified if we specialize the functions GmU( .) to be the generalized centroid conditions, i.e., GCru(zi, Y ) = T,,(a/ay,)Vi,.</p><p>Given such a definition of G,,(-) the first term in the brackets of (15) vanishes due to (13). The second term in the bracket of (15) can be rewritten as</p><p>If we assume that the distortion measure Vi, is a convex function of y, then the only solution of (17) is jj, = 0. In cases where the reference vectors y, are not the centroids of the respective clusters a, e.g., self-organizing feature maps, we find solutions with y, p 0. Equations ( <ref type="formula">14</ref>) and (15) can be inserted into (12) and (13) that reduces the order parameter equations to two systems of K transcendental equations</p><formula xml:id="formula_5">(18) 0 = -F , X ( M Y ) T Y a -D E a ( Z z , Y,),<label>(19)</label></formula><p>Equation The resulting free energy is</p><formula xml:id="formula_7">3K =</formula><p>The minimization in (10) is performed under the constraint y, # yp, V a # p, i.e., all configurations with degenerate reference vectors are rejected. The parameters yEin, PE'" that define the free energy 3 K are the expectation values of the random variables y,, p , defined in (4), (5). Since we study the limit of mapy data points (N + m) and since we assume that the free energy and variables derived from it are self-averaging we will use the same notation for the random variables y, , p ,</p><p>and their expectation values.</p><p>min</p><formula xml:id="formula_8">{%I&gt; { P a )</formula><p>(Mi,) can be interpreted as the probability that data point i is assigned to reference vector a. Consequently, p , measures the percentage of data points assigned to a, hence p, is the assignment probability. y, is the generalized centroid of cluster a. Equations ( <ref type="formula">18</ref>) and ( <ref type="formula" target="#formula_5">19</ref>) demonstrate explicitly, that the placement of reference vectors as well as their assignment probabilities depend on the particular complexity measure. It has to be emphasized that a solution of ( <ref type="formula">18</ref>) and ( <ref type="formula" target="#formula_5">19</ref>) ensures only an extrema1 value of the free energy 3 K , but does not guarantee that we have found the global minimum of (lo)</p><p>or, equivalently, the maximum entropy estimation of {Mi,}. Depending on the data distribution the minimization in (lo), ( <ref type="formula">21</ref>) might be a highly nonconvex optimization problem with many local minima.</p><p>The statistical mechanics approach for the K-means clustering cost function (1) that corresponds to the case X = 0 and fixed K has first been discussed in [38], <ref type="bibr">[39]</ref>. Note, that this case corresponds to the specific choice of C = l / p , where the conjugate potential $ , = -X/p, = -XC, exactly cancels the complexity term, adding essentially a constant cost term (chemical potential) XK to the free energy 3. Rose et al. advocated a deterministic annealing approach for K-means clustering in the spirit of <ref type="bibr">[29]</ref>. They did not include complexity costs to limit the number of codebook vectors but proposed to start with many codebook vectors at high temperature, to lower the temperature to a finite value and to use the resulting set of different vectors as the codebook. This procedure, however, is plagued by the problem of degenerated reference vectors, i.e., y, = y, for a # 7 are solutions at high temperature. The degeneration of reference vectors affects the data assignments and, thereby, the values of the reference vectors.</p><p>The order parameter equations ( <ref type="formula">18</ref>), ( <ref type="formula" target="#formula_5">19</ref>) form a system of K(d + 1) transcendental equations that have to be solved simultaneously for a particular vector quantization problem. The fuzziness3 of the assignment process in a maximum entropy sense is expressed by the gradual membership functions (Mi,). For very high temperature (p + 0) the data point i is assigned with equal probability 1/K to all clusters and the reference vectors y, are identical to the center of mass of the data distribution. p , in (18) can be interpreted in an intuitive way as the fuzzy cluster probability which adds up the fuzzy assignment variables (Mi,) of all data points zi.</p><p>y, in ( <ref type="formula" target="#formula_5">19</ref>) is the centroids of all points assigned to cluster a, weighted with the corresponding membership probability. In the limit of zero temperature we recover the hard clustering case. A datapoint z i is assigned to cluster a, if and only if</p><formula xml:id="formula_9">Eia + X(&amp;/~P,)P, &lt; Eip + X(&amp;/&amp;p)pp, VP # a* w. AN ALGORITHM FOR THE DESIGN OF COMPLEXITY OPnMIZED CODEBOOKS</formula><p>Finding the maximum entropy estimation of the cost function (3) is a AfP-hard problem <ref type="bibr">[20]</ref> plagued by the characteristic nonconvexity of the free energy. In the following, we propose an iterative cluster splitting algorithm which approximates the maximum entropy solution and which determines the optimal size K of the codebook. An iteration of the following two step procedure is required.</p><p>3Note, that fuzziness should be understood as a partial membership of data points in clusters. Our approach is still a probabilistic one and is not related to fuzzy set theory as proposed by Zadeh [MI.</p><p>1) We have to calculate the free energy 3 K for fixed K, which requires to search for the global minimum in (lo), 2) We have to vary K to determine the optimal number of A variety of different methods exist to perform the minimization required in step 1). The algorithm described below is based on the reestimation of y,, pa, i.e., to solve the transcendental equations ( <ref type="formula">18</ref>), ( <ref type="formula" target="#formula_5">19</ref>) and to evaluate the respective free energy. In case that Di, and are not differentiable, we have to minimize (lo), ( <ref type="formula">21</ref>) directly. The problem of local minima can be alleviated by simulated annealing, an optimization strategy which was introduced by Kirkpatrick [29] and which proved to be successful in a large variety of difficult nonconvex optimization problems like the traveling salesman problem. The reestimation equations ( <ref type="formula">18</ref>), ( <ref type="formula" target="#formula_5">19</ref>) explicitly show the functional dependence on the computational temperature that is slowly decreased in simulated annealing. It has to be mentioned that step 1) does not require a complete cooling schedule from high temperature to zero temperature since cluster splitting and cooling are coupled <ref type="bibr">[38]</ref>. We have abstained from temperature variations in the simulation examples presented below since a T = 0 strategy yielded sufficiently good estimates of the global minimum of 3 K . The second step 2) is necessary4 since we do not have a direct estimate of the optimal codebook size K.</p><p>The following dgorithm implements the two step procedure under the constraint that no two reference vectors are degenerate. The heuristics to split only a single cluster, could be replaced by a more sophisticated splitting strategy, e.g., larger portions of the codebook or even the whole codebook as in the LBG-algorithm could be split simultaneously. Thereby, the computational complexity would be drastically reduced, but we might have to merge clusters again to find the optimal codebook size. The cluster splitting can be interpreted as an optimization step of the cost function EK along the K coordinate. The algorithm employed in the simulations in this section and in the Section VI and VI1 comprises the following steps: Determine a random order of codebook vectors y, with list index IC 1. Split codebook vector IC, i.e., yK+l t y, +z, y, yn, p ~+ 1 t p,/2, p , t pn/2, (z is a "small" random vector with 11~11 &lt;&lt; ~~y n ~~) .</p><p>Reestimate p,, g, using ( <ref type="formula">18</ref>), ( <ref type="formula" target="#formula_5">19</ref>).</p><p>IF PKTl &lt; Fkd, THEN accept new codebook with K + 1 vectors, increment K t K + 1 and GOTO step 2). 4A more general approach with the optimal K being an order parameter has to be based on the grand canonical partition function (GCF'F) instead of the canonical partition function. The GCPF sums over all possible codebook sizes K = 1 , . . . , N . Unfortunately, the summation over K is analytically intractable. A variation of the codebook size K can be interpreted as a discrete approximation of the GCPF.</p><p>The resulting solution is a minimum of 3 K under single cluster splitting, although it is not guaranteed to be a global minimum since there might exist configurations with lower free energy than the one found by our algorithm. The iterative cluster splitting is necessary since we might find a local minimum 3 K with K reference vectors where the global minimum has a codebook size smaller than K . The random ordering of codebook vectors a* in step 1 can be replaced by an application dependent heuristics, e.g., to order the clusters according to sue. The proposed algorithm is a generalization of the vector quantization algorithm discussed by Chou et al. For very high complexity costs the algorithm finds only two cluster centers. The plus signs indicate the centers of the data sources. For X = 2.5 four reference vectors are positioned at the centers of the data sources. In the limit of very small complexity costs, the best optimization of the cost function found by our iterative cluster splitting algorithm densely covers the data distribution. The specific choice of the logarithmic complexity measure causes the homogeneous density of reference vectors that is known to yield the smallest entropy per codebook vector in the limit of asymptotic quantization levels [9], [23]. This demonstrates how different complexity measures can drastically modify the codebook structure. We analyse this observation in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>v. ASYMF'TOTIC QUANTIZATION LEVEL DENSITY</head><p>Different complexity measures influence the distributions of reference vectors, even if we choose the same data distribution and the same distortion measure. In this section, we study the asymptotic level density for codebooks with high complexity, i.e., dense quantization levels, and we derive the dependency of the cluster density Y (2) on the probability density rI(o) for the case of hard clustering. The analysis follows Zador's [49], <ref type="bibr">Gersho's [22]</ref>, and Yamada et d ' s <ref type="bibr">[46]</ref> line of reasoning, although we use a variational approach to determine the density of quantization levels as a function of the probability density instead of searching a bound for the distortion costs. The variational approach is required since we treat the case of general complexity costs which balance the monotonous decrease of distortion costs with increasing codebook vector density. In the special case of K-means clustering or entropic complexity costs the solutions reduce to the classic results of Zador [49] and of Gish and Pierce We denote the codebook size by K . Sa, a = 1 , . . , K are the partitions of %* that define the quantizer. Following we define the codebook density function as where V(S,) is the volume of partition S,. In the limit of dense quantization levels, we expect g K (z) to closely approx-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>imate a conthuous density function Y(z) M l / ( K V ( S , ) ) .</head><p>The total distortion costs can be written as</p><formula xml:id="formula_10">' K PD) = D(z7 Y,)wz) dz ,=I sa K (23) ,=l</formula><p>The approximation holds for a smoothly varying probability density II(z) M II(y,) for z E S,. Following Gersho The assignment probability af codebook vector cx in the limit of asympfotic quantization levels is a function of the probability density and the codebook vector density, i.e., p , is given by Inserting (26) into C,, we obtain for the total complexity</p><p>In the continuum limit of asymptotic quantization levels the summation E,"==, V(S,) is approximated by the integral J' dy and the total quantization costs for complexity optimized vector quantization are measured by the functional</p><formula xml:id="formula_11">E = (2)) + A(C&gt;</formula><p>The optimal distribution of codebook vectors is determined by the function Y(z) that -es the functional (28). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>W. PERFORMANCE COMPARISON BETWEEN K-MEANS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AND ENTROPY OPTIMIZED CLUSTERIN0</head><p>The quantization results shown in Fig. <ref type="figure">1</ref> and the analysis of the asymptotic quantization level density (30), (31) demonj strate that the Complexity measure drastically influences the placement of the codebook vectors. How does it influence the performance of vector quantizers? We will address this question twofold: 1) In this section, we quantize an artificial, d-dimensional data distribution using the K-means and the entropic complexity measure. 2) In Section W, we compress wavelet decomposed images and measure the reconstruction error after auantization with the K-means and the entrobic complexity costs. We restrict the discussion of both compression applications to the zero temperature limit since the influence of the complexity measure is the focus of this study and not so much the performance gain by temperature variations.</p><p>The independence of the codebook vector density from the data density for entropy encoding indicates that sparsely populated areas of data space are more accurately represented by an entropy encoding scheme than by other quantization strategies.</p><p>To verify this hypothesis we select an artificial, d-dimensional data distribution which consists of two regions with uniform densities nhigh and nlow respectively and vanishing density outside. The respective volumes are Vhigh and Kow. IhOw is assumed to be small. Such a limit is interesting for many pattern recognition applications, e.g., image prockssing where the psychophysically important edge information is sparse. The functional relationship that an increase in the frequency of data features decreases their information content favors quantization schemes which minimize the error in sparsely populated (outlier) regons.</p><p>Let us compare the compression efficiencies of K-means clustering (Km) and of entropy coding (ent) for the two cases of 1) identical overall distortions costs and 2) identical distortion costs in the outlier region with low data density For identical outlier distortion, we insert (39) in (42) and we derive the entropy difference</p><formula xml:id="formula_12">R K m -xent d --- - + log n o w</formula><p>A specific choice for the parameters of a data distribution 2-d/(d+T)IIrL:+'). This scaling behavior of the low-density region ensures that its distortion error does not vanish if we take the limit Illow + 0.</p><p>Calculations of the relative gain by entropy encoding x E ( H K ~ -Hent)/Hent in the case of identical overall distortion show improvements of up to 20% for Kent = 20. The gain, however, increases dramatically if we require identical distortion error in the outlier region. The difference in encoding costs diverges logarithmically in the limit IIlow + 0. The ratio of the codebook sizes (39) diverges as well in that limit. The divergence means that we need much larger codebooks for K-means clustering to describe sparse data than for entropy encoding.</p><p>The approximations introduced to derive (43) and (44) are asymptotically valid. They should, therefore, serve only as a crude estimate of the potential efficiency gain. To test the quality of our estimate of x in the case of identical distortion error in the outlier region, we have quantized one-dimensional step distributions with two different densities TIlOw = 0.1 and HlOw = 0.05 in the low-density part of the distribution and with the other parameters adjusted as previously stated. The simulation results and the theoretical estimates (in brackets) are shown in Table <ref type="table">I</ref>.</p><p>The simulation results are consistent with the x values derived from (43) and (44). Deviations from the theoretical value are caused by locally optimal arrangements of reference vectors which are not globally optimal. We conclude that it pays off to introduce a problem adequate complexity measure for vector quantizer design when too large distortions in outlier regions cannot be tolerated, an issue that will be of importance in the next section. x v e n t x 10-5 x is the gain in quantization efficiency applying the constraint of equal outlier distortions. The values in brackets are the theoretical estimate5 according to (43) and ( <ref type="formula">44</ref>). Table <ref type="table">I-</ref> These wavelet coefficients essentially are a combination of low-pass filtering in 2-direction and bandpass filtering in ydirection (Q') or vice versa (Q2) or bandpass filtering in both directions (q3). The coefficients of a two band wavelet decomposition can be conveniently arranged in matrix form as shown in Fig. <ref type="figure" target="#fig_5">2@</ref>).</p><formula xml:id="formula_13">v ( K m ) x Kent K W m ) X 0.</formula><p>To compare the quantization efficiency of entropy optimized VQ with K-means clustering we transform 8-bit gray-level images (size 128 x 128) of human faces using a two-band wavelet transformation. The resulting coefficients are combined to three dimensional vectors for the bandpassed signals and scalar values for the low-pass filtered image. In each frequency band, we combine the three wavelet coefficients</p><formula xml:id="formula_14">(D!&amp;Z)(i), v = 1, 2, 3 at position i (notation as in [36])</formula><p>to a three-dimensional vector z i (see Fig. <ref type="figure" target="#fig_5">2</ref>). The index p denotes the reduction factor 2 -P of the bandpass filters. On the highest frequency level (p = 1) this data set, referred to as c1 = {zi = ((~i-~z&gt;(i), (D,2-lZ)(i), (Di-lZ)(i))), In the first series of compression experiments, we set X = 5 for entropy optimized vector quantization of L1 and L2 and X = 0.5 for entropy optimized scalar quantization of d2. The resulting codebooks for L1, L2, A2 had sizes K = 112, 123, 146, respectively. We have not optimized the bit allocation per frequency level for our specific training set but used the same complexity parameter X = 5 for C1 and L2 quantization. Furthermore, we introduced a small constant cost term of 32 x d bits per reference vector to reflect the costs for transmitting or storing the codebook. That cost term, which is independent of the data volume and whose relative contribution to the total quantization costs vanishes for N ---$ 00, prevents very sparsely populated reference vectors with less than five data vectors assigned to. The codebooks for K-means clustering were determined such that they quantized the training data generating the same entropy as the codebooks for entropy optimized quantization. (The resulting entropies of the test data might differ for both quantization schemes since single images do not exactly match the statistics of the training set). The K-means clustering codebooks for L1, L2, d2 had sizes K = 12, 40, 104. Note that the codebooks for K-means clustering are much smaller than the codebooks for entropy optimized quantization. This computational disadvantage of entropy optimized quantization is outweighed by the considerably smaller error rate of compressed images. It also has to be mentioned that not all codebook vectors are used in the compression experiments due to differences between the statistics of individual images and the statistics of the training set. Table <ref type="table" target="#tab_3">I1</ref> summarizes four different compression experiments. The first three test images were taken from the same gallery as the training images, the fourth face image (terry) was generated under different lighting conditions with another camera setting. The entropy optimized quantizers produce a 1 0 4 % smaller error for comparable or superior compression ratios than K-means clustering. Furthermore, face images that have been compressed by optimizing the complexity C, = -logpa, look sharper and more natural than the results  <ref type="bibr">(d, e)</ref> shows the differences between the original image and the compressed and reconstructed images @), (c), respectively. According to our efficiency criterion entropy optimized compression is 36.8% more efficient than K-means clustering for a compression factor 24.5. The peak SNR values for (b), (c) are 30.1 and 27.1, respectively. The considerable higher error near edges in the reconstruction based on K-means clustering (e) demonstrates that entropy optimized clustering of wavelet coefficients preserves psychophysically important image features like edges more faithfully than conventional compression schemes.</p><formula xml:id="formula_15">is of size 4096 vectors. The intermediate (p = 2) frequency set L2 = { z i = ( ( D i -J ) ( i ) , (D&amp;2Z)(i), ( D ~-z z ) ( i ) ) }</formula><p>VIII. ONLINE VECTOR Q L J A N ~~A ~O N</p><p>The described vector quantization procedures are batch algorithms since they require all data points {si} to be available at the same time, a very restrictive constraint for applications dealing with a large data volume. Online vector quantization algorithms process a data stream sequentially by updating y, and pa in an iterative fashion [30], <ref type="bibr">[35]</ref>. But how do we have to change the reference vectors and the cluster probabilities after data points ZN has been processed? This question is normally studied in stochastic approximation theory. We will estimate the most likely changes of the codebook {y,} by making a Taylor expansion of the reestimation equations 18), ( <ref type="formula" target="#formula_5">19</ref>)) i.e., we assume that the changes Ap, p&amp;" -iN-1)9Ay, E y&amp;" -yiN-" are of order 0(1/N). The upper indices (N) and (N -1) denote the estimates of p,, y, at the time steps t = N and t = N -1, respectively. The update rule for pa, y, after N -1 data points have been processed is Online optimization of the codebook size K relies on a heuristics for cluster merging and cluster creation. We have explored the following heuristics for cluster creation: A data point z; initializes a new reference vector K + 1 with yK+l = z i if the costs of assigning z i to an already existing cluster exceeds the complexity costs of the new codebook vector K + 1, i.e., C K + ~ &lt; min, (IIziy,llr + Ma). We found in a series of quantization experiments that this strategy causes a slight overestimation of the optimal codebook size but the resulting codebooks have comparable quantization costs to codebooks found in batch optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. DISCUSSION</head><p>Vector quantization and data clustering have a wide spectrum of engiwering applications ranging from data compression for transmission and storage purposes to image and speech processing for pattern recognition tasks <ref type="bibr">[l]</ref>. When we assign partitions of a data set to a reduced set of reference vectors we have to make a compromise between the simplicity and precision of the resulting representation, e.g., between the size of the codebook and the distortion error due to data quantization. The key point of our paper is to explicitly express this compromise in a cost function that comprises both complexity and distortion costs. Joint optimization of various complexity and distortion terms results in an intrinsic limitation of the number of reference vectors, in contrast to other currently known approaches like K-means clustering or the LBG algorithm.</p><p>We have applied the maximum entropy principle and the formalism of statistical mechanics to estimate optimal solutions of our vector quantization problem. There exist several distinct advantages of the statistical mechanics approach for the design of vector quantizers.</p><p>1) Maximizing the entropy allows us to study a variety of different distortion measures and complexity measures and to derive a system of reestimation equations for the reference vectors and the assignment probabilities.</p><p>2) The resulting algorithm to determine the optimal codebook maps naturally to a simulated annealing approach where a slow, controlled decrease of the computational temperature produces nearly optimal solutions. The computational temperature controls the degree of noise in the data assignments, which essentially interpolates between a hard and a fuzzy clustering solution.</p><p>3) The structure of the reestimation equations suggests hardware implementations in analog VLSI as they are known from neural network research. Complexity optimized vector quantization maps onto a two layer neural network with a winner-take-all architecture as discussed in <ref type="bibr">[6]</ref>. Hardware implementations of such network architectures have been successfully tested [2], <ref type="bibr">[31]</ref>. Our study of entropy optimized quantization of wavelet transformed gray-level images revealed the interesting fact that entropy optimized codebooks reproduce sparse image features like edges more faithfully than the conventional K-means clustering approach. Those image features, however, possess a high-information content due to their rare occurence and compression errors there impair any subsequent information processing step much more than errors in other parts of data space. The sharp appearance of the reconstruction in Fig. <ref type="figure">3(b</ref>) supports this finding.</p><p>In a related study, we have extended the maximum entropy approach for vector quantization to the case of supervised data clustering [6], <ref type="bibr">[8]</ref>. An additional cost term is used to penalize partitionings of data space which are in conflict with a priori known class knowledge. The respective algorithm is implemented by a three layer neural network with classification units in the third layer <ref type="bibr">[6]</ref>. We consider the similarity of the discussed algorithms and the underlying reestimation equa-tions for codebook parameters with neural network systems as a very fruitful direction for future research which might not only produce better information processing systems but might also lead to a more fundamental understanding of perception. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>It is a pleasure to thank H. Somopolinsky, T. Tishby, and P. Tavan for helpful discussions and two anonymous reviewers for useful suggestions to improve the manuscript. J. Buhmann acknowledges the hospitality of the Aspen Center for Physics where this work was initiated during a workshop on Physics of Neural Networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>[19], [34], [50]. The new cost function with a discussion of different choices for distortion and complexity measures is introduced in Section 11. A maximum entropy solution for this cost function is derived in Section 111. Our approach is inspired by the work of Rose et al. [38], [39] on K-means clustering. An optimization algorithm and simulation results are summarized in Section IV. The asymptotic level density of vector quantizers in the high complexity limit is studied in Section V. These calculations determine the functional dependence of the codebook density on the data density and are related to [a], [49]. We also address the question in E ~~( { M ~} &gt; = minza (cL1 E,"==, ~i,~i,(z~, 2,)) (see ZIn the spirit of the work on K-means clustering and referring to the close relationship between hard and fuzzy clustering in the maximum entropy framework, we will use the terms "cluster center" and "reference vector" synonymously throughout this paper, although we arc aware of approaches like hierarchical clustering [16] or Bayesian clustering [Z] which are designed to unwver "natural" structures in a data set and that do not necessarily optimize the wst function (1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>functions G a u ( t i , Y ) and specialize to the centroid condition necessary. Sa, is the Kronecker symbol (Sa, = 1 if a = Y and S , , = 0 if a # v). We have indexed the quantization costs € K ( { M ~~} ) with the size K of the codebook since we compare VQ solutions with different codebook sizes K. The minimum of € K ( { M ~, } ) for different assignments {Mi,} and for different numbers of reference vectors K determines the optimal size of the codebook. Too complex codebooks are penalized by large complexity costs C ( p a ) C, that only depends on the assignment probability. Note that the index Q in (3) is supposed to sum only over the set of different clusters (2) by setting L(zi, Y )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>d</head><label></label><figDesc>Both measures (6) and (7) leave EK ( { Mi,}) invariant under permutations of the reference vectors 9 ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>10) is derived in Appendix A. The parameters be introduced to strictly enforce the definition of the reference vectors 1/N E, E, M,uG,u(z,, Y ) = 0 and the definition { p , } are the order parameters of the vector quantization problem. The respective conjugate fields {y,}, {Pa) have to N a=1^(=1 ay, of the assignment probabilities p , = 1/N E, ME,. The differential quantization costs E,, in (10) are with the fuzzy assignment variables</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>the centroid of the data set and place the first codebook vector there (K = 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>IF F ~T ~ 2</head><label>2</label><figDesc>~k~ or 3a: p, = o or 3a, p: y, = yp, THEN reject new codebook and increment IC. IF IC &gt; K THEN QUIT, ELSE GOT0 3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>[9] in two respects: 1) we optimize the codebook size K by a systematic search for the lowest free energy 3 K ; 2) temperature variation in the spirit of simulating annealing allows us to avoid suboptimal local minima of 3 K . In the first quantization example (Fig. I) we have chosen a two dimensional data distribution, generated by four normally distributed sources of equal variance (Fig. l(a)). Three zero temperature solutions for the complexity measure C = -logpa, calculated for different values of A, are shown in Fig. l@)-(d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>[23]. Our results are restricted to rth-power Euclidean distortions D ( z , y) = 11% -ylJ', although the analysis can be generalized to the class of difference distortion measures [46].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Fig. 1. (a) Data distribution consisting of 4OOO points, generated by four normally distributed sources of equal variance. Centers of the sources are marked with plus signs + @)-(d). Zero temperature solutions of entropy optimized quantization C , = -logp, are shown for the complexity weights h = 5.0, 2.5, 0.4 in @), (c), (d). The locations of the reference vectors are marked with asterisks * .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>[22], we make the basic assumption that for large K the partitions S, approximate the optimal polytope S* far dimension d and distortion measure D ( z , y). Given D ( z , y) = ( 1%yll', we substituteD ( z , y,) dz = I(d, r)(V(S,))(d+')/d, (24) I ( d , r ) = Js. D ( z , y,) dz/(V(S*))(*+')/* being the normalized inertia of the optimal polytope s*. Inserting (24) into (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Therefore, we vary this cost functional with respect to Y(z) to derive the dependence of the codebook density Y(z) on the probability density zI(z). The resulting Euler-Lagrange equation is For the cases of load balancing (lb) and entropic complexity (ent), C(p) = l/p8 and C(p) = -1ogp respectively, the solutions of (29) )dependence of Y on II for the K-means density ((30) for 8 = 1) and the entropic density (31) have been derived in the classic papers of Gish and Pierce [23] b d Zador [49].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>'</head><label></label><figDesc>low. The rth-power Euclidean distortion costs Dsingle caused by a single codebook vector with partition volume Vsingle amount to The volume of a single cluster in the high-and lowdensity parts of the distribution is given by Vsingle = Vhigh/low/Khigh/low respectively, Khigh/low being the number of bdebook vector in the respective regions. Using (30) and (31), we find in the K-means clustering case and for entropy coding complexity Vhigh, low Vhigh + Kow ' Khigh, low = Kent Using these expressions, we find the 'distortion costs Our first criterion, that the overall distortion are identical for both vector quantizers (Dent G D K ~) , establishes a relationship between the required codebook sizes, i.e., The second, more stringent condition that the outlier error is the same for both VQ's, i.e, (Dent, low D K ~, low) constrains the sizes of the codebooks to The cluster probabilities in the high-and low-density respectively. The codebook entropy in the K-mean clustering case is therefore, gions are k$ven by Pa = nhigh/lowVhigh/low/Khigh/low, rewhile in case of entropy coding complexity we find The difference of these entropies is x K mxent Demanding identical overall distortion errors, we find using (38)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>is</head><label></label><figDesc>Vhigh = 1, Kow = (2n10~)-"(~+') and nhigh = 1 -VII. COMPRESSION OF WAVELET-DECOMPOSED IMAGES Lossy image compression is a "real-world" information processing problem that is well suited to study the influence of different complexity measures on the performance of vector quantization algorithms. Image compression based on orthogonal [15], [36], [45] or nonorthogonal [14], [44] wavelet decompositions has witnessed increasing popularity in recent years. The wavelet data format is supposedly optimal for natural image representation since the coefficients are statistically independent if we average over a large set of natural images [21], [44]. Quantization of wavelet coefficients yields a psychophysically more pleasing image quality [ 151 than quantization schemes which are based on raw pixel blocks or on the discrete cosine transform of pixel blocks. In this section, we use the image compression task to compare the efficiency of the K-means clustering scheme (C, = l/p,) with entropy optimized vector quantization (C, = -log p a ) . The compression results have been published in part [7]. The images are decomposed with the wavelet algorithm of Mallat [36] which uses quadrature mirror filtering for the subband decomposition. The partitioning of the Fourier plane by Mallat's algorithm is shown in Fig. qa). The filter functions Q p , p = 1, 2, 3 decompose an image Z in a low-nlow = 0.05, nhigh = 0.89, Vhigh = 1, &amp;ow = 2.15, N = 6000</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>A and I-B show results for one-dimensional distributions and squared Euclidean (r = 2) distortion costs and for two different data densities nlow in the low-density region of the distribution. The density in the highdensity region and the size V of the highand lowdensity regions are chosen a s described in the text. For every A value in Table LA four (eight in Table I-B) different configurations were generated using logarithmic complexity costs. frequency support of the wavelet filters Q', Q2, and form: The upper index v E (1, 2, 3) of D&amp;,Z refers to the respective filter function \E", the lower index p denotes the reduction factor. A z -l l and Az-zZ are the low-pass filtered images after one or two reductions steps, respectively. Filter coefficients from a particular image position i are combined to a threedimensional vector 2,. Q f in Fourier space. (b) Wavelet coefficients of image Z arranged in matrix pass filtered image A2-1Z and a bandpassed image signal represented by the wavelet coefficients V[_,Z, p = 1, 2, 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>has size 1024. A low-pass filtered image of size 32 x 32 forms the set d2 = (2, = (A2-2Z)(i)} of 1024 scalar values. The training data for the K-means algorithm and the complexity optimized vector quantizer are the union of L2-sets, the union of L1-sets and the union of d2 sets, all taken from 10 different face images. During the training stage we solve (U), (19) for each of these training sets and for both complexity measures under comparison. This procedure yields three codebooks for entropy optimized quantization and three codebooks for K-means clustering. All distortions are measured as the squared Euclidean distance Vi&amp;*, y,) = (zi -The complexity weights X in the K-means clustering case are adjusted such that the compression ratio of both quantization schemes is the same, i.e., we require that K-means clustering and entropy optimized clustering compress the training data with the same resulting entropy. The error between an original test image and its reconstruction from the quantized wavelet representation allows us to quantify the efficiency of the complexity measures C , = l/p, and C, = -log p , for image compression. The reconstruction error is measured as the absolute difference between the original pixel value Z(i) and its reconstructed value Zc(i), i.e., 1/N ELl I Z(i) -Zc(i) 1 . Such an error measve supposedly is compatible with the sensitivity of the visual system [15].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>from K-means clustering. Psychophysically important image features like edges generate large wavelet amplitudes in the high frequency band. The corresponding outlier regions of t ' and L2 are more accurately sampled by entropy optimized quantization than by K-means clustering. Differences in the peak signal to noise ratios (SNR) are listed in the table to make a comparison with published compression results [40] possible. Images of a compression and reconstruction experiment are shown in Fig. 3. We have used a complexity parameter X = 50 to design entropy optimized codebooks for tl, L2, and X = 5 to design an entropy optimized codebook for d2. The respective codebook sizes are K = 11, 15, 50 for t', t2, d2. The resulting compression ratio is 24.5. The corresponding K-means clustering codebooks with the same resulting entropy on the training set have sizes K = 4, 5, 28 for Ll, L2, A2. The high complexity costs have been chosen to demonstrate the different types of reconstruction errors caused by the two vector quantization schemes. Image (a) is the original, the first row shows reconstructions of compressed, wavelet decomposed image using entropy optimized vector quantization (b) or K-means clustering (c). The second row</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>1 NFig. 3 .</head><label>13</label><figDesc>Fig. 3. Quantization of a 128 x 128, 8-bit gray-level image. (a) Original picture. @) Image reconstruction from wavelet coefficients which were quantized with entropic complexity. (c) Reconstruction from wavelet coefficients quantized by K-means clustering. (d) Reconstruction error of image @). (e) Reconstruction error of image (c). Black is normalized in image (d) and (e) to a deviation of 92 gray values. Note the large errors near edges in (e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>FREE ENERGY 3 K The factor expP3K in the Gibbs distribution (9) can be written as 2 is known as the partition function in statistical mechanics and plays the role of a generating function. The differential quantization costs Ei, in (48) are defined by (11). The sum runs over all K~ legal configurations {Mi, 1 i = 1,. . . , N ; a = 1, a , K}, i.e., over all configurations with Mia E (0, 1) and C,Mia = 1. The product of delta functions strictly enforces the constraints p , = ELl Mi,/N The integral representation of 6-functions allows us to and E,, MiuBcw(zi, Y) = 0. rewrite the partition function (48) as The function 3 ;( y a , f,, p,, &amp;) is extensive, i.e., it scales as O ( N ) . The integral in (50) is dominated by the global minimum of the function (51), which is called the free energy.Necessary conditions for the global minimum of F&amp; are dF&amp;/dfi, = a3&amp;/dp, = 0, d3&amp;/dg, = 83&amp;/dg, = 0, which are used to derive the reestimation equations (12H15).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>[30], [37]. Mathematically, we replace the distortion costs Dia in (3) by the averaged distortion errors K K (Did = CT,,Di,(Z,, 9,). (8) ,=l</head><label></label><figDesc></figDesc><table><row><cell>Therefore, the transitions T , , cause additiopal distortions</cell></row><row><cell>which have to be taken into account when we estimate the</cell></row><row><cell>most likely assignment variables. The channel noise breaks the</cell></row><row><cell>permutation symmetry of the reference vectors and imposes a topology on the set of indices {a I (Y = 1, -, K}. Codeword</cell></row><row><cell>assignments that take the characteristics of the channel noise</cell></row><row><cell>into account yield superior results [19], [50]. Such a procedure</cell></row><row><cell>is also known as source-channel-coding, Following the same</cell></row><row><cell>line of thought, Luttrell [34] established a connection to a</cell></row><row><cell>class of topological vector quantization algorithms known as</cell></row><row><cell>self-organizing feature maps</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I1 SUMMARY</head><label>I1</label><figDesc>OF FOUR COMPRESSION TRIAL3 .</figDesc><table><row><cell cols="2">Face Images Pixel Entropy</cell><cell></cell><cell>Entropy of I ?</cell><cell>A2</cell><cell>Average Error</cell><cell cols="2">Peak SNR Compr. ratio</cell><cell>Efficiency</cell></row><row><cell></cell><cell></cell><cell>1.57</cell><cell>1.01</cell><cell>1.47</cell><cell>2.83</cell><cell>35.91</cell><cell>14.8</cell><cell></cell></row><row><cell>doro</cell><cell>5.9</cell><cell>1.77</cell><cell>1.05</cell><cell>1.44</cell><cell>3.14</cell><cell>34.45</cell><cell>13.4</cell><cell>11.0% (4.2%)</cell></row><row><cell></cell><cell></cell><cell>1.88</cell><cell>1.05</cell><cell>1.31</cell><cell>2.46</cell><cell>35.75</cell><cell>12.9</cell><cell></cell></row><row><cell>brix</cell><cell>4.6</cell><cell>1.81</cell><cell>1.05</cell><cell>1.28</cell><cell>3.01</cell><cell>33.35</cell><cell>13.2</cell><cell>22.4% (7.2%)</cell></row><row><cell></cell><cell></cell><cell>1.76</cell><cell>1.04</cell><cell>1.76</cell><cell>3.08</cell><cell>35.06</cell><cell>13.3</cell><cell></cell></row><row><cell>yildir</cell><cell>6.2</cell><cell>1.82</cell><cell>1.07</cell><cell>1.82</cell><cell>3.63</cell><cell>32.99</cell><cell>13.0</cell><cell>17.8% (6.3%)</cell></row><row><cell></cell><cell></cell><cell>2.39</cell><cell>1.18</cell><cell>1.55</cell><cell>3.60</cell><cell>33.69</cell><cell>10.4</cell><cell></cell></row><row><cell>terry</cell><cell>7.0</cell><cell>2.17</cell><cell>1.15</cell><cell>1.49</cell><cell>4.66</cell><cell>30.11</cell><cell>11.3</cell><cell>29.4% (11.9%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>The upper numbers in the stacks refer to entropy optimized quantization, the lower numbers result from K-means clustering experiments. The average error is the average, absolute difference between the original pixel value</head><label></label><figDesc></figDesc><table><row><cell>Z"(i), i.e., 1 / N CL, 1 Z(L) -Z c ( i ) I.</cell><cell>Z(i) and its reconstructed value</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>The efficiency is defined as the relative difference in reconstruction error between the two quantization methods. The efficiency values in brackets are the gains measured by peak SNR changes.</head><label></label><figDesc></figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>was supported by the German Federal Ministry of Science and Technology (lTR-8800-Hl) and by the Aif Force Office of Scientific Research (884274) (C. von der Malsburg, PI) while working at the Center for Neural Engineering, supported by a graduate fellowship of the Technische Universitat Miinchen. This work was supported by the US. Department of Energy, Lawrence Livermore National Laboratory under Contract W-7405-Eng-48.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Current mode subthreshold MOS circuits for analog VLSI neural systems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Malt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Melton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Andreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Boahen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">0</forename><surname>Pouliquen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pavasovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Strohbehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Net</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>Neural Net.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A convergence theorem for the fuzzy ISODATA clustering algorithms</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Bezdek</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="1967-01">1967. Jan. 1980</date>
		</imprint>
	</monogr>
	<note>Behavioral Sciences</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">C-means clustering with the 11 and 1, norms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bobrowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Bezdek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="545" to="554" />
			<date type="published" when="1991-06">May-June 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Complexity optimized data clustering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kiihnel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computat</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="88" />
			<date type="published" when="1990">1990. 205-213, 1991. 1992</date>
		</imprint>
	</monogr>
	<note>by competitive neural networks</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Complexity optimized vector quantization: A neural network approach</title>
	</analytic>
	<monogr>
		<title level="m">Dam Compression Conference &apos;92</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Storer</surname></persName>
		</editor>
		<meeting><address><addrLine>Ed. Los Alamitos</addrLine></address></meeting>
		<imprint>
			<publisher>C A IEEE Computer Society Press</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised and supervised data clustering with competitive neural networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Baltimore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Lookabaugh</surname></persName>
		</author>
		<author>
			<persName><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE LICNN Int. Conj on Neural Net</title>
		<imprint>
			<date type="published" when="1989-03">Mar. 1989</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="299" to="315" />
		</imprint>
	</monogr>
	<note>IEEE Trans. Inform. Theory</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast adaptive K-means clustering: Some empirical results</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Darken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moody</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Joint Conj Neural Net</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<publisher>New York John Wiley</publisher>
			<date type="published" when="1990">1991. June 17-21, 1990</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="233" to="238" />
		</imprint>
	</monogr>
	<note>Elements oflnformation Theory</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Complete discrete 2-D Gabor transforms by neural networks for image analysis and compression</title>
		<author>
			<persName><forename type="first">C</forename><surname>Darken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Daugman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech Signal Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1169" to="1179" />
			<date type="published" when="1988">1992. July 1988</date>
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Mateo, CA</pubPlace>
		</imprint>
	</monogr>
	<note>Towards faster stochastic gradient search</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image compression through wavelet transform coding</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Devore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jawerth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Lucier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Classification and Scene Analysis</title>
		<meeting><address><addrLine>New York Wiley</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1973">1973</date>
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
	<note>pt. 11, R. 0. Duda and</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A fuzzy relative of the ISODATA process and its use in detecting compact well-separated clusters</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Dunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cybern</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An analogue approach to the travelling salesman problem using an elastic net method</title>
		<author>
			<persName><forename type="first">R</forename><surname>Drubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Willshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">326</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A study of vector quantization for noisy channels</title>
		<author>
			<persName><forename type="first">N</forename><surname>Farvardin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE MD</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="689" to="691" />
			<date type="published" when="1974">1992. Jan. 1989. p ~. 719-746, Mar. 1992. 32-57. 1974. 1987</date>
		</imprint>
	</monogr>
	<note>IV-796-N-801</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Trans. Inform Theoj</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="799" to="809" />
			<date>July 19h</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Optimal algorithms for approximate clustering</title>
		<author>
			<persName><forename type="first">T</forename><surname>Feder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Greene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pioc. 20th Ann. ACM Symp. Theory of Computing (STOC)</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="434" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Relations between the statistics of natural images and the response properties of cortical cells</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Amer. A</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Asymptotically optimal block quantization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform Theory</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="373" to="380" />
			<date type="published" when="1979-07">July 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Asymptbtically efficient quantizing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Pierce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="676" to="683" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vector quantization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Acoust., Speech Signal Processing Mag</title>
		<imprint>
			<biblScope unit="page" from="4" to="29" />
			<date type="published" when="1984-04">Apr. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adaptive block transfom d i n g of speech based on LPC vector quantization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cheeseman</surname></persName>
		</author>
		<author>
			<persName><surname>Farvardin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2611" to="2620" />
			<date type="published" when="1991-12">1991. Dec. 1991</date>
		</imprint>
		<respStmt>
			<orgName>NASA Ames Research Center</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. FIA-90-12-7-01</note>
	<note>Bayesian classification theory</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Information theory and statistical mtchanics</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Jaynes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Rev</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="620" to="630" />
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the rationale of maximum-entropy methods</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ieee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Gelatt</surname></persName>
		</author>
		<author>
			<persName><surname>Vecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="page" from="671" to="680" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
	<note>Optimization by simulated annealing</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Self-Organization and Associative Memory</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Winnertake-all detworks of O(n) complexity,&quot; in Neutral Information Processing Systems 1</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lazzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ryckebusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Mahowald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Mead</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<pubPlace>San Mateo, C A Morgan Kaufmann</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An algorithm for vector quantizer design</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Linde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="84" to="95" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Least squares quantization in PCM</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Injorm. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Luttrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Berkeley Symp. Mathematical Statist. and Probability</title>
		<meeting>5th Berkeley Symp. Mathematical Statist. and Probability</meeting>
		<imprint>
			<date type="published" when="1967">1967</date>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
	<note>Hierarchical vector quantisation</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A theory for multiresolution signal decomposition: The wavelet representation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine IntelL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">H</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Martinetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schulten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation and Self-Organizing Maps</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<pubPlace>New York Addison Wesley</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A deterministic annealing approach to clustering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gurewitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn. Lett</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="589" to="594" />
			<date type="published" when="1990-11">Nov. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Statistical mechanics and phase transitions in clustering</title>
	</analytic>
	<monogr>
		<title level="j">Physical Rev. Len</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="945" to="948" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Vector quantization for entropy coding of image subbands</title>
		<author>
			<persName><forename type="first">T</forename><surname>Senoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint/>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Statistical mechanics as the underlying theory of &quot;elastic&quot; and &quot;neural&quot; optimizations</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Simic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="89" to="103" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Constrained nets for graph matching and other quadratic assignment problems</title>
	</analytic>
	<monogr>
		<title level="j">Neural Computat</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="268" to="281" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Statistical dependence between orientation filter outputs used in an human vision based image code</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tikochinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">2</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wegmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zetsche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Proceedings of the Vuual Communications and Image Processing &apos;90</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Kunt</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1984">1984. 1990</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2638" to="2644" />
		</imprint>
	</monogr>
	<note>Physical Rev. A</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Asymptotic performance of block quantizers with difference distortion measures</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Westerink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Boekee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Biemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint/>
	</monogr>
	<note>IEEE Trans. Inform</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Generalized deformable models, statistical physics, and matching problems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computat</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fuzzy sets</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Contr</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="338" to="353" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Asymptotic quantizatjon error of continuous signals and the quantization dimension</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Zador</surname></persName>
		</author>
		<author>
			<persName><surname>It-K</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeger</surname></persName>
		</author>
		<author>
			<persName><surname>Gersho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Theory</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="713" to="719" />
			<date type="published" when="1982">Dec. 1990. 12. 1987. 1982. 1989. 1989. 1988</date>
		</imprint>
	</monogr>
	<note>IEEE Trans. Commun.</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName><surname>Theory</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980-01-28">Jan. 1980. 28. 1982</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="139" to="149" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
