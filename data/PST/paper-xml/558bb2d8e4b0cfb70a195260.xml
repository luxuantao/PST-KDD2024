<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<postCode>E1 4NS</postCode>
									<settlement>London</settlement>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">99CCA4C57035E5D48E27762B27BB5050</idno>
					<idno type="DOI">10.1109/TSP.2013.2245663</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-This article deals with learning dictionaries for sparse approximation whose atoms are both adapted to a training set of signals and mutually incoherent. To meet this objective, we employ a dictionary learning scheme consisting of sparse approximation followed by dictionary update and we add to the latter a decorrelation step in order to reach a target mutual coherence level. This step is accomplished by an iterative projection method complemented by a rotation of the dictionary. Experiments on musical audio data and a comparison with the method of optimal coherence-constrained directions (MOCOD) and the incoherent K-SVD (INK-SVD) illustrate that the proposed algorithm can learn dictionaries that exhibit a low mutual coherence while providing a sparse approximation with better signal-to-noise ratio (SNR) than the benchmark techniques.</p><p>Index Terms-Dictionary learning, iterative projections, mutual coherence, sparse approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION: LEARNING INCOHERENT DICTIONARIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Sparse Approximation and Dictionary Learning</head><p>I N this paper we consider a sparse synthesis model where a signal is approximated by a sparse linear combination of elementary functions called atoms. Arranging the atoms along the columns of the dictionary matrix , we can express the model as:</p><p>(</p><p>where is a sparse vector of approximation coefficients, with . Here the pseudo-norm counts the number of non-zero coefficients of its argument and is the number of active atoms. The parameters of this model can be determined by solving a sparse approximation problem and optimizing: <ref type="bibr" target="#b1">(2)</ref> Note that the dual formulation where the objective function is the number of the non-zero elements of the approximation and the constraint is a fixed level of residual norm can be also considered, although we will not employ it in the present work.</p><p>A dictionary learning problem for sparse approximation consists of optimizing the set of atoms given a set of observed data , such that every signal in the training set can be effectively represented by the sparse model <ref type="bibr" target="#b0">(1)</ref>  <ref type="bibr" target="#b26">[27]</ref>. This can be concisely written by arranging the observed signals along the columns of the matrix :</p><p>(</p><p>where is a sparse matrix whose columns contain the vectors of approximation coefficients. Optimizing the dictionary is a challenging problem for which no general analytic solution can be found. The numerical strategy commonly employed consists in iterative algorithms that start from an initial dictionary and alternate between the following steps:</p><p>• Sparse coding: given a fixed dictionary , the matrix of sparse approximation coefficients is calculated using any suitable algorithm for sparse approximation. • Dictionary update: given a fixed approximation matrix , the dictionary is updated in order to minimize the residual cost function . In addition, the dictionary is usually constrained to belong to a set of admissible dictionaries whose atoms have unit norm, and for the reminder of the paper we will consider to work with normalized dictionaries without further specification. Many dictionary learning algorithms <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b28">[29]</ref> that follow this approach have been proposed in the literature.</p><p>The sparse approximation (2) that is at the core of the sparse coding step of dictionary learning has been proved to be a NP hard problem <ref type="bibr" target="#b10">[11]</ref>, and a great number of sub-optimal algorithms that run in polynomial time <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b24">[25]</ref> have been developed in order to tackle it. An important research effort has been devoted to understand how the different strategies and algorithms for sparse modelling perform in different settings. For example, sparse recovery deals with retrieving a sparse signal from a set of incomplete measurements and has applications in the field of compressed sensing <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b31">[32]</ref>, while sparse approximation is concerned with how efficiently a general signal can be approximated by linear combinations of a few atoms from an over-complete dictionary <ref type="bibr" target="#b11">[12]</ref>.</p><p>The theorems that have been proposed in the literature to this aim link the success of the algorithms with the coherence of the dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Importance of Incoherent Dictionaries</head><p>The coherence of a dictionary indicates the degree of similarity between different atoms or different collections of atoms. A simple measure that has been proposed in the literature is the mutual coherence , which is defined as the maximum absolute inner product between any two different atoms of the dictionary: where we use the ordinary euclidean inner product for real vectors . In the reminder of this paper, we will omit the dependency on the dictionary whenever unambiguous from the context.</p><p>Tropp <ref type="bibr" target="#b31">[32]</ref> showed that, given a sparse signal generated according to the model (1), the orthogonal matching pursuit algorithm (OMP) <ref type="bibr" target="#b24">[25]</ref> is guaranteed to retrieve the correct support of the representation coefficients if <ref type="bibr" target="#b3">(4)</ref> and further refined this bound by defining the cumulative coherence function as a measure of the correlation between different groups of atoms in the dictionary. Schnass and Vandergheynst <ref type="bibr" target="#b27">[28]</ref> proved that essentially the same results also hold for the thresholding algorithm <ref type="bibr" target="#b3">[4]</ref>.</p><p>Equation (4) implies that only signals which are synthesized from active atoms are guaranteed to be correctly recovered. However, for a dictionary, the mutual coherence is lower-bounded by <ref type="bibr" target="#b30">[31]</ref> (5)</p><p>As an illustrative example, a dictionary containing 200 atoms in has a mutual coherence , and the sparse representation of a signal generated with such dictionary is guaranteed to be correctly retrieved if the number of active atoms is . Based on results for sparse recovery, Gribonval and Vandergheynst <ref type="bibr" target="#b15">[16]</ref> extended the work of Tropp <ref type="bibr" target="#b31">[32]</ref> and showed that the residual error resulting from running matching pursuit (MP) <ref type="bibr" target="#b21">[22]</ref> for a finite number of steps on a signal to be approximated is upper bounded by a constant times the residual error achieved by the best -term approximant of (as it would be returned by a combinatorial search over all the possible sets of atoms). As for the results on sparse recovery, the number of active atoms is constrained by the mutual coherence of the dictionary .</p><p>In addition, Tropp <ref type="bibr" target="#b32">[33]</ref> showed that the coherence of a dictionary is linked to the condition number of its sub-dictionaries (i.e., matrices defined by selecting a subset of the atoms), and used this relation to prove average-case results on sparse recovery for based algorithms. This implies that achieving a low mutual coherence results in well-conditioned sub-dictionaries and further motivates the objective of the present work.</p><p>Incoherent dictionaries are desirable whenever sparse approximations are sought in order to reveal an underlying structure or clustering in the data. For example, morphological component analysis <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b4">[5]</ref> decomposes a signal over a set of dictionaries that have been previously learned from different training data consisting of morphologically dissimilar classes (i.e., edges and textures for an image, or different classes of instruments for a musical audio signal). The mutual incoherence between different learned sets of atoms is a prerequisite that allows for a sparse coding where the position of the non-zero coefficients can be informative for classification and source separation applications.</p><p>Previous research attempting to join the approximation and incoherence objectives will be reviewed in Section II. In this paper we propose a novel technique which employs a decorrelation step inspired by a method used to construct Grassmannian frames <ref type="bibr" target="#b33">[34]</ref>. Our main contributions are that we employ this technique within the context of incoherent dictionary learning, as explained in Section III, and adapt it to the approximation objective through a novel rotation step. Section IV presents numerical experiments on musical audio data, and a comparison with the methods previously proposed in <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b25">[26]</ref>. Section V contains our conclusions and plans for further investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PREVIOUS WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Method of Optimal Coherence-Constrained Directions (MOCOD)</head><p>Ramirez et al. <ref type="bibr" target="#b25">[26]</ref> proposed a dictionary learning algorithm inspired by the method of optimal directions (MOD) <ref type="bibr" target="#b12">[13]</ref> in which the sparse approximation is performed using a novel penalty term derived from a probabilistic formulation of the sparse model <ref type="bibr" target="#b0">(1)</ref>, and the dictionary update step is modified in order to promote mutually incoherent atoms.</p><p>In particular, the incoherence objective is pursued by introducing into the dictionary learning optimization the term where each element of the Gram matrix contains the inner product between the -th and the -th atom of the dictionary. This expression measures the Frobenius distance between the Gram matrix of the dictionary and the identity matrix, which corresponds to the Gram matrix of an orthonormal dictionary whose mutual coherence is zero.</p><p>Overall, the optimization presented in <ref type="bibr" target="#b25">[26]</ref> reads as: <ref type="bibr" target="#b5">(6)</ref> In this unconstrained minimization, the first term represents the modelling error, while the desired properties of dictionary and representation coefficients are enforced through penalty terms. In particular, the penalty factor multiplied by promotes sparsity of the representation coefficients, while the factors multiplied by and promote mutual incoherence and unit norm of the dictionary atoms respectively.</p><p>In order to solve this optimization, the sparse approximation is followed by a MOCOD dictionary update step, obtained by setting to zero the derivative of the above cost function with respect to the dictionary . The resulting update can be written as <ref type="bibr" target="#b25">[26]</ref>:</p><p>Note that setting to zero the penalty factors and results in the MOD update <ref type="bibr" target="#b12">[13]</ref>.</p><p>As will be detailed in Section IV, the MOCOD algorithm is to some extent effective in constraining the mutual coherence of a dictionary. However, the unconstrained optimization <ref type="bibr" target="#b5">(6)</ref> makes it difficult to identify an explicit relationship between the penalization factor and the coherence level of the resulting dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dictionary Decorrelation and INK-SVD</head><p>An alternative strategy for learning incoherent dictionaries can be pursued by including a decorrelation step to the iterative scheme illustrated in Section I. At each iteration of the dictionary learning algorithm consisting of sparse approximation followed by dictionary update, we add the following optimization problem: <ref type="bibr" target="#b6">(7)</ref> where the objective is a cost function that expresses the approximation quality of the dictionary and is a fixed target mutual coherence level. Mailhé et al. <ref type="bibr" target="#b18">[19]</ref> proposed a matrix nearness problem where <ref type="bibr" target="#b7">(8)</ref> and is the matrix returned by the dictionary step, which translates as finding the closest dictionary (in a Frobenius norm sense) to a given dictionary subject to a mutual coherence constraint. In order to tackle this optimization, the authors propose an iterative algorithm which consists of identifying a sub-dictionary of highly correlated atoms and decorrelating pairs of atoms in a greedy fashion, until the desired mutual coherence is achieved. This technique was used in conjunction with the K-SVD algorithm <ref type="bibr" target="#b0">[1]</ref> and is called incoherent K-SVD (INK-SVD) dictionary learning.</p><p>The choice of the cost function <ref type="bibr" target="#b7">(8)</ref> does not explicitly measure the approximation accuracy, but it rather implicitly assumes that dictionaries that are close to each other are well suited to represent the same set of data. In contrast, we use in the present work the cost function that measures the Frobenius norm of the residual. In Section IV both the MOCOD and the INK-SVD algorithms are compared to the method proposed in this paper, which is shown to achieve a better performance overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Other Related Work</head><p>Dai et al. <ref type="bibr" target="#b8">[9]</ref> recently observed that the K-SVD dictionary learning algorithm can converge to ill-conditioned sub-dictionaries that perform poorly for sparse approximation. To address this issue they proposed a penalized optimization which promotes approximation coefficients with bounded Frobenius norm. They show how this strategy results in well-conditioned sub-dictionaries, and in a smaller approximation residual. A dictionary with small mutual coherence has been shown to contain well-conditioned sub-dictionaries <ref type="bibr" target="#b32">[33]</ref>. Therefore, even if the method proposed by Dai et al. does not specifically attempt to learn incoherent dictionaries, it can still be regarded as a related work that motivates the research presented here.</p><p>Yaghoobi et al. <ref type="bibr" target="#b34">[35]</ref> proposed a dictionary design method for coding of audio signals where the parameters of gammatone atoms <ref type="bibr" target="#b29">[30]</ref> are optimized in order to minimize the mutual coherence of the resulting dictionary. In this work, the authors are inspired by the iterative projections method that also is at the core of our proposed dictionary learning, and show through experimental results the advantages of using an incoherent dictionary for sparse recovery and sparse approximation. Despite the similarity in the motivation and in part of the optimization technique, dictionary design is substantially different from dictionary learning: while the former involves optimizing the parameters of a set of parametric functions that are designed to be suited for a given class of signals, the latter is adapted to an arbitrary set of observed variables and can therefore be extended to classes of signals for which an efficient dictionary is not known. Moreover, in the case of dictionary design there is not a mixed objective consisting of good approximation and mutual incoherence because the former is implicitly assumed given the nature of the parametric functions and of the signals to be analyzed. For this reason we limit our experimental comparisons to dictionary learning techniques.</p><p>Apart from incoherent dictionary learning or design, Schnass and Vandergheynst <ref type="bibr" target="#b27">[28]</ref> presented a method for dictionary preconditioning that aims at tackling the problem of coherent dictionaries for sparse recovery. In this work, a sensing matrix is multiplied by a coherent dictionary in order to obtain an equivalent sparse recovery problem with low cross-cumulative coherence (i.e. the cumulative coherence between atoms of the sensing matrix and atoms of the dictionary), and improve the performance of greedy sparse approximation algorithms. Although related to the present work, we choose not to further detail or benchmark this algorithm as it does not involve dictionary learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ITERATIVE PROJECTIONS AND ROTATIONS ALGORITHM</head><p>In Section II we reviewed previous work on learning incoherent dictionaries, including the MOCOD dictionary update and the INK-SVD algorithm. The former addresses the incoherence objective by defining an unconstrained optimization problem with penalization terms, while the latter employes a constrained minimization that explicitly bounds the mutual coherence of the dictionary. Both theses methods have disadvantages:</p><p>• MOCOD does not allow to specify a given mutual coherence level, but rather relies on setting correct values of the penalty factors and in <ref type="bibr" target="#b5">(6)</ref>. The relationship between the two factors and the mutual coherence of the learned dictionary is difficult to evaluate, and heuristic choices or a computationally expensive search over the space of parameters must be carried out. Moreover, in Section IV we document that, even when performing such search, the mutual coherence of the resulting dictionaries does not drop below a level with the experimental settings considered. • INK-SVD does constrain the learned dictionary to a fixed mutual coherence level, and defines a minimization problem based on the cost function <ref type="bibr" target="#b7">(8)</ref> to achieve this goal. This objective does not take into account the approximation performance of the dictionary, but rather its Frobenoius distance from the output of the dictionary update stage of dictionary learning. For this reason, the dictionary resulting from INK-SVD does not match the data set well and results in poor approximation performance at low mutual coherence levels. This will be documented in the numerical experiments presented in Section IV. In the present work, we propose a dictionary learning algorithm that allows us to update the dictionary fixing a constrained mutual coherence, while at the same time minimizing the residual error of the resulting sparse approximation. <ref type="bibr" target="#b8">(9)</ref> For this purpose, we employ a standard dictionary learning scheme consisting of sparse coding followed by dictionary update, and we add to the latter a dictionary de-correlation consisting of the following steps:</p><p>• Atoms decorrelation: obtained through an iterative projection algorithm, this step ensures that the mutual coherence constraint is satisfied. • Dictionary rotation: this step optimizes the dictionary with respect to the objective function <ref type="bibr" target="#b8">(9)</ref> without affecting its mutual coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Constructing Grassmannian Frames With Iterative Projections</head><p>A Grassmannian frame is a collection of atoms that have unit norm and minimal mutual coherence. It can be proved that, for an dictionary, the mutual coherence is bounded by <ref type="bibr" target="#b4">(5)</ref>, and the lower bound is reached when the dictionary is an equiangular tight frame, that is, a Grassmannian frame where any pair of different atoms have the same absolute inner product <ref type="bibr" target="#b30">[31]</ref>. It is also worth noting that equiangular tight frames do not exist for any pair , but necessarily (and not sufficiently) require if the atoms are real or if the atoms are complex.</p><p>Constructing Grassmannian frames is an open research problem for which there is generally no analytic solution. One possible approach is to use an iterative projection method <ref type="bibr" target="#b33">[34]</ref>. To illustrate this algorithm, we define two constraint sets, namely the structural constraint set as the set of symmetric square matrices with unit diagonal values and off-diagonal values with magnitude smaller or equal than :</p><p>and the spectral constraint set as the set of symmetric positive semidefinite square matrices with rank smaller than or equal to :</p><p>In the above expressions, the operators and return the vector of diagonal elements and the vector of eigenvalues of their arguments respectively.</p><p>The iterative projection algorithm starts from an initial dictionary , calculates its Gram matrix , and iteratively projects it onto the sets and until a stopping criterion is met. • Projection onto the structural constraint set. Given an arbitrary Gram matrix , its projection onto the structural constraint set can be obtained by setting its diagonal values to one and by limiting the magnitude of its off-diagonal values: 1) Set 2) Limit the off-diagonal elements so that, for ,</p><p>• Projection onto the spectral constraint set. Given an arbitrary dictionary , its Gram matrix is by construction a symmetric, positive semidefinite matrix. Its projection onto the spectral constraint set can be obtained through the following steps: 1) Calculate an eigenvalue decomposition (EVD)</p><p>2) Threshold the eigenvalues by keeping only the largest positive ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>if and if or</head><p>where the eigenvalues in are ordered from the largest to the smallest. Following this step, at most eigenvalues of the Gram matrix are different from zero.</p><p>3) Update the Gram matrix as , so that . Once the Gram matrix has been iteratively projected onto the two sets and the stopping criterion has been met, it is factorized as the product <ref type="bibr" target="#b9">(10)</ref> through the following steps:</p><p>1) Calculate an EVD 2) Set so that .</p><p>Note that at this point, the dictionary is not guaranteed to have a mutual coherence bounded by . The intersection between the sets and may be empty for certain values of and (in fact, it is empty whenever is lower than the bound (5)). The iterative projections algorithm is only guaranteed to converge to an accumulation point <ref type="bibr" target="#b33">[34]</ref> consisting of a pair of matrices and that are not necessarily located at a minimal distance between the constraint sets. However, we found in our numerical experiments that the algorithm works well for values of close to the lower bound <ref type="bibr" target="#b4">(5)</ref>, providing a dictionary with constrained mutual coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dictionary Rotation</head><p>We can use the iterative projection algorithm illustrated so far to de-correlate a dictionary starting from the matrix returned by the dictionary update step. However, optimizing the Gram matrix with the only objective being reducing the mutual coherence means that the decomposition ( <ref type="formula">10</ref>) is likely to lead to an updated dictionary that does not approximate the training set well. To resolve this issue, we employ a dictionary rotation 1 which does not modify the mutual coherence and that is optimized for the dictionary learning objective <ref type="bibr" target="#b8">(9)</ref>.</p><p>The decomposition <ref type="bibr" target="#b9">(10)</ref> is not unique, since for any orthogonal matrix we obtain:</p><p>Therefore, it is possible to apply an orthogonal matrix to the dictionary obtained from the iterative projection algorithm in order to minimize the residual norm expressed in <ref type="bibr" target="#b8">(9)</ref>. The resulting optimization problem can be expressed aws: <ref type="bibr" target="#b10">(11)</ref> where is the set of orthogonal matrices. The solution to this problem can be traced back to an algorithm proposed by Horn et al. <ref type="bibr" target="#b16">[17]</ref> to align sets of points measured in different coordinate systems for stereo photogrammetry and robotics applications.</p><p>Let us define as the matrix containing the sparse approximation of the observed data. The minimization problem <ref type="bibr" target="#b10">(11)</ref> can be expressed as <ref type="bibr" target="#b16">[17]</ref>:</p><p>Since the first two terms do not depend on and since for every pair of matrices and , , we can instead consider the maximization problem: <ref type="bibr" target="#b11">(12)</ref> 1 Rotation is from now on employed with an abuse of terminology, referring to any linear transformation obtained through an orthonormal matrix that include flips and rotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The notation</head><p>indicates the sample covariance between the observed signals and their approximations, which can be decomposed using an SVD as . The objective function in <ref type="bibr" target="#b11">(12)</ref> can be written as: where the matrix is orthonormal because resulting from the product of three orthonormal matrices. Considering that is diagonal, the following holds: <ref type="bibr" target="#b12">(13)</ref> The singular values are non-negative because resulting from the SVD decomposition of a covariance matrix, and the entries are upper-bounded by 1 because the norm of the vectors is unitary. Therefore, the value maximizes the above equation, and implies . This can be obtained by setting:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Iterative Projections and Rotations (IPR) Algorithm</head><p>The dictionary rotation can be performed only once after the decorrelation algorithm, or at every step of the iterative projections. We chose the latter strategy as it leads to an algorithm that adapts the dictionary to the approximation objective (9) at each step of the decorrelation and resulted in superior experimental results. It is worth mentioning that the whole dictionary decorrelation could be performed only once after dictionary learning, but we found in our numerical experiments that this strategy led to poor approximation results too.</p><p>We initialize the algorithm with the dictionary returned by the update step of dictionary learning and perform at each iteration the following steps summarized in Algorithm 1:</p><p>• Compute the Gram matrix: . • Calculate the projection onto the structural constraint set: . • Factorize as in <ref type="bibr" target="#b9">(10)</ref> including thresholding its eigenvalues. This returns an updated dictionary whose Gram matrix is projected onto the spectral constraint set. • Rotate the dictionary using an optimal orthonormal transform by updating . Note that the rotation step does not modify the Gram matrix of the dictionary, and therefore is irrelevant for the purpose of the convergence of the iterative projections algorithm to a dictionary with bounded coherence. The convergence analysis of the general dictionary learning optimization described by ( <ref type="formula">9</ref>) is very difficult and is outside the scope of the present paper. The interested reader can find insights on related problems by reading the work of Aaron et al. <ref type="bibr" target="#b1">[2]</ref>, Gribonval and Schnass <ref type="bibr" target="#b14">[15]</ref>, Geng et al. <ref type="bibr" target="#b13">[14]</ref> or Mailhé and Plumbley <ref type="bibr" target="#b19">[20]</ref>.</p><p>Nonetheless, it is worth highlighting the fact that the rotation step finds the optimal solution of the problem <ref type="bibr" target="#b10">(11)</ref>, and therefore is guaranteed to improve (or leave unchanged) the cost function (9) without violating its constraints set. This is sufficient to say that adding a rotation step to the dictionary decorrelation algorithm improves the approximation quality of dictionary learning if compared to the iterative projections algorithm alone.</p><p>It is possible to quantify this improvement by considering the bounds of the square of the residual cost function <ref type="bibr" target="#b10">(11)</ref> with respect to the rotation matrix defined as . Letting be a constant, and recalling that (13) provides bounds for the quantity , the cost function can assume values within the interval where refers to the singular values of the covariance matrix . The lower bound is reached in correspondence with the optimal rotation matrix . The value obtained discarding the rotation step is , where depends on the inner products between vectors from the unitary matrices and .</p><p>It is worth noting that when the covariance matrix is zero (i.e., when the signals and their sparse approximations are uncorrelated), then the rotation step does not lead to any improvement of the cost function. However, this case is unlikely to happen as is produced to approximate . The IPR algorithm includes the calculation of the optimal rotation matrix described in III-B which replaces our early formulation based on a Lie group method <ref type="bibr" target="#b2">[3]</ref>. Beside offering a closed-form solution to a problem that was previously tackled with an iterative method, this substantially improved the computational time required by the algorithm and allowed for a simpler analysis of its complexity.</p><p>Since , the running time of the algorithm per iteration is dominated (in order) by the following steps:</p><p>• Computation of the EVD of the Gram matrix requiring operations. • Computation of the covariance matrix requiring operations.</p><p>• Computation of the SVD of the covariance matrix requiring operations. In the numerical experiments presented in Section IV, we observed that these three operations accounted for around 90% of the computational time required by every iteration of the IPR algorithm, which order of magnitude is comparable to the one relative to the time required by running a dictionary update step using K-SVD or MOD. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. NUMERICAL EXPERIMENTS</head><p>We tested the proposed decorrelation method with the K-SVD dictionary learning algorithm in order to assess if it converges to a dictionary that exhibits bounded mutual coherence and good approximation quality. The test signal we used is the musical excerpt music03_16kHz, a 16 kHz guitar recording that is part of the data included in SMALLBOX <ref type="bibr" target="#b9">[10]</ref>, a Matlab toolbox for testing and benchmarking dictionary learning algorithms used in our evaluation and containing the code needed to reproduce the results presented here<ref type="foot" target="#foot_0">2</ref> . A musical audio signal was chosen because previous informal experiments resulted in K-SVD learning a highly coherent dictionary for this type of data.</p><p>We divided the recording into 50% overlapping blocks of 256 samples (corresponding to 16 ms) with rectangular windows and arranged the resulting time-domain signals as columns of the training data matrix . Then, we initialized a twice overcomplete dictionary for sparse approximation using either a randomly chosen subset of the training data or an over-complete Gabor dictionary. We run the dictionary learning algorithms for 50 iterations, allowing for non-zero coefficients in each representation (which corresponds to about 5% of active elements if compared with the dimension of the audio frames ). When testing the algorithm proposed in <ref type="bibr" target="#b25">[26]</ref>, we used OMP as a sparse approximation step setting the stopping criterion to the maximum number of active atoms and MOCOD for the dictionary update. INK-SVD and IPR were implemented using OMP for the sparse approximation step and K-SVD for the dictionary update. Table <ref type="table" target="#tab_1">I</ref> summarizes the tested algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. MOCOD Updates</head><p>The unconstrained optimization illustrated in ( <ref type="formula">6</ref>) relies on the penalty factors and in order to promote incoherence of the dictionary and unit norm of the atoms respectively. To evaluate the MOCOD dictionary update for the purpose of incoherent dictionary learning, we tested different values of these factors on a logarithmic scale between and , assessing the resulting mutual coherence and signal-to-noise ratio (SNR) achieved by the optimized dictionary, the latter being defined as: Fig. <ref type="figure" target="#fig_0">1</ref> depicts the results of our experiment using respectively randomly chosen data from the training set and a twice over-complete Gabor dictionary for the initialization. We run the experiment 5 times to increase the significance of our results whenever the initialization involved choosing a random subset of the training data as the initial dictionary. When and , the optimization (6) converges to a standard dictionary learning where the atoms are not forced to be incoherent, but are constrained to be unit norm. This case corresponds to the left corner of the surf plots in Fig. <ref type="figure" target="#fig_0">1</ref>. We can note that a data initialization produces a highly coherent dictionary with the best approximation quality, while a Gabor initialization results in a lower coherence at the expense of a worse SNR. Continuing our analysis in the case of data initialization, keeping and increasing the coherence penalty factor results in a dictionary with lower mutual coherence, but also in a worse approximation quality. This behavior is further illustrated by the mutual coherence-reconstruction scatter plot, which depicts against SNR of the sparse approximation for every learned dictionary and exhibits a clear (although highly variable) trend. In the case of Gabor initialization, on the other hand, it seems that the parameter does not affect mutual coherence and reconstruction error for high values of , while decreasing the penalty factor has generally a negative effect on both and SNR of the learned dictionaries.</p><p>To understand the poor performance of the MOCOD algorithm, especially when initialised with a Gabor dictionary, we inspected and SNR of the sparse approximation at every iteration, along with the percentage change of the dictionary with respect to the Frobenious norm, that is defined as: <ref type="bibr" target="#b13">(14)</ref> were</p><p>indicates the dictionary at iteration .</p><p>The main observation that underlies the poor performance of MOCOD is that the percentage change of the dictionary does not converge to zero as the number of iterations increases and, therefore, the algorithm does not converge to a fixed point of the objective function <ref type="bibr" target="#b5">(6)</ref>. Whenever is set to be small (that is, when the dictionary atoms are not forced to be unit norm), the optimization is very unstable and we often observed that the mutual coherence ends being greater than the one of the initial dictionary, especially for low values of .</p><p>When is set to be large, the algorithm still does not converge to a fixed point of the objective function, but the mutual coherence and SNR are much more stable. In this case different initializations lead to different solution paths as evaluated in terms of SNR and mutual coherence as a function of the optimization iterations. In the case of data initialization, the mutual coherence drops and the SNR oscillates, while in the case of Gabor initialization, the SNR does not change significantly and the mutual coherence slightly increases. Moreover, the minimum mutual coherence achieved by MOCOD in the results shown is smaller than 0.3, and further experiments with penalization terms confirmed that the algorithm is unable to reach lower mutual coherence levels.</p><p>Unlike MOCOD, INK-SVD and the proposed IPR algorithm allow us to set a target coherence and to run the dictionary decorrelation iteratively until it is achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. IPR and INK-SVD</head><p>After experimenting with different combinations of dictionary learning and decorrelation iteration numbers, we found that consistently good results can be achieved by performing 50 iterations of the K-SVD dictionary learning combined with 5 iterations of the relevant decorrelation method. This also led to comparable running times, as will be discussed in Section IV-C. We set the target mutual coherence in logarithmically spaced intervals from 0.05 to 1 and compared the two algorithms by evaluating the achieved SNR. When applying the methods to an initial dictionary formed by randomly selected vectors from the training set, we run the experiment for 10 independent trials to obtain more significant results.</p><p>Fig. <ref type="figure" target="#fig_2">2</ref> depicts the results of our experiment. As can be noted, both algorithms succeed in matching the target coherence levels for both initializations except for the lower end on the left side of the plots, with IPR performing slightly better in achieving the smallest mutual coherence in the case of data initialization, reaching a value of around 0.055 compared to the 0.06 of INK-SVD. Whenever the target coherence is larger than the coherence level achieved without dictionary de-correlation, the two methods simply act as a K-SVD without any mutual coherence constraint. In the case of data initialization, we can observe that INK-SVD obtains a good SNR for mutual coherence values greater than , after that its performance degrades substantially. On the contrary, the proposed IPR does not perform as well for high coherence values, but does not significantly degrade from to . The results for Gabor initialization, on the other hand, favour the proposed algorithm showing a better SNR and no significant approximation degradation for all the target coherence values.</p><p>It is worth noting that using the IPR algorithm both data and Gabor initializations lead to incoherent dictionaries producing similar SNR values. This suggests that the algorithm might converge to equivalent fixed points of the optimization function regardless of its initialization. Despite this being a desirable result, we cannot claim it to be a general property of the IPR algorithm, as we would require a formal analysis of the convergence of dic- tionary learning that is outside the scope of the present paper. On the other hand, the particular SNR value of about 15 dB reached by the optimization depends on the training set used to learn the dictionary, as will be sown in Section IV-D that presents results obtained with a different training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Running Times</head><p>Fig. <ref type="figure" target="#fig_3">3</ref> shows the running times of the IPR and INK-SVD algorithms for different coherence levels, tested on a iMac with a 3.06 GHz Intel Core 2 Duo processor running MATLAB R2011a and the cputime function. The IPR values are not dependant on the coherence level and are just below 100 seconds, whereas INK-SVD takes longer to compute less coherent dictionaries. This is because INK-SVD acts in a greedy fashion by decorrelating pair of atoms until the target mutual coherence is reached (or until a maximum number of iterations) and therefore the number of pairs of atoms to decorrelate increases for low values of the target coherence.</p><p>The time required to compute a non de-correlated dictionary can be found in the right end of the plots and is around 20 seconds, which is also consistent with the average time of 23 seconds needed by the MOCOD algorithm. This means that the cost of IPR is about 5 times the cost of a standard K-SVD for the problem sizes considered in our experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Sparse Approximation Results</head><p>The relation between the coherence of a dictionary and its approximation properties for different classes of signals is a complex topic. In this section we do not attempt a formal convergence analysis of the tested dictionary learning algorithms as this is outside the scope of the present paper.</p><p>The trade-off between mutual coherence and SNR of the sparse approximation visible in Figs. <ref type="figure" target="#fig_1">1(a</ref>), 2(a) and 2(b) is consistent with the fact that the different decorrelation methods aim at solving penalized or constrained optimization problems. If we compare the general dictionary learning problem introduced in Section I-A to the incoherent formulations presented in this paper, the penalty factors used to promote incoherence in the unconstrained optimization <ref type="bibr" target="#b5">(6)</ref> and the feasible set consisting of dictionaries with bounded mutual coherence in the constrained problem <ref type="bibr" target="#b8">(9)</ref> suggest that an incoherent dictionary is expected to have a worse approximation performance if compared to a coherent one. On the other hand, dictionary learning is a non-convex optimization problem that to the best of our knowledge lacks strong and general convergence results, relying instead on the ability of practical algorithms to converge to local minima of the optimization cost function.</p><p>For the purpose of the experimental evaluation of the IPR algorithm, we tested whether the mutual coherence versus SNR trade-off is consistent over different training and testing signals. We considered the following test material:</p><p>• music03_16 kHz, a guitar recording distributed as part of the SMALLBOX that was used to train the dictionaries in the experiments presented so far.</p><p>Fig. <ref type="figure">4</ref>. Mutual coherence versus SNR of the sparse approximation using a dictionary learned from track n.6 of the jazz section of the RWC database using data initialization, OMP and 5% of active atoms in the sparse coding step of dictionary learning. In the testing phase, OMP with 5% of active atoms was also used to approximate signals from the training set, from music03_16 kHz that is a different guitar recording and form track n.1 of the jazz section of the RWC database that is a piano recording.</p><p>• track n.6 of the jazz section of the RWC music database 3 , which is an electric guitar recording. • track n.1 of the jazz section of the RWC music database, which is an acoustic piano recording. After running the IPR dictionary learning algorithm on the guitar recording track n.6 using the data initialization, the same problem parameters specified in Section IV and the target mutual coherence levels specified in Section IV-B, we employed the learned dictionaries to approximate the two remaining test signals, using the OMP algorithm and 5% of active atoms, as in the learning phase.</p><p>Fig. <ref type="figure">4</ref> displays the results of the experiment. If we compare these values to the ones presented in Fig. <ref type="figure" target="#fig_2">2</ref>(a), we can note that the trade-off between mutual coherence and SNR is no longer present, and that the approximation of the training set (which in the case of the training guitar is inversely proportional to the residual norm in the cost function ( <ref type="formula">9</ref>)) is around 12 and 13 dB for the two guitar signals and around 10 dB for the piano signal. The absence of a steep peak in correspondence with a dictionary with high mutual coherence and the overall worse approximation performance can be explained by the fact that music03_16 kHz is a relatively short signal, that as a consequence when learning a dictionary from this signal the number of training vectors compared to the size of the dictionary is relatively low and that we observed a few signals that could be approximated very well using only one atom in the dictionary. This does not happen when learning a dictionary from a longer training set obtained using track n.6 and results in overall worse but more consistent results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS AND PLANS FOR FUTURE INVESTIGATION</head><p>We presented the iterative projections and rotations (IPR) decorrelation algorithm, a method for dictionary decorrelation to be used within the context of dictionary learning. Our technique is based on an iterative projection optimization used to 3 available at http://www.staff.aist.go.jp/m.goto/RWC-MDB/ construct Grassmannian frames and includes a dictionary rotation step that makes it suitable for the approximation objective (9) of dictionary learning. Experiments on musical audio data demonstrate the performance of IPR and suggest that it can outperform state-of-the-art algorithms especially when a very low mutual coherence is required. The computational time of IPR is of the same order of magnitude than the time required by a standard K-SVD dictionary learning.</p><p>Exploring the applications of the proposed work is one of the main objectives for future investigation. On one hand, incoherent dictionary learning can be adapted and applied to compressed sensing technologies, both for audio and for other types of signals that are amenable to sparse approximations. On the other hand, classification and separation tasks can benefit from the proposed algorithm in the context of morphological component analysis. IPR acts on the Gram matrix by thresholding the correlation between the atoms of the dictionary, and can be easily adapted to decorrelate only certain subsets of the dictionary that correspond to different morphological components or sources.</p><p>Finally, extending the decorrelation strategy to more accurate measures of coherence, such as the cumulative coherence proposed by Tropp <ref type="bibr" target="#b31">[32]</ref>, and a better theoretical understanding of the interplay between coherence and approximation performance are both objects of current endeavours.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 :</head><label>1</label><figDesc>Iterative Projections and Rotations:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Mutual coherence and reconstruction error achieved using the MOCOD dictionary update and (a) randomly chosen samples from the training set or (b) a Gabor frame as the initial dictionary. The surf plots show the mutual coherence and SNR of the sparse approximation as a function of the two regularization parameters and in (6). In the scatter plots, the levels and indicate the maximum and minimum coherence attainable by a dictionary.</figDesc><graphic coords="8,129.00,66.12,336.00,558.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Mutual coherence and reconstruction error achieved using the proposed iterative projections and rotations (IPR) algorithm and INK-SVD dictionary decorrelation, initialised with (a) randomly chosen samples from the training set as the initial dictionary or (b) a twice over-complete Gabor dictionary. The error bars in (a) represent the standard deviation resulting from 10 independent trials of the experiment and indicate that the results are consistent, regardless the random element introduced in the initialization.</figDesc><graphic coords="9,40.98,65.16,245.10,295.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Running times of IPR and INK-SVD for different mutual coherence levels and dictionaries initialised with (a) randomly chosen samples from the training set or (b) a twice over-complete Gabor dictionary. The error bars indicate the standard deviation resulting from 10 independent trials of the experiments.</figDesc><graphic coords="9,304.98,64.14,246.07,277.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I ALGORITHMS</head><label>I</label><figDesc>FOR LEARNING INCOHERENT DICTIONARIES</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>http://www.small-project.eu/software-data/smallbox</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the Queen Mary University of London School Studentship, the EU FET-Open project FP7-ICT-225913-SMALL Sparse Models, Algorithms and Learning for Large-scale data, and by a Leadership Fellowship from the UK Engineering and Physical Sciences Research Council (EPSRC).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4311" to="4322" />
			<date type="published" when="2006-11">Nov. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the uniqueness of overcomplete dictionaries, and a practical way to retrieve them</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra Appl</title>
		<imprint>
			<biblScope unit="volume">416</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="67" />
			<date type="published" when="2006-07">Jul. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dictionary Learning for the Sparse Approximation of Audio Signals Research Open Day, School of Electron</title>
		<author>
			<persName><forename type="first">D</forename><surname>Barchiesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eng. Comput. Sci</title>
		<imprint>
			<date type="published" when="2012-04">Apr. 2012</date>
			<pubPlace>London, U.K.</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Queen Mary Univ</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Iterative thresholding for sparse approximations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Blumensath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Davies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Fourier Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="629" to="654" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enhanced source separation by morphological component analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bobin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Moudden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Starck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoust., Speech Signal ess. (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="833" to="866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Morphological component analysis: An adaptive thresholding strategy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bobin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Starck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Fadili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Moudden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2675" to="2681" />
			<date type="published" when="2007-11">Nov. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An introduction to compressive sampling</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Wakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="21" to="30" />
			<date type="published" when="2008-03">Mar. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Atomic decomposition by basis pursuit</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Sauders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="129" to="159" />
			<date type="published" when="2001-03">Mar. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dictionary learning and update based on simultaneous codeword optimization (SIMCO)</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech Signal ess</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2037" to="2040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SMALLbox-An evaluation framework for sparse representations and dictionary learning algorithms</title>
		<author>
			<persName><forename type="first">I</forename><surname>Damnjanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E P</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Latent Variable Anal. Signal Separation (LVA/ICA)</title>
		<meeting>Int. Conf. Latent Variable Anal. Signal Separation (LVA/ICA)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="418" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adaptive greedy approximations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Avellaneda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Construct. Approx</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="98" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nonlinear approximation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Devore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Numerica</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="51" to="150" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Method of optimal directions for frame design</title>
		<author>
			<persName><forename type="first">K</forename><surname>Engan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">O</forename><surname>Aase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Husøy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoust., Speech Signal ess. (ICASSP)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="2443" to="2446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">On the local correctness of -minimization for dictionary learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<ptr target="http://arxiv.org/pdf/1101.5672.pdfFeb" />
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dictionary identification: Sparse matrix-factorisation via -minimisation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schnass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3523" to="3539" />
			<date type="published" when="2010-07">Jul. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the exponential convergence of matching pursuit in quasi-incoherent dictionaries</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="255" to="261" />
			<date type="published" when="2006-01">Jan. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Closed-form solution of absolute orientation using orthonormal matrices</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Hilden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Negahdaripour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Amer. A</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1127" to="1135" />
			<date type="published" when="1988-07">July 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning overcomplete representations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lewicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computat</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="337" to="365" />
			<date type="published" when="2000-02">Feb. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">INK-SVD: Learning incoherent dictionaries for sparse representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mailhé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barchiesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech Signal ess</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3573" to="3576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Local optimality of dictionary learning algorithms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mailhé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop on Signal Process. Adapt. Sparse Structured Represent. (SPARS)</title>
		<meeting>Workshop on Signal ess. Adapt. Sparse Structured Represent. (SPARS)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">67</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Online learning for matrix factorization and sparse coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="19" to="60" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Matching pursuit with time-frequency dictionaries</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3397" to="3415" />
			<date type="published" when="1993-12">Dec. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">CoSaMP: Iterative signal recovery from incomplete and inaccurate samples</title>
		<author>
			<persName><forename type="first">D</forename><surname>Needell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Computat. Harmon. Anal</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="301" to="321" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Uniform uncertainty principle and signal recovery via regularized orthogonal matching pursuit</title>
		<author>
			<persName><forename type="first">D</forename><surname>Needell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vershynin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Computat. Math</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="317" to="334" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rezaiifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krishnaprasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 27th Asilomar Conf. Signals, Syst. Comput</title>
		<meeting>27th Asilomar Conf. Signals, Syst. Comput</meeting>
		<imprint>
			<date type="published" when="1993-11">Nov. 1993</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="40" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Ramírez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lecumberry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Sparse modeling with universal priors and learned incoherent dictionaries Inst. Math. and its Appl.s, Unvi. Minnesota, Tech. Rep. 2279</title>
		<imprint>
			<date type="published" when="2009-09">Sep. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dictionaries for sparse representation modeling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="1045" to="1057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dictionary preconditioning for greedy algorithms</title>
		<author>
			<persName><forename type="first">K</forename><surname>Schnass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1994" to="2002" />
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recursive least squares dictionary learning algorithm</title>
		<author>
			<persName><forename type="first">K</forename><surname>Skretting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Engan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2121" to="2130" />
			<date type="published" when="2010-04">Apr. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient auditory coding</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">439</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="978" to="982" />
			<date type="published" when="2006-02">Feb. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Grassmannian frames with applications to coding and communication</title>
		<author>
			<persName><forename type="first">T</forename><surname>Strohmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W J</forename><surname>Heath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Computat. Harmon. Anal</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="257" to="275" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Greed is good: Algorithmic results for sparse approximation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2231" to="2242" />
			<date type="published" when="2004-10">Oct. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On the conditioning of random subdictionaries</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Computat. Harmon. Anal</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2008-07">Jul. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Designing structured tight frames via an alternating projection method</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W J</forename><surname>Heath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Strohmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="188" to="209" />
			<date type="published" when="2005-01">Jan. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Parametric dictionary design for sparse coding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yaghoobi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Daudet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Davies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4800" to="4810" />
			<date type="published" when="2009-12">Dec. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">His main research interests are on sparse approximation and dictionary learning with applications to audio signals, audio scenes classification, and audio events recognition</title>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Barchiesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">M&apos;12) received the B.Sc. degree in applied mathematics in 2008 from the Università di Roma Tor Vergata, Italy, the M.Sc. degree in 2009 and the Ph</title>
		<imprint/>
		<respStmt>
			<orgName>Queen Mary University of London</orgName>
		</respStmt>
	</monogr>
	<note>D. degree in electronic engineering in 2013, both from Queen Mary University of London, U.K. He is now a Postdoctoral Research Assistant with the Centre for Digital Music</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
