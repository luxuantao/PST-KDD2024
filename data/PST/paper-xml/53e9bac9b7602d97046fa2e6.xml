<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Advances in design and application of spiking neural networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2006-03-03">3 March 2006</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Ammar</forename><surname>Belatreche</surname></persName>
							<email>a.belatreche@ulster.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Liam</forename><forename type="middle">P</forename><surname>Maguire</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Martin</forename><surname>Mcginnity</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Intelligent Systems</orgName>
								<orgName type="laboratory">Intelligent Systems Engineering Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Engineering</orgName>
								<orgName type="institution">University of Ulster</orgName>
								<address>
									<addrLine>Magee campus, Northland Road</addrLine>
									<postCode>BT48 7JL</postCode>
									<settlement>Derry</settlement>
									<country key="GB">Northern Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Advances in design and application of spiking neural networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2006-03-03">3 March 2006</date>
						</imprint>
					</monogr>
					<idno type="MD5">6CC14D9CE28708CFCD1D6ECBAB1E5EF6</idno>
					<idno type="DOI">10.1007/s00500-006-0065-7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Spiking neurons</term>
					<term>Spike response model</term>
					<term>Integrate-and-fire model</term>
					<term>Dynamic synapse</term>
					<term>Evolutionary strategy</term>
					<term>Temporal coding</term>
					<term>Supervised learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents new findings in the design and application of biologically plausible neural networks based on spiking neuron models, which represent a more plausible model of real biological neurons where time is considered as an important feature for information encoding and processing in the brain. The design approach consists of an evolutionary strategy based supervised training algorithm, newly developed by the authors, and the use of different biologically plausible neuronal models. A dynamic synapse (DS) based neuron model, a biologically more detailed model, and the spike response model (SRM) are investigated in order to demonstrate the efficacy of the proposed approach and to further our understanding of the computing capabilities of the nervous system. Unlike the conventional synapse, represented as a static entity with a fixed weight, employed in conventional and SRM-based neural networks, a DS is weightless and its strength changes upon the arrival of incoming input spikes. Therefore its efficacy depends on the temporal structure of the impinging spike trains. In the proposed approach, the training of the network free parameters is achieved using an evolutionary strategy where, instead of binary encoding, real values are used to encode the static and DS parameters which underlie the learning process. The results show that spiking neural networks based on both types of synapse are capable of learning non-linearly separable data by means of spatio-temporal encoding. Furthermore, a comparison of the obtained performance with classical neural networks (multi-layer perceptrons) is presented.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Spiking neural networks (SNNs) have been the subject of significant recent research reflecting the view that spikes have a key role in biological information processing <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. Unlike classical artificial neural networks (ANNs), which are based on a high abstraction of realistic neurons, as they only use rate coding to represent neuronal activity, spiking models offer a more detailed description of real biological neuron behaviour. It is believed that real neurons use more information than the average firing rate to perform computations. For example, the difference in firing times could convey relevant information about the input stimuli and the relative order of neurons firing times could be used as an alternative to rate coding <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>.</p><p>Many experiments have been carried out by neuroscientists in order to study the chemical interactions between neuron populations and much has been discovered about the dynamics of neuron potential and the spike generation process. Most simulations of neural networks share the assumption that synaptic efficacy (weight) is considered to be static during the reception of afferent spike trains. However, recent experimental studies of real biological neuron cells show that the synaptic efficacy generating the postsynaptic potential or PSP (the change in the membrane potential of the postsynaptic neuron when a spike or a series of spikes is received), is a variable (dynamic) quantity which depends on the presynaptic activity, i.e. the temporal structure of the presynaptic spike train <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. A specific model of these dynamic synapses (DSs), combined with a feature extraction approach, has been applied to speech applications in <ref type="bibr" target="#b13">[14]</ref><ref type="bibr">[15]</ref><ref type="bibr" target="#b14">[16]</ref>.</p><p>Learning is an important feature of the nervous system and it is believed that synapses play a key role in the learning process. Most existing spiking network training algorithms, also referred to as synaptic plasticity, developed to date are unsupervised and based on the classical Hebbian rule or the spike-timing dependent plasticity (STDP) <ref type="bibr" target="#b15">[17]</ref>. These forms of learning are less appropriate when a desired output is known in advance, a task for which supervised training algorithms are more preferred. A supervised training algorithm has been presented in <ref type="bibr" target="#b16">[18]</ref>, based on an adaptation of the classical backpropagation (BP) to a SRM based network. However, it suffers the problem of 'silent neurons', as the algorithm cannot be applied when a neuron does not fire. Moreover its convergence is not guaranteed and no information is given as to how the initial parameters are set.</p><p>In this paper, activity-dependent synapses (DSs) are used to connect integrate-and-fire neurons and their computation capability is evaluated when applied to perform pattern recognition of non-linearly separable data. A supervised training algorithm is developed, where the spiking network free parameters are adjusted using an evolutionary strategy (ES), which is a derivative free method. The performance of a DS based network is compared against one based on spike response model (SRM), where static synapses are used for neuronal connectivity. The performance obtained is also compared against conventional neural networks with respect to classification accuracy, the number of neurons employed and the number of adjustable parameters.</p><p>The inputs are represented in the form of spike trains, the means by which spiking neurons represent and communicate information. In DS based network, the pattern class is represented by the time to maximum response of the output neurons whereas in the SRM network the firing time of output neurons is used. The network free parameters are successfully tuned in a supervised way, using an adaptation of the self-adaptive ES used in <ref type="bibr" target="#b17">[19]</ref>. Real value encoding is used, instead of binary encoding, to encode the synaptic parameters, which are adjusted to minimize the error between the output neurons actual and desired times (firing times or times to maximum response) representing different input classes. The STDP algorithm <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b18">20]</ref> has also been used to train the network free parameters in an unsupervised way; however, the resulting poor performance proved that the use of an unsupervised approach is inappropriate for this task.</p><p>The remaining part of this paper is organised as follows: Sect. 2 gives an overview of spiking neurons dynamics and related models. In Sect. 3, the behaviour of DSs is described in detail along with the proposed architecture. Section 4 describes the SRM based architecture and its connectivity, while different spiking encoding schemes are illustrated in Sect. 5. Section 6 describes in detail the self-adaptive ES and the mapping mechanism between a spiking network (phenotype) and its corresponding genotype. The demonstration of this approach on the classification of non-linearly separable data and a comparison of spiking network performance with classical neural networks (MLPs) are given in Sect. 7. Finally, conclusions and additional perspectives are given in Sect. 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Spiking neuron models</head><p>The state of a spiking neuron is represented by a voltage across its cell membrane and a threshold. The neuron sub-threshold activity is determined by the integration of its excitatory and inhibitory postsynaptic potential (EPSP, IPSP). An EPSP causes an increase in the neuron potential while an IPSP decreases it. When its membrane potential reaches certain threshold from below the neuron generates a spike or action potential, which is then propagated forward to the next neurons through its axon.</p><p>When an action potential (the electrical signal which rapidly propagates along a neuron axon to other neurons. It is the result of a change in membrane potential) is received, the synapse transforms it into a change in postsynaptic neuron membrane potential (post synaptic potential) whose typical shape is shown in Fig. <ref type="figure">1</ref>. An action potential, once generated by a presynaptic neuron, takes a certain time to reach the postsynaptic neuron; this time is called synaptic delay.</p><p>Several mathematical models have been proposed to describe neuronal activity. They differ by the level of abstraction and the computation complexity offered by each model. The Hodgkin-Huxley <ref type="bibr" target="#b19">[21]</ref> model represents a detailed description of the neuron dynamics where the effects of different ionic channels conductances on the change in the neuron membrane potential are described using a set of ordinary differential equations. The thresholding process (firing when membrane potential reaches a certain threshold) is not explicitly modelled in this model which is computationally intensive for numerical simulations. In the integrate-and-fire model (IAF) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, however, a neuron is simulated by an RC-circuit where the membrane potential is represented by the voltage across the capacitor and an action potential is generated whenever the capacitor voltage reaches a certain threshold. The SRM is another model, which approximates the IAF model by a linear summation of the PSPs caused by the impinging spike trains from predecessor neurons. This model, whose dynamics are further detailed in section 4, is used with static synapses, i.e. neurons are interconnected through a simple scalar weight that modulates the amplitude of the incoming postsynaptic potentials at the receiving neuron. However, in the case of DSs (or weightless synapses) the synaptic strength changes upon the arrival of spike trains. This synapse model is used to connect IAF neurons in a feed-Fig. <ref type="figure">1</ref> Typical shape of a postsynaptic potential forward network, and their computing capabilities are investigated and evaluated. The behaviour of DS is described and illustrated in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dynamic synapses based architecture</head><p>This section summarises the DS model introduced in <ref type="bibr" target="#b8">[9]</ref>. The model assumes that a DS is represented by a finite amount of resources called neurotransmitters (chemicals released by nerve cells, i.e. neurons, at synapses that influence or communicate with other nerve cells). Each pre-synaptic spike (arriving at time t sp ) activates a fraction U SE (utilization of synaptic efficacy) of resources, which then quickly inactivate with a time constant (τ in ) and recover with a time constant (τ rec ). The synapse dynamics are governed by the following set of ordinary differential equations:</p><formula xml:id="formula_0">⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ dx dt = z τ rec -U SE × x × Ap(t -t sp ) dy dt = -y τ in + U SE × x × Ap(t -t sp ) dz dt = y τ in -z τ rec ,<label>(1)</label></formula><p>where x, y, and z are the fractions of resources in the recovered, active, and inactive states, respectively. The postsynaptic current is taken to be proportional to the fraction of resources in the active state, I syn = A SE y(t). A SE is the maximum strength of the synapse and Ap(tt sp ) is the action potential received at time t sp (see the Matlab simulations illustrated in Fig. <ref type="figure" target="#fig_0">2a,</ref><ref type="figure">b</ref>). These dynamics underlie the behaviour of a single synaptic connection between two neurons or between the input spike train sources and an input neuron <ref type="bibr" target="#b20">[22]</ref>. The synaptic current (I syn ) is fed to an IAF neuron, whose state is represented by a voltage across its cell membrane and a threshold. The status of the neuron is determined by the integration of its EPSP and IPSP. When its membrane potential reaches a certain threshold, the neuron generates (fires) a spike or action potential. The neuron dynamics are modelled by the following first order differential equation:</p><formula xml:id="formula_1">τ m dV dt = -V + R m I syn ,<label>(2)</label></formula><p>where τ m is the time constant of the neuron membrane, R m is its resistance and I syn represents the total synaptic inputs.</p><p>(The parameter values employed are τ m = 40 ms, R m = 100 M ). A feed-forward fully connected spiking network is used, where the different layers are labelled I, H, O for the input, hidden and output layer, respectively as shown in Fig. <ref type="figure">3</ref>. The spiking neurons are connected via DSs whose behaviour is described by the system defined in Eq. ( <ref type="formula" target="#formula_0">1</ref>). The variation of the time constant τ in of a DS connecting to a neuron i leads to a change in the 'time to maximum response' of that neuron. The neuron potential reaches its maximum response 'earlier' for smaller values of τ in , and 'later' for bigger values of τ in (see Fig. <ref type="figure">4</ref>). When the output neuron is considered, the early/late time to maximum response could therefore indicate the detection of a particular input (classification task). It is this parameter which is tuned during the training procedure for each connection in a chosen network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Spike response model based architecture</head><p>The SRM has been introduced in <ref type="bibr" target="#b1">[2]</ref>, and it is an approximation of the dynamics of integrate-and-fire neurons expressed by differential equations. The neuron status is updated through a linear summation of the postsynaptic potentials resulting from the impinging spike trains at the connecting synapses. A neuron fires whenever its accumulated potential reaches a threshold from below. Let us consider that a neuron j has a set j of immediate predecessors called pre-synaptic neurons and receives a set of spikes with firing times t i , i ∈ j . At most one spike is generated by each neuron during the simulation time. Neurons fire when their state variable x(t), called membrane potential, reaches a certain threshold θ . The internal state of a neuron is determined by</p><formula xml:id="formula_2">x j (t) = i∈ j w ji y i (t), (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where y i (t) is the unweighted PSP of a single spike coming from neuron i and impinging on neuron j. The height of the PSP is modulated by the synaptic weight w ji to obtain the actual PSP. The unweighted contribution y i (t) of a single synaptic terminal to the state variable of neuron j is given by</p><formula xml:id="formula_4">y i (t) = ε(t -t i -d ji ),<label>(4)</label></formula><p>where d ji is the associated synaptic delay as shown in Fig. <ref type="figure">5</ref>, t i is the firing time of the presynaptic neuron i, and ε(t) defines a spike response function describing a standard PSP, as shown previously in Fig. <ref type="figure">1</ref>. The function ε(t) is modelled with an α-function as</p><formula xml:id="formula_5">ε(t) = t τ e 1-(t/τ ) for t &gt; 0, else ε(t) = 0, (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where τ is the membrane potential time constant defining the decay time of the PSP. The firing time t j of neuron j is determined as the first time the state variable crosses the threshold from below. Thus, the firing time t j is a non-linear function of the state-variable x j . The threshold θ is constant and equal for all neurons in the network. The range of the data is first calculated, and then each input feature is encoded with a population of neurons that cover the whole data range. For a variable with range of Max-Min, a set of m Gaussian receptive RF neurons are used. The centre C i and the width σ of each RF neuron i are determined by the following equations:</p><formula xml:id="formula_7">C i = Min + 1 m -2 (2i -3) (Max -Min) , (<label>6</label></formula><formula xml:id="formula_8">) σ = 1 m -2 1 β(Max -Min) , (<label>7</label></formula><formula xml:id="formula_9">)</formula><p>where a value of 1.5 (for comparison with <ref type="bibr" target="#b16">[18]</ref>) is used for the variable β. For each n-dimensional input pattern, the encoding scheme results in a matrix n × m of values between 0 and 1. These values are then converted to delay times, where An illustration of this encoding scheme is shown in Fig. <ref type="figure">6</ref>, which shows the firing times resulting from the encoding of the real value '40' using four RF.</p><p>An important drawback of the sparse encoding is the increase of the input dimensionality and therefore the number of input neurons and weights. To avoid such a problem, another alternative encoding scheme is proposed, where each real value only generates one temporal value or firing time, allowing the original data dimension to be retained. This encoding scheme is referred to as "1-D coding". The mapping between real values and temporal values is performed through a simple linear function, which converts the real value features into a limited time interval called "encoding Fig. <ref type="figure">5</ref> Network architecture with connections between a neuron j and its predecessors highlighted. A connecting synapse is characterised by a weight, w, reflecting its strength and a delay, d, that a spike generated at the presynaptic neuron needs to reach the postsynaptic neuron Fig. <ref type="figure">6</ref> Sparse coding of a real value, and its corresponding firing time defined by the intersection of the vertical line with the Gaussians interval". The range of the original data is calculated and the following linear function is applied to the real feature values:</p><formula xml:id="formula_10">y( f ) = (b -a) range × f + (a × Max -b × Min) range , (<label>8</label></formula><formula xml:id="formula_11">)</formula><p>where f represents the original features values, [a, b] represents the temporal coding interval, range = Max -Min is the range of the original data and (Min, Max) represents the minimum and the maximum bounds of the original data, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Supervised training using evolutionary strategies</head><p>Genetic algorithms (GAs) and evolutionary algorithms <ref type="bibr" target="#b21">[23]</ref><ref type="bibr" target="#b22">[24]</ref><ref type="bibr" target="#b23">[25]</ref> evolve a population of candidate solutions to a given optimization problem with two types of operators: selection and variation. Selection biases the search towards highquality solutions by making more copies of good solutions at the expense of their inferior competitors. Variation operators generate new candidate solutions from the set of promising solutions. Two variation operators are common in current genetic and evolutionary computation: recombination or crossover, and mutation. GAs focus primarily on recombination, which creates new candidate solutions by combining features of promising solutions. On the other hand, the primary variation operator in ES <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b24">26]</ref> is mutation, which creates new candidate solutions by adding a random perturbation to the promising solutions. The power of an ES is mainly based upon its ability to perform a 'second order' evolutionary process. This process adapts the internal strategy parameters especially the mutation strength, in such a way that the whole algorithm exhibits near optimal performance. Thus an ES drives itself into an optimal working regime <ref type="bibr" target="#b24">[26]</ref>.</p><p>In this work, the training algorithm aims to learn, for a given set of temporal input patterns fed to the input layer, a set of target firing times at the output layer for the SRM-based network and a set of target maximum response times in the case of the DS based network. For this purpose, an ES, where only a mutation operator is used to search for the optimum set of network free parameters, allow the spiking network to learn the input temporal patterns. The free parameters to be tuned are represented by the set of time constants τ in in the case of DS based network, and the connection weights and delays in the case of SRM based network.</p><p>For example, consider a set of temporal input patterns denoted by {P(t 1 , . . . , t m )}, where P(t 1 , . . . , t m ) represents a single input pattern such that the components t 1 , . . . , t m define the firing times of each input neuron i ∈ I . For these input patterns a set of target times is assigned, denoted by {t t o }, at the output neurons o ∈ O. In order to learn the set of target times corresponding to the set of input patterns presented to the input neurons, the ES (derivative-free) searches for the optimum parameters (weights and delays or time constants τ in ) that minimise the total error between actual and target output times. Thus the objective function to be minimized is given by</p><formula xml:id="formula_12">E = T t o∈O (t a o (t) -t t o (t)) 2 , (<label>9</label></formula><formula xml:id="formula_13">)</formula><p>where t a o (t) and t t o (t) are, respectively, actual and target output times of node i for pattern t, and T is the total number of patterns in the training set. Other error functions such as the sum of absolute differences could also be used within this framework.</p><p>The choice of an ES is motivated by its ability to work on real numbers without the overhead of complex binary encoding schemes. ESs have been proved to be well suited for treating continuous optimization problems <ref type="bibr" target="#b24">[26]</ref>. A number of different mutation operators have been developed. The traditional mutation operator adds to the alleles of the genes some random values selected on a Gaussian normal distribution. Other mutation operators include the use of a Cauchy distribution.</p><p>In this work a slightly modified ES is used to train the spiking network in a supervised way, where a combination of both the Cauchy and Gaussian mutation is used. The use of the Cauchy distribution allows exploration of the search space by making large mutations and helping to prevent premature convergence. On the other hand the use of the Gaussian mutation allows the exploitation of the best solutions found as it performs a local search.</p><p>The spiking network is mapped to a vector of real values as shown in Figs. <ref type="figure" target="#fig_2">7</ref> and<ref type="figure">8</ref>, which consists of the arranged network free parameters. Figure <ref type="figure" target="#fig_2">7</ref> shows the transformation of the DS based network parameters (synapse time constants) into a chromosome, such as the time constants of the synapses connecting the input layer to the hidden layer form the first half of the chromosome, and the time constants of the synapses connecting the hidden layer to the output layer form the second half of the chromosome. On the other hand, Fig. <ref type="figure">8</ref> shows the transformation of the SRM-based network (or phenotype) into its corresponding genotype. In this case, the weights of the input to hidden layer and the hidden to output layer form the fist half of the chromosome, while the delays of the input to hidden layer and the hidden to output layer form the second half. A set of such vectors/chromosomes (also called individuals) will form a population whose evolution converges to a global optimum network. The ES is implemented as follows: Each population member of the ES consists of n-dimensional vectors, where n is the total number of tuneable network parameters within input, hidden and output layer. The population at any given generation g is denoted as P(g).</p><p>The implementation of the self-adaptive ES illustrated in Fig. <ref type="figure">9</ref>, is a modified version of that described in <ref type="bibr" target="#b4">[5]</ref>, and an abbreviated form is summarised below:</p><p>1. Generate the initial population of μ individuals (spiking network), and set g = 1. Each individual is taken as a pair of real-valued vectors, (xi, η i ), ∀i ∈ {1, . . . , μ}, where x i 's are objective variables representing the synaptic free parameters, and η i 's are standard deviations for mutations.</p><p>The vector x i is the genotype representation of a SNN (phenotype) obtained by applying the mapping process mentioned above. 2. Evaluate the fitness score for each individual (xi, η i ), i = 1, . . . , μ, of the population based on the error function, which is defined by Eq. ( <ref type="formula" target="#formula_12">9</ref>). Each individual (genotype) is decoded into a network (phenotype) and run with the training patterns to determine the error value 3. Each parent (x i , η i ) generates a single offspring (x i , η i ) by: for j = 1, . . . , n, where n is the total number of existing weights and delays, Fig. <ref type="figure">9</ref> Steps involved in the evolutionary strategy based learning process. Initial population is randomly created and a mapping between a genotype and its network representation (phenotype) is carried out prior to its evaluation. The mapping is shown previously in Figs. <ref type="figure" target="#fig_2">7</ref> and<ref type="figure">8</ref> x i ( j) = x i ( j) + η i ( j)N j (0, 1), or <ref type="bibr" target="#b9">(10)</ref> x i ( j) = x i ( j) + η i ( j)δ j (11) η i ( j) = η i ( j) exp(τ N(0, 1) + τ N j (0, 1)) <ref type="bibr" target="#b11">(12)</ref> x i ( j), x i ( j), η i ( j) and η i ( j) denote the j-th component of the vectors x i , x i , η i and η i , respectively. N(0, 1) denotes a normally distributed one dimensional random number with mean 0 and standard deviation 1. N j (0, 1) indicates that the random number is generated anew for each value of j. The factors τ and τ are set to [1/sqrt(2×sqrt(n))] and [1/sqrt(2×n)], δ j is a Cauchy random variable with a scale of 1, and is generated anew for each value of j. Functions ( <ref type="formula">10</ref>) and ( <ref type="formula">11</ref>) implement alternative mutation functions; any particular run will use either Gaussian mutation <ref type="bibr" target="#b9">(10)</ref> or Cauchy mutation (11). 4. Calculate the fitness of each offspring (x i , η i ), i = 1, . . . , μ. 5. Generate a new population P(g) using tournament selection and elitism to keep track of the best individual at each generation. 6. Stop if a maximum number of generations is reached (g = g max ) or a target error is met; otherwise, g = g + 1 and go to step 3.</p><p>The one dimensional Cauchy density function centred at the origin is defined by:</p><formula xml:id="formula_14">f t (x) = 1 π t t 2 + x 2 , (t &gt; 0 is a scale parameter, -∞ &lt; x &lt; ∞), with F(x) = 1 2 + 1 π arctan x t its corresponding distribution.</formula><p>The shape of the Cauchy density function resembles that of a Gaussian density function but approaches the axis so slowly that an expectation does not exist. As a result, the variance of a Cauchy distribution is infinite. Figure <ref type="figure" target="#fig_3">10</ref> shows the difference between the two distributions where it is obvious that the Cauchy function is more likely to generate a random number far away from the origin because of its long tail <ref type="bibr" target="#b17">[19]</ref>. This implies that the use of Cauchy mutation is more likely to escape from local minimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>The architectures which have been discussed are implemented in Matlab R13, and are validated on classifying non-linearly separable problems, namely the XOR problem, the IRIS and the Breast Cancer data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">The XOR problem</head><p>The Table <ref type="table" target="#tab_0">1</ref> below shows the temporal encoding of the logical values of the XOR truth table. The input logic value '0' is assigned an early firing time '0 ms' while a later firing time of '6 ms' is assigned to the input logic value '1'. In  At the output a logic value '0' is assigned a late firing time (40 ms), while a logic value of '1' is assigned an early firing time (10 ms). At the input this assignment is 0 and 6 ms, respectively order to classify both output classes represented by {0, 1}, the network free parameters (DS time constants for the dynamic based network, and the weights and delays for the SRM based network) are tuned using the ES described previously so that the output firing times converge towards an early time '10 ms' for one class 'output logic value 1', and towards a late time '40 ms' for the second class 'output logic value 0'. The obtained results show that this task is perfectly performed with both networks, but proved more time consuming for the DS based network due to the discrete integration of the differential equations underlying the behaviour of DS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">The IRIS data set</head><p>The approach is also demonstrated on the IRIS data benchmark <ref type="bibr" target="#b25">[27]</ref>, which consists of 150 feature vectors belonging to 3 groups of equal size. The groups represent three species of iris, and each of the feature vectors is described by four valued features: petal-length, petal-width, sepal-length, and sepal-width. These feature values are temporally encoded using the 1-D encoding scheme for the dynamic based network, whereas both sparse and 1-D encoding are employed for the SRM based network. Table <ref type="table" target="#tab_1">2</ref> shows a comparison of the performances obtained for both architectures together with results from an alternative SRM based network trained by an adapted BP approach, called Spikeprop <ref type="bibr" target="#b16">[18]</ref>, as opposed to our ES approach. It also shows a reduction in the dimensions of the network employed and the number of adjustable parameters. The proposed approach also offers considerable potential for a hardware implementation in terms of the required resources and the processing speed as less neurons and considerably smaller number of weights are required when compared to existing approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">The breast cancer data set</head><p>The breast cancer data<ref type="foot" target="#foot_1">1</ref> set <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b27">29]</ref> consists of 699 samples belonging to two groups, namely benign and malignant cell tissues. Each data point is described with 9 attributes, represented by an integer ranging from 1 to 10 with larger numbers indicating a greater likelihood of malignancy. The data set is split into two parts, training and test data sets with 341 and (342 + 16) samples in each set respectively. There are 16 samples with missing data, which are used for test where both the mean and mode were used to replace the missing data. However, comparable results are obtained for both approaches. For this data benchmark, only the 1-D encoding scheme is used to temporally encode the feature values. Table <ref type="table" target="#tab_2">3</ref> indicates that the DS and the SRM based approaches show comparable classification accuracies with other approaches, even when less neurons are used in the input layer. However for this slightly larger data set, both approaches proved time consuming due to the huge number of evaluations required by the ES. However, the design methodology presented still demonstrates that biologically plausible neurons, when trained with evolutionary strategies, are capable of pattern classification using temporal coding. This is believed to have a key role in information representation and processing in the brain. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>This paper has presented a methodology for designing SNNs. The detailed models underlying the behaviour of DS are shown to be capable of performing pattern classification using temporal encoding. A supervised training approach is also presented and successfully tested. This training is based on an ES, a derivative free method, used to search for the optimum parameters that underlie the learning process. The adjustable parameters are represented by real values instead of binary encoding. The experimental evaluation with different nonlinearly separable problems demonstrates that the proposed approach has proved successful in performing temporal learning and good performance is obtained when compared to previous published work. The approach, however, is time consuming when very large scale architectures are considered. This limitation is due to the iterative step by step discreet integration of the differential equations. To alleviate this problem, an event driven implementation could be used to speed up numerical simulations, and the use of a parallel hardware implementation (e.g. reconfigurable computing platforms such as FPGAs) could also be envisaged in order to make the simulation of very large scale networks feasible. The approach has been validated on static data, and its extension to dynamic problems where time is inherent in the input signals (such as speech signal, time series and video sequences) might need an adaptation in the network topology (e.g. recurrent networks) and further potential pre and post processing stages (such as feature extraction). By maintaining the feedforward topology, the network could also be used as a feature detection system, as it transforms the dynamic inputs into a spatiotemporal space, from which relevant features could be extracted and exploited in different applications.</p><p>Finally, it is worth noting that it is not the aim of this work to claim that the presented architectures outperform all the existing classical methods in classification tasks, as according to the no free lunch theorem, no method could outperform all the other methods on all data sets, but it is rather to further our understanding of the way real neurons represent, communicate, and process information, and to propose a new approach of designing and training SNNs based on both static and DSs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 a</head><label>2</label><figDesc>Fig. 2 a Time course of the three states of a dynamic synapse (X , Y , Z ) and the response of a neuron (V) connected to this depressing synapse. This dynamic synapse is injected with a regular spike train with 20 Hz (that is, an Inter Spike Interval ISI = 50 ms) (with first spike at time t = 200 ms, last spike at t = 700 ms). b Time course of a neuron (V) connected to three separate dynamic synapses represented by their states (X 1, X 2, X 3), each DS is injected with a spike train with a frequency 20 Hz and starting at different onset times (200, 350, 400, respectively). Parameters values are: U SE = 0.8, A SE = 250 pA, τ m = 40 ms, R m = 100 M , τ rec = 800 ms τ in = 3 ms</figDesc><graphic coords="4,115.88,64.74,368.55,464.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 Fig. 4</head><label>34</label><figDesc>Fig. 3 Network architecture: input neurons are sources of spike trains, connected through dynamic synapses to other postsynaptic neurons in the next layers. Each dynamic synapse causes a change in the postsynaptic potential and the receiving neuron merely integrates the different changes caused by different dynamic synapse. The input spike trains are transformed into a spatio-temporal structure to be learned by the output neuron using the time of its maximum response</figDesc><graphic coords="5,46.27,64.31,240.99,170.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 7</head><label>7</label><figDesc>Fig. 7 Dynamic synapse based network mapping between a spiking neural network (phenotype) and its genomic representation (genotype) which represent a single individual in an evolved population, IH Input to Hidden layer; HO Hidden to Output layer</figDesc><graphic coords="6,307.24,405.40,240.99,96.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 10</head><label>10</label><figDesc>Fig. 10 Gaussian and Cachy distribution</figDesc><graphic coords="8,51.51,65.19,242.07,168.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="7,81.27,360.52,170.19,312.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Temporal encoding of the XOR problem</figDesc><table><row><cell cols="2">Logic values</cell><cell></cell><cell cols="2">Temporal codes (ms)</cell><cell></cell></row><row><cell>Input 1</cell><cell>Input 2</cell><cell>Output</cell><cell>Input 1</cell><cell>Input 2</cell><cell>Output</cell></row><row><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>4 0</cell></row><row><cell>0</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>6</cell><cell>1 0</cell></row><row><cell>1</cell><cell>0</cell><cell>1</cell><cell>6</cell><cell>0</cell><cell>1 0</cell></row><row><cell>1</cell><cell>1</cell><cell>0</cell><cell>6</cell><cell>6</cell><cell>4 0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Comparison of the performance for the range of different approaches for the IRIS data set The table lists the dimensions of each network architecture (Net), and the training (train) and test classification accuracies. Four receptive fields neurons are used with the SRM based network (sparse coding), which explains the 16 input neurons. For the last three approaches 50 neurons are used in the input layer to provide an exact comparison with the SpikeProp approach which employed 12 receptive field neurons (12 × 4 + 2 = 50) with extra two input neurons</figDesc><table><row><cell cols="2">IRIS data set</cell></row><row><cell>Net</cell><cell>Train (%) Test (%)</cell></row></table><note><p>comparison with classical MLP networks. The results demonstrate comparable accuracy with other approaches, a</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Comparison of the performance obtained from different approaches for the Breast Cancer data set Network architecture, training and test classification accuracy are shown. Only the 1-D encoding scheme is applied, which maintains the data dimensionality at the input layer. Alternative approaches used receptive fields to encode the data resulting in increased neuron numbers in the input layer</figDesc><table><row><cell></cell><cell cols="3">BREAST CANCER data set</cell></row><row><cell></cell><cell>Net</cell><cell>Train (%)</cell><cell>Test (%)</cell></row><row><cell>Dynamic synapse based SNN</cell><cell>9×6×1</cell><cell>97.2</cell><cell>97.3</cell></row><row><cell>SRM based SNN</cell><cell>9×6×1</cell><cell>97.2</cell><cell>98.2</cell></row><row><cell>Matlab BP</cell><cell>64×15×2</cell><cell>98.5</cell><cell>96.9</cell></row><row><cell>Matlab LM</cell><cell>64×15×3</cell><cell>98.0</cell><cell>97.3</cell></row><row><cell>SpikeProp [18]</cell><cell>64×15×3</cell><cell>97.8</cell><cell>97.6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_0"><p>Spike timing encoding schemesSpike timing encoding is the process of transforming measurements of sensory inputs into a spike train representation, which is the form of input a spiking neuron can handle. Thus the multidimensional raw data, which consists of real values, needs to be mapped into a temporal space before being fed to the network. In this work, two encoding schemes are used, namely sparse coding, employed in<ref type="bibr" target="#b16">[18]</ref> and a proposed alternative scheme, named '1-D encoding' whose performance is compared to the existing one. In sparse coding, a real value is encoded by an array of receptive</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>ftp://www.ftp.ics.uci.edu/pub/machine-learning-databases/breastcancer-wisconsin/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors acknowledge the financial and technical support of the SenseMaker project (IST-2001-34712) which is funded by the EU under the FET Life-Like Perception initiative.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Time structure of the activity in neural network models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gerstner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys Rev E</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="738" to="758" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Gerstner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kistler</surname></persName>
		</author>
		<title level="m">Spiking neuron models: single neurons, populations, plasticity</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pulsed neural networks</title>
		<editor>Maass W, Bishop CM</editor>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pattern recognition computation using action potential timing for stimulus representation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hopfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">376</biblScope>
			<biblScope unit="page" from="33" to="36" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Networks of spiking neurons: the third generation of neural network models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1659" to="1671" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speed of processing in the human visual system</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fize</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Marlot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="page" from="520" to="522" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spike-based strategies for rapid processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Delorme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Rullen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="715" to="725" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rate coding versus temporal order coding: a theoretical approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gautrais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thorpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioSystems</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="57" to="65" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural networks with dynamic synapses</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tsodyks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pawelzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Markram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="821" to="835" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Coding of temporal information by activity dependent synapses</title>
		<author>
			<persName><forename type="first">G</forename><surname>Fuhrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Segev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Markram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tsodyks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Neurophysiol</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="140" to="148" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Redistribution of synaptic efficacy between pyramidal neurons</title>
		<author>
			<persName><forename type="first">H</forename><surname>Markram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tsodyks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">382</biblScope>
			<biblScope unit="page" from="807" to="810" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Coincidence detection with dynamic synapses</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Netw Comput Neural Syst</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="34" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic synapses in the cortex</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dobrunz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Using dynamic synapse based neural networks with wavelet preprocessing for speech applications, Neural Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dibazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Dibazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Namarvar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international joint conference</title>
		<meeting>the international joint conference</meeting>
		<imprint>
			<date type="published" when="2003-07-20">2003. July 2003 15. 2003. July 20-24, 2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3146" to="3150" />
		</imprint>
	</monogr>
	<note>Proceedings of the international joint conference</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An architecture for feature detection utilizing dynamic synapses, Circuits and Systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Heittmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Ramacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MWSCAS &apos;04. The 2004 47th Midwest symposium on</title>
		<imprint>
			<date type="published" when="2004-07">2004. 2004. July 2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="373" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Competitive hebbian learning through spike-timing-dependent synaptic plasticity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Neurosci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="919" to="926" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">SpikeProp: errorbackpropagation for networks of spiking neurons</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Bohte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">La</forename><surname>Poutré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000. 2000</date>
			<biblScope unit="page" from="419" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Evolutionary programming made faster</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Evol Comput</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="82" to="102" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spike-timing-dependent Hebbian plasticity as temporal difference learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="2221" to="2237" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A quantitative description of membrane current and its application to conduction and excitation in nerve</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Hodgkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Huxley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Physiol</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="500" to="544" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pattern recognition with spiking neural networks and dynamic synapses</title>
		<author>
			<persName><forename type="first">A</forename><surname>Belatreche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Maguire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcginnity</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied computational intelligence</title>
		<imprint>
			<biblScope unit="page" from="205" to="210" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>world scientific, proceedings of the 6th international FLINS conference</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An overview of evolutionary algorithms for parameter optimization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bäck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H-P</forename><surname>Schwefel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evol Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Genetic algorithms in search, optimization, and machine learning</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Addison Wesley</publisher>
			<pubPlace>Reading</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adaptation in natural and artificial systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Holland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>University of Michigan Press</publisher>
			<pubPlace>Ann Arbor</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Toward a theory of evolution strategies: self-adaptation</title>
		<author>
			<persName><forename type="first">H-G</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evol Comput</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="311" to="347" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The use of multiple measurements in taxonomic problems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann Eugen</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="179" to="188" />
			<date type="published" when="1936">1936</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multisurface method of pattern separation for medical diagnosis applied to breast cytology</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Woldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">L</forename><surname>Mangasarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc Natl Acad Sci</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="9193" to="9196" />
			<date type="published" when="1990">1990</date>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cancer diagnosis via linear programming</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">L</forename><surname>Mangasarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Wolberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM News</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An evolutionary strategy for supervised training of biologically plausible neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Belatreche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Maguire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcginnity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The sixth international conference on computational intelligence and natural computing (CINC), proceedings of the 7th joint conference on information sciences</title>
		<meeting><address><addrLine>North Carolina, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1524" to="1527" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
