<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sarcasm Detection on Twitter: A Behavioral Modeling Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ashwin</forename><surname>Rajadesingan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Reza</forename><surname>Zafarani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
							<email>huan.liu@asu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sarcasm Detection on Twitter: A Behavioral Modeling Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B3818018EE5DAD356EA2F68687268E05</idno>
					<idno type="DOI">10.1145/2684822.2685316</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.2.8 [Database Management]: Database Applications-Data mining Sarcasm Detection</term>
					<term>Behavioral Modeling</term>
					<term>Social Media</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sarcasm is a nuanced form of language in which individuals state the opposite of what is implied. With this intentional ambiguity, sarcasm detection has always been a challenging task, even for humans. Current approaches to automatic sarcasm detection rely primarily on lexical and linguistic cues. This paper aims to address the difficult task of sarcasm detection on Twitter by leveraging behavioral traits intrinsic to users expressing sarcasm. We identify such traits using the user's past tweets. We employ theories from behavioral and psychological studies to construct a behavioral modeling framework tuned for detecting sarcasm. We evaluate our framework and demonstrate its efficiency in identifying sarcastic tweets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In recent years, social media sites such as Twitter have gained immense popularity and importance. These sites have evolved into large ecosystems where users express their ideas and opinions uninhibitedly. Companies leverage this unique ecosystem to tap into public opinion on their products or services and to provide real-time customer assistance. Not surprisingly, most large companies have a social media presence and a dedicated team for marketing, after-sales service, and consumer assistance through social media.</p><p>With the high velocity and volume of social media data, companies rely on tools such as HootSuite 1 , to analyze data and to provide customer service. These tools perform tasks such as content management, sentiment analysis, and extraction of relevant messages for the company's customer service representatives to respond to. However, these tools lack the sophistication to decipher more nuanced forms of language such as sarcasm or humor, in which the meaning of a message is not always obvious and explicit. This imposes an extra burden on the social media team -already inundated with customer messages -to identify these messages and respond appropriately. Table <ref type="table" target="#tab_0">1</ref> provides two examples where the customer service representatives fail to detect sarcasm. Such public gaffes not only upset the already disgruntled customers but also ruin the public images of companies.</p><p>Our goal in this study is to tackle the difficult problem of sarcasm detection on Twitter. While sarcasm detection is inherently challenging, the style and nature of content on Twitter further complicate the process. Compared to other, more conventional sources such as news articles and novels, Twitter is (1) more informal in nature with an evolving vocabulary of slang words and abbreviations and (2) has a limit of 140 characters per tweet which provides fewer word-level cues and adds more ambiguity.</p><p>Current research on sarcasm detection on Twitter <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b29">29]</ref> has primarily focused on obtaining information from the text of the tweets. These techniques treat sarcasm as a linguistic phenomenon, with limited emphasis on the psychological aspects of sarcasm. However, sarcasm has been extensively studied in psychological and behavioral sciences and theories explaining when, why, and how sarcasm is expressed have been established. These theories can be extended and employed to automatically detect sarcasm on Twitter. For example, Rockwell <ref type="bibr" target="#b32">[32]</ref> identified a positive correlation between cognitive complexity and the ability to produce sarcasm. A high cognitive complexity of an individual can be manifested on Twitter in terms of the language complexity of the tweets.</p><p>Hence, to follow a systematic approach, we first theorize the core forms of sarcasm using existing psychological and behavioral studies. Next, we develop computational features to capture these forms of sarcasm using user's current and past tweets. Finally, we combine these features to train a learning algorithm to detect sarcasm. We make the following contributions in this paper:</p><p>1. We identify different forms of sarcasm and demonstrate how these forms are manifested on Twitter.</p><p>2. We introduce behavioral modeling as a new, effective approach for detecting sarcasm on Twitter; we pro- 3. We investigate and demonstrate the importance of historical information available from past tweets for sarcasm detection.</p><p>In section 2, we review related sarcasm detection research. In section 3, we formally define sarcasm detection on Twitter. In section 4, we discuss different forms of sarcasm and outline SCUBA, our behavioral modeling framework for detecting sarcasm. In section 5, we demonstrate how different forms of sarcasm can be identified within Twitter. In section 6, we detail our experiments. Section 7 concludes this research with directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Sarcasm has been widely studied by psychologists, behavioral scientists and linguists for many years. Theories explaining the cognitive processes behind sarcasm usage such as the echoic reminder theory <ref type="bibr" target="#b18">[18]</ref>, allusional pretense theory <ref type="bibr" target="#b19">[19]</ref>, and implicit display theory <ref type="bibr" target="#b39">[39]</ref> have been extensively researched. However, automatic detection of sarcasm is a relatively unexplored research topic and a challenging problem <ref type="bibr" target="#b25">[25]</ref>. While studies on automatic detection of sarcasm in speech <ref type="bibr" target="#b35">[35]</ref> utilizes prosodic, spectral and contextual features, sarcasm detection in text has relied on identifying text patterns <ref type="bibr" target="#b4">[4]</ref> and lexical features <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b17">17]</ref>.</p><p>Davidov et al. <ref type="bibr" target="#b4">[4]</ref> devised a semi-supervised technique to detect sarcasm in Amazon product reviews and tweets. They used interesting pattern-based (high frequency words and content words) and punctuation-based features to build a weighted k-nearest neighbor classification model to perform sarcasm detection. Reyes et al. <ref type="bibr" target="#b28">[28]</ref> focused on developing classifiers to detect verbal irony based on ambiguity, polarity, unexpectedness and emotional cues derived from text. González-Ibáñez et al. <ref type="bibr" target="#b10">[10]</ref> introduced a sarcasm detection technique using numerous lexical features (derived from LWIC <ref type="bibr" target="#b27">[27]</ref> and Wordnet Affect <ref type="bibr" target="#b34">[34]</ref>) and pragmatic features such as emoticons and replies. Liebrecht et al. <ref type="bibr" target="#b20">[20]</ref> used unigrams, bigrams and trigrams as features to detect sarcastic Dutch tweets using a balanced winnow classifier. More recently, Riloff et al. <ref type="bibr" target="#b29">[29]</ref>, used a well-constructed lexiconbased approach to detect sarcasm based on an assumption that sarcastic tweets are a contrast between a positive sentiment and a negative situation.</p><p>As described, past studies on sarcasm detection have primarily focused on linguistic aspects of sarcasm and used only the text of the tweet. We introduce a systematic approach for effective sarcasm detection by not only analyzing the content of the tweets but by also exploiting the behavioral traits of users derived from their past activities. We map research on <ref type="bibr" target="#b1">(1)</ref> what causes people to use sarcasm?, (2) when is sarcasm used? and (3) how is sarcasm used?, to observable user behavioral patterns on Twitter that can help build a comprehensive supervised framework to detect sarcasm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROBLEM STATEMENT</head><p>Sarcasm, while similar to irony, differs in that it is usually viewed as being caustic and derisive. Some researchers even consider it to be aggressive humor <ref type="bibr" target="#b1">[1]</ref> and a form of verbal aggression <ref type="bibr" target="#b37">[37]</ref>. While researchers in linguistics and psychology debate about what exactly constitutes sarcasm, for the sake of clarity, we use the Oxford dictionary's definition of sarcasm<ref type="foot" target="#foot_0">2</ref> as "a way of using words that are the opposite of what you mean in order to be unpleasant to somebody or to make fun of them." We formally define the sarcasm detection problem on Twitter as follows:</p><p>Definition. Sarcasm Detection on Twitter. Given an unlabeled tweet t from user U along with a set of U's past tweets T, a solution to sarcasm detection aims to automatically detect if t is sarcastic or not.</p><p>In addition to following a behavioral modeling approach, our problem is different from past sarcasm detection research which use only text information from t and do not consider the user's past tweets T that are available on Twitter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SCUBA: BEHAVIORAL MODELING FRAMEWORK</head><p>Sarcastic tweets are not always created in isolation. When posting sarcastic tweets, users make conscious efforts to express their thoughts through sarcasm. They may decide to use sarcasm as a behavioral response to a certain situation, observation, or emotion. These situations, observations, or emotions may be observed and analyzed on Twitter.</p><p>It is observed that some individuals have more difficulty in creating or recognizing sarcasm than others due to cul-tural differences, language barriers, and the like. In contrast, some individuals have a higher propensity to use sarcasm than others. Hence, SCUBA also considers the user's likelihood of being a sarcastic person. This can be achieved on Twitter by analyzing the user's past tweets. Using existing research on sarcasm and our observations on Twitter, we find that sarcasm generation can be characterized as one (or a combination) of the following:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sarcasm as a contrast of sentiments</head><p>A popular perception of sarcasm among researchers is that sarcasm is a contrast of sentiments. A classical view of sarcasm, based on the traditional pragmatic model <ref type="bibr" target="#b11">[11]</ref>, argues that sarcastic utterances are first processed in the literal sense and if the literal sense is found incompatible with the present context, only then is the sentence processed in its opposite (ironic) form. This perceived contrast may be expressed with respect to mood, affect or sentiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sarcasm as a complex form of expression</head><p>Rockwell <ref type="bibr" target="#b32">[32]</ref> showed that there is a small but significant correlation between cognitive complexity and the ability to produce sarcasm. A high cognitive complexity involves understanding and taking into account, multiple perspectives to make cogent decisions. Furthermore, expressing sarcasm requires determining if the environment is suitable for sarcasm, creating an appropriate sarcastic phrase and assessing if the receiver would be capable of recognizing sarcasm. Therefore, sarcasm is a complex form of expression needing more effort than usual from the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sarcasm as a means of conveying emotion</head><p>Sarcasm is primarily a form of conveying one's emotions. While sarcasm is sometime interpreted as aggressive humor <ref type="bibr" target="#b1">[1]</ref> or verbal aggression <ref type="bibr" target="#b37">[37]</ref>, it also functions as a tool for self expression. Past studies <ref type="bibr" target="#b12">[12]</ref>, recognize that sarcasm is usually expressed in situations with negative emotions and attitudes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sarcasm as a possible function of familiarity</head><p>Friends and relatives are found to be better at recognizing sarcasm than strangers <ref type="bibr" target="#b31">[31]</ref>. Further, it has been demonstrated that the knowledge of language <ref type="bibr" target="#b3">[3]</ref> and culture <ref type="bibr" target="#b33">[33]</ref> also play an important role in the recognition and usage of sarcasm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sarcasm as a form of written expression</head><p>Sarcasm in psychology has been studied primarily as a spoken form of expression. However, sarcasm is quite prevalent in written form as well, especially with the advent of online social networking sites. Through time, users have become more adept at conveying sarcasm in writing by including subtle markers that indicate to the unassuming reader, that the phrase might be sarcastic. For example, while "you're so smart" does not hint at sarcasm, "Woowwww you are SOOOO cool " 3 elicits some doubts about the statement's sincerity. 3 An original tweet collected.</p><p>We believe that when expressing sarcasm, the user would invariably exhibit one or more of these forms. Therefore, SCUBA incorporates a behavioral modeling approach <ref type="bibr" target="#b42">[42]</ref> for sarcasm detection that utilizes features which capture the different forms of sarcasm. These extracted features are utilized in a supervised learning framework along with some labeled data to determine if the tweet is sarcastic or not. In our setting, labeled data is a set of tweets, among which sarcastic tweets are known. As the novelty of the approach lies in the behavioral modeling and not the actual classifier, we explain in detail how sarcasm can be modeled and incorporated into SCUBA in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">SCUBA: REPRESENTING FORMS OF SARCASM</head><p>Users' efforts in generating sarcasm are manifested in many ways on Twitter. In this section, we describe how the aforementioned forms are realized on Twitter and how one can construct relevant features to capture these form in the context of Twitter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Sarcasm as a contrast of sentiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Contrasting connotations</head><p>A common means of expressing sarcasm is to use words with contrasting connotations within the same tweet. For example, in I love getting spam emails!, spam obviously has a negative connotation while love is overwhelmingly positive. To model such occurrences, we construct features based on (1) affect and (2) sentiment scores.</p><p>We obtain affect score of words from a dataset compiled by Warriner et al. <ref type="bibr" target="#b41">[41]</ref>. This dataset contains affect (valence) scores for 13,915 English lemmas which are on a 9-point scale, with 1 being the least pleasant.</p><p>The sentiment score is calculated using SentiStrength <ref type="bibr" target="#b36">[36]</ref>. SentiStrength is a lexicon-based tool optimized for tweet sentiment detection based on sentiments of individual words in the tweet. Apart from providing a ternary sentiment result {positive, negative, neutral} for the whole tweet, Sen-tiStrength outputs two scores for each word. A negative sentiment score from -1 to -5 (not-negative to extremelynegative) and a positive sentiment score from 1 to 5 (notpositive to extremely-positive). Here, we use SentiStrength's lexicon to obtain word sentiment scores. From these sentiment and affect scores, we compute the following:</p><formula xml:id="formula_0">A = { affect(w) | w t},<label>(1)</label></formula><formula xml:id="formula_1">S = { sentiment(w) | w t},<label>(2)</label></formula><formula xml:id="formula_2">∆ affect = max(A) -min(A),<label>(3)</label></formula><formula xml:id="formula_3">∆ sentiment = max(S) -min(S), (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where t is the tweet and w is a word in t. The affect(w) outputs the affect score of word w. The sentiment(w) outputs the sentiment score of word w. ∆ affect and ∆ sentiment indicate the level of contrast in terms of affect and sentiment infused into the tweet by the user. We use ∆ affect and ∆ sentiment as features (2 features).</p><p>SentiStrength and the dataset provided by Warriner et al. <ref type="bibr" target="#b41">[41]</ref> can only provide sentiment and affect scores for unigrams. Hence, we construct a lexicon of positive and negative sentiment bigrams and trigrams used on Twitter following an approach similar to Kouloumpis et al. <ref type="bibr" target="#b16">[16]</ref> as follows:</p><p>1. We collect about 400,000 tweets with positive sentiment hashtags such as #love, #happy, #amazing, etc., and 400,000 tweets with negative sentiment hashtags such as #sad, #depressed, #hate, among others.</p><p>2. From these tweets, we extracted the bigrams and trigrams along with their respective frequencies. We filter out bigrams and trigrams with frequencies less than 10. We filter out bigrams or trigrams with sentiment scores ∈ (-0.1, 0.1). This sentiment measure is similar to association scores given by Liu et al. <ref type="bibr" target="#b21">[21]</ref>.</p><p>Using the generated lexicon, we include as features, the number of n-grams with positive sentiment scores, the number of n-grams with negative sentiment scores, the summation of scores for positive n-grams, and the summation of scores for negative n-grams (4 features).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Contrasting present with the past</head><p>Sometimes, the user may set up a contrasting context in her previous tweet and then, choose to use a sarcastic remark in her current tweet. To model such behavior, we obtain the sentiment expressed by the user (i.e., positive, negative, neutral) in the previous tweet and the current tweet using Sen-tiStrength. Then, we include the type of sentiment transition taking place from the past tweet to the current tweet (for example, positive → negative, negative → positive) as a feature (1 feature). In total, there are nine such transitions involving the combinations of positive, negative and neutral sentiments. To provide a historical perspective on the user's likelihood for such sentiment transitions, we compute the probability for all nine transitions using the user's past tweets. The transition probabilities along with the probability of the current transition are included as features in our framework (10 features).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Sarcasm as a complex form of expression</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Readability</head><p>As sarcasm is widely acknowledged to be hard to read and understand, we adapt standardized readability tests to measure the degree of complexity and understandability of the tweet. We use as features: number of words, number of syllables, number of syllables per word in the tweet derived from the Flesch-Kincaid Grade Level Formula <ref type="bibr">[8]</ref>, number of polysyllables 4 and the number of polysyllables per word in the tweet derived from SMOG <ref type="bibr" target="#b22">[22]</ref> (6 features).</p><p>Inspired by the average word length feature used in the Automated Readability Index <ref type="bibr" target="#b15">[15]</ref>, we formulate a more comprehensive set of features using the word length distribution L = {li} 19  i=1 constructed from tweet t as follows: 1. For each word w in t, we compute its character length |w|. For convenience, we ignore words of length 20 or more. We construct a word length distribution L = {li} 19 i=1 for t, where li denotes the number of words in the tweet with character length i. 4 Polysyllables are words containing three or more syllables.</p><p>2. L may be represented succinctly using the following 6-tuple presentation:</p><formula xml:id="formula_5">&lt; E[lw], med[lw], mode[lw], σ[lw], min w∈t lw, max w∈t lw &gt;,<label>(5)</label></formula><p>where E is the mean, med is the median, mode is the mode and σ is the standard deviation.</p><p>We include the 6-tuple representation as features in our framework (6 features).</p><p>Further, given the availability of the user's past tweets, we examine if there is a noticeable difference in the word length distribution between the user's current tweet and her past tweets. It must be noted that while sarcastic tweets may also be present in the user's past tweets, because of their relative rarity, the past tweets when taken in entirety, would 'average out' any influence possibly introduced by a few past sarcastic tweets. Therefore, any difference from the norm in the word length distribution of the current tweet can be captured. To capture differences in word length distribution, we perform the following steps:</p><p>1. From the user's current tweet, we construct a probability distribution D1 over length of words in the tweet.</p><p>2. From the user's past tweets, we construct a probability distribution D2 over length of words in all the past tweets.</p><p>3. To calculate the difference between the world length distribution of the current tweet and the past tweets, we calculate the Jensen-Shannon (JS) divergence between D1 and D2:</p><formula xml:id="formula_6">JS(D1||D2) = 1 2 KL(D1||M ) + 1 2 KL(D2||M ),<label>(6)</label></formula><p>where</p><formula xml:id="formula_7">M = D 1 +D 2 2</formula><p>and KL is the KL-divergence:</p><formula xml:id="formula_8">KL(T1||T2) = i ln( T 1 (i)</formula><p>T 2 (i) )T1(i). We include the JS-divergence value as a feature (1 feature).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Sarcasm as a means of conveying emotion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Mood</head><p>Mood represents the user's state of emotion. Intuitively, the mood of the user may be indicative of her propensity to use sarcasm; if the user is in a bad (negative) mood, she may choose to express it in the form of a sarcastic tweet. Therefore, we gauge the user's mood using sentiment expressed in her past tweets. However, we cannot assume that the user's mood is encapsulated in her last n tweets. Therefore, we capture the mood using her past tweets as follows:</p><p>1. For each past tweet t, we compute its positive sentiment score pos(t) and its absolute negative sentiment score neg(t) using SentiStrength.</p><p>2. We divide the user's past tweets into overlapping buckets based on the number of tweets posted prior to the current tweet.</p><p>3. Each bucket bn consists of the previous n tweets posted by the user. We select n ∈ {1, 2, 5, 10, 20, 40, 80}.</p><p>4. In each bn, we capture the user's perceived mood using two tuples. The first tuple consists of four features:</p><formula xml:id="formula_9">&lt; + , - , P, max ( + , - ) &gt;,<label>(7)</label></formula><p>where + and -are the total positive and negative sentiments in bn: + = t∈bn pos(t), -= t∈bn neg(t).</p><p>P is either + or -. P = +, when + &gt; -, and P = -, otherwise. The second tuple consists of six features:</p><formula xml:id="formula_10">&lt; n+, n-, n0, n, Q, max (n+, n-, n0) &gt;, (<label>8</label></formula><formula xml:id="formula_11">)</formula><p>where n+ is the number of positive tweets, n-is the number of negative tweets, and n0 is the number of neutral tweets present in bn(found using SentiStrength). n is the total tweets present in bn and Q indicates what the majority of tweets are, i.e., Q ∈ {+, -, 0}. For example, Q = +, when n+ = max (n+, n-, n0). We include both tuples for each bn as features in SCUBA (7 × (4 + 6) = 70 features).</p><p>As one's mood remains constant for a limited amount of time, we also gauge the user's mood within a specific time window. However, we cannot assume that the user's mood is encapsulated within t minutes. Therefore, we divide the user's past tweets into buckets bt, which consists of all the tweets posted by the user within t minutes from the current tweet. Here, t ∈ {1, 2, 5, 10, 20, 60, 720, 1440} minutes (1440 minutes = 1 day). For each bucket bt, we include the tuples in ( <ref type="formula" target="#formula_9">7</ref>) and ( <ref type="formula" target="#formula_10">8</ref>) also as features (8 × 10 = 80 features).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Affect and sentiment</head><p>As sarcasm is a combination of affect and sentiment expression, we examine how affect and sentiment are expressed in sarcastic tweets. To this end, we construct a sentiment score distribution SD. SD consists of 11 values, each value being the number of words in the tweet with sentiment score i, where i ∈ [-5, 5]. We also construct an affect score distribution AD. AD contains 9 values. Each value is the number of words in the tweet with an affect score j, where j ∈ <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b9">9]</ref>. We normalize counts in SD and AD. We include both distributions as features (11+9=20 features). Similar to Eq. ( <ref type="formula" target="#formula_5">5</ref>), we represent these distributions as 6-tuples and include them as features (12 features). We also include the number of affect words, number of sentiment words, and the tweet's overall sentiment (positive, negative, or neutral) as features in SCUBA (3 features).</p><p>To capture differences in sentiment expression in sarcastic tweets versus non-sarcastic ones, we compare the sentiment score distribution of the user's past tweets to that of her current tweet. Following a procedure similar to that of Section 5.2.1, we calculate the JS-divergence value between the past and current sentiment score distributions and include it as a feature (1 feature).</p><p>Finally, to gain insights into how a user employs Twitter to express emotion, we determine the range of sentiments expressed by the user in the past. To perform this, for all sentiment scores i ∈ [-5, 5], we compute the number of tweets with sentiment score i in all past tweets of the user. By normalizing these counts, we obtain a probability distribution over sentiment scores in past tweets. We include this probability distribution as a feature (11 features).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Frustration</head><p>When individuals experience unjust situations, they sometimes turn to social media as an effective outlet for their complaints and frustrations <ref type="bibr" target="#b2">[2]</ref>. These frustrations are often expressed in the form of sarcasm <ref type="bibr" target="#b9">[9]</ref> (see example tweets in Table <ref type="table" target="#tab_0">1</ref>). We hypothesize that as users encounter unpleasant situations in real life, they react spontaneously by posting tweets to vent out their frustration. Therefore, they diverge from their regular tweeting patterns.</p><p>To capture this behavioral pattern, using the user's past tweets, we construct an expected tweet posting time probability distribution. From each of the user's past tweets, we extract the tweet creation time, using which, we build a normalized 24 bin distribution T D (one for each hour). T D approximates the probability of the user tweeting at each hour. For each tweet, using the T D for the user posting it, we find the likelihood of the user posting the tweet at that hour. The lower the likelihood, the more divergent the tweet is from the user's usual tweeting patterns. Low likelihood scores indicate that the user is not expected to tweet at that particular time and that the user has gone out of her way to tweet at that time, therefore, in some sense, the tweet is spontaneous in nature. We include the likelihood of the user tweeting at that particular hour as a feature (1 feature).</p><p>We also observe that users tend to post successive tweets in short quick bursts when they vent out their frustrations; therefore, we include the time difference between the examined tweet and the previous tweet posted by the user as a feature (1 feature). Finally, a common way to express frustration is by using swear words. Using the list of most common swear words provided by Wang et al. <ref type="bibr" target="#b40">[40]</ref>, we check for the presence of such words in the tweet and include their presence using a boolean feature (1 feature).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Sarcasm as a possible function of familiarity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Familiarity of language</head><p>Intuitively, one would expect a user who uses a form of language as complex as sarcasm to have good command over the language. Therefore, we measure the user's language skills with features that are inspired by standardized language proficiency cloze tests. In cloze tests, proficiency is evaluated based on vocabulary, grammar, dictation, and reading levels <ref type="bibr" target="#b23">[23]</ref>. As dictation and reading levels pertain to the oratory and reading skills of the user that cannot be measured from written text, we focus on constructing features that best represent the vocabulary and grammar skills.</p><p>Vocabulary Skills. We determine the size of user's vocabulary from user's past tweets. We include as features, the total number of words, total number of distinct words used and the ratio of distinct words to total words used to measure the user's redundancy in word usage (3 features).</p><p>Grammar Skills. To measure grammar skills, we investigate how the user employs different parts-of-speech (POS). The POS tags for words in the tweets are generated using TweetNLP's <ref type="bibr" target="#b24">[24]</ref> POS tagger. TweetNLP generates 25 possible tags such as interjections or emoticons. We obtain the POS tag for every word in the tweet and build a corresponding POS probability distribution and include it as features (25 features). As English grammar is intricate and nuanced, it is difficult to extensively measure a user's grammar exper-tise. However, one can check for correct grammatical usage for commonly used words. We check the correct grammatical usage for "your" and "its", both frequently used. We observe that users often mistakenly use words such as "your" instead of "you're' and "its" instead of "it's". Using all past tweets of a user, we obtain the POS of the word used immediately after "your" and "its". If the word has been used in the correct grammatical sense, the POS of the succeeding word should not be a verb (example, "your doing great!" is incorrect), adverb (example, "its freakin' amazing" is incorrect) or a determiner (such as "a" or "the"). We include as features, the fraction of times the two words were used in incorrect grammatical form by the user (2 features). There are other POS that can render the usage of "your" or "its" incorrect; however, for correctness, we adopt a conservative strategy by checking only for verbs, adverbs, and determiners.</p><p>Sarcastic users, in addition to their vocabulary and grammar skills, are familiar with sarcasm as an expression form.</p><p>Familiarity with Sarcasm. For measuring user's familiarity with sarcasm, we include the number of past occurrences of #not or #sarcasm hashtags as a feature (1 feature). Further, it has been shown that people in different regions perceive and use sarcasm differently (see Dress et al.'s study <ref type="bibr" target="#b5">[5]</ref>). Thus, we try to infer the location of the user. However, the user provided location on Twitter is often noisy as it is a free-text field in which any text may be inputted. Therefore, we approximate the user's location with her time zone and include it as a feature (1 feature).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Familiarity of environment</head><p>Users express sarcasm better when they are well acquainted with their environment. Just as people are less likely to use sarcasm at a new, unfamiliar setting, users take time to get familiar with Twitter before posting sarcastic tweets. We measure a user's familiarity with Twitter in terms of her usage familiarity, parlance familiarity, and social activity.</p><p>Usage Familiarity. We measure usage familiarity using the number of tweets posted, number of days on Twitter (i.e., Twitter age), and the average number of daily tweets and include all as features (3 features). These features provide indications of the duration and the intensity at which the user has been using Twitter. We also measure familiarity in terms of the user's frequency of Twitter usage. From the user's past tweets, we compute the time differences between all pairs of successive tweets. We represent the distribution of these time differences as a 6-tuple similar to Eq. ( <ref type="formula" target="#formula_5">5</ref>), and include them as features (6 features).</p><p>Twitter Parlance Familiarity. To capture user familiarity with Twitter parlance, we include the number of retweets, mentions and hashtags used in past tweets as features (3 features). Experienced Twitter users often use shortened words (by removing vowels, using numbers, etc.) to circumvent the 140 character limit. Hence, we include the presence of alphanumeric words (boolean), presence of words without vowels (boolean), as well as the percentage of dictionary words present in the tweet as features (3 features).</p><p>Social Familiarity. We measure social familiarity by identifying how embedded a user is in Twitter's social graph. Hence, we include the number of friends and followers as features (2 features). To adjust for longevity, we divide the numbers of friends and followers by the user's Twitter age and include them as features (2 features).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Sarcasm as a form of written expression</head><p>While low pitch, high intensity and a slow tempo <ref type="bibr" target="#b30">[30]</ref> are vocal indicators of sarcasm, users attempting to express sarcasm in writing are devoid of such devices. Therefore, users may be forced to use certain writing styles to compensate for the lack of visual or verbal cues. We categorize such behavior into prosodic and structural variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Prosodic variations</head><p>Users often repeat letters in words to stress and overemphasize certain parts of the tweet (for example, sooooo, awesomeeee) to indicate that they mean the opposite of what is written. We capture such usage by including as boolean features, the presence of repeated characters (3 or more) and the presence of repeated characters (3 or more) in sentimentloaded words (such as, loveeee) (2 features). We also include the number of characters used, and the ratio of the number of distinct characters to the total characters used in the tweet as features (2 features).</p><p>We also observe that users often capitalize certain words to emphasize changes in tone (if the tweet were to be read out loud). We account for such changes by including the number of capitalized words in the tweet as a feature (1 feature). Other users capitalize certain parts-of-speech (POS) to exaggerate or to vent their frustration. Using TweetNLP, we obtain the POS tag for each capitalized word in the tweet. Then, we compute the probability distribution of POS tags for capitalized words and include it as features (25 features).</p><p>Users also use certain punctuations to express non-verbal cues that are crucial for sarcasm deliverance in speech. For example, users use "*" to indicate emphasis, "..." to indicate pause, "!!!" for exclamations (sometimes over-done to indicate sarcasm). Thus, we include as features, the normalized distribution of common punctuation marks (.,!?'*") (7 features). To compare the user's current usage of punctuations to her past usage, similar to Eq. ( <ref type="formula" target="#formula_6">6</ref>), we calculate the JSdivergence value between the current and past punctuation distribution and include it as a feature (1 feature).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Structural variations</head><p>We observe that sarcastic tweets sometimes have a certain structure wherein the user's views are expressed in the first few words of the tweet, while in the later parts, a description of a particular scenario is put forth, e.g., I love it when my friends ignore me. To capture possible syntactic idiosyncrasies arising from such tweet construction, we use as features, the POS tags of the first three words and the last three words in the tweet (6 features). We also include the position of the first sentiment-loaded word (0 if not present) and the first affect-loaded word (0 if not present) as a feature (2 features). Given the structure followed in constructing sarcastic tweets, we also check for positional variations in the hashtags present in the tweet. We trisect the tweet based on the number of words present and include as features the number of hashtags present in the each of the three parts of the tweet (3 features).</p><p>To capture differences in syntactic structures, we examine parts of speech tags present in the tweet. Similar to Eq. ( <ref type="formula" target="#formula_6">6</ref>), we construct a probability distribution over the POS tags present in the current tweet as well as POS tags in past tweets and include the Jensen-Shannon divergence value between the two distribution as a feature (1 feature).</p><p>Past studies on quantifying linguistic style <ref type="bibr" target="#b14">[14]</ref> have used lexical density, intensifiers, and personal pronouns as important measures to gauge the writing style of the user. Lexical density is the fraction of information carrying words present in the tweet (nouns, verbs, adjectives, and adverbs). Intensifiers are words that maximize the effect of adverbs or adjectives (for example, so, or very). Personal pronouns are pronouns denoting a person or group (for example, me, our, or her). We include as features the lexical density, the number of intensifiers used, the number of first-person singular, first-person plural, second-person, and third-person pronouns present in the tweet (6 features).</p><p>In total, we construct 335 features based on the behavioral aspects of sarcasm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENTS</head><p>In this section, the SCUBA framework is systematically evaluated through a series of experiments. First, we detail the data collection process and our dataset. Next, we train SCUBA and compare its performance to baselines. Then, we determine the contribution of different feature sets to SCUBA's performance. We conduct feature importance analysis to determine a small set of features that are most beneficial for sarcasm detection. Finally, we examine the robustness of our framework under different scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Data collection</head><p>We validate our framework using a dataset<ref type="foot" target="#foot_1">5</ref> of tweets from Twitter. To obtain a set of sarcastic tweets, we query the Streaming API using keywords #sarcasm and #not filtering out non-English tweets and retweets. We also remove tweets containing mentions and URLs as obtaining information from media and URLs is computationally expensive. We limit our analysis to tweets which contain more than three words as we found that tweets with fewer words were very noisy or clichéd (e.g., yeah, right! #sarcasm). Davidov et al. <ref type="bibr" target="#b4">[4]</ref> noted that some tweets containing the #sarcasm hashtag were about sarcasm and that the tweets themselves were not sarcastic. To limit such occurrences, we include only tweets that have either of the two hashtags as its last word; this reduces the chance of obtaining tweets that are not sarcastic. After preprocessing, we obtained about 9104 sarcastic tweets which were self described by users as being sarcastic using the appropriate hashtags. We remove the #sarcasm and #not hashtags from the tweets before proceeding with the evaluation.</p><p>To collect a set of general tweets (not sarcastic), we used Twitter's Sample API which provides a random sample of tweets. These tweets were subjected to the same aforementioned preprocessing technique. Finally, for each tweet in the collected dataset, we extract the user who posted the tweet and then, we obtained that user's past tweets (we obtain the past 80 tweets for each user).</p><p>Some examples of tweets in the dataset are:</p><p>1. This paper is coming along... #not 2. Finding out your friends' lives through tweets is really the greatest feeling. #sarcasm</p><p>The above examples illustrate the difficulty of the task at hand. The first tweet may or may not be sarcastic purely de-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Performance evaluation</head><p>Naturally, the class distribution over tweets is skewed towards the non-sarcastic tweets. Therefore, we evaluate the SCUBA framework using different class distributions (1:1, 10:90, 20:80, where 1:1 means for every sarcastic tweet in the dataset, we introduce 1 tweet that is not sarcastic.). We include AUC (Area under the ROC Curve) apart from accuracy as a performance measure as AUC is robust to class imbalances <ref type="bibr" target="#b7">[7]</ref>. This analysis gives an insight into how well SCUBA performs under varied distributions. We compare SCUBA to the following baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Baselines</head><p>We compare our framework against 6 baselines. The first two are based on a state-of-the-art lexicon-based technique by Riloff et al. <ref type="bibr" target="#b29">[29]</ref>. The basic premise of their method is that sarcasm can be viewed as a contrast between a positive sentiment and a negative situation. They construct three phrase lists (positive verb phrases, positive predicative expressions, and negative situations) from 175,000 tweets using a partsof-speech aware bootstrapping technique that extracts relevant phrases. Different combinations of these phrase lists were used to decide if a tweet is sarcastic or not. Using their phrase lists, we re-implement two of their approaches:</p><p>[1] Contrast Approach. The method marks a tweet as sarcastic if it contains a positive verb phrase or positive predicative expression along with a negative situation phrase.</p><p>[2] Hybrid Approach. The method marks a tweet as sarcastic if it is marked sarcastic either by the bootstrappedlexicon approach or by a bag-of-words classifier trained on unigrams, bigrams, and trigrams. To provide a comparable framework to the Hybrid Approach, we include the prediction of the discussed n-gram classifier into SCUBA as a feature. We call our n-gram augmented framework, SCUBA++.</p><p>[3] SCUBA -#sarcasm. To quell doubts that SCUBA merely labels all tweets from users who have previously used #sarcasm or #not as sarcastic, we completely remove that particular feature and perform the same classification task.</p><p>[4] Random Classifier. The baseline classifies the tweets randomly into sarcastic and non-sarcastic.</p><p>[5] Majority Classifier. It classifies all tweets into the majority class (known from the class distribution).</p><p>[6] n-gram Classifier. The same n-gram model used in the hybrid approach that classifies tweets into sarcastic and nonsarcastic based on their unigrams, bi-grams, and tri-grams. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Training the Framework</head><p>Before training, we select a suitable classifier for SCUBA. We evaluate SCUBA's performance using multiple supervised learning algorithms on our collected dataset (with class distribution 1:1). We evaluate using a J48 decision tree, 1-regularized logistic regression, and 1-regularized 2-loss SVM <ref type="foot" target="#foot_2">6</ref> to obtain an accuracy of 78.06%, 83.46%, and 83.05%, respectively. We choose 1-regularized logistic regression for comparison with the baselines.</p><p>We use 10-fold cross-validation technique to evaluate the framework's performance, the results of which are given in Table <ref type="table" target="#tab_1">2</ref>. From the results, we observe that SCUBA++ clearly outperforms all other techniques for every class distribution and for both accuracy and AUC. Note that only SCUBA and SCUBA++ perform better than the majority classifier for highly skewed distributions (90:10). We also observe that while the Hybrid Approach performs much better than the Contrast Approach, it is still not very effective for skewed distributions. Also, we notice that when the past sarcasm feature is removed from SCUBA, we obtain similar performance outcomes indicating the minimal effect of using this feature on the framework's performance. Both random classifier and the majority classifier obtain an AUC score of 0.5, which is the minimum possible AUC score attainable. To evaluate the benefit of using different feature sets, we perform the following feature set analysis. This analysis allows us to make informed decisions about which feature sets to consider if computationally constrained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Feature set analysis</head><p>We divide the list features into sets depending on the different forms of sarcasm from which they were derivedfeatures based on complexity, based on contrast, based on expression of emotion, based on familiarity, and based on expression in text form.</p><p>Table <ref type="table" target="#tab_2">3</ref> shows the performance of SCUBA using each of the feature sets individually. While all feature sets contribute to SCUBA's performance, they do so unequally. Clearly, all feature sets perform much better than contrast-based features. This further shows the need to view sarcasm through its varied facets and not a particular form of expression (such as contrast seeking).</p><p>To gain deeper insights into which specific features are most important for detecting sarcasm, we perform the following feature importance analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Feature importance analysis</head><p>As observed, different feature sets have different effects on the performance. While we may use many features to detect sarcasm, clearly, some features are more important than others. Therefore, we perform a thorough analysis of features to determine the features that contribute the most to detecting sarcasm. This analysis can be done with any feature selection algorithm. We use the odds-ratio (coefficients from 1-regularized logistic regression) for the importance analysis. The top 10 features in decreasing order of importance for sarcasm detection are the following: Interestingly, we observe that features derived from all forms of sarcasm: text expression-based features (1, 2, 5, 10), emotion-based features <ref type="bibr" target="#b3">(3,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b7">7)</ref>, familiarity based features <ref type="bibr">(8)</ref>, contrast-based features <ref type="bibr" target="#b9">(9)</ref> and complexity-based features (4) rank high in terms of discriminative power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Evaluating effectiveness of historical information</head><p>In our framework for detecting sarcastic tweets, we have included the user's historical information on Twitter in the form of past tweets. However, it might be computationally expensive to process and use all the past tweets for classification. Furthermore, it is unrealistic to assume access to so many past tweets for each user will be always available. Therefore, it is imperative that we identify the optimum number of past tweets to be used to detect sarcasm. To do this, we measure SCUBA's performance by executing the sarcasm classification multiple times while varying the number of past tweets available to us.</p><p>Figure <ref type="figure" target="#fig_1">1</ref> shows the performance obtained with varied past tweets (smoothened using a moving-average model). We observe that with no historical information, we obtain an accuracy of 79.38%, which still outperforms all baselines. Interestingly, using only the user's past 30 tweets, we obtain a considerable gain (+4.14%) in performance. However, as we add even more historical tweets, the performance does not significantly improve. Therefore, if computationally constrained, one can use only the past 30 tweets and expect a comparable performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS AND FUTURE WORK</head><p>In this paper, we introduce SCUBA, a behavioral modeling framework for sarcasm detection. We discuss different forms that sarcasm can take, namely: (1) as a contrast of sentiments, (2) as a complex form of expression, (3) as a means of conveying emotion, (4) as a possible function of familiarity and (5) as a form of written expression. We construct relevant features to represent these forms on Twitter. We train a supervised learning algorithm using the constructed features to detect sarcastic tweets. Through multiple experiments, we demonstrate that SCUBA is effective in detecting sarcastic tweets. SCUBA's main two advantages are considering psychological and behavioral aspects of sarcasm and leveraging users' historical information to decide whether tweets are sarcastic or not.</p><p>Importantly, we have demonstrated that even limited historical information may greatly help improve the efficiency of sarcasm detection. This makes SCUBA a good fit for real-world, real-time applications which have high computational constraints. It is important to note that while we perform our evaluation and experiments on a Twitter dataset, SCUBA can be generalized to other social media sites. It can be easily expanded by including other site-specific features. This further widens the scope of applicability of SCUBA to different social media sites.</p><p>With nearly all major companies having a social media presence, SCUBA can complement existing sentiment analysis technologies to better serve the needs of consumer assistance teams online. With consumer assistance teams aiming for a zero-waiting time response to customer queries through social media, undetected sarcasm can result in embarrassing gaffes and potential PR disasters. Using SCUBA, social media teams can better detect sarcasm and deliver appropriate responses to sarcastic tweets.</p><p>In the future, we wish to expand SCUBA to also factor in users' social networks and their current and past interactions for sarcasm detection. This bodes well with existing research <ref type="bibr" target="#b31">[31]</ref> which suggests that users are more likely to use sarcasm with friends than with strangers. Further, we wish to apply our behavioral modeling framework to detect other non-literal forms of language such as humor.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3 .</head><label>3</label><figDesc>For each bigram or trigram b, we find its associated sentiment score, P OS(b) -N EG(b) P OS(b) + N EG(b) , where P OS(b) is the number of occurrences of b in the positive tweets dataset and N EG(b) is the number of occurrences of b in the negative tweets dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 .</head><label>1</label><figDesc>Percentage of emoticons in the tweet. 2. Percentage of adjectives in the tweet. 3. Percentage of past words with sentiment score 3. 4. Number of polysyllables per word in the tweet. 5. Lexical density of the tweet. 6. Percentage of past words with sentiment score 2. 7. Percentage of past words with sentiment score -3. 8. Number of past sarcastic tweets posted. 9. Percentage of positive to negative sentiment transitions made by the user. 10. Percentage of capitalized hashtags in the tweet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Effect of Historical Information on Sarcasm Detection Performance.</figDesc><graphic coords="9,53.80,53.80,238.12,194.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Examples of Misinterpreted Sarcastic Tweets. Who could predict heavy travel between #Thanksgiving and #NewYearsEve. And bad cold weather in Dec! Crazy! 1 Major U.S Airline We #love the kind words! Thanks so much.</figDesc><table><row><cell>Example</cell><cell>Users</cell><cell>Tweets</cell></row><row><cell></cell><cell cols="2">User 1 you are doing great! User 1 wow, just wow, I guess I should have #sarcasm</cell></row><row><cell></cell><cell>User 2</cell><cell>Ahhh..**** reps. Just had a stellar experience w them at Westchester, NY last week. #CustomerSvcFail</cell></row><row><cell cols="2">2 Major U.S Airline</cell><cell>Thanks for the shout-out Bonnie. We're happy to hear you had a #stellar experience flying with us. Have a great day.</cell></row><row><cell></cell><cell>User 2</cell><cell>You misinterpreted my dripping sarcasm. My experience at Westchester was 1 of the worst I've had with ****. And there are many.</cell></row></table><note><p>pose and evaluate the SCUBA framework -Sarcasm Classification Using a Behavioral modeling Approach.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance Evaluation using 10-fold Cross-Validation.</figDesc><table><row><cell>Technique</cell><cell>1:1</cell><cell>Dataset Distribution 20:80 10:90</cell></row><row><cell></cell><cell cols="2">Acc. AUC Acc. AUC Acc. AUC</cell></row><row><cell>SCUBA</cell><cell cols="2">83.46 0.83 88.10 0.76 92.24 0.60</cell></row><row><cell cols="3">Contrast Approach 56.50 0.56 78.98 0.57 86.59 0.57</cell></row><row><cell>SCUBA++</cell><cell cols="2">86.08 0.86 89.81 0.80 92.94 0.70</cell></row><row><cell>Hybrid Approach</cell><cell cols="2">77.26 0.77 78.40 0.75 83.87 0.67</cell></row><row><cell cols="3">SCUBA -#sarcasm 83.41 0.83 87.53 0.74 91.87 0.63</cell></row><row><cell>n-gram Classifier</cell><cell cols="2">78.56 0.78 81.63 0.76 87.89 0.65</cell></row><row><cell cols="3">Majority Classifier 50.00 0.50 80.00 0.50 90.00 0.50</cell></row><row><cell cols="3">Random Classifier 49.17 0.50 50.41 0.50 49.78 0.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Feature Set Analysis.</figDesc><table><row><cell>Features</cell><cell>Accuracy</cell></row><row><cell>All features</cell><cell>83.46 %</cell></row><row><cell>-Complexity-based features</cell><cell>73.00 %</cell></row><row><cell>-Contrast-based features</cell><cell>57.34 %</cell></row><row><cell>-Emotion expression-based features</cell><cell>71.52 %</cell></row><row><cell>-Familiarity-based features</cell><cell>73.67 %</cell></row><row><cell>-Text expression-based features</cell><cell>76.72 %</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>www.oxfordlearnersdictionaries.com/definition/english/sarcasm</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>The dataset can be obtained by contacting the first author. pending on the context (which is not available in the tweet). Even if some background is provided, as in the case of the second tweet, clearly, it is still a complicated task to map that information to sarcasm.It must also be noted that, to avoid confusion and ambiguity when expressing sarcasm in writing, the users choose to explicitly mark the sarcastic tweets with appropriate hashtags. The expectation is that these tweets, if devoid of these hashtags, might be difficult to comprehend as sarcasm even for humans. Therefore, our dataset might be biased towards the hardest forms of sarcasm. Using this dataset, we evaluate our framework and compare it with existing baselines.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>We use Weka<ref type="bibr" target="#b13">[13]</ref>, LIBLINEAR<ref type="bibr" target="#b6">[6]</ref>, and Scikit-learn<ref type="bibr" target="#b26">[26]</ref>.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported, in part, by the Office of Naval Research grants N000141410095 and N000141310835. We would also like to thank Dr. Heather Pon-Barry and Dr. Subbarao Kambhampati for their constructive criticisms and suggestions that helped improve this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Dictionary of psychology</title>
		<author>
			<persName><forename type="first">M</forename><surname>Basavanna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Allied Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Customer service 2.0: Where social computing meets customer relations</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="93" to="95" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recognizing sarcasm without language: A cross-linguistic study of english and cantonese</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Cheang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Pell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pragmatics &amp; Cognition</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semi-supervised recognition of sarcastic sentences in twitter and amazon</title>
		<author>
			<persName><forename type="first">D</forename><surname>Davidov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tsur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Fourteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="107" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Regional variation in the use of sarcasm</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Dress</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Kreuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Link</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Caucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Language and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="85" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An introduction to {ROC} analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fawcett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">{ROC} Analysis in Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="861" to="874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A new readability yardstick</title>
		<author>
			<persName><forename type="first">R</forename><surname>Flesch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of applied psychology</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">221</biblScope>
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Irony in talk among friends</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Gibbs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Metaphor and symbol</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="5" to="27" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Identifying sarcasm in twitter: A closer look</title>
		<author>
			<persName><forename type="first">R</forename><surname>González-Ibáñez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wacholder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (Short Papers)</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="581" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Logic and conversation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Grice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Syntax and semantics</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Cole</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Morgan</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1975">1975</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Some further notes on logic and conversation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Grice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Syntax and Semantics</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Cole</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="113" to="127" />
			<date type="published" when="1978">1978</date>
			<publisher>Pragmatics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The weka data mining software: an update</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD explorations newsletter</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="18" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dude, srsly?: The surprisingly formal nature of twitter&apos;s language</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talamadupula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kambhampati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICWSM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Kincaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Fishburne</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Chissom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
		</imprint>
		<respStmt>
			<orgName>DTIC Document</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Kouloumpis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moore</surname></persName>
		</author>
		<title level="m">Twitter sentiment analysis: The good the bad and the omg! ICWSM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="538" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lexical influences on the perception of sarcasm</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Kreuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Caucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on computational approaches to Figurative Language</title>
		<meeting>the Workshop on computational approaches to Figurative Language</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">How to be sarcastic: The echoic reminder theory of verbal irony</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Kreuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Glucksberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">374</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">How about another piece of pie: The allusional pretense theory of discourse irony</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kumon-Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Glucksberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The perfect solution for detecting sarcasm in tweets# not</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liebrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kunneman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Bosch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WASSA</title>
		<imprint>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">What&apos;s in a name? using first names as features for gender inference in twitter</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ruths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Analyzing Microtext: 2013 AAAI Spring Symposium</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Smog grading: A new readability formula</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Mclaughlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of reading</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="639" to="646" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scoring methods and difficulty levels for cloze tests of proficiency in english as a second language</title>
		<author>
			<persName><forename type="first">J</forename><surname>Oller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Modern Language Journal</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="151" to="158" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improved part-of-speech tagging for online conversational text with word clusters</title>
		<author>
			<persName><forename type="first">O</forename><surname>Owoputi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="380" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Opinion mining and sentiment analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and trends in information retrieval</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="135" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Linguistic inquiry and word count: Liwc</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Booth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<publisher>Lawrence Erlbaum Associates</publisher>
			<biblScope unit="page">71</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">From humor recognition to irony detection: The figurative language of social media</title>
		<author>
			<persName><forename type="first">A</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Buscaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data &amp; Knowledge Engineering</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sarcasm as contrast between a positive sentiment and negative situation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Surve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silva</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="704" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Lower, slower, louder: Vocal cues of sarcasm</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rockwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Psycholinguistic Research</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="483" to="495" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Empathy and the expression and recognition of sarcasm by close relations or strangers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rockwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perceptual and motor skills</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="251" to="256" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The effects of cognitive complexity and communication apprehension on the expression and recognition of sarcasm</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rockwell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Nova Science Publishers</publisher>
			<pubPlace>Hauppauge, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Culture, gender, and gender mix in encoders of sarcasm: A self-assessment analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rockwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Theriot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communication Research Reports</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="52" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Wordnet affect: an affective extension of wordnet</title>
		<author>
			<persName><forename type="first">C</forename><surname>Strapparava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Valitutti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1083" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">yeah right&quot;: sarcasm recognition for spoken dialogue systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tepperman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Traum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>INTERSPEECH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sentiment strength detection in short informal text</title>
		<author>
			<persName><forename type="first">M</forename><surname>Thelwall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Paltoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kappas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2544" to="2558" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On the uses of sarcastic irony</title>
		<author>
			<persName><forename type="first">M</forename><surname>Toplak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Pragmatics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1467" to="1488" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Icwsm-a great catchy name: Semi-supervised recognition of sarcastic sentences in online product reviews</title>
		<author>
			<persName><forename type="first">O</forename><surname>Tsur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Davidov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICWSM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Verbal irony as implicit display of ironic environment: Distinguishing ironic utterances from nonirony</title>
		<author>
			<persName><forename type="first">A</forename><surname>Utsumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Pragmatics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1777" to="1806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cursing in english on twitter</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Thirunarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Sheth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM conference on Computer supported cooperative work &amp; social computing</title>
		<meeting>the 17th ACM conference on Computer supported cooperative work &amp; social computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="415" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Norms of valence, arousal, and dominance for 13,915 english lemmas</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Warriner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kuperman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brysbaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior research methods</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Connecting users across social media sites: a behavioral-modeling approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zafarani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="41" to="49" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
