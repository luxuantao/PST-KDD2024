<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enhancing Photographs with Near Infrared Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
							<email>zhangxi7@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Terence</forename><surname>Sim</surname></persName>
							<email>tsim@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoping</forename><surname>Miao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Enhancing Photographs with Near Infrared Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5B9A95AB24A6EF196606DFA5C0F59784</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Near Infra-Red (NIR) images of natural scenes usually have better contrast and contain rich texture details that may not be perceived in visible light photographs (VIS). In this paper, we propose a novel method to enhance a photograph by using the contrast and texture information of its corresponding NIR image. More precisely, we first decompose the NIR/VIS pair into average and detail wavelet subbands. We then transfer the contrast in the average subband and transfer texture in the detail subbands. We built a special camera mount that optically aligns two consumergrade digital cameras, one of which was modified to capture NIR. Our results exhibit higher visual quality than tonemapped HDR images, showing that NIR imaging is useful for computational photography.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The radiance from natural scenes usually spans a very wide dynamic range, far exceeding what a digital camera can capture. For instance, in a sunny outdoor environment, the dynamic range could reach as high as 10 9 . In contrast, a professional-grade digital camera that uses 14 bits per channel can capture a range of only 10 4 . Consumer-grade cameras are even worse. One common technique around this problem is to first compute an high dynamic range (HDR) image, usually from multiple shots of varying exposures, and then to map this into a lower dynamic range (LDR) image suitable for display devices. However, such a tonemapping procedure does not usually produce a perceptually pleasing result. Usually, pixels end up becoming too bright or too dark, and rich scene information such as color and texture are almost completely lost. Fig. <ref type="figure" target="#fig_0">1</ref>(a) shows a typical photo taken under an HDR environment, where the footpath is very bright but the region inside the building can barely be seen.</p><p>Recently, many methods have been proposed to recover HDR radiance map and convert it to LDR images. Debevec and Malik presented a way to recover HDR radiance from multiply exposed photographs <ref type="bibr" target="#b3">[4]</ref>. Mapping HDR to LDR, also known as tone mapping, can be roughly divided into two categories: spatially uniform mapping and spatially varying mapping. For instance, Reinhard et al. used a spatially uniform operator to compress dynamic range globally and then manipulated contrast locally based on luminance values <ref type="bibr" target="#b12">[13]</ref>. A review of tone mapping techniques can be found in <ref type="bibr" target="#b4">[5]</ref>. From HDR photos, it is possible to retrieve all details and color correctly. But obtaining an HDR map requires multiple images captured with different exposures. This in turn requires the scene to be static, which greatly limits its applicability.</p><p>Another class of techniques involves modifying the imaging sensor. For example, Nayar and Branzoi <ref type="bibr" target="#b9">[10]</ref> use an LCD panel to modulate the light falling onto the sensor, while Nayar et al. <ref type="bibr" target="#b10">[11]</ref> use a digital micromirror array for the same purpose. The goal is to adaptively control the exposure of small groups of pixel according to the radiance falling on them.</p><p>Another possible solution, widely used by professional photographers, is to take photos in RAW format and manually adjust contrast region by region. Usually RAW pictures use 12 or 14 bits per channel to record scene radiance, thus resulting in a higher dynamic range than normal JPEG photos. Such manual adjustment is tedious and requires experience, and the dynamic range of RAW format is still quite limited.</p><p>In contrast, our method uses Near Infrared (NIR) light. This lies between visible red light and Long Infra-Red (LIR) light in the electromagnetic spectrum. NIR light has wavelength in the range 750 -1400 nm, which is longer than visible light (380 -750 nm). Human eyes can not see NIR light but most digital cameras can sense it very well. For example, some models of SONY digital cameras or camcorders have a Night Shot mode which increases cameras visual range by letting the sensor acquire more NIR light. However, most manufacturers insert an IR cutoff filter over the camera sensor to filter out NIR light, to avoid some unwanted artifacts. In fact, NIR images usually have better brightness contrast and provide rich texture details, as seen 978-1-4244-2243-2/08/$25.00 ©2008 IEEE in Fig. <ref type="figure" target="#fig_0">1</ref>(a) and 1(b). The details of trees and leaves are barely seen in the visible image, but look clear and sharp in the NIR image. We exploit this fact in our work.</p><p>Inspired by the camera's ability to record NIR light and by recent works on tonal transfer <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12]</ref>, we propose a novel method that can adjust a photograph's contrast adaptively and enrich texture details fully automatically with just one shot (i.e. one VIS/NIR image pair). NIR photography is not new; it is commonly appreciated for its artistic value <ref type="bibr" target="#b7">[8]</ref>, but has not been fully exploited in computational photography. Morris et al. observed that the wavelet coefficients of long infrared (LIR) natural images closely follows the Laplacian distribution <ref type="bibr" target="#b8">[9]</ref>. Encouraged by their work, we build a dualcamera system that can capture visible photo and NIR photo of the same scene simultaneously, and find that NIR images also have similar statistical properties. Moreover, we notice that NIR images of natural scenes usually exhibit lower dynamic range and contain rich texture details. In terms of transfer techniques, the Neumann brothers showed how to transfer color style from a source image to an arbitrary target image by applying histogram matching <ref type="bibr" target="#b11">[12]</ref>. Similarly, Bae et al. also presented a method to transfer tonal quality from one image to another, using histogram matching and bilateral filter <ref type="bibr" target="#b1">[2]</ref>. In the light of their work, we propose an original way of using NIR information to enhance visible photographs. Given as input one VIS image and its corresponding NIR image, our approach can adaptively detect unsatisfactory pixels in the VIS image, and transfer contrast information and high frequency texture from its NIR counterpart. We use histogram matching in the gradient domain to transfer contrast and use wavelet coefficients to transfer texture information. We were able to achieve very pleasing results (see Fig. <ref type="figure" target="#fig_0">1</ref>).</p><p>The highlights of our work include:</p><p>• A novel method that uses NIR information to adjust the contrast of photographs adaptively, and to enrich texture details automatically with one single shot.</p><p>• A threshold-free method to detect regions that require enhancement.</p><p>• A study of the statistical properties of NIR images of natural scenes.</p><p>As far as we can tell, we are the first to use NIR for photograph enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Near Infrared Imaging</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Dual-camera system</head><p>NIR light lies adjacent to visible red light in the electromagnetic spectrum, and has longer wavelength than visible light. NIR is not visible to human eyes, but can be recorded by CCD or CMOS sensors. However, most manufacturers of digital cameras install an IR cutoff filter over the sensor to suppress infrared light and avoid unwanted artifacts.</p><p>To capture both visible and NIR pictures for the same scene simultaneously, we built a dual-camera system which comprises two Sony F828 digital cameras and one hot mirror. A hot mirror is a specialized dielectric mirror which can reflect NIR light when incident light arrives at a certain angle. We used a 45 • hot mirror, meaning it can reflect NIR light with angle of incidence of 45 • but does not block visible light. Fig. <ref type="figure" target="#fig_1">2</ref>(a) illustrates how our system works. Although the Sony F828 has built-in Night Shot mode which can temporarily move the IR cutoff filter away to allow NIR imaging, Sony has intentionally limited such NIR imaging to only allow long exposure times. Our modified camera does not suffer from this limitation. We also modified the remote control of the camera so that it can trigger two cameras at the same time. We have carefully setup two cameras to ensure that they are optically aligned. They also share the same camera settings, such as focal length and aperture size, to guarantee the geometric alignment of the image pair. Currently, we do not force the two cameras to use the same shutter speed, because digital cameras are designed to be less sensitive to NIR thus requiring a slightly longer exposure.</p><p>The NIR picture captured in this way is actually an RGB color image and looks reddish since NIR light is just adjacent to red light. However, because of the filters we use, the NIR light we capture is almost monochromatic and should not contain any color information. So we use only intensity information by converting to HSV color space and using V channel. Fig. <ref type="figure" target="#fig_0">1</ref>(a) and Fig. <ref type="figure" target="#fig_0">1</ref>(b) show an example image pair captured by our dual-camera system. Our prototype hardware may look bulky, but this can be miniaturized. Our goal is to show the usefulness of NIR images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Statistics of NIR images</head><p>Huang and Mumford <ref type="bibr" target="#b6">[7]</ref> have shown that the gradient histograms of natural images follow a generalized Laplace distribution which can be expressed as Eq. 1:</p><formula xml:id="formula_0">P (x) = k • e -|x/s| α .<label>(1)</label></formula><p>Recently, Morris et al. <ref type="bibr" target="#b8">[9]</ref> found wavelet coefficients of LIR (wavelength lies in 4000 -120000 nm) images of natural scenes can also be well fitted with a Laplacian curve. In this paper, we show that NIR natural images share similar statistical properties, as illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. We collect a total of 220 NIR photos for statistical analysis. Some of them are collected from web and others are captured by ourselves, mostly covering subjects of natural scene and people. Similar to <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref>, we use gradient magnitude and the Haar wavelet coefficients. We calculate the histograms of all images for both gradient magnitude and wavelet coefficients in horizontal (H), vertical (V) and diagonal (D) directions. All these histograms are calculated on logarithm of the actual values and normalized based on image pixels. In Fig. <ref type="figure" target="#fig_1">2</ref>, all gray lines denote the actual histograms, the blue lines show the average histogram distribution, and the red dash lines show the fitted Laplacian curve (Eq. 1). We can see that the fit is good, meaning that NIR images have similar statistical properties as visible and LIR images (Please refer details to <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref>). In Sec.3.2.2 we will show how we can use these statistical properties to guide the enhancement process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Visible Image Enhancement</head><p>The workflow of our approach is illustrated in Fig. <ref type="figure">3</ref>. There are three main steps: computing the weighted region mask, transferring contrast and transferring texture. The details are explained in Sec. 3.1, Sec. 3.2 and Sec. 3.3 respectively. Note that all inputs are the logarithm of original image values as we mentioned in the previous section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Computing the weighted region mask</head><p>Intuitively, regions that suffer a loss of details are typically too bright or too dark, and have low saturation. From this observation, a weighted mask can be calculated based on saturation and brightness value. Let W s and W v denote weighted mask of saturation and brightness, W denote the final weighted region mask indicating areas to be enhanced. Then W can be obtained using following equations:</p><formula xml:id="formula_1">Ws = 1 -e -ps|s-1| , ps ∈ [0, 1], s ∈ [0, 1] Wv = 1 -e -pv |v-0.5| , pv ∈ [0, 1], v ∈ [0, 1] W = Ws • Wv ,<label>(2)</label></formula><p>where s and v are the saturation and brightness intensity, and p s and p v denote the probability that s and v appear in visible image respectively. p s and p v can be easily obtained from the normalized histograms of channels S and V. The meaning of p s and p v is that the pixels to be enhanced should distribute over large areas, rather than in small regions. Enhancing large areas while ignoring small regions usually achieves better perceptual quality. shows the histograms of saturation and brightness channel (gray regions). The red and blue lines illustrate the weights computed according to Eq. 2. The curves are smoothed for reducing noise.</p><p>The sky and cloud have high brightness and relatively low saturation, and thus have higher weights; vice versa for the tree tops.</p><p>Note that W is calculated adaptively and fully automatically, not requiring any thresholds. The weighted region mask is used as a mask for brightness and texture transfer later. Fig. <ref type="figure">4</ref> shows what our weighted region mask looks like and how it is generated. A higher value in W means more information will be transferred from NIR image, and vice versa. To reduce noise, a Gaussian blurring is first applied on W .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Transferring contrast</head><p>Our contrast transfer method is based on histogram matching and gradient techniques. Bae et al. also applied histogram matching to transfer photograph tonality <ref type="bibr" target="#b1">[2]</ref>. Instead of matching histogram in the intensity domain as they did, we show that histogram matching in the gradient magnitude can achieve better and reliable results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Histogram matching</head><p>The histogram matching problem can be simply defined as: given an image I and a target histogram (pdf) h(z), the problem is to find a new image J by transforming I, so as to make histogram of J be as same as h. The problem can be solved by using the cumulative distribution function (CDF), f . Define f (x) = R x o h(z)dz, where x is image intensity. Let I ij and J ij denote each pixel intensity in I and J. Then the desired image J can be obtained using Eq. 3, and the detailed proof can be found in <ref type="bibr" target="#b5">[6]</ref>.</p><formula xml:id="formula_2">J ij = f -1 J (f I (I ij ))<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Large-Scale contrast transfer</head><p>The brightness contrast of a visible image is affected by environment illumination, as well as object shape and texture in the scene. Therefore, the brightness map of an image should change smoothly while preserving major features such as strong edges. To achieve a smooth brightness map of visible image V and NIR image N (V and N are actually the average subbands in the Haar decomposition, as shown in Fig. <ref type="figure">3</ref>), we apply bilateral filtering <ref type="bibr" target="#b13">[14]</ref> to decompose images to large-scale layer and detail layer, and use the larger-scale layer as brightness map, as in Eq. 4:</p><formula xml:id="formula_3">VL = bf(V ), VD = V -VL N L = bf(N), N D = N -N L .<label>(4)</label></formula><p>V L and N L are large-scale layers, and V D , N D are corresponding detail layers (after taking the logarithm). We use a similar definition for the bilateral filter function bf and parameter selection as in Bae et al.'s work.</p><p>We implement three different methods to transfer contrast from the NIR image to the VIS image. A comparison of their results can found in Fig. <ref type="figure" target="#fig_3">5</ref> and<ref type="figure" target="#fig_5">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method 1: Histogram Matching Inspired by Bae et al.'s</head><p>method <ref type="bibr" target="#b1">[2]</ref>, we can simply match intensity histogram of V L with N L to transfer intensity distribution. This method is easy and efficient, but histogram matching blindly alters pixel values and thus very possibly destroy illumination consistency. From Fig. <ref type="figure" target="#fig_3">5</ref>, we see that histogram matching does improve the contrast significantly. However, we also see that pixels in the tree bark are over brightened and inconsistent with the illumination in the original image. After applying the gradient constraint, the result looks more natural.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method 2: Histogram Matching with Gradient Constraint</head><p>To maintain illumination consistency, we can check the gradient direction of the altered brightness map pixel by pixel. Once we find the gradient direction that is reversed or changed too much from the original brightness map, we force them to be zero. After applying the gradient constraint, the enhanced result looks more natural compared with method 1 (see Fig. <ref type="figure" target="#fig_3">5</ref>). But in some cases, where gradients change abruptly along their original directions due to the histogram matching step, this constraint will fail, as shown in Fig. <ref type="figure" target="#fig_5">6</ref>. The gradient constraint cannot remove the banding-effect on the pillar and wall, because the gradients in those areas are not actually reversed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method 3: Gradient Magnitude Matching</head><p>To strictly maintain smoothness of the transferred brightness map, we match the histogram of brightness gradient magnitude instead of brightness intensity. We define V G and N G as the gradient magnitude of V L and N L :</p><formula xml:id="formula_4">V G = q V 2 Gx + V 2 Gy = s ( ∂VL ∂x ) 2 + ( ∂VL ∂y ) 2 NG = q N 2 Gx + N 2 Gy = s ( ∂N L ∂x ) 2 + ( ∂N L ∂y ) 2 .<label>(5)</label></formula><p>In Sec. 2.2 we have shown that gradient magnitude histogram of NIR image can be well fitted with a generalized Laplacian curve. Because N L is a smoothed version of the NIR image, its gradient magnitude N G also has same statistical property. Let l denote the Laplacian curve that can fit histogram of N G . Instead of matching histogram of V G with histogram of N G directly, we use l as the target histogram to produce a smoother and noise-free distribution transfer. In this case, the functions f I and f J in Eq. 3 are the CDFs of l. Let V G 0 denote the histogram matching result, we can easily compute new gradients by scaling V Gx and V G y along their original directions respectively: From V G 0 x and V G 0 y , we reconstruct new large-scale brightness map V L 0 by using Agrawal et al.'s improved Poisson solver <ref type="bibr" target="#b0">[1]</ref>. The final contrast transferred V 0 is obtained by blending enhanced brightness map and its original version V together using alpha-blending</p><formula xml:id="formula_5">V G 0 x = V G 0 VG • VG x V G 0 y = V G 0 VG • VG y .<label>(6)</label></formula><formula xml:id="formula_6">V 0 = W • (V L 0 + VD) + (1 -W ) • V,<label>(7)</label></formula><p>where the weighted map W is used as the alpha channel and |•| denotes pixel-wise multiplication. This method naturally maintains illumination consistency and achieves the best result among these three methods. See Fig. <ref type="figure" target="#fig_5">6</ref> for comparison. Note that the bandingeffects of methods 1 and 2 are completely suppressed, while overall contrast has been improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Transferring texture</head><p>As we state in our workflow (Fig. <ref type="figure">3</ref>), after applying Haar wavelet transformation, the wavelet subbands in horizontal, vertical, and diagonal directions actually contain rich texture information. To transfer those details, we use alpha blending again to combine corresponding subbands together:</p><formula xml:id="formula_7">V H 0 = W • NH + (1 -W ) • V H.<label>(8)</label></formula><p>V V 0 and V D 0 are obtained similarly. The new subbands V H 0 , V V 0 , and V D 0 not only inherit texture details from the VIS image, but are also enhanced by rich high frequency details from NIR image.  high frequency details from NIR to visible image, those lost textures are successfully recovered, and those weak textures are also reinforced greatly. Finally, we apply inverse Haar wavelet transform to enhance the V channel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>A common HDR scene is the natural outdoor environment under bright sunlight. To demonstrate the strength of our techniques, we test our approach with pictures taken in such HDR situations. All input visible and NIR image pairs have been geometrically aligned. In outdoor daylight, tree leaves and some objects, such as cloth and skin, reflect NIR light strongly, so they look bright and have much details even in shaded areas (see Fig. <ref type="figure" target="#fig_7">1, 8,</ref> and<ref type="figure">9</ref>). Such features in NIR images are useful for enhancing visible images.</p><p>We also find that contrast transfer and texture transfer are both equally important for enhancement. As shown in Fig. <ref type="figure" target="#fig_4">7</ref>: Fig. <ref type="figure" target="#fig_4">7</ref>(e) is the enhanced result with only texture transferred, where most of roof details are successfully recovered but the picture still looks over-exposed; Fig. <ref type="figure" target="#fig_6">7(f</ref>) is the result with only contrast transferred, which has lower contrast but roof details are still lost. Obviously, after transferring the contrast and texture, Fig. <ref type="figure" target="#fig_4">7</ref>(g) exhibits better visual quality.</p><p>To show that histogram matching of gradient magnitude (Method 3) can preserve overall illumination map and achieve higher perceptual quality, we compare of our results with a naively blended output (see Fig. <ref type="figure" target="#fig_7">8(d</ref>) and Fig. <ref type="figure">9(d)</ref>). This trivial result is obtained using alpha-blending of V and N based on the weighted region mask W , i.e. using each pixel value in W as alpha value.</p><p>We also compare our results with tone-mapped HDR images of same scenes, as shown in Fig. <ref type="figure" target="#fig_7">8</ref>(h) and Fig. <ref type="figure">9</ref>(e). To get the HDR image, we take multiple images with different exposure (usually 5-7 pictures with exposure difference of 1 stop), and assemble them using the HDR Shop software developed by Debevec <ref type="bibr" target="#b2">[3]</ref>. We recovered the camera response curve of our camera to generate HDR image more precisely, and we applied Reinhard et al.'s algorithm for tone mapping <ref type="bibr" target="#b12">[13]</ref>. Tone-mapped HDR images are supposed to produce a range-compressed image with rich details and high visible quality. However, tone mapping algorithms usually have a strict assumption that the scene must be static, i.e. no moving objects throughout the whole image sequence. Such an assumption is easily broken in outdoor scenes, as shown in Fig. <ref type="figure" target="#fig_7">8</ref>(h). The walking pedestrian and leaves waving in the wind cause serious "ghosting effect" (shown in red boxes) in the tone mapped results. Because the inputs of our approach are captured in a single shot, our results are free of such artifacts. Besides, our approach preserves consistency of overall illumination distribution while recovering scene details, therefore our results gain better perceptual quality on brightness contrast than tone-mapped results, as shown in Fig. <ref type="figure" target="#fig_7">8</ref>(h) and Fig. <ref type="figure">9</ref>(e).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Discussions</head><p>In this paper, we presented an approach of enhancing visible photograph using NIR information based on a dualcamera prototype. Without manual segmentation or interaction, our method can calculate the enhancing weight for each pixel automatically and transfer brightness and texture details from the NIR image to the visible image. We show that our histogram matching of gradient magnitude can well maintain large-scale illumination and achieve better perceptual quality. Also, by combining the wavelet H, V, and D subbands of NIR and visible image high frequency details are effectively enhanced.</p><p>Since common digital cameras are capable of recording NIR light, ours is a practical method to enhance LDR images. Compared with tone mapping an HDR image, our method requires only one shot (i.e. one VIS/NIR image pair). Benefiting from the better contrast in the NIR images and a spatially varying weighted mask, our method can produce results that are aesthetically more pleasing than tonemapped methods.</p><p>As far as we can tell from searching the published literature, we are the first to make use of NIR information for photograph enhancement. We believe that the unique property of NIR light has great potential for computational photography. Photography in low light condition may also benefit from NIR imaging, because in low visible light situation there could still be sufficient NIR light and objects that strongly reflect NIR can be captured. In the near future, we hope to investigate how NIR images can be used to guide HDR tone-mapping. Figure <ref type="figure">9</ref>. Another comparison of our approach with alpha-blending and HDR tone mapping. Our method successfully enhances brightness and texture by using NIR information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. We proposed a novel image enhancement method by transferring contrast and texture from near infrared image to visible image. Fig. 1(a) and Fig. 1(b) are an improper exposed photo taken under high dynamic range environment and its corresponding near infrared photo. With only these two input images, our approach can adaptively and automatically adjust contrast and enrich visible details in overor under-exposed areas, as shown in Fig. 1(c).</figDesc><graphic coords="2,66.97,67.42,147.24,117.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. (a): Our VIS-NIR dual-camera prototype. Camera V and N are optically aligned and connected to the same remote control, allowing a VIS/NIR image pair of the same scene to be captured with a single shot. (b-c): Statistical properties of NIR images. (b) shows distribution of gradient magnitude, similar to statistics of visible images [7]. (c) shows distribution of H wavelet subband of Haar transform, similar to statistics of IR images [9]. Subbands V and D have similar distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .Figure 4 .</head><label>34</label><figDesc>Figure 3. The workflow of our approach. The enhancement process uses the Haar wavelet decomposition, and comprises three major steps: computing the weighted region mask, transferring contrast, and transferring texture. See text for details.</figDesc><graphic coords="4,63.10,256.08,112.44,89.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Comparison of histogram matching (method 1) with histogram matching with gradient constraint (method 2). The result of histogram matching (Fig. 5(c)) looks artificial because it breaks the consistency of overall brightness distribution. After applying gradient constraints, Fig. 5(d) looks more natural.</figDesc><graphic coords="5,318.33,63.22,233.79,203.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 (</head><label>7</label><figDesc>g) show the result with high frequency details transferred. The textures on the roof in the original image are almost lost completely. By transferring</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Comparison of Methods 1, 2, and 3. Note that the bandeffects (regions in red box) due to blind histogram matching have been successfully suppressed by our guided histogram matching of gradient magnitude. The result of Method 3 achieves the least artifacts and best perceptual quality.</figDesc><graphic coords="6,59.76,65.11,232.62,330.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Comparison of results with either contrast transfer or texture transfer. Fig. 7(c-g) show the zoomed-in roof details.</figDesc><graphic coords="6,324.10,589.30,234.00,89.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure8. Comparison of our approach with alpha-blending and HDR tone mapping. The naive alpha-blending result appears bad, since simple pixel-wise blending cannot transfer overall contrast. As for HDR tone mapping, the result exhibits "ghosting effect" (shown in red boxes) because objects have moved during the capture of multiple exposures.</figDesc><graphic coords="8,76.53,342.59,146.99,138.23" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgments</head><p>We thank all reviewers for their valuable suggestions. We also thank Shaojie Zhuo and Mai Lan Ha, for their insightful discussions. We acknowledge the generous support of NUS.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">What is the range of surface reconstructions from a gradient field</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Two-scale tone management for photographic look</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2005">2006. 2, 4, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName><surname>Shop</surname></persName>
		</author>
		<ptr target="http://www.hdrshop.com/.7" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recovering high dynamic range radiance maps from photographs</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A review of tone reproduction techniques</title>
		<author>
			<persName><forename type="first">K</forename><surname>Devlin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>University of Bristol</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Woods</surname></persName>
		</author>
		<title level="m">Digital Image Processing</title>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Statistics of natural images and models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Maher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Berman</surname></persName>
		</author>
		<ptr target="http://www.infrareddreams.com/.2" />
		<title level="m">Fine art infrared photography</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Statistics of infrared images</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J W</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive Dynamic Range Imaging: Optical Control of Pixel Exposures over Space and Time</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Branzoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Programmable Imaging using a Digital Micromirror Array</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Branzoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Color style transfer techniques using hue, lightness and saturation histogram matching</title>
		<author>
			<persName><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Aesthetics in Graphics, Visualization and Imaging</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Photographic tone reproduction for digital images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shirley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ferwerda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bilateral filtering for gray and color images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manduchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
