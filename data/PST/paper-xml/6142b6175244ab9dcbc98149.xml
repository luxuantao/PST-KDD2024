<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Union: A Unified HW-SW Co-Design Ecosystem in MLIR for Evaluating Tensor Operations on Spatial Accelerators</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Geonhwa</forename><surname>Jeong</surname></persName>
							<email>geonhwa.jeong@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gokcen</forename><surname>Kestor</surname></persName>
							<email>gokcen.kestor@pnnl.gov</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Pacific Northwest National Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Prasanth</forename><surname>Chatarasi</surname></persName>
							<email>prasanth@ibm.com</email>
							<affiliation key="aff2">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Angshuman</forename><surname>Parashar</surname></persName>
							<email>aparashar@nvidia.com</email>
						</author>
						<author>
							<persName><forename type="first">Po-An</forename><surname>Tsai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sivasankaran</forename><surname>Rajamanickam</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Sandia National Laboratories</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Roberto</forename><surname>Gioiosa</surname></persName>
							<email>roberto.gioiosa@pnnl.gov</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Pacific Northwest National Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
							<email>tushar@ece.gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Union: A Unified HW-SW Co-Design Ecosystem in MLIR for Evaluating Tensor Operations on Spatial Accelerators</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Spatial accelerators</term>
					<term>MLIR</term>
					<term>Deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To meet the extreme compute demands for deep learning across commercial and scientific applications, dataflow accelerators are becoming increasingly popular. While these "domain-specific" accelerators are not fully programmable like CPUs and GPUs, they retain varying levels of flexibility with respect to data orchestration, i.e., dataflow and tiling optimizations to enhance efficiency. There are several challenges when designing new algorithms and mapping approaches to execute the algorithms for a target problem on new hardware. Previous works have addressed these challenges individually. To address this challenge as a whole, in this work, we present a HW-SW codesign ecosystem for spatial accelerators called Union 1 within the popular MLIR compiler infrastructure. Our framework allows exploring different algorithms and their mappings on several accelerator cost models. Union also includes a plug-and-play library of accelerator cost models and mappers which can easily be extended. The algorithms and accelerator cost models are connected via a novel mapping abstraction that captures the map space of spatial accelerators which can be systematically pruned based on constraints from the hardware, workload, and mapper. We demonstrate the value of Union for the community with several case studies which examine offloading different tensor operations (CONV/GEMM/Tensor Contraction) on diverse accelerator architectures using different mapping schemes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Numerous custom ASIC accelerators have emerged in the recent past to effectively exploit massive parallelism and locality in the Machine Learning (ML) applications. The most popular examples, such as TPU <ref type="bibr" target="#b0">[1]</ref>, xDNN <ref type="bibr" target="#b1">[2]</ref>, RAPID <ref type="bibr" target="#b2">[3]</ref>, are based on the systolic arrays. There are also more advanced forms including NVDLA <ref type="bibr" target="#b3">[4]</ref>, Eyeriss <ref type="bibr" target="#b4">[5]</ref>, ShiDianNao <ref type="bibr" target="#b5">[6]</ref> and MAERI <ref type="bibr" target="#b6">[7]</ref>. These accelerators have demonstrated lower runtime and higher energy efficiency relative to existing popular architectures such as multi-core CPUs and many-core GPUs <ref type="bibr" target="#b0">[1]</ref>. The main architectural features that distinguish these "spatial" accelerators from CPUs and GPUs are parallelism using hundreds to thousands of processing elements (PEs), efficient communication using a fast network-on-chip § Joint second authors 1 https://github.com/union-codesign/union (NoC) to connect those PEs, and aggressive data reuse using private/shared scratchpad buffers with efficient scheduling. The success of these accelerators within the context of ML draws researchers' attention to using these accelerators in other compute-intensive domains as well, such as High Performance Computing (HPC) applications. On the other hand, the accelerators are evolving rapidly with novel designs to support new application targets or to provide better performance. Comparing all those novel designs and understanding whether they can be good solutions to a specific algorithm/workload have become incredibly difficult for computer architects and compiler researchers. Hence, there is a strong need for a flexible, composable, and reusable framework for evaluating new algorithms, their mappings on new spatial accelerator architectures.</p><p>There are three critical components, algorithm/workload, mapping, and hardware for such an ecosystem. In the previous works, these are tightly coupled to each other. For example, simulators <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> and analytical cost models <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> are focusing on a limited set of accelerators. They are also tightly coupled to a set of tensor operations as their inputs. New high level interfaces or new algorithms sometimes require intrusive changes to the cost models. In this work, we develop unified abstractions in order to design a modular framework and mitigate the aformentioned problems.</p><p>The workload inputs for cost models vary depending on the cost models as well. State of the art cost models require users to translate the operation in a specific format for the cost model <ref type="bibr" target="#b10">[11]</ref> or translate a coarse-grained operation into fine-grained operations that the cost model understands <ref type="bibr" target="#b9">[10]</ref>. Since this process is different depending on the frameworks, it requires manual efforts by users, which can be error-prone and tedious. A unified workload abstraction for the cost models that we are presenting would get rid of this inefficiency. The current cost models also differ in the mapping abstractions. For example, MAESTRO <ref type="bibr" target="#b9">[10]</ref> uses data-centric mapping, Interstellar <ref type="bibr" target="#b11">[12]</ref> uses Halide scheduling, and Timeloop <ref type="bibr" target="#b10">[11]</ref> uses loop-nest mapping. These abstractions have different strengths and limitations in expressing all possible mappings of various tensor computations and estimating cost metrics for these mappings on a new accelerator. The existing mappers <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b16">[17]</ref>, which find optimal mappings for the target workload and accelerator, are tightly dependent on their cost models due to the different mapping representations. This limits the interoperability and reusability of the mappers even though conceptually mappers could be used among different cost models if they use a unified mapping abstraction. Finally, a unified hardware abstraction is needed to represent a broad set of accelerators with diverse interconnects and memory hierarchies <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref> to explore future hardware designs.</p><p>This work introduces Union, a unified ecosystem to evaluate tensor operations on spatial accelerators while addressing the challenges mentioned above. The ecosystem is designed with unified abstractions at every level, starting from a tensor operation and its mapping description to hardware description. These abstractions enable the usage of different mappers and cost models interchangeably. Also, these abstractions are generic enough to use for future cost models and mappers. Our ecosystem leverages the recently introduced MLIR infrastructure <ref type="bibr" target="#b20">[21]</ref> to integrate with different high-level languages or frameworks, such as Tensorflow, PyTorch for ML, and COMET <ref type="bibr" target="#b21">[22]</ref> for HPC. To the best of our knowledge, Union is the first framework unifying multiple high-level frameworks for tensor computations, mappers, and cost models for spatial accelerators. We believe that our work would reduce the burden of computer architects, compiler researchers, and algorithm designers with our unified abstractions and ecosystem. In summary, the contributions of this paper are listed below:</p><p>• We provide a plug-and-play unified ecosystem to quickly evaluate tensor operations in various domains such as ML and HPC on spatial accelerators leveraging the MLIR infrastructure.</p><p>• We introduce new unified abstractions to describe tensor operations and their mappings on spatial accelerators to integrate different mappers and cost models. This allows us to evaluate diverse tensor operations from HPC kernels and ML use cases. • We introduce operation-level/loop-level analysis to identify operations to be evaluated with the target spatial accelerator using a cost model. • We show how our framework can be used with various workloads using different mappers and cost models for the diverse set of accelerators, including flexible and chiplet-based ones. The studies provide an inspiration for the future co-design of tensor operations and spatial accelerators. We believe Union framework could enhance the co-design opportunities between compiler researchers, algorithm developers, computer architects and simulation tool developers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Tensor Operations</head><p>In this section, we discuss several key tensor operations across ML and HPC. </p><formula xml:id="formula_0">xx = x × stride + r 9 yy = y × stride + s 10 OA[n][k][x][y]+ = IA[n][k][xx][yy] × F [k][c][r][s]</formula><p>Algorithm 2: A loop nest for a TC Operation Input:</p><formula xml:id="formula_1">A: A 4D input tensor with [d][f ][g][b] B: A 4D input tensor with [g][e][a][c] C: A 6D output tensor with [a][b][c][d][e][f ] 1 for a = 0 to A-1 do 2 for b = 0 to B-1 do 3 for c = 0 to C-1 do 4 for d = 0 to D-1 do 5 for e = 0 to E-1 do 6 for f = 0 to F-1 do 7 for g = 0 to G-1 do 8 C[a][b][c][d][e][f ]+ = A[d][f ][g][b] × B[g][e][a][c]</formula><p>Deep Neural Network (DNN) Models. Recently, DNN models are outperforming other ML conventional techniques in various domains. Convolution layers and fully-connected layers form the bulk of most DNN models, with the former dominating computer vision models and the latter dominating Natural Language Processing (NLP) and recommendation models. From an acceleration perspective, the 2D convolution (CONV2D) and generalized matrix-multiplication (GEMM) operations are being widely used to represent these two layers respectively. The algorithm 1 describes the convolution operation using the loop nest representation. Some accelerators such as TPU <ref type="bibr" target="#b0">[1]</ref> use algorithmic transformations such as the im2col <ref type="bibr" target="#b25">[26]</ref> to convert CONV2D to the GEMM operation while others directly compute convolution operations.</p><p>HPC Kernels. Tensor Contraction (TC) operations are generalization of matrix multiplications with arbitrary dimensions. They are popular in HPC domains including many scientific and engineering problems, such as quantum chemistry and finite-element methods. For example, the perturbative triples correction in couple cluster CCSD(T) <ref type="bibr" target="#b26">[27]</ref> methods used in the NWChem computational chemistry framework <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> produces a 6D output tensor from two 4D inputs tensor. The Tensor contractions are computationally intensive and dominate the execution time of many computational applications, thus many optimizations have been developed to improve the performance of executing these kernels. Traditional compilers mostly focus on optimizations such as loop fusions, loop tiling, and loop reordering. High-level Domain-Specific Language (DSL) compiler, instead, can take advantages from re-formulating tensor contractions in a form that is amenable for execution of heterogeneous devices. For example, the COMET compiler <ref type="bibr" target="#b21">[22]</ref>, a DSL compiler for dense and sparse tensor algebra for chemistry and graph analytics, reformulates tensor contractions by rewriting them with equivalent transposetranspose-GEMM-transpose (TTGT) expressions. The TTGT computation first flattens the tensors into matrices via explicit tensor transposition and reshape operations, then executes GEMM, and finally folds back the resulting matrix into the original tensor layout. The main advantage of this reformulation comes from leveraging highly efficient GEMM accelerators such as the NVIDIA tensor core <ref type="bibr" target="#b29">[30]</ref> or other novel dataflow accelerators, such as the ones targeted in this work. These advantages usually overcome the additional transpositions and generally yield higher performance. However, rebuilding the semantics of a tensor contraction from optimized loops is complicated. To achieve high performance on novel dataflow architectures, it is paramount that a compiler retains the semantics of the language operations throughout all the optimization steps, which explain why most of the novel dataflow accelerators proposed leverage high-level languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-Level Intermediate Representation (MLIR)</head><p>To bridge the semantic gap between high-level language and low level Intermediate Representations (IRs), we leverage the MLIR framework. MLIR has been proposed for both reusability and extensibility <ref type="bibr" target="#b20">[21]</ref> and allows intergration of multiple IRs with different level of semantics at the same time.</p><p>Currently, many languages and libraries exist, including TensorFlow, Rust, Swift, and Julia, that rely on their own specific IR. On the other hand, multiple target architectures are emerging, especially in the Artificial Intelligence (AI) domain. Maintaining all these compiler frameworks and porting each of them to any new architecture are challenging tasks, which may limit the scope of each language to a limited number of target architectures. The MLIR framework addresses this fragmentation problem by proposing a modular and reusable IR stack that sits in between the language representation and the architectural representation <ref type="bibr" target="#b20">[21]</ref>. In this way, architectural specific operations and types can be encapsulated in specific IRs, while sharing common operations, types, and optimizations across languages and target architectures.</p><p>MLIR also supports the compilation of high-level abstractions and domain-specific constructs while providing a disciplined and extensible compiler pipeline with gradual and partial lowering. The design of MLIR is based on minimal fundamental concepts and most of the IRs in MLIR could be fully customized. Users can build domain-specific compilers and customized IRs, as well as combining with existing IRs, opting in to optimizations and analysis. The core MLIR concepts include the followings.</p><p>• Operations are the units of semantics and model concepts from "instructions" to "functions" and "modules".</p><p>An operation always has an unique opcode. It takes arbitrary number of static single assignment (SSA) operands and produces results. It may also have attributes, regions, blocks arguments, and location information as well. • Values are the results of an operation or block arguments, and a value always has a type defined by the type system. A type contains compile-time semantics for the value. • Dialects consist of a set of operations, attributes and types which are logically grouped and work together. • Regions are attached to an instance of an operation to provide the semantics (e.g., the method of reduction in reduction operation). Moreover, a region comprises a list of blocks, and a block comprises a list of operations. Beyond the built-in dialect in MLIR system, MLIR users can easily define new dialects, types, operations, analysis or transformation passes and so on. This feature makes MLIR easily extensible.</p><p>In this work, we leverage MLIR to decouple high-level language semantics, general optimizations and transformations, and architecture-specific mappings focusing on operations, attributes, and dialects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Spatial Accelerators</head><p>To increase the compute throughput while achieving high energy-efficiency for DNN operations, various spatial accelerators have been proposed recently from both industry and academia. A simple spatial accelerator architecture composed of eight PEs with shared L2 buffer are shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>1) Architecture: The spatial accelerators can be categorized into three groups based on their structure: rigid accelerators (e.g., TPU <ref type="bibr" target="#b0">[1]</ref>, NVDLA <ref type="bibr" target="#b3">[4]</ref>, Eyeriss), flexible accelerators (e.g., Eyeriss v2 <ref type="bibr" target="#b18">[19]</ref>, MAERI <ref type="bibr" target="#b6">[7]</ref>, SIGMA <ref type="bibr" target="#b30">[31]</ref>) and multi-chiplet accelerator (e.g., Simba <ref type="bibr" target="#b17">[18]</ref>). Unlike traditional architectures including CPUs and GPUs, spatial architectures use scratchpads as their intermediate buffers. Scratchpads are programmable so that the user can stage intermediate data tiles to maximize data reuse by properly mapping the data at the right time at the right location.</p><p>2) Cost Models: To quickly evaluate the performance and energy-efficiency of accelerators, the architecture community has been developing various cost models. Unlike CPUs and GPUs, where runtime contention for shared resources in the datapath and memory hierarchy can lead to non-determinism, accelerators can actually be modeled to the fairly accurate degree as their datapaths and memory hierarchies are tailored to the operation they are designed to accelerate. This allows accelerators to be modeled analytically without requiring cycle-level simulations. Different cost models exist today in the community for modeling different kinds of accelerators at varying degrees of fidelity. For e.g., SCALE-sim <ref type="bibr" target="#b7">[8]</ref> models systolic arrays (e.g., Google TPU), MAESTRO <ref type="bibr" target="#b9">[10]</ref> models spatial arrays with configurable aspect ratios <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b18">[19]</ref>, Timeloop <ref type="bibr" target="#b10">[11]</ref> can model hierarchical spatial arrays with complex memory hierarchies (e.g., partitioned buffers and buffer bypassing <ref type="bibr" target="#b19">[20]</ref>), and Tetris <ref type="bibr" target="#b31">[32]</ref> can model 3D arrays.</p><p>3) Mappers: Using an accelerator cost model, one can estimate the performance of the program with a specific mapping on the target hardware. However, it is not straightforward to find the optimal mapping for a given workload and an architecture for two reasons. First, the space of mappings can be extremely large <ref type="bibr" target="#b10">[11]</ref> which makes exhaustive searches infeasible. This has led to several mappers being developed to reduce the search time by pruning the search space or searching with efficient methods. Marvel <ref type="bibr" target="#b12">[13]</ref> proposes a decoupled approach to decouple the off-chip map-space from the on-chip one, Timeloop <ref type="bibr" target="#b10">[11]</ref> leverages sampling-based search methods, Interstellar <ref type="bibr" target="#b11">[12]</ref> uses heuristic-based search, Mind Mapping <ref type="bibr" target="#b32">[33]</ref> develops a surrogate model to perform gradientbased search, and GAMMA <ref type="bibr" target="#b14">[15]</ref> uses genetic-algorithm based method to efficiently progress by leveraging the previous results. This is currently an active area of research and we expect many more to come. Next, defining the map-space can often be complex by itself since different operations and diverse hardware accelerators may impose constraints on the mappings that are feasible. This is the reason why the mappers described above are highly tied to specific cost models today, limiting interoperability. We discuss this further in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Challenges with Existing Frameworks</head><p>The main challenge of the existing frameworks is that they have been developed in a tightly-coupled manner. For example, MAESTRO is a cost model which estimates the performance of the hardware only when a mapping is given. Therefore, it does not find an optimal mapping for the hardware for a workload. GAMMA and Marvel are mappers which search for the optimal mapping for the target hardware/workload using MAESTRO as the cost model. Since both GAMMA and Marvel are tied to MAESTRO, it is not possible to reuse mappers in GAMMA and Marvel using another cost model like Timeloop without having non-trivial engineering effort. On the other hand, Timeloop includes both a cost model and a mapper. Similar to the previous example, it is not possible to use MAESTRO as the backend cost model using the Timeloop's mapper without significant engineering effort. We summarize the comparison of our Union framework with prior frameworks in Table <ref type="table" target="#tab_0">I</ref>. Since the goal of our work is to bring such Accelerator Design-Space Exploration tools under a unified framework, to the best of our knowledge, there is no such framework to compare directly with our approach.</p><p>Unfortunately, the lack of interoperability stifles innovation, since none of the mappers and cost models is perfect. Most new accelerator proposals develop new cost models for their design, but they are only able to demonstrate their efficiency for a few hand-optimized mappings. Similarly, researchers working on mapping/compilation for accelerators typically evaluate its efficiency on a specific accelerator for which they have access to the specific cost model (or real hardware).</p><p>This problem gets exacerbated as we move up the software stack since DNN model developers using high-level frameworks rely on very simple metrics like total number of Multiply-Accumulate (MAC) operations or the number of trainable parameters in their model to estimate the efficiency of the model which has been shown to be ineffective and oftentimes misleading <ref type="bibr" target="#b33">[34]</ref> as it loses all nuances related to the dataflow of the accelerator and data reuse capabilities.</p><p>We believe it is crucial to enable domain-experts, compiler experts, and computer architects to have an access to an endto-end infrastructure that provides a library of plug-and-play mappers and cost models so that users can explore different options interchangeably, and focus on their specific research target (e.g., a new DNN model or a new mapper or a cost model for a new accelerator) without having to engineer or approximate the other parts. Considering the features we discussed previously, MLIR can play a role as the right bridge for this effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OVERVIEW OF UNION</head><p>In this section, we describe our framework, Union. The overview of the framework is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. A user will use Union by specifying workload in high-level language like TensorFlow or DSL, target hardware (with an architecture file and a mapping constraint file), and optimizer options including mapper type, cost model type and unit operation. Union analyzes and lowers the given problem to a Union problem which is used for finding an efficient Union mapping that captures how the data should be tiled and delivered within the memory hierarchy. The affine dialect annotated with a Union mapping can further be lowered to accelerator specific configurations to run the specific accelerators, but this is not the scope of this project and we leave it to the users who want to run their own accelerators. One of the key contributions of Union is a set of abstractions for problem/hardware/mapping to unify different modules which will be presented in Sec. IV. Here, we introduce the overview of Union.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Frontend: Using MLIR as a Bridge</head><p>To demonstrate the composability and flexibility of our framework, we consider two high level DSLs which target very different application domains, TensorFlow for ML and COMET DSL for computational chemistry.  lowering TensorFlow code to mid-level MLIR dialects (such as linear algebra) including IREE <ref type="bibr" target="#b34">[35]</ref> and NpComp <ref type="bibr" target="#b35">[36]</ref>, we follow the Tensor Operator Set Architecture (TOSA) dialect approach <ref type="bibr" target="#b36">[37]</ref> in this work. Moreover, current efforts, including ours, mostly focus the inference side and assume that the machine learning model has been already built and trained. This approach is common on state-of-the-art DNN accelerators, such as the NVIDIA Deep Learning Accelerator (NVDLA) <ref type="bibr" target="#b3">[4]</ref>, where models are trained on GPUs and the NVDLA is used for inference.</p><p>We use a trained machine learning model using the standard execution flow on CPU, GPU, or TPU, which is saved as a graph. The graph is associated with a set of properties, including shape, types, and number of layers. Next, the generated graph is optimized and converted to its functional counterpart by removing some of the specific TensorFlow information, such as TensorFlow control regions and islands. At this point, this graph can be converted to the TOSA dialect, which is a generic MLIR dialect for tensor algebra targeting machine learning applications. The TOSA dialect is also the lowest domain-specific dialect in our framework. As explained next, the rest of the compilation pipeline including mappers and cost models are shared across the various domains.</p><p>2) COMET DSL: The COMET compiler <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b37">[38]</ref> supports the COMET DSL for sparse and dense tensor algebra computations, focusing on computational chemistry kernels in NWChem and graph analytics. The compiler is based on MLIR <ref type="bibr" target="#b20">[21]</ref> which performs a progressive lowering process to map high-level operations to low-level architectural resources. It also includes a series of optimizations performed in the lowering process, and various IR dialects to represent key concepts, operations, and types at each level of the MLIR. At each level of the IR stack, COMET performs different optimizations and code transformations. Domain-specific, hardware-agnostic optimizations that rely on high-level semantic information are applied at high-level IRs. These include reformulation of highlevel operations in a form that is amenable for execution on heterogeneous devices (e.g., rewriting TC operations as TTGT) and automatic parallelization of high-level primitives (e.g., tiling for thread-and task-level parallelism). Currently, the compiler generates efficient code for traditional central processing unit (CPU) and GPU architectures as well as Verilog code for FPGAs.</p><p>3) Lowering to Linalg/Affine Dialect: Regardless of the language used for the original application, we lower the code down to the language-specific description of the application to frontend MLIR dialects, e.g., TensorFlow to TOSA or COMET DSL to COMET Tensor Algebra (TA) dialect. This is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Next, we further lower from the domain-specific dialects to generic, language-independent constructs and operations, such as CONV2D and GEMM. In our framework, both TOSA and COMET TA dialects are lowered to a common Linear Algebra (Linalg) MLIR dialect. At this stage, the IR is effectively decoupled from the original language and we can analyze the operations independently from the language. Depending on the accelerator cost model (as discussed next), the operations may be lowered further to Affine dialect for a loop-nest representation.</p><p>Cost Model Dependent Conformability Passes. The next step needs to consider the requirements from the underlying cost models. Here, Union transforms and annotates the generic IR obtained in the previous step with information that is necessary for the mapping design space exploration. The cost models we consider in this work targeting spatial accelerators have different constraints for the workloads that they can evaluate. For e.g., MAESTRO natively supports CONV2D and GEMM operations, where as Timeloop supports perfectly affine nested loops with no conditionals. Hence, our framework includes operation-level or loop-level conformability passes to check if the tensor operation is conformable to the underlying cost model for evaluation. These conformability passes embody different constraints (such as checking for specific operations or loop bounds <ref type="bibr" target="#b12">[13]</ref>) of different cost models to determine whether it can be evaluated by the cost models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. An Optimizer for an Efficient Mapping: Union-opt</head><p>After processing at the Union-frontend, the target problem is translated into an instance of Union problem abstraction (Sec. IV). The Union optimizer, Union-opt, searches for the efficient mapping of the problem based on the target metric such as latency, energy, Energy-Delay-Product (EDP).</p><p>To do so, Union-opt explores the map-space for the given problem, architecture and constraint. Different mappings can incur different PE utilization, data distribution, reduction, and data reuse affecting to the performance and energy efficiency. Fig. <ref type="figure" target="#fig_2">3</ref> shows how the latency and energy consumption can vary for different mappings that Union explores for a layer from DLRM <ref type="bibr" target="#b38">[39]</ref> on a simple spatial architecture with 16×16 PE array. We will discuss more about how Union-opt can be used through the case studies in Sec. V. Since the mapping space for a simple problem can be extremely large due to the exponential and multiplicative characteristics of number of cases, it is inevitable to have efficient mappers other than exhaustive search <ref type="bibr" target="#b10">[11]</ref>. 1) Mappers: We currently integrate a few mappers in Union including exhaustive search, random sampling based search (from Timeloop <ref type="bibr" target="#b10">[11]</ref>), decoupled approach (from Marvel <ref type="bibr" target="#b12">[13]</ref>) and a few heuristic-based approaches. Users can add their own mappers and/or cost models by supporting our abstractions directly or adding converter from their format to our abstractions (Sec. IV).</p><p>2) Domain-Specific Accelerator Cost Models: We currently implement Timeloop and MAESTRO as the cost models in Union to evaluate the mappings for proof of concept, but Union can easily be integrated with other cost models.</p><p>MAESTRO <ref type="bibr" target="#b9">[10]</ref> takes a high-level DNN operation such as CONV2D, GEMM, and DWCONV as an input problem. Therefore, whether the given problem is conformable or not depends on the high-level operation type. On the other hand, Timeloop <ref type="bibr" target="#b10">[11]</ref> can take a fully nested loop which satisfies a few rules as an input problem. The fully nested loop should have affine indexes and every loop re-ordering should not change the result of the problem. Furthermore, each cost model assumes an unit operation for a PE such as two-operand MAC with certain data type. To evaluate the performance of // C4: L2 to L1 for tm4 = <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref> for tn4 = <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref> for tk4 = the given problem, the unit operation should be supported in its energy model. For example, CONV2D can be used as an input problem for Timeloop since it can be described as a fully nested loop following the given rules as shown in algorithm 1 assuming that the energy model is configured with two-operand MAC as its unit operation. Similarly, GEMM or Tensor Contraction can be evaluated using the Timeloop cost model since they all can be described as a fully nested loop following the given rules and using the two-operand MAC operation as its unit operation. Matricized tensor times Khatri-Rao product (MTTKRP) operation cannot be evaluated using Timeloop if its energy model is configured with two-operand MAC as its unit operation, but it can be done by changing three-operand-multiply-add as its unit operation and provide the necessary energy model. The backend of Union can be customized for generating configurations of individual accelerator targets. The backend is beyond the scope of this paper and part of our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. UNION ABSTRACTIONS FOR WORKLOAD, ARCHITECTURE, AND MAPPING</head><p>Evaluating a mapped problem on a target spatial architecture requires abstractions between the architecture, mapping, and the workload. Different frameworks that evaluates spatial accelerators have come up with different abstractions respectively to compare the performance and energy consumption of mappings of tensor operations on spatial architectures. We first discuss some of their limitations and present the abstractions we developed for Union.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Limitations of Current Mapping Abstractions</head><p>1) Memory-target Loop-centric Approach: Most of previous frameworks <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b14">[15]</ref> use each hardware memory level as the target of a loop tiling level (i.e. tiling can only happen in each memory hierarchy level, such as between L2 and L1 buffers) to exploit temporal and spatial locality. Fig. <ref type="figure" target="#fig_3">4</ref> shows a memory-target loop-centric mapping σ and four different possible interpretations of such mapping on a 8×8 2D PE array. Its loop nest representation is shown in Fig. <ref type="figure" target="#fig_3">4(a)</ref> where for loops describe the temporal mapping while spatial_for loops describe spatial mapping (i.e., parallel units). Since the σ does not have the information about in which direction the problem dimensions is parallelized in physical spatial units, the mapping can be realized by all options in Fig. <ref type="figure" target="#fig_3">4(b)</ref>-(e). To circumvent the ambiguity the mapping representation, prior frameworks either assume certain implicit rules specific to the accelerators, or introduce extra annotations to indicate the mapping with spatial distribution and physical spatial axis. Another limitation of such abstraction is that there is a 1-to-1 mapping between a tensor rank and physical spatial dimension in the memory-target representations. For example, memory-target abstraction cannot describe parallelizing the M dimension onto both horizontal and vertical axis in the PE array. Similarly, it is impossible to precisely describe a mapping which distributes M and N dimensions on horizontal axis and distributes N and K dimensions on the vertical axis using memory-target loop-centric mapping scheme. Moreover, due to the hierarchical order between spatial_for loops, two iterators cannot change concurrently except at the loop bounds. Such limitation forbids a mapping which parallelizes different problem dimensions at the same time.</p><p>2) Cluster-target Data-centric Approach: MAESTRO [10] introduces the notion of clusters. A cluster means a logical group of PEs. Instead of fixed hardware memory levels, MAESTRO targets each logical cluster level for tiling to explore more fine-grained tiling opportunities and remove the ambiguity caused by memory-target approach. However, MAESTRO's mapping abstractions use a data-centric notation which is not suitable to reason about using high-level computebased abstractions, such as MLIR. Moreover, MAESTRO assumes a fixed accelerator architecture: a 2-level memory hierarchy with private L1 buffers and shared L2 buffer, so it is not possible to explore mappings for accelerator architectures with more complex memory hierarchy.</p><p>Union combines the best of both these approaches discussed above and introduces a logical cluster-target loopcentric approach. We adopt a cluster-target approach to be able to describe more mappings (addressing the shortcoming in Timeloop's representation) while still enabling a straightforward translation between our notation and loops from MLIR (addressing the shortcoming in MAESTRO's representation). Table <ref type="table" target="#tab_0">II</ref> shows the differences between prior abstractions and Union.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. First Abstraction: From MLIR Dialects to a Problem Instance</head><p>Common cost models support a set of workloads, defined in different levels (ex. operation level for MAESTRO <ref type="bibr" target="#b9">[10]</ref> while loop level for Timeloop <ref type="bibr" target="#b10">[11]</ref>). From the workload written in // C4: DRAM to L2 for tm3 = 0 for tn3 = 0…1 for tk3 = 0…1 spatial_for sm3 = 0 spatial_for sn3 = 0 spatial_for sk3 = 0 // C3: L2 to V2 for tm2 = 0…1 for tn2 = 0…1 for tk2 = 0…1 spatial_for sm2 = 0 spatial_for sn2 = 0…1 spatial_for sk2 = 0 // C2: V2 to L1 for tm1 = 0 for tn1 = 0 for tk1 = 0 spatial_for sm1 = 0 spatial_for sn1 = 0 spatial_for sk1 = 0 … 3 // C1: L1 to MAC for tm0 = 0…7 for tn0 = 0…7 for tk0 = 0…1 spatial_for sm0 = 0 spatial_for sn0 = 0 spatial_for sk0 = 0  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE II COMPARISON BETWEEN PRIOR ABSTRACTION AND UNION.</head><p>Hardware Memory-target Logical Cluster-target Data-centric N/A MAESTRO <ref type="bibr" target="#b9">[10]</ref> Loop-centric Timeloop <ref type="bibr" target="#b10">[11]</ref>, Interstellar <ref type="bibr" target="#b11">[12]</ref> Union (This work)</p><p>high-level language and the target cost model, Union-frontend extracts the information from both levels as an affine dialect with an operation annotation. To handle the given problem with an affine dialect with an operation annotation, our abstractions includes loops, projections of the data spaces from array references, and operation type as shown in Fig. <ref type="figure" target="#fig_4">5</ref>(a) 2 . Loop iterators in the affine loop are set as dimensions and array references set each data in data-space with their projections. Finally, the size of each dimension is derived from the loop bounds. The affine dialect is analyzed and re-organized to set dimensions, data-space, projection, and instance. We use the attribute Operation to indicate the operation (if given). This abstraction captures both operation-level and loop-level information so that any cost model which supports one of them can be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Second Abstraction: Describing Architecture</head><p>One of the key features of Union is to describe a logical spatial architecture instead of a fixed one. We start with the hierarchical architecture abstraction used in the previous work <ref type="bibr" target="#b10">[11]</ref> and extend it to describe the architecture in the logical cluster-target manner. Fig. <ref type="figure" target="#fig_4">5</ref>(b) shows a Union architecture abstraction for the target spatial architecture illustrated in Fig. <ref type="figure" target="#fig_4">5(c</ref>). The target architecture is composed of a 2D PE 2 inspired from Timeloop problem instance description array, an L2 buffer shared across all PEs, and a private L1 buffer for each PE. We call the top cluster level in the nlevel cluster architecture as C n in this paper. For example, in Fig. <ref type="figure" target="#fig_4">5</ref>(c), we call the outermost cluster level which has DRAM as its local memory as C 4 while the innermost cluster which has a L1 buffer as its local memory is called as C 1 .</p><p>Various features can be specified in each cluster level such as compute, memory, and sub-clusters (and size of each subcluster). We also add two new attributes in each cluster level, Virtual and Dimension in addition to the abstractions used in the previous work <ref type="bibr" target="#b10">[11]</ref>. The first attribute, Virtual, indicates whether the cluster has a dedicated physical memory or not. The second attribute, Dimension, defines how the sub-clusters are laid in the physical dimension. In the example architecture shown in Fig. <ref type="figure" target="#fig_4">5</ref>(c), the cluster at C 4 has DRAM as its memory and is composed of a single sub-cluster as C 3 . A cluster at C 3 has L2 buffer and is composed of two instances of C 2 which are laid in Y-axis. A cluster at C 2 is composed of four instances of C 1 which are laid in the X axis. Note that Virtual is True only for C 2 since C 2 does not have a dedicated memory. Instead, we draw V 2 in the figure which will always be bypassed since it is virtual (imaginary) buffer, but it provides a way to describe the intermediate tiling. The innermost cluster, C 1 , includes L1 buffer and a MAC unit. With Union architecture abstraction, one can describe how a multi-level clusters mapped on to multi-dimensional PE arrays. We assume that the parallelism can only be defined across sub-clusters. For example, one can put another virtual cluster between C 2 and C 1 to exploit more fine-grained parallelism. One can also describe partitioned buffer by introducing sibling clusters in the same cluster level, similar to the way how Timeloop <ref type="bibr" target="#b10">[11]</ref> describes. Some architectures are limited to support certain loop orders depending on its dataflow such as input stationary, output stationary, weight stationary or row stationary <ref type="bibr" target="#b4">[5]</ref>. Those architectures can be realized by specifying the limitations in the constraint file which we discuss later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Third abstraction: Describing a Mapping between a Problem Instance and a Spatial Accelerator</head><p>A mapping describes how a problem instance will be executed on a logical cluster-based architecture. We propose a cluster-target mapping representation using loop-centric approach. Previous loop-based representations <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref> describe the temporal/spatial behavior of tiles in each memory level while our proposed mapping describes the behaviors in each cluster level. In our mapping, the parallelism across sub-clusters can be described at each cluster level. Unlike the memory-target representations, one can describe tiling at a virtual cluster level even though this level does not have dedicated memory units.</p><p>Semantics and characteristics. In our mapping abstraction, each tiling level explicitly targets a cluster, not memory, to cover broader mapping variants and remove ambiguity. An example Union mapping is shown in Fig. <ref type="figure" target="#fig_4">5(d</ref>) and its loop nest representation is shown in Fig. <ref type="figure" target="#fig_4">5(e)</ref>. target_cluster defines the cluster level of the following tiling directives. temporal_order defines the temporal ordering between dimensions in the cluster level. temporal_tile_sizes and spatial_tile_sizes defines the size of temporal and spatial tile for each dimension.</p><p>The spatial tile sizes defined in (i + 1)th level cluster, C (i+1) , can further be divided into sub-tiles in C i . The tile can be divided into multiple time steps using temporal tiles in C i . Each temporal tiles have the size as specified in temporal_tile_sizes in C i A temporal tile in C i can be divided into smaller spatial tiles and be spatially distributed into multiple instances of sub-clusters C (i−1) . Therefore, the parallelism in ith level can be calculated by dividing temporal tile size by spatial tile size. Note that we do not define spatial_order in each cluster level. We change the semantic of spatial_for so that it can change iterators concurrently in the same cluster level, inspired from MAESTRO data-centric notations <ref type="bibr" target="#b9">[10]</ref>.</p><p>Finally, Union introduces a few rules to check if a mapping is legal for the target logical architecture and the problem instance as shown in the following.</p><p>• The mapping will be illegal if the spatial tile size of the problem dimension d at ith cluster level is smaller than the temporal tile size of the same problem dimension d at (i − 1)th cluster level. • The parallelism for the problem dimension d at ith cluster level, which can be derived as</p><formula xml:id="formula_2">T T i d ST i d</formula><p>should be equal to or smaller than the number of (i − 1)th clusters in a ith cluster level.</p><p>• If a ith cluster is not virtual, the size of its memory should be as large as the memory sizes required by temporal tile  sizes. T T i d and ST i d are the temporal and spatial tile size of problem dimension d in ith cluster respectively.</p><formula xml:id="formula_3">C3[0] C3[1] C2[0] C1[0] C1[2] C1[2] X=2 S=2 X=1 S=1 X=3 S=3 C2[1] C1[0] C1[2] C1[2] X=2 S=2 X=1 S=1 X=3 S=3 Y=3 R=3 X=1 ⋯ 3 S=1 ⋯ 3 C2[2] C1[0] C1[2] C1[2] Y=2 R=2 X=1 ⋯ 3 S=1 ⋯ 3 Y=1 R=1 X=1 ⋯ 3 S=1 ⋯ 3 K=3 K=3 K=3 K=3 K=3 K=3 K=3 K=3 K=3 X=2 S=2 X=1 S=1 X=3 S=3 C2[0] C1[0] C1[2] C1[2] X=2 S=2 X=1 S=1 X=3 S=3 C2[1] C1[0] C1[2] C1[2] X=2 S=2 X=1 S=1 X=3 S=3 Y=3 R=3 X=1 ⋯ 3 S=1 ⋯ 3 C2[2] C1[0] C1[2] C1[2] Y=2 R=2 X=1 ⋯ 3 S=1 ⋯ 3 Y=1 R=1 X=1 ⋯ 3 S=1 ⋯ 3 K=1 K=1 K=1 K=1 K=1 K=1 K=1 K=1 K=1 K=1 ⋯ 2 X=1 ⋯ 6 N=1 K=1 ⋯ 4 C=1 X=1 ⋯ 6 Y=1 ⋯ 3 R=1 ⋯ 3 S=1 ⋯ 3 C4 X=2 S=2 X=1 S=1 X=3 S=3 K=3 ⋯ 4 X=1 ⋯ 6 C3[0] C3[1] C2[0] C1[0] C1[2] C1[2] X=2 S=2 X=1 S=1 X=3 S=3 C2[1] C1[0] C1[2] C1[2] X=2 S=2 X=1 S=1 X=3 S=3 Y=3 R=3 X=1 ⋯ 3 S=1 ⋯ 3 C2[2] C1[0] C1[2] C1[2] Y=2 R=2 X=1 ⋯ 3 S=1 ⋯ 3 Y=1 R=1 X=1 ⋯ 3 S=1 ⋯ 3 K=4 K=4 K=4 K=4 K=4 K=4 K=4 K=4 K=4 X=2 S=2 X=1 S=1 X=3 S=3 C2[0] C1[0] C1[2] C1[2] X=2 S=2 X=1 S=1 X=3 S=3 C2[1] C1[0] C1[2] C1[2] X=2 S=2 X=1 S=1 X=3 S=3 Y=3 R=3 X=1 ⋯ 3 S=1 ⋯ 3 C2[2] C1[0] C1[2] C1[2] Y=2 R=2 X=1 ⋯ 3 S=1 ⋯ 3 Y=1 R=1 X=1 ⋯ 3 S=1 ⋯ 3 K=2 K=2 K=2 K=2 K=2 K=2 K=2 K=2 K=2 K=1 ⋯ 2 X=1 ⋯ 6 N=1 K=1 ⋯ 4 C=1 X=1 ⋯ 6 Y=1 ⋯ 3 R=1 ⋯ 3 S=1 ⋯ 3 C4 X=3 S=3 X=2 S=2 X=4 S=4 K=3 ⋯ 4 X=1 ⋯ 6 C3[0] C3[1] C2[0] C1[0] C1[2] C1[2] X=3 S=3 X=2 S=2 X=4 S=4 C2[1] C1[0] C1[2] C1[2] X=3 S=3 X=2 S=2 X=4 S=4 Y=3 R=3 X=2 ⋯ 4 S=2 ⋯ 4 C2[2] C1[0] C1[2] C1[2] Y=2 R=2 X=2 ⋯ 4 S=2 ⋯ 4 Y=1 R=1 X=2 ⋯ 4 S=2 ⋯ 4 K=3 K=3 K=3 K=3 K=3 K=3 K=3 K=3 K=3 X=3 S=3 X=2 S=2 X=4 S=4 C2[0] C1[0] C1[2] C1[2] X=3 S=3 X=2 S=2 X=4 S=4 C2[1] C1[0] C1[2] C1[2] X=3 S=3 X=2 S=2 X=4 S=4 Y=3 R=3 X=2 ⋯ 4 S=2 ⋯ 4 C2[2] C1[0] C1[2] C1[2] Y=2 R=2 X=2 ⋯ 4 S=2 ⋯ 4 Y=1 R=1 X=2 ⋯ 4 S=2 ⋯ 4 K=1 K=1 K=1 K=1 K=1 K=1 K=1 K=1 K=1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>• The mapping should cover all the iteration vectors defined by the problem.</p><p>Walk-through Example. Fig. <ref type="figure" target="#fig_4">5</ref>(d) shows a Union mapping and Fig. <ref type="figure" target="#fig_4">5</ref>(e) describes the mapping using the loop nest representation. A Union mapping can describe multi-level clusters of multi-dimensional PE arrays precisely and specify temporal and spatial tiling of data at each level. Also, one can distribute different problem dimensions at the same time in each cluster level over sub-clusters, i.e. there is no temporal ordering between spatial fors in the same cluster level. A complex example mapping for a small CONV2D operation using a flexible accelerator (such as MAERI <ref type="bibr" target="#b6">[7]</ref>) is illustrated in Fig. <ref type="figure" target="#fig_6">6</ref> and the corresponding Union mapping and loop nest representation are shown in Fig. <ref type="figure">7</ref>. In Fig. <ref type="figure" target="#fig_6">6</ref> We call this mapping as a K YR XS partitioned mapping to show the parallelism. Each C 2 cluster is assigned for a row of a channel of a filter and a row of a channel of a input activation and each column in the row will be processed in the C 1 clusters concurrently. Each C 3 cluster is assigned for a channel of a filter and the corresponding input activations. As a result, inputs and outputs are reused between time step 1 and 2 while fetching different filters from the upper memory levels. Between time step 2 and 3, a part of input activations are reused in MAC units and others are being fetched from the upper memory levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Constraint File</head><p>In addition to Union abstractions, a user can also provide constraints derived from a specific accelerator, such as feasible tile sizes, loop orders, parallelizing dimensions, and aspect ratio. Such constraints provide the framework extra rules to eliminate illegal mappings and/or prune the mapping space for specific accelerators. For example, to describe a fully flexible accelerator like MAERI, the user will not provide constraint file to describe the hardware. On the other hand, a NVDLAstyle <ref type="bibr" target="#b3">[4]</ref> architecture can be realized by having a constraint file that forces parallelization on dimensions C and K for a convolution operation with a fixed aspect ratio. Furthermore, a user can set some constraints to prune the map space based on min/max PE utilization or specific loop orders or tile sizes that the user wants to explore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CASE STUDIES USING UNION</head><p>In this section, we show three case studies for algorithm exploration, mapping exploration, and hardware exploration to illustrate how Union can be used by domain experts, compiler researchers, and hardware architects, respectively. We evaluate two types of accelerators, edge and cloud, as shown in Table <ref type="table">V</ref>. We assume 1GHz as the clock frequency and 8 bits for the wordsize with uint8 MAC units. In Union, we directly use Timeloop and MAESTRO, which are already validated against RTL for different existing accelerators. Thus, the validation of the performance numbers is dependent on the fidelity of the underlying cost models.</p><p>We choose tensor contractions from the TCCG benchmark suite <ref type="bibr" target="#b39">[40]</ref>, using the reference problem sizes. The input sets are taken from different domains, including machine learning, molecular dynamics, and quantum chemistry. We use 16, 32, 64 as the Tensor Dimension Sizes (TDS) and assume that every dimension has the size as TDS in a TC problem instance. We use a few representative DNN layers from the MLPerf benchmark including ResNet50 for computer vision, DLRM for recommendation, and BERT for natural language processing. We use N as a batch size, and K, C, X, Y, S, R, NIN, NON as the number of filters, input channels, input cols, intput rows, filter columns, filter rows, input neurons, output neurons. We summarize the TC and DNN workloads that we use in the case studies in Table <ref type="table" target="#tab_5">III</ref> and Table <ref type="table" target="#tab_0">IV</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Algorithm Exploration</head><p>A single tensor operation can be computed via several algorithms. The Union-frontend determines whether to run an operation natively, or transform it to other operations, depending on which algorithm provides better performance on the accelerator. We demonstrate this use case using tensor contraction running on a cloud type 2D spatial accelerator via two algorithms: (1) running natively and (2) running through TTGT. We use the Timeloop cost model and a mapper based    on both heuristic and random sampling. We use the cloud configuration in Table V with 32×64 as the aspect ratio of the accelerator to balance the parallelism across rows and columns. Note that for TTGT cost estimation, the cost model only estimates the cost of the GEMM operation assuming that the cost of transpose operations would not be significant. Since TTGT does not incur duplicated elements of the original tensors, the memory footprint for both running TC natively and running TC with TTGT have the same memory footprint. Fig. <ref type="figure" target="#fig_7">8</ref> plots the Energy-Delay-Product (EDP) for three tensor contractions with tensor dimensions 16 and 64 on the cloud accelerator. We observe that the lower EDP is achieved when running with TTGT for all cases with TDS=16. This is because running natively will under-utilize the available compute units since the target accelerator has 32×64 PEs while the each tensor dimension has size of 16. For example, Fig. <ref type="figure">9</ref> shows the mappings generated from Union for Intensli2. In Fig. <ref type="figure">9</ref>(a) C 3 level, we observe that the optimal mapping found by Union distributes the problem dimension A across 16 C 2 s and distributes the dimension E across 16 C 1 s, resulting in utilizing 256 PEs with A E partitioned mapping. In Fig. <ref type="figure">9</ref>(b), the optimal mapping with GEMM distributes K across 16 C 2 s and distributes M across 64 C 1 s, resulting in utilizing 1024 PEs with K M partitioned mapping.</p><formula xml:id="formula_4">Dimensions ResNet50-1 N=32 K=C=64 X=Y=56 R=S=1 ResNet50-2 N=32 K=C=64 X=Y=56 R=S=3 ResNet50-3 N=32 K=512 C=1024 X=Y=14 R=S=1 DLRM-1 N=512 NIN=1024 NON=1024 DLRM-2 N=512 NIN=1024 NON=64 DLRM-3 N=512 NIN=2048 NON=2048 BERT-1 N=256 NIN=768 NON=768 BERT-2 N=256 NIN=3072 NON=768 BERT-3 N=256 NIN=768 NON=3072</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Mapping Exploration</head><p>Flexible accelerators like Eyeriss v2 <ref type="bibr" target="#b18">[19]</ref> and MAERI <ref type="bibr" target="#b6">[7]</ref> can logically configure to different aspect ratios for the under-   MAESTRO cost model as it has support to model such flexible accelerators. The flexibility in aspect ratios gets captured by allowing cluster sizes to be variable. In the Union constraint file, we specify different cluster sizes to explore different aspect ratio. We evaluate the DNN workloads shown in Table IV using different aspect ratio for the edge (1×256, 2×128, 4×64, 8×32, and 16×16) and cloud accelerators (1×2048, 2×1024, 4×512, 8×256, 16×128, and 32×64). Each aspect ratio corresponds to a configuration of the flexible accelerator. Fig. <ref type="figure" target="#fig_0">10</ref> plots the EDP. We observe that the EDP gets saturated once it maximizes the PE utilization after the mapper finds the optimal tile sizes and loop orders to maximize the data reuse. Even though the balanced aspect ratio showed the best performance for most of the cases that we evaluate, this can be sub-optimal if the workload is unbalanced. For example, GEMM with 4×2048 or 2048×2 or 4×2 will be able to fully utilize an accelerator with 1×2048 aspect ratio by parallelizing K dimension while 32×64 accelerator will be underutilized. This is where Union's cluster-centric approach to describe mappings helps as it enables mapping the same workload dimensions to different spatial dimensions to fully exploit the available parallelism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Hardware Exploration</head><p>In our last case study, we study the impact of chipletization on an accelerator's performance. Multi-chiplet based architectures are gaining popularity as they can reduce manufacturing cost and provide scalabililty. NVIDIA's Simba <ref type="bibr" target="#b17">[18]</ref> is a recent example. However, the inter-chiplet network is more expensive than on-chip network resulting in lower bandwidth and higher energy. For this case study, we use an accelerator which is composed of 16 chiplets, and each chiplet has the same configuration with the edge accelerator in Table <ref type="table">V</ref>. The total number of PEs are equal to 4096. We study the impact of the interconnect bandwidth by varying the fill bandwidth of the global buffer in each chiplet, i.e. the bandwidth from DRAM to the global buffer in each chiplet. We use Timeloop for this case study as it can model hierarchical architectures like Simba and also comes with the Accelergy <ref type="bibr" target="#b40">[41]</ref> energy model for accurately estimating on-chip versus on-package energy.</p><p>Fig. <ref type="figure" target="#fig_0">11</ref> plots our results. For all models, we observe that EDP drops rapidly with the increase in fill bandwidth when the fill bandwidth is low, and it gets saturated once the fill bandwidth is sufficient so that it is not bounded by the fill bandwidth. According to the result, different layers get saturated in the different fill bandwidth depending on the available data reuse. We also observe that ResNet-2 gets saturated when fill bandwidth is 2GB/s while others get saturated between 6 -12 GB/s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this work, we propose Union, a unified framework for evaluating tensor operations on spatial accelerators. Our MLIR based framework allows to map both HPC and ML tensor operations using multiple mappers to multiple cost models for spatial accelerators. The three case studies presented demonstrate the flexibility of the framework by evaluating very different operations, mappings, and hardware features with a single framework. While the number of operations, mappings and accelerators are currently limited to what we have demonstrated here, we plan to extend to other kernels such as tensor decomposition, other accelerators and mappers in the near future. There are also advanced features that can be added to Union abstractions to support fused operations, sparsity-aware accelerator cost models, and unimodular/polyhedral mappings, but we leave those as a future work. The modular framework allows us to add such changes without requiring a redesign of the whole software stack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. ACKNOWLEDGMENT</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 :</head><label>1</label><figDesc>A loop nest for a CONV2D Operation Input: IA: An input activation with [n][c][x][y] OA: An output activation with [k][c][x ][y ] F: An array of filters with [k][c][r][s] stride: Stride for sliding windows 1 for n = 0 to N-1 do 2 for k = 0 to K-1 do 3 for x = 0 to (X-R) / stride do 4 for y = 0 to (Y-S) / stride do 5 for c = 0 to C-1 do 6 for r = 0 to R-1 do 7 for s = 0 to S-1 do 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Union overview. The pink boxes indicate the inputs of Union while green boxes are showing how the codes are getting lowered. Rectangles and arrows with dotted lines are out of the scope of this paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Normalized energy consumption and latency with EDP for different mappings of a layer from DLRM on a 3-level spatial architecture with 16×16 PE array.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. An example of a memory-target loop-centric mapping and its interpretations on a 8×8 2D PE array.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Union abstractions to describe a GEMM problem with a mapping on an accelerator which is composed of a simple 2D PE array. (a) describes a Union problem for a GEMM problem and (b) describes the Union architecture of the target architecture shown in (c). (d) shows a Union mapping that shows how to map the data to the architecture to run the GEMM problem. (e) represents the Union mapping in loop nest form.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>K=1 ⋯ 2 ⋯ 6 N=1 K=1 ⋯ 4 C=1</head><label>264</label><figDesc>X=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. The visualization of a K YR XS partitioned mapping for CONV2D using with 18 PEs for 3 time steps. A cluster containing DRAM, C 5 , is not shown here. A red box indicates a MAC unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Three tensor contraction examples with different dimensions using different algorithms (native and TTGT) on a cloud accelerator. We explore Tensor Dimension Sizes (TDS) with 16 and 64 for intensli2 and ccsd7, and 16 and 32 for ccsd-t4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>intensli2 C[a, b, c, d] = A[d, b, e, a] × B[e, c] a = b = c = d = e = 64 a = b = c = d = e = 16 M = 262144, N = 64, K = 64 M = 4096, N = 16, K = 16 ccsd7 C[a, b, c] = A[a, d, e, c] × B[e, b, d] a = b = c = d = e = 64 a = b = c = d = e = 16 M = 4096, N = 64, K = 4096 M = 256, N = 16, K = 256 ccsd-t4 C[a, b, c, d, e, f] = A[d, f, g, b] × B[g, e, a, c] a = b = c = d = e = f = g = 32 a = b = c = d = e = f = g = 16 M = 32768, N = 32768, K = 32 M = 4096, N = 4096, K = 16</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .Fig. 10 .Fig. 11 .</head><label>91011</label><figDesc>Fig. 9. Generated mappings from Union for intensli2 using different algorithms with tensor dimension sizes as 16. The orders of dimensions in tile sizes are ABCDE and MNK for the mappings in (a) and (b) respectively. Blue tilesizes show the spatial distribution while red tilesizes show the temporal distribution for the dimension.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF OUR FRAMEWORK UNION WITH OTHER EXISTING FRAMEWORKS.</figDesc><table><row><cell>Framework</cell><cell>Target hardware</cell><cell>Cost models</cell><cell>Mappers</cell><cell>Operation abstraction</cell><cell>Mapping abstraction</cell><cell>Hardware abstraction</cell><cell>Integration with high-level frameworks</cell><cell>Target usecase</cell></row><row><cell>AutoSA [23]</cell><cell>Systolic</cell><cell>Custom</cell><cell>Custom</cell><cell>Polyhedral models</cell><cell>Space-Time projections</cell><cell>Custom format</cell><cell>N/A</cell><cell>Co-design</cell></row><row><cell>MAESTRO [10]</cell><cell>Spatial</cell><cell>Generic</cell><cell>Marvel, GAMMA</cell><cell>Fixed operations</cell><cell>Cluster-target Data-centric</cell><cell>3-level accelerators</cell><cell>Custom parser (ML) from TF, PyTorch</cell><cell>Co-design</cell></row><row><cell>Timeloop [11]</cell><cell>Spatial</cell><cell>Generic</cell><cell>Mind Mapping, Random-based, Brute-force</cell><cell>Nested loops</cell><cell>Memory-target Loop-centric</cell><cell>Hierarchical</cell><cell>Custom parsers from TF (ML)</cell><cell>Co-design</cell></row><row><cell>Interstellar [12]</cell><cell>Spatial</cell><cell>Generic</cell><cell>Heuristics</cell><cell>Fixed operations</cell><cell>Halide scheduling</cell><cell>3-level accelerators</cell><cell>N/A</cell><cell>Co-design</cell></row><row><cell>XLA</cell><cell>TPU</cell><cell>Custom</cell><cell>Custom</cell><cell>LHLO</cell><cell>LHLO</cell><cell>Specific to TPU</cell><cell>TF (ML)</cell><cell>Compilation</cell></row><row><cell>ZigZag [24]</cell><cell>Spatial</cell><cell cols="2">Generic Heuristics, LOMA</cell><cell>Nested loops</cell><cell>Memory-target Loop-centric</cell><cell>Hierarchical</cell><cell>N/A</cell><cell>Co-design</cell></row><row><cell>XLA</cell><cell>TPU</cell><cell>Custom</cell><cell>Custom</cell><cell>LHLO</cell><cell>LHLO</cell><cell>Specific to TPU</cell><cell>TF (ML)</cell><cell>Compilation</cell></row><row><cell>TVM [25]</cell><cell>Specific (e.g., VTA)</cell><cell>Generic</cell><cell>Annealing</cell><cell>TVM statements</cell><cell>TVM scheduling</cell><cell>Specific to target</cell><cell>TF, ONNX (ML)</cell><cell>Compilation</cell></row><row><cell>Union</cell><cell>Spatial</cell><cell>Generic</cell><cell>Unified</cell><cell>Nested loops</cell><cell>Cluster-target Loop-centric</cell><cell>Hierarchical</cell><cell>TF (ML), COMET (HPC)</cell><cell>Co-design</cell></row></table><note>corresponding loop nest is shown in algorithm 2.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>, the right column shows which data dimension values are mapped onto each cluster. Clusters with solid lines (C 4 and C 1 ) have the dedicated memory in the level while clusters with dotted lines (C 3 and C 2 ) do not. We use L2 and L1 to indicateFig. 7. A Union mapping of a K YR XS partitioned mapping and its loop nest representation. The order of dimensions in tile sizes is NKCYXRS. the memory in C 4 and C 1 and V2 and V1 to indicate the virtual (imaginary) memory in C 3 and C 2 . The left column of Fig. 6 visualizes the mapped elements in input activation, filter, and output activation. Purple elements are mapped onto MAC units at each time step. In this mapping, dimension K is spatially distributed across C 3 clusters, Y and R are spatially distributed together across C 2 , and X and S are spatially distributed together across C 1 .</figDesc><table><row><cell cols="2">// C5: DRAM to L2 target_cluster: C5 temporal_order: NKCYXRS temporal_tile_sizes: 1, 4, 1, 3, 6, 3, 3 spatial_tile_sizes: 1, 4, 1, 3, 6, 3, 3</cell><cell>// C5: DRAM to L2 for tn4 = 1 … (2/1) for tc4 = 1 … (3/1) spatial_for sk3 = 1</cell></row><row><cell cols="2">// C4: L2 to V2 target_cluster: C4 temporal_order: NKCYXRS temporal_tile_sizes: 1, 4, 1, 3, 6, 3, 3</cell><cell>// C4: L2 to V2 for tk2 = 1 spatial_for sk2 = 1 … (4/2)</cell></row><row><cell>spatial_tile_sizes:</cell><cell>1, 2, 1, 3, 6, 3, 3</cell></row><row><cell cols="2">// C3: V2 to V1 target_cluster: C3 temporal_order: NKCYXRS temporal_tile_sizes: 1, 2, 1, 3, 3, 3, 3</cell><cell>// C3: V2 to L1 for tx1 = 1 … (6/3) spatial_for sy1 = 1 … (3/1) spatial_for sr1 = 1 … (3/1)</cell></row><row><cell>spatial_tile_sizes:</cell><cell>1, 2, 1, 1, 3, 1, 3</cell></row><row><cell cols="2">// C2: V1 to L1 target_cluster: C2 temporal_order: NKCYXRS temporal_tile_sizes: 1, 2, 1, 1, 3, 1, 3</cell><cell>// C2: V1 to L1 for tk0 = 1 spatial_for sx0 = 1 … (3/1) spatial_for ss0 = 1 … (3/1)</cell></row><row><cell>spatial_tile_sizes:</cell><cell>1, 2, 1, 1, 1, 1, 1</cell></row><row><cell cols="2">// C1: L1 to MAC target_cluster: C1 temporal_order: NKCYXRS temporal_tile_sizes: 1, 1, 1, 1, 1, 1</cell><cell>// C1: L1 to MAC for tk0 = 1 … (2/1) spatial_for sx0 = 1 spatial_for ss0 = 1</cell></row><row><cell>spatial_tile_sizes:</cell><cell>1, 1, 1, 1, 1, 1</cell></row><row><cell cols="2">(a) Union mapping for a K_YR_XS mapping</cell><cell>(b) Loop nest representation</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III TENSOR</head><label>III</label><figDesc>CONTRACTION PROBLEMS AND THE CORRESPONDING GEMM DIMENSION SIZES FOR TTGT</figDesc><table><row><cell>Name</cell><cell>Equation</cell><cell>Tensor Dimension Sizes</cell><cell>GEMM Dimension Sizes</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We thank Ruiqin Tian at PNNL for her help with the COMET compiler. We also thank Hyoukjun Kwon, Clayton Hughes, Mark Plagge, Juan Escobedo, Rizwan Ashraf for insightful comments and discussions on this work. We also thank the anonymous reviewers for their valuable feedback. Support for this work was provided through U.S. Department of Energy's (DOE) Office of Advanced Scientific Computing Research as part of the Center for Artificial Intelligencefocused Architectures and Algorithms. PNNL is operated by Battelle for the DOE under Contract DE-AC05-76RL01830. Sandia National Laboratories is a multimission laboratory managed and operated by National Technology and Engineering Solutions of Sandia, LLC., a wholly owned subsidiary of Honeywell International, Inc., for the U.S. Department of Energy's National Nuclear Security Administration under contract DE-NA-0003525.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borchers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>-L. Cantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Coriell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gelb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">V</forename><surname>Ghaemmaghami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gottipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gulland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hagmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hogberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaffey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaworski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khaitan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Killebrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lundin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mackean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maggiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Narayanaswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Norrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Penukonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Phelps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Salek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Samadiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Severn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sizikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Snelham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Souter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Steinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Thorson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Toma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tuttle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture</title>
				<meeting>the 44th Annual International Symposium on Computer Architecture<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Accelerating dnns with xilinx alveo accelerator cards</title>
		<ptr target="https://www.xilinx.com/support/documentation/whitepapers/wp504-accel-dnns.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Scalable Multi-TeraOPS Deep Learning Processor Core for AI Trainina and Inference</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Fleischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Symposium on VLSI Circuits</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="35" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The nvidia deep learning accelerator (nvdla)</title>
		<ptr target="http://nvdla.org/hw/v1/ias/programmingguide.html" />
		<imprint/>
		<respStmt>
			<orgName>NVIDIA</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International Symposium on Computer Architecture</title>
				<meeting>the 43rd International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="367" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Shidiannao: Shifting vision processing closer to the sensor</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fasthuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ienne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="92" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Maeri: Enabling flexible dataflow mapping over dnn accelerators via reconfigurable interconnects</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Samajdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
				<meeting>the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A systematic methodology for characterizing scalability of dnn accelerators using scale-sim</title>
		<author>
			<persName><forename type="first">A</forename><surname>Samajdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Whatmough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="58" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Muñoz-Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Abellán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Acacio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07137</idno>
		<title level="m">STONNE: A Detailed Architectural Simulator for Flexible Neural Network Accelerators</title>
				<imprint>
			<date type="published" when="2020-06">Jun. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Understanding reuse, performance, and hardware cost of dnn dataflow: A data-centric approach</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chatarasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pellauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Timeloop: A systematic approach to dnn accelerator evaluation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mukkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Khailany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="304" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Interstellar: Using halide&apos;s scheduling language to analyze dnn accelerators</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Setter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
				<meeting>the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Marvel: A data-centric compiler for dnn operators on spatial accelerators</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chatarasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Haridas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pellauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sarkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07752</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dmazerunner: Executing perfectly nested loops on dataflow accelerators</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avancha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Embed. Comput. Syst</title>
		<imprint>
			<date type="published" when="2019-10">Oct. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gamma: Automating the hw mapping of dnn models on accelerators via genetic algorithm</title>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Computer-Aided Design (ICCAD)</title>
				<meeting>the 39th International Conference on Computer-Aided Design (ICCAD)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evaluating spatial accelerator architectures with tiled matrix-matrix multiplication</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chatarasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajamanickam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Loma: Fast auto-scheduling on dnn accelerators through loop-order-based memory allocation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Symons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verhelst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Simba: Scaling deep-learning inference with multi-chip-module-based architecture</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clemons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fojtik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klinefelter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pinckney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="14" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Eyeriss v2: A flexible accelerator for emerging deep neural networks on mobile devices</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Emerging and Selected Topics in Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="292" to="308" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Buffets: An efficient and composable storage idiom for explicit decoupled data orchestration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pellauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clemons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Crago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="137" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mlir: Scaling compiler infrastructure for domain specific computation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lattner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Bondhugula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pienaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Riddle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shpeisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zinenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CGO</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Comet: A domain-specific compilation of high-performance computational chemistry</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gioiosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pienaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kestor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Languages and Compilers for Parallel Computing (LCPC&apos;20)</title>
				<imprint>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Autosa: A polyhedral compiler for high-performance systolic arrays on fpga</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2021 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, ser. FPGA &apos;21</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Zigzag: Enlarging joint architecture-mapping design space exploration for dnn accelerators</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Houshmand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Giraldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verhelst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1160" to="1174" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Tvm: An automated end-to-end optimizing compiler for deep learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th USENIX Conference on Operating Systems Design and Implementation, ser. OSDI&apos;18</title>
				<meeting>the 13th USENIX Conference on Operating Systems Design and Implementation, ser. OSDI&apos;18</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="579" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0759</idno>
		<title level="m">cudnn: Efficient primitives for deep learning</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A fifth-order perturbation comparison of electron correlation theories</title>
		<author>
			<persName><forename type="first">K</forename><surname>Raghavachari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Trucks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Pople</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Head-Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical Physics Letters</title>
		<imprint>
			<biblScope unit="volume">157</biblScope>
			<biblScope unit="page" from="479" to="483" />
			<date type="published" when="1989-05">05 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">E</forename><surname>Aprà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Bylaska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Jong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Govind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kowalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Straatsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J J</forename><surname>Van Dam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Alexeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Anchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Anisimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">W</forename><surname>Aquino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Atta-Fynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Autschbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Bauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Becca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Bernholdt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bhaskaran-Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bogatko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Borowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boschen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brabec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cauët</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename><surname>Chuev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Cramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Daily</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J O</forename><surname>Deegan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H D</forename><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dupuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Dyall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>Fann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fonari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Früchtl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gagliardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Garza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gawande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Glaesemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Götz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Helms</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Hermes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hirata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jacquelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jónsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Klemm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Konkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Lins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Littlefield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Logsdail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lopata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Marenich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Del Campo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mejia-Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mullin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nieplocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tipparaju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Truhlar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsemekhman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">V</forename><surname>Voorhis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vázquez-Mayagoitia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Villa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vishnu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Vogiatzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Weare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Windus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Woliński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zacharias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Harrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NWChem: Past, Present, and Future</title>
		<editor>B. Palmer, A. Panyala, T. Pirojsirikul, B. Peng, R. Peverati, J. Pittner, L. Pollack, R. M. Richard, P. Sadayappan, G. C. Schatz, W. A. Shelton, D. W. Silverstein, D. M. A. Smith, T. A. Soares, D. Song, M. Swart,</editor>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page">184102</biblScope>
			<date type="published" when="2020-05">May 2020</date>
		</imprint>
	</monogr>
	<note>Journal of Chemical Physics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">From NWChem to NWChemEx: Evolving with the computational chemistry landscape</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kowalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Bauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Boschen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Bylaska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Daily</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Jong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T H</forename><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Govind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kec ¸eli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keipert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Panyala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Straatsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sushko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Valeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J J</forename><surname>Van Dam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Waldrop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Williams-Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Windus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical Reviews</title>
		<imprint/>
	</monogr>
	<note>accepted for publication</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Nvidia tesla v100 gpu architecture</title>
		<ptr target="https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sigma: A sparse and irregular gemm accelerator with flexible interconnects for dnn training</title>
		<author>
			<persName><forename type="first">E</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Samajdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nadella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="58" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tetris: Scalable and efficient neural network acceleration with 3d memory</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="751" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mind mappings: Enabling efficient algorithm-accelerator mapping space search</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Fletcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
				<meeting>the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">How to evaluate deep neural network processors: Tops/w (alone) considered harmful</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">IEEE Solid-State Circuits Magazine</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="28" to="41" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">IREE: Intermediate representation execution environment</title>
		<ptr target="https://github.com/google/iree" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">NPComp -mlir based compiler toolkit for numerical python programs</title>
		<ptr target="https://github.com/llvm/mlir-npcomp" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Tensor operator set architecture (tosa) dialect</title>
		<ptr target="https://mlir.llvm.org/docs/Dialects/TOSA/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A high-performance sparse tensor algebra compiler in Multi-Level IR</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kestor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05187</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep learning recommendation model for personalization and recommendation systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Azzolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dzhulgakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mallevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cherniavskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kondratenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1906.00091" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1906">1906.00091. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Design of a high-performance gemm-like tensor-tensor multiplication</title>
		<author>
			<persName><forename type="first">P</forename><surname>Springer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bientinesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Softw</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Accelergy: An architecturelevel energy estimation methodology for accelerator designs</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/ACM International Conference on Computer-Aided Design</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
