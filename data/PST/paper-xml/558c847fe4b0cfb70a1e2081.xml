<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revisiting Common Bug Prediction Findings Using Effort-Aware Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yasutaka</forename><surname>Kamei</surname></persName>
							<email>kamei@cs.queensu.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="laboratory">Software Analysis and Intelligence Lab (SAIL)</orgName>
								<orgName type="institution">Queen&apos;s University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of Engineering</orgName>
								<orgName type="institution">Kobe University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shinsuke</forename><surname>Matsumoto</surname></persName>
							<email>shinsuke@cs.kobe-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="laboratory">Software Analysis and Intelligence Lab (SAIL)</orgName>
								<orgName type="institution">Queen&apos;s University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of Engineering</orgName>
								<orgName type="institution">Kobe University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Akito</forename><surname>Monden</surname></persName>
							<email>akito-m@is.naist.jp</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="laboratory">Software Analysis and Intelligence Lab (SAIL)</orgName>
								<orgName type="institution">Queen&apos;s University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of Engineering</orgName>
								<orgName type="institution">Kobe University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Graduate School of Information Science</orgName>
								<orgName type="institution">Nara Institute of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ken-Ichi</forename><surname>Matsumoto</surname></persName>
							<email>matumoto@is.naist.jp</email>
							<affiliation key="aff2">
								<orgName type="department">Graduate School of Information Science</orgName>
								<orgName type="institution">Nara Institute of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bram</forename><surname>Adams</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="laboratory">Software Analysis and Intelligence Lab (SAIL)</orgName>
								<orgName type="institution">Queen&apos;s University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of Engineering</orgName>
								<orgName type="institution">Kobe University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ahmed</forename><forename type="middle">E</forename><surname>Hassan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="laboratory">Software Analysis and Intelligence Lab (SAIL)</orgName>
								<orgName type="institution">Queen&apos;s University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of Engineering</orgName>
								<orgName type="institution">Kobe University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Revisiting Common Bug Prediction Findings Using Effort-Aware Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D3C3ABFD4F82FC5AF4E2383068CE7A6F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Bug prediction models are often used to help allocate software quality assurance efforts (e.g. testing and code reviews). Mende and Koschke have recently proposed bug prediction models that are effort-aware. These models factor in the effort needed to review or test code when evaluating the effectiveness of prediction models, leading to more realistic performance evaluations. In this paper, we revisit two common findings in the bug prediction literature: 1) Process metrics (e.g., change history) outperform product metrics (e.g., LOC), 2) Packagelevel predictions outperform file-level predictions. Through a case study on three projects from the Eclipse Foundation, we find that the first finding holds when effort is considered, while the second finding does not hold. These findings validate the practical significance of prior findings in the bug prediction literature and encourage their adoption in practice.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Software quality assurance activities (e.g., source code inspection and unit testing) are becoming increasingly important as software systems are being widely used in our society. Software faults (or bugs) in released products have expensive consequences for a company and affect its reputation. Since a company has only limited resources (e.g., developers and cost) for software quality assurance activities, these activities have to be performed as efficiently as possible.</p><p>To prioritize quality assurance efforts, fault prediction techniques are often used to prioritize modules based on their probability of having a fault or the number of expected faults <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>. With these models, practitioners can allocate limited testing or reviewing efforts to the most fault-prone modules.</p><p>However, as pointed out by Mende and Koschke <ref type="bibr" target="#b3">[4]</ref>, traditional prediction models <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b9">[10]</ref> typically ignore the effort needed to fix bugs, i.e., they do not distinguish between a predicted bug in a small module and a predicted bug in a large module. Clearly, both bugs require a different amount of effort to inspect and fix, yet both are considered equal when measuring the effectiveness of prediction models.</p><p>Mende and Koschke <ref type="bibr" target="#b10">[11]</ref> proposed effort-aware models that include the notion of effort. In their study, they use lines of code as a proxy for effort. An experimental result using publicly-available data sets shows that the prediction performance of effort-aware models improved from a cost-effectiveness point, compared to no effort-aware models (i.e., traditional prediction models).</p><p>Since effort-aware prediction models offer a totally new interpretation and the practical adoption-oriented view of bug prediction results, it is necessary to reconsider some of the major findings in the fault prediction literature (e.g., <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b14">[15]</ref>) by taking into account effort. In particular, we are interested in addressing the following two research questions: RQ1 Are process metrics still more effective than product metrics in effort-aware models? It is a known fact that process metrics are more efficient fault predictors than product metrics <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref>. Since, Mende and Koschke <ref type="bibr" target="#b10">[11]</ref> only used product metrics, in their effort-aware models, it is not clear whether process metrics still outperform product metrics when considering effort. RQ2 Are package-level predictions still more effective than file-level predictions?</p><p>Traditionally, package-level bug prediction models have been shown to have higher precision and recall than file-level bug prediction models <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. We examine whether or not effort-aware models at the package-level are still as effective. This paper provides the following contributions:</p><p>• We show that process metrics still outperform product metrics using effort-aware bug prediction models.</p><p>• We show that package-level predictions are not more effective than file-level predictions. This finding holds even when considering Martin's package design metrics <ref type="bibr" target="#b16">[17]</ref>.</p><p>• We show that the effectiveness of package-level predictions can improve if we perform our predictions at the file-level then lift it to the package-level instead of collecting all metrics at the package-level. However this new model still does not outperform file-level predictions when considering the quality assurance efforts. In what follow, Section II introduces related work. Section III provides the design of our experiment, and Section IV gives the results. Section V discusses the performance differences between package-level and file-level predictions. Section VI presents the threats to validity and Section VII summarizes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we discuss related work in fault prediction models and effort-aware models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Fault prediction models</head><p>Many fault prediction models have been proposed in literature <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref>. Zimmermann and Nagappan <ref type="bibr" target="#b9">[10]</ref> have introduced the use of network analysis on dependency graphs for fault module prediction. They conducted an experiment using industrial datasets. The result of their experiment showed that network measures could identify 60% of the files developers considered as critical. Mizuno and Kikuno <ref type="bibr" target="#b7">[8]</ref> applied a generic text discriminator (i.e., as Spam filter) to predict faults. They showed that their approach could classify 78% of the actual faulty files as fault-prone. Kim et al. <ref type="bibr" target="#b17">[18]</ref> proposed a new bug prediction technique that works at the granularity of an individual file level change. Their classifier is trained using features (e.g., terms in the added delta source code and terms in the change log) extracted from a version archive such as CVS. They showed that their approach could classify the files as buggy or not, with a 78 percent accuracy. Ratzinger et al. <ref type="bibr" target="#b18">[19]</ref> introduced non-refactoring and refactoring related features to predict faults with high performance. Our study evaluates the effect of well-known fault prediction models at the package-level instead of at the file-level.</p><p>There are also several studies on evaluation methods for prediction models <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Lessmann et al. <ref type="bibr" target="#b20">[21]</ref> proposed a framework for comparative software fault classification (i.e., fault-prone or not). They considered three sources for bias, such as, relying on accuracy indicators that are conceptually inappropriate for software fault prediction and cross-study comparisons. The AUC (Area Under the receiver operating characteristics Curve) was recommended as the primary accuracy indicator for comparative studies. More recently, Mende and Koschke <ref type="bibr" target="#b3">[4]</ref> introduced a method for comparative fault density prediction based on the cost of quality assurance activities. In our study, the experiments for our two research questions are evaluated based on Mende and Koschke's method.</p><p>Some studies evaluated the performance of fault prediction models for package-level modules. Nagappan et al. <ref type="bibr" target="#b22">[23]</ref> predicted the likelihood of post-release faults at the package-level using a regression model and principal component analysis. Schröter et al. <ref type="bibr" target="#b14">[15]</ref> also predicted the number of faults at the package-level. In contrast to those studies, we evaluate the effect of prediction models on the costs of software quality assurance activities. Also, while these studies used a single technique for the prediction of package, such as lifting the file-level metrics up to the package-level, our study uses three techniques, in order to study the impact of these techniques on our findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Effort-Aware Models</head><p>Many fault prediction studies evaluate the performance of prediction models that classify a module into fault-prone or not fault-prone <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Test managers and quality managers identify fault-prone modules using a prediction model and allocate more test efforts to the modules that are detected to fault-prone.</p><p>However, the prediction model would not be effective because the costs of quality assurance activities are largely ignored. Conventional studies evaluate the prediction performance based on the assumption that the effort of test/inspection is the same across modules. The assumption is rarely true in many cases. Arisholm et al. <ref type="bibr" target="#b23">[24]</ref> pointed out that the effort of testing or reviewing a module is roughly likely to be proportional to the size.</p><p>This paper evaluates the performance of prediction models in terms of effort as inspired by Mende and Koschke's study <ref type="bibr" target="#b10">[11]</ref>. Their model considers the effort required to review a module (i.e., file or package) and predicts the relative risk R dd (x) of a module:</p><formula xml:id="formula_0">R dd (x) = #errors(x) E(x) ,<label>(1)</label></formula><p>where #errors(x) is the number of errors in a module and E(x) is defined as the effort required to test or inspect a module x. In this study, we use the lines of code as a measure of effort, similar to Mende and Koschke <ref type="bibr" target="#b10">[11]</ref>.</p><p>We use the P opt evaluation metrics <ref type="bibr" target="#b3">[4]</ref> to evaluate the prediction performance of models. P opt is defined as the area ∆ opt between the LOC-based cumulative lift charts of the optimal model and the prediction model (Figure <ref type="figure" target="#fig_0">1</ref>). In the optimal model, all modules are ordered by the decreasing actual fault density. While in the predicted model, all modules are ordered by decreasing predicted fault density. As shown in the following equation, a larger P opt value means a smaller difference between the optimal and predicted model.</p><formula xml:id="formula_1">P opt = 1 -∆ opt<label>(2)</label></formula><p>However, the minimum value of P opt depends on the number of bugs in our dataset. In this study, we used a normalized value based on the following equation:</p><formula xml:id="formula_2">Norm(P opt ) = P opt -min(P opt ) max(P opt ) -min(P opt ) ,<label>(3)</label></formula><p>where max(P opt ) and min(P opt ) are calculated on the LOCbased cumulative lift chart in which all modules are ordered by fault density.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENT SETTING</head><p>In this section, we describe our experiment setting. We describe the modeling techniques and the data sources. We explain the recovery of bugs from software repositories. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Used Modeling Techniques</head><p>We use three well-known modeling techniques; regression model <ref type="bibr" target="#b24">[25]</ref>, regression tree <ref type="bibr" target="#b25">[26]</ref> and random forest <ref type="bibr" target="#b26">[27]</ref>. We used the statistical computing and graphics toolkit R <ref type="bibr" target="#b27">[28]</ref> and its MASS, rpart and randomForest libraries to build the three models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Used systems</head><p>The target of our study is three subprojects within the Eclipse software system, one of the best-known open development platforms. We collected module (i.e., file and package) datasets from three versions (v.3.0, v.3.1 and v.3.2) in each subproject (Platform, JDT and PDE) from the Eclipse CVS repository. We consider a Java file as file-level and a package as package-level. For example, in case of /org/eclipse/jdt/core/ElementChangedEvent.java, the file is ElementChangedEvent.java and the package is org.eclipse.jdt.core. Table <ref type="table">I</ref> summarizes the statistics of the used datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Recovery of bugs</head><p>To obtain the number of bugs in source code files, we implemented the SZZ algorithm <ref type="bibr" target="#b28">[29]</ref>. This algorithm identifies when a bug was injected into the code and who injected it by linking a version archive (such as CVS) to a bug tracking system (such as Bugzilla). The SZZ algorithm basically consists of three steps. First step, identifying the commit that fixes a bug, SZZ searches for keywords such as "Fixed" or "Bug" in the CVS comments. We used "bug", "fix", "defect" and "patch" as keywords and identified the commit that had the keyword and a digit number (e.g., bug 12345) as a bug-fix commit. The second step confirms whether that commit is really a bug-fix commit using information from Bugzilla. We link the digit number of the CVS comments to the bug number of Bugzilla.</p><p>The commit is more likely to be a bug-fix commit if the author of the commit has been assigned to the identified bug report.</p><p>Other, heuristics to confirm whether a commit is really a bugfix commit are discussed where <ref type="bibr" target="#b28">[29]</ref>. The third step identifies when the bug is introduced, we use the CVS diff and annotate command (Figure <ref type="figure">2</ref>). We locate fixed lines (e.g., line #3) of the bug-fix commit (rev. C) and original lines (e.g., line #3) of the previous commit (rev. B) using the diff command. We identify the most recent revision (rev. A) in which the original lines were changed using the annotate command. We consider the commit of the identified revision as the bug-introducing commit.</p><p>When we identify the dates of the introduction and fixing of a bug using the SZZ algorithm, we count that file A has one bug in v3.1 but file B has no bug in v3.1 because the bug of file B is already fixed before v3.1 (Figure <ref type="figure" target="#fig_1">3</ref>). We used the Eclipse CVS repository and Eclipse Bugzilla reports provided by the MSR 2008 Mining Challenge <ref type="bibr" target="#b29">[30]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>The goal of our experiment is to study the effect of effortaware models on prior findings in the bug prediction literature. We now present the results of our study with respect to our two research questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RQ1: Are process metrics still more effective than product metrics in the effort-aware models?</head><p>Overview. In the experiment for RQ1, we compare models based on product metrics to models based on process metrics at the file-level. We apply these metrics to three well-known modeling techniques (regression model, regression tree, and random forest) and evaluate the prediction performance. We also study the performance of the product and process metrics when combined together.</p><p>Motivation. Several studies continue to show that process metrics are more effective than product metrics in predicting faulty modules <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref>. Since, Mende and Koschke <ref type="bibr" target="#b10">[11]</ref> only used product metrics, in their effort-aware models, it is not clear whether process metrics still outperform product metrics when considering effort.</p><p>Used Metrics. For our file-level analysis, we measure product metrics and process metrics (Table <ref type="table">II</ref>). The product metrics measure the static structure of source code such as source lines of codes and McCabe's cyclomatic complexity. The product metrics are measured using the Eclipse Metrics plug-in <ref type="bibr" target="#b30">[31]</ref>.</p><p>For process metrics, we used the metrics proposed by Moser et al. <ref type="bibr" target="#b13">[14]</ref>. The process metrics measure the change history of source code, such as the number of revisions and the number of times a file has been refactored. We wrote a script that calculates the process metrics from the change history (i.e., the Eclipse CVS repository).</p><p>Since many product and process metrics have a strong correlation with SLOC, we normalized metrics with a correlation coefficient that is higher than 0.4 by dividing by SLOC <ref type="bibr" target="#b31">[32]</ref>. The normalized metrics were MLOC, NBD, PAR, VG, NOF, NOM, WMC, Codechurn, LOCAdded, LOCDeleted and Revisions.</p><p>Model Building Approach. In order to address our research questions, we build fault prediction models that are based on product and process metrics. We use three modeling techniques (regression model, regression tree and random forest) to build the prediction models.</p><p>Results of our Experiment. We performed two types of evaluation analyses to study the performance of our experiments. The two analyses are: cross validation and cross-release prediction of post-release failures. For the cross-validation analysis, we randomly divided our module (i.e., files or packages) dataset into two sets with equal sizes. One of the datasets was used as training and the other was used as test. This division was repeated 20 times based on the experimental result by Kirsopp and Shepperd <ref type="bibr" target="#b32">[33]</ref>. For the cross-release analysis, a training dataset was built from a past release of a project, and a test dataset was built from the following release. The cross-release evaluation leads to an evaluation in a more practical setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-validation analysis</head><p>Table <ref type="table" target="#tab_3">III</ref> shows the experimental results of our cross validation analysis at the file-level. The values in each row are the average value of P opt for 20 iterations. A "*" symbol next to a value indicates that it is the best performance among the combinations of the three metric types (product metrics, process metrics and both) and three modeling techniques (LM -linear model, RT -regression tree and RF -Random Forest). The value next to the "LOC" in the header of a table indicates the P opt of a LOC based file order, which is the simplest classifier that orders files just by their decreasing size (LOC). In other words, if one were to just review files by picking the biggest files to review first then proceeding to smaller files. The value beside the 'LOC" is used as the baseline for the models.</p><p>We find that regardless of modeling techniques (LM, RT and RF), using process metrics is better than using product metrics. In the case of RF, the P opt range of process metrics is from 0.80 to 0.94, while that of product metrics is from 0.56 to 0.75. Among the models, the improvement of P opt by using process metrics is 0.07 at minimum (in PDE v3.0, RF) and 0.35 at maximum (in Platform v3.2, RF). The prediction performance of a combined model of product metrics and process metrics shows no difference from using only process metrics. We also note that using product metrics is better than LOC based file order. In short, process metrics outperform product metrics and combing both types of metrics does not lead to an improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-release analysis</head><p>Table <ref type="table" target="#tab_7">VII</ref> shows the experimental results for the crossrelease analysis at the file-level. The values in each row are the value of P opt for just a single iteration unlike the cross validation study. Similar to the experimental results of the cross validation study, process metrics show a better prediction performance than product metrics across all datasets. The improvement of P opt using process metrics is 0.01 at minimum (in PDE v3.0, RT) and 0.43 at maximum (in PDE v3.0, LM). For product metrics, LM and RT are worse in some cases than the baseline (LOC based file) order, but RF is always better than LOC based file order. For process metrics, RF gives the best performance among three techniques for all datasets except PDE v.3.0 → v.3.1. Again, we find that process metrics outperform product metrics and combing both types of metrics does not lead to an improved performance.</p><p>Figure <ref type="figure" target="#fig_2">4</ref> shows the LOC-based cumulative lift charts of the random forest in Platform v.3.1 → v.3.2 of Table <ref type="table" target="#tab_4">IV</ref>. We order all files by decreasing fault density. The x-axis shows the cumulative SLOC and the y-axis shows the cumulative number of bugs. The dashed line plots the cumulative lift chart for files ordered by decreasing actual (i.e., optimal) fault density, and solid line and dot-line plot the cumulative lit chart for files ordered by decreasing the predicted fault density of random forest using process metrics and product metrics. This result indicates that when we conduct a test on only 20% (of the lines) of all files based on the predicted fault density, we could detect 29% of all faults using product metrics and 74% of all faults using process metrics. That is, using our bug predictions, we only need to spend 20% of the efforts that it would take to test all files to detect up to 74% of all faults. We find that process metrics outperform product metrics by a factor of 2.6(=74/29) when considering effort.</p><p>The performance of our product metrics are consistent with results reported earlier by Mende and Koschke's <ref type="bibr" target="#b10">[11]</ref>. They showed that the result of random forest using only product metrics was 35% in Eclipse v.3.0.</p><p>Process metrics outperform product metrics as predictors of fault density when taking test effort into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RQ2: Are package-level predictions more effective than filelevel predictions?</head><p>Overview. In the experiment for RQ2, we compare file-level predictions to package-level predictions. We compare the prediction performance of one file-level prediction against three types of package-level predictions. We use three modeling techniques (i.e., LM, RT and RF) to build these three packagelevel predictions. Motivation. Traditionally, package-level bug prediction models have been shown to have higher precision and recall than file-level bug prediction models <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. We clarify whether or not effort-aware models at the package-level module are still more effective. Used Metrics. For package-level predictions, in addition to product metrics and process metrics, we use the metrics suite proposed by Martin <ref type="bibr" target="#b16">[17]</ref>. Table <ref type="table">V</ref> shows the Martin metrics used in our study. Martin's package design metrics indicate instability (Ca, Ce, I) and abstractness (NA, NC, A) of packages, and imbalance (D) of the instability and the abstractness. In this paper, we consider that a highly instable package is likely to have many bugs because when classes that   the package depends on are modified then the package needs to be modified too increasing the chances that a bug might be introduced. We also consider that a low-abstractness package is likely to have many bugs because many program logics could be implemented in the package. Additionally, we expect that a highly imbalanced package is likely to have many bugs because the design quality of the package might be worse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Building Approach.</head><p>When building a file-level prediction model, product metrics and process metrics can be used as is. However, since a package consists of multiple modules (i.e., files), we either need to lift the file-level metrics up to the package-level or use special package-level metrics (e.g., Martin's metrics). Figure <ref type="figure" target="#fig_3">5</ref> shows an overview of the construction process of package-level prediction model. B0 File(Prod. + Proc.) shows building a file-level prediction model. In this paper, we use the following building methods for the prediction of packages in the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B1</head><p>Lifting the file-level metrics up to the packagelevel First, this building method measures metrics at the file-level and calculates representative value (e.g., maximum and median) in each package. For example, if there are three files in package with a cyclomatic complexity of 8, 4 and 2, then the representative value for the package is 8 in the case of median. Next, this building method predicts the fault density of a package. This method has been used in other studies <ref type="bibr" target="#b22">[23]</ref>. We use the mean for this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B2</head><p>Metrics for package-level modules This building method measures the metrics that can   be directly collected at the package-level. We use Martin' package-level metrics and predict the fault density at the package-level. To our knowledge, no study has reported the effects of Martin metrics on fault density prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B3</head><p>Lifting file-level prediction results to the packagelevel Instead of lifting metrics, we lift prediction results. First, this building method measures metrics at the file-level and predicts the fault density of each file. Next, this method lifts the file-level fault density predictions to the package-level while taking into account the SLOC of a file since we lift density values not basic bug counts. To our knowledge, this paper represents the first experiment to ever consider this method of building package-level predictions.</p><p>Results of our experiment. The presentation of our results follows the same style as done in RQ1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-validation analysis</head><p>Table <ref type="table" target="#tab_7">VI</ref> shows the experimental results using a cross validation analysis. The values in each row are the average value of P opt for 20 iterations. A "*" symbol next to a value indicates that it is the best performance in a particular dataset (i.e., project). Package(LiftUp) is the result of lifting file-level combined process and product predictions (File(Prod. + Proc.)) to the package-level.</p><p>There is little difference between File(Prod. + Proc.) and Package(LiftUp). For example, in the case of RF, the P opt range of File(Prod. + Proc.) is from 0.78 to 0.93, while that of Package(LiftUp) is from 0.81 to 0.93.</p><p>We find that the prediction performance of Package(LiftUp) using random forests is the best for all 3 datasets across three releases. On the other hand, the prediction performance of Package <ref type="bibr">(Martin)</ref> is the worst for all three datasets across the three releases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-release analysis</head><p>Table <ref type="table" target="#tab_7">VII</ref> shows the experimental results on a cross-release study. The values in each row are the value of P opt for 1 iteration unlike the cross validation study. The result shows a somewhat similar tendency as the cross validation study. There is little difference between File(Prod. + Proc.) and Package(LiftUp).</p><p>The result show that Package(LiftUp) is better than the Package(Prod. + Proc.) and the Package <ref type="bibr">(Martin)</ref>. In the case of RF, the P opt range of Package(LiftUp) is from 0.72 to 0.93, while those of Package(Prod. + Proc.) and Package <ref type="bibr">(Martin)</ref> are from 0.51 to 0.89 and from 0.48 and 0.71.</p><p>Figures <ref type="figure">6</ref> and<ref type="figure">7</ref> show the LOC-based cumulative lift charts of the best performance of File(Prod. + Proc.) and Package(LiftUp) from Table <ref type="table" target="#tab_7">VII</ref>. We order all files/packages by decreasing fault density. The x-axis shows the cumulative SLOC and the y-axis shows the cumulative number of bugs. The dashed line plots the cumulative lift chart for files ordered by decreasing actual (i.e., optimal) fault density, and solid line plots the cumulative lift chart for files ordered by decreasing the predicted fault density of random forest. Note that the cumulative lift charts of the optimal fault density is different between file-level and package-level because the distribution of faults inside the dataset is different at file-level versus the package-level.</p><p>Figures <ref type="figure">6</ref> and<ref type="figure">7</ref> indicate that when we conduct a test on 20% (of the lines) of all files/packages based on the predicted fault density, we could detect 74% and 62% of the faults at the file-level and package-level respectively. That is, we only need to spend 20% of the efforts that it would take to test all files to detect up to 74% and 62% of all faults using file-level and package-level model. In short, the file-level model is 20% better than the package-level model.</p><p>There is little difference between P opt of the file-level and package-level when taking test effort into account.</p><p>However, file-level predictions are more effective than package-level prediction, since only 20% of test effort is needed to detect up to 74% of all faults. Also, we find that lifting up prediction results is better than building package-level models using lifted metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. SUMMARY AND ANALYSIS</head><p>In this paper, we evaluated the prediction performance of effort-aware models at the file-level and package-level using three data sets collected from the Eclipse Platform. Our results indicated that, at the file-level, process metrics outperform product metrics as predictors of fault density in effort-aware models, and random forest is the best prediction model among the three models.</p><p>To illustrate the impact of product metrics and process metrics, we use IncNodePurity in the output of the R randomForest library <ref type="bibr" target="#b33">[34]</ref>. IncNodePurity shows the mean decrease in node impurity. That is, a higher IncNodePurity means that a variable plays a more important role in a built prediction model. Figure <ref type="figure">8</ref> shows IncNodePurity, sorted decreasingly from top to bottom, of the metrics as assigned by the random forest (Prod. + Proc.) for Platform v.3.1 → v.3.2. The top five metrics are all process metrics (Revisions, BugFixes, Age, LOCDeleted and Codechurn). By using random forest and process metrics, test managers and quality managers could allocate inspection/test effort to find faulty modules more effectively.</p><p>For package-level prediction, the prediction performance of Martin metrics is the worst for all three datasets. To illustrate this, we build a random forest using the 7 Martin metrics and the 22 lifted-up product metrics and process metrics in Platform v. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17(D) and 21(I).</head><p>There is little difference between P opt at the file and package-level. However, we find that if we test 20% of all modules based on the predicted fault density, we would detect 74% of faults using file-level models and 62% of faults using package-level models. Such performance shows that fault density prediction model at the file-level is more effective than that at package-level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. THREATS TO VALIDITY</head><p>In this section, we discuss the threats to the validity of our work. We use a dataset collected from one foundation. In Tables <ref type="table" target="#tab_3">III</ref> and<ref type="table" target="#tab_4">IV</ref>, and Tables VI and VII, we can see that the tendency of evaluation value P opt is similar in three subprojects and three versions. We need to analyze other open source and closed source systems to generalize our findings.</p><p>This study uses the lines of code as a measure of effort, similar to Mende and Koschke <ref type="bibr" target="#b10">[11]</ref>, because Arisholm et al. <ref type="bibr" target="#b23">[24]</ref> pointed out that the effort is likely to be proportional to the size. Replicated studies using other measures (e.g., McCabe cyclomatic complex) of effort will be useful to assess the generalizability of our findings.</p><p>We use linear regression, regression tree, and random forest techniques to evaluate the effect of the effort-aware models, since these modeling techniques are well-known for bug prediction. However, using other modeling techniques may produce different results. We also use the mean value to build package-level models using lifted metrics. Using other representative values (e.g., maximum and median) may lead to different results.</p><p>This study obtains the number of bugs in source code files using the SZZ algorithm. The algorithm is commonly used in fault prediction research <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b13">[14]</ref>, but has the limitation that faults not recorded in CVS log comments cannot be collected. Further research is required to improve the accuracy of faults collection from repositories. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of LOC-based Cumulative Lift Chart.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Example of Counting the Number of Bugs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. File-level: LOC-based Cumulative Lift Chart for Product Metrics v.s. Process Metrics (RF, Platform v3.1→v3.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Overview of the construction process of prediction models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. LOC-based Cumulative Lift Chart at the File-level (RF, Platform v3.1→v3.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III FILE</head><label>III</label><figDesc>-LEVEL: EXPERIMENTAL RESULT FOR PRODUCT METRICS V.S. PROCESS METRICS (CROSS-VALIDATION)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">(a) Platform</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Popt</cell><cell cols="3">v.3.0 (LOC : 0.49)</cell><cell cols="3">v.3.1 (LOC : 0.46)</cell><cell cols="3">v.3.2 (LOC : 0.43)</cell></row><row><cell></cell><cell>LM</cell><cell>RT</cell><cell>RF</cell><cell>LM</cell><cell>RT</cell><cell>RF</cell><cell>LM</cell><cell>RT</cell><cell>RF</cell></row><row><cell>Product</cell><cell cols="3">0.59 0.52 0.56</cell><cell cols="3">0.58 0.54 0.58</cell><cell cols="3">0.60 0.54 0.59</cell></row><row><cell>Process</cell><cell cols="3">0.81 0.80 0.87</cell><cell cols="3">0.82 0.83 0.89</cell><cell cols="3">0.90 0.89 0.94*</cell></row><row><cell>Prod. + Proc.</cell><cell cols="3">0.83 0.80 0.88*</cell><cell cols="3">0.85 0.83 0.90*</cell><cell cols="3">0.90 0.88 0.93</cell></row><row><cell cols="2">*: Best Performance</cell><cell cols="2">LM: Linear Model</cell><cell cols="3">RT: Regression Tree</cell><cell cols="2">RF: Random Forest</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b) JDT</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Popt</cell><cell cols="3">v.3.0 (LOC : 0.50)</cell><cell cols="3">v.3.1 (LOC : 0.54)</cell><cell cols="3">v.3.2 (LOC : 0.47)</cell></row><row><cell></cell><cell>LM</cell><cell>RT</cell><cell>RF</cell><cell>LM</cell><cell>RT</cell><cell>RF</cell><cell>LM</cell><cell>RT</cell><cell>RF</cell></row><row><cell>Product</cell><cell cols="3">0.55 0.50 0.57</cell><cell cols="3">0.54 0.56 0.61</cell><cell cols="3">0.52 0.56 0.60</cell></row><row><cell>Process</cell><cell cols="3">0.80 0.80 0.86*</cell><cell cols="3">0.74 0.76 0.80</cell><cell cols="3">0.75 0.78 0.83</cell></row><row><cell>Prod. + Proc.</cell><cell cols="3">0.76 0.73 0.82</cell><cell cols="3">0.79 0.78 0.87*</cell><cell cols="3">0.74 0.76 0.85*</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(c) PDE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Popt</cell><cell cols="3">v.3.0 (LOC : 0.53)</cell><cell cols="3">v.3.1 (LOC : 0.60)</cell><cell cols="3">v.3.2 (LOC : 0.51)</cell></row><row><cell></cell><cell>LM</cell><cell>RT</cell><cell>RF</cell><cell>LM</cell><cell>RT</cell><cell>RF</cell><cell>LM</cell><cell>RT</cell><cell>RF</cell></row><row><cell>Product</cell><cell cols="3">0.61 0.58 0.75</cell><cell cols="3">0.62 0.67 0.66</cell><cell cols="3">0.50 0.49 0.61</cell></row><row><cell>Process</cell><cell cols="3">0.78 0.78 0.82</cell><cell cols="3">0.78 0.77 0.83*</cell><cell cols="3">0.78 0.83 0.84*</cell></row><row><cell>Prod. + Proc.</cell><cell cols="3">0.75 0.75 0.84*</cell><cell cols="3">0.82 0.74 0.78</cell><cell cols="3">0.76 0.73 0.81</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV FILE</head><label>IV</label><figDesc>-LEVEL: EXPERIMENTAL RESULT FOR PRODUCT METRICS V.S. PROCESS METRICS (CROSS-RELEASE)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">(a) Platform</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Popt</cell><cell></cell><cell cols="2">v.3.0 → v.3.1 (LOC : 0.45)</cell><cell cols="3">v.3.1 → v.3.2 (LOC : 0.43)</cell><cell></cell><cell cols="2">v.3.1 → v.3.2 (LOC : 0.43)</cell></row><row><cell></cell><cell>LM</cell><cell>RT</cell><cell>RF</cell><cell>LM</cell><cell>RT</cell><cell>RF</cell><cell>LM</cell><cell>RT</cell><cell>RF</cell></row><row><cell>Product</cell><cell>0.59</cell><cell cols="2">0.53 0.61</cell><cell cols="3">0.53 0.49 0.57</cell><cell>0.58</cell><cell cols="2">0.54 0.64</cell></row><row><cell>Process</cell><cell>0.81</cell><cell cols="2">0.82 0.87*</cell><cell cols="3">0.82 0.86 0.91*</cell><cell>0.85</cell><cell cols="2">0.86 0.92*</cell></row><row><cell>Prod. + Proc.</cell><cell>0.82</cell><cell cols="2">0.77 0.87*</cell><cell cols="3">0.83 0.82 0.91*</cell><cell>0.87</cell><cell cols="2">0.84 0.92*</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b) JDT</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Popt</cell><cell></cell><cell cols="2">v.3.0 → v.3.1 (LOC : 0.54)</cell><cell cols="3">v.3.1 → v.3.2 (LOC : 0.47)</cell><cell></cell><cell cols="2">v.3.1 → v.3.2 (LOC : 0.47)</cell></row><row><cell></cell><cell>LM</cell><cell>RT</cell><cell>RF</cell><cell>LM</cell><cell>RT</cell><cell>RF</cell><cell>LM</cell><cell>RT</cell><cell>RF</cell></row><row><cell>Product</cell><cell>0.48</cell><cell cols="2">0.55 0.58</cell><cell cols="3">0.52 0.52 0.54</cell><cell>0.52</cell><cell cols="2">0.53 0.62</cell></row><row><cell>Process</cell><cell>0.76</cell><cell cols="2">0.75 0.80*</cell><cell cols="3">0.76 0.76 0.82*</cell><cell>0.80</cell><cell cols="2">0.77 0.84*</cell></row><row><cell>Prod. + Proc.</cell><cell>0.72</cell><cell cols="2">0.71 0.79</cell><cell cols="3">0.73 0.74 0.81</cell><cell>0.75</cell><cell cols="2">0.78 0.83</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(c) PDE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Popt</cell><cell></cell><cell cols="2">v.3.0 → v.3.1 (LOC : 0.59)</cell><cell cols="3">v.3.1 → v.3.2 (LOC : 0.50)</cell><cell></cell><cell cols="2">v.3.1 → v.3.2 (LOC : 0.50)</cell></row><row><cell></cell><cell>LM</cell><cell>RT</cell><cell>RF</cell><cell>LM</cell><cell>RT</cell><cell>RF</cell><cell>LM</cell><cell>RT</cell><cell>RF</cell></row><row><cell>Product</cell><cell>0.41</cell><cell cols="2">0.65 0.74</cell><cell cols="3">0.50 0.53 0.58</cell><cell>0.49</cell><cell cols="2">0.51 0.58</cell></row><row><cell>Process</cell><cell cols="3">0.84* 0.66 0.83</cell><cell cols="3">0.71 0.67 0.76*</cell><cell>0.72</cell><cell cols="2">0.56 0.73</cell></row><row><cell>Prod. + Proc.</cell><cell>0.82</cell><cell cols="2">0.65 0.83</cell><cell cols="3">0.70 0.68 0.73</cell><cell cols="3">0.74* 0.55 0.68</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI EXPERIMENTAL</head><label>VI</label><figDesc>RESULT FOR FILE-LEVEL V.S. PACKAGE-LEVEL (CROSS-VALIDATION)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">(a) Platform</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Popt</cell><cell></cell><cell>v.3.0</cell><cell></cell><cell></cell><cell>v.3.1</cell><cell></cell><cell></cell><cell>v.3.2</cell></row><row><cell></cell><cell>LM</cell><cell>RT</cell><cell>RF</cell><cell>LM</cell><cell>RT</cell><cell>RF</cell><cell>LM</cell><cell>RT</cell><cell>RF</cell></row><row><cell>File (Prod. + Proc.)</cell><cell cols="3">0.83 0.80 0.88</cell><cell>0.85</cell><cell cols="2">0.83 0.90</cell><cell cols="3">0.90 0.88 0.93*</cell></row><row><cell>Package (Prod. + Proc.)</cell><cell cols="3">0.82 0.79 0.84</cell><cell>0.82</cell><cell cols="2">0.83 0.88</cell><cell cols="3">0.89 0.85 0.91</cell></row><row><cell>Package (Martin)</cell><cell cols="3">0.65 0.56 0.57</cell><cell>0.55</cell><cell cols="2">0.60 0.60</cell><cell cols="3">0.54 0.59 0.52</cell></row><row><cell>Package (LiftUp)</cell><cell cols="3">0.86 0.87 0.90*</cell><cell>0.88</cell><cell cols="2">0.89 0.92*</cell><cell cols="3">0.90 0.92 0.93*</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">(b) JDT</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Popt</cell><cell></cell><cell>v.3.0</cell><cell></cell><cell></cell><cell>v.3.1</cell><cell></cell><cell></cell><cell>v.3.2</cell></row><row><cell></cell><cell>LM</cell><cell>RT</cell><cell>RF</cell><cell>LM</cell><cell>RT</cell><cell>RF</cell><cell>LM</cell><cell>RT</cell><cell>RF</cell></row><row><cell>File (Prod. + Proc.)</cell><cell cols="3">0.76 0.73 0.82*</cell><cell>0.79</cell><cell cols="2">0.78 0.87*</cell><cell cols="3">0.74 0.76 0.85</cell></row><row><cell>Package (Prod. + Proc.)</cell><cell cols="3">0.56 0.67 0.74</cell><cell>0.72</cell><cell cols="2">0.70 0.72</cell><cell cols="3">0.75 0.75 0.83</cell></row><row><cell>Package (Martin)</cell><cell cols="3">0.49 0.39 0.46</cell><cell>0.66</cell><cell cols="2">0.63 0.60</cell><cell cols="3">0.54 0.57 0.50</cell></row><row><cell>Package (LiftUp)</cell><cell cols="3">0.76 0.76 0.82*</cell><cell>0.79</cell><cell cols="2">0.83 0.87*</cell><cell cols="3">0.75 0.82 0.87*</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">(c) PDE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Popt</cell><cell></cell><cell>v.3.0</cell><cell></cell><cell></cell><cell>v.3.1</cell><cell></cell><cell></cell><cell>v.3.2</cell></row><row><cell></cell><cell>LM</cell><cell>RT</cell><cell>RF</cell><cell>LM</cell><cell>RT</cell><cell>RF</cell><cell>LM</cell><cell>RT</cell><cell>RF</cell></row><row><cell>File (Prod. + Proc.)</cell><cell cols="3">0.75 0.75 0.84</cell><cell>0.82</cell><cell cols="2">0.74 0.78</cell><cell cols="3">0.76 0.73 0.81</cell></row><row><cell>Package (Prod. + Proc.)</cell><cell cols="3">0.72 0.72 0.74</cell><cell>0.83</cell><cell cols="2">0.83 0.78</cell><cell cols="3">0.63 0.73 0.71</cell></row><row><cell>Package (Martin)</cell><cell cols="3">0.46 0.50 0.56</cell><cell>0.47</cell><cell cols="2">0.54 0.26</cell><cell cols="3">0.60 0.58 0.53</cell></row><row><cell>Package (LiftUp)</cell><cell cols="3">0.76 0.78 0.88*</cell><cell cols="3">0.84* 0.75 0.81</cell><cell cols="3">0.76 0.82 0.85*</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE VII</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">EXPERIMENTAL RESULT FOR FILE-LEVEL V.S. PACKAGE-LEVEL (CROSS-RELEASE)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">(a) Platform</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Popt</cell><cell>LM</cell><cell cols="2">v.3.0 → v.3.1 RT RF</cell><cell>LM</cell><cell cols="2">v.3.0 → v.3.2 RT RF</cell><cell>LM</cell><cell cols="2">v.3.1 → v.3.2 RT RF</cell></row><row><cell>File (Prod. + Proc.)</cell><cell>0.82</cell><cell cols="2">0.77 0.87</cell><cell cols="3">0.83 0.82 0.91</cell><cell>0.87</cell><cell cols="2">0.84 0.92</cell></row><row><cell>Package (Prod. + Proc.)</cell><cell>0.81</cell><cell cols="2">0.72 0.85</cell><cell cols="3">0.79 0.82 0.84</cell><cell>0.87</cell><cell cols="2">0.83 0.89</cell></row><row><cell>Package (Martin)</cell><cell>0.63</cell><cell cols="2">0.62 0.71</cell><cell cols="3">0.66 0.56 0.63</cell><cell>0.61</cell><cell cols="2">0.56 0.63</cell></row><row><cell>Package (LiftUp)</cell><cell>0.86</cell><cell cols="2">0.87 0.90*</cell><cell cols="3">0.81 0.90 0.92*</cell><cell>0.86</cell><cell cols="2">0.92 0.93*</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">(b) JDT</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Popt</cell><cell>LM</cell><cell cols="2">v.3.0 → v.3.1 RT RF</cell><cell>LM</cell><cell cols="2">v.3.0 → v.3.2 RT RF</cell><cell>LM</cell><cell cols="2">v.3.1 → v.3.2 RT RF</cell></row><row><cell>File (Prod. + Proc.)</cell><cell>0.72</cell><cell cols="2">0.71 0.79*</cell><cell cols="3">0.73 0.74 0.81*</cell><cell>0.75</cell><cell cols="2">0.78 0.83</cell></row><row><cell>Package (Prod. + Proc.)</cell><cell>0.50</cell><cell cols="2">0.52 0.51</cell><cell cols="3">0.54 0.72 0.71</cell><cell>0.78</cell><cell cols="2">0.74 0.74</cell></row><row><cell>Package (Martin)</cell><cell>0.42</cell><cell cols="2">0.38 0.58</cell><cell cols="3">0.48 0.37 0.52</cell><cell>0.53</cell><cell cols="2">0.52 0.61</cell></row><row><cell>Package (LiftUp)</cell><cell>0.65</cell><cell cols="2">0.69 0.72</cell><cell cols="3">0.72 0.79 0.80</cell><cell>0.81</cell><cell cols="2">0.84 0.85*</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">(c) PDE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Popt</cell><cell>LM</cell><cell cols="2">v.3.0 → v.3.1 RT RF</cell><cell>LM</cell><cell cols="2">v.3.0 → v.3.2 RT RF</cell><cell>LM</cell><cell cols="2">v.3.1 → v.3.2 RT RF</cell></row><row><cell>File (Prod. + Proc.)</cell><cell>0.82</cell><cell cols="2">0.65 0.83</cell><cell cols="3">0.70 0.68 0.73</cell><cell cols="3">0.74* 0.55 0.68</cell></row><row><cell>Package (Prod. + Proc.)</cell><cell>0.87</cell><cell cols="2">0.88 0.85</cell><cell cols="3">0.65 0.58 0.61</cell><cell>0.64</cell><cell cols="2">0.59 0.63</cell></row><row><cell>Package (Martin)</cell><cell>0.69</cell><cell cols="2">0.75 0.70</cell><cell cols="3">0.59 0.61 0.57</cell><cell>0.54</cell><cell cols="2">0.55 0.48</cell></row><row><cell>Package (LiftUp)</cell><cell cols="3">0.89* 0.57 0.89*</cell><cell cols="3">0.71 0.77 0.79*</cell><cell cols="3">0.74* 0.64 0.73</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We are grateful to the reviewers for their valuable comments. This research is being conducted as a part of the Next Generation IT Program by the Ministry of Education, Culture, Sports, Science and Technology, Japan.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper, we sought to revisit some of the major findings in the prediction literature by taking into account the cost of additional software quality assurance efforts. We experimentally evaluated the performance of effort-aware models using data from three subprojects (Platform, JDT and PDE) from the Eclipse Foundation across three releases. Our major findings include the following:</p><p>• At the file-level, process metrics still outperform product metrics when considering quality assurance efforts. We show a 2.6 times improvement between process metric and product metrics;</p><p>• At the file-level, random forest produce the best predictor performance compared to linear models and regression trees;</p><p>• Package-level predictions are less effective than file-level predictions. When we test or review 20% (of the lines) of all modules based on the predicted fault density, we could detect almost 74% of faults using file-level models versus 62% of faults using package-level models;</p><p>• At the package-level, lifting up prediction results is better than building package-level models using lifted metrics. The major limitation of this paper is that we used only a dataset collected from one foundation. Our future work is to confirm our results using other project datasets. We also plan to explore other more appropriate measures as a proxy for the effort instead of lines of code.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Predicting fault-prone software modules in telephone switches</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ohlsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Alberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="886" to="894" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A predictive metric based on discriminant statistical analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pighin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zamolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conference on Software Engineering (ICSE&apos;97)</title>
		<meeting>Int&apos;l Conference on Software Engineering (ICSE&apos;97)</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The detection of fault-prone programs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Munson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="423" to="433" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Revisiting the evaluation of defect prediction models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Koschke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conference on Predictor Models in Software Engineering (PROMISE&apos;09</title>
		<meeting>Int&apos;l Conference on Predictor Models in Software Engineering (PROMISE&apos;09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A validation of objectoriented design metrics as quality indicators</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>Basili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Briand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Melo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="751" to="761" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Predicting faults using the complexity of code changes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Hassan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conference on Software Engineering (ICSE&apos;09</title>
		<meeting>Int&apos;l Conference on Software Engineering (ICSE&apos;09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="16" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modeling software quality with classification trees</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Reliability and Quality Engineering</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="247" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Prediction of fault-prone software modules using a generic text discriminator</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mizuno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kikuno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Transactions on Information and Systems</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="888" to="896" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Software defect association mining and defect correction effort prediction</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shepperd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cartwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="69" to="82" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Predicting defects using network analysis on dependency graphs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nagappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conference on Software Engineering (ICSE&apos;08)</title>
		<meeting>Int&apos;l Conference on Software Engineering (ICSE&apos;08)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Effort-aware defect prediction models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Koschke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of European Conference on Software Maintenance and Reengineering (CSMR&apos;10)</title>
		<meeting>of European Conference on Software Maintenance and Reengineering (CSMR&apos;10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="109" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Predicting fault incidence using software change history</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Karr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Marron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Siy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="653" to="661" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Use of relative code churn measures to predict system defect density</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nagappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ball</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conference on Software Engineering (ICSE&apos;06)</title>
		<meeting>Int&apos;l Conference on Software Engineering (ICSE&apos;06)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="284" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A comparative analysis of the efficiency of change metrics and static code attributes for defect prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Moser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Succi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conference on Software Engineering (ICSE&apos;08)</title>
		<meeting>Int&apos;l Conference on Software Engineering (ICSE&apos;08)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="181" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Predicting component failures at design time</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schröter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2006 ACM/IEEE international symposium on Empirical software engineering (ISESE&apos;06)</title>
		<meeting>of the 2006 ACM/IEEE international symposium on Empirical software engineering (ISESE&apos;06)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="18" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Predicting defects for eclipse</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Workshop on Predictor Models in Software Engineering (PROMISE&apos;07)</title>
		<meeting>Int&apos;l Workshop on Predictor Models in Software Engineering (PROMISE&apos;07)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Martin</surname></persName>
		</author>
		<title level="m">Agile Software Development, Principles, Patterns, and Practices</title>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Classifying software changes: Clean or buggy?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Whitehead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jr</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="196" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the relation of refactorings and software defect prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ratzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sigmund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Working Conference on Mining Software Repositories (MSR&apos;08)</title>
		<meeting>Int&apos;l Working Conference on Mining Software Repositories (MSR&apos;08)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="35" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A hybrid faulty module prediction using association rule mining and logistic regression analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kamei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Monden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Morisaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Symposium on Empirical Software Engineering and Measurement</title>
		<meeting>Int&apos;l Symposium on Empirical Software Engineering and Measurement</meeting>
		<imprint>
			<publisher>ESEM</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="279" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Benchmarking classification models for software defect prediction: A proposed framework and novel findings</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lessmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Baesens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pietsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="485" to="496" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Techniques for evaluating fault prediction models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cukic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Empirical Software Engineering</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="561" to="595" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mining metrics to predict component failures</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nagappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conference on Software Engineering (ICSE&apos;06)</title>
		<meeting>Int&apos;l Conference on Software Engineering (ICSE&apos;06)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A systematic and comprehensive investigation of methods to build and evaluate fault prediction models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Arisholm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Briand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Johannessen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Systems and Software</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="17" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Introduction to Linear Regression and Correlation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Edwards</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976">1976</date>
			<publisher>W.H.Freeman &amp; Co Ltd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Classification and regression trees</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Olshen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Chapman and Hall/CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<ptr target="http://www.r-project.org/" />
		<title level="m">The R Project for Statistical Computing</title>
		<imprint>
			<date type="published" when="2010-04">Apr-2010</date>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">When do changes induce fixes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Śliwerski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conference on Mining Software Repositories (MSR&apos;05)</title>
		<meeting>Int&apos;l Conference on Mining Software Repositories (MSR&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<ptr target="http://msr.uwaterloo.ca/msr2008/challenge/index.html" />
		<title level="m">MSR Mining Challenge</title>
		<imprint>
			<date type="published" when="2008-04">2008. Apr-2010</date>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Eclipse Metrics plugin</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Sauer</surname></persName>
		</author>
		<ptr target="http://sourceforge.net/projects/metrics" />
		<imprint>
			<date type="published" when="2010-04">Apr-2010</date>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Evaluating the applicability of reliability prediction models between different software</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Monden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Working on Principles of Software Evolution (IWPSE&apos;02)</title>
		<meeting>Int&apos;l Working on Principles of Software Evolution (IWPSE&apos;02)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Making inferences with small numbers of training sets</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kirsopp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shepperd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEE Proceedings Software</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="123" to="130" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Applied Statistical Genetics with R</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Foulkes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
