<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Maximum relevance minimum common redundancy feature selection for nonlinear data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-05-10">10 May 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jinxing</forename><surname>Che</surname></persName>
							<email>jinxingche@163.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematics and Statistics</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<addrLine>266 Xinglong Section of Xifeng Road</addrLine>
									<postCode>710126</postCode>
									<settlement>Xi&apos;an, Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Jiangxi Province Key Laboratory of Water Information Cooperative Sensing and Intelligent Processing</orgName>
								<orgName type="institution">Nanchang Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Science</orgName>
								<orgName type="institution">Nanchang Institute of Technology</orgName>
								<address>
									<postCode>330099</postCode>
									<settlement>Nanchang</settlement>
									<region>Jiangxi</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Youlong</forename><surname>Yang</surname></persName>
							<email>ylyang@mail.xidian.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematics and Statistics</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<addrLine>266 Xinglong Section of Xifeng Road</addrLine>
									<postCode>710126</postCode>
									<settlement>Xi&apos;an, Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Science</orgName>
								<orgName type="institution">Nanchang Institute of Technology</orgName>
								<address>
									<postCode>330099</postCode>
									<settlement>Nanchang</settlement>
									<region>Jiangxi</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuying</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematics and Statistics</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<addrLine>266 Xinglong Section of Xifeng Road</addrLine>
									<postCode>710126</postCode>
									<settlement>Xi&apos;an, Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shenghu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Mathematical Sciences</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>10 0 049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chengzhi</forename><surname>Deng</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Jiangxi Province Key Laboratory of Water Information Cooperative Sensing and Intelligent Processing</orgName>
								<orgName type="institution">Nanchang Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Maximum relevance minimum common redundancy feature selection for nonlinear data</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-05-10">10 May 2017</date>
						</imprint>
					</monogr>
					<idno type="MD5">C4626763DE0F2574642EA94DE0E24005</idno>
					<idno type="DOI">10.1016/j.ins.2017.05.013</idno>
					<note type="submission">Received 20 January 2016 Revised 8 May 2017 Accepted 9 May 2017</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Feature selection Mutual information Normalization Minimal common redundancy Maximal relevance</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, feature selection based on relevance redundancy trade-off criteria has become a very promising and popular approach in the field of machine learning. However, the existing algorithmic frameworks of mutual information feature selection have certain limitations for the common feature selection problems in practice. To overcome these limitations, the idea of a new framework is developed by introducing a novel maximum relevance and minimum common redundancy criterion and a minimax nonlinear optimization approach. In particular, a novel mutual information feature selection method based on the normalization of the maximum relevance and minimum common redundancy (N-MRMCR-MI) is presented, which produces a normalized value in the range [0, 1] and results in a regression problem. We perform extensive experimental comparisons over numerous state-of-art algorithms using different forecasts (Bayesian Additive Regression tree, treed Gaussian process, k -NN, and SVM) and different data sets (two simulated and five real datasets). The results show that the proposed algorithm outperforms the others in terms of feature selection and forecasting accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Feature selection plays a critical role in regression systems, especially for nonparametric models <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b33">34]</ref> . Forecasting becomes unnecessarily complex or over-fitting if irrelevant or too many redundant attributes are included. Accordingly, to deal with large quantities of data, it is highly desirable to design a feature selection method which would include relevant features but exclude irrelevant or redundant ones <ref type="bibr" target="#b49">[50]</ref> . To this end, we contribute this study to feature selection for nonlinear data, e.g., nonlinear classification or regression problems.</p><p>The feature selection problem for linear regression or simple parametric models has been studied extensively by different researchers and from different perspectives <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b38">39]</ref> . In this work, we consider feature selection for nonparametric relationships. Classical metaheuristic approaches consider feature selection as a discrete optimization problem with the so-lution space spanning spanning all 2 m possible subsets from a candidate pool of m features, and require finding the best feature subset <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b46">47]</ref> , where the objective functions of such discrete optimization problems are distinct in the context of individual evaluation criteria, such as the minimization of the sum-of-squares error in the regression. By retraining a neural network repeatedly, Setiono and Lui proposed a decision tree method to exclude irrelevant or redundant features one by one <ref type="bibr" target="#b39">[40]</ref> . The method needs to retrain NN to explore almost every combination of feature subsets. Since 2 m is the total candidate number of feature subsets, the application of the method becomes extremely difficult to search the whole feature subset exhaustively for large values of m . This also brings such challenges as the management of the computational time complexity while extracting compact yet effective models <ref type="bibr" target="#b4">[5]</ref> . A common way of overcoming these challenges is to use information measure based (e.g., feature subset selection) techniques.</p><p>In probability theory and information theory, the measure of dependence between two random variables is an important research topic <ref type="bibr" target="#b32">[33]</ref> , where the correlation coefficient and mutual information are the two major metrics. In fact, the mutual information (MI) between two variables X and Y measures how similar the joint distribution p ( X, Y ) is to the product p ( X ) p ( Y ) of the factored marginal distributions, which does provide a generalized measure of the variables' mutual dependence <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b25">26]</ref> . Specifically, such dependence, not limited to a linear dependence relationship such as the correlation coefficient, works for both linear and nonlinear cases. Accordingly, an MI-based algorithm is an attractive alternative to a correlation coefficientbased method.</p><p>For the feature selection problem, we aim to classify all candidate features into three subsets <ref type="bibr" target="#b5">[6]</ref> : <ref type="bibr" target="#b0">(1)</ref> the relevant feature subset which is required for any modeling tool; (2) the indifferent feature subset which consists of useless features that have bad effects on data analysis and increase the complexity of modeling; and (3) the redundant feature subset which includes features that are useful, but depend on relevant features, so that if we make some mistake in measuring a relevant feature, the predictor may work badly, but if the predictor is derived considering highly correlated redundant features, the mistakes can be avoided once a measurement error occurs in one of the relevant features. Based on the above analysis, it is desired to include certain redundant features to improve the robustness of the predictor. Accordingly, it is expected that any feature selection problem should consider <ref type="bibr" target="#b6">[7]</ref> : (1) including relevant features; (2) excluding indifferent features; and (3) using redundant features.</p><p>To reduce the number of combinations, Battiti introduced a mutual information feature selector (MIFS) which makes use of mutual information between inputs and outputs <ref type="bibr" target="#b1">[2]</ref> , and demonstrated the effectiveness of mutual information in feature selection. Since then, many feature selection approaches based on the Max-Relevance and Min-Redundancy criterion have been proposed to improve the performance of feature selection, such as the NMIFS <ref type="bibr" target="#b19">[20]</ref> , MIFS-U <ref type="bibr" target="#b29">[30]</ref> and mRMR <ref type="bibr" target="#b37">[38]</ref> , but these methods also have some limitations. For example, in most cases, only a part of the redundancy term is contained in the relevance term. Moreover, the NMIFS may produce a value outside the range [0, 1].</p><p>For this, conditional mutual information, which integrates both terms of relevance and redundancy, was introduced for feature selection, e.g., the Joint Mutual Information (JMI) <ref type="bibr" target="#b50">[51]</ref> , Double Input Symmetrical Relevance (DISR) <ref type="bibr" target="#b34">[35]</ref> , Conditional Mutual Information Maximization <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b41">42]</ref> , JMIM and NJMIM <ref type="bibr" target="#b2">[3]</ref> . However, two difficulties arise. First, accurate calculation of conditional mutual information is hard, due to the amount of calculations and the limited number of observations available for the evaluation of the 3-dimensional probability density function. Second, to extend the calculation of I ( X i , X j , Y ) to I ( X i , S, Y ), these methods use either cumulative sum approximation or 'maximum of the minimum' approximation, which may exclude relevant features or include indifferent features.</p><p>Recently, some improved feature selection methods based on the max-relevance and min-redundancy (mRMR) criterion were proposed to further use information about both relevance and redundancy. Wang et al. built a multi-objective optimization problem by considering two objectives, namely, the maximization of relevance and minimization of redundancy, and performed an evolutionary algorithm for the feature selection process <ref type="bibr" target="#b45">[46]</ref> . Using the information about already selected features, Chernbumroong et al. showed how a feature complements already selected features, and then established feature selection according to maximum relevancy and maximum complementary <ref type="bibr" target="#b9">[10]</ref> .</p><p>Inspired by these works, this paper proposes a new filter framework which introduces a novel criterion in terms of maximum relevance and minimum common redundancy (MRMCR), and results in a 'maximum of the minimum' nonlinear approach. It can properly select relevant features and control the use of redundant features, while discarding indifferent features at the same time. More specifically, to make the relevance and redundancy term comparable, common redundancy is first calculated to evaluate the common information of the candidate feature, already selected features and response feature. Then, a novel feature selection method based on the normalization of maximum relevance and minimum common redundancy (N-MRMCR-MI) is presented for the studied nonlinear optimization problem, which produces a normalized value in the range [0, 1], and further extends the NMIFS approach to the regression problem.</p><p>The key contributions of this paper are as follows: (i) the goal of any feature selection problem is introduced; (ii) the existing mutual information (MI) feature selection methods are classified into two frameworks, and their limitations are analyzed; and (iii) a new framework to attain the above goal is proposed, where a novel Max-Relevance and Min-Common-Redundancy criterion is developed to make relevance and redundancy comparable.</p><p>This paper is organized as follows. In Section 2 , the basic principles of the information theory and forecast methods are briefly presented, including concepts such as mutual information, the Bayesian Additive Regression tree, and treed Gaussian process. In Section 3 , a review of related works is provided, and then the limitations of previous approaches are discussed in detail. In Section 4 , an improved feature selection criterion for regression and classification is proposed. In Section 5 , the proposed feature selection approach is applied to several regression and classification problems to illustrate its effectiveness. Finally, conclusions are made in Section 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>Before proposing an improved framework in the following section, we introduce two implementation issues related to mutual information and multiple types of forecasts used in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Mutual information (MI)</head><p>From the point of view of the evaluation of candidate features, mutual information (MI) is a measure of features' mutual dependence in feature selection problems. For two discrete features X and Y , their mutual information can be defined as:</p><formula xml:id="formula_0">I(X ; Y ) = y ∈ Y x ∈ X p(x, y ) log p(x, y ) p(x ) p(y ) (1)</formula><p>where p ( x, y ) is the joint distribution, and p ( x ) and p ( y ) are the marginal distributions.</p><p>In the case of continuous features, the summation needs to be replaced by a definite double integral. We use the entropy package in R to compute the MI <ref type="bibr" target="#b24">[25]</ref> .</p><p>The MI can also be conditioned -conditional mutual information is defined by</p><formula xml:id="formula_1">I(X ; Y | Z) = z∈ Z p(z) y ∈ Y x ∈ X p(x, y | z) log p(x, y | z) p(x | z) p(y | z) (2)</formula><p>However, the calculation of conditional mutual information needs more samples than the MI, due to the calculation of the 3-dimensional probability density function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multiple forecasts</head><p>To demonstrate the robustness of our improved feature selection method, we evaluate the selected features using various types of forecasts. To test, we consider some widely used forecasts, namely, the Bayesian Additive Regression tree (BART), treed Gaussian process (TGP) <ref type="bibr" target="#b7">[8]</ref> , 1-Nearest Neighbour (1NN), and support vector machine (SVM).</p><p>The BART model is an ensemble which views the sum of the Bayesian CART decision tree models as a flexible and efficient estimator of the response Y <ref type="bibr" target="#b10">[11]</ref> . We use the bartMachine package in R for the implementation of the BART <ref type="bibr" target="#b11">[12]</ref> .</p><p>In a treed Gaussian process (TGP), every point in an input space has a jointly Gaussian distribution. By introducing the idea of Bayesian partition models, the TGP becomes a flexible nonparametric model to deal with the problems of nonstationarity, heteroscedasticity, and the dataset sizes. We use the tgp package in R for the implementation of the TGP <ref type="bibr" target="#b23">[24]</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related work</head><p>For the variable selection problem, let X i , i = 1 , 2 , • • • , p be the the independent variables, Y be the target/dependent variable. Now if a variable selection algorithm is employed to select variable progressively, any independent variable would be a selected or non-selected state in a certain stage. Definition 1. The set S is called a selected variable subset, and the set T is called a non-selected variable subset if S ∪ T = { X i , i = 1 , 2 , • • • , p} and S ∩ T = Ø, where all selected variables are contained in S and all non-selected variables are contained in T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Review of MI feature subset selection approaches</head><p>Feature relevance analysis and redundancy analysis have always been two challenging problems in the field of feature selection <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44]</ref> . In recent years, several MI feature subset selection approaches based on the Max-Relevance and Min-Redundancy criteria were proposed for the classification problem. In the above approaches, Max-Relevance and Min-Redundancy are measured using the MI method: the left-hand side term computes the relevance of the feature to be selected, and the right-hand side term computes the redundancy of the feature with respect to the subset of previously selected features.</p><p>Suppose that X i ∈ T is a candidate feature, Y is the class attribute, S is the selected feature subset, and f ( X i ) is a 'scoring' criterion that measures how potentially useful a candidate feature X i may be when used in the classifier <ref type="bibr" target="#b3">[4]</ref> . Battiti <ref type="bibr" target="#b1">[2]</ref> first proposed heuristic MI approximation of Max-Relevance and Min-Redundancy (MIFS) to select a feature subset.</p><formula xml:id="formula_2">f (X i ) = I(X i ; Y ) -β X j ∈ S I(X i ; X j ) (3)</formula><p>where β adjusts the subtraction comparability of the relevance and redundancy term, and f estimates the goodness of the feature X i .</p><p>By making more considered use of mutual information between input features and output classes, Kwak and Choi proposed a modified version MIFS-U to reach the performance of the ideal greedy selection algorithm when information is distributed uniformly <ref type="bibr" target="#b29">[30]</ref> .</p><formula xml:id="formula_3">f (X i ) = I(X i ; Y ) -β X j ∈ S I(X j ; Y ) H(X j ) I(X i ; X j ) (4)</formula><p>where H ( X j ) is the entropy of the random variable X j . Despite the improvement made by later works, the selection of the parameter β is difficult. When β is large, both above algorithms tend to select features based on minimum redundancy, and when β is small, they tend to select features based on maximum relevance. To this end, Yu and Liu introduced a new framework that decoupled relevance analysis and redundancy analysis <ref type="bibr" target="#b53">[54]</ref> . First, it selects a subset of relevant features based on a predefined threshold. Second, it selects predominant features from relevant ones. Based on the above work, García-Torres et al. proposed a novel search strategy by introducing feature grouping <ref type="bibr" target="#b22">[23]</ref> . Its main advantage is that it provides a fast search method for the feature selection problem in high-dimensional scenarios. However, the selection of the predefined threshold is also a difficult problem. Moreover, the definition of the Approximate Markov blanket does not explicitly provide common redundant information among the candidate feature, already selected features and response feature, and, hence, the redundancy and relevance terms cannot be made comparable. To avoid the need for a predefined threshold, Peng et al. <ref type="bibr" target="#b37">[38]</ref> proposed the following condition (mRMR):</p><formula xml:id="formula_4">f (X i ) = I (X i ; Y ) - 1 | S| X j ∈ S I (X i ; X j )<label>(5)</label></formula><p>By using the definition of the MI, Estevez et al. <ref type="bibr" target="#b19">[20]</ref> obtained the following interval of the MI:</p><formula xml:id="formula_5">0 ≤ I(X i ; X j ) ≤ min { H (X i ) ; H (X j ) }<label>(6)</label></formula><p>and produced a normalized value in the range [0, 1] by dividing by the minimum value of the entropies. The authors then presented a normalized selection strategy (NMIFS) as follows:</p><formula xml:id="formula_6">f (X i ) = I (X i ; Y ) - 1 | S| X j ∈ S I (X i ; X j ) min { H (X i ) ; H (X j ) }<label>(7)</label></formula><p>Vinh et al. <ref type="bibr" target="#b42">[43]</ref> normalized the left-hand side term of the above selection criterion, which was rewritten as:</p><formula xml:id="formula_7">f (X i ) = I(X i ; Y ) min { H(X i ) ; H(Y ) } - 1 | S| X j ∈ S I(X i ; X j ) min { H(X i ) ; H(X j ) } (8)</formula><p>One problem with the NMIFS approach is that only a part of the right-side mutual information is contained in the leftside mutual information: both terms of the subtraction in Eq. ( <ref type="formula">8</ref>) (NMIFS) are within the range [0, 1], but the value of Eq. ( <ref type="formula">8</ref>) (NMIFS) may be outside the range [0, 1]. Generally, only a part of the right-side mutual information is contained in the left-side mutual information. For example, if I ( X i ; X j ) ≥ I ( X i ; Y ) for any X j ∈ S , then f ( X ( i ) ) ≤ 0. On the other hand, the mutual information based selection algorithms focus mainly on the classification problem. This study extends them to regression problems.</p><p>The MIFS criterion consists of the relevancy and redundancy terms. An alternative approach was presented by Yang and Moody <ref type="bibr" target="#b50">[51]</ref> , and also later by Meyer et al. <ref type="bibr" target="#b35">[36]</ref> , Bennasar et al. <ref type="bibr" target="#b2">[3]</ref> and others, using the Joint Mutual Information (JMI), to integrate the relevancy and redundancy terms. The JMI score of the feature X i is defined as:</p><formula xml:id="formula_8">f (X i ) = X j ∈ S I(X i , X j ; Y ) (9) where I(X i , X j ; Y ) = H(Y ) -H(Y/X i , X j ) = [ - y ∈ Y p(y ) log(p(y ))] -[ - y ∈ Y x i ∈ X i x j ∈ X j p(x i , x j , y ) log(p(y/x i , x j ))]</formula><p>The JMIM score of the feature X i is defined as:</p><formula xml:id="formula_9">f (X i ) = max X j ∈ S I(X i , X j ; Y ) (10)</formula><p>The MIFS and JMI schemes were among the first of many criteria that attempted to manage the relevance-redundancy trade-off using various heuristic terms. However, it is clear that they have very different motivations <ref type="bibr" target="#b3">[4]</ref> . All these criteria can be concluded as some frameworks in terms of the information theory, where the overall aim is to manage the relevance-redundancy trade-off, and each new framework takes a different direction <ref type="bibr" target="#b3">[4]</ref> . Several questions arise here. Which framework should we believe? What assumptions do they make regarding the data? Are there other useful frameworks, yet undiscovered? In the following section we provide a novel perspective on this problem, and then present a new framework for feature selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Limitations of the existing feature selection approaches</head><p>For the feature selection problem with p variables, we can classify the features into three groups according to the following definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2.</head><p>Let Y be the target/dependent variable, I ( X i , Y ) be the population mutual information as given in Eq. ( <ref type="formula" target="#formula_18">1</ref>) , and I ( X i , S, Y ) be the common redundancy information among ( X i , S, Y ).</p><p>1. The relevant variable subset, denoted by RL , consists of the true variables included in the regression, that is I ( X i , Y ) &gt; 0 and I ( X i , Y ) &gt; I ( X i , S, Y ), for any X i ∈ RL .</p><p>2. The irrelevant variable subset, denoted by IR , consists of the indifferent variables, that is I(X i , Y ) = 0 , for any X i ∈ IR .</p><p>3. The redundant variable subset, denoted by RD , consists of the useful variables, which depend on some relevant variables, that is</p><formula xml:id="formula_10">I ( X i , Y ) &gt; 0 and I ( X i , Y ) ≤ I ( X i , S, Y ), for any X i ∈ RD .</formula><p>The goal of any variable selection problem should be to include relevant variables, and exclude irrelevant ones. Regarding redundant variables, they can also be viewed as useful variables, which depend on each other, such as two correlated variables <ref type="bibr" target="#b6">[7]</ref> . Here, if we make a mistake in measuring a relevant variable, the predictor may provide poor forecast. On the other hand, if the predictor is based on highly correlated redundant variables, we are likely to avoid mistakes when some measurement errors occur in one of the relevant variables. Based on the above analysis, the inclusion of all redundant variables is not necessary, and only some are needed to solve the problem. Overall, the goal of any feature selection problem should be to: (1) include relevant features; (2) exclude bad or irrelevant features; and (3) control the use of redundant features. Therefore, we want to do accurate calculation of the relevancy term to include relevant features and exclude irrelevant ones, without overestimation of the redundancy term to control the use of redundant features.</p><p>In this study, we group feature selection methods into three types: Type-1 uses conditional mutual information, Type-2 uses Max-Relevance and Min-Redundancy criteria, and Type-3 is the proposed feature selection framework that uses Max-Relevance and Min-Common-Redundancy criteria.</p><p>The limitations of Type-1 methods: Some recent methods, such as the JMIM <ref type="bibr" target="#b2">[3]</ref> , try to use conditional mutual information to estimate the amount of information the candidate feature X i contributes given a pre-selected feature subset S . This type of feature selection can be seen as a framework, as shown in Fig. <ref type="figure" target="#fig_0">1</ref> . However, such methods have the following two difficulties: first, accurate calculation of conditional mutual information is a difficult task due to the amount of calculations and limited number of observations available for the calculation of the 3-dimensional probability density function. Second, to extend the calculation of I ( X i , X j , Y ) to I ( X i , S, Y ), these methods use either cumulative sum approximation or 'maximum of the minimum' approximation, which is integrated approximation of the relevancy and redundancy terms. As a result, this type feature selection methods have the problem of overestimation or underestimation of feature significance. Therefore, this type of feature selection may exclude relevant features or include irrelevant ones.</p><p>The limitations of Type-2 methods: As an alternative, feature selection methods based on Max-Relevance and Min-Redundancy employ criteria consisting of two elements: the relevancy and redundancy terms. This type of feature selection can be seen as another framework, as shown in Fig. <ref type="figure" target="#fig_1">2</ref> . The methods provide accurate calculation of the relevancy term. However, they always overestimate the redundancy term. As a result, one problem with the NMIFS approach is that only a part of the right-side mutual information is contained in the left-side mutual information: both terms of the subtraction in Eq. ( <ref type="formula">8</ref>) (NMIFS) are within the range [0, 1], but the value of Eq. ( <ref type="formula">8</ref>) (NMIFS) may be outside the range [0, 1]. For example, if I ( X i ; X j ) ≥ I ( X i ; Y ) for any X j ∈ S , then f ( X ( i ) ) ≤ 0. Generally, only a part of the right-side mutual information is contained in the left-side mutual information. Similar problems always happen to feature selection methods using cumulative sum approximation, such as the Joint Mutual Information (JMI), Double Input Symmetrical Relevance (DISR), MIFS, mRMR, MIFS-ND and IGFS.</p><p>When the redundancy term is underestimated, the relevancy term is not weakened/affected. However, if the redundancy term is overestimated, the relevancy term is weakened/affected. The Type-2 framework regulates the relevancy and redundancy terms by setting a trade-off parameter; as a result, it may underestimate some features and overestimate other features.</p><p>Yet, another problem shared by normalized methods (NMIFS, NJMIM) is that local normalization is calculated based on the candidate feature and S , without the global consideration of the response/target feature. As a result, methods that employ the local normalized MI, such as the NJMIM and DISR, do not perform as well as the un-normalized methods JMIM and JMI <ref type="bibr" target="#b2">[3]</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">An improved MI approach to feature selection</head><p>Although there is significant progress, there are still some limitations of the existing methods, as pointed out above. In practice, the significance of each of the above methods is related to the characteristics of each particular dataset <ref type="bibr" target="#b2">[3]</ref> . In this section, we propose a novel maximum relevance and minimum common redundancy criterion for feature selection in regressions, which is expected to provide accurate calculation of the relevancy term to include relevant features and exclude irrelevant ones, and avoid overestimation of the redundancy term to control the use of redundant features. Furthermore, the existing mutual information based selection algorithms focus mainly on the classification problem, and this study extends them to regression problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The proposed new framework</head><p>By analyzing the shortcomings of the existing frameworks, we conclude that an improved framework should provide accurate estimation of the relevancy term, and avoid overestimation (with possible underestimation) of the redundancy term. To this end, a novel idea of feature selection is achieved through the introduction of a novel maximum relevance and minimum common redundancy criterion and 'maximum of the minimum' nonlinear approach. This type of feature selection can be seen as a new framework, as shown in Fig. <ref type="figure" target="#fig_2">3</ref> . Under the 'maximum of the minimum' nonlinear approach, this framework provides underestimation of the redundancy term, which includes relevant features and excludes irrelevant ones without damaging the relevancy term. Regarding redundant features, this framework may select some of them due to the underestimation of the redundancy term. Therefore, this framework can achieve our intended target: (1) include relevant features; (2) exclude irrelevant features; and (3) control the use of redundant features.</p><p>Specifically, this framework avoids the calculation of conditional mutual information, provides normalization with global consideration of the response/target feature, delivers accurate calculation of the relevancy term to include relevant features and exclude irrelevant ones, avoids overestimation of the redundancy term to control the use of redundant features, and uses a combination of relevancy and redundancy terms, rather than a trade-off approach, to make the relevancy and redundancy terms comparable.</p><p>Based on the idea of the proposed new framework, some feature selection algorithms may be proposed. In the following subsections, we present the maximum relevance and minimum common redundancy feature selection algorithm under the proposed framework. Of course, other selection algorithms based on this proposed framework are expected to be proposed in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Normalization MI for regression model</head><p>In a regression problem, the goal is to infer the uncertainty of the response variable Y by selecting the "best subset of co-variables". Here, we define the normalization MI for the regression model according to Theorem 1 , the proof of which is given in <ref type="bibr" target="#b12">[13]</ref> : Theorem 1. For any discrete random variables X and Y, I ( X, Y ) ≥ 0 . Moreover, I(X, Y ) = 0 if and only if X and Y have no relationship.</p><p>From Theorem 1 it follows that the value I ( X, Y ) can represent the inferential capability of X for the uncertainty of the response variable Y , i.e. the larger the value I ( X, Y ), the stronger the inferential capability of X . However, I ( X, Y ) can vary greatly for different X . We normalize this index by finding its supremum regarding the information of Y . I(X, Y ) = 0 means that observing X does not infer any uncertainty in Y . If, however, we observe Y , then we can infer all uncertainty in Y . Therefore, I ( X, Y ) is bounded by</p><formula xml:id="formula_11">I ( Y, Y ): 0 ≤ I(X, Y ) ≤ I(Y, Y ) (11)</formula><p>The above equation can also be proved using the theory of mutual information. We propose to use the self-information I ( Y, Y ) of the response variable Y as a criterion that measures the ratio of uncertainty in Y . Then, we propose to normalize the term I ( X, Y ) by dividing it by the self-information I ( Y, Y ) as follows:</p><formula xml:id="formula_12">NI (X, Y ) = I (X, Y ) I (Y, Y )<label>(12)</label></formula><p>If the response variable Y can be completely inferred by a co-variable X , then I(X, Y ) = I(Y, Y ) , and NI(X, Y ) = 1 . If X and Y are completely independent, then I(X, Y ) = 0 , and NI(X, Y ) = 0 . Therefore, the range of NI ( X, Y ) is [0, 1], and the value of NI ( X, Y ) denotes the degree of dependence between the co-variable and the response variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Measuring the common redundancy information by using MI</head><p>For a feature X j ∈ S ( S is the subset of selected features), the redundancy information between X j and a candidate feature X i can be measured by I ( X j , X i ), so the corresponding redundancy information ratio of the feature X i is defined as follows:</p><formula xml:id="formula_13">RI (X i , X j ) = I (X i ; X j ) Max { I (X i ; X j ) , I(X i ; Y ) , I(X j ; Y ) }<label>(13)</label></formula><p>Instead of computing the redundancy information between a candidate for a newly selected feature plus already selected features in S and the response feature Y , most MI-based approaches only compute I ( X i ; X j ). To be selected, a feature which cannot be predicted from the already selected features in S must be informative regarding the dependent feature Y . For this, we focus on the part of common redundancy related to the dependent feature Y . To make the left-side and right-side terms in Eq. ( <ref type="formula" target="#formula_6">7</ref>) comparable, we introduce the common mutual information (CI) CI ( X i , X j , Y ) by multiplying it by</p><formula xml:id="formula_14">Min { I ( X i ; Y ), I ( X j ; Y )} CI(X i , X j , Y ) = RI(X i ; X j ) × Min { I(X i ; Y ) , I(X j ; Y ) } = I(X i ; X j ) × Min { I(X i ; Y ) , I(X j ; Y ) } Max { I(X i ; X j ) , I(X i ; Y ) , I(X j ; Y ) } (14)</formula><p>As defined in the above equation, the mutual information among X i , X j and Y measures the amount of information commonly found in these features. For a feature set T = { X 1 , X 2 , . . . , X p } , the feature selection process identifies a subset of T , denoted by S = { X 1 , X 2 , . . . , X k } , having dimension k , where k ≤ p . Similarly, we extend the common MI CI ( X i , X j , Y ) to CI ( X i , S, Y ) defined as:</p><formula xml:id="formula_15">CI(X i , S, Y ) = I(X i ; S) Max { I(X i ; S) , I(X i ; Y ) , I(S; Y ) } × Min { I(X i ; Y ) , I(S; Y ) }<label>(15)</label></formula><p>where Theorem 2. For the common mutual information CI ( X i , S, Y ) as shown in Eq. ( <ref type="formula" target="#formula_15">15</ref>) , we have</p><formula xml:id="formula_16">I(S; Y ) = I(X 1 , X 2 , . . . , X k ; Y ) .</formula><formula xml:id="formula_17">1. CI ( X i , S, Y ) ≤ Min { I ( X i ; S ), I ( X i ; Y ), I ( S ; Y )} ; 2.</formula><p>As a function of I ( X i ; S ), CI ( X i , S, Y ) is monotonically increasing.</p><p>Proof.</p><p>(1) On the one hand, it is obvious that</p><formula xml:id="formula_18">I(X i ; S) Max { I(X i ; S) , I(X i ; Y ) , I(S; Y ) } ≤<label>1</label></formula><p>Therefore, we have CI(X i , S, Y ) ≤ Min { I(X i ; Y ) , I(S; Y ) }</p><p>On the other hand, it is clear that</p><formula xml:id="formula_19">Min { I(X i ; Y ) , I(S; Y ) } Max { I(X i ; S) , I(X i ; Y ) , I(S; Y ) } ≤ 1 Therefore, we have CI(X i , S, Y ) ≤ I(X i ; S)</formula><p>Integrating the above two aspects, we always have</p><formula xml:id="formula_20">CI(X i , S, Y ) ≤ Min { I(X i ; S) , I(X i ; Y ) , I(S; Y ) } .</formula><p>(2) From the definition of CI ( X i , S, Y ) in Eq. ( <ref type="formula" target="#formula_15">15</ref>) , we compute its derivative with respect to I ( X i ; S ):</p><formula xml:id="formula_21">dCI(X i , S, Y ) dI(X i ; S) = 0 I(X i ; S) ≥ Max { I(X i ; Y ) , I(S; Y ) } Min { I (X i ;Y ) ,I (S;Y ) } Max { I(X i ;Y ) ,I(S;Y ) } I(X i ; S) &lt; Max { I(X i ; Y ) , I(S; Y ) }<label>(16)</label></formula><p>If I ( X i ; S ) &lt; Max { I ( X i ; Y ), I ( S ; Y )}, the derivative in Eq. ( <ref type="formula" target="#formula_21">16</ref>) is clearly positive. Thus, we always have dCI(X i ,S,Y ) dI(X i ;S) ≥ 0 , and the conclusion follows.</p><p>Clearly, we have two observations. First, the common MI CI ( X i , S, Y ) measures the common information among I ( X i ; S ), I ( X i ; Y ) and I ( S ; Y ); so we can intuitively observe that CI ( X i , S, Y ) must be less than or equal to the smallest of I ( X i ; S ), I ( X i ; Y ) and I ( S ; Y ). Second, the redundancy information is measured by I ( X i ; S ), and the common redundancy information is measured by CI ( X i , S, Y ); so we can intuitively observe that CI ( X i , S, Y ) is generally monotonically increasing in I ( X i ; S ). These observations are consistent with the properties studied in Theorem 2 .</p><p>Most of the related methods attempt to approximate the solution for I ( X i ; S ) and I ( S ; Y ). A typical method is to employ cumulative sum approximation. Considering that the cumulative sum X j ∈ S { I(X i ; X j ) } contains the overlapping and stochastic information of S , we can obtain the following inequality:</p><formula xml:id="formula_22">X j ∈ S { I(X i ; X j ) Max { I(X i ; X j ) , I(X i ; Y ) , I(X j ; Y ) } × Min { I(X i ; Y ) , I(X j ; Y ) }} CI(X i , S, Y )<label>(17)</label></formula><p>Therefore, this method overestimates the common redundancy term. In this study, we want to achieve underestimation of the common redundancy term. To this end, an alternative approach is constructed inspired by the popular 'maximum of the minimum' nonlinear approach <ref type="bibr" target="#b20">[21]</ref> , which alleviates the problem of overestimation of feature significance both theoretically and experimentally <ref type="bibr" target="#b2">[3]</ref> . Next, we consider the minimum estimation of the redundancy term.</p><p>For the sake of simplicity, Max X j ∈ S { I(X i ;X j ) Max { I(X i ;X j ) ,I(X i ;Y ) ,I(X j ;Y ) } } is denoted by R i . The larger the value of R i , the stronger the dependence between X i and the subset S . R i = 0 indicates that X i and the subset S of selected features are independent; R i = 1 indicates that X i is highly correlated with S . Based on the above analysis, we propose to use the maximum common MI Max X j ∈ S { CI(X i , X j , Y ) } as a measure of redundancy between the candidate feature X i and the subset of selected features S with respect to Y ; that is:</p><formula xml:id="formula_23">CI(X i , S, Y ) = M ax X j ∈ S I(X i ; X j ) M ax { I(X i ; X j ) , I(X i ; Y ) , I(X j ; Y ) } × M in { I(X i ; Y ) , I(X j ; Y ) }<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Improved normalized mutual information approach</head><p>As described above, we measure the relevance information between X i and Y using the MI I ( X i , Y ), and measure the redundancy information between the candidate feature X i and the subset of selected features S with respect to Y using the term CI ( X i , S, Y ). According to the Max-Relevance and Min-Common-Redundancy criterion (MRMCR), the amount of mutual information is given by</p><formula xml:id="formula_24">f (X i ) = I(X i ; Y ) -CI(X i , S, Y ) (19)</formula><p>By applying the normalization method to the above equation, the resulting function (N-MRMCR-MI) can be written as</p><formula xml:id="formula_25">f (X i ) = I(X i ; Y ) I(Y ; Y ) - CI(X i , S, Y ) I(Y ; Y ) = I(X i ; Y ) I(Y ; Y ) -Max X j ∈ S I(X i ; X j ) × Min { I(X i ; Y ) , I(X j ; Y ) } Max { I(X i ; X j ) , I(X i ; Y ) , I(X j ; Y ) } × I(Y ; Y ) (20) X (k ) = arg max X i ∈ T [ f (X i )] (21)</formula><p>In this study, the proposed selection criterion used in the N-MRMCR-MI selects the feature that maximizes the above measure Eq. <ref type="bibr" target="#b19">(20)</ref> .</p><p>We prove in the following that Eq. ( <ref type="formula">20</ref>) is a correlation measure, i.e. the function Eq. ( <ref type="formula">20</ref>) takes values in [0, 1]. This type of selection is called a normalization scheme. We have the following theorem. Theorem 3. For any candidate variable X i , and the final goal function f ( X i ) as shown in Eq. ( <ref type="formula">20</ref>) , we have</p><formula xml:id="formula_26">0 ≤ f (X i ) ≤ 1 . (22) Furthermore, f (X i ) = 0 if</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and only if X i and Y are completely independent; f (X i ) = 1 if and only if the response variable Y can be completely inferred from X i .</head><p>Proof. Similar to the proof of Theorem 2 , we have</p><formula xml:id="formula_27">0 ≤ Max X j ∈ S I(X i ; X j ) × Min { I(X i ; Y ) , I(X j ; Y ) } Ma x { I( X i ; X j ) , I(X i ; Y ) , I(X j ; Y ) } ≤ I( X i ; Y ) . According to Subsection 4.2 , it is clear that 0 ≤ f (X i ) ≤ 1 .</formula><p>Thus, Eq. ( <ref type="formula">22</ref>) is proved.</p><p>It follows from Eq. ( <ref type="formula">22</ref>) that f (X i ) = 0 if and only if</p><formula xml:id="formula_28">I(X i ; Y ) = 0 ; f (X i ) = 1 if and only if I(X i ; Y ) = I(Y ; Y ) and I(X i ; S) = 0 .</formula><p>Thus, the conclusion follows. Clearly, the normalization method restricts the whole value of the final goal function to the range [0, 1]. Because of this, we refer to the term Max X j ∈ S { I(X i ;X j ) Max { I(X i ;X j ) ,I(X i ;Y ) ,I(X j ;Y ) } } as the redundancy factor.</p><p>The improved normalized mutual information approach N-MRMCR-MI using greedy selection is as follows.</p><p>1) Initialization: The full set of features T = (X 1 , X 2 , . . . , X p ) , the selected set of features S = ∅ .</p><p>2) Select the first feature: Compute f (X i ) = I(X i ;Y ) I(Y ;Y ) for all i = 1 , 2 , . . . , p, and</p><formula xml:id="formula_29">X i * = argmax i =1 , 2 , ... ,p { f (X i ) } . 3) Update the set of features: T = T -{ X i * } , and S = { X i * } .</formula><p>4) Greedy selection: Repeat until the desired number of features is selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.1) Compute the N-MRMCR-MI with the output variable</head><formula xml:id="formula_30">: Compute f (X i ) = I(X i ;Y ) I(Y ;Y ) - CI(X i ,S,Y )</formula><p>I(Y ;Y ) for all candidate features, and</p><formula xml:id="formula_31">X i * = argmax X i ∈ T { f (X i ) } . 4.2) Update the set of features: T = T -{ X i * } , and S = S { X i * } .</formula><p>5) Output the selected set S of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>The existing mutual information based selection algorithms focus mainly on the classification problem, and our study extends them to regression problems. We empirically evaluate the effectiveness of the proposed method by performing synthetic and real regression tasks in Sections 5.1 and 5.2 , respectively. For extensive comparison with the state-of-art methods, we also discuss the results of real classification tasks in Section 5.3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Simulation studies</head><p>To demonstrate the effectiveness of the proposed N-MRMCR-MI method, we present the simulation results for two simulated data sets, redundant features and highly correlated features. These two examples are chosen for the following three reasons. (i) Nonlinear simulated data sets are generated for testing the performance of the proposed N-MRMCR-MI method, where the identity of the true types of features is kept unknown to the designer while the method is being used <ref type="bibr" target="#b7">[8]</ref> . (ii) In the first case, it allows comparison of the three types of features, namely, relevant features, irrelevant features and redundant features. (iii) In the second case, it allows comparison of the more complex case, where there exists high correlation among the three types of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Redundant features</head><p>The first data set can be described by the following function.</p><formula xml:id="formula_32">Y = 10 sin (π X 1 X 3 ) + 1 . 5(X 5 -0 . 5) 2 + 1 . 3(X 7 + 0 . 5) 2 + 2 . 5 X 11<label>(23)</label></formula><p>In this simulated data set, there are p = 14 features and 150 observations generated from the standard normal distribution with the pairwise correlation ρ(X i , X j</p><formula xml:id="formula_33">) = 0 . 1 | i -j| ( i, j = 1 , 2 , 3 , 5 , 7 , 8 , 9 , 11 , 12 , 13 , 14 ), ρ(X i , X j ) = 0 . 01 ( i = j, i, j = 4 , 6 , 10 ) except for ρ(X 4 , X 5 ) = ρ(X 6 , X 7 ) = ρ(X 10 , X 11 ) = 0 . 5 and ρ(X 5 , X 4 ) = ρ(X 7 , X 6 ) = ρ(X 11 , X 10 ) = 0 . 5 .</formula><p>Then, the response Y is generated using the above function. To focus on the complex function itself, no noise is added. In the training phase, the identity of the true features is kept unknown to the learner while the algorithm is being used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">Highly correlated features</head><p>The second data set with highly correlated features can be described as follows. <ref type="bibr" target="#b23">(24)</ref> In this simulated data set, there are p = 20 features and 150 observations generated from the standard normal distribution with the pairwise correlation ρ(X i , X j</p><formula xml:id="formula_34">Y = X 3 1 -3 sin (5 π X 3 ) -2 . 8 sin (π X 5 ) -3 . 5(X 7 + 0 . 1) × (X 11 + 0 . 2) × (X 15 + 0 . 3) + 2 X 19</formula><formula xml:id="formula_35">) = 0 . 65 | i -j| ( i, j = 1 , 2 , • • • , 20 ).</formula><p>Then, the response Y is generated by the above function. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>The description of Boston housing data set.</p><p>Feature No. The type of feature 1 → 13 The original 13 covariates 14 → 23</p><p>The irrelevant 10 features independently sampled from Uniform (0, 1) 24 → 36</p><p>The irrelevant (maybe redundant) 13 features generated by a random permutation 37 → 43</p><p>The irrelevant (maybe redundant) 7 features generated by by a random linear combination</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head><p>The results for Boston housing data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Ordering and ranking of features </p><formula xml:id="formula_36">(top → bottom) N-MRMCR-MI<label>13</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.3. Experimental results</head><p>The selection results using the mRMR and NMIFS methods are compared and summarized in Tables <ref type="table" target="#tab_1">1</ref> and<ref type="table" target="#tab_2">2</ref> , respectively (the bold values are significant). One of our tasks is to identify truly relevant features. For the data set of redundant features, the N-MRMCR-MI method has stronger ability to filter out redundant and non-informative features than the other two comparison methods (Features (6, 10) are redundant and features <ref type="bibr" target="#b8">(9,</ref><ref type="bibr" target="#b11">12)</ref> are weakly redundant). For the data set of highly correlated features, Features 3 and 11 are difficult to find for two reasons. First, the first-order effect of Feature 3 is zero. Second, each feature <ref type="bibr" target="#b6">(7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15)</ref> has small total effect, as it is only a part of three-way interaction effect with the other two features. Overall, we can observe that the proposed N-MRMCR-MI method is able to exclude unimportant features and identify nearly all important features, and has better performance than the mRMR and NMIFS methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Analyzing Boston housing data set</head><p>We demonstrate the main part of the method on Boston housing data set, which is available from the UCI Machine Learning Repository ( http://www.archive.ics.uci.edu/ml/datasets.html ). In this dataset, the median housing price is the feature to be forecasted. The data has 506 observations and 13 covariates, which might be useful in describing the median housing price. Inspired by the idea of Mkhadri et al. <ref type="bibr" target="#b36">[37]</ref> , we introduce thirty irrelevant variables to demonstrate the sparsistency properties of our method. Ten of them are randomly drawn from the Uniform (0,1), thirteen of them are generated by a random permutation of the original thirteen covariates, and the rest are generated by a random linear combination of the original thirteen covariates <ref type="bibr" target="#b36">[37]</ref> . The description of Boston housing data set is shown in Table <ref type="table">3</ref> .</p><p>By using the mRMR, NMIFS and N-MRMCR-MI methods, the order of all features is summarized in Table <ref type="table">4</ref> . Note that the bold values are the original 13 covariates. From the results for the N-MRMCR-MI method, we can easily see that the 13th, 11th, 6th, 1st, 7th, 12th and 3rd features are clearly significant. The following are the social implications of these seven features. <ref type="bibr" target="#b12">13</ref>.</p><p>LSTAT: % lower status of the population. 11.</p><p>PTRATIO: pupil-teacher ratio by town. 6.</p><p>RM: average number of rooms per dwelling. 1.</p><p>CRIM: per capita crime rate by town. 7.</p><p>AGE: proportion of owner-occupied units built prior to 1940. 12.</p><p>B: 10 0 0(Bk -0 . 63) 2 where Bk is the proportion of blacks by town.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>INDUS: proportion of non-retail business acres per town.</p><p>However, from the results of the mRMR and NMIFS methods, we can only see that the 13th, 6th and 3rd features are clearly significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5</head><p>The average RMSE and its improvement when all the original thirteen covariates are progressively employed as explanatory features.</p><p>No.</p><p>Selected If all the original thirteen covariates are progressively employed as explanatory features, the average RMSE and the improvement RMSE are calculated, as shown in Table <ref type="table">5</ref> . The predicted results are obtained by running the BART package. From the above results, we can observe that the 6th, 13th and 3rd features are clearly significant. This observation is consistent with the results in Table <ref type="table">4</ref> . However, some other weak relevant features will be affected by redundancy and interaction effects, and are therefore hard to be detected. Following this analysis, we conclude that the N-MRMCR-MI method detects 4 weak relevant features significantly.</p><p>The proposed feature selection method does not convolve with specific forecasts. To evaluate the performance of the selection method, we perform 10-fold cross validation with two widely used forecast methods, namely, the TGP and BART packages. Then, the average RMSE in the 10 simulations is used to measure the performance.</p><p>Following our analysis, if we reduce the inputs to the first seven features listed above, the performance results becomes as shown in Table <ref type="table">6</ref> . For the N-MRMCR-MI method, the average RMSE for predictions is 5.723688 and 4.35747 for the TGP and BART packages, respectively. However, for the mRMR and NMIFS methods, the average RMSE for predictions is 7.064677 and 5.578657 for the TGP and BART packages, respectively. We can conclude that the proposed N-MRMCR-MI method performs better in selecting significant features with potentially nonlinear effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">An extensive comparison with state-of-art methods</head><p>In this subsection, the performance of the presented method N-MRMCR-MI is compared with the results produced by three other methods: the JMI, JMIM, and NMIFS. These methods are chosen for the following four reasons. (i) The JMI method is reported in the study to provide good performance in terms of classification accuracy and stability <ref type="bibr" target="#b3">[4]</ref> , and the JMIM method is reported to perform better than the other methods (such as the CMIM, DISR, mRMR, IG) on most tested public datasets, reducing almost by 6% the relative average error in comparison to the next best performing method <ref type="bibr" target="#b2">[3]</ref> .</p><p>(ii) This allows the comparison of the effects of a Type-1 framework with the cumulative summation and "maximum of the minimum" approach, which are employed in the JMI and JMIM, respectively. (iii) This allows the comparison of the effects of a Type-2 framework, which is employed in the NMIFS. (iv) The NMIFS is a typical representative of the Type-2 framework, and the mRMR and NMIFS have similar performance based on the above simulations.</p><p>The four methods are applied to the four datasets as shown in Table <ref type="table" target="#tab_5">7</ref> . In order to clearly show the property, we introduce thirty irrelevant features (some of which are probably redundant features) in the first 2 datasets. Similar to the idea of <ref type="bibr" target="#b36">Mkhadri et al. (2015)</ref>, ten of them are randomly drawn from the Uniform ( min ( X ), max ( X )), ten of them are generated by a random permutation of the original ten covariates, and the rest are generated by a random linear combination of the original covariates <ref type="bibr" target="#b36">[37]</ref> .</p><p>Two classifiers 1-Nearest Neighbour (1-NN) and support vector machine (SVM) are employed to evaluate the quality of the selected subsets. Five-fold cross-validation (5-fold CV) is used to compute the average test accuracy. Before the feature selection, discretization is used for all data.</p><p>Figs. 4-7 show the average test accuracy for the two relatively low dimensional datasets (Ionosphere and Lung cancer) by using 1-NN and SVM, respectively. Figs. <ref type="figure" target="#fig_3">4</ref> and<ref type="figure" target="#fig_4">5</ref> show that, for the Ionosphere dataset, the overall performance of the four     methods is similar. The N-MRMCR-MI once again achieves slightly higher accuracy when the feature number is close to the number of original covariates, within the range between 20 and 29. On the whole, the N-MRMCR-MI achieves the highest average test accuracy, 95.14% and 95.43%, for 1-NN and SVM, respectively; higher than the accuracy of the JMI (94.29% and 94.57%), JMIM (94.86% and 95.14%), and NMIFS (94.29% and 94.57%). As shown in Table <ref type="table" target="#tab_6">8</ref> , on the other hand, Type-1 methods that use conditional mutual information, such as the JMI and JMIM, have higher computational complexity than the N-MRMCR-MI and NMIFS, which use mutual information. This is expected for all datasets, because conditional mutual information requires a lot of calculations for the estimation of the 3-dimensional probability density function. Note that, for the low dimensional datasets (Ionosphere and Lung cancer), calculating and storing the mutual information matrix of all features make them faster. Figs. <ref type="figure" target="#fig_5">6</ref> and<ref type="figure" target="#fig_6">7</ref> show that, for the Lung cancer dataset, the N-MRMCR-MI and NMIFS have better overall performance than the JMI and JMIM. On the whole, the N-MRMCR-MI achieves the highest average test accuracy, 76.6 6 6 67% and 80%, for 1-NN and SVM, respectively; higher than the accuracy of the JMI (56.6 6 6 67% and 6 6.6 6 6 67%), JMIM (53.33333% and 6 6.6 6 6 67%), and NMIFS (73.33333% and 80%).</p><p>Though the SVM has better performance than the 1-NN, the accuracy curves are consistent for both datasets, which demonstrates the credibility of the evaluation.</p><p>For the high-dimensional microarrays datasets, we use the 3-NN to reduce the effect of small samples. Figs. <ref type="figure">8</ref> and<ref type="figure" target="#fig_7">9</ref> show the average test accuracy of the high dimensional dataset (Colon) by using the 3-NN and SVM, respectively. The N-MRMCR-MI achieves slightly higher accuracies when the feature number is within the range between 30 and 35. On the whole, the N-MRMCR-MI achieves the highest average test accuracy, 85% and 85%, for the 3-NN and SVM, respectively; slightly higher than the accuracy of the JMI (85% and 83.33%), JMIM (85% and 83.33%), and NMIFS (83.33333% and 81.67%). Figs. <ref type="figure" target="#fig_8">10</ref> and<ref type="figure" target="#fig_9">11</ref> show the average test accuracy of the high dimensional dataset (Leukemia) by using the 3-NN and SVM, respectively. For this microarrays dataset, we can see that all methods chosen for feature selection are reasonable, and the N-MRMCR-MI achieves the highest average test accuracy for both 3-NN and SVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Significance test</head><p>In this part, a statistical significance test is conducted to compare the methods.</p><p>For the regression on the synthetic datasets and Boston housing dataset, we know beforehand the type of each candidate feature, so we do not need to do a hypothesis test, and only need to analyze the selection results. However, for the classification using real datasets with many features, we often do not know the type of each candidate feature, especially for high-dimensional datasets. Therefore, it is difficult to analyze the selection results directly. Accordingly, we employ test accuracy on a selected subset of features as an indirect evaluation, and then conduct a hypothesis test in order to show the effectiveness of a feature selection algorithm.</p><p>In Tables <ref type="table">9</ref> and<ref type="table" target="#tab_7">10</ref> , the highest 5-fold cross-validation (5-CV) averaged accuracy of the NN and SVM is shown, respectively. For the experiments of the extensive comparison, a paired two-sided t -test by using R is applied to evaluate the statistical significance of the difference between two 5-CV classification accuracies: one comes from the N-MRMCR-MI and the other comes from the JMIM, JMI or NMIFS. The null hypothesis of this hypothesis test is that true difference in means is equal  to 0. Each value in the p -value column reports the probability w.r.t. the Student's paired two-sided t -test. The smaller the p -value, the more significant the difference between two 5-CV classification accuracies is. At significance level 0.1, the last row (L/W/T) in Tables 9 and 10 summarizes the losses/wins/ties (L/W/T) in 5-CV classification accuracy over the classification tasks, comparing various feature subsets with those selected by the N-MRMCR-MI. We can observe that, in general, the N-MRMCR-MI achieves higher or significantly higher accuracy compared with the three comparison methods in the above cases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 9</head><p>The accuracy of the NN on selected features for the experiments of extensive comparison: Acc denotes 5-CV averaged accuracy rate and p -Val denotes the probability w.r.t. the Students paired two-sided t -test. The symbols "+"and "-" indicates statistically significant (at significance level 0.1) wins or losses over N-MRMCR-MI. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>Feature selection for regression systems is a complicated task, which becomes even more difficult when high nonlinearity exists. In this paper, a novel feature selection method based on the normalization of maximum relevance and minimum common redundancy (N-MRMCR-MI) is presented, which integrates the advances of existing methods using max-relevance and min-redundancy (mRMR) and normalized mutual information feature selection (NMIFS). For the proposed minimal common redundancy metric, the normalized term measures the essential information of selected features, which produces a normalized value within the range [0, 1], and extends the NMIFS approach to the regression problem. We analyze the theoretical properties of the common redundancy CI ( X i , S, Y ), where the resulting function Eq. ( <ref type="formula">20</ref>) is proved to be a correlation measure, i.e. this "scoring" criterion takes values in [0, 1].</p><p>The proposed framework aims to overcome the limitations of the state-of-art filter feature selection methods, such as the overestimation of feature significance, which causes the selection of redundant and irrelevant features. The effectiveness of the introduced framework is empirically evaluated using both simulation studies and real data sets and two different tasks: classification and regression. We demonstrate the superiority of the proposed method on two synthetic datasets (the regression task) and a real benchmark dataset (the regression task of Boston housing), as shown in Sections 5.1 and 5.2 , respectively. In addition, we perform extensive comparison with the state-of-art methods, as shown in Section 5.3 (the classification task). The experiments using eight different data sets and different forecasts (the Bayesian Additive Regression tree, treed Gaussian process, k-NN, and SVM) show the computational superiority of the proposed algorithm.</p><p>Future works may appear in the following directions. Further improvements can be obtained by improving the estimation of the redundancy term. It is of great interest to combine a metaheuristic-based strategy, such as <ref type="bibr" target="#b22">[23]</ref> , with our selection criterion. Some recent data-driven algorithms have also become very promising and popular in the big data age <ref type="bibr">[15,28,45,4 8,4 9,52,53]</ref> . Given certain features in common between data-driven and MI-based algorithms, it is useful to integrate such data-driven algorithms to further improve the effectiveness of the MI based algorithm. To reduce computational costs, some distributed algorithms or decomposition techniques <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b31">32]</ref> can be introduced into the feature selection process to achieve desired performance. Last but not the least, by analyzing the properties of the "score" function, one can automatically control the size of the optimal feature subset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A framework of Type-1 feature selection.</figDesc><graphic coords="5,82.73,177.32,278.68,61.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. A framework of Type-2 feature selection.</figDesc><graphic coords="6,81.71,56.06,376.84,178.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. A framework of Type-3 feature selection.</figDesc><graphic coords="7,86.03,55.88,376.39,178.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The average test accuracy achieved for the ionosphere dataset. Classifier 1-NN was used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The average test accuracy achieved for the ionosphere dataset. Classifier SVM was used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The average test accuracy achieved for the Lung cancer dataset. Classifier 1-NN was used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The average test accuracy achieved for the Lung cancer dataset. Classifier SVM was used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. The average test accuracy achieved for the Colon dataset. Classifier SVM was used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. The average test accuracy achieved for the Leukemia dataset. Classifier 3-NN was used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. The average test accuracy achieved for the Leukemia dataset. Classifier SVM was used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>However, accurate calculation of I ( S ; Y ) is impractical, due to the amount of calculations and limited number of observations available for the calculation of the high-dimensional probability density function. Therefore, many researchers employ heuristic methods to approximate the ideal solution. First, we provide analysis of CI ( X</figDesc><table /><note><p>i , S, Y ); then, we propose approximation of I ( S ; Y ). The properties of the function CI ( X i , S, Y ) are summarized in the following theorem.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>The results for the first simulated data set.</figDesc><table><row><cell>Method</cell><cell cols="6">Ordering and ranking of features</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">(top → bottom)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>N-MRMCR-MI</cell><cell>5</cell><cell>7</cell><cell>3</cell><cell>1</cell><cell>11</cell><cell>9</cell><cell>12</cell><cell>10</cell><cell>6</cell><cell>13</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>14</cell></row><row><cell>mRMR</cell><cell>5</cell><cell>1</cell><cell>12</cell><cell>6</cell><cell>9</cell><cell>10</cell><cell>7</cell><cell>13</cell><cell>3</cell><cell>11</cell><cell>14</cell><cell>8</cell><cell>2</cell><cell>4</cell></row><row><cell>NMIFS</cell><cell>5</cell><cell>1</cell><cell>12</cell><cell>6</cell><cell>9</cell><cell>10</cell><cell>7</cell><cell>3</cell><cell>13</cell><cell>11</cell><cell>14</cell><cell>8</cell><cell>2</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>The results for the second simulated data set.</figDesc><table><row><cell>Method</cell><cell cols="6">Ordering and ranking of features</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">(top → bottom)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>N-MRMCR-MI</cell><cell>1</cell><cell>19</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>7</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>9</cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell><cell>16</cell><cell>17</cell><cell>18</cell><cell>20</cell></row><row><cell>mRMR</cell><cell>1</cell><cell>19</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>2</cell><cell>15</cell><cell>6</cell><cell>11</cell><cell>12</cell><cell>17</cell><cell>9</cell><cell>7</cell><cell>3</cell><cell>14</cell><cell>4</cell><cell>16</cell><cell>8</cell><cell>13</cell><cell>18</cell></row><row><cell>NMIFS</cell><cell>1</cell><cell>19</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>2</cell><cell>15</cell><cell>6</cell><cell>11</cell><cell>12</cell><cell>9</cell><cell>17</cell><cell>7</cell><cell>3</cell><cell>14</cell><cell>4</cell><cell>16</cell><cell>8</cell><cell>13</cell><cell>18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>, 11, 6, 1, 7, 12, 3</head><label></label><figDesc>, 33, 35, 24, 16, 28, 29, 4 , 17, 20, 31, 30, 32, 14, 25, 27, 36 34, 26, 19, 22, 2, 5, 8, 9, 10 , 15, 18, 21, 23, 37, 38, 39, 40 ,41, 42, 43 mRMR 13, 6, 3 , 37, 11, 5 , 43, 7 , 38 , 39 10, 2, 1, 8, 12 , 42, 17, 27, 9 , 25, 15, 18, 29 26, 31, 35, 4 , 14, 19, 20, 33, 30, 21, 36, 16, 24, 28, 22, 32, 34, 23, 41, 40 NMIFS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7</head><label>7</label><figDesc>Datasets used in the extensive experiments.</figDesc><table><row><cell>No.</cell><cell>Data set</cell><cell>Number of features</cell><cell>Number of samples</cell><cell>Classes</cell></row><row><cell>1</cell><cell>Ionosphere</cell><cell>34(original covariates) + 10(random uniform)</cell><cell>351</cell><cell>2</cell></row><row><cell></cell><cell></cell><cell>+ 10(random permutation) + 10(random linear combination)</cell><cell></cell><cell></cell></row><row><cell>2</cell><cell>Lung cancer</cell><cell>56(original covariates) + 10(random uniform)</cell><cell>32</cell><cell>3</cell></row><row><cell></cell><cell></cell><cell>+ 10(random permutation) + 10(random linear combination)</cell><cell></cell><cell></cell></row><row><cell>3</cell><cell>Colon</cell><cell>20 0 0(original covariates)</cell><cell>62</cell><cell>2</cell></row><row><cell>4</cell><cell>Leukemia</cell><cell>7070(original covariates)</cell><cell>72</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8</head><label>8</label><figDesc>The average running time obtained using 5-fold cross-validation. The average test accuracy achieved for the Colon dataset. Classifier 3-NN was used.</figDesc><table><row><cell>Mehods</cell><cell>N-MRMCR-MI</cell><cell>JMI</cell><cell>JMIM</cell><cell>NMIFS</cell></row><row><cell>Ionosphere</cell><cell>2.518</cell><cell>1012.628</cell><cell>1012.748</cell><cell>2.206</cell></row><row><cell>Lung cancer</cell><cell>2.964</cell><cell>868.124</cell><cell>868.644</cell><cell>2.17</cell></row><row><cell>Colon</cell><cell>4664.398</cell><cell>44331.86</cell><cell>45139.74</cell><cell>2380.608</cell></row><row><cell>Leukemia</cell><cell>2775.486</cell><cell>14728.37</cell><cell>16351.62</cell><cell>1202.802</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 10</head><label>10</label><figDesc>The accuracy of the SVM on selected features for the experiments of extensive comparison: Acc denotes 5-CV averaged accuracy rate and p -Val denotes the probability w.r.t. the Students paired two-sided t test. The symbols "+ " and "-" indicates statistically significant (at significance level 0.1) wins or losses over N-MRMCR-MI.</figDesc><table><row><cell>Title</cell><cell>N-MRMCR-MI</cell><cell>JMI</cell><cell></cell><cell>JMIM</cell><cell></cell><cell>NMIFS</cell><cell></cell></row><row><cell></cell><cell>Acc</cell><cell>Acc</cell><cell>p -Val</cell><cell>Acc</cell><cell>p -Val</cell><cell>Acc</cell><cell>p -Val</cell></row><row><cell>Ionosphere</cell><cell>0.9514286</cell><cell>0.94</cell><cell>0.0993 -</cell><cell>0.9485714</cell><cell>0.04019 -</cell><cell>0.94</cell><cell>0.0993 -</cell></row><row><cell>Lung cancer</cell><cell>0.7666667</cell><cell>0.5666667</cell><cell>0.1087</cell><cell>0.5333333</cell><cell>0.05161 -</cell><cell>0.7333333</cell><cell>0.6213</cell></row><row><cell>Colon</cell><cell>0.85</cell><cell>0.85</cell><cell>1</cell><cell>0.85</cell><cell>1</cell><cell>0.8333333</cell><cell>0.3739</cell></row><row><cell>Leukemia</cell><cell>0.9714286</cell><cell>0.9714286</cell><cell>1</cell><cell>0.9714286</cell><cell>1</cell><cell>0.9714286</cell><cell>1</cell></row><row><cell>L/W/T</cell><cell>-</cell><cell>1/0/3</cell><cell></cell><cell>2/0/2</cell><cell></cell><cell>1/0/3</cell><cell></cell></row><row><cell>Title</cell><cell>N-MRMCR-MI</cell><cell>JMI</cell><cell></cell><cell>JMIM</cell><cell></cell><cell>NMIFS</cell><cell></cell></row><row><cell></cell><cell>Acc</cell><cell>Acc</cell><cell>p -Val</cell><cell>Acc</cell><cell>p -Val</cell><cell>Acc</cell><cell>p -Val</cell></row><row><cell>Ionosphere</cell><cell>0.9542857</cell><cell>0.9428571</cell><cell>0.0993 -</cell><cell>0.9514286</cell><cell>0.6213</cell><cell>0.9428571</cell><cell>0.0993 -</cell></row><row><cell>Lung cancer</cell><cell>0.8</cell><cell>0.6666667</cell><cell>0.2943</cell><cell>0.6666667</cell><cell>0.1778</cell><cell>0.8</cell><cell>1</cell></row><row><cell>Colon</cell><cell>0.85</cell><cell>0.8333333</cell><cell>0.6213</cell><cell>0.8333333</cell><cell>0.6213</cell><cell>0.8166667</cell><cell>0.1778</cell></row><row><cell>Leukemia</cell><cell>0.9857143</cell><cell>0.9857143</cell><cell>1</cell><cell>0.9714286</cell><cell>0.3739</cell><cell>0.9714286</cell><cell>0.3739</cell></row><row><cell>L/W/T</cell><cell>-</cell><cell>1/0/3</cell><cell></cell><cell>0/0/4</cell><cell></cell><cell>1/0/3</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We especially want to acknowledge Professor YUAN Jing, Professor James E. Fowler for invaluable writing support and improving the English expression. We would also like to thank the anonymous reviewers and the editor-in-chief for their helpful comments to improve the paper. The research was supported by National Natural Science Foundation of China (Grant No. 71301067 , Grant No. 61573266 , and Grant No. 11561047 ), and Natural Science Foundation of JiangXi Province (Grant No. 20142BAB217015 ). The research was also supported by the Jiangxi Provincial Humanities and Social Sciences Research Project (Grant No. JC162020).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Feature selection with SVD entropy: some modification and extension</title>
		<author>
			<persName><forename type="first">M</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">264</biblScope>
			<biblScope unit="page" from="118" to="134" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Using mutual information for selecting features in supervised neural net learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Battiti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="537" to="550" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Feature selection using joint mutual information maximisation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bennasar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Setchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="8520" to="8532" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Conditional likelihood maximisation: a unifying framework for information theoretic feature selection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pocock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="27" to="66" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A comparison of simulated annealing algorithms for variable selection in principal component analysis and discriminant analysis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Brusco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Stat. Data Anal</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="38" to="53" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Selecting useful groups of features in a connectionist framework</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="381" to="396" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Feature selection using a neural framework with controlled redundancy</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn.Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="50" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Variable selection via a multi-stage strategy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Stat</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="762" to="774" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stochastic correlation coefficient ensembles for variable selection</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1080/02664763.2016.1221913</idno>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Stat</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Maximum relevancy maximum complementary feature selection for multi-sensor activity recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chernbumroong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="573" to="583" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BART: Bayesian additive regression trees</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Chipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">I</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Mcculloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Appl. Stat</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="266" to="298" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bayesian methods for tree based models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcculloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bayestree</forename></persName>
		</author>
		<ptr target="http://www.cran.r-project.org/web/packages/BayesTree/index.html" />
	</analytic>
	<monogr>
		<title level="m">R Package Version 0.3-1.1</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<title level="m">Elements of Information Theory</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley and Sons, Inc</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Algorithms for subset selection in linear regression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kempe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing</title>
		<meeting>the Fortieth Annual ACM Symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="45" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical estimation algorithms for multivariable systems using measurement information</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">277</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="396" to="405" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Parameter estimation for pseudo-linear systems using the auxiliary model and the decomposition technique</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alsaedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Control Theory Appl</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="390" to="400" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Decomposition based least squares iterative identification algorithm for multivariate pseudo-linear ARMA systems using the data filtering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Franklin Inst</title>
		<imprint>
			<biblScope unit="volume">354</biblScope>
			<biblScope unit="page" from="1321" to="1339" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Performance analysis of the generalised projection identification for time-varying systems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Control Theory Appl</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="2506" to="2514" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Binary grey wolf optimization approaches for feature selection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Emary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Zawbaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Hassanien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="371" to="381" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Normalized mutual information feature selection</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Estévez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="189" to="201" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast binary feature selection with conditional mutual information</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1531" to="1555" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An evaluation of classifier-specific filter measure performance for feature selection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kuli Ć</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Basir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1812" to="1826" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">High-dimensional feature selection via feature grouping: a variable neighborhood search approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>García-Torres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gómez-Vela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Melián-Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Moreno-Vega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">326</biblScope>
			<biblScope unit="page" from="102" to="118" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">tgp: An r package for bayesian nonstationary, semiparametric nonlinear regression and design by treed gaussian process models</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Gramacy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Stat. Softw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Entropy inference and the james-stein estimator, with application to nonlinear gene association networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Strimmer</surname></persName>
		</author>
		<ptr target="http://www.strimmerlab.org/software/entropy/" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1469" to="1484" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Effective feature selection scheme using mutual information</title>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W S</forename><surname>Chow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="325" to="343" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Irrelevant feature and the subset selection problem</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pfleger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Machine Learning</title>
		<meeting>the Eleventh International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="121" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adaptive region boosting method with biased entropy for path planning in changing environment</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CAAI Trans. Intell. Technol</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="179" to="188" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Kernel discriminant analysis for regression problems</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<date type="published" when="2012">2012. 2019-2031</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Input feature selection for classification problems</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="143" to="159" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Toward optimal feature selection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Machine Learning</title>
		<meeting>the Thirteenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="284" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distributed filtering for discrete-time linear systems with fading measurements and time-correlated noise</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Process</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="211" to="219" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
		<title level="m">Information Theory, Inference, and Learning Algorithms</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Kernel penalized k-means: a feature selection method based on kernel k-means</title>
		<author>
			<persName><forename type="first">S</forename><surname>Maldonado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Carrizosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">322</biblScope>
			<biblScope unit="page" from="150" to="160" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the use of variable complementarity for feature selection in cancer classification</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bontempi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Workshop on Applications of Evolutionary Computing</title>
		<meeting>European Workshop on Applications of Evolutionary Computing</meeting>
		<imprint>
			<publisher>Evo Workshops</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="91" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Information-theoretic feature selection in microarray data using variable complementarity</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schretter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bontempi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Signal Process</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="261" to="274" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A group VISA algorithm for variable selection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mkhadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ouhourane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Methods Appl</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="41" to="60" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Feature selection based on mutual information: criteria of max-dependency, max-relevance, and min-redundancy</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach.Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1226" to="1238" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mixed integer second-order cone programming formulations for variable selection in linear regression</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ryuhei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yuichi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">247</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="721" to="731" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Neural network feature selector</title>
		<author>
			<persName><forename type="first">R</forename><surname>Setiono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="654" to="662" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">mr2PSO: a maximum relevance minimum redundancy feature selection method based on swarm intelligence for support vector machine classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Unler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Murat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Chinnam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="4625" to="4641" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Object recognition with informative features and linear classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vidal-Naquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th IEEE International Conference on Computer Vision</title>
		<meeting>the 10th IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="281" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An improved maximum relevance and minimum redundancy feature selection algorithm based on normalized mutual information</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Thang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth International Symposium on Applications and the Internet</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="395" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Axiomatic approach to feature subset selection based on relevance</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Murtagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach.Intell</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="271" to="277" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Data filtering based recursive least squares algorithm for hammerstein systems using the key-term separation principle</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">222</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="203" to="212" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A multi-objective evolutionary algorithm for feature selection based on mutual information with a new redundancy measure</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">307</biblScope>
			<biblScope unit="page" from="73" to="88" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Parameter estimation algorithms for multivariable hammerstein CARMA systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="page" from="237" to="248" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">The damping iterative parameter identification method for dynamical systems based on the sine signal measurement, Signal Process</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="660" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Application of the newton iteration algorithm to the parameter estimation for dynamical systems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">288</biblScope>
			<biblScope unit="page" from="33" to="43" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by feature clustering for regression problems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">299</biblScope>
			<biblScope unit="page" from="42" to="57" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Feature selection based on joint mutual information</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moody</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International ICSC Symposium on Advances in Intelligent Data Analysis</title>
		<meeting>International ICSC Symposium on Advances in Intelligent Data Analysis</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="22" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Improved PLS focused on key performance indictor related fault diagnosis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kaynak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1651" to="1658" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Data-driven process monitoring based on modified orthogonal projections to latent structures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Control Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1480" to="1487" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Efficient feature selection via analysis of relevance and redundancy</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1205" to="1224" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
