<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Protein Structure Representation Learning by Geometric Pretraining</title>
				<funder>
					<orgName type="full">Amazon Faculty Research Award, Tencent AI Lab Rhino-Bird Gift Fund</orgName>
				</funder>
				<funder ref="#_3jwsKuT">
					<orgName type="full">NRC Collaborative R&amp;D Project</orgName>
				</funder>
				<funder>
					<orgName type="full">Canada CIFAR AI Chair Program</orgName>
				</funder>
				<funder ref="#_QvmPrE9">
					<orgName type="full">Natural Sciences and Engineering Research Council</orgName>
					<orgName type="abbreviated">NSERC</orgName>
				</funder>
				<funder ref="#_Dh5sJZC">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder>
					<orgName type="full">Microsoft Research and Mila</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zuobai</forename><surname>Zhang</surname></persName>
							<email>&lt;zuobai.zhang@mila.quebec&gt;</email>
							<affiliation key="aff0">
								<orgName type="institution">Mila -Qu?bec AI Institute</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Universit? de Montr?al</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minghao</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mila -Qu?bec AI Institute</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Universit? de Montr?al</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Arian</forename><surname>Jamasb</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vijil</forename><surname>Chenthamarakshan</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">IBM</orgName>
								<address>
									<addrLine>Research 5 HEC Montr?al 6</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">CIFAR AI Chair</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aur?lie</forename><surname>Lozano</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">IBM</orgName>
								<address>
									<addrLine>Research 5 HEC Montr?al 6</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">CIFAR AI Chair</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Payel</forename><surname>Das</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">IBM</orgName>
								<address>
									<addrLine>Research 5 HEC Montr?al 6</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">CIFAR AI Chair</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
							<email>&lt;jian.tang@hec.ca&gt;.</email>
							<affiliation key="aff0">
								<orgName type="institution">Mila -Qu?bec AI Institute</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Jian Tang</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Protein Structure Representation Learning by Geometric Pretraining</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning effective protein representations is critical in a variety of tasks in biology such as predicting protein function or structure. Existing approaches usually pretrain protein language models on a large number of unlabeled amino acid sequences and then finetune the models with some labeled data in downstream tasks. Despite the effectiveness of sequence-based approaches, the power of pretraining on smaller numbers of known protein structures has not been explored for protein property prediction, though protein structures are known to be determinants of protein function. In this paper, we propose to pretrain protein representations according to their 3D structures. We first present a simple yet effective encoder to learn protein geometry features. We pretrain the protein graph encoder by leveraging multiview contrastive learning and different selfprediction tasks. Experimental results on both function prediction and fold classification tasks show that our proposed pretraining methods outperform or are on par with the state-of-the-art sequence-based methods using much less data. All codes and models will be published upon acceptance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Proteins are workhorses of the cell and are implicated in a broad range of applications ranging from therapeutics to material. They consist of a linear chain of amino acids (residues) folding into a specific 3D conformation. Due to the advent of low cost sequencing technologies, in recent years a massive volume of protein sequences has been newly discovered. As the function annotation of a new protein sequence remains costly and time-consuming, accurate and As a large number of protein functions are governed by their folded structures, several data-driven approaches rely on learning representations of the protein structures, which then can be used for a variety of tasks such as protein design <ref type="bibr" target="#b36">(Ingraham et al., 2019;</ref><ref type="bibr" target="#b69">Strokach et al., 2020;</ref><ref type="bibr" target="#b12">Cao et al., 2021;</ref><ref type="bibr" target="#b38">Jing et al., 2021)</ref>, structure classification <ref type="bibr" target="#b33">(Hermosilla et al., 2021)</ref>, model quality assessment <ref type="bibr" target="#b3">(Baldassarre et al., 2021;</ref><ref type="bibr" target="#b19">Derevyanko et al., 2018)</ref>, and function prediction <ref type="bibr" target="#b24">(Gligorijevi? et al., 2021)</ref>. Unfortunately, due to the challenge of experimental protein structure determination, the number of reported protein structures is orders of magnitude lower than the size of datasets in other machine learning application domains. For example, 182K structures exist in Protein Data Bank (PDB) <ref type="bibr" target="#b9">(Berman et al., 2000)</ref> (vs 47M protein sequences in Pfam <ref type="bibr" target="#b50">(Mistry et al., 2021)</ref>) and vs 10M annotated images in ImageNet <ref type="bibr" target="#b60">(Russakovsky et al., 2015)</ref>.</p><p>As a remedy to this problem, recent works have leveraged the large volume of unlabeled protein sequences data to learn an effective representation of known proteins <ref type="bibr" target="#b7">(Bepler &amp; Berger, 2019;</ref><ref type="bibr" target="#b16">2021;</ref><ref type="bibr">Rives et al., 2021;</ref><ref type="bibr" target="#b21">Elnaggar et al., 2021)</ref>. A number of studies have employed pretraining protein encoders on millions of sequences via self-supervised learning <ref type="bibr" target="#b25">(Hadsell et al., 2006;</ref><ref type="bibr" target="#b20">Devlin et al., 2018;</ref><ref type="bibr" target="#b14">Chen et al., 2020)</ref>. However, these methods neither directly capture nor leverage the available protein structural information that is known to be the determinants of protein function.</p><p>To better utilize this critical structural information, several structure-based protein encoders <ref type="bibr" target="#b33">(Hermosilla et al., 2021;</ref><ref type="bibr" target="#b32">Hermosilla &amp; Ropinski, 2022)</ref> are proposed. However, due to the scarcity of protein structures, these encoders are often specifically designed for individual tasks, and it is unclear how well they generalize to other tasks. Little attempts have been made until recently to develop universal protein encoders that exploit 3D structures <ref type="bibr" target="#b32">(Hermosilla &amp; Ropinski, 2022)</ref> due to the above-mentioned reasons of scarcity and sparsity of protein structures. Thanks to the recent advances in highly accurate deep learning-based protein structure prediction methods <ref type="bibr" target="#b2">(Baek et al., 2021;</ref><ref type="bibr" target="#b40">Jumper et al., 2021)</ref>, it is now possible to efficiently predict the structure of a large number of protein sequences with reasonable confidence.</p><p>Motivated by this development, we develop a universal protein encoder pretrained on the largest possible number of protein structures (both experimental and predicted) that is able to generalize to a variety of property prediction tasks. We propose a simple yet effective structure-based encoder called GeomEtry-Aware Relational Graph Neural Network (GearNet), which encodes spatial information by adding different types of sequential or structural edges and then performs relational message passing on protein residue graphs. Inspired by the recent geometry-based encoders for small molecules <ref type="bibr" target="#b43">(Klicpera et al., 2020)</ref>, we propose an edge message passing mechanism to enhance the protein structure encoder.</p><p>We further introduce different geometric pretraining methods for learning the protein structure encoder by following popular self-supervised learning frameworks such as contrastive learning and self-prediction. In the contrastive learning scenario, we aim to maximize the similarity between the learned representations of different augmented views from the same protein, while minimizing the similarity between those from different proteins. For self-prediction, the model performs masked prediction of different geometric or biochemical properties, such as residue types, Euclidean distances, angles and dihedral angles, during training.</p><p>Extensive experiments on several existing benchmarks, including Enzyme Commission number prediction <ref type="bibr" target="#b24">(Gligorijevi? et al., 2021)</ref>, Gene Ontology term prediction <ref type="bibr" target="#b24">(Gligorijevi? et al., 2021)</ref>, fold classification <ref type="bibr" target="#b34">(Hou et al., 2018)</ref> and reaction classification <ref type="bibr" target="#b33">(Hermosilla et al., 2021)</ref> verify our GearNet augmented with edge message passing can consistently and significantly outperform existing protein encoders on most tasks in a supervised setting. Further, by employing the proposed pretraining methods, our model trained on less than a million samples achieves state-of-the-art results on different tasks, even better than the sequence-based encoders pretrained on million-or billion-scale datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Previous works seek to learn protein representations based on different modalities of proteins, including amino acid sequences <ref type="bibr" target="#b55">(Rao et al., 2019;</ref><ref type="bibr" target="#b21">Elnaggar et al., 2021;</ref><ref type="bibr">Rives et al., 2021)</ref>, multiple sequence alignments (MSAs) <ref type="bibr" target="#b56">(Rao et al., 2021;</ref><ref type="bibr" target="#b10">Biswas et al., 2021;</ref><ref type="bibr" target="#b48">Meier et al., 2021)</ref> and protein structures <ref type="bibr" target="#b33">(Hermosilla et al., 2021;</ref><ref type="bibr" target="#b24">Gligorijevi? et al., 2021;</ref><ref type="bibr" target="#b66">Somnath et al., 2021)</ref>. These works share the common goal of learning informative protein representations that can benefit various downstream applications, like predicting protein function <ref type="bibr" target="#b24">(Gligorijevi? et al., 2021;</ref><ref type="bibr">Rives et al., 2021)</ref> and protein-protein interaction <ref type="bibr" target="#b76">(Wang et al., 2019)</ref>, as well as designing protein sequences <ref type="bibr" target="#b10">(Biswas et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Sequence-Based Methods</head><p>Sequence-based protein representation learning is mainly inspired by the methods of modeling natural language sequences. Recent methods aim to capture the biochemical and co-evolutionary knowledge underlying a large-scale protein sequence corpus by self-supervised pretraining, and such knowledge is then transferred to specific downstream tasks by finetuning. Typical pretraining objectives explored in existing methods include next amino acid prediction <ref type="bibr" target="#b1">(Alley et al., 2019;</ref><ref type="bibr" target="#b21">Elnaggar et al., 2021)</ref>, masked language modeling (MLM) <ref type="bibr" target="#b55">(Rao et al., 2019;</ref><ref type="bibr" target="#b21">Elnaggar et al., 2021;</ref><ref type="bibr">Rives et al., 2021)</ref>, pairwise MLM <ref type="bibr" target="#b30">(He et al., 2021)</ref> and contrastive predictive coding (CPC) <ref type="bibr" target="#b46">(Lu et al., 2020)</ref>. Compared to sequence-based approaches that learn in the whole sequence space, MSA-based methods <ref type="bibr" target="#b56">(Rao et al., 2021;</ref><ref type="bibr" target="#b10">Biswas et al., 2021;</ref><ref type="bibr" target="#b48">Meier et al., 2021)</ref> leverage the sequences within a protein family to capture the conserved and variable regions of homologous sequences, which imply specific structures and functions of the protein family.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Structure-Based Methods</head><p>Although sequence-based methods pretrained on large-scale databases perform well, structure-based methods should be, in principle, a better solution to learning an informative protein representation, as the function of a protein is determined by its structure. This line of works seeks to encode spatial information in protein structures by 3D CNNs <ref type="bibr" target="#b19">(Derevyanko et al., 2018)</ref> or graph neural networks (GNNs) <ref type="bibr" target="#b24">(Gligorijevi? et al., 2021;</ref><ref type="bibr" target="#b3">Baldassarre et al., 2021;</ref><ref type="bibr" target="#b38">Jing et al., 2021)</ref>. Among these methods, IEConv <ref type="bibr" target="#b33">(Hermosilla et al., 2021)</ref> tries to fit the inductive bias of protein structure modeling, which introduced a graph convolution layer incorporating intrinsic and extrinsic distances between nodes. Another potential direction is to extract features from protein surfaces <ref type="bibr" target="#b22">(Gainza et al., 2020;</ref><ref type="bibr" target="#b72">Sverrisson et al., 2021;</ref><ref type="bibr" target="#b17">Dai &amp; Bailey-Kellogg, 2021)</ref>. <ref type="bibr" target="#b66">Somnath et al. (2021)</ref> combined the advantages of both worlds and proposed a parameterefficient multi-scale model. Besides, there are also works that enhance pretrained sequence-based models by incorporating structural information in the pretraining stage <ref type="bibr" target="#b8">(Bepler &amp; Berger, 2021)</ref> or finetuning stage <ref type="bibr" target="#b77">(Wang et al., 2021)</ref>.</p><p>Despite progress in the design of structure-based encoders, there are few works focusing on structure-based pretraining for proteins. To the best of our knowledge, the only attempt is a concurrent work <ref type="bibr" target="#b32">(Hermosilla &amp; Ropinski, 2022)</ref>, which applies a contrastive learning method on an encoder <ref type="bibr" target="#b33">(Hermosilla et al., 2021)</ref> relying on cumbersome convolutional and pooling layers. Compared with these existing works, our proposed encoder is conceptually simpler and more effective on many different tasks, thanks to the proposed relational graph convolutional layer and edge message passing layer, which are able to efficiently capture the sequential and structural information. Furthermore, we introduce five structure-based pretraining methods within the contrastive learning and self-prediction frameworks, which can serve as a solid starting point for enabling self-supervised learning on protein structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Structure-Based Protein Encoder</head><p>Existing protein encoders are either designed for specific tasks or cumbersome for pretraining due to the dependency on computationally expensive convolutions. In contrast, here we propose a simple yet effective protein structure encoder, named GeomEtry-Aware Relational Graph Neural Network (GearNet). We utilize edge message passing to enhance the effectiveness of GearNet, which is novel in the field of protein structure modeling. In contrast, previous works <ref type="bibr" target="#b33">(Hermosilla et al., 2021;</ref><ref type="bibr" target="#b66">Somnath et al., 2021)</ref> only consider message passing among residues or atoms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Geometry-Aware Relational Graph Neural Network</head><p>Given protein structures, our model aims to learn representations encoding their spatial and chemical information. These representations should be invariant under translations and rotations in 3D space. To achieve this requirement, we first construct our protein graph based on the spatial features invariant under these transformations. Protein graph construction. We represent the structure of a protein as a relational graph G = (V, E, R), where V and E denotes the set of nodes and edges respectively, and R is the set of edge types. We use (i, j, r) to denote the edge from node i to node j with type r. We use n and m to denote the number of nodes and edges, respectively. In this work, each node in the protein graph represents the alpha carbon of a residue with the 3D coordinates of all nodes x ? R n?3 .</p><p>Then, we add three different types of edges into our graphs: sequential edges, radius edges and K-nearest neighbor edges. Among these, sequential edges will be further divided into different edge types according to the relative position d between two end nodes. These edge types reflect different geometric properties, which all together yield a comprehensive featurization of proteins. The graph construction process is shown in Fig. <ref type="figure" target="#fig_0">1</ref> and more details can be found in Appendix C.1.</p><p>Node and edge features. Most previous structure-based encoders designed for biological molecules used many chemical and spatial features, some of which are difficult to obtain or time-consuming to calculate. In contrast, we only use the one-hot encoding of residue types with one additional dimension for unknown types as node features, denoted as f ? {0, 1} n?21 , which is enough to learn good representation as shown in our experiments.</p><p>The feature f (i,j,r) for an edge (i, j, r) is the concatenation of the node features of two end nodes, the one-hot encoding of the edge type, and the sequential and spatial distances between them:</p><formula xml:id="formula_0">f (i,j,r) = Cat (f i , f j , onehot(r), |i -j|, x i -x j 2 ) ,<label>(1)</label></formula><p>where Cat(?) denotes the concatenation operation.</p><p>Relational graph convolutional layer. Upon the protein graphs defined above, we utilize a GNN to derive per-residue and whole-protein representations. One simple example of GNNs is the GCN <ref type="bibr" target="#b42">(Kipf &amp; Welling, 2017)</ref>, where messages are computed by multiplying node features with a convolutional kernel matrix shared among all edges. To increase the capacity of GCN in protein structure modeling, IEConv <ref type="bibr" target="#b33">(Hermosilla et al., 2021)</ref> proposed to apply a learnable kernel function on edge features. In this way, m different kernel matrices can be applied on different edges, which achieves good performance but induces high memory costs. To balance model capacity and memory cost, we use a relational graph convolutional neural network <ref type="bibr" target="#b62">(Schlichtkrull et al., 2018)</ref> to learn graph representations, where a convolutional kernel matrix is shared within each edge type and there are |R| different kernel matrices in total.</p><p>Formally, the relational graph convolutional layer used in our model is defined as</p><formula xml:id="formula_1">h (0) i = BN(FC(f i )),<label>(2)</label></formula><formula xml:id="formula_2">u (l) i = ? ? ? BN ? ? r?R W r j?Nr(i) h (l-1) j ? ? ? ? , (3) h (l) i = h (l-1) i + Dropout u (l) i .<label>(4)</label></formula><p>Specifically, we first apply a linear transformation FC(?) and a batch norm layer BN(?) on node features to get the initial representation for each node. Then, given the input node representation h (l)</p><p>i for node i at the l-th layer, we compute updated node representation u (l) i by aggregating features from neighboring nodes N r (i), where N r (i) = {j ? V|(j, i, r) ? E} denotes the neighborhood of node i with respect to the edge type r and W r denotes the learnable convolutional kernel matrix for edge type r. Here we use a ReLU function as the activation ?(?). Finally, we apply the dropout on the update u (l) i and add a residual connection from the previous layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Edge Message Passing Layer</head><p>As in the literature of molecular representation learning, many geometric encoders show benefits from explicitly modeling interactions between edges. For example, DimeNet <ref type="bibr" target="#b43">(Klicpera et al., 2020)</ref> uses a 2D spherical Fourier-Bessel basis function to represent angles between two edges and pass messages between edges. Inspired by this mechanism, we propose a variant of our structure-based encoder enhanced with an edge message passing layer.</p><p>We first construct a relational graph G = (V , E , R ) among edges, which is also known as line graph in the literature <ref type="bibr" target="#b27">(Harary &amp; Norman, 1960)</ref>. Each node in the graph G corresponds to an edge in the original graph. G links edge (i, j, r 1 ) in the original graph to edge (w, k, r 2 ) if and only if j = w and i = k. The type of this edge is determined by the angles between (i, j, r 1 ) and (w, k, r 2 ). We discretize the range [0, ?] into 8 bins and use the index of the bin as the edge type.</p><p>Then, we apply a similar relational graph convolutional network on the graph G to obtain the message function for each edge. Formally, the edge message passing layer is defined as</p><formula xml:id="formula_3">m (0) (i,j,r1) = f (i,j,r1) , m (l) (i,j,r1) = ? ? ? ?BN ? ? ? r?R W r (w,k,r 2 )? N r ((i,j,r 1 )) m (l-1) (w,k,r2) ? ? ? ? ? ? .</formula><p>(5)</p><p>Here we use m (l) (i,j,r1) to denote the message function for edge (i, j, r 1 ) in the l-th layer. Similar as Eq. (3), the message function for edge (i, j, r 1 ) will be updated by aggregating features from its neighbors N r ((i, j, r 1 )), where N r ((i, j, r 1 )) = {(w, k, r 2 ) ? V |((w, k, r 2 ), (i, j, r 1 ), r) ? E } denotes the set of incoming edges of (i, j, r 1 ) with relation type r in graph G .</p><p>Finally, we replace the aggregation function Eq. (3) in the original graph with the following one:</p><formula xml:id="formula_4">u (l) i = ? BN r?R W r j?Nr(i) (h (l-1) j + FC(m (l) (j,i,r) ) . (<label>6</label></formula><formula xml:id="formula_5">)</formula><p>This variant of GearNet with edge message passing mechanism is referred as GearNet-Edge in the sequel.</p><p>Notably, though most components in our model are adapted from encoders designed for small molecules, the idea of using a relational graph to model spatial information within protein structures is novel. In addition, to the best of our knowledge, this is the first work that explores edge message passing for protein representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Geometric Pretraining Methods</head><p>As a large amount of unlabeled protein structures exist, which are potentially helpful to protein representation learning, we study how to leverage these unlabeled structures.</p><p>We follow two popular self-supervised learning frameworks: contrastive learning and self-prediction, and propose five different pretraining strategies for structure-based encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Multiview Contrastive Learning</head><p>Inspired by recent contrastive learning methods <ref type="bibr" target="#b52">(Oord et al., 2018;</ref><ref type="bibr" target="#b14">Chen et al., 2020;</ref><ref type="bibr" target="#b29">He et al., 2020)</ref>, our framework learns representations by maximizing the similarity between representations of different augmented views of the same protein while minimizing the agreement between views of different proteins. The high-level idea is illustrated in Fig. <ref type="figure">2</ref>.</p><p>More specifically, given a protein graph G, we first sample two different views G x and G y via a stochastic augmentation module. We then compute the graph representations h x and h y of the two views using our structure-based encoder. Following SimCLR <ref type="bibr" target="#b14">(Chen et al., 2020)</ref>, a two-layer projection head is further applied to map the representations to lower-dimensional space, denoted as z x and z y . Finally, a contrastive loss function is defined by distinguishing views from the same or different proteins using their similarities. For a positive pair x and y, we treat views from other proteins in the same mini-batch as negative pairs. Mathematically, the loss function for a positive pair of views x and y can be written as:</p><formula xml:id="formula_6">L x,y = -log exp(sim(z x , z y )/? ) 2B k=1 1 [k =x] exp(sim(z y , z k )/? ) ,<label>(7)</label></formula><p>where B denotes the batch size,</p><formula xml:id="formula_7">1 [k =x] ? {0, 1} is an indi- cator function that is equal to 1 if and only if k = x.</formula><p>And the similarity function sim(u, v) is defined by the cosine similarity between u and v.</p><p>Data augmentation. As suggested in <ref type="bibr" target="#b14">Chen et al. (2020)</ref>, diverse data augmentation schemes play an important role in contrastive learning. In this work, we propose a principled way to generate diverse views of a protein.</p><p>We first randomly choose a cropping function to make the protein graph smaller so that large-size batches can be used for pretraining. Here we consider two different cropping functions:</p><p>Figure <ref type="figure">2</ref>: Demonstration of geometric pretraining methods. For multiview contrastive learning, we aim to align representations of different views from the same protein while minimizing the similarity between those from different proteins. For self-prediction methods, we construct four masked prediction objectives by inferring masked geometric or biochemical quantities with learned representations.</p><p>? Subsequence: randomly sample a consecutive segment of protein sequences and take the corresponding subgraph from the protein residue graph. ? Subspace: randomly sample a residue as the center and select all residues within a Euclidean ball with a predefined radius.</p><p>Then, we randomly choose one of the following two transformations with equal probability to apply on the cropped protein graphs.</p><p>? Identity: no transformation.</p><p>? Random edge masking: randomly mask edges with a fixed mask rate equal to 0.15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Self-Prediction Methods</head><p>Another line of research is based on the recent progress of self-prediction methods in natural language processing <ref type="bibr" target="#b20">(Devlin et al., 2018;</ref><ref type="bibr" target="#b11">Brown et al., 2020)</ref>. Given a protein, our objective is to predict one part of the protein given the remainder of the structure. Here, we propose four self-supervised tasks based on geometric or biochemical properties.</p><p>Residue Type Prediction. Our first method is based on the masked language modeling objective, which has been widely used in pretraining large-scale protein language models <ref type="bibr" target="#b8">(Bepler &amp; Berger, 2021)</ref>. For each protein, we randomly mask the types (i.e., node features) of some residues and then let the structure-based encoders to predict these masked residue types. For a masked node i, the learning objective is defined as:</p><formula xml:id="formula_8">L i = CE(f residue (h i ), f i ),</formula><p>where f residue (?) is an MLP classification head and CE denotes the cross entropy loss. This method is also known as Attribute Masking in the literature of pretraining on small molecules <ref type="bibr" target="#b35">(Hu et al., 2019)</ref>.</p><p>Distance Prediction. In order to learn local spatial structures, we use our learned representations to predict the Euclidean distance between two nodes connected in the protein graph. First, we randomly select a fixed number of edges from the original graph. Then, these edges will be removed when feeding the graph into the encoder. For a selected edge, the representations of its two end nodes will be concatenated to predict the distance between them. More concretely, the loss function for an edge (i, j, r) will be defined as:</p><formula xml:id="formula_9">L (i,j,r) = (f dist (h i , h j ) -x i -x j 2 ) 2 ,</formula><p>where f dist (?) is an MLP prediction head and h i , h j denotes the representations of node i and j after masking selected edges.</p><p>Angle Prediction. Besides distances, angles between edges are also important features that reflect the relative position between residues. Similarly, we can define a masked geometric loss by 1) randomly selecting two adjacent edges, 2) removing these edges from the graph, 3) using the three end nodes of the pair of edges sharing a single end node to predict the angles between them. Here we discretized the angles by cutting the range [0, ?] into 8 bins. The objective for a selected pair of edges (i, j, r 1 ) and (j, k, r 2 ) is to predict which bin the angle between them will belong to:</p><formula xml:id="formula_10">L (i,j,r1),(j,k,r2) = CE(f angle (h i , h j , h k ), bin(?ijk)),</formula><p>where f angle (?) is an MLP classification head and bin(?) is used to discretize the angle.</p><p>Dihedral Prediction. As shown in <ref type="bibr" target="#b44">Klicpera et al. (2021)</ref>, the dihedral angles between three edges can provide important clues about the relative directional information. Therefore, we can also construct a masked geometric objective by predicting the dihedral angles between three consecutive edges (i, j, r 1 ), (j, k, r 2 ) and (k, t, r 3 ):</p><formula xml:id="formula_11">L (i,j,r1),(j,k,r2),(k,t,r3) = CE(f (h i , h j , h k , h t ), bin(?ijkt)),</formula><p>where f (?) is an MLP classification head and bin(?) is used to discretize the dihedral angles.</p><p>The framework of self-prediction methods is depicted in Fig. <ref type="figure">2</ref>. In the sequel, we will treat the tasks above as four different pretraining methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we first introduce our experimental setup for pretraining and then evaluate our models on four standard downstream tasks including Enzyme Commission number prediction, Gene Ontology term prediction, fold classification and reaction classification. More experiments on protein engineering tasks can be found in Appendix G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Pretraining Setup</head><p>We use the AlphaFold protein structure database <ref type="bibr" target="#b40">(Jumper et al., 2021;</ref><ref type="bibr" target="#b73">Varadi et al., 2021)</ref> for pretraining. This database contains the protein structures predicted by the AlphaFold2 model, and we employ both 365K proteomewide predictions and 440K Swiss-Prot <ref type="bibr" target="#b16">(Consortium, 2021)</ref> predictions in our experiments. In Appendix F, we further report the results of pretraining on different datasets.</p><p>For pretraining, the model is trained on the AlphaFold protein database for 50 epochs. All these models are trained on 4 Tesla A100 GPUs. See Appendix E.3 for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Downstream Tasks 1 &amp; 2: EC and GO Prediction</head><p>We first adopt two tasks proposed in <ref type="bibr" target="#b24">Gligorijevi? et al. (2021)</ref> for downstream evaluation. Enzyme Commission (EC) number prediction seeks to predict the EC numbers of different proteins, which describe their catalysis of biochemical reactions. The EC numbers are selected from the third and fourth levels of the EC tree, forming 538 binary classification tasks. Gene Ontology (GO) term prediction aims to predict whether a protein belongs to some GO terms. These terms classify proteins into hierarchically related functional classes organized into three ontologies: molecular function (MF), biological process (BP) and cellular component (CC).</p><p>We follow the split method in <ref type="bibr" target="#b24">Gligorijevi? et al. (2021)</ref> to ensure that the test set only contains PDB chains with sequence identity no more than 95% to the training set.</p><p>Baselines. Following <ref type="bibr" target="#b77">Wang et al. (2021)</ref>, we compare our encoders with many existing protein representation learning methods, including four sequence-based encoders (CNN <ref type="bibr" target="#b65">(Shanehsazzadeh et al., 2020)</ref>, ResNet <ref type="bibr" target="#b55">(Rao et al., 2019)</ref>, LSTM <ref type="bibr" target="#b55">(Rao et al., 2019)</ref> and Transformer <ref type="bibr" target="#b55">(Rao et al., 2019)</ref>), four structure-based encoders (GAT <ref type="bibr" target="#b75">(Veli?kovi? et al., 2018)</ref>, GVP <ref type="bibr" target="#b38">(Jing et al., 2021)</ref>, DeepFRI <ref type="bibr" target="#b24">(Gligorijevi? et al., 2021)</ref> and New IEConv <ref type="bibr" target="#b32">(Hermosilla &amp; Ropinski, 2022)</ref>). We also include three models pretrained on largescale sequence datasets (ProtBERT-BFD <ref type="bibr" target="#b21">(Elnaggar et al., 2021)</ref>, ESM-1b <ref type="bibr">(Rives et al., 2021)</ref> and LM-GVP <ref type="bibr" target="#b77">(Wang et al., 2021)</ref>).</p><p>Training and evaluation. For comparison, we train Gear-Net and GearNet-Edge from scratch and also include the results of GearNet-Edge pretrained with five proposed methods. The models are trained for 200 epochs. We evaluate the performance with two metrics commonly used in the CAFA challenges <ref type="bibr" target="#b54">(Radivojac et al., 2013)</ref>: (1) proteincentric maximum F-score F max ;</p><p>(2) pair-centric area under precision-recall curve AUPR pair (See Appendix E.2 for details). Models are selected based on the F max on validation sets.</p><p>Results. Table <ref type="table" target="#tab_0">1</ref> summarizes the results on EC and GO prediction. For encoders without pretraining, we find that the vanilla GearNet can already obtain competitive results with other baselines. After adding the edge message passing mechanism, our method GearNet-Edge significantly outperforms other baselines on EC, GO-BP and GO-MF and achieves competitive results on GO-CC. With pretraining , all five proposed variants show large improvements over models trained from scratch. After pretraining with the best among these methods, i.e., Multiview Contrast, our model can achieve state-of-the-art results on EC and GO-BP and competitive results on GO-MF against pretrained sequencebased encoders. It should be noted that our models are pretrained on a dataset with fewer than one million structures, whereas all three language model based baselines are pretrained on million-or billion-scale sequence databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Downstream Tasks 3 &amp; 4: Fold and Reaction Classification</head><p>We further include two tasks used in <ref type="bibr" target="#b33">Hermosilla et al. (2021)</ref>. Fold classification is first proposed in <ref type="bibr" target="#b34">Hou et al. (2018)</ref>, Training and evaluation. We find that the IEConv layer is important for predicting fold labels (but not for GO and EC prediction tasks as there was no performance improvement observed), so we enhance our model by incorporating this as an additional layer for fold prediction task. The models will be referred as GearNet-IEConv and GearNet-Edge-IEConv, respectively. All models are trained for 300 epochs on the downstream datasets. Details of these models can be found in Appendix C.2. We pretrain GearNet-Edge and GearNet-Edge-IEConv with our proposed five methods.</p><p>The performance is measured with the mean accuracy on test sets and validation sets are used for model selection.</p><p>Baselines. To verify the effectiveness of our approaches, we compare them with the baselines provided in <ref type="bibr" target="#b33">Hermosilla et al. (2021)</ref>. Moreover, we add more baselines including four sequence-based encoders (CNN <ref type="bibr" target="#b65">(Shanehsazzadeh et al., 2020)</ref>, ResNet <ref type="bibr" target="#b55">(Rao et al., 2019)</ref>, LSTM <ref type="bibr" target="#b55">(Rao et al., 2019)</ref> and Transformer <ref type="bibr" target="#b55">(Rao et al., 2019)</ref>) and the SOTA method New IEConv <ref type="bibr" target="#b32">(Hermosilla &amp; Ropinski, 2022)</ref>.</p><p>Results. Table <ref type="table" target="#tab_1">2</ref> summarizes the results on fold and reaction classification. As observed in the table, our model GearNet-Edge-IEConv can achieve better results than the SOTA methods on fold classification tasks. Both the edge message passing and IEConv layers contribute a lot to the final performance. With pretraining, the model can significantly benefit from all the five proposed methods. After pretraining with the Multiview Contrast method, the model can achieve SOTA results on both tasks, which proves the effectiveness of our pretraining strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Studies</head><p>To analyze the contribution of different components in our proposed methods, we perform ablation studies on the EC prediction task. The results are shown in Table <ref type="table" target="#tab_2">3</ref>.</p><p>Relational graph convolutional layers. To show the effects of relational convolutional layers, we replace it with graph convolutional layers that share a kernel matrix among all edges. As reported in the table, results can be significantly improved by using relational convolution, which suggests the importance of treating edges as different types.</p><p>Edge message passing layers. We also compare the results of GearNet with and without edge message passing layers, the results of which are shown in Tables <ref type="table" target="#tab_0">1,</ref><ref type="table" target="#tab_1">2</ref>, 3. It can be observed that the performance consistently increases after performing edge message passing. This demonstrates the effectiveness of our proposed mechanism.</p><p>Different augmentations in Multiview Contrast. We investigate the contribution of each augmentation operation proposed in the Multiview Contrast method. Instead of randomly sampling cropping functions and transformations, we pretrain our model with four deterministic combinations of augmentations, respectively. As shown in Table <ref type="table" target="#tab_2">3</ref>, all the four combinations can yield good results, which suggests that arbitrary combinations of the proposed cropping and transformation schemes can yield informative partial views of proteins.</p><p>Sampling schemes in Self-Prediction methods. Different sampling schemes may lead to different results for self-prediction methods. We study the effects of sampling schemes using Dihedral Prediction as an example. Instead of sampling dihedral angles formed by three consecutive edges, we try to predict the dihedrals formed by four randomly sampled nodes. We observe that this change of sampling schemes will make the self-prediction task more difficult to solve, which even brings negative effects after pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we propose a simple yet effective structurebased encoder for protein representation learning, which performs relational message passing on protein residue graphs.</p><p>A novel edge message passing mechanism is introduced to explicitly model interactions between edges, which show consistent improvements. Moreover, five self-supervised pretraining methods are proposed following two standard frameworks: contrastive learning and self-prediction methods. Comprehensive experiments over multiple benchmark tasks verify that our model outperforms previous state-ofthe-art baselines under both from scratch and pretrained settings.</p><p>We believe that our work is an important step to adopt selfsupervised learning methods on protein structure understanding. As the AlphaFold Protein Structure Database is planned to cover over 100 million proteins in UniRef90, it would be possible to train huge and more advanced structurebased models on larger datasets in the future. Another promising direction is to apply our proposed methods on more tasks, e.g., protein-protein interaction modeling and protein-guided ligand molecule design, which underpins many important biological processes and applications. More broader impacts will be discussed in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. More Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Structure-based Encoders for Biological Molecules</head><p>Following the early efforts <ref type="bibr" target="#b6">(Behler &amp; Parrinello, 2007;</ref><ref type="bibr" target="#b4">Bart?k et al., 2010;</ref><ref type="bibr">2013;</ref><ref type="bibr" target="#b15">Chmiela et al., 2017)</ref> of building machine learning systems for molecules by hand-crafted atomic features, recent works exploited end-to-end message passing neural networks (MPNNs) <ref type="bibr" target="#b23">(Gilmer et al., 2017)</ref> to encode the structures of small molecules and macromolecules like proteins. Specifically, existing methods employed node/atom message passing <ref type="bibr" target="#b23">(Gilmer et al., 2017;</ref><ref type="bibr">Sch?tt et al., 2017a;</ref><ref type="bibr">b)</ref>, edge/bond message passing <ref type="bibr" target="#b39">(J?rgensen et al., 2018;</ref><ref type="bibr" target="#b13">Chen et al., 2019</ref>) and directional information <ref type="bibr" target="#b43">(Klicpera et al., 2020;</ref><ref type="bibr" target="#b45">Liu et al., 2021;</ref><ref type="bibr" target="#b44">Klicpera et al., 2021)</ref> to encode 2D or 3D molecular graphs.</p><p>Compared to small molecules, structural representations of proteins are more diverse, including residue-level graphs, atom-level graphs and protein surfaces. There are some recent models designed to encode residue-level graphs <ref type="bibr" target="#b33">(Hermosilla et al., 2021;</ref><ref type="bibr" target="#b32">Hermosilla &amp; Ropinski, 2022)</ref> and protein surfaces <ref type="bibr" target="#b22">(Gainza et al., 2020;</ref><ref type="bibr" target="#b72">Sverrisson et al., 2021)</ref>, and they achieved impressive results on various tasks. However, these models are either not expressive enough to capture edge interactions or too complicated for representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Pretraining Graph Neural Networks</head><p>Our work is also related to the recent efforts of pretraining graph neural networks (GNNs), which sought to learn graph representations in a self-supervised fashion. In this domain, various self-supervised pretext tasks, like edge prediction <ref type="bibr" target="#b41">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b26">Hamilton et al., 2017)</ref>, context prediction <ref type="bibr" target="#b35">(Hu et al., 2019;</ref><ref type="bibr" target="#b59">Rong et al., 2020)</ref>, node/edge attribute reconstruction <ref type="bibr" target="#b35">(Hu et al., 2019)</ref> and contrastive learning <ref type="bibr" target="#b28">(Hassani &amp; Khasahmadi, 2020;</ref><ref type="bibr" target="#b53">Qiu et al., 2020;</ref><ref type="bibr" target="#b80">You et al., 2020;</ref><ref type="bibr" target="#b79">Xu et al., 2021)</ref>, are designed to acquire knowledge from unlabeled graphs. In this work, we focus on learning representations of residue-level graphs of proteins in a self-supervised way. To attain this goal, we design several novel protein-specific pretraining methods to learn the proposed structure-based GNN encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Broader Impact</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Details of GearNet</head><p>In this section, we describe more details about the implementation of our GearNet. The whole pipeline of our structurebased encoder is depicted in Fig. <ref type="figure" target="#fig_1">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Protein Graph Construction</head><p>For graph construction, we use three different ways to add edges:</p><p>1. Sequential edges. The i-th residue and the j-th residue will be linked by an edge if the sequential distance between them is below a predefined threshold d seq , i.e., |j -i| &lt; d seq . The type of each sequential edge is determined by their relative position d = j -i in the sequence. Hence, there are 2d seq -1 types of sequential edges. 2. Radius edges. Following previous works, we also add edges between two nodes i and j when the Euclidean distance between them is smaller than a threshold d radius . 3. K-nearest neighbor edges. Since the scales of spatial coordinates may vary among different proteins, a node will be also connected to its k-nearest neighbors based on the Euclidean distance. In this way, the density of spatial edges are guaranteed to be comparable among different protein graphs.</p><p>Since we are not interested in spatial edges between residues close together in the sequence, we further add a filter to the latter two kinds of edges. Specifically, for an edge connecting the i-th residue and j-th residue, it will be removed if the sequential distance between them is lower than a long range interaction cutoff d long , i.e., |i -j| &lt; d long .</p><p>In this paper, we set the sequential distance threshold d seq = 3, the radius d radius = 10.0 ?, the number of neighbors k = 10 and the long range interaction cutoff d long = 5. By regarding radius edges and KNN edges as two separate edge types, there will be totally 2d seq + 1 = 7 different types of edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Enhance GearNet with IEConv Layers</head><p>In our experiments, we find that IEConv layers are very useful for predicting fold labels in spite of their relatively poor performance on function prediction tasks. Therefore, we enhance our models by adding a simplified IEConv layer as an additional layer, which achieve better results than the original IEConv. In this section, we describe how to simplify the IEConv layer and how to combine it with our model.</p><p>Simplify the IEConv layer. The original IEConv layer relies on the computation of intrinsic and extrinsic distances between two nodes, which are computationally expensive. Hence, we follow the modifications proposed in <ref type="bibr" target="#b32">Hermosilla &amp; Ropinski (2022)</ref>, which show improvements as reported in their experiments. Although these modifications are not proposed by us, we still briefly describe the model for completeness.</p><p>In the IEConv layer, we keep the edges in our graph G and use h(l) i to denote the hidden representation for node i in the l-th layer. The update equation for node i is defined as:</p><formula xml:id="formula_12">h(l) i = j?N (i) k o (f (G, i, j)) ? h (l-1) j ,<label>(8)</label></formula><p>where N (i) is the set of neighbors of i, f (G, i, j) is the edge feature between i and j and k o (?) is an MLP mapping the feature to a kernel matrix. Instead of intrinsic and extrinsic distances in the original IEConv layer, we follow New IEConv, which adopts three relative positional features proposed in <ref type="bibr" target="#b36">Ingraham et al. (2019)</ref> and further augments them with additional input functions.</p><p>We aim to apply this layer on our constructed protein residue graph instead of the radius graph in the original paper. Therefore, we simply remove the dynamically changed receptive fields, pooling layer and smoothing tricks in our setting.</p><p>Combine IEConv with GearNet. Our model is very flexible to incorporate other message passing layers. To incorporate IEConv layers, we just use our graph and hidden representations as input and replace the update equation Eq. 4 with</p><formula xml:id="formula_13">h (l) i = h (l-1) i + Dropout u (l) i + h(l) i .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Augmentations in Multiview Contrast</head><p>For multiview contrastive learning methods, we propose two cropping functions and two transformation functions to generate different views of proteins. Here we illustrate these augmentation functions in Fig. <ref type="figure" target="#fig_2">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Experimental Details E.1. Dataset Statistics</head><p>Dataset statistics of our four downstream tasks are summarized in Table <ref type="table" target="#tab_3">4</ref>. More details are introduced as follows.</p><p>Enzyme Commission and Gene Ontology. Following DeepFRI <ref type="bibr" target="#b24">(Gligorijevi? et al., 2021)</ref>, the EC numbers are selected from the third and fourth levels of the EC tree, forming 538 binary classification tasks, while the GO terms with at least 50 and no more than 5000 training samples are selected. The non-redundant sets are partitioned into For subsequence, we randomly sample a consecutive segment of the protein (2-7 in this case) and take the corresponding subgraph. For subspace, we first sample a center residue (4 in this case) and then sample all residues within a distance threshold r. Then, a random transformation function will be applied on the output subgraph in the last step.</p><p>For identity, we directly return the graph without transformation while for random edge masking, we randomly remove a fixed ratio of edges from the graph. training, validation and test set according to the sequence identity. We retrieve all protein chains from PDB used the code provided in their codebase and remove those with obsolete pdb ids, so the statistics will be slightly different from the number reported in the original paper.</p><p>Fold Classification. We directly use the dataset processed in <ref type="bibr" target="#b33">(Hermosilla et al., 2021)</ref>, which consolidated 16,712 proteins with 1,195 different folds from the SCOPe 1.75 database <ref type="bibr" target="#b51">(Murzin et al., 1995)</ref>.</p><p>Reaction Classification. The dataset comprises 37,428 proteins categorized into 384 reaction classes. The split methods are described in <ref type="bibr" target="#b33">Hermosilla et al. (2021)</ref>, where they cluster protein chains via sequence similarities and ensure that protein chains belonging to the same cluster are in the same set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Evaluation Metrics</head><p>Now we introduce the details of evaluation metrics for EC and GO prediction. These two tasks aim to answer the question: whether a protein has a particular function, which can be seen as multiple binary classification tasks.</p><p>The first metric, protein-centric maximum F-score F max , is defined by first calculating the precision and recall for each protein and then take the average score over all proteins. More specifically, for a given target protein i and some decision threshold t ? [0, 1], the precision and recall are computed as:</p><formula xml:id="formula_14">precision i (t) = f 1[f ? P i (t) ? T i ] f 1[f ? P i (t)] ,<label>(10)</label></formula><p>and</p><formula xml:id="formula_15">recall i (t) = f 1[f ? P i (t) ? T i ] f 1[f ? T i ] , (<label>11</label></formula><formula xml:id="formula_16">)</formula><p>where f is a function term in the ontology, T i is a set of experimentally determined function terms for protein i, P i (t) denotes the set of predicted terms for protein i with scores greater than or equal to t and 1[?] ? {0, 1} is an indicator function that is equal to 1 iff the condition is true. New IEConv <ref type="bibr" target="#b32">(Hermosilla &amp; Ropinski, 2022)</ref>. Since the code for New IEConv has not been made public when the paper is written, we reproduce the method according to the description in the paper and achieve similar results on Fold and Reaction classification tasks. Then, we evaluate the method on EC and GO prediction tasks with the default hyperparameters reported in the original paper and follow the standard training procedure on these two tasks.</p><p>Our methods. For pretraining, we use Adam optimizer with learning rate 0.001 and train a model for 50 epochs.</p><p>Then, the pretrained model will be finetuned on downstream datasets.</p><p>For Multiview Contrast, we set the cropping length of subsequence operation as 50, the radius of subspace operation as 15 ?, the mask rate of random edge masking operation as 0.15. The temperature ? in the InfoNCE loss function is set as 0.07. When pretraining GearNet-Edge and GearNet-Edge-IEConv, we use 96 and 24 as batch sizes, respectively.</p><p>For Distance Prediction, we set the number of sampled residue pairs as 256. And the batch size will be set as 128 and 32 for GearNet-Edge and GearNet-Edge-IEConv, respectively. For Residue Type, Angle and Dihedral Prediction, we set the number of sampled residues, residue triplets and residue quadrants as 512. And the batch size will be set as 96 and 32 for GearNet-Edge and GearNet-Edge-IEConv, respectively.</p><p>For downstream evaluation, the hidden representations in each layer of GearNet will be concatenated for the final prediction. Table <ref type="table" target="#tab_4">5</ref> lists the hyperparameter configurations for different downstream tasks. For the four tasks, we use the same optimizer and number of epochs as in the original papers to make fair comparison. And for EC and GO prediction, we use ReduceLROnPlateau scheduler with factor 0.6 and patience 5, while we use StepLR scheduler with step size 50 and gamma 0.5 for fold and reaction classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Pretraining on Different Datasets</head><p>We use the AlphaFold protein structure database as our pretraining database, as it contains the largest number of protein structures and is planned to cover over 100 million proteins in the future. However, the structures in this database are not experimentally determined but predicted by AlphaFold2. Therefore, it is interesting to see the performance of our methods when pretraining on different datasets.</p><p>To study the effects of the choice of pretraining dataset, we build another dataset using structures extracted from Protein Data Bank (PDB) <ref type="bibr" target="#b9">(Berman et al., 2000)</ref>. Specifically, we extract 123,505 experimentally-determined protein structures from PDB whose resolutions are between 0.0 and 2.5 angstroms, and we further extract 305,265 chains from these proteins to construct the final pretraining dataset.</p><p>Next, we pretrain our five methods on AlphaFold Database v1 (proteome-wide structure predictions), AlphaFold Database v2 (Swiss-Prot structure predictions) and Protein Data Bank and then evaluate the pretrained models on the EC prediction task. The results are reported in Table <ref type="table" target="#tab_5">6</ref>. As can be seen in the table, our methods can achieve comparable performance on different pretraining datasets. Consequently, our methods are robust to the choice of pretraining datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Results on Protein Engineering Tasks</head><p>Besides four standard tasks considered in Section 5, another important kind of downstream tasks is related to protein engineering, which is heavily relied on mutations on protein sequences. These tasks aim to predict the ability of a protein to perform a desired function, termed protein fitness. Good models are expected to have sufficient precision to distinguish between closely-related protein sequences upon mutations. In this section, we further evaluate our model on four protein engineering related tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1. Setup</head><p>We choose two protein engineering tasks from <ref type="bibr" target="#b55">Rao et al. (2019)</ref> and two landscape prediction tasks from <ref type="bibr" target="#b21">Dallago et al. (2021)</ref>, all of which are standard benchmarks to evaluate protein language models. The statistics of four datasets are shown in Table <ref type="table" target="#tab_6">7</ref> and we describe each task as follows.</p><p>Fluoresence landscape prediction <ref type="bibr" target="#b61">(Sarkisyan et al., 2016)</ref> This task aims to predict the log-fluorescence intensity of mutants of the parent green fluorescent protein (GFP). The training set consists of single, double and triple mutants, while the test set includes variants with four or more mutations. Stability landscape prediction <ref type="bibr" target="#b58">(Rocklin et al., 2017)</ref> This is a regression task that maps each input protein to a value measuring the most extreme circumstances in which the protein can maintain its fold above a concentration threshold. To test the generalization ability from a broad set of relevant sequences to local neighborhood of a few sequences, the training set includes proteins from four rounds of experimental design, whereas the test set only contains 1-hop neighbors of top candidate proteins.</p><p>GB1 <ref type="bibr" target="#b78">(Wu et al., 2016)</ref> This task uses the GB1 landscape to test the model's ability to predict the effects of interactions between mutations, termed epistasis. We adopt the low-vs-high split proposed in <ref type="bibr" target="#b21">Dallago et al. (2021)</ref>, where sequences with fitness value equal to or below wild type are used to train, while sequences with fitness value above wild type are used to test.</p><p>Thermostability <ref type="bibr" target="#b37">(Jarzab et al., 2020)</ref> We use the screening landscape curated from <ref type="bibr" target="#b21">Dallago et al. (2021)</ref> to measure the model's ability to predict thermostability of proteins. This landscape includes both global and local variation instead of only mutants of a single protein. Similarly, we use the low-vs-high split proposed in <ref type="bibr" target="#b21">Dallago et al. (2021)</ref>.</p><p>All these four tasks are evaluated via Spearman's ? (rank correlation coefficient).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2. Implementation Details</head><p>Protein structure generation. Since all these tasks are originally designed for evaluating sequence-based encoders, the experimentally-determined structures for proteins in these datasets are not available. To solve this issue, we use AlphaFold2 to generate structures for all datasets except GB1. For GB1 dataset, we only generate the structure of the parent protein. Because the differences between mutant structures are almost negligible on the residue level, we directly use the residue graph constructed from the parent protein for all mutants and replace node and edge features with corresponding residue types after mutations. We scale up the number of predictions that can be performed by AlphaFold2 by using the fast homology search of MMSeqs2 <ref type="bibr" target="#b68">(Steinegger &amp; S?ding, 2017)</ref> and running AlphaFold2 in batch mode using ColabFold <ref type="bibr">(Mirdita et al., 2021)</ref>. Five predictions were made for each sequence and prediction with the highest pLDDT score was chosen. The number of recycles were set to 3 and relaxation using amber force fields was not used.</p><p>For those proteins the structures of which AlphaFold2 fails to generate, we only add sequential edges in the protein residue graph based on sequential information.</p><p>Training details. We use the same pretraining and downstream setting as in Section E.3. Hyperparameters for each dataset are described in Table <ref type="table" target="#tab_7">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3. Results</head><p>The results on four protein engineering tasks are reported in Table <ref type="table" target="#tab_8">9</ref>. First, our model achieves the best result on all tasks among models without pretraining. Moreover, it obtains comparable or even better performance than pretrained sequence encoders on Fluoresence and Stability tasks. This can be understood because our model is a generalization of CNN enhanced with structural information encoded in the protein graph and thus at least as good as CNN. Surprisingly, the improvements of edge message passing aren't significant on these four tasks. One potential reason is that the structures of protein mutants are undistinguishable on the residue level and some structures are missing in the dataset.</p><p>As for pretraining, structure-based methods show significant improvements on Stability and GB1 datasets, which achieve the state-of-the-art performance. However, no positive effects of pretraining are shown on the other two tasks.</p><p>Besides the reason mentioned above, it may be also because sequential information plays a more important role in these tasks.</p><p>Overall, our methods achieve the state-of-the-art performance on three of four datasets. Nevertheless, it still needs further exploration and ablation studies on these mutationbased datasets, which should be considered in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Latent Space Visualization</head><p>For qualitatively evaluating the quality of the protein embeddings learned by our pretraining method, we visualize the latent space of the GearNet-Edge model pretrained by Multiview Contrast. Specifically, we utilize the pretrained model to extract the embeddings of all the proteins in Al </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Residue-Level Explanation</head><p>Protein functions are often reflected by specific regions on the 3D protein structures. For example, the binding ability of a protein to a ligand is highly related to the binding interface between them. Hence, to better interpret our prediction, we apply Integrated Gradients (IG) <ref type="bibr" target="#b70">(Sundararajan et al., 2017)</ref>, a model-agnostic attribution method, on our model to obtain residue-level interpretation. Specifically, we first select two molecular functions, ATP binding (GO:0005524) and Heme binding (GO:0020037), from GO terms that are related to ligand binding. For each functional term, we pick one protein and feed it into the best model trained on the GO-MF dataset. Then, we use IG to generate the feature attribution scores for each protein. The method will integrate the gradient along a straight-line path between a baseline input and the original input. Here the original input and baseline input are the node feature f and a zero vector, respectively. The final attribution score for each protein will be obtained by summing over the feature dimension. The normalized score distribution over all residues are visualized in Figure <ref type="figure" target="#fig_6">6</ref>. As can be seen, our model is able to identify the active sites around the ligand, which are likely to be responsible for binding. Note that these attributions are directly generated from our model without any supervision, which suggests the decent interpretability of our model.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Protein residue graph construction. Sequential, radius and knn edges are added into the graph and treated as different types. Some edges are omitted to save space.</figDesc><graphic url="image-1.png" coords="3,55.44,413.87,234.00,94.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: The pipeline for GearNet and GearNet-edge. First, we construct a relational protein residue graph with sequential, radius and knn edges (some edges are omitted in the figure to save space). Then, a relational graph convolutional layer is applied. Similar message passing layers can be applied on the edge graph to improve the model capacity. This figure shows the update iteration for node 4 and edge (4, 7, red), respectively.</figDesc><graphic url="image-3.png" coords="14,104.04,67.06,388.79,218.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of four different augmentation functions. First, we randomly apply one of the two cropping functions shown in the figure.For subsequence, we randomly sample a consecutive segment of the protein (2-7 in this case) and take the corresponding subgraph. For subspace, we first sample a center residue (4 in this case) and then sample all residues within a distance threshold r. Then, a random transformation function will be applied on the output subgraph in the last step. For identity, we directly return the graph without transformation while for random edge masking, we randomly remove a fixed ratio of edges from the graph.</figDesc><graphic url="image-4.png" coords="15,104.04,67.06,388.79,218.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>-phaFold Database v1, and these embeddings are mapped to the two-dimensional space by UMAP<ref type="bibr" target="#b47">(McInnes et al., 2018)</ref> for visualization. Following<ref type="bibr" target="#b0">Akdel et al. (2021)</ref>, we highlight the 20 most common superfamilies within the database by different colors. The visualization results are shown in Fig.5. It can be observed that our pretrained model tends to group the proteins from the same superfamily together and divide the ones from different superfamilies apart. In particular, it succeeds in clearly separating three superfamilies, i.e., Protein kinase superfamily, Cytochrome P450 family and TRAFAC class myosin-kinesin ATPase superfamily. Such a decent capability of discriminating protein superfamilies, to some degree, interprets our model's superior performance on Fold Classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Protein</head><label></label><figDesc>Structure Representation Learning by Geometric Pretraining</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Latent space visualization of GearNet-Edge (Multiview Contrast) on AlphaFold Database v1.</figDesc><graphic url="image-5.png" coords="21,79.74,141.16,437.39,477.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Identification of active sites on proteins responsible for binding based on attribution scores. Two proteins binding to specific targets are selected for illustration (1NYR-A for ATP binding and 1B85-A for Heme binding). For these two complexes, ligands are shown in yellow spheres while the residues of the receptors are colored based on attribution scores. Residues with higher attribution scores are colored in red while those with lower scores are colored in blue.</figDesc><graphic url="image-6.png" coords="22,104.04,275.41,388.80,173.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-2.png" coords="5,91.89,67.06,413.09,196.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results on EC and GO prediction. [*]  means the results are taken from<ref type="bibr" target="#b77">Wang et al. (2021)</ref>.</figDesc><table><row><cell>Category</cell><cell>Method</cell><cell>EC</cell><cell></cell><cell cols="2">GO-BP</cell><cell cols="2">GO-MF</cell><cell cols="2">GO-CC</cell></row><row><cell></cell><cell></cell><cell cols="3">AUPR pair F max AUPR pair</cell><cell>F max</cell><cell>AUPR pair</cell><cell>F max</cell><cell>AUPR pair</cell><cell>F max</cell></row><row><cell></cell><cell cols="3">Without Pretraining</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>CNN (Shanehsazzadeh et al., 2020)</cell><cell>0.540</cell><cell>0.545</cell><cell>0.165</cell><cell>0.244</cell><cell>0.380</cell><cell>0.354</cell><cell>0.261</cell><cell>0.387</cell></row><row><cell>Sequence-based</cell><cell>ResNet (Rao et al., 2019) LSTM (Rao et al., 2019)</cell><cell>0.137 0.032</cell><cell>0.187 0.082</cell><cell>0.166 0.130</cell><cell>0.280 0.248</cell><cell>0.281 0.100</cell><cell>0.267 0.166</cell><cell>0.266 0.150</cell><cell>0.403 0.320</cell></row><row><cell></cell><cell>Transformer (Rao et al., 2019)</cell><cell>0.187</cell><cell>0.219</cell><cell>0.135</cell><cell>0.257</cell><cell>0.172</cell><cell>0.240</cell><cell>0.170</cell><cell>0.380</cell></row><row><cell></cell><cell>GAT (Veli?kovi? et al., 2018)</cell><cell>0.320</cell><cell>0.368</cell><cell>0.171*</cell><cell>0.284*</cell><cell>0.329*</cell><cell>0.317*</cell><cell>0.249*</cell><cell>0.385*</cell></row><row><cell>Structure-based</cell><cell>GVP (Jing et al., 2021) DeepFRI (Gligorijevi? et al., 2021)</cell><cell>0.482 0.547</cell><cell>0.489 0.631</cell><cell>0.224* 0.282</cell><cell>0.326* 0.399</cell><cell>0.458* 0.462</cell><cell>0.426* 0.465</cell><cell>0.278* 0.363</cell><cell>0.420* 0.460</cell></row><row><cell></cell><cell>New IEConv (Hermosilla &amp; Ropinski, 2022)</cell><cell>0.775</cell><cell>0.735</cell><cell>0.273</cell><cell>0.374</cell><cell>0.572</cell><cell>0.544</cell><cell>0.316</cell><cell>0.444</cell></row><row><cell>Ours</cell><cell>GearNet GearNet-Edge</cell><cell>0.751 0.872</cell><cell>0.730 0.810</cell><cell>0.211 0.251</cell><cell>0.356 0.403</cell><cell>0.490 0.570</cell><cell>0.503 0.580</cell><cell>0.276 0.303</cell><cell>0.414 0.450</cell></row><row><cell></cell><cell></cell><cell cols="2">With Pretraining</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>ESM-1b (Rives et al., 2021)</cell><cell>0.889</cell><cell>0.864</cell><cell>0.343</cell><cell>0.470</cell><cell>0.639</cell><cell>0.657</cell><cell>0.384</cell><cell>0.488</cell></row><row><cell>Sequence Pretrained</cell><cell>ProtBERT-BFD (Elnaggar et al., 2021)</cell><cell>0.859</cell><cell>0.838</cell><cell>0.188*</cell><cell>0.279*</cell><cell>0.464*</cell><cell>0.456*</cell><cell>0.234*</cell><cell>0.408*</cell></row><row><cell></cell><cell>LM-GVP (Wang et al., 2021)</cell><cell>0.710</cell><cell>0.664</cell><cell>0.302*</cell><cell>0.417*</cell><cell>0.580*</cell><cell>0.545*</cell><cell>0.423*</cell><cell>0.527*</cell></row><row><cell></cell><cell>GearNet-Edge (Multiview Contrast)</cell><cell>0.892</cell><cell>0.874</cell><cell>0.292</cell><cell>0.490</cell><cell>0.596</cell><cell>0.650</cell><cell>0.336</cell><cell>0.486</cell></row><row><cell></cell><cell>GearNet-Edge (Residue Type Prediction)</cell><cell>0.870</cell><cell>0.834</cell><cell>0.267</cell><cell>0.430</cell><cell>0.583</cell><cell>0.604</cell><cell>0.311</cell><cell>0.465</cell></row><row><cell>Structure Pretrained</cell><cell>GearNet-Edge (Distance Prediction)</cell><cell>0.863</cell><cell>0.839</cell><cell>0.274</cell><cell>0.448</cell><cell>0.586</cell><cell>0.616</cell><cell>0.327</cell><cell>0.464</cell></row><row><cell></cell><cell>GearNet-Edge (Angle Prediction)</cell><cell>0.880</cell><cell>0.853</cell><cell>0.291</cell><cell>0.458</cell><cell>0.603</cell><cell>0.625</cell><cell>0.331</cell><cell>0.473</cell></row><row><cell></cell><cell>GearNet-Edge (Dihedral Prediction)</cell><cell>0.881</cell><cell>0.859</cell><cell>0.304</cell><cell>0.458</cell><cell>0.603</cell><cell>0.626</cell><cell>0.338</cell><cell>0.465</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Accuracy (%) on fold and reaction classification. [*] denotes results taken from<ref type="bibr" target="#b33">Hermosilla et al. (2021)</ref> and<ref type="bibr" target="#b32">Hermosilla &amp; Ropinski (2022)</ref>. For pretraining, we select the model with the best performance when training from scratch.</figDesc><table><row><cell>Category</cell><cell>Method</cell><cell></cell><cell cols="2">Fold Classification</cell><cell></cell><cell>Reaction</cell></row><row><cell></cell><cell></cell><cell cols="4">Fold Super. Fam. Avg.</cell><cell>Acc.</cell></row><row><cell></cell><cell>Without Pretraining</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>CNN (Shanehsazzadeh et al., 2020)</cell><cell>11.3</cell><cell>13.4</cell><cell>53.4</cell><cell>26.0</cell><cell>51.7</cell></row><row><cell>Sequence-based</cell><cell>ResNet (Rao et al., 2019) LSTM (Rao et al., 2019)</cell><cell>10.1 6.41</cell><cell>7.21 4.33</cell><cell>23.5 18.1</cell><cell>13.6 9.61</cell><cell>24.1 11.0</cell></row><row><cell></cell><cell>Transformer (Rao et al., 2019)</cell><cell>9.22</cell><cell>8.81</cell><cell>40.4</cell><cell>19.4</cell><cell>26.6</cell></row><row><cell></cell><cell>GCN (Kipf &amp; Welling, 2017)</cell><cell cols="4">16.8* 21.3* 82.8* 40.3*</cell><cell>67.3*</cell></row><row><cell></cell><cell>GVP (Jing et al., 2021)</cell><cell>16.0</cell><cell>22.5</cell><cell>83.8</cell><cell>40.7</cell><cell>65.5</cell></row><row><cell></cell><cell>3DCNN MQA (Derevyanko et al., 2018)</cell><cell cols="4">31.6* 45.4* 92.5* 56.5*</cell><cell>72.2*</cell></row><row><cell>Structure-based</cell><cell>GraphQA (Baldassarre et al., 2021)</cell><cell cols="4">23.7* 32.5* 84.4* 46.9*</cell><cell>60.8*</cell></row><row><cell></cell><cell>DeepFRI (Gligorijevi? et al., 2021)</cell><cell cols="4">15.3* 20.6* 73.2* 36.4*</cell><cell>63.3*</cell></row><row><cell></cell><cell>IEConv (Hermosilla et al., 2021)</cell><cell cols="4">45.0* 69.7* 98.9* 71.2*</cell><cell>87.2*</cell></row><row><cell></cell><cell>New IEConv (Hermosilla &amp; Ropinski, 2022)</cell><cell cols="4">47.6* 70.2* 99.2* 72.3*</cell><cell>87.2*</cell></row><row><cell></cell><cell>GearNet</cell><cell>28.4</cell><cell>42.6</cell><cell>95.3</cell><cell>55.4</cell><cell>79.4</cell></row><row><cell>Ours</cell><cell>GearNet-IEConv GearNet-Edge</cell><cell>42.3 44.0</cell><cell>64.1 66.7</cell><cell>99.1 99.1</cell><cell>68.5 69.9</cell><cell>83.7 86.6</cell></row><row><cell></cell><cell>GearNet-Edge-IEConv</cell><cell>48.3</cell><cell>70.3</cell><cell>99.5</cell><cell>72.7</cell><cell>85.3</cell></row><row><cell></cell><cell>With Pretraining</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sequence Pretrained</cell><cell>ESM-1b (Rives et al., 2021) ProtBERT-BFD (Elnaggar et al., 2021)</cell><cell cols="4">26.8 26.6* 55.8* 97.6* 60.0* 60.1 97.8 61.5</cell><cell>83.1 72.2*</cell></row><row><cell></cell><cell>New IEConv (Contrast Learning)</cell><cell cols="4">50.3* 80.6* 99.7* 76.9*</cell><cell>87.6*</cell></row><row><cell></cell><cell>GearNet-Edge(-IEConv) (Multiview Contrast)</cell><cell>54.1</cell><cell>80.5</cell><cell>99.9</cell><cell>78.1</cell><cell>87.5</cell></row><row><cell>Structure Pretrained</cell><cell cols="2">GearNet-Edge(-IEConv) (Residue Type Prediction) 48.8 GearNet-Edge(-IEConv) (Distance Prediction) 50.9</cell><cell>71.0 73.5</cell><cell>99.4 99.4</cell><cell>73.0 74.6</cell><cell>86.6 87.5</cell></row><row><cell></cell><cell>GearNet-Edge(-IEConv) (Angle Prediction)</cell><cell>56.5</cell><cell>76.3</cell><cell>99.6</cell><cell>77.4</cell><cell>86.8</cell></row><row><cell></cell><cell>GearNet-Edge(-IEConv) (Dihedral Prediction)</cell><cell>51.8</cell><cell>77.8</cell><cell>99.6</cell><cell>75.9</cell><cell>87.0</cell></row><row><cell cols="2">with the goal to predict the fold class label given a protein.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Reaction classification aims to predict the enzyme-catalyzed</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p><p><p><p><p>reaction class of a protein, in which all four levels of the EC number are employed to depict the reaction class. Although the latter task is essentially the same with EC prediction, we include it to make a fair comparison with the baselines in</p><ref type="bibr" target="#b33">Hermosilla et al. (2021)</ref></p>.</p>Dataset splits. For fold classification,</p><ref type="bibr" target="#b34">Hou et al. (2018)</ref> </p>provides three different test sets: Fold, in which proteins from the same superfamily are unseen during training; Superfamily, in which proteins from the same family are</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation study results on EC prediction.</figDesc><table><row><cell>Method</cell><cell cols="2">AUPR pair F max</cell></row><row><cell>GearNet-Edge</cell><cell>0.872</cell><cell>0.810</cell></row><row><cell>-w/o relational convolution</cell><cell>0.778</cell><cell>0.752</cell></row><row><cell>-w/o edge message passing</cell><cell>0.751</cell><cell>0.730</cell></row><row><cell>GearNet-Edge (Multiview Contrast)</cell><cell>0.892</cell><cell>0.874</cell></row><row><cell>-subsequence + identity</cell><cell>0.874</cell><cell>0.866</cell></row><row><cell>-subspace + identity</cell><cell>0.894</cell><cell>0.872</cell></row><row><cell>-subsequence + random edge masking</cell><cell>0.885</cell><cell>0.869</cell></row><row><cell>-subspace + random edge masking</cell><cell>0.887</cell><cell>0.876</cell></row><row><cell>GearNet-Edge (Dihedral Prediction)</cell><cell>0.881</cell><cell>0.859</cell></row><row><cell>-w/ random sampling</cell><cell>0.841</cell><cell>0.821</cell></row><row><cell cols="3">not present during training; and Family, in which proteins</cell></row><row><cell cols="3">from the same family are present during training. For re-</cell></row><row><cell cols="3">action classification, we adopt dataset splits proposed in</cell></row><row><cell cols="3">Hermosilla et al. (2021), where proteins have less than 50%</cell></row><row><cell>sequence similarity in-between splits.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Dataset statistics for downstream tasks.</figDesc><table><row><cell>Dataset</cell><cell cols="3"># Proteins # Train # Validation # Test</cell></row><row><cell>Enzyme Commission</cell><cell>15,550</cell><cell>1,729</cell><cell>1,919</cell></row><row><cell>Gene Ontology</cell><cell>29,898</cell><cell>3,322</cell><cell>3,415</cell></row><row><cell>Fold Classification -Fold</cell><cell>12,312</cell><cell>736</cell><cell>718</cell></row><row><cell cols="2">Fold Classification -Superfamily 12,312</cell><cell>736</cell><cell>1,254</cell></row><row><cell>Fold Classification -Family</cell><cell>12,312</cell><cell>736</cell><cell>1,272</cell></row><row><cell>Reaction Classification</cell><cell>29,215</cell><cell>2,562</cell><cell>5,651</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Hyperparameter configurations of our model on different datasets. The batch size reported in the table referrs to the batch size on each GPU. All the hyperparameters are chosen by the performance on the validation set.</figDesc><table><row><cell cols="2">Hyperparameter</cell><cell>EC</cell><cell>GO</cell><cell cols="2">Fold Reaction</cell></row><row><cell></cell><cell>#layer</cell><cell>6</cell><cell>6</cell><cell>6</cell><cell>6</cell></row><row><cell>GNN</cell><cell>hidden dim.</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell></row><row><cell></cell><cell>dropout</cell><cell>0.1</cell><cell>0.1</cell><cell>0.2</cell><cell>0.2</cell></row><row><cell></cell><cell>optimizer</cell><cell cols="3">AdamW AdamW SGD</cell><cell>SGD</cell></row><row><cell></cell><cell>learning rate</cell><cell>1e-4</cell><cell>1e-4</cell><cell>1e-3</cell><cell>1e-3</cell></row><row><cell>Learning</cell><cell>weight decay</cell><cell>0</cell><cell>0</cell><cell>5e-4</cell><cell>5e-4</cell></row><row><cell></cell><cell>batch size</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell></row><row><cell></cell><cell># epoch</cell><cell>200</cell><cell>200</cell><cell>300</cell><cell>300</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Results of GearNet-Edge pretrained on different pretraining datasets with different methods. Models are evaluated on the EC prediction task.</figDesc><table><row><cell>Dataset</cell><cell># Proteins</cell><cell cols="2">Multivew Contrast</cell><cell cols="2">Residue Type Prediction</cell><cell cols="2">Distance Prediction</cell><cell cols="2">Angle Prediction</cell><cell cols="2">Dihedral Prediction</cell></row><row><cell></cell><cell></cell><cell>AUPRpair</cell><cell>Fmax</cell><cell>AUPRpair</cell><cell>Fmax</cell><cell>AUPRpair</cell><cell>Fmax</cell><cell cols="2">AUPRpair Fmax</cell><cell>AUPRpair</cell><cell>Fmax</cell></row><row><cell>AlphaFold Database (v1 + v2)</cell><cell>804,872</cell><cell>0.892</cell><cell>0.874</cell><cell>0.870</cell><cell>0.834</cell><cell>0.863</cell><cell>0.839</cell><cell>0.880</cell><cell>0.853</cell><cell>0.881</cell><cell>0.859</cell></row><row><cell>AlphaFold Database (v1)</cell><cell>365,198</cell><cell>0.890</cell><cell>0.874</cell><cell>0.869</cell><cell>0.842</cell><cell>0.871</cell><cell>0.843</cell><cell>0.879</cell><cell>0.854</cell><cell>0.877</cell><cell>0.852</cell></row><row><cell>AlphaFold Database (v2)</cell><cell>439,674</cell><cell>0.890</cell><cell>0.874</cell><cell>0.868</cell><cell>0.838</cell><cell>0.868</cell><cell>0.846</cell><cell>0.881</cell><cell>0.853</cell><cell>0.883</cell><cell>0.861</cell></row><row><cell>Protein Data Bank</cell><cell>305,265</cell><cell>0.881</cell><cell>0.859</cell><cell>0.870</cell><cell>0.841</cell><cell>0.865</cell><cell>0.847</cell><cell>0.880</cell><cell>0.857</cell><cell>0.886</cell><cell>0.858</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Dataset statistics for protein engineering tasks.</figDesc><table><row><cell>Dataset</cell><cell cols="3"># Proteins # Train # Validation # Test</cell></row><row><cell>Fluorescence</cell><cell>21,446</cell><cell>5,362</cell><cell>27,217</cell></row><row><cell>Stability</cell><cell>53,614</cell><cell>2,512</cell><cell>12,851</cell></row><row><cell>GB1</cell><cell>4,580</cell><cell>509</cell><cell>3,644</cell></row><row><cell>Thermostability</cell><cell>5,149</cell><cell>643</cell><cell>1,366</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Hyperparameter configurations of our model on protein engineering datasets. The batch size reported in the table referrs to the batch size on each GPU. All the hyperparameters are chosen by the performance on the validation set.</figDesc><table><row><cell cols="2">Hyperparameter</cell><cell cols="2">Fluores Stability</cell><cell>GB1</cell><cell>Thermo</cell></row><row><cell></cell><cell>#layer</cell><cell>6</cell><cell>6</cell><cell>6</cell><cell>6</cell></row><row><cell>GNN</cell><cell>hidden dim.</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell></row><row><cell></cell><cell>dropout</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell></cell><cell>optimizer</cell><cell cols="4">AdamW AdamW AdamW AdamW</cell></row><row><cell></cell><cell>learning rate</cell><cell>1e-4</cell><cell>1e-4</cell><cell>1e-4</cell><cell>1e-4</cell></row><row><cell>Learning</cell><cell>weight decay</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>batch size</cell><cell>8</cell><cell>32</cell><cell>8</cell><cell>2</cell></row><row><cell></cell><cell># epoch</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>200</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Spearman's ? (rank correlation coefficient) on four protein engineering tasks. [*] means the results are taken from<ref type="bibr" target="#b77">Wang et al. (2021)</ref>, while [ ?] means the results are taken from<ref type="bibr" target="#b21">Dallago et al. (2021)</ref>.</figDesc><table><row><cell>Category</cell><cell>Method</cell><cell cols="4">Fluores Stability GB1 Thermo</cell></row><row><cell></cell><cell>Without Pretraining</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>CNN (Shanehsazzadeh et al., 2020)</cell><cell>0.656</cell><cell>0.717</cell><cell>0.51  ?</cell><cell>0.49  ?</cell></row><row><cell>Sequence-based</cell><cell>ResNet (Rao et al., 2019) LSTM (Rao et al., 2019)</cell><cell>0.369 0.124</cell><cell>0.478 0.477</cell><cell>0.294 0.552</cell><cell>0.412 0.142</cell></row><row><cell></cell><cell>Transformer (Rao et al., 2019)</cell><cell>0.522</cell><cell>0.645</cell><cell>0.001</cell><cell>OOM</cell></row><row><cell></cell><cell>GAT (Veli?kovi? et al., 2018)</cell><cell>0.390*</cell><cell>0.565*</cell><cell>-</cell><cell>-</cell></row><row><cell>Structure-based</cell><cell>GVP (Jing et al., 2021)</cell><cell>0.545*</cell><cell>0.680*</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>New IEConv (Hermosilla &amp; Ropinski, 2022)</cell><cell>0.635</cell><cell>0.529</cell><cell>0.205</cell><cell>OOM</cell></row><row><cell>Ours</cell><cell>GearNet GearNet-Edge</cell><cell>0.682 0.677</cell><cell>0.719 0.740</cell><cell>0.546 0.545</cell><cell>0.632 0.654</cell></row><row><cell></cell><cell>With Pretraining</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>ESM-1b (Rives et al., 2021)</cell><cell>0.682</cell><cell>0.734</cell><cell>0.59  ?</cell><cell>0.76  ?</cell></row><row><cell>Sequence Pretrained</cell><cell>ProtBERT-BFD (Elnaggar et al., 2021)</cell><cell>0.677*</cell><cell>0.734*</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>LM-GVP (Wang et al., 2021)</cell><cell>0.679*</cell><cell>0.733*</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>GearNet-Edge (Multiview Contrast)</cell><cell>0.675</cell><cell>0.752</cell><cell>0.296</cell><cell>0.640</cell></row><row><cell></cell><cell>GearNet-Edge (Residue Type Prediction)</cell><cell>0.675</cell><cell>0.787</cell><cell>0.597</cell><cell>0.543</cell></row><row><cell>Ours</cell><cell>GearNet-Edge (Distance Prediction)</cell><cell>0.675</cell><cell>0.688</cell><cell>0.583</cell><cell>0.622</cell></row><row><cell></cell><cell>GearNet-Edge (Angle Prediction)</cell><cell>0.673</cell><cell>0.727</cell><cell>0.512</cell><cell>0.645</cell></row><row><cell></cell><cell>GearNet-Edge (Dihedral Prediction)</cell><cell>0.670</cell><cell>0.746</cell><cell>0.521</cell><cell>0.634</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>This research project focuses on learning effective protein representations via pretraining with a large number of unlabeled protein structures. Compared to the conventional sequence-based pretraining methods, our approach is able to leverage structural information and thus provide better representations. This merit enables more in-depth analysis of protein research and can potentially benefit many real-world applications, like protein function prediction and sequence design.However, it cannot be denied that some harmful activities could be augmented by powerful pretrained models, e.g., designing harmful drugs. We expect future studies will mitigate these issues.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This project is supported by <rs type="programName">AIHN IBM-MILA partnership program</rs>, the <rs type="funder">Natural Sciences and Engineering Research Council (NSERC)</rs> <rs type="grantName">Discovery Grant</rs>, the <rs type="funder">Canada CIFAR AI Chair Program</rs>, collaboration grants between <rs type="funder">Microsoft Research and Mila</rs>, <rs type="institution">Samsung Electronics Co., Ltd.</rs>, <rs type="funder">Amazon Faculty Research Award, Tencent AI Lab Rhino-Bird Gift Fund</rs>, a <rs type="funder">NRC Collaborative R&amp;D Project</rs> (<rs type="grantNumber">AI4D-CORE-06</rs>) as well as the <rs type="programName">IVADO Fundamental Research Project</rs> grant <rs type="grantNumber">PRF-2019-3583139727</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_QvmPrE9">
					<orgName type="grant-name">Discovery Grant</orgName>
					<orgName type="program" subtype="full">AIHN IBM-MILA partnership program</orgName>
				</org>
				<org type="funding" xml:id="_3jwsKuT">
					<idno type="grant-number">AI4D-CORE-06</idno>
					<orgName type="program" subtype="full">IVADO Fundamental Research Project</orgName>
				</org>
				<org type="funding" xml:id="_Dh5sJZC">
					<idno type="grant-number">PRF-2019-3583139727</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Then, the average precision and recall over all proteins at threshold t is defined as:</p><p>and</p><p>where we use N to denote the number of proteins and M (t) to denote the number of proteins on which at least one prediction was made above threshold t, i.e., |P i (t)| &gt; 0.</p><p>Combining these two measures, the maximum F-score is defined as the maximum value of F-measure over all thresholds. That is,</p><p>The second metric, pair-centric area under precision-recall curve AUPR pair , is defined as the average precision scores for all protein-function pairs, which is exactly the micro average precision score for multiple binary classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. Implementation Details</head><p>In this subsection, we describe implementation details of all baselines and our methods. For all models, the outputs will be fed into a three-layer MLP to make final prediction.</p><p>The dimension of hidden layers in the MLP is equal to the dimension of model outputs.</p><p>CNN <ref type="bibr" target="#b65">(Shanehsazzadeh et al., 2020)</ref>. Following the finding in <ref type="bibr" target="#b65">Shanehsazzadeh et al. (2020)</ref>, we directly employ a shallow convolutional neural network (CNN) to encode protein sequences. Specifically, 2 convolutional layers with 1024 hidden dimensions and kernel size 5 constitute this baseline model.</p><p>ResNet <ref type="bibr" target="#b55">(Rao et al., 2019)</ref>. We also adopt a deep CNN model, i.e., the ResNet for protein sequences proposed by <ref type="bibr" target="#b55">Rao et al. (2019)</ref>, in our benchmark. This model is with 12 residual blocks and 512 hidden dimensions, and it uses the GELU <ref type="bibr" target="#b31">(Hendrycks &amp; Gimpel, 2016)</ref> activation function.</p><p>LSTM <ref type="bibr" target="#b55">(Rao et al., 2019)</ref>. The bidirectional LSTM model proposed by <ref type="bibr" target="#b55">(Rao et al., 2019)</ref> is another baseline for protein sequence encoding. It is composed of three bidirectional LSTM layers with 640 hidden dimensions.</p><p>Transformer <ref type="bibr" target="#b55">(Rao et al., 2019)</ref>. The self-attention-based Transformer encoder <ref type="bibr" target="#b74">(Vaswani et al., 2017</ref>) is a strong model in natural language processing (NLP), <ref type="bibr" target="#b55">Rao et al. (2019)</ref> adapts this model into the field of protein sequence modeling. We also adopt it as one of our baselines. This model has a comparable size with BERT-Small <ref type="bibr" target="#b20">(Devlin et al., 2018)</ref>, which contains 4 Transformer blocks with 512 hidden dimensions and 8 attention heads, and it is activated by GELU <ref type="bibr" target="#b31">(Hendrycks &amp; Gimpel, 2016)</ref>.</p><p>ESM-1b <ref type="bibr">(Rives et al., 2021)</ref>. Besides the from-scratch sequence encoders above, we also compare with two state-ofthe-art pretrained protein language models. ESM-1b <ref type="bibr">(Rives et al., 2021</ref>) is a huge Transformer encoder model whose size is larger than BERT-Large <ref type="bibr" target="#b20">(Devlin et al., 2018)</ref>, and it is pretrained on 24 million protein sequences from UniRef50 <ref type="bibr" target="#b71">(Suzek et al., 2007)</ref> by masked language modeling (MLM) <ref type="bibr" target="#b20">(Devlin et al., 2018)</ref>. In our evaluation, we finetune the ESM-1b model with the learning rate that is one-tenth of that of the MLP prediction head.</p><p>ProtBERT-BFD <ref type="bibr" target="#b21">(Elnaggar et al., 2021)</ref>. The other protein language model evaluated in our benchmark is ProtBERT-BFD <ref type="bibr" target="#b21">(Elnaggar et al., 2021)</ref> whose size also excesses BERT-Large <ref type="bibr" target="#b20">(Devlin et al., 2018)</ref>. This model is pretrained on 2.1 billion protein sequences from BFD (Steinegger &amp; S?ding, 2018) by MLM <ref type="bibr" target="#b20">(Devlin et al., 2018)</ref>. The evaluation of ProtBERT-BFD uses the same learning rate configuration as ESM-1b.</p><p>GVP <ref type="bibr" target="#b38">(Jing et al., 2021)</ref>. The GVP model <ref type="bibr" target="#b38">(Jing et al., 2021)</ref> is a decent protein structure encoder. It iteratively updates the scalar and vector representations of a protein, and these representations possess the merit of invariance and equivariance. In our benchmark, we evaluate this baseline method following the official source code. In specific, 3 GVP layers with 32 feature dimensions (20 scalar and 4 vector channels) constitute the GVP model.</p><p>LM-GVP <ref type="bibr" target="#b77">(Wang et al., 2021)</ref>. To further enhance the effectiveness of GVP <ref type="bibr" target="#b38">(Jing et al., 2021)</ref>, <ref type="bibr" target="#b77">Wang et al. (2021)</ref> proposed to prepend a protein language model, i.e. Prot-BERT <ref type="bibr" target="#b21">(Elnaggar et al., 2021)</ref>, before GVP to additionally utilize protein sequence representations. We also adopt this hybrid model as one of our baselines, and its implementation follows the official source code.</p><p>DeepFRI <ref type="bibr" target="#b24">(Gligorijevi? et al., 2021)</ref>. We also evaluate DeepFRI <ref type="bibr" target="#b24">(Gligorijevi? et al., 2021)</ref> in our benchmark, which is a popular structure-based encoder for protein function prediction. DeepFRI employs an LSTM model to extract residue features and further constructs a residue graph to propagate messages among residues, in which a 3-layer graph convolutional network (GCN) <ref type="bibr" target="#b42">(Kipf &amp; Welling, 2017)</ref> is used for message passing. We directly utilize the official model checkpoint for baseline evaluation.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Akdel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>J?nes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Zalevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>M?sz?ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Good</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Laskowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pozzati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A structural biology community assessment of alphafold 2 applications</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unified rational protein engineering with sequence-based deep representation learning</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Alley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Khimulya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alquraishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1315" to="1322" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Accurate prediction of protein structures and interactions using a three-track neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dimaio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Anishchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dauparas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ovchinnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Kinch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Schaeffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">373</biblScope>
			<biblScope unit="issue">6557</biblScope>
			<biblScope unit="page" from="871" to="876" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graphqa: protein model quality assessment using graph convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Baldassarre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Men?ndez Hurtado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elofsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="360" to="366" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Gaussian approximation potentials: The accuracy of quantum mechanics, without the electrons. Physical review letters</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Bart?k</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cs?nyi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page">136403</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On representing chemical environments</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Bart?k</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cs?nyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review B</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page">184115</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generalized neural-network representation of high-dimensional potential-energy surfaces</title>
		<author>
			<persName><forename type="first">J</forename><surname>Behler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Parrinello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">146401</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning protein sequence embeddings using information from structure</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bepler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Berger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08661</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning the protein language: Evolution, structure, and function</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bepler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="654" to="669" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The protein data bank</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Westbrook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gilliland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Weissig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">N</forename><surname>Shindyalov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Bourne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="235" to="242" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Low-n protein engineering with dataefficient deep learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Khimulya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Alley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Esvelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="389" to="396" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fold2seq: A joint sequence(1d)fold(3d) embedding-based generative model for protein design</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chenthamarakshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Melnyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v139/cao21a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021-07">Jul 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph networks as a universal machine learning framework for molecules and crystals</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Ong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemistry of Materials</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3564" to="3572" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Machine learning of accurate energy-conserving molecular force fields</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Sauceda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Poltavsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science advances</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1603015</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Uniprot: the universal protein knowledgebase in 2021</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">U</forename><surname>Consortium</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="480" to="D489" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Protein interaction interface region prediction by geometric deep learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bailey-Kellogg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Flip: Benchmark tasks in fitness landscape inference for proteins</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wittmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep convolutional networks for quality assessment of protein folds</title>
		<author>
			<persName><forename type="first">G</forename><surname>Derevyanko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grudinin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lamoureux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="4046" to="4053" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Prottrans: Towards cracking the language of lifes code through selfsupervised deep learning and high performance computing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elnaggar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heinzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rehawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gibbs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Feher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Angerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bhowmik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2021.3095381</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gainza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sverrisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Correia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="184" to="192" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Structure-based protein function prediction using graph convolutional networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gligorijevi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Renfrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kosciolek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Leman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Berenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vatanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Fisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vlamakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Some properties of line digraphs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Z</forename><surname>Norman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rendiconti del circolo matematico di palermo</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="168" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Contrastive multi-view representation learning on graphs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Khasahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4116" to="4126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Pre-training coevolutionary protein representation via a pairwise masked language model</title>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.15527</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Contrastive representation learning for 3d protein structures</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hermosilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=VINWzIM6_6" />
	</analytic>
	<monogr>
		<title level="m">Submitted to The Tenth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Hermosilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sch?fer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fackelmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>V?zquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kozl?kov?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deepsf: deep convolutional neural network for mapping protein sequences to folds</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Adhikari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1295" to="1303" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12265</idno>
		<title level="m">Strategies for pre-training graph neural networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generative models for graph-based protein design</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="15820" to="15831" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Meltome atlas-thermal proteome stability across the tree of life</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jarzab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kurzawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moerch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zecha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Leijten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Musiol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maschberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stoehr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="495" to="503" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning from protein structure with geometric vector perceptrons</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eismann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Soni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Dror</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=1YLJDvSx6J4" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Neural message passing with edge updates for predicting Protein Structure Representation Learning by Geometric Pretraining properties of molecules and materials</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>J?rgensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Schmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03146</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Highly accurate protein structure prediction with alphafold</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tunyasuvunakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>??dek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Potapenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">596</biblScope>
			<biblScope unit="issue">7873</biblScope>
			<biblScope unit="page" from="583" to="589" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<title level="m">Variational graph auto-encoders</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Directional message passing for molecular graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gro?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
		<author>
			<persName><surname>Gemnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08903</idno>
		<title level="m">Universal directional graph neural networks for molecules</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Oztekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05013</idno>
		<title level="m">Spherical message passing for 3d graph networks</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Selfsupervised contrastive learning of protein representations by mutual information maximization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Moses</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioRxiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Melville</surname></persName>
		</author>
		<author>
			<persName><surname>Umap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<title level="m">Uniform manifold approximation and projection for dimension reduction</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Language models enable zero-shot prediction of the effects of mutations on protein function</title>
		<author>
			<persName><forename type="first">J</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verkuil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Colabfold -making protein folding accessible to all</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mirdita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sch?tze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Moriwaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ovchinnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steinegger</surname></persName>
		</author>
		<idno type="DOI">10.1101/2021.08.15.456425</idno>
		<ptr target="https://www.biorxiv.org/content/early/2021/10/29/2021.08.15.456425" />
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pfam: The protein families database in 2021</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mistry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chuguransky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Sonnhammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Tosatto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Paladin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="412" to="D419" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Scop: a structural classification of proteins database for the investigation of sequences and structures</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Murzin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chothia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of molecular biology</title>
		<imprint>
			<biblScope unit="volume">247</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="536" to="540" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Gcc: Graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A large-scale evaluation of computational protein function prediction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Radivojac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Oron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Schnoes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wittkop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sokolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Graim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Funk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Verspoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Hur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="221" to="227" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Evaluating protein transfer learning with tape</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verkuil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rives</surname></persName>
		</author>
		<author>
			<persName><surname>Msa</surname></persName>
		</author>
		<author>
			<persName><surname>Transformer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Global analysis of protein folding using massively parallel design, synthesis, and testing</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Rocklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Chidyausiku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goreshnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Houliston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lemak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Mulligan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chevalier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">357</biblScope>
			<biblScope unit="issue">6347</biblScope>
			<biblScope unit="page" from="168" to="175" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Self-supervised graph transformer on largescale molecular data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02835</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Local fitness landscape of the green fluorescent protein</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Sarkisyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Bolotin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Meer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Usmanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Mishin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Sharonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Ivankov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Bozhanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Baranov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Soylemez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">533</biblScope>
			<biblScope unit="issue">7603</biblScope>
			<biblScope unit="page" from="397" to="401" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European semantic web conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Quantum-chemical insights from deep tensor neural networks</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Arbabzadah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Sauceda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><surname>Schnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08566</idno>
		<title level="m">A continuousfilter convolutional neural network for modeling quantum interactions</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Is transfer learning necessary for protein landscape prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shanehsazzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.03443</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Multi-scale representation learning on proteins</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>Somnath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bunne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Clustering huge protein sequence sets in linear time</title>
		<author>
			<persName><forename type="first">M</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>S?ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Mmseqs2 enables sensitive protein sequence searching for the analysis of massive data sets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>S?ding</surname></persName>
		</author>
		<idno type="DOI">10.1038/nbt.3988</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Biotechnology</title>
		<idno type="ISSN">1546-1696</idno>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1026" to="1028" />
			<date type="published" when="2017-11">Nov 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Fast and flexible protein design using deep graph neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Strokach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Becerra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Corbi-Verge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perez-Riba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="402" to="411" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Axiomatic attribution for deep networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3319" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Uniref: comprehensive and non-redundant uniprot reference clusters</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Suzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcgarvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1282" to="1288" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Fast end-to-end learning on protein surfaces</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sverrisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feydy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Correia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15272" to="15281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Alphafold protein structure database: massively expanding the structural coverage of protein-sequence space with high-accuracy models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Varadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anyango</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Natassia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yordanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Stroe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Laydon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJXMpikCZ" />
		<title level="m">Graph Attention Networks. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>accepted as poster</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">A high efficient biological language model for predicting protein-protein interactions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cells</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">122</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Lm-gvp: A generalizable deep learning framework for protein property prediction from sequence and structure</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Combs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Calvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Golovach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">O</forename><surname>Salawu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Wise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Ponnapalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Adaptation in protein fitness landscapes is facilitated by indirect paths</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Lloyd-Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Elife</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2016">16965. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Selfsupervised graph-level representation learning with local and global structure</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04113</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<title level="m">Graph contrastive learning with augmentations. Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5812" to="5823" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
