<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EXPHORMER: Sparse Transformers for Graphs</title>
				<funder>
					<orgName type="full">Canada CIFAR AI Chairs program</orgName>
				</funder>
				<funder>
					<orgName type="full">Digital Resource Alliance of Canada</orgName>
				</funder>
				<funder>
					<orgName type="full">Natural Sciences and Engineering Resource Council of Canada</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hamed</forename><surname>Shirzad</surname></persName>
							<email>&lt;shirzad@cs.ubc.ca&gt;</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ameya</forename><surname>Velingker</surname></persName>
							<email>&lt;ameyav@google.com&gt;</email>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
								<address>
									<addrLine>Mountain View</addrLine>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Balaji</forename><surname>Venkatachalam</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
								<address>
									<addrLine>Mountain View</addrLine>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Danica</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Alberta Machine Intelligence Institute</orgName>
								<address>
									<settlement>Edmonton</settlement>
									<region>Alberta</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ali</forename><forename type="middle">Kemal</forename><surname>Sinop</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
								<address>
									<addrLine>Mountain View</addrLine>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<settlement>Balaji Venkatachalam</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EXPHORMER: Sparse Transformers for Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph transformers have emerged as a promising architecture for a variety of graph learning and representation tasks. Despite their successes, though, it remains challenging to scale graph transformers to large graphs while maintaining accuracy competitive with message-passing networks. In this paper, we introduce EXPHORMER, a framework for building powerful and scalable graph transformers. EXPHORMER consists of a sparse attention mechanism based on two mechanisms: virtual global nodes and expander graphs, whose mathematical characteristics, such as spectral expansion, pseduorandomness, and sparsity, yield graph transformers with complexity only linear in the size of the graph, while allowing us to prove desirable theoretical properties of the resulting transformer models. We show that incorporating EXPHORMER into the recentlyproposed GraphGPS framework produces models with competitive empirical results on a wide variety of graph datasets, including state-of-the-art results on three datasets. We also show that EX-PHORMER can scale to datasets on larger graphs than shown in previous graph transformer architectures. Codes can be found at https: //github.com/hamed1375/Exphormer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graph learning has become an important and popular area of study that has yielded impressive results on a wide variety of graphs and tasks, including molecular graphs, social network graphs, knowledge graphs, and more. While much research around graph learning has focused on graph neural networks (GNNs), which are based on local message-passing, a more recent approach to graph learning that has garnered much interest involves the use of graph transformers (GTs). Graph transformers largely operate by encoding graph structure in the form of a soft inductive bias. These can be viewed as a graph adaptation of the Transformer architecture <ref type="bibr" target="#b41">(Vaswani et al., 2017)</ref> that are successful in modeling sequential data in applications such as natural language processing. Graph transformers allow nodes to attend to all other nodes in a graph, allowing for direct modeling of long-range interactions, in contrast to GNNs. This allows them to avoid several limitations associated with local message passing GNNs, such as oversmoothing <ref type="bibr" target="#b33">(Oono &amp; Suzuki, 2020)</ref>, oversquashing <ref type="bibr" target="#b2">(Alon &amp; Yahav, 2021;</ref><ref type="bibr" target="#b40">Topping et al., 2022)</ref>, and limited expressivity <ref type="bibr" target="#b30">(Morris et al., 2019;</ref><ref type="bibr" target="#b43">Xu et al., 2018)</ref>. The promise of graph transformers has led to a large number of different graph transformer models that have been proposed in recent years <ref type="bibr">(Dwivedi &amp; Bresson, 2020;</ref><ref type="bibr" target="#b25">Kreuzer et al., 2021;</ref><ref type="bibr" target="#b44">Ying et al., 2021;</ref><ref type="bibr" target="#b29">Mialon et al., 2021)</ref>.</p><p>One major challenge for graph transformers is their poor scalability, as the standard global attention mechanism incurs time and memory complexity of O(|V | 2 ), quadratic in the number of nodes in the graph. While this cost is often acceptable for datasets with small graphs (e.g., molecular graphs), it can be prohibitively expensive for datasets containing larger graphs, where graph transformer models often do not fit in memory even for high-memory GPUs, and hence would require much more complex and slower schemes to apply. Moreover, despite the expressivity advantages of graph transformer networks <ref type="bibr" target="#b25">(Kreuzer et al., 2021)</ref>, these architectures have often lagged message-passing counterparts in accuracy in many practical settings.</p><p>A recent breakthrough came with the advent of GraphGPS <ref type="bibr" target="#b35">(Ramp?sek et al., 2022)</ref>, a modular framework for constructing networks by combining local message passing and a global attention mechanism together with a choice of various positional and structural encodings. To attempt to overcome the quadratic complexity of the "dense" full transformer and improve scalability, the architecture allows for "sparse" attention mechanisms, like Performer <ref type="bibr" target="#b9">(Choromanski et al., 2021)</ref> or Big Bird <ref type="bibr" target="#b47">(Zaheer et al., 2020)</ref>. This combination of Transformers and GNNs achieves state-ofthe-art performance on a wide variety of datasets.</p><p>In almost all cases, however, the best results of <ref type="bibr">Ramp?sek et al.</ref> were obtained by combining a message-passing network with a full transformer; their sparse transformers performed relatively poorly by comparison, and indeed their ablation studies showed that on a number of datasets it is better to avoid using attention at all than to use their implementation of BigBird. This may be related to the fact that sparse attention mechanisms like BigBird have largely been designed for sequences; this is natural for language tasks, but graphs behave quite differently. Thus, it is natural to ask whether one can design sparse attention mechanisms more tailored to learning interactions on general graphs.</p><p>Another major question concerns graph transformers' scalability. While BigBird and Performer are linear attention mechanisms, they still incur computational overhead that dominates the per-epoch computation time for moderatelysized graphs. The GraphGPS work tackles datasets with graphs of up to 5,000 nodes, a regime in which the fullattention transformer is in fact computationally faster than many sparse linear-attention mechanisms. Perhaps more suitable sparse attention mechanisms could enable their framework to operate on even larger graphs. Additionally, for smaller graphs, they may require a much smaller batch size to fit into the GPU memory, resulting in slower training, or the need for high-end GPUs. For instance, on the MalNet-Tiny dataset, GraphGPS with a full transformer runs out of memory (on a 40GB NVIDIA A100 GPU) with a batch size of just 16; as we shall see, our techniques allow us to extend the batch size to 256 without any memory issues.</p><p>Our contributions. We propose sparse attention mechanisms with computational cost linear in the number of nodes and edges. We introduce EXPHORMER that combines the two techniques for creating sparse overlay graphs. The first sparse attention mechanism is to use global nodes -nodes that are connected to all other nodes of the graph. The number of additional edges added to the graph is linear in the number of nodes. We also introduce expander graphs as a powerful primitive in designing scalable graph transformer architectures. The expander graph as an overlay graph has edges linear in the number of nodes. Expander graphs have several desirable properties -small diameter, spectral approximation of a complete graph, good mixing properties -which make them a suitable ingredient in a sparse attention mechanism. We are able to show that EX-PHORMER, which combines expander graphs with global nodes and local neighborhoods, spectrally approximates the full attention mechanism with only a small number of layers, and has universal approximation properties. Its attention scheme provides good inductive bias for places the model "should look," in addition to being more efficient and less memory-intensive.</p><p>We show that EXPHORMER are a powerful Transformer that produces results comparable to GraphGPS with a full transformer. Moreover, when combined with MPNNs in the GraphGPS framework, we can achieve SOTA or close to SOTA. EXPHORMER (1) produces better results than other sparse transformers on all datasets we evaluate; (2) has results comparable to and in some cases better than the full transformer, despite having fewer parameters; (3) achieves state-of-the-art results on many datasets, better than MPNNs, including on Long Range Graph Benchmark (LRGB; <ref type="bibr" target="#b16">Dwivedi et al., 2022)</ref> datasets that require longrange dependencies between nodes; and (4) can scale to larger graphs than previously shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Graph Neural Networks (GNNs). Early works in the area of graph learning and GNNs include the development of a number of architectures such as GCN <ref type="bibr" target="#b12">(Defferrard et al., 2016;</ref><ref type="bibr" target="#b24">Kipf &amp; Welling, 2017)</ref>, GraphSage <ref type="bibr" target="#b20">(Hamilton et al., 2017)</ref>, GIN <ref type="bibr" target="#b43">(Xu et al., 2018)</ref>, GAT <ref type="bibr" target="#b42">(Veli?kovi? et al., 2018)</ref>, GatedGCN <ref type="bibr" target="#b6">(Bresson &amp; Laurent, 2017)</ref>, and more. GNNs are based on a message-passing architecture that generally confines their expressivity to the limits of the 1-Weisfeiler-Lehman (1-WL) isomorphism test <ref type="bibr" target="#b43">(Xu et al., 2018)</ref>.</p><p>A number of recent papers have sought to augment GNNs to improve their expressivity. For instance, one approach has been to use additional features that allow nodes to be distinguished -such as using a one-hot encoding of the node <ref type="bibr" target="#b31">(Murphy et al., 2019)</ref> or a random scalar feature <ref type="bibr" target="#b36">(Sato et al., 2021)</ref> -or to encode positional or structural information of the graph -e.g., skip-gram based network embeddings <ref type="bibr" target="#b34">(Qiu et al., 2018)</ref>, substructure counts <ref type="bibr" target="#b5">(Bouritsas et al., 2020)</ref>, or Laplacian eigenvectors <ref type="bibr" target="#b15">(Dwivedi et al., 2021</ref>). Another direction has been to modify the message passing rule to allow the network to take further advantage of the graph structure -including directional graph networks (DGN; <ref type="bibr" target="#b3">Beaini et al., 2021)</ref> that use Laplacian eigenvectors to define directional flows for anisotropic message aggregation -or to modify the underlying graph over which message passing occurs with higher-order GNNs <ref type="bibr" target="#b30">(Morris et al., 2019)</ref> or the use of substructures such as junction trees <ref type="bibr" target="#b17">(Fey et al., 2020)</ref> and simplicial complexes <ref type="bibr" target="#b4">(Bodnar et al., 2021)</ref>.<ref type="foot" target="#foot_0">1</ref> Graph transformer architectures. Attention mechanisms have been extremely successful in sequence modeling since the seminal work of <ref type="bibr" target="#b41">Vaswani et al. (2017)</ref>. The GAT architecture <ref type="bibr" target="#b42">(Veli?kovi? et al., 2018)</ref> proposed using an attention mechanism to determine how a node aggregates information from its neighbors; it does not use a positional encoding for nodes, limiting its ability to exploit global structural information. GraphBert <ref type="bibr" target="#b48">(Zhang et al., 2020)</ref> uses the graph structure to determine an encoding of the nodes, but not for the underlying attention mechanism.</p><p>Graph transformer models typically operate on a fullyconnected graph in which every pair of nodes is connected, regardless of the connectivity structure of the original graph. Spectral Attention Networks (SAN) <ref type="bibr" target="#b25">(Kreuzer et al., 2021)</ref> make use of two attention mechanisms, one on the fullyconnected graph and one on the original edges of the input graph, while using Laplacian positional encodings for the nodes. Graphormer <ref type="bibr" target="#b44">(Ying et al., 2021)</ref> uses a single dense attention mechanism but adds structural features in the form of centrality and spatial encodings. Meanwhile, GraphiT <ref type="bibr" target="#b29">(Mialon et al., 2021)</ref> incorporates relative positional encodings based on diffusion kernels.</p><p>GraphGPS <ref type="bibr" target="#b35">(Ramp?sek et al., 2022)</ref> proposed a general framework for combining message-passing networks with attention mechanisms, while allowing for the mixing and matching of positional and structural embeddings. Specifically, the framework also allows for sparse transformer models like BigBird <ref type="bibr" target="#b47">(Zaheer et al., 2020)</ref> and Performer <ref type="bibr" target="#b9">(Choromanski et al., 2021)</ref>.</p><p>Moreover, recent works have proposed sampling-based scalable graph transformers, e.g., Gophormer <ref type="bibr" target="#b49">(Zhao et al., 2021)</ref>, NAGphormer <ref type="bibr">(Chen et al., 2022b)</ref>.</p><p>Sparse Transformers. Standard (dense) transformers have quadratic complexity in the number of tokens, which limits their scalability to extremely long sequences. By contrast, sparse transformer models improve computational and memory efficiency by restricting the attention pattern, i.e., the pairs of nodes that can interact with each other. In addition to BigBird and Performer, there have been a number of other proposals for sparse transformers; <ref type="bibr" target="#b38">Tay et al. (2020)</ref> provide a survey.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Sparse Attention on Graphs</head><p>This section describes three sparse patterns that can be used in transformers in individual layers of a graph transformer architecture. We begin by describing graph attention mechanisms in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Attention mechanism on graphs</head><p>An attention mechanism on n tokens can be modeled by a directed graph H on [n] = {1, 2, . . . , n}, where a directed edge from i to j indicates a direct interaction between tokens i and j, i.e., an inner product that will be computed by the attention mechanism. More precisely, a transformer block can be viewed as a function on the d-dimensional embed-dings for each of n tokens, mapping from R d?n to R d?n . Let X = (x 1 , x 2 , . . . , x n ) ? R d?n . A generalized (dotproduct) attention mechanism ATTN H : R d?n ? R d?n with attention pattern given by H is defined by</p><formula xml:id="formula_0">ATTN H (X) :,i = x i + h j=1 W j O W j V X N H (i) ?? W j K X N H (i) T W j Q x i ,</formula><p>where h is the number of heads and m is the head size, while</p><formula xml:id="formula_1">W j K , W j Q , W j V ? R m?d and W j O ? R d?m .</formula><p>(The subscript K is for "keys," Q for "queries," V for "values," and O for "output.") Here X N H (i) denotes the submatrix of X obtained by picking out only those columns corresponding to elements of N H (i), the neighbors of i in H. We can see that the total number of inner product computations for all i ? [n] is given by the number of edges of H. A (generalized) transformer block consists of ATTN H followed by a feedforward layer:</p><formula xml:id="formula_2">FF(X) = ATTN H (X) + W 2 ? ReLU(W 1 ? ATTN H X) + b 1 1 T n + b 2 1 T n ,</formula><p>where</p><formula xml:id="formula_3">W 1 ? R r?d , W 2 ? R d?r , b 1 ? R r , and b 2 ? R d .</formula><p>In the standard setting, the n tokens are part of a sequence (e.g., language applications). However, we are concerned with the graph transformer setting in which the tokens are nodes of some underlying graph</p><formula xml:id="formula_4">G = (V, E) with V = [n].</formula><p>The attention computation is nearly identical, except that one can also optionally augment it with edge features, as is done in SAN <ref type="bibr" target="#b25">(Kreuzer et al., 2021)</ref>:</p><formula xml:id="formula_5">ATTN H (X) :,i = x i + h j=1 W j O W j V X N H (i) ? ? W j E E N H (i) W j K X N H (i) T W j Q x i , where W j E ? R m?d E , E N H (i) is the d E ? |N H (i)</formula><p>| matrix whose columns are d E -dimensional edge features for the edges connected to node i, and denotes element-wise multiplications.</p><p>The most typical cases of graph transformers use full (dense) attention, where every token attends to every other node: H is the fully-connected directed graph. As this results in computational complexity O(n 2 ) for the transformer block, which is prohibitively expensive for large graphs, we wish to replace full attention with a sparse attention mechanism, where H has o(n 2 ) edges -ideally, O(n).</p><p>A number of sparse attention mechanisms have been proposed to address the aforementioned issue (see <ref type="bibr" target="#b38">Tay et al., 2020)</ref>, but the vast majority are designed specifically for functions on sequences. EXPHORMER, on the other hand, is a graph-centric sparse attention mechanism that makes use of the underlying structure of the input graph G. We introduce three sparse patterns: expander graphs, global connectors, and local neighborhoods as three patterns that can be used in transformers. They can combine together to make an EXPHORMER layer, but it is not necessary to have all components in each layer.</p><p>GraphGPS <ref type="bibr" target="#b35">(Ramp?sek et al., 2022)</ref> shows an effective way to include the transformer layers beside an MPNN model. We use the same architecture as GraphGPS, replacing only the transformer part with an EXPHORMER layer. We will show that sparse transformers can get comparable and even better results on many benchmarks compared to full transformers, or sparse transformer variants originally designed for sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The EXPHORMER Architecture</head><p>We 1. Expander graph attention: Edges from a random expander graph can be used as attention patterns. These graphs have several useful theoretical properties related to spectral approximation and random walk mixing (see Section 4), which allow propagating information between pairs of nodes that are distant in the input graph G without connecting all pairs of nodes. They introduce many alternative short paths between the nodes and avoid the information bottleneck that can be caused by the virtual nodes. In particular, we use a regular expander graph of constant degree, which allows the number of edges to be just O(|V |). The process we use to construct a random expander graph is described in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Global attention:</head><p>The next component is global attention, whereby a small number of virtual nodes are added to the interaction graph, and each such node is connected to all the non-virtual nodes. These nodes enable a global "storage sink" and they have universal approximator functions for full transformers. We will generally add a constant number of virtual nodes, in which case the total number of edges due to global attention will be O(|V |).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Local neighborhood attention:</head><p>Another desirable property to capture is locality. Graphs carry much more topological structure than sequences, and the neighborhoods of individual nodes carry a lot of information about connectivity. Thus, we model local interactions by allowing each node v to attend to every other node that is an immediate neighbor of v in G: that is, H includes the input graph edges E as well as their reverses, introducing O(|E|) interaction edges. One generalization would be to allow direct attention within k-hop neighborhoods, but this might introduce a superlinear number of interactions on general graphs.</p><p>We use learnable embeddings for expander and global connection edge features, and virtual nodes' features. Dataset edge features are used for the local neighborhood edge features. We always use local neighborhoods in EXPHORMER layers, but expander graphs and global attention are not always helpful; depending on the dataset, we may use both, or only one or the other. We discuss this further in Appendix D.</p><p>Some previous graph-oriented transformers, such as the SAN architecture <ref type="bibr" target="#b25">(Kreuzer et al., 2021)</ref>, use separate attention mechanisms for different sources of edges. By using a single attention mechanism, EXPHORMER achieves a more compact model that can still distinguish the "types" of attention edges based on edge features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Theoretical Properties of EXPHORMER</head><p>EXPHORMER is based on expander graphs, which have a number of properties that make them suitable as a key building block of our approach. In this section, we describe relevant properties of expander graphs along with their implications for EXPHORMERs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Expander Graphs Approximate Complete Graphs</head><p>For simplicity, let us consider d-regular graphs (where every node has <ref type="bibr" target="#b0">(Alon, 1986)</ref>.</p><formula xml:id="formula_6">d neighbors). Suppose G is a d-regular undirected graph on n vertices. Let A G be the n ? n adjacency matrix of G. It is known that A G has n real eigenvalues d = ? 1 ? ? 2 ? ? ? ? ? ? n ? -d. The graph G is said to be an -expander if max{|? 2 |, |? n |} ? (1 -)d</formula><p>Expander graphs are sparse approximations of complete graphs. As we discuss, expander graphs with only O(n) edges exist that can preserve certain desirable properties of the complete graph with ?(n 2 ) edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">SPECTRAL PROPERTIES</head><p>A useful tool to study expanders is the Laplacian matrix of a graph, which captures several important spectral properties. Letting D G denote the n ? n diagonal matrix whose i-th diagonal entry is the degree of the i-th node, we define</p><formula xml:id="formula_7">L G = D G -A G to be the Laplacian of G.</formula><p>The following theorem is well-known in spectral graph theory. Theorem 4.1. A d-regular -expander G on n vertices spectrally approximates the complete graph K n on n vertices:</p><formula xml:id="formula_8">2 (1 -) 1 n L K 1 d L G (1 + ) 1 n L K .</formula><p>Spectral approximation is known to preserve the cut structure in graphs. As a result, a sparse attention mechanism based on expander edges retains spectral properties of the full attention mechanism: cuts, vertex expansion, and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">MIXING PROPERTIES</head><p>Another property of expanders is that random walks mix well. Let G = (V, E) be a d-regular -expander. Consider 2 For matrices A and B, we say that A B if B -A is a positive semi-definite matrix.</p><p>a random walk v 0 , v 1 , v 2 , . . . on G, where v 0 ? ? (0) , and then each subsequent v t+1 is one of the d neighbors of v t chosen uniformly at random. We then have v t ? ? (t) , given recursively by t) . It turns out that after a logarithmic number of steps, a random walk from a starting probability distribution on the vertices is close to uniformly distributed along all nodes of the graph. Lemma 4.2. <ref type="bibr" target="#b0">(Alon, 1986)</ref> </p><formula xml:id="formula_9">? (t+1) = D -1 G A G ? (</formula><formula xml:id="formula_10">Let G = (V, E) be a d-regular -expander graph on n = |V | nodes. For any initial distri- bution ? (0) : V ? R + and any ? &gt; 0, ? (t) satisfies ? (t) -1 n 1 ? ? as long as t = ? 1 log(n/?) .</formula><p>In an attention mechanism of a transformer, one can consider the graph of pairwise interactions (i.e., i is connected to j if i and j attend to each other). If the attention mechanism is dense, then each node is connected to every other node and it is trivial to see that every pair of nodes interacts with each other in a single transformer layer. In a sparse attention mechanism, on the other hand, some pairs of nodes are not directly connected, meaning that a single transformer layer will not model interactions between all pairs of nodes. However, if we stack transformer layers on top of each other, the stack will be able to model longer range interactions. In particular, a consequence of the above lemma is that if our sparse attention mechanism is modeled after an -expander graph, then stacking at least t = 1 log(n/?) layers will model "most" pairwise interactions between nodes.</p><p>Relatedly, the diameter of expander graphs is asymptotically logarithmic in the number of nodes. Theorem 4.3. <ref type="bibr" target="#b0">(Alon, 1986</ref>) Suppose G = (V, E) is a dregular -expander graph on n vertices. Then, for every vertex v and k ? 0, the k-hop neighborhood</p><formula xml:id="formula_11">B(v, r) = {w ? V : d(v, w) ? k} has |B(v, r)| ? min{(1 + c) k , n}</formula><p>for some constant c &gt; 0 depending on d, . In particular, we have that diam(G) = O d, (log n).</p><p>As a consequence, we obtain the following, which shows that using logarithmically many successive transformer layers allows each node to propagate information to every node. Corollary 4.4. If a sparse attention mechanism on n nodes is a d-regular -expander graph, then stacking O d, (log n) transformer layers models all pairwise node interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Universal Approximability of EXPHORMER Models</head><p>The Dense transformer models are known to be universal approximators, i.e., with the use of positional encodings, they can approximate any continuous sequence-to-sequence function on a compact domain arbitrarily closely <ref type="bibr">(Yun et al., 2020a)</ref>.</p><p>In the realm of graph learning, this provides a compelling motivation for considering graph transformers over standard MPNNs, which are known to be limited in expressive power by the Weisfeiler-Lehman (WL) hierarchy.</p><p>Universal approximation properties for dense transformers do not automatically hold for sparse transformers, but we show every continuous function f : [0, 1] d?|V | ? R d?|V | can be approximated to any desired accuracy by an EX-PHORMER network using either global attention or a suitable version of expander attention. Details are in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we evaluate the empirical performance of graph transformer models based on EXPHORMER on a wide variety of graph datasets with graph prediction and node prediction tasks <ref type="bibr">(Dwivedi et al., 2020;</ref><ref type="bibr" target="#b21">Hu et al., 2020;</ref><ref type="bibr" target="#b18">Freitas et al., 2021;</ref><ref type="bibr" target="#b37">Shchur et al., 2018;</ref><ref type="bibr" target="#b32">Namata et al., 2012)</ref>. In particular, we present experiments on fifteen benchmark datasets, including image-based graph datasets (CIFAR10, MNIST, PascalVOC-SP, COCO-SP), synthetic SBM datasets (PATTERN, CLUSTER), code graph datasets (MalNet-Tiny), and molecular datasets (Peptides-Func, Peptides-Struct, PCQM-Contact). Moreover, we demonstrate the ability of EXPHORMER to allow graph transformers to scale to larger graphs (with more than 5,000 nodes) by including results on five transductive graph datasets consisting of citation networks (CS, Physics, ogbn-arxiv) and co-purchasing networks (Computer, Photo).</p><p>For the experimental setup, we combine EXPHORMER together with MPNNs in the GraphGPS framework <ref type="bibr" target="#b35">(Ramp?sek et al., 2022)</ref>, which constructs graph transformer models by composing attention mechanisms with message-passing schemes, together with an appropriate choice of positional and structural encodings.</p><p>In addition to comparisons with baselines on the aforementioned datasets, we also run ablation studies on the various attention components of EXPHORMER (from Section 3.2).</p><p>In summary, our experiments show that: (a) EXPHORMER achieves SOTA performance on a variety of datasets, (b) EXPHORMER consistently outperforms other sparse attention mechanisms while often surpassing dense transformers using fewer parameters, and (c) EXPHORMER successfully allows GraphGPS to overcome memory bottlenecks and scale to larger graphs (on &gt; 10, 000 nodes) while still providing competitive performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparison to Sparse Attention Mechanisms</head><p>We first discuss a comparison of our EXPHORMER-based models with other sparse transformer architectures, which are a natural first point of comparison. In particular, we perform a series of experiments that compare EXPHORMERbased architectures with sparse transformer baselines in the GraphGPS framework. More specifically, for each dataset, we compare our best EXPHORMER model with GraphGPS models that use BigBird and Performer for the underlying attention mechanism.</p><p>The results are shown in Table <ref type="table" target="#tab_2">2</ref>. Note that on each of the four highlighted datasets, an EXPHORMER model outperforms the sparse attention models GPS-BigBird and GPS-Performer. Furthermore, it even outperforms the full (dense) transformer model (GPS-Transformer) on three of the datasets while performing competitively on the fourth. Since GraphGPS models make use of both attention (transformer) and message passing components, we additionally point out that the EXPHORMER-based sparse attention component is, indeed, providing lift over a standard MPNN baseline that does not make use of attention at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Benchmarking GNNs Datasets</head><p>We have showed in Section 5.1 that EXPHORMER consistently outperforms relevant sparse attention baselines. The natural next question is how EXPHORMER performs relative to a wider range of baselines, including not just graph transformer models but also other architectures such as MPNNs.</p><p>Table <ref type="table" target="#tab_0">1</ref> shows results on five datasets, including four from the Benchmarking GNNs collection <ref type="bibr">(Dwivedi et al., 2020)</ref> as well as the code graph dataset MalNet-Tiny <ref type="bibr" target="#b18">(Freitas et al., 2021)</ref>. We note that an EXPHORMER-based graph transformer with message-passing in the GraphGPS framework yields state-of-the-art (SOTA) performance on three of the Observe that on all five datasets, our EXPHORMER models provide better accuracy than GraphGPS models based on full (dense) transformers. Additionally, the EXPHORMER models outperform the full transformer-based SAN model as well as a variety of MPNN baselines. Remark 5.1. Our EXPHORMER models often outperform full transformer models with a much smaller number of parameters. For instance, on PATTERN, our results reported in Table <ref type="table" target="#tab_0">1</ref> are obtained from an EXPHORMER model with just 90,000 parameters, compared to 340,000 parameters in the comparable full transformer GraphGPS model! Similarly, on CLUSTER, our EXPHORMER model uses just 280,000 parameters, as compared to 500,000 parameters for the full transformer GraphGPS model. This results in simpler models with time and memory advantages. For further details on hyperparameters, see Table <ref type="table" target="#tab_7">7</ref> in the appendix. Remark 5.2. EXPHORMER models often enable larger batch sizes during training. On MalNet-Tiny, which contains some graphs with around 5,000 nodes, the full transformer GraphGPS model runs out of memory on a 40GB NVIDIA A100 GPU when using a batch size of just 16, while our sparse EXPHORMER models handle a batch size of 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Long-Range Graph Benchmark</head><p>We have additionally conducted a set of experiments on the Long-Range Graph Benchmark (LRGB, <ref type="bibr" target="#b16">Dwivedi et al., 2022)</ref>, which consists of five challenging datasets that test a model's ability to learn long-range dependencies in input graphs. The results are presented in Table <ref type="table" target="#tab_3">3</ref>, which shows that our EXPHORMER-based sparse attention models are able to outperform GraphGPS on three of the five datasets, achieving SOTA performance. Furthermore, on the remaining two datasets in the LRGB suite, EXPHORMER is competitive with the best single model results, obtained by full attention GraphGPS models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Scaling to Larger Graphs</head><p>One of the shortcomings of graph transformer architectures has been their poor scalability to larger graphs on thousands of nodes. Dense attention mechanisms (e.g., SAN and Graphormer) have quadratic memory complexity and time complexity, which restrict their applicability to datasets on graphs with relatively few nodes, such as molecular graphs.</p><p>GraphGPS has made use of sparse attention mechanisms, but even so, the architecture handles graphs of up to about 5,000 nodes (e.g., MalNet-Tiny, <ref type="bibr" target="#b18">Freitas et al., 2021)</ref>, often with very small batch size (see Remark 5.2).</p><p>A natural question is whether the sparse attention mechanism of EXPHORMER models allows us to extend graph transformer architectures to significantly larger graphs while providing competitive performance. Indeed, we show this is the case by training EXPHORMER models on larger datasets, including Amazon Computer, Amazon Photo, Coauthor CS, Coauthor Physics <ref type="bibr" target="#b37">(Shchur et al., 2018)</ref>, which has up issues on a number of these datasets, demonstrating the utility of EXPHORMER. To provide meaningful comparisons to other transformer architectures, we thus include NAGphormer <ref type="bibr">(Chen et al., 2022b)</ref>, a scalable sampling-based graph transformer architecture, as a baseline. EXPHORMER performs competitively and, in fact, achieves SOTA accuracy on Computer: the best non-transformer method has an accuracy of 90.74 <ref type="bibr" target="#b27">(Luo et al., 2022)</ref>.</p><p>We would also like to highlight that EXPHORMER can be used for even larger graphs, with hundreds of thousands of nodes. In particular, we provide results on ogbn-arxiv <ref type="bibr" target="#b21">(Hu et al., 2020)</ref>, a challenging transductive dataset consisting of a single large graph of the citation network of over 160K arXiv papers, containing over a million edges. Specifically, we achieve a test accuracy of 0.7244 using the EXPHORMER architectures. At the time of writing, a relevant leaderboard (ogb) shows 0.7966 as the highest reported test accuracy <ref type="bibr">(Zhao et al., 2022a)</ref>. Table <ref type="table" target="#tab_10">15</ref> compares EXPHORMER to other MPNNs and sparse transformers. A dense full transformer does not even fit in memory on a 40GB NVIDIA A100 GPU (even for a tiny network of only 2 layers and 32 hidden dimensions). We then fixed the network size to 3 hidden layers and 96 hidden dimensions to be able to fit the sparse models in memory. Notice that BigBird and Performer have significantly longer epoch times and worse performance than with just a MPNN. EXPHORMER with virtual nodes only (without expander edges) improves on the MPNN and provides an accuracy of 0.7222. EX-PHORMER with expander edges further improves the accu-racy to 0.7244.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Ablation Studies</head><p>Here we analyze the effect of each of the components of the model. Our EXPHORMER model has three main components: local neighborhood, expander edges, and virtual nodes. In Table <ref type="table" target="#tab_5">5</ref>, we analyze which parts of the model have been useful, and which parts can be removed without reducing the performance. In all of these experiments, the MPNN part is fixed, so it gives a baseline number for the results, and the transformer part boosts over the MPNN part. Through these experiments we can see that local neighborhood have been always a good option to add to the Exphormer, but between virtual nodes and expander graphs, sometimes one of them causes reduction in the performance. This issue is discussed further in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>EXPHORMER is a new class of sparse graph transformer architectures. We introduced two types of sparse networks with virtual nodes and using expander graphs. We have shown that the mathematical properties of our architecture make EXPHORMER a suitable choice for graph learning.</p><p>Our sparse architecture uses fewer training parameters, is faster to train and has memory complexity linear in the size of the graph. These properties help us scale to larger graphs which were typically elusive to other Transformer-based methods. EXPHORMER outperforms other sparse transformers and performs comparably or better than full transformers.</p><p>We have shown the applicability of EXPHORMER on a wide variety of graph learning datasets. Combining EXPHORMER with MPNN in the GraphGPS framework allows us to obtain</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset Descriptions</head><p>Below, we provide descriptions of the datasets on which we conduct experiments.</p><p>CIFAR10 and MNIST <ref type="bibr">(Dwivedi et al., 2020)</ref> CIFAR10 and MNIST are the graph equivalents of the image classification datasets of the same name. A graph is created by constructing the 8-nearest neighbor graph of the SLIC superpixels of the image. These are both 10-class graph classification problems.</p><p>PascalVOC-SP and COCO-SP <ref type="bibr" target="#b15">(Dwivedi et al., 2021)</ref> These are similar graph versions of image datasets, but they are larger images and the task is to perform node classification, i.e. semantic segmentation of superpixels. These graphs are larger, and the task more complex, than CIFAR10 and MNIST.</p><p>CLUSTER and PATTERN <ref type="bibr">(Dwivedi et al., 2020)</ref> PATTERN and CLUSTER are node classification problems. Both are synthetic datasets that are sampled from a Stochastic Block Model (SBM), is a popular way to model communities. In PATTERN, the prediction task is to identify if a node belongs to one of the 100 possible predetermined subgraph patterns.</p><p>In CLUSTER, the goal is to classify nodes into six different clusters with the same distribution.</p><p>MalNet-Tiny <ref type="bibr" target="#b18">(Freitas et al., 2021)</ref> Malnet-Tiny is a smaller dataset generated from a larger dataset for identifying malware based on function call graphs from Android APKs. The tiny dataset contains 5000 graphs, each with up to 5000 nodes. The task is to predict the graph as being benign or from one of four types of malware.</p><p>ogbn-arxiv <ref type="bibr" target="#b22">(Hu et al., 2021)</ref> The ogbn-arxiv dataset consists of one large directed graph of 169343 nodes and 1,166,243 edges representing a citation network between all computer science papers on arXiv that were indexed by the Microsoft academic graph. Nodes in the graph represent papers, while a directed edge indicates that a paper cites another. Each node has an 128-dimensional feature vector derived from embeddings of words in the title and abstract of the underlying paper.</p><p>The prediction task is a 40-class node classification problem -to identify the primary category of each arXiv paper, as listed by the authors. Moreover, the nodes of the citation graph are split into 90K training nodes, 30K validation notes, and 48K test nodes.</p><p>Coauthor datasets Coauthor CS and Physics are co-authorship graphs from Microsoft Academic Graph. The nodes represent the authors and two authors who share a paper are connected by an edge. The node features are from the keywords in the papers. The class represent the active area of study for the author.</p><p>Amazon datasets Amazon Computers and Amazon photo are Amazon co-purchase graphs. Nodes represents products purchased an edges indicate pairs of products purchased together. Node features are bag-of-words encoded reiews of the products. Class labels are the product category.</p><p>Peptides-Func, Peptides-Struct, and PCQM-Contact <ref type="bibr" target="#b15">(Dwivedi et al., 2021)</ref> These datasets are molecular graphs introduced as a part of the Long Range Graph Benchmark (LRGB). Graphs in these datasets have relatively large diameters: the average diameter for PCQM-Contact is 9.86?1.79, and for the Peptides datasets 56.99?28.72. Average shortest path lengths are 4.63?0.63 and 20.89?9.79 accordingly. On PCQM-Contact the task is edge-level, and we need to rank the edges. Peptides-Func is a multi-label graph classification task, with 10 labels. Peptides-Struct is graph-level regression of 11 structural properties of the molecules.</p><p>Table <ref type="table" target="#tab_6">6</ref> shows a summary of the statistics of the aforementioned datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Hyperparameters</head><p>Our hyperparameter choices, including the optimizer, positional encodings, and structural encodings, were guided by the instructions in GraphGPS <ref type="bibr" target="#b35">(Ramp?sek et al., 2022)</ref>. There were some cases, however, when more layers with smaller dimensions gave better results in EXPHORMER. This may be due to the fact that each node gets fewer inputs for each layer, but EXPHORME requires more layers in order to propagate well. Additionally, we observed that Equivstable Laplacian Positional Encoding (ESLapPE) performed better than normal Laplacian Positional Encoding (LapPE) in some cases, in Through our model, some extra hyperparameters are introduced -the degree of the graph expander and the number of virtual nodes. For these hyperparameters, we used linear search and found that expander degree 3-7 was the most effective.</p><p>Depending on the graph size, we used 1-6 virtual nodes. As the number of hyperparameters is large, grid search was not feasible on all the parameters. As a result, for other hyperparameters, we did a linear search for each parameter to find out which parameters work better.</p><p>To make fair comparisons, we used a similar parameter-budget to GraphGPS. For PATTERN and CLUSTER, we used a parameter-budget of 500K, and for CIFAR and MNIST, we used a parameter-budget of around 100K. See details in Tables <ref type="table" target="#tab_7">7  to 9</ref>. However, our approach departs from that of BigBird in some important ways. While BigBird uses w-width "window attention" to capture locality of reference, we use local neighborhood attention to capture locality and graph topology. In particular, the interaction graph due to window attention in BigBird can be viewed as a Cayley graph on Z n , which is sequence-centric, while EXPHORMER is graph-centric and, therefore, uses the structure of the input graph itself to capture locality. BigBird, as implemented for graphs by <ref type="bibr" target="#b35">Ramp?sek et al. (2022)</ref>, instead simply orders the graph nodes in an arbitrary </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Ramanujan Graphs</head><p>A natural question is how strong the spectral expansion properties of a d-regular graph can be, i.e., for how large an &gt; 0 does a d-regular -expander exist. The following theorem gives a bound on how large the spectral gap can be.</p><p>Theorem C.1 (Alon-Boppana). Let d &gt; 0. The eigenvalues of the adjacency matrix of a d-regular graph on n nodes satisfy</p><formula xml:id="formula_12">max{|? 2 |, |? n |} ? 2 ? d -1 -o n (1).</formula><p>In other words, a d-regular -expander graph can exist only for ? 1 -2</p><formula xml:id="formula_13">? d-1 d + o n (1).</formula><p>As it turns out, there exist -expander graphs with achieving this bound. In fact, a d-regular -expander graph satisfying</p><formula xml:id="formula_14">? 1 -2 ? d-1 d</formula><p>is known as a Ramanujan graph. Ramanujan graphs are essentially the best possible spectral expanders, and several constructions have been considered over the years <ref type="bibr" target="#b26">(Lubotzky et al., 1988;</ref><ref type="bibr" target="#b28">Margulis, 1988)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Random Regular Graphs</head><p>While there exist deterministic constructions of Ramanujan graphs, they are often algebraic/number theoretic in nature and therefore exist only for specific choices of d (e.g., the constructions of <ref type="bibr" target="#b26">Lubotzky et al. (1988)</ref> as well as independently of <ref type="bibr" target="#b28">Margulis (1988)</ref>, for which one requires d ? 2 (mod 4) and d -1 to be a prime). Recently, the work of <ref type="bibr" target="#b1">Alon (2021)</ref> showed a construction of strongly explicit near-Ramanujan graphs of every degree, but it should be noted that the construction needs the number of nodes to be sufficiently large. It is, therefore, often convenient to use a probabilistic construction of an expander.</p><p>A natural choice for an expander graph is a random d-regular graph on n vertices, formed by taking d/2 independent uniform permutations on {1, 2, . . . , n}. <ref type="bibr" target="#b19">Friedman (2003)</ref> proved a conjecture of Alon, establishing that random regular graphs are weakly-Ramanujan. ? Remove any self loops from E ; for large n, this will happen with probability o(1).</p><formula xml:id="formula_15">? If ?(G) ? ? 2d-1+ d</formula><p>, then stop; otherwise generate a new random permutation ? and try again.</p><p>It is easy to see that this procedure is equivalent to sampling d permutations, so Theorem C.2 shows that the resulting graph will be a 2d-regular expander with high probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. On Expander Graphs versus Global Connectors</head><p>From ablation studies in Section 5.5, we can see that sometimes only having expander graphs leads to the best results, sometimes virtual nodes only, and sometimes using a combination of both. Although this is a hyperparameter to tune, we can have a general understanding from the datasets on which cases virtual nodes work better and vice versa. Here we remark on some of these patterns.</p><p>Graph diameter: Using virtual nodes, the diameter of the graph becomes exactly 2 (unless the original graph was already complete, with diameter 1). Expander graphs also help to reduce the diameter of the graph, but only achieve a (probabilistic) diameter guarantee of O(log n).</p><p>Information Bottleneck: Virtual nodes serve as a global sink, and so even a single node should be able to keep all the information flowing from the whole graph. In case the graphs are large or many nodes rely on the virtual nodes to pass information, though, virtual nodes are likely to cause information bottleneck, and may even reduce the performance of the model. Expander graphs, to the contrary, introduce many new paths; as they rely on the local information storage of the nodes, they do not have this same problem.</p><p>Interference with the local structure: Virtual nodes introduce a new node, through which all of their new connections pass. Expander edges, on the other hand, are much easier to confuse with the actual edges of the graph; this can make it more difficult for the model to use the graph structure appropriately. As our model uses shared weights among the types of activations, which substantially helps with model size, the only way to understand the edges are different is through the edge embeddings. For datasets like MalNet-Tiny which rely heavily on the structure, while node and edge features are less useful, expander edges can interfere with the information propagation.</p><p>We observed that on molecular datasets, virtual nodes generally help substantially, while expander edges don't help much or can even hurt. The situation for image-based graphs is the opposite, where especially on Pascal-VOC, expander graphs are highly useful to the model, but virtual nodes can cause information bottleneck.</p><p>Virtual nodes can also make incorporating ideas like the batching mechanism used in GraphSAGE <ref type="bibr" target="#b20">(Hamilton et al., 2017)</ref> more difficult, where having virtual nodes means every batch includes the whole graph. Expander edges can still be used with batching techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Universality of EXPHORMER</head><p>In this section, we detail the universal approximation properties of EXPHORMER-based graph transformers.</p><p>The work of <ref type="bibr">Yun et al. (2020a)</ref> showed that for sequences, transformers are universal approximators, i.e., they can approximate any permutation equivariant function mapping one sequence to another arbitrarily closely when provided with enough parameters. A function f : R d?n ? R d?n is said to be permutation equivariant if f (XP) = f (X)P, i.e., if permuting the columns of an input X ? R d?n results in the columns of f (X) being permuted the same way.</p><p>Theorem E.1 <ref type="bibr">(Yun et al., 2020a)</ref>. Let 1 ? p &lt; ? and &gt; 0. For any function f : R d?n ? R d?n that is permutation equivariant, there exists a transformer network g such that p (f, g) &lt; .</p><p>Otherwise, assume that instead condition (2) holds. Then, we can apply Theorem 1 of <ref type="bibr">Yun et al. (2020b)</ref>. In particular, we note that since (a.) H contains self-loops, (b.) H contains a Hamiltonian path, and (c.) diam(H) = O(log n) due to Corollary 4.4, it follows that the assumptions of the theorem of <ref type="bibr">Yun et al. (2020b)</ref> are satisfied. (Note that in our setup, ? is the softmax function, which satisfies their Assumption 2.) Hence, the desired g exists for any f, p, ?.</p><p>This completes the proof.</p><p>This result implies that there exist sparse transformers based on the variant of EXPHORMER in Theorem E.3 which can solve graph isomorphism problems. This does not, however, imply the existence of an efficient algorithm for solving graph isomorphism problems, as we have not shown these networks can be efficiently identified; see further discussion by <ref type="bibr" target="#b25">Kreuzer et al. (2021)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The components of EXPHORMER: (a) shows local neighborhood attention, i.e., edges of the input graph. (b) shows an expander graph with degree 3. (c) shows global attention with a single virtual node. (d) All of the aforementioned components are combined into a single interaction graph that determines the attention pattern of EXPHORMER.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>now describe the details of the construction of EX-PHORMER, an expander-based sparse attention mechanism for graph transformers with O(|V | + |E|) computation, where G = (V, E) is the underlying input graph. The EXPHORMER architecture constructs an interaction graph H that consists of three main components, as shown in Figure 1. The construction always has bidirectional edges, and so H can be viewed as an undirected graph. The mechanism uses three types of edges:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of EXPHORMER with baselines on various datasets. Best results are colored in first, second, third.</figDesc><table><row><cell>Model</cell><cell>CIFAR10</cell><cell>MalNet-Tiny</cell><cell>MNIST</cell><cell>CLUSTER</cell><cell>PATTERN</cell></row><row><cell></cell><cell>Accuracy ?</cell><cell>Accuracy ?</cell><cell>Accuracy ?</cell><cell>Accuracy ?</cell><cell>Accuracy ?</cell></row><row><cell>GCN (Kipf &amp; Welling, 2017)</cell><cell>55.710?0.381</cell><cell>81.0</cell><cell>90.705?0.218</cell><cell>68.498 ? 0.976</cell><cell>71.892 ? 0.334</cell></row><row><cell>GIN (Xu et al., 2018)</cell><cell>55.255?1.527</cell><cell>88.98?0.557</cell><cell>96.485?0.252</cell><cell>64.716 ? 1.553</cell><cell>85.387 ? 0.136</cell></row><row><cell>GAT (Veli?kovi? et al., 2018)</cell><cell>64.223?0.455</cell><cell>92.1 ?0.242</cell><cell>95.535?0.205</cell><cell>70.587 ? 0.447</cell><cell>78.271 ? 0.186</cell></row><row><cell>GatedGCN (Bresson &amp; Laurent, 2017;</cell><cell>67.312?0.311</cell><cell>92.23?0.65</cell><cell>97.340?0.143</cell><cell>73.840 ? 0.326</cell><cell>85.568 ? 0.088</cell></row><row><cell>Dwivedi et al., 2020)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PNA (Corso et al., 2020)</cell><cell>70.35?0.63</cell><cell>-</cell><cell>97.94?0.12</cell><cell>-</cell><cell>-</cell></row><row><cell>DGN (Beaini et al., 2021)</cell><cell>72.838?0.417</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>86.680?0.034</cell></row><row><cell>CRaWl (Toenshoff et al., 2021)</cell><cell>69.013?0.259</cell><cell>-</cell><cell>97.944?0.050</cell><cell>-</cell><cell>-</cell></row><row><cell>GIN-AK+ (Zhao et al., 2022b)</cell><cell>72.19?0.13</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>86.850?0.057</cell></row><row><cell>SAN (Kreuzer et al., 2021)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>76.691?0.65</cell><cell>86.581?0.037</cell></row><row><cell>K-Subgraph SAT (Chen et al., 2022a)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>77.856?0.104</cell><cell>86.848?0.037</cell></row><row><cell>EGT (Hussain et al., 2021)</cell><cell>68.702?0.409</cell><cell></cell><cell>98.173?0.087</cell><cell>79.232?0.348</cell><cell>86.821?0.020</cell></row><row><cell>GraphGPS (Ramp?sek et al., 2022)</cell><cell>72.298?0.356</cell><cell>93.50?0.41</cell><cell>98.051?0.126</cell><cell>78.016?0.180</cell><cell>86.685?0.059</cell></row><row><cell>EXPHORMER (ours)</cell><cell>74.754?0.194</cell><cell>94.02 ? 0.209</cell><cell>98.414 ? 0.038</cell><cell>78.22 ? 0.045</cell><cell>86.734?0.008</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>expander graph attention and global attention components of EXPHORMER ensure that a sublinear number of graph transformer layers is sufficient to allow each node to interact (directly or indirectly) with every other node, alleviating potential underreaching issues (even in the absence of global nodes, O(log n) layers are sufficient, by Corollary 4.4). Nevertheless, a natural question is whether a sparse transformer model based on EXPHORMER allows universal approximation of functions.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of attention mechanisms in GPS. EXPHORMER outperforms other sparse transformer architectures (BigBird and Performer) while also beating the full transformer GPS models on three of four datasets. Best results are colored in first, second, third.</figDesc><table><row><cell>Model/Dataset</cell><cell>Cifar10</cell><cell>MalNet-Tiny</cell><cell>PascalVOC-SP</cell><cell>Peptides-Func</cell></row><row><cell></cell><cell>Accuracy ?</cell><cell>Accuracy ?</cell><cell>F1 score ?</cell><cell>AP ?</cell></row><row><cell>GPS (MPNN-only)</cell><cell>69.948 ? 0.499</cell><cell>92.23 ? 0.65</cell><cell>0.3016 ? 0.0031</cell><cell>0.6159 ? 0.0048</cell></row><row><cell>GPS-BigBird</cell><cell>70.480 ? 0.106</cell><cell>92.34 ? 0.34</cell><cell>0.2762 ? 0.0069</cell><cell>0.5854 ? 0.0079</cell></row><row><cell>GPS-Performer</cell><cell>70.670 ? 0.338</cell><cell>92.64 ? 0.78</cell><cell>0.3724 ? 0.0131</cell><cell>0.6475 ? 0.0056</cell></row><row><cell>GPS-Transformer</cell><cell>72.305 ? 0.344</cell><cell>93.50 ? 0.41</cell><cell>0.3736 ? 0.0158</cell><cell>0.6535 ? 0.0041</cell></row><row><cell>EXPHORMER</cell><cell>74.754 ? 0.194</cell><cell>94.02 ? 0.21</cell><cell>0.3966 ? 0.0027</cell><cell>0.6527 ? 0.0043</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison of EXPHORMER with baselines from the Long-Range Graph Benchmarks (LRGB, Dwivedi et al., 2022). Best results are colored in first, second, third. ? 0.0219 0.2641 ? 0.0045 0.5864 ? 0.0077 0.3420 ? 0.0013 0.3218 ? 0.0011 GatedGCN+RWSE 0.2860 ? 0.0085 0.2574 ? 0.0034 0.6069 ? 0.0035 0.3357 ? 0.0006 0.3242 ? 0.0008 Transformer+LapPE 0.2694 ? 0.0098 0.2618 ? 0.0031 0.6326 ? 0.0126 0.2529 ? 0.0016 0.3174 ? 0.0020 SAN+LapPE 0.3230 ? 0.0039 0.2592 ? 0.0158* 0.6384 ? 0.0121 0.2683 ? 0.0043 0.3350 ? 0.0003 SAN+RWSE 0.3216 ? 0.0027 0.2434 ? 0.0156* 0.6439 ? 0.0075 0.2545 ? 0.0012 0.3341 ? 0.0006 GraphGPS 0.3748 ? 0.0109 0.3412 ? 0.0044 0.6535 ? 0.0041 0.2500 ? 0.0005 0.3337 ? 0.0006</figDesc><table><row><cell>Model</cell><cell>PascalVOC-SP F1 score ?</cell><cell>COCO-SP F1 score ?</cell><cell>Peptides-Func Peptides-Struct PCQM-Contact AP ? MAE ? MRR ?</cell></row><row><cell>GCN</cell><cell cols="3">0.1268 ? 0.0060 0.0841 ? 0.0010 0.5930 ? 0.0023 0.3496 ? 0.0013 0.3234 ? 0.0006</cell></row><row><cell>GINE</cell><cell cols="3">0.1265 ? 0.0076 0.1339 ? 0.0044 0.5498 ? 0.0079 0.3547 ? 0.0045 0.3180 ? 0.0027</cell></row><row><cell cols="4">GatedGCN 0.2873 Exphormer (ours) 0.3966 ? 0.0027 0.3430 ? 0.0008 0.6527 ? 0.0043 0.2481 ? 0.0007 0.3637 ? 0.0020</cell></row></table><note><p>datasets and is competitive with the best single model accuracies on the remaining datasets.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Accuracy of models with different attention mechanisms on transductive graph datasets (numbers in top rows, other than arXiv, are fromChen et al., 2022b). Chen et al. did not report NAGphormer results on this dataset.</figDesc><table><row><cell>Model</cell><cell>ogbn-arxiv</cell><cell>Computer</cell><cell>Photo</cell><cell>CS</cell><cell>Physics</cell></row><row><cell>SAN</cell><cell>OOM</cell><cell cols="2">89.83 ? 0.16 94.86 ? 0.10</cell><cell>94.51 ? 0.15</cell><cell>OOM</cell></row><row><cell>GraphGPS</cell><cell>OOM</cell><cell>OOM</cell><cell>95.06 ? 0.13</cell><cell>93.93 ? 0.12</cell><cell>OOM</cell></row><row><cell>NAGphormer</cell><cell>NA</cell><cell cols="2">91.22 ? 0.14 95.49 ? 0.11</cell><cell>95.75 ? 0.09</cell><cell>97.34 ? 0.03</cell></row><row><cell cols="2">EXPHORMER 72.44 ? 0.28</cell><cell>91.47 ?0.17</cell><cell>95.35 ?0.22</cell><cell cols="2">94.93 ? 0.0046 96.89 ? 0.09</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Results for the full model versus removing each of the components. For the Peptides-Struct dataset, lower is better; for all the others, higher is better. For MalNet-Tiny, here we used one virtual node, but the best result in Table1 used 4 virtual nodes.</figDesc><table><row><cell>Dataset</cell><cell cols="3">No Local Edges No Expander Edges No Global Nodes</cell><cell>All Components</cell></row><row><cell>Cifar10</cell><cell>74.15 ? 0.143</cell><cell>74.53 ? 0.189</cell><cell>74.57 ? 0.183</cell><cell>74.75 ? 0.194</cell></row><row><cell>Malnet-Tiny</cell><cell>92.32 ? 0.12</cell><cell>93.34 ? 0.224</cell><cell>92.26 ? 0.415</cell><cell>93.16 ? 0.137</cell></row><row><cell>Pattern</cell><cell cols="2">86.632 ? 0.0255 86.702 ? 0.0111</cell><cell>86.452 ? 0.0182</cell><cell>86.728 ? 0.010</cell></row><row><cell cols="2">PascalVOC-SP 0.3719 ? 0.003</cell><cell>0.3606 ? 0.00335</cell><cell cols="2">0.3966 ? 0.00272 0.3788 ? 0.0039</cell></row><row><cell cols="3">Peptides-Struct 0.2631 ? 0.0007 0.2481 ? 0.0007</cell><cell>0.2655 ? 0.0003</cell><cell>0.2643 ? 0.0008</cell></row><row><cell>Computer</cell><cell>88.73 ? 0.84</cell><cell>90.02 ? 0.62</cell><cell>91.47 ? 0.17</cell><cell>90.06 ? 0.58</cell></row></table><note><p><p><p>to 35K nodes and 250K edges. We also show the use of EXPHORMER on ogbn-arxiv, which has 169K nodes and 1.1M edges. The results are shown in Table</p>4</p>. Standard GraphGPS models suffer from Out Of Memory (OOM)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Dataset statistics you can see ESLapPE version of GraphGPS results in Tables 10, 11, 13 and 14. Except for the large scale graphs, which we use GCN, we have used CustomGatedGCN beside the Exphormer in all of the experiments.</figDesc><table><row><cell>Dataset</cell><cell cols="4">Graphs Avg. nodes Avg. edges Prediction Level</cell><cell>No. Classes</cell><cell>Metric</cell></row><row><cell>MNIST</cell><cell>70,000</cell><cell>70.6</cell><cell>564.5</cell><cell>graph</cell><cell>10</cell><cell>Accuracy</cell></row><row><cell>CIFAR10</cell><cell>60,000</cell><cell>117.6</cell><cell>941.1</cell><cell>graph</cell><cell>10</cell><cell>Accuracy</cell></row><row><cell>PATTERN</cell><cell>14,000</cell><cell>118.9</cell><cell>3,039.3</cell><cell>inductive node</cell><cell>2</cell><cell>Accuracy</cell></row><row><cell>CLUSTER</cell><cell>12,000</cell><cell>117.2</cell><cell>2,150.9</cell><cell>inductive node</cell><cell>6</cell><cell>Accuracy</cell></row><row><cell>MalNet-Tiny</cell><cell>5,000</cell><cell>1,410.3</cell><cell>2,859.9</cell><cell>graph</cell><cell>5</cell><cell>Accuracy</cell></row><row><cell>PascalVOC-SP</cell><cell>11,355</cell><cell>479.4</cell><cell>2,710.5</cell><cell>inductive node</cell><cell>21</cell><cell>F1</cell></row><row><cell>COCO-SP</cell><cell>123,286</cell><cell>476.9</cell><cell>2,693.7</cell><cell>inductive node</cell><cell>81</cell><cell>F1</cell></row><row><cell>PCQM-Contact</cell><cell>529,434</cell><cell>30.1</cell><cell>61.0</cell><cell>inductive link</cell><cell>(link ranking)</cell><cell>MRR</cell></row><row><cell>Peptides-func</cell><cell>15,535</cell><cell>150.9</cell><cell>307.3</cell><cell>graph</cell><cell>10</cell><cell>Average Precision</cell></row><row><cell>Peptides-struct</cell><cell>15,535</cell><cell>150.9</cell><cell>307.3</cell><cell>graph</cell><cell cols="2">11 (regression) Mean Absolute Error</cell></row><row><cell>ogbn-arxiv</cell><cell>1</cell><cell>169,343</cell><cell>1,166,243</cell><cell>node</cell><cell>40</cell><cell>Accuracy</cell></row><row><cell>Amazon Computer</cell><cell>1</cell><cell>13381</cell><cell>245778</cell><cell>node</cell><cell>10</cell><cell>Accuracy</cell></row><row><cell>Amazon Photo</cell><cell>2</cell><cell>7487</cell><cell>119043</cell><cell>node</cell><cell>8</cell><cell>Accuracy</cell></row><row><cell>Coauthor CS</cell><cell>1</cell><cell>18333</cell><cell>81894</cell><cell>node</cell><cell>15</cell><cell>Accuracy</cell></row><row><cell>Coauthor Physics</cell><cell>1</cell><cell>34493</cell><cell>247962</cell><cell>node</cell><cell>5</cell><cell>Accuracy</cell></row><row><cell>cases we replaced</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Hyperparameters used for EXPHORMER for datasets: CIFAR10, MNIST, MalNet-Tiny, PATTERN, CLUSTER.</figDesc><table><row><cell>Hyperparameter</cell><cell cols="2">CIFAR10 MNIST</cell><cell cols="3">MalNet-Tiny PATTERN CLUSTER</cell></row><row><cell>Num Layers</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>4</cell><cell>20</cell></row><row><cell>Hidden Dim</cell><cell>40</cell><cell>40</cell><cell>64</cell><cell>40</cell><cell>32</cell></row><row><cell>Num Heads</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>8</cell></row><row><cell>Dropout</cell><cell>0.1</cell><cell>0.1</cell><cell>0</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>PE</cell><cell cols="3">ESLapPE ESLapPE None</cell><cell>ESLapPE</cell><cell>ESLapPE</cell></row><row><cell>Batch Size</cell><cell>16</cell><cell>16</cell><cell>16</cell><cell>32</cell><cell>16</cell></row><row><cell>Learning Rate</cell><cell>0.001</cell><cell>0.001</cell><cell>0.0005</cell><cell>0.0002</cell><cell>0.0002</cell></row><row><cell>Num Epochs</cell><cell>150</cell><cell>150</cell><cell>150</cell><cell>120</cell><cell>150</cell></row><row><cell>Expander Degree</cell><cell>5</cell><cell>5</cell><cell>-</cell><cell>7</cell><cell>3</cell></row><row><cell cols="2">Num Virtual Nodes 1</cell><cell>1</cell><cell>4</cell><cell>4</cell><cell>3</cell></row><row><cell>Num parameters</cell><cell>111,095</cell><cell>111,015</cell><cell>286,277</cell><cell>91,045</cell><cell>282,970</cell></row><row><cell cols="2">B.2. Full Comparison of Attention Mechanisms</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Remark B.1. EXPHORMER has some conceptual similarities with BigBird, as mentioned previously. For instance, we also</cell></row><row><cell cols="4">make use of virtual global attention nodes, corresponding to BIGBIRD-ETC.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 .</head><label>10</label><figDesc>Results with varying attention and MPNNs on CIFAR10. EXPHORMER with MPNN provides the highest accuracy. Also, pure transformer models based on EXPHORMER (without the use of MPNNs) are comparable.</figDesc><table><row><cell>Model</cell><cell cols="5">#Layers Hidden #Positional Expander #Parameters</cell><cell>Time</cell><cell>Accuracy</cell></row><row><cell></cell><cell></cell><cell>layers</cell><cell>encoding</cell><cell>degree</cell><cell></cell><cell>Epch/Total</cell><cell></cell></row><row><cell>GPS-MPNN: GatedGCN</cell><cell>3</cell><cell>52</cell><cell>LapPE</cell><cell>-</cell><cell>79,654</cell><cell>43s/1.18h</cell><cell>69.95 ? 0.499</cell></row><row><cell>GPS: Transformer</cell><cell>3</cell><cell>52</cell><cell>LapPE</cell><cell>-</cell><cell>70,762</cell><cell>40s/1.11h</cell><cell>68.86 ? 1.138</cell></row><row><cell>GPS: Transformer + MPNN</cell><cell>5</cell><cell>40</cell><cell>ESLapPE</cell><cell>-</cell><cell>111,735</cell><cell>104s/2.89h</cell><cell>73.53 ? 0.238</cell></row><row><cell>GPS: Transformer + MPNN</cell><cell>3</cell><cell>52</cell><cell>LapPE</cell><cell>-</cell><cell>112,726</cell><cell>62s/1.72h</cell><cell>72.31 ? 0.344</cell></row><row><cell>GPS: Performer + MPNN</cell><cell>5</cell><cell>40</cell><cell>ESLapPE</cell><cell>-</cell><cell>283,935</cell><cell>132s/3.65h</cell><cell>70.18 ? 0.095</cell></row><row><cell>GPS: Performer + MPNN</cell><cell>3</cell><cell>52</cell><cell>LapPE</cell><cell>-</cell><cell>239,554</cell><cell>77s/2.14h</cell><cell>70.67 ? 0.338</cell></row><row><cell>GPS: BigBird + MPNN</cell><cell>5</cell><cell>40</cell><cell>ESLapPE</cell><cell>-</cell><cell>128,335</cell><cell>243s/6.75h</cell><cell>70.51 ? 0.256</cell></row><row><cell>GPS: BigBird + MPNN</cell><cell>3</cell><cell>52</cell><cell>LapPE</cell><cell>-</cell><cell>129,418</cell><cell>145s/4h</cell><cell>70.48 ? 0.106</cell></row><row><cell>EXPHORMER w/o MPNN</cell><cell>5</cell><cell>44</cell><cell>ESLapPE</cell><cell>5</cell><cell>84,134</cell><cell>64s/1.78h</cell><cell>72.33 ? 0.155</cell></row><row><cell>EXPHORMER w/o MPNN</cell><cell>7</cell><cell>44</cell><cell>ESLapPE</cell><cell>5</cell><cell>119,022</cell><cell>80s/2.23h</cell><cell>72.88 ? 0.166</cell></row><row><cell>EXPHORMER</cell><cell>5</cell><cell>40</cell><cell>ESLapPE</cell><cell>5</cell><cell>111,095</cell><cell>115s/3.21h</cell><cell>74.75 ? 0.194</cell></row><row><cell>EXPHORMER</cell><cell>5</cell><cell>44</cell><cell>ESLapPE</cell><cell>5</cell><cell>133,819</cell><cell>114s/3.19h</cell><cell>75.03 ? 0.186</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11 .</head><label>11</label><figDesc>Ablation studies results for MNIST</figDesc><table><row><cell>Model</cell><cell cols="5">#Layers Hidden #Positional Expander #Parameters</cell><cell>Time</cell><cell>Accuracy</cell></row><row><cell></cell><cell></cell><cell>layers</cell><cell>encoding</cell><cell>degree</cell><cell></cell><cell>Epoch/Total</cell><cell></cell></row><row><cell>GPS: Transformer + MPNN</cell><cell>5</cell><cell>40</cell><cell>ESLapPE</cell><cell>-</cell><cell>111,655</cell><cell>131s/5.45h</cell><cell>98.336 ? 0.0189</cell></row><row><cell>GPS: Transformer + MPNN</cell><cell>3</cell><cell>52</cell><cell>LapPE</cell><cell>-</cell><cell>115,394</cell><cell>76s/2.13h</cell><cell>98.051 ? 0.126</cell></row><row><cell>GPS: Performer + MPNN</cell><cell>5</cell><cell>40</cell><cell>ESLapPE</cell><cell>-</cell><cell>283,855</cell><cell>156s/6.52h</cell><cell>98.34 ? 0.0349</cell></row><row><cell>GPS: BigBird + MPNN</cell><cell>5</cell><cell>40</cell><cell>ESLapPE</cell><cell>-</cell><cell>128,255</cell><cell cols="2">267s/11.11h 98.176 ? 0.0146</cell></row><row><cell>EXPHORMER w/o MPNN</cell><cell>5</cell><cell>44</cell><cell>ESLapPE</cell><cell>5</cell><cell>92,146</cell><cell>75s/3.14h</cell><cell>98.08 ? 0.051</cell></row><row><cell>EXPHORMER w/o MPNN</cell><cell>7</cell><cell>44</cell><cell>ESLapPE</cell><cell>5</cell><cell>127,698</cell><cell>93s/3.87h</cell><cell>98.238 ? 0.0387</cell></row><row><cell>EXPHORMER</cell><cell>5</cell><cell>40</cell><cell>ESLapPE</cell><cell>5</cell><cell>111,015</cell><cell>132s/5.49h</cell><cell>98.414 ? 0.035</cell></row><row><cell>EXPHORMER</cell><cell>5</cell><cell>44</cell><cell>ESLapPE</cell><cell>5</cell><cell>133,731</cell><cell>137s/5.72h</cell><cell>98.424 ? 0.018</cell></row><row><cell>spectral expansion properties.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 15 .</head><label>15</label><figDesc>Comparison of attention mechanisms on the ogbn-arxiv dataset by fixing the network size.</figDesc><table><row><cell>Model</cell><cell>Accuracy</cell><cell cols="2">Epoch times #Parameters</cell></row><row><cell cols="2">GCN+EXPHORMER with expander edges 72.44 ? 0.28</cell><cell>1.72</cell><cell>268,552</cell></row><row><cell>GCN+EXPHORMER with virtual nodes</cell><cell>72.22 ? 0.13</cell><cell>1.76</cell><cell>269,704</cell></row><row><cell>GCN only</cell><cell>71.35 ? 0.31</cell><cell>0.21</cell><cell>74,152</cell></row><row><cell>GCN+BigBird</cell><cell>71.16 ? 0.19</cell><cell>17.85</cell><cell>325,480</cell></row><row><cell>GCN+Performer</cell><cell>70.92 ? 0.04</cell><cell>5.77</cell><cell>452,776</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p><ref type="bibr" target="#b11">Deac et al. (2022)</ref>, in work concurrent to and independent of ours, propose an expander-based graph learning mechanism for message-passing networks, alternating between layers based on the input graph with ones based on an auxiliary expander graph. This scheme is rather different from ours; we do not compare further.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported in part by the <rs type="funder">Natural Sciences and Engineering Resource Council of Canada</rs>, the <rs type="funder">Canada CIFAR AI Chairs program</rs>, the <rs type="institution">BC DRI Group</rs>, <rs type="person">Compute Ontario</rs>, <rs type="person">Calcul Qu?bec</rs>, and the <rs type="funder">Digital Resource Alliance of Canada</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>sequence and uses windows within that sequence.</p><p>Both BigBird and EXPHORMER also make use of a random attention model. While BigBird uses an Erd?s-R?nyi graph on |V | nodes, our approach is to use a d-regular expander for fixed constant d. The astute reader may recall that a Erd?s-R?nyi graph G(n, p) has spectral expansion properties for large enough p. However, it is known that p = log n n is the connectivity threshold, i.e., for p &lt; (1 -) log n n , G(n, p) is almost surely a disconnected graph. Therefore, in order to obtain even a connected graph in the Erd?s-R?nyi model -let alone one with expansion properties -one would need p = ? log n n , giving superlinear complexity for the number of edges. BigBird uses p = ?(1/n), keeping a linear number of edges but losing expansion properties. Our expander graphs, by contrast, allow both a linear number of edges and guaranteed spectral expansion properties.</p><p>We will see in the practical experiments of Section 5 that EXPHORMER-based models often substantially outperform BigBird-based equivalents, with fewer parameters.</p><p>In Section 5.1, we presented two approaches for the comparison of models trained using different attention mechanisms -fixing the hyperparameters and fixing a budget on the total number of trainable parameters. The results showed the advantage of EXPHORMER over other attention mechanisms for CIFAR10 (Table <ref type="table">10</ref>) and PATTERN (Table <ref type="table">13</ref>). Here, we present similar results for the remaining datasets -MNIST in Table <ref type="table">11</ref>; MalNet-Tiny in Table <ref type="table">12</ref>; PATTERN in Table <ref type="table">13</ref>; and CLUSTER in Table <ref type="table">14</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Details of Expander Graph Construction</head><p>A major component of EXPHORMER is the use of the edges of an expander graph. Thus far, we have not specified which expander graph to use. In this section, we provide details of the specific expander graphs we use as well and quantify their Table <ref type="table">12</ref>. Ablation studies results for MalNet-Tiny. We want to remark that these results are based on one virtual node. For the best result reported in the main paper, we have used 4 virtual nodes, which led to much better numbers. The model marked with * did not fit in memory with batch size 16, and was trained with batch size 8. Theorem C.2 <ref type="bibr" target="#b19">(Friedman 2003)</ref>. Fix &gt; 0 and an even integer d &gt; 0. Then, suppose G is a random graph generated by taking d independent uniformly random permutations ? 1 , ? 2 , . . . , ? d on V = {1, 2, . . . , n}, then choosing the edges as</p><p>Then, with probability</p><p>In our experiments, we use a random regular graph to instantiate the expander graph component of EXPHORMER. We describe the details below.</p><p>Generating a Random Regular Expander Let G = (V, E) be the original graph, where V = {1, 2, . . . , n}. Inspired by the expansion properties of the random graph process analyzed in <ref type="bibr" target="#b19">Friedman (2003)</ref> (see Theorem C.2), we generate a random regular graph G = (V, E ) on the same node set V as follows.</p><p>? Let s = (1, 1, . . . , 1, 2, 2, . . . , 2, . . . , n, n, . . . , n), where each value appears d times.</p><p>? Pick a random permutation ? on {1, 2, . . . , nd}, chosen uniformly at random from (nd)! possible permutations.</p><p>? Let E be the multiset {(s i , s ?(i) ) : 1 ? i ? nd}. The same work shows an extension to all (not necessarily permutation equivariant) sequence-to-sequence functions that are defined on a compact domain, say, [0, 1] d?n provided that one uses a positional encoding. More specifically, for any transformer g : R d?n ? R d?n , one can define a transformer with positional encoding g p : R d?n ? R d?n such that g p (X) = g(X + E). The following results shows that trainable positional encodings allow a transformer to approximate any sequence-to-sequence continuous function on the compact domain.</p><p>Theorem E.2 <ref type="bibr">(Yun et al., 2020a)</ref>. Let 1 ? p &lt; ? and &gt; 0. For any continuous function f : [0, 1] d?n ? R d?n that is permutation equivariant, there exists a transformer with positional encoding, g P , such that p (f, g) &lt; .</p><p>Note that the above theorems hold for full (dense) transformers. However, under certain conditions about the sparsity pattern, one can obtain similar universality for sparse attention mechanisms <ref type="bibr">(Yun et al., 2020b)</ref>.</p><p>One important consideration is that the aforementioned results hold for functions on sequences. Since we are concerned with functions on graphs, it is interesting to ask what the implications are for graph transformers.</p><p>We follow the approach of <ref type="bibr" target="#b25">Kreuzer et al. (2021)</ref>: Given a graph G, we can view a node transformer as a function from R n?n ? R n?n which uses the padded adjacency matrix of G as a positional encoding. Similarly, an edge transformer takes as input a sequence ((i, j), ? i,j ) with i, j ? [n] and i ? j such that ? i,j = 1 if i and j are connected in G or ? i,j = 0 otherwise. Any ordering of these vectors corresponds to the same graph. Moreover, we can capture the relevant functions going from R N (N -1)/2?2 ? R N (N -1)/2?2 with permutation equivariance. Ideally, they can be approximated as closely as desired by suitable transformers on the edge input. Now, simply observe (see <ref type="bibr" target="#b25">Kreuzer et al. 2021</ref>) that one can choose a function (in either the node transformer or edge transformer case) that is (a.) invariant under node permutations and (b.) maps non-isomorphic graphs to distinct values. We would like to apply one of the above thoerems to such a function.</p><p>However, we cannot quite apply Theorem E.1 or Theorem E.2, as it is specifically for full transformers in which all nodes are pairwise connected in the attention interaction graph.</p><p>Therefore, we provide the following theorem, which shows that a slight modification of EXPHORMER is able to provide universal approximation properties using just O(n) edges.</p><p>Theorem Then, it follows that a sparse transformer model, with positional encodings and an attention mechanism following H, can universally approximate continuous functions f : [0, 1] d?n ? R d?n . That is, for any 1 &lt; p &lt; ? and &gt; 0, there exists a sparse transformer network g, which uses the attention graph H and some positional encodings, such that p (f, g) &lt; .</p><p>Note that in the above theorem, only one of the enumerated conditions needs to be satisfied; in other words, one does not need to use both global attention and expander graph attention components (see Section 5.1). This is relevant, as for certain datasets, it may be desirable to use one of these attention components but not both (see Appendix D), but universal approximation properties do not depend on this choice.</p><p>Proof of Theorem E.3. Let H be the attention graph of a modified sparse EXPHORMER attention mechanism satisfying the conditions in Theorem E.3.</p><p>First, assume that condition (1) is satisfied. This implies that there is a subgraph of H which forms a star graph on the [n] graph nodes (plus, potentially, the virtual node). In this case, the existence of our desired g is directly implied by the following universal approximation result of <ref type="bibr" target="#b47">Zaheer et al. (2020)</ref>:</p><p>Theorem E.4 <ref type="bibr" target="#b47">(Zaheer et al., 2020)</ref>. Let 1 &lt; p &lt; ? and &gt; 0. For any graph H on [n] that contains the star graph, we have that if f ? [0, 1] n?d ? R n?d is a continuous function, then there exists a sparse transformer network g (with trainable positional encodings) such that p (f, g) &lt; .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Eigenvalues and expanders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="83" to="96" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Explicit expanders of every degree and size</title>
		<author>
			<persName><forename type="first">N</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="447" to="463" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the bottleneck of graph neural networks and its practical implications</title>
		<author>
			<persName><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Directional graph networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Passaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>L?tourneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="748" to="758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weisfeiler and lehman go cellular: Cw networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bodnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Otter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Montufar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improving graph neural network expressivity via subgraph isomorphism counting</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bouritsas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09252</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07553</idno>
		<title level="m">Residual gated graph convnets</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Structureaware transformer for graph representation learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>O'bray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03036</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Nagphormer: Neighborhood aggregation graph transformer for node classification in large graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno>CoRR, abs/2206.04910</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sarl?s</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Expander graph propagation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Deac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lackenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2210.02997" />
	</analytic>
	<monogr>
		<title level="m">Learning on Graphs 2022</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Convolutional neural networks on graphs with fast localized spectral filtering. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A generalization of transformer networks to graphs</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno>CoRR, abs/2012.09699</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Benchmarking graph neural networks</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno>CoRR, abs/2003.00982</idno>
		<ptr target="https://arxiv.org/abs/2003.00982" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07875</idno>
		<title level="m">Graph neural networks with learnable structural and positional representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Long range graph benchmark</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ramp?sek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parviz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<idno>CoRR, abs/2206.08164</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Hierarchical intermessage passing for learning on molecular graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-G</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Weichert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12179</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A largescale database for graph representation learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Yeung</surname></persName>
		</editor>
		<meeting>the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021</meeting>
		<imprint>
			<date type="published" when="2021-12">December 2021, virtual, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A proof of Alon&apos;s second eigenvalue conjecture</title>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th Annual ACM Symposium on Theory of Computing</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Larmore</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">X</forename><surname>Goemans</surname></persName>
		</editor>
		<meeting>the 35th Annual ACM Symposium on Theory of Computing<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">June 9-11, 2003. 2003</date>
			<biblScope unit="page" from="720" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020, virtual, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">OGB-LSC: A large-scale challenge for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>CoRR, abs/2103.09430</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Edgeaugmented graph transformers: Global self-attention is enough for graphs</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Subramanian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.03348</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Rethinking graph transformers with spectral attention</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kreuzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>L?tourneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tossou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03893</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ramanujan graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lubotzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sarnak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comb</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="261" to="277" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Inferring from references with differences for semi-supervised node classification on graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.3390/math10081262</idno>
		<ptr target="https://www.mdpi.com/2227-7390/10/8/1262" />
	</analytic>
	<monogr>
		<title level="j">Mathematics</title>
		<idno type="ISSN">2227-7390</idno>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Explicit group-theoretic constructions of combinatorial schemes and their applications in the construction of expanders and concentrators</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Margulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Problemy Peredachi Informatsii</title>
		<idno type="ISSN">0555- 2923</idno>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="60" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Encoding graph structure in transformers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mialon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Selosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><surname>Graphit</surname></persName>
		</author>
		<idno>CoRR, abs/2106.05667</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Relational pooling for graph representations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4663" to="4673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Query-driven active surveying for collective classification</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="http://linqs.cs.umd.edu/basilic/web/Publications/2012/namata:mlg12-wkshp/namata-mlg12.pdf" />
	</analytic>
	<monogr>
		<title level="m">Workshop on Mining and Learning with Graphs</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh ACM international conference on web search and data mining</title>
		<meeting>the eleventh ACM international conference on web search and data mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="459" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Recipe for a general, powerful, scalable graph transformer</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ramp?sek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<idno>CoRR, abs/2205.12454</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Random features strengthen graph neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 SIAM International Conference on Data Mining (SDM)</title>
		<meeting>the 2021 SIAM International Conference on Data Mining (SDM)</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="333" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno>CoRR, abs/1811.05868</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Efficient transformers: A survey</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<idno>CoRR, abs/2009.06732</idno>
		<ptr target="https://arxiv.org/abs/2009.06732" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Graph learning with 1d convolutions on random walks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Toenshoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08786</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Understanding over-squashing and bottlenecks on graphs via curvature</title>
		<author>
			<persName><forename type="first">J</forename><surname>Topping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Giovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event</title>
		<imprint>
			<date type="published" when="2022">April 25-29, 2022, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04">2017. December 4-9, 2017. 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Do transformers really perform bad for graph representation?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<idno>ArXiv, abs/2106.05234</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Are transformers universal approximators of sequence-to-sequence functions?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020. 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">O(n) connections are expressive enough: Universal approximability of sparse transformers</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note>December 6-12, 2020, virtual, 2020b</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Onta??n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<editor>Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Graph-bert: Only attention is needed for learning graph representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR, abs/2001.05140</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Gophormer: Ego-graph transformer for node classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<idno>CoRR, abs/2110.13094</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Learning on large-scale text-attributed graphs via variational inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2210.14709" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">From stars to subgraphs: Uplifting any GNN with local structure awareness</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
