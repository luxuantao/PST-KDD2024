<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge-Based Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Te</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Energy and Power Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Laboratory of Control and Simulation of Power System and Generation Equipment</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Energy and Power Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Key Laboratory for Thermal Science and Power Engineering of Ministry of Education</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenguang</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Energy and Power Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Laboratory of Control and Simulation of Power System and Generation Equipment</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dongxiang</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Energy and Power Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Laboratory of Control and Simulation of Power System and Generation Equipment</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Energy and Power Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Knowledge-Based Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E808E1DCAC2C5AC721CDEF49F1556C20</idno>
					<idno type="DOI">10.1016/j.knosys.2018.12.019</idno>
					<note type="submission">Received 6 June 2018 Received in revised form 11 December 2018 Accepted 13 December 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Adversarial training Generative adversarial network Deep convolutional neural network Intelligent fault diagnosis Rotating machinery</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, deep learning has become an emerging research orientation in the field of intelligent monitoring and fault diagnosis for industry equipment. Generally, the success of supervised deep models is largely attributed to a mass of typically labeled data, while it is often limited in real diagnosis tasks. In addition, the diagnostic model trained with data from limited conditions may generalize poorly for conditions not observed during training. To tackle these challenges, adversarial learning is introduced as a regularization into the convolutional neural network (CNN), and a novel deep adversarial convolutional neural network (DACNN) is accordingly proposed in this paper. By adding an additional discriminative classifier, an adversarial learning framework can be developed to train the convolutional blocks with the split data subsets, leading to a minimax two-player game. This process contributes to making the feature representation robust, boosting the generalization ability of the trained model as well as avoiding overfitting with a small size of labeled samples. The comparison studies with respect to conventional deep models on two fault datasets demonstrate the applicability and superiority of proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Industry 4.0 is essentially the integration of information technology and the next-generation manufacturing <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. This concept encourages information sharing and reuse to save costs and time in product realization and to support the intelligent industry process. For machines and equipment in operation, the development of information technology also makes them become typical cyberphysical systems (CPSs), where the condition monitoring and diagnosis system (CMDS) is more and more applied to optimize the operating and manufacturing process, facilitate the conditionbased maintenance and raise the outcome <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>.</p><p>To build a reliable CMDS, intensive research has been conducted over the past two decades, mainly in regard to two aspects: (i) the development of advanced signal processing algorithms to allow better feature extraction and selection <ref type="bibr" target="#b5">[6]</ref>, such as wavelet analysis <ref type="bibr" target="#b6">[7]</ref>, empirical mode decomposition (EMD) <ref type="bibr" target="#b7">[8]</ref>, stochastic resonance (SR) <ref type="bibr" target="#b8">[9]</ref>, maximum correlated kurtosis deconvolution (MCKD) <ref type="bibr" target="#b9">[10]</ref> and sparse coding (SC) <ref type="bibr" target="#b10">[11]</ref>, and (ii) application of pattern classification and recognition techniques to machine diagnosis (noted as intelligent diagnosis) <ref type="bibr" target="#b11">[12]</ref>, such as artificial neural network (ANN), support vector machine (SVM) and recent deep learning approaches <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>.</p><p>Deep learning has gained increasing popularity, leading to a series of works dealing either with the design of intelligent diagnostic models or with the problems of prognostics <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>. Through these studies, deep learning has been demonstrated to significantly outperform traditional shallow methods in terms of its self-adaptive feature learning capacity, multilayer nonlinear mapping ability as well as the potential to handle mechanical big data. Successful applications of several deep learning methods, such as the deep belief network (DBN) <ref type="bibr" target="#b20">[21]</ref>, stacked auto-encoder (SAE) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>, recurrent neural network (RNN) <ref type="bibr" target="#b23">[24]</ref>, long shortterm memory networks (LSTM) <ref type="bibr" target="#b24">[25]</ref> and, in particular, convolutional neural networks (CNN) <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref> are often reported. Shao et al. proposed an ensemble SAE algorithm using a series of different activation functions for unsupervised feature learning from massive vibration data <ref type="bibr" target="#b22">[23]</ref>. Verstraete et al. investigated different time-frequency methods to convert the vibration signal into timefrequency images, which were then fed into CNN for bearing condition diagnosis <ref type="bibr" target="#b25">[26]</ref>. Jing et al. utilized the 1D CNN to adaptively learn sensitive features from the frequency spectrum of raw data and achieved high diagnosis rates for combined gear-bearing-shaft faults in the gearbox <ref type="bibr" target="#b26">[27]</ref>. <ref type="bibr">Zhang et al.</ref> applied the 1D CNN with wide kernels in the first convolutional layer to capture the low frequency features and restrain the high frequency noise from the bearing fault signal <ref type="bibr" target="#b27">[28]</ref>. One of the key challenges in the community is that the trained model mostly works on the training data or the data that is similar to the training data. The reason is that the learned model is probably overfitted as some of the activated features do not represent the generalizable features but may be the local ones, or even noise.</p><p>To address the non-generalizability and overfitting issues of the deep learning structures, this work presents a novel adversarial learning strategy for deep models, which is inspired by the generative adversarial network (GAN). GAN was proposed by Goodfellow et al. in 2014 containing two models: a generative model and a discriminative model <ref type="bibr" target="#b32">[33]</ref>. The generative model tries to capture the distribution of raw data and generates the fake sample. The discriminative model needs to determine whether a sample is from the generative model or the raw dataset. By simultaneously training the two models, an adversarial process is introduced and drives both models to improve their performances <ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref>.</p><p>In the light of adversarial learning between the real data and the generated data, a specific deep adversarial convolutional neural network (DACNN) is proposed in this work. The whole training set is partitioned into two data subsets, which act as the two datasets with different distributions in GAN. Sequentially, the generative model evolves into the convolutional blocks of CNN, playing the role of feature descriptor. With an additional discriminative classifier, adversarial learning can be utilized to train the feature descriptor to learn the universal feature of two data subsets so that making the discriminator fool. This process contributes to making the feature representation robust, boosting the generalization ability of trained model as well as avoiding overfitting with a small size of labeled samples.</p><p>The main contributions of this work include: (1) The establishment of a novel adversarial learning framework in the deep model for intelligent fault diagnosis. An additional discriminator is appended to minimize the generalization gap by further adjusting the feature descriptor. <ref type="bibr" target="#b1">(2)</ref> The specific DACNN approach is easy to be implemented and deployed in a variety of applications, as standard backpropagation and stochastic gradient descent (SGD) algorithms can be utilized to train this network. (3) The effectiveness and superiority are validated through experiments on two datasets, which are from a wind turbine test rig and a generic gearbox for industrial applications.</p><p>The remaining parts are organized as follows. The background and preliminaries are presented in Section 2. The proposed DACNN is described in Section 3. Section 4 states the experimental details, results and discussions. Finally, the conclusions are drawn in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Convolutional neural network</head><p>A CNN generally consists of a multiple combination of one or more convolutional layers and pooling layers, and is followed by one or more fullyconnected layers. For completeness, a brief description of convolutional layers, pooling layers and fully-connected layers is provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolutional layer:</head><p>Each convolutional layer generally contains a series of filters or kernels. The convolutional operation involves the application of dot products between a receptive field in raw signal and a filter. By sliding the filter along the signal, the convolutional operation can go through the entire signal. The output of this process is referred as a feature map. The convolution process with nonlinear activation is described as:</p><formula xml:id="formula_0">c n r = ReLU( ∑ m v m r-1 * w n r + b n r ) (1)</formula><p>where c n r is the output of nth filter in convolutional layer r; v m r-1 is the mth output of previous layer r -1; * represents the convolution and w n r denotes the nth filter kernel of the current layer r; b n r is the bias; and ReLU denotes the nonlinear activation function, i.e., rectified linear unit (ReLU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pooling layer:</head><p>To reduce the spatial dimensions of feature map and avoid over-fitting, a pooling layer is often appended to the convolutional layer. Max pooling, which only takes the most important part (the highest value) of the input, is the most used type of pooling . The max pooling is described as:</p><formula xml:id="formula_1">h n r+1 = max (i-1)l+1≤t≤il c n r (t)<label>(2)</label></formula><p>where c n r is the feature map, h n r+1 is the outcome of pooling layer, and l is the length of a local region for pooling.</p><p>Fully-connected layer: With the features extracted by sequential convolution and pooling, the fully-connected layer, just like the layer in the multilayer perceptron (MLP), is finally applied for classification and recognition. A softmax regression is generally used on the top classification layer. The output of softmax function is defined as:</p><formula xml:id="formula_2">O j = ⎡ ⎢ ⎣ P(y = 1)|x; θ P(y = 2)|x; θ • • • P(y = k)|x; θ ⎤ ⎥ ⎦ = 1 ∑ k j=1 exp(θ j x) ⎡ ⎢ ⎣ exp(θ 1 x) exp(θ 1 x) • • • exp(θ k x) ⎤ ⎥ ⎦ (3)</formula><p>where k is the number of categories and θ j x is the parameters of the classification layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Adversarial training</head><p>The GAN contains two models, a generative model G and a discriminative model D (as shown in Fig. <ref type="figure" target="#fig_0">1</ref>). The G maps the latent random variable z with a prior p z (z) to the distribution p g , that is, G(z; θ g ), where θ g is the parameters of G. The D acts as a two class classifier that can be defined as D(x, θ d ). D(x) represents the probability that x is from real data rather than p g . Generally, the G and D are represented by a nonlinear mapping function, such as MLP and CNN. In the training process, the D tries to give a high probability for real data x and a low probability for samples from G. On the contrary, the G learns to capture the distribution of real data, and generates the fake sample to confuse D. In this way, the adversarial training is introduced to simultaneously improve the performance of G and D. The optimization is a minimax two-player game that can be formulated as:</p><formula xml:id="formula_3">min G max D V (D, G) = E x∼p data (x) [log D(x)] +E z∼pz (z) [log(1 -D(G(z)))] (4)</formula><p>The training strategy is to fix one model and update the parameters of the other one. The SGD algorithms can be utilized to update D and G. Given a batch of m real samples and m generated faked samples, the stochastic gradient of D is calculated as:</p><formula xml:id="formula_4">∇ θ d 1 m m ∑ i=1 [log D(x (i) ) + log(1 -D(G(Z i )))] (5)</formula><p>and the stochastic gradient of G is calculated as:</p><formula xml:id="formula_5">∇ θ d 1 m m ∑ i=1 log(1 -D(G(Z i ))) (6)</formula><p>Through alternate iterations, the minimax game holds the potential to achieve a global optimum at p g = p data . The detailed analysis of global optimality and convergence can be referred in <ref type="bibr" target="#b32">[33]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The proposed method</head><p>Generally, the diagnostic model is often trained with a fault dataset and then deployed in machinery for monitoring and diagnosis. The trained models always suffer from poor generalization and degraded performance on the data from unseen conditions during training. Besides, the typically labeled fault data are often scarce and expensive to be obtained in real industry tasks. Using the limited samples to train a model, especially the deep neural networks with millions of parameters, may easily lead to overfitting. In view of the above problems, a novel DACNN is developed in this part, the training strategy is stated and the general procedure of the diagnosis framework is finally presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Deep adversarial convolutional neural network</head><p>As introduced in GAN, the discriminator distinguishes two categories of data, i.e., the real data and the fake data. The adversarial process is accordingly induced, and finally, the generator captures the distribution of real data. Herein, we randomly divide the training set into two parts to form the two categories of data. Our goal is to exploit the two data subsets for adversarial learning and improving the generalization ability of diagnostic model. Similar to the generator in GAN, the feature descriptor, namely the front convolutional blocks in CNN, is treated as a competitor in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>It can be defined as the function</head><formula xml:id="formula_6">G f = G f (x; θ f ) : x → R D with parameters θ f , where the input data is mapped into D-dimensional feature space. An additional discriminative classifier (two class classifier) is designed as the opponent, namely, G d = G d (G f (x); θ d ),</formula><p>with parameters θ d . Both of the two data subsets are fed into feature descriptor. The output features are further distinguished by the discriminative classifier G d (real 1 for subset i and fake 0 for subset ii). The binary cross entropy (BCE) loss for the output is calculated as:</p><formula xml:id="formula_7">L(G d (G f (x i )), d i ) = d i log 1 G d (G f (x i )) + (1 -d i ) × log 1 1 -G d (G f (x i )) (7)</formula><p>where d i denotes the binary variable for x i . By competitively training the two parts with the adversarial strategy, the feature descriptor G f tends to capture the common distribution of the two categories of data and makes the discriminative classifier unable to differentiate 0 or 1. In this way, we deem that the deep model has been properly trained to generalize well between the two data subsets. Given n samples in subset i and Nn samples in subset ii, the optimization objective is described as:</p><formula xml:id="formula_8">E(θ f , θ d ) = -( 1 n n ∑ i=1 L i d (θ f , θ d ) + 1 n ′ N ∑ i=n+1 L i d (θ f , θ d )) (8)</formula><p>where</p><formula xml:id="formula_9">L i d (θ f , θ d ) = L d (G d (G f (x i ; θ f ); θ d ), d i ) and n ′ = N -n.</formula><p>Specifically, (8) includes a minimization problem with respect to θ f and a maximization problem with respect to θ d .</p><p>Besides, a standard supervised training for all the labeled data should be conducted so as to prevent deviations of the diagnostic target in the adversarial process. A classifier constructed by the subsequent fully-connected layers in CNN is defined as G y = G y (G f (x); θ y ) : R D → R L with parameters θ y , where L denotes the number of categories. With the help of softmax function, the cross entropy loss for a labeled sample (x i , y i ) is calculated as:</p><formula xml:id="formula_10">L y (G y (G f (x i )), y i ) = log 1 G y (G f (x i )) y i (9)</formula><p>Now, by adding the supervised constraint term to ( <ref type="formula">8</ref>), the optimization of DACNN can be reformulated as:</p><formula xml:id="formula_11">E(θ f , θ y , θ d ) = 1 n n ∑ i=1 L i y (θ f , θ y ) -λ( 1 n n ∑ i=1 L i d (θ f , θ d ) + 1 n ′ N ∑ i=n+1 L i d (θ f , θ d )) (10)</formula><p>where</p><formula xml:id="formula_12">L i y (θ f , θ y ) = L y (G y (G f (x i ; θ f ); θ y ), y i )</formula><p>and λ is a non-negative hyper-parameter. The entire learning process of DACNN is to seek the parameters θf , θy , θd so that ( θf , θy ) = arg max</p><formula xml:id="formula_13">θ f ,θy E(θ f , θ y , θd ) (11)</formula><p>and θd = arg min</p><formula xml:id="formula_14">θ d E( θf , θy , θ d ) (12)</formula><p>The illustration of proposed DACNN is presented in Fig. <ref type="figure" target="#fig_1">2</ref>. By solving the optimization problems in <ref type="bibr" target="#b10">(11)</ref> and <ref type="bibr" target="#b11">(12)</ref>, the DACNN attempts to learn a feature descriptor G f that can make a proper feature representation for input samples (either subset i or subset ii) allowing the classifier G y to accurately diagnose the category, but weakening the capacity of the discriminative classifier G d to distinguish whether a sample is from the subset i or subset ii.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training strategy</head><p>Unlike the classical deep models, DACNN cannot possess an explicit loss function for model learning since the updates need to be made in the opposite direction to the gradient in the adversarial process. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, the update in the discriminative classifier (stage 1) is to reduce the loss with the purpose of improving the discriminative ability. However, the update in feature descriptor (stage 2) maximizes the loss to fool the discriminator. To frame a flexible implement of the SGD algorithms in the training of DACNN, we use a circuitous way by rewriting the loss as</p><formula xml:id="formula_15">L ′i d (θ f , θ d ) = L d (G d (G f (x i ; θ f ); θ d ), 1-d i ) in stage 2. Maximizing the L i d (θ f , θ d ) can be accomplished by minimizing L ′i d (θ f , θ d ).</formula><p>During the backpropagation, the feature descriptor takes the gradient of the recalculated L ′i d (θ f , θ d ) from the discriminator, and updates its parameters with the SGD.</p><p>Overall, the update rules of parameters θ f , θ y , θ d can be formulated as: </p><formula xml:id="formula_16">θ f ← θ f -µ( ∂L i y ∂θ f + λ ∂L ′i d ∂θ f ) (13)</formula><formula xml:id="formula_17">θ d ← θ d -µ ∂L i d ∂θ d (14) θ y ← θ y -µ ∂L i y ∂θ y (<label>15</label></formula><formula xml:id="formula_18">)</formula><p>where µ is the learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Proposed diagnosis framework</head><p>To sum up, the novel DACNN, which merges the idea of adversarial training into the feature learning of deep model, is proposed in this work. The diagnosis framework on the basis of DACNN is summarized as the following three steps.</p><p>(1) Pre-training: Train a base CNN by using the labeled data.</p><p>Although the model may be easily overfitted with a small data size, this initialization of parameters is important for building the deep architecture. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Comparison studies</head><p>Compared with the shallow methods, the superiority of deep learning flow has been often reported in various studies <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref>. Also considering a fair comparison, this work mainly focuses on the performance of deep networks. All the used models follow an endto-end diagnosis procedure without manual feature extraction and selection. In CNN, the architectures and parameters setting generally have significant influences on model performance. The increase of model parameters, such as the depth of layers, the number and width of convolutional kernel, contributes to improving the model capacity, but also raises the risk of overfitting with only limited supervised training. To make a credible comparative study, we design three CNNs with different model architectures for reference, as illustrated in Fig. <ref type="figure" target="#fig_3">3</ref>. In general, the size of the first convolutional kernel is set to a large value (64 and 128) with the purpose of restraining the high frequency noise. The number of kernel increases with the depth of layer for achieving a hierarchical and abstract feature representation. The dropout layers are added to avoid overfitting. The final output layer gives the diagnostic category for each sample. Other details are stated as: the activation function is ReLU, the batch size is selected from 16 to 64, the optimizer is the SGD with a learning rate of 0.01 and the experiments are implemented in the PyTorch framework.</p><p>Moreover, another two types of supervised deep model, i.e., the MLPs-based deep neural networks (DNN) <ref type="bibr" target="#b19">[20]</ref> and the LSTM network <ref type="bibr" target="#b24">[25]</ref>, are also used as baseline methods for comparison. In MLPs, the numbers of nodes in hidden layers is 1000-500-200-100. In LSTM, the hidden size is 128 and the number of RNN layer is 2. For DACNN, we transfer the architecture and parameters of pretrained CNN to G f and G y . The additional discriminative classifier G d has a structure of 120-64-1, which represents two hidden layers with 120 and 64 neurons, respectively, and an output layer for predicting the binary variable (0 or 1). The other implementation details stay consistent with the settings in the three CNNs.</p><p>In each comparison experiment, 20 trials are conducted for the average results to avoid particularity and contingency. The diagnostic accuracy, precision and F1-score for the health machine condition and different types of fault are utilized as the evaluation metrics of model performance. For the precision and F1-score, since the diagnostic experiments are generally multilabel targets, we calculate the metrics for each label, and find their average weighted mean by support (the number of true instances for each label).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Case study on wind turbine fault dataset 4.1.1. Experiments description</head><p>The original data collected from our wind turbine experimental platform are used to build the first fault dataset. As shown in Fig. <ref type="figure" target="#fig_4">4</ref>, the experimental platform mainly includes a wind tunnel, a directdrive wind turbine test bench, an accumulator, and a signal acquisition system. The experiment creates ten machine conditions, i.e., health, front bearing pedestal loosening (FB), back bearing pedestal loosening (BB), rolling element fault of front bearing (RF), inner-ring fault of front bearing (IF), outer-ring fault of front bearing (OF), misalignment in horizontal direction (MH), misalignment in vertical direction (MV), variation in airfoil of blades (VB) and yaw fault (YF), respectively.</p><p>The wind tunnel, which can generate a wind source at a maximum speed of 15 m/s is utilized to drive the direct-drive wind turbine. The experiments are conducted under six different load conditions (i.e., with the wind speeds of 5.8 m/s, 6.9 m/s, 8 m/s, 9.2 m/s, 10.3 m/s and 11.5 m/s). The corresponding speed of the wind wheel ranges from 255 to 300 rpm. One point should be noted that the wind speed for each load fluctuated slightly over time, and the averaged values over a period of sampling time are listed here. The vibration data is measured by two accelerometers, which are installed on front and back bearing pedestal, respectively, with a sampling frequency of 20 kHz. Fig. <ref type="figure" target="#fig_5">5</ref> shows the raw waveforms collected by the front accelerometer for the ten machine conditions under load 6. The corresponding frequency spectra are displayed in Fig. <ref type="figure" target="#fig_6">6</ref>.</p><p>From the vibration waveforms, it is clear that no obvious impacts appear in terms of the health condition, whereas an acute impulse phenomenon is noticed under certain fault conditions, such as bearing-related faults. The features of faults occurred in the wind wheel and rotor, such as YF and MH, are more close to the ones of the health condition, due to the relatively long transmission path. In the frequency spectra, the high frequency resonance components appear at around 5000 Hz and 7000 Hz for most fault conditions. Although there are some distinctions in time and frequency domains between the signals of different categories, several conditions are still very similar, such as health and MH, RF and IF. The deep learning-based technique will be adopted for adaptively feature learning and fault diagnosis.</p><p>The signal segments from the two accelerometers are combined together as a sample (4096 points) for fusing the data. The six load conditions are partitioned into the seen conditions (the first five load conditions, load 1-5) and unseen condition (the last load condition, load 6). That said, the model will be trained with the data under load 1-5, but evaluated with the data under load 6. The load 6 data are unseen in the process of model learning, which contributes to evaluating the generalization ability of trained model. It should be noted that the data under load 1-5 are also splitted into a training set and a testing set for model training and verification, which means the testing samples for seen conditions are not contained in the aforementioned training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Results</head><p>In this section, we focus on the performance of the diverse methods on the wind turbine fault dataset. Tables <ref type="table" target="#tab_0">1</ref><ref type="table" target="#tab_1">2</ref><ref type="table" target="#tab_2">3</ref>give the results of the first set of experiments. For DNN, since the input dimension is too high (4096), the results are unsatisfactory, indicating that additional feature extraction may be necessary for this deep model. It can be seen that three classic CNNs achieve relatively high diagnosis accuracies when using a large amount of labeled data (20 000, 10 000), but present an obviously degraded trend of performance with the decrease of sample number (1500, 1000). Similar results can be found in LSTM. In practice, a good amount of labeled data is often necessary to train a deep model. It is also worth noting that, compared with CNN1 and CNN2, the CNN3 always retains a superior performance in both the seen and unseen conditions, suggesting the networks with deeper structure  and more parameters possess better capacity. Thereby, we use the trained CNN3 to initialize the G f and G y in DACNN. From the results of DACNN, we find the significant improvement in each experiment, especially under the conditions of limited labeled samples. For instance, in the case 6 experiment, DACNN makes an 11.7% improvement under seen conditions, and a 15.5% improvement under unseen condition, compared with the base accuracies of 84.3% and 71.8% in CNN3.</p><p>Taking the case 6 experiment as an example, the detailed receiver operating characteristic (ROC) curves are displayed in Figs. <ref type="figure">7</ref> and<ref type="figure">8</ref>. Note, the micro-averaging calculates metrics globally by considering each element of the label indicator matrix as a label, while the macro-averaging calculates metrics for each category and then average these per-category scores to compute the global means. It can be found the diagnostic results of several conditions, such as H, BB, MH and VB, achieve considerable improvement with DACNN framework for both seen and unseen conditions. For instance, the area under the curve (AUC) values of class BB for classic CNN3 and DACNN are 0.84 and 0.99 in Fig. <ref type="figure">8</ref>. DACNN makes a 0.15 improvement. Overall, the empirically achieved success by DACNN demonstrates the benefits of adversarial learning strategy in the deep feature learning flow.</p><p>To give an intuitional interpretation of models predictions, the t-distributed stochastic neighbor embedding (t-SNE) is applied to visualize the learned features in the flatten layer and the last hidden FC layer. Figs. 9 and 10 show the visualization results by CNN3 and DACNN in case 6 experiment. As shown in Fig. <ref type="figure">9</ref>, the mapped features of CNN3 do not cluster well in each category, and the samples of certain categories, such as VB and YF, largely overlapped in both flatten layer and the last hidden FC layer. The two types of   The three classic CNNs are trained with all the training samples. The DACNN is initialized with the architectures and parameters of CNN3. Then, the training set is partitioned into two parts to adversarially train the DACNN. b The testing samples are from seen and unseen conditions respectively. c The results are with testing samples on seen and unseen respectively.  faults that occurred in the wind wheel are more likely to present a similar signal characteristic in the vibration of drivetrain, which increases the hardness of recognition. In contrast, in the flattened layer of DACNN, the samples of the same machine condition are clearly gathered and even separable, indicating the good feature representation ability of the learned feature descriptor. After the nonlinear mapping in classifier, the features of different machine conditions are well separated in the last hidden FC layer, except the slightly overlapped MH and H, which is consistent with the high diagnostic accuracy in Table <ref type="table" target="#tab_0">1</ref>. Furthermore, from the visualization in Fig. <ref type="figure" target="#fig_8">10</ref>, we can observe a larger area of overlapping in classic CNN for the data from unseen conditions, and the learned feature by DACNN achieves a better separability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Case study on gearbox fault dataset 4.2.1. Experiments description</head><p>The gearbox fault dataset from the IEEE international conference on prognostics and health management (PHM) 2009 is analyzed as the second case study <ref type="bibr" target="#b36">[37]</ref>. The experiment is performed on a generic industrial gearbox, whose schematic and partial details of the gearbox are shown in Fig. <ref type="figure" target="#fig_9">11</ref>. The gearbox contains 3 shafts, 4 gears and 6 bearings. Two sets of gears, i.e. spur gears and helical gears, are tested. In this work, we utilize the vibration data of spur gear faults (eight different health conditions) to evaluate performance of algorithms. The descriptions of the detailed fault patterns are listed in Table <ref type="table" target="#tab_3">4</ref>. The used data is collected from output shaft end by an accelerometer with a sampling frequency of 66.67 kHz (as shown in Fig. <ref type="figure" target="#fig_10">12</ref>). Shaft speeds of <ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr">45</ref> and 50 Hz under high loading are considered. Similar to case study one, we select 30, 40 and 50 Hz as the seen conditions, while 35 and 45 Hz are used as the unseen conditions. Since the data length is limited in this dataset (around 530 000 points per fault type and working condition), we partition the signal into the segments with the length of 6144 points by a 2048 stride. And thus, 256 fault samples per fault type and working condition are achieved. In total, there are 6144 samples in the seen conditions and 4096 samples in the unseen conditions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Results</head><p>The DACNN is initialized by the architecture and parameters of CNN3. The results of the gearbox fault dataset are displayed in Tables 5-7 and Figs. <ref type="figure" target="#fig_3">13</ref><ref type="figure" target="#fig_11">14</ref><ref type="figure" target="#fig_12">15</ref><ref type="figure" target="#fig_13">16</ref>. Similar to the case study in wind turbine fault dataset, DACNN also presents the superior performance in comparison with the classic CNNs, as expressed through the diagnosis accuracies on seen and unseen conditions. Due to space limitations, we only list the results here. The detailed analysis and network visualization are parallel to the ones in wind turbine case study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Discussion</head><p>Deep learning technique-based intelligent methods have been widely used in fault diagnosis tasks. In most of existing works, however, both the training data and testing data are from the same frame. Thus, the generalization of deep learning methods for mechanical fault diagnosis is not investigated comprehensively. In <ref type="bibr" target="#b22">[23]</ref>, an ensemble deep auto-encoders method is proposed for intelligent fault diagnosis of rolling bearing using the dataset from Case Western Reserve University Lab <ref type="bibr" target="#b37">[38]</ref>. The data of twelve bearing conditions under 1797 rpm are used. In each bearing condition, the training set are randomly selected, and the remaining data are      condition. Although a superior performance and satisfactory diagnostic results can be achieved by the methods, these related works overlook the fact that the rotating machinery generally runs under varying working conditions, and the generalization of diagnostic model for diverse loads, speeds and even noise environments is significantly important for mechanical fault diagnosis tasks. Consequently, new insights into the comparison between the proposed DACNN and other deep methods in the unseen conditions are provided in this paper to investigate the generalization ability of each method. Moreover, the discriminative nature of deep neural networks makes training using a small amount of data for a large number of network parameters quite challenging. Experiments using different sizes of training samples are also conducted for DACNN and comparative methods. The empirically success achieved by DACNN verifies that using network in the presence of adversarial learning is less prone to overfitting with a small sample size and thus has better applicability in industry tasks.</p><formula xml:id="formula_19">G G G G G G G G G G G G 2 C G E G G G G G G G G G 3 G G E G G G G G G G G G 4 G G E Br B G G G G G G G 5 C G E Br In B O G G G G G 6 G G G Br In B O G G G Im G 7 G G G G In G G G G G G Ks 8 G G G G G B O G G G</formula><p>In recent years, the adversarial learning has gradually seeped into the field of transfer learning <ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref>. By setting up an adversarial game between the feature descriptor and discriminator using the data from diverse domains, the feature descriptor is driven to capture the common distribution across domains, and thus realizes the unsupervised domain adaptation. Essentially, the main motivation of this work comes from the adversarial transfer learning. The two data subsets divided from the training set are more likely to represent the different distributions, especially when the sample size is small. Herein, the adversarial learning acts as a cross domain regularizer, which discourages the model from being tailored to fit the quirks and random noise in the specific sample instead of reflecting the overall population. In this context, the adversarial learning is beneficial for learning universal and domaininvariant features, rather than the specific representations closely related to a limited training set, which explains the significantly improved diagnostic accuracy and generalization ability of the proposed DACNN in two case studies. The experiments on network visualizations also validate this statement. It is clear the feature distributions between diverse classes are separable for both seen and unseen conditions, indicating the features learned by DACNN are more likely to be domain-invariant. On the contrary, an obvious  Considering the challenge of real diagnostic applications, another straightforward extension of the adversarial learning framework is semi-supervised learning. As discussed above, a large volume of labeled data is often required to properly train a diagnostic model with millions of parameters. However, the acquisition of typically labeled fault data is labor-intensive and expensive. The explorations on semi-supervised method in deep learning flow, which can exploit a small amount of labeled data in conjunction with a large amount of unlabeled data and preserve an appreciable accuracy, is of great significance. In the proposed framework, the adversarial game can be introduced between labeled data and unlabeled data so that driving the trained model to generalize well from labeled domain to unlabeled domain. It is necessary and meaningful to apply the adversarial learning for semi-supervised methods in the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, a novel DACNN which integrates the adversarial learning framework into convolutional neural network, is proposed for intelligent fault diagnosis of mechanical systems. A classic CNN is first trained, and its architecture and parameters are </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The architecture of GAN.</figDesc><graphic coords="3,53.16,63.48,228.34,168.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The illustration of proposed DACNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 2 )</head><label>2</label><figDesc>Adversarial learning: Transfer the parameters of pre-trained model to G f and G y in DACNN. Divide the training set into two parts equally and train the DACNN with the adversarial strategy, as stated above. (3) Diagnostic model deployment: Extract the trained G f and G y in DACNN to construct the diagnostic model. Deploy the diagnostic model on equipment for real time monitoring and fault diagnosis.Before presenting case studies on the fault datasets, we note a few remarks on the proposed framework: (i) As opposed to the conventional deep learning approaches where specific objective functions guide the optimization of networks towards the goals, the adversarial learning learns entirely from data and frees the model of any constraints or limitations to optimization in the problem domain. (ii) The proposed framework is closely related to GAN, where two networks compete with each other in a minimax fashion. However, we directly partition the training set into two subsets for adversarial learning, and the generator evolves into the front feature descriptor of CNN. Generally, training a generator without upfront modeling faces more uncontrolled factors when processing high-dimensional data. This work only utilizes the adversarial training part and avoids generation to encourage convergence of the minimax game. (iii) The deep diagnostic model trained with limited target supervision may generalize poorly to the mechanical conditions not observed during training, such as diverse load conditions and diverse noise environments. Technically, as a new way of regularizing the deep model, the adversarial learning is capable of discouraging the diagnostic model from learning specific representations closely related to a limited training set. And thus, the DACNN shows the promise of learning universal and domain-invariant features without overfitting to the distributions of training data. (iv) The CNN is selected as the base network in this work, while the adversarial learning framework is not limited to a specific network structure, and this framework can be easily extended to other deep models, such as SAE and RNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The designed architecture of three classic CNNs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Schematic diagram of wind turbine experimental platform.</figDesc><graphic coords="6,130.53,55.32,324.60,189.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Collected vibration data of ten machine conditions under load 6.</figDesc><graphic coords="6,99.87,278.93,385.77,275.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The corresponding frequency spectra of ten machine conditions under load 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Fig. 7. ROC curves for CNN3 and DACNN under the seen conditions of no. 6 experiment.</figDesc><graphic coords="7,108.94,365.50,387.21,161.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Features visualization by CNN3 and DACNN. Data of unseen conditions in case 6 experiment is used.</figDesc><graphic coords="9,125.41,55.32,354.46,246.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Schematic and details of the gearbox in PHM 2009 Challenge Data.</figDesc><graphic coords="9,341.01,335.37,192.07,127.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Collected vibration data of eight gearbox conditions under 50 Hz.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. ROC curves for CNN3 and DACNN under the unseen conditions of case 11 experiment.</figDesc><graphic coords="11,108.94,352.10,387.21,161.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Features visualization by CNN3 and DACNN. Data of seen conditions in case 11 experiment is used.</figDesc><graphic coords="12,122.07,55.32,341.46,246.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Features visualization by CNN3 and DACNN. Data of unseen conditions in case 11 experiment is used. performance drop can be observed in unseen condition for the classic CNN, especially under the conditions of limited training samples.Considering the challenge of real diagnostic applications, another straightforward extension of the adversarial learning framework is semi-supervised learning. As discussed above, a large volume of labeled data is often required to properly train a diagnostic model with millions of parameters. However, the acquisition of typically labeled fault data is labor-intensive and expensive. The explorations on semi-supervised method in deep learning flow, which can exploit a small amount of labeled data in conjunction with a large amount of unlabeled data and preserve an appreciable</figDesc><graphic coords="12,122.07,336.13,341.46,246.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="8,115.60,444.71,354.46,246.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Performance comparison of accuracy metric between classic deep methods and proposed DACNN on wind turbine fault dataset.</figDesc><table><row><cell cols="5">Case No. training samples No.testing samples Accuracy (%)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>DNN</cell><cell>CNN1</cell><cell>CNN2</cell><cell>CNN3</cell><cell>LSTM</cell><cell>DACNN</cell></row><row><cell>1</cell><cell>20 000 a</cell><cell>2000/2000 b</cell><cell cols="4">82.9/53.1 c 99.1/92.7 98.4/91.5 99.5/93.9 97.1/87.2 99.8/97.1</cell></row><row><cell>2</cell><cell>10 000</cell><cell>2000/2000</cell><cell cols="4">66.1/43.1 98.0/90.1 97.2/88.7 98.2/91.6 96.2/85.7 99.7/96.3</cell></row><row><cell>3</cell><cell>4 000</cell><cell>2000/2000</cell><cell cols="4">37.2/23.3 95.9/85.4 94.8/84.0 96.4/89.7 92.7/78.6 99.1/92.2</cell></row><row><cell>4</cell><cell>2 000</cell><cell>2000/2000</cell><cell cols="4">31.3/19.0 93.5/81.7 90.2/77.1 94.4/86.9 87.6/70.3 98.0/91.9</cell></row><row><cell>5</cell><cell>1 500</cell><cell>2000/2000</cell><cell cols="4">26.7/18.8 91.4/79.0 89.7/76.3 93.2/83.9 85.1/64.5 97.6/88.8</cell></row><row><cell>6</cell><cell>1 000</cell><cell>2000/2000</cell><cell cols="4">24.1/18.5 86.0/72.3 77.1/64.9 84.3/71.8 76.6/54.4 96.0/87.3</cell></row><row><cell>a</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Performance comparison of precision metric between classic deep methods and proposed DACNN on wind turbine fault dataset.</figDesc><table><row><cell cols="5">Case No. training samples No.testing samples Precision (%)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>DNN</cell><cell>CNN1</cell><cell>CNN2</cell><cell>CNN3</cell><cell>LSTM</cell><cell>DACNN</cell></row><row><cell>1</cell><cell>20 000</cell><cell>2000/2000</cell><cell cols="4">82.9/52.8 99.1/93.3 98.5/92.1 99.6/94.5 97.1/87.5 99.8/97.7</cell></row><row><cell>2</cell><cell>10 000</cell><cell>2000/2000</cell><cell cols="4">67.8/50.8 97.6/90.3 97.1/88.5 99.1/93.3 95.0/85.5 99.6/96.6</cell></row><row><cell>3</cell><cell>4 000</cell><cell>2000/2000</cell><cell cols="4">41.1/27.2 95.5/83.6 94.4/83.0 96.6/90.0 90.8/78.7 98.9/93.9</cell></row><row><cell>4</cell><cell>2 000</cell><cell>2000/2000</cell><cell cols="4">39.8/29.4 93.5/80.0 90.2/76.6 93.8/86.5 87.7/70.5 98.0/92.7</cell></row><row><cell>5</cell><cell>1 500</cell><cell>2000/2000</cell><cell cols="4">33.9/27.4 91.5/79.0 89.3/76.2 93.2/84.0 85.4/66.2 97.3/90.7</cell></row><row><cell>6</cell><cell>1 000</cell><cell>2000/2000</cell><cell cols="4">31.8/26.1 86.1/70.2 76.4/61.7 83.1/71.9 77.2/55.8 96.0/87.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 Performance</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>DNN</cell><cell>CNN1</cell><cell>CNN2</cell><cell>CNN3</cell><cell>LSTM</cell><cell>DACNN</cell></row><row><cell>1</cell><cell>20 000</cell><cell>2000/2000</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>93</cell></row><row><cell>4</cell><cell>2 000</cell><cell>2000/2000</cell><cell cols="4">0.31/0.19 0.93/0.78 0.90/0.75 0.94/0.85 0.88/0.69 0.98/0.91</cell></row><row><cell>5</cell><cell>1 500</cell><cell>2000/2000</cell><cell cols="4">0.24/0.18 0.91/0.78 0.89/0.75 0.93/0.83 0.85/0.63 0.97/0.89</cell></row><row><cell>6</cell><cell>1 000</cell><cell>2000/2000</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>comparison of F1-score between classic deep methods and proposed DACNN on wind turbine fault dataset. Case No. training samples No.testing samples F1-score 0.83/0.53 0.99/0.92 0.98/0.90 1.00/0.93 0.97/0.86 1.00/0.98 2 10 000 2000/2000 0.67/0.43 0.97/0.89 0.97/0.87 0.99/0.92 0.95/0.84 1.00/0.96 3 4 000 2000/2000 0.37/0.22 0.95/0.82 0.94/0.82 0.97/0.89 0.91/0.77 0.99/0.0.22/0.17 0.86/0.69 0.76/0.61 0.83/0.71 0.77/0.53 0.96/0.86 Fig. 9. Features visualization by CNN3 and DACNN. Data of seen conditions in case 6 experiment is used and t-SNE is applied to obtain the 2 dimensional embedding from the high dimensional feature space.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Descriptions of detailed fault patterns.</figDesc><table><row><cell>Label</cell><cell>Gear</cell><cell></cell><cell></cell><cell></cell><cell>Bearing</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Shaft</cell></row><row><cell></cell><cell>32T</cell><cell>96T</cell><cell>48T</cell><cell>80T</cell><cell>IS:IS</cell><cell>ID:IS</cell><cell>OS:IS</cell><cell>IS:OS</cell><cell>ID:OS</cell><cell>OS:OS</cell><cell>Input</cell><cell>Output</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>Performance comparison of accuracy metric between classic deep methods and proposed DACNN on gearbox fault dataset.</figDesc><table><row><cell cols="4">Case. No. training samples No.testing samples Accuracy (%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>DNN</cell><cell>CNN2</cell><cell>CNN3</cell><cell>DACNN</cell></row><row><cell>7</cell><cell></cell><cell>1000/2000</cell><cell cols="2">43.6/39.3 98.6/89.3 94.9/84.1 98.6/81.7 97.1/66.2 99.4/92.5</cell></row><row><cell>8</cell><cell>3000</cell><cell>1000/2000</cell><cell cols="2">36.9/33.3 95.1/84.6 88.0/76.5 97.8/80.7 94.0/64.3 98.9/91.3</cell></row><row><cell>9</cell><cell>2000</cell><cell>1000/2000</cell><cell cols="2">30.7/29.6 88.3/77.9 78.2/69.3 93.2/76.2 92.8/61.9 98.6/89.5</cell></row><row><cell>10</cell><cell>1500</cell><cell>1000/2000</cell><cell cols="2">30.0/26.5 80.3/70.0 68.5/60.7 87.2/73.0 86.6/59.2 98.6/89.4</cell></row><row><cell>11</cell><cell>1000</cell><cell>1000/2000</cell><cell cols="2">28.2/26.3 65.8/59.1 59.3/54.3 78.1/67.3 85.9/55.3 97.6/88.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc>Performance comparison of precision metric between classic deep methods and proposed DACNN on gearbox fault dataset.</figDesc><table><row><cell cols="5">Case. No. training samples No.testing samples Precision (%)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>DNN</cell><cell>CNN1</cell><cell>CNN2</cell><cell>CNN3</cell><cell>LSTM</cell><cell>DACNN</cell></row><row><cell>7</cell><cell>5000</cell><cell>1000/2000</cell><cell cols="4">45.9/40.7 98.1/90.5 96.4/84.6 98.5/84.3 97.3/66.8 99.3/92.3</cell></row><row><cell>8</cell><cell>3000</cell><cell>1000/2000</cell><cell cols="4">40.7/32.4 95.5/86.2 86.1/76.3 98.5/83.9 94.7/67.1 99.0/92.3</cell></row><row><cell>9</cell><cell>2000</cell><cell>1000/2000</cell><cell cols="4">33.9/30.9 88.7/78.9 77.5/68.9 93.6/79.3 93.7/64.9 99.0/90.0</cell></row><row><cell>10</cell><cell>1500</cell><cell>1000/2000</cell><cell cols="4">32.9/28.3 80.3/70.7 67.8/61.0 88.5/75.4 85.5/62.8 98.3/89.4</cell></row><row><cell>11</cell><cell>1000</cell><cell>1000/2000</cell><cell cols="4">31.3/27.8 67.1/60.9 59.5/54.0 78.3/68.5 84.7/57.9 96.9/88.6</cell></row></table><note><p><p><p><p><p>used for testing. In</p><ref type="bibr" target="#b26">[27]</ref></p>, the 1D CNNs with various configurations are performed on PHM 2009 gearbox challenge data. The data under conditions of 30, 40 and 50 Hz are selected to form the dataset, which is further equally partitioned into a training set and a testing set. In</p><ref type="bibr" target="#b28">[29]</ref></p>, a hybrid convolutional neural network and hidden Markov models (CNN-HMMs) is presented for bearing fault identification. The twelve bearing conditions from aforementioned bearing fault dataset are also considered, and similarly, the training and testing samples are randomly chosen from the same bearing</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7</head><label>7</label><figDesc>Performance comparison of F1-score between classic deep methods and proposed DACNN on gearbox fault dataset.</figDesc><table><row><cell cols="4">Case. No. training samples No.testing samples F1-score</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>DNN</cell><cell>CNN1</cell><cell>CNN2</cell><cell>CNN3</cell><cell>LSTM</cell><cell>DACNN</cell></row><row><cell>7</cell><cell>5000</cell><cell>1000/2000</cell><cell cols="4">0.44/0.37 0.98/0.89 0.96/0.84 0.97/0.82 97.3/63.3 0.99/0.92</cell></row><row><cell>8</cell><cell>3000</cell><cell>1000/2000</cell><cell cols="4">0.37/0.31 0.95/0.85 0.86/0.75 0.98/0.82 0.95/0.62 0.99/0.92</cell></row><row><cell>9</cell><cell>2000</cell><cell>1000/2000</cell><cell cols="4">0.30/0.29 0.88/0.78 0.77/0.67 0.93/0.78 0.94/0.59 0.99/0.89</cell></row><row><cell>10</cell><cell>1500</cell><cell>1000/2000</cell><cell cols="4">0.29/0.25 0.80/0.69 0.67/0.60 0.88/0.74 0.85/0.58 0.98/0.88</cell></row><row><cell>11</cell><cell>1000</cell><cell>1000/2000</cell><cell cols="4">0.28/0.25 0.66/0.60 0.59/0.53 0.77/0.67 0.83/0.57 0.97/0.87</cell></row></table><note><p>Fig. 13. ROC curves for CNN3 and DACNN under the seen conditions of case 11 experiment.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table A . 8</head><label>A8</label><figDesc>Description of statistical information of diverse machine conditions for wind turbine fault dataset regarding the training and testing set. Case 2 Case 3 Case 4 Case 5 Case 6 label</figDesc><table><row><cell cols="2">Machine Seen conditions</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Unseen conditions</cell><cell>Condition</cell></row><row><cell>condition Case 1</cell><cell>Case 2</cell><cell>Case 3</cell><cell>Case 4</cell><cell>Case 5</cell><cell>Case 6</cell><cell>Case 1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>T.Han, C. Liu, W. Yang et al. / Knowledge-Based Systems xxx (xxxx) xxxx</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by National Natural Science Foundation of China (Grant No. 11572167 and 51174273). The authors would like to express their sincere gratitude to Mr. Shaohua Li for his contributions on the acquisition of experimental data.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>transferred to partially initialize the DACNN. Then, the adversarial learning implemented between the front convolutional blocks and an additional discriminative classifier helps the network to capture the universal and domain-invariant features. The proposed method not only inherits the advantages of deep models, such as adaptive feature learning ability and less dependence on prior knowledge, but also makes the feature representation more robust and improves the generalization capacity under the conditions of limited training data. The extensive experiments on two typical fault datasets have proved the effectiveness and superiority of proposed method, especially when diagnosing the unseen conditions during model training, showing its promising application prospect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>The further description of experiment data setting is added in this part. In the both two case studies, the monitoring data of diverse machine conditions under different working conditions are divided into small samples to form the balanced fault dataset. Then, the training and testing samples are randomly selected from the whole fault dataset. The numbers of samples in different class are not the same in each cases, but not differ largely. Since 20 random experiments are performed to achieve average results for each case, we listed the detailed statistical information of diverse categories in one trial for two case studies (displayed in Tables A. <ref type="bibr" target="#b7">8</ref> and A.9).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Data-driven cost estimation for additive manufacturing in cybermanufacturing</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Manuf. Syst</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="115" to="126" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning for smart manufacturing: Methods and applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Manuf. Syst</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A novel fault diagnosis technique for enhancing maintenance and reliability of rotating machines</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yunusa-Kaltungo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Nembhard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Struct. Health Monit</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="231" to="262" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An unsupervised spatiotemporal graphical modeling approach for wind turbine condition monitoring</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Renew. Energy</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page" from="230" to="241" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hvsrms localization formula and localization law: Localization diagnosis of a ball bearing outer ring fault</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mech. Syst. Signal Process</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="608" to="629" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quantitative and localization diagnosis of a defective ball bearing based on vertical-horizontal synchronization signal analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="8695" to="8706" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Wavelets for fault diagnosis of rotary machines: A review with applications, Signal Process</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Toward condition monitoring of damper windings in synchronous motors via emd analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Antonino-Daviu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riera-Guasp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pons-Llinares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Roger-Folch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Prez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Charlton-Prez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Energy Convers</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="432" to="439" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Condition monitoring and state classification of gearboxes using stochastic resonance and hidden markov models</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">U</forename><surname>Mba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Makis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marchesiello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fasana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Garibaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Measurement</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="76" to="95" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Application of an improved maximum correlated kurtosis deconvolution method for fault diagnosis of rolling element bearings</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mech. Syst. Signal Process</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="173" to="195" />
			<date type="published" when="2017">2017</date>
			<pubPlace>Switzerland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Application of pattern recognition in gear faults based on the matching pursuit of a characteristic waveform</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Measurement</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="212" to="222" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Eemd method and wnn for fault diagnosis of locomotive roller bearings</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="7334" to="7341" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Comparison of random forest, artificial neural networks and support vector machine for intelligent diagnosis of rotating machinery</title>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Inst. Meas. Control</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2681" to="2693" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Intelligent fault diagnosis method for rotating machinery via dictionary learning and sparse representation-based classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Measurement</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page" from="181" to="193" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Global geometric similarity scheme for feature selection in fault diagnosis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3585" to="3595" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep neural networks: A promising tool for fault characteristic mining and intelligent diagnosis of rotating machinery with massive data</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mech. Syst. Signal Process</title>
		<imprint>
			<biblScope unit="page" from="303" to="315" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep neural networks for learning spatio-temporal features from tomography sensors</title>
		<author>
			<persName><forename type="first">O</forename><surname>Costilla-Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Scully</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Ozanyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="645" to="653" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep coupling autoencoder for fault diagnosis with multimodal sensory data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Inf</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1137" to="1145" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network based regression approach for estimation of remaining useful life</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Database Systems for Advanced Applications</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="214" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Thermal analysis mlp neural network based fault diagnosis on worm gears</title>
		<author>
			<persName><forename type="first">T</forename><surname>Waqar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Demetgul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Measurement</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="56" to="66" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rolling bearing fault diagnosis using an optimization deep belief network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Meas. Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Intelligent fault diagnosis of rolling bearing using deep wavelet auto-encoder with extreme learning machine</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl.-Based Syst</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A novel method for intelligent fault diagnosis of rolling bearings using ensemble deep auto-encoders</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mech. Syst. Signal Process</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="278" to="297" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A recurrent neural network based health indicator for remaining useful life prediction of bearings</title>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">240</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="98" to="109" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fault diagnosis of wind turbine based on Long Short-term memory networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Renew. Energy</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page" from="422" to="432" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deep learning enabled fault diagnosis using time-frequency image analysis of rolling element bearings, Shock Vib</title>
		<author>
			<persName><forename type="first">D</forename><surname>Verstraete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ferrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Droguett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Meruane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Modarres</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A convolutional neural network based feature learning and fault diagnosis method for the condition monitoring of gearbox</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Measurement</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A new deep learning model for fault diagnosis with good anti-noise and domain adaptation ability on raw vibration signals</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">425</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Convolutional neural network-based hidden markov models for rolling element bearing fault identification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl.-Based Syst</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="page" from="65" to="76" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A new convolutional neural network based data-driven fault diagnosis method</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="5990" to="5998" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An adaptive spatiotemporal feature learning approach for fault diagnosis in complex systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mech. Syst. Signal Process</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="170" to="187" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A multivariate encoder information based convolutional neural network for intelligent fault diagnosis of planetary gearboxes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl.-Based Syst</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="237" to="250" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Improved techniques for training gans</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adversarial training methods for semisupervised text classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eprint Arxiv</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Semi-supervised learning with generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Eprint Arxiv</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Phm data challenge</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Phm</surname></persName>
		</author>
		<ptr target="https://www.phmsociety.org/competition/PHM/09" />
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Center</surname></persName>
		</author>
		<ptr target="http://csegroups.case.edu/bearingdatacenter/pages/welcomecasewestern-reserve-university-bearing-data-center-website" />
		<title level="m">Case western reserve university bearing data</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semi-supervised domain adaptation via convolutional neural network</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2841" to="2845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with adversarial residual transform networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eprint Arxiv</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cross domain regularization for neural ranking models using adversarial learning</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H W B C</forename><surname>Daniel Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eprint Arxiv</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
