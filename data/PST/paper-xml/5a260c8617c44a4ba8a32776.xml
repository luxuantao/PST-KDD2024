<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UnDeepVO: Monocular Visual Odometry through Unsupervised Deep Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ruihao</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sen</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhiqiang</forename><surname>Long</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dongbing</forename><surname>Gu</surname></persName>
						</author>
						<title level="a" type="main">UnDeepVO: Monocular Visual Odometry through Unsupervised Deep Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4E245FA574902FE9CE46C3F177536A7F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel monocular visual odometry (VO) system called UnDeepVO in this paper. UnDeepVO is able to estimate the 6-DoF pose of a monocular camera and the depth of its view by using deep neural networks. There are two salient features of the proposed UnDeepVO: one is the unsupervised deep learning scheme, and the other is the absolute scale recovery. Specifically, we train UnDeepVO by using stereo image pairs to recover the scale but test it by using consecutive monocular images. Thus, UnDeepVO is a monocular system. The loss function defined for training the networks is based on spatial and temporal dense information. A system overview is shown in Fig. <ref type="figure">1</ref>. The experiments on KITTI dataset show our UnDeepVO achieves good performance in terms of pose accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Visual odometry (VO) enables a robot to localize itself in various environments by only using low-cost cameras. In the past few decades, model-based VO or geometric VO has been widely studied and its two paradigms, feature-based method <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref> and direct method <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>, have both achieved great success. However, model-based methods tend to be sensitive to camera parameters and fragile in challenging settings, e.g., featureless places, motion blurs and lighting changes.</p><p>In recent years, data-driven VO or deep learning based VO has drawn significant attention due to its potentials in learning capability and the robustness to camera parameters and challenging environments. Starting from the relocalization problem with the use of supervised learning, Kendall et al. <ref type="bibr" target="#b6">[7]</ref> first proposed to use a Convolutional Neural Network (CNN) for 6-DoF pose regression with raw RGB images as its inputs. Li et al. <ref type="bibr" target="#b7">[8]</ref> then extended this into a new architecture for raw RGB-D images with the advantage of facing the challenging indoor environments. Video clips were employed in <ref type="bibr" target="#b8">[9]</ref> to capture the temporal dynamics for relocalization. Given pre-processed optical flow, a CNN based frame-to-frame VO system was reported in <ref type="bibr" target="#b9">[10]</ref>. Wang et al. <ref type="bibr" target="#b10">[11]</ref> then presented a Recurrent Convolutional Neural Network (RCNN) based VO method resulting in a competitive performance against model-based VO methods. Ummenhofer <ref type="bibr" target="#b11">[12]</ref> proposed "DeMoN" which can simultaneously estimate the camera's ego-motion, image depth, surface normal and optical flow. Visual inertial odometry with deep learning was also developed in <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b13">[14]</ref>. 1 Ruihao Li, Dongbing Gu are with School of Computer Science and Electronic Engineering, University of Essex, Colchester, CO4 3SQ, UK.</p><p>{rlig, dgu}@essex.ac.uk 2 Sen Wang is with Edinburgh Centre for Robotics, Heriot-Watt University, Edinburgh, EH14 4AS, UK. s.wang@hw.ac.uk However, all the above mentioned methods require the ground truth of camera poses or depth images for conducting the supervised training. Currently obtaining ground truth datasets in practice is typically difficult and expensive, and the amount of existing labeled datasets for supervised training is still limited. These limitations suggest us to look for various unsupervised learning VO schemes, and consequently we can train them with easily collected unlabeled datasets and apply them to localization scenarios.</p><p>VO related unsupervised deep learning research mainly focuses on depth estimation, inspired by the image wrap technique "spatial transformer" <ref type="bibr" target="#b14">[15]</ref>. Built upon it, Garg et al. <ref type="bibr" target="#b15">[16]</ref> proposed a novel unsupervised depth estimation method by using the left-right photometric constraint of stereo image pairs. This method was further improved in <ref type="bibr" target="#b16">[17]</ref> by wrapping the left and right images across each other. In this way, the accuracy of depth prediction was improved by penalizing both left and right photometric losses. Instead of using stereo image pairs, Zhou et al. <ref type="bibr" target="#b17">[18]</ref> proposed to use consecutive monocular images to train and estimate both ego-motion and depth, but the system cannot recover the scale from learning monocular images. Nevertheless, these unsupervised learning schemes have brought deep learning technologies and VO methods closer and showed great potential in many applications.</p><p>In this paper, we propose UnDeepVO, a novel monocular VO system based on unsupervised deep learning scheme (see Fig. <ref type="figure" target="#fig_0">1</ref>). Our main contributions are as follows:</p><p>• We demonstrate a monocular VO system with recovered absolute scale, and we achieve this in an unsupervised manner by harnessing both spatial and temporal geometric constraints. • Not only estimated pose but also estimated dense depth map are generated with absolute scales thanks to the use of stereo image pairs during training. • We evaluate our VO system using KITTI dataset, and the results show UnDeepVO achieves good performance in pose estimation for monocular cameras. Since UnDeepVO only requires stereo imagery for training without the need of labeled datasets, it is possible to train it with an extremely large number of unlabeled datasets to continuously improve its performance.</p><p>The rest of this paper is organized as follows. Section II introduces the architecture of our proposed system. Section III describes different types of losses used to facilitate the unsupervised training of our system. Section IV presents experimental results. Finally, conclusion is drawn in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. SYSTEM OVERVIEW</head><p>Our system is composed of a pose estimator and a depth estimator, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Both estimators take consecutive monocular images as inputs, and produce scaled 6-DoF pose and depth as outputs, respectively.</p><p>For the pose estimator, it is a VGG-based <ref type="bibr" target="#b18">[19]</ref> CNN architecture. It takes two consecutive monocular images as input and predicts the 6-DoF transformation between them. Since rotation (represented by Euler angles) has high nonlinearity, it is usually difficult to train compared with translation. For supervised training, a popular solution is to give a bigger weight to the rotational loss <ref type="bibr" target="#b6">[7]</ref> as a way of normalization. In order to better train the rotation with unsupervised learning, we decouple the translation and the rotation with two separate groups of fully-connected layers after the last convolutional layer. This enables us to introduce a weight normalizing the rotation and the translation predictions for better performance. The specific architecture of the pose estimator is shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>The depth estimator is mainly based on an encoderdecoder architecture to generate dense depth maps. Different from other depth estimation methods <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> which produce disparity images (inverse of the depth) from the network, the depth estimator of UnDeepVO is designed to directly predict depth maps. This is because training trails report that the whole system is easier to converge when training in this way.</p><p>For most monocular VO methods, a predefined scale has to be applied. One feature of our UnDeepVO is to recover absolute scale of 6-DoF pose and depth, it is credited to As shown in Fig. <ref type="figure" target="#fig_3">3</ref>, we utilize both spatial and temporal geometric consistencies of a stereo image sequence to formulate the loss function. The red points in one image all have the corresponding ones in another. Spatial geometric consistency represents the geometric projective constraint between the corresponding points in left-right image pairs, while temporal geometric consistency represents the geometric projective constraint between the corresponding points in two consecutive monocular images (more details in section IV). By using these constraints to construct the loss function and minimizing them all together, the UnDeepVO learns to estimate scaled 6-DoF poses and depth maps in an end-toend unsupervised manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OBJECTIVE LOSSES FOR UNSUPERVISED TRAINING</head><p>UnDeepVO is trained with losses through backpropagation. Since the losses are built on geometric constraints rather than labeled data, UnDeepVO is trained in an unsupervised manner. Its total loss includes spatial image losses and  temporal image losses, as shown in Fig. <ref type="figure" target="#fig_3">3</ref>. The spatial image losses drive the network to recover scaled depth maps by using stereo image pairs, while the temporal image losses are designed to minimize the errors on camera motion by using two consecutive monocular images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Spatial Image Losses of a Stereo Image Pair</head><p>The spatial losses are constructed from the geometric constraints between left and right stereo images. It is composed of left-right photometric consistency loss, disparity consistency loss and pose consistency loss. UnDeepVO relies on these losses to recover the absolute scale for the monocular VO.</p><p>1) Photometric Consistency Loss: The left-right projective photometric error is used as photometric consistency loss to train the network. Specifically, for the overlapped area between two stereo images, every pixel in one image can find its correspondence in the other with horizontal distance D p <ref type="bibr" target="#b15">[16]</ref>. Assume p l (u l , v l ) is a pixel in the left image and p r (u r , v r ) is its corresponding pixel in the right image. Then, we have the spatial constraint u l = u r and v l = v r + D p . The distance D p can be calculated by</p><formula xml:id="formula_0">D p = B f /D dep (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where B is the baseline of the stereo camera, f is the focal length and D dep is the depth value of the corresponding pixel. We can construct a D p map with the depths estimated from the network to define the constraints between the left and right images. With this spatial constraint and the calculated D p map, we could synthesize one image from the other through "spatial transformer" <ref type="bibr" target="#b14">[15]</ref>. The combination of an L1 norm and an SSIM term <ref type="bibr" target="#b19">[20]</ref> is used to construct the left-right photometric consistency loss:</p><formula xml:id="formula_2">L l pho = λ s L SSIM (I l , I l ) + (1 -λ s )L l 1 (I l , I l ) (2) L r pho = λ s L SSIM (I r , I r ) + (1 -λ s )L l 1 (I r , I r )<label>(3)</label></formula><p>where I l , I r are the original left and right images respectively, I l is the synthesized left image from the original right image I r , and I r is the synthesized right image from the original left image I l , L SSIM is the operation defined in <ref type="bibr" target="#b20">[21]</ref> with a weight λ s , and L l 1 is the L1 norm operation.</p><p>2) Disparity Consistency Loss: Similarly, the left and right disparity maps (inverse of depth) are also constrained by D p . The disparity map UnDeepVO used is</p><formula xml:id="formula_3">D dis = D p × I W (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where I W is the width of original image size. Denote the left and right disparity maps as D l dis and D r dis , respectively. We can use D p to synthesize D l dis , D r dis from D r dis , D l dis . Then, the disparity consistency losses are</p><formula xml:id="formula_5">L l dis = L l 1 (D l dis , D l dis ) (5) L r dis = L l 1 (D r dis , D r dis )<label>(6)</label></formula><p>3) Pose Consistency Loss: We use both left and right image sequences to predict the 6-DoF transformation of the camera separately during training. Ideally, these two predicted transformations should be basically identical. Therefore, we can penalize the difference between them by</p><formula xml:id="formula_6">L pos = λ p L l 1 (x l , x r ) + λ o L l 1 (ϕ l , ϕ r )<label>(7)</label></formula><p>where λ p is the left-right position consistency weight, λ o is the left-right orientation consistency weight, and [x l , ϕ l ] and [x r , ϕ r ] are the predicted poses from the left and right image sequences, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Temporal Image Losses of Consecutive Monocular Images</head><p>Temporal loss is defined according to the geometric constraints between two consecutive monocular images. It is an essential part to recover the 6-DoF motion of camera. It comprises photometric consistency loss and 3D geometric registration loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Photometric Consistency Loss:</head><p>The photometric loss is computed from two consecutive monocular images. Similar to DTAM <ref type="bibr" target="#b3">[4]</ref>, in order to estimate 6-DoF transformation, the projective photometric error is employed as the loss to minimize. Denote I k , I k+1 as the kth and (k + 1)th image frame, respectively, and p k (u k , v k ) as one pixel in I k , and p k+1 (u k+1 , v k+1 ) as the corresponding pixel in I k+1 . Then, we can derive p k+1 from p k through</p><formula xml:id="formula_7">p k+1 = KT k,k+1 D dep K -1 p k (<label>8</label></formula><formula xml:id="formula_8">)</formula><p>where K is the camera intrinsics matrix, D dep is the depth value of the pixel in the kth frame, T k,k+1 is the camera coordinate transformation matrix from the kth frame to the (k + 1)th frame. We can synthesize I k and I k+1 from I k+1 and I k by using estimated poses and "spatial transformer" <ref type="bibr" target="#b14">[15]</ref>. Therefore, the photometric losses between the monocular image sequence are</p><formula xml:id="formula_9">L k pho = λ s L SSIM (I k , I k ) + (1 -λ s )L l 1 (I k , I k ) (<label>9</label></formula><formula xml:id="formula_10">)</formula><formula xml:id="formula_11">L k+1 pho = λ s L SSIM (I k+1 , I k+1 ) + (1 -λ s )L l 1 (I k+1 , I k+1 ) (10)</formula><p>2) 3D Geometric Registration Loss: 3D geometric registration loss is to estimate the transformation by aligning two point clouds, similar to the Iterative Closest Point (ICP) technique. Assume P k (x, y, z) is a point in the kth camera coordination. It can then be transformed to the (k + 1)th camera coordination as P k (x, y, z) by using T k,k+1 . Similarly, points in the (k + 1)th frame can be transformed to kth frame. Then, the 3D geometric registration losses between two monocular images are</p><formula xml:id="formula_12">L k geo = L l 1 (P k , P k )<label>(11)</label></formula><p>L k+1 geo = L l<ref type="foot" target="#foot_1">1</ref> (P k+1 , P k+1 )</p><p>In summary, the final loss function of our system combines the previous spatial and temporal losses together. The leftright photometric consistency loss has been used in <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b16">[17]</ref> to estimate depth map. <ref type="bibr" target="#b17">[18]</ref> introduced the photometric loss of a monocular image sequence for ego-motion and depth estimation. However, to the best of our knowledge, UnDeepVO is the first to recover both scaled camera poses and depth maps by benefiting all these losses together with the 3D geometric registration and pose consistency losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL EVALUATION</head><p>In this section, we evaluated the proposed UnDeepVO system. 1 The network models were implemented with the TensorFlow framework and trained with NVIDIA Tesla P100 GPUs. For testing, we used a laptop equipped with NVIDIA GeForce GTX 980M and Intel Core i7 2.7GHz CPU. The GPU memory needed for pose estimation is less than 400MB with 40Hz real-time performance.</p><p>Adam optimizer was employed to train the network for up to 20-30 epochs with parameter β 1 = 0.9 and β 2 = 0.99. The learning rate started from 0.001 and decreased by half for Fig. <ref type="figure">4</ref>: Trajectories of Sequence 02, 05, 07 and 08. Results of SfMLearner <ref type="bibr" target="#b17">[18]</ref> are post-processed with 7-DoF alignment to ground truth since it cannot recover the scale. UnDeepVO and SfMLearner use images with size 416×128. Images used by VISO2-M are 1242×376. every 1/5 of total iterations. The sequence length of images feeding to the pose estimator was 2. The size of image input to the networks was 416 × 128. We also resized the output images to a higher resolution to compute the losses and fine-tuned the networks in the end. Different kinds of data augmentation methods were used to enhance the performance and mitigate possible overfitting, such as image color augmentation, rotational data augmentation and left-right pose estimation augmentation. Specifically, we randomly selected 20% images for color augmentation with random brightness in range [0.9, 1.1], random gamma in range [0.9, 1.1] and random color shifts in range [0.9, 1.1]. For rotational data augmentation, we increased the proportion of rotational data to achieve better performance in rotation estimation. Pose estimation consistency of left-right images was also used for left-right pose estimation augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Trajectory Evaluation</head><p>We adopted the popular KITTI Odometry Dataset <ref type="bibr" target="#b21">[22]</ref> to evaluate the proposed UnDeepVO system, and compared the results with SfMLearner <ref type="bibr" target="#b17">[18]</ref>, monocular VISO2-M and ORB-SLAM-M (without loop closure). In order to implement fair qualitative and quantitative comparison, we used the same training data as in SfMLearner <ref type="bibr" target="#b17">[18]</ref> (sequences: 00-08). The trajectories produced by different methods are shown in Fig. <ref type="figure">4</ref>, the comparison here shows the goodness of the network fit and is meaningful for structure-from-motion  The detailed results (shown in Fig. <ref type="figure">4</ref>) are listed in Table <ref type="table" target="#tab_0">I</ref> for quantitative evaluation. We use the standard evaluation method provided along with KITTI dataset. Average translational root-mean-square error (RMSE) drift (%) and average rotational RMSE drift ( • /100m) on length of 100m-800m are adopted. Since SfMLearner and ORB-SLAM-M cannot recover the scale of 6-DoF poses, we aligned their poses to the ground-truth with 6-DoF and scale (7-DoF). For monocular VISO2-M and ORB-SLAM without loop closure, they can not work with our input settings (image resolution 416 × 128), so we provide the results of both system with high resolution 1242 × 376. All the methods here did not use any loop closure technology. As shown in Table <ref type="table" target="#tab_0">I</ref>, our method achieves good pose estimation performance among the monocular methods even with low resolution images and without the scale post-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Depth Estimation Evaluation</head><p>Our system can also produce the scaled depth map by using the depth estimator. Fig. <ref type="figure" target="#fig_5">6</ref> shows some raw RGB images and their corresponding depth images estimated from our system. As shown in Fig. <ref type="figure" target="#fig_5">6</ref>, the different depths of cars and trees are explicitly estimated, even the depth of trunks is predicted successfully. The detailed depth estimation results are listed in Table <ref type="table" target="#tab_1">II</ref>. As shown in the table, our method outperforms the supervised one <ref type="bibr" target="#b22">[23]</ref> and the unsupervised one without scale <ref type="bibr" target="#b17">[18]</ref>, but performs not as good as <ref type="bibr" target="#b16">[17]</ref>. This could be caused by a few reasons. First, we only used parts of KITTI dataset (KITTI odometry dataset) for training while all other methods use full KITTI dataset to train their networks. Second, <ref type="bibr" target="#b16">[17]</ref>   the temporal image sequence loss we used could introduce some noise (such as moving objects) for depth estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper, we presented UnDeepVO, a novel monocular VO system with unsupervised deep learning. The system makes use of spatial losses and temporal losses between stereo image sequences for unsupervised training. During testing, the proposed system can perform the pose estimation and dense depth map estimation with monocular images. Our system recovers the scale during the training stage, which distincts itself from other model based or learning based monocular VO methods. In general, unsupervised learning based VO methods have the potential to improve their performance with the increasing size of training datasets. In the next step, we will investigate how to train the UnDeepVO with large amount of datasets to improve its performance, such as robustness to image blurs, camera parameters, or illumination changes. In the future, we also plan to extend our system to a visual SLAM system to reduce the drift. Developing an unsupervised DeepVO system with stereo cameras or RGB-D cameras is also in consideration.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3Fig. 1 :</head><label>1</label><figDesc>Fig. 1: System overview of the proposed UnDeepVO. After training with unlabeled stereo images, UnDeepVO can simultaneously perform visual odometry and depth estimation with monocular images. The estimated 6-DoF poses and depth maps are both scaled without the need for scale postprocessing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Architecture of our UnDeepVO.</figDesc><graphic coords="2,492.59,334.01,61.02,56.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>dep Bf /D dep -Bf /D dep -Bf /D dep T k,k+1 T -1 k,k+1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Training scheme of UnDeepVO. The pose and depth estimators take stereo images as inputs to estimate 6-DoF poses and depth maps, respectively. The total loss including spatial losses and temporal losses can then be calculated based on raw RGB images, estimated depth maps and poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Trajectories of KITTI dataset with our UnDeepVO. No ground truth of poses is available for these sequences. Trajectories with both monocular VISO2-M and stereo VISO2-S are plotted. Our UnDeepVO works well on these sequences and is comparable to VISO2-S.</figDesc><graphic coords="5,60.11,475.25,111.29,90.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Depth images produced by our depth estimator. The left column are raw RGB images, and the right column are the corresponding depth images estimated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>VO results with our proposed UnDeepVO. All the methods listed in the table did not use loop closure. Note that monocular VISO2-M and ORB-SLAM-M (without loop closure) did not work with image size 416 × 128, the results were obtained with image size 1242×376. 7-DoF (6-DoF + scale) alignment with the ground-truth is applied for SfMLearner and ORB-SLAM-M. t rel : average translational RMSE drift (%) on length of 100m-800m.• r rel : average rotational RMSE drift ( • /100m) on length of 100m-800m.</figDesc><table><row><cell></cell><cell cols="2">UnDeepVO</cell><cell cols="2">SfMLearner [18]</cell><cell cols="2">VISO2-M</cell><cell cols="2">ORB-SLAM-M</cell></row><row><cell>Seq.</cell><cell cols="2">(416×128)</cell><cell cols="2">(416×128)</cell><cell cols="2">(1242×376)</cell><cell cols="2">(1242×376)</cell></row><row><cell></cell><cell>t rel (%)</cell><cell>r rel ( • )</cell><cell>t rel (%)</cell><cell>r rel ( • )</cell><cell>t rel (%)</cell><cell>r rel ( • )</cell><cell>t rel (%)</cell><cell>r rel ( • )</cell></row><row><cell>00</cell><cell>4.14</cell><cell>1.92</cell><cell>65.27</cell><cell>6.23</cell><cell>18.24</cell><cell>2.69</cell><cell>25.29</cell><cell>7.37</cell></row><row><cell>02</cell><cell>5.58</cell><cell>2.44</cell><cell>57.59</cell><cell>4.09</cell><cell>4.37</cell><cell>1.18</cell><cell>×</cell><cell>×</cell></row><row><cell>05</cell><cell>3.40</cell><cell>1.50</cell><cell>16.76</cell><cell>4.06</cell><cell>19.22</cell><cell>3.54</cell><cell>26.01</cell><cell>10.62</cell></row><row><cell>07</cell><cell>3.15</cell><cell>2.48</cell><cell>17.52</cell><cell>5.38</cell><cell>23.61</cell><cell>4.11</cell><cell>24.53</cell><cell>10.83</cell></row><row><cell>08</cell><cell>4.08</cell><cell>1.79</cell><cell>24.02</cell><cell>3.05</cell><cell>24.18</cell><cell>2.47</cell><cell>32.40</cell><cell>12.13</cell></row><row><cell>mean</cell><cell>4.07</cell><cell>2.02</cell><cell>36.23</cell><cell>4.56</cell><cell>17.93</cell><cell>2.80</cell><cell>27.05</cell><cell>10.23</cell></row></table><note><p>•</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Depth estimation results on KITTI using the split of Eigen et al.<ref type="bibr" target="#b22">[23]</ref>.</figDesc><table><row><cell>Methods</cell><cell cols="2">Dataset Scale</cell><cell cols="4">Error metric Abs Rel Sq Rel RMSE RMSE log</cell></row><row><cell>Eigen [23]</cell><cell>K (raw)</cell><cell></cell><cell>0.214</cell><cell cols="2">1.605 6.563</cell><cell>0.292</cell></row><row><cell cols="2">MonoDepth [17] K (raw)</cell><cell></cell><cell>0.148</cell><cell cols="2">1.344 5.927</cell><cell>0.247</cell></row><row><cell cols="2">SfMLearner [18] K (raw)</cell><cell>×</cell><cell>0.208</cell><cell cols="2">1.768 6.856</cell><cell>0.283</cell></row><row><cell>UnDeepVO</cell><cell>K (odo)</cell><cell></cell><cell>0.183</cell><cell>1.73</cell><cell>6.57</cell><cell>0.268</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2018" xml:id="foot_0"><p>IEEE International Conference on Robotics and Automation (ICRA) May 21-25, 2018, Brisbane, Australia 978-1-5386-3081-5/18/$31.00 ©2018 IEEE</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>Video:https://www.youtube.com/watch?v=5RdjO93wJqo&amp; t (a) 02 (b) 05 (c) 07 (d) 08</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank Robin Dowling for his support in experiments. The first author has been financially supported by scholarship from China Scholarship Council. This work was supported in part by EPSRC Robotics and Artificial Intelligence ORCA Hub (grant No. EP/R026173/1).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MonoSLAM: Real-time single camera SLAM</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Molton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Stasse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1052" to="1067" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Parallel tracking and mapping for small AR workspaces</title>
		<author>
			<persName><forename type="first">G</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMAR 2007. 6th IEEE and ACM International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="225" to="234" />
		</imprint>
	</monogr>
	<note>Mixed and Augmented Reality</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ORB-SLAM: a versatile and accurate monocular SLAM system</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1147" to="1163" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">DTAM: Dense tracking and mapping in real-time</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2320" to="2327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">LSD-SLAM: Large-scale direct monocular SLAM</title>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schöps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="834" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Direct sparse odometry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">PoseNet: A convolutional network for real-time 6-DOF camera relocalization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2938" to="2946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Indoor relocalization in challenging environments with dual-stream convolutional neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automation Science and Engineering</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">VidLoc: 6-DoF video-clip relocalization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploring representation learning with CNNs for frame-to-frame ego-motion estimation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Costante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Ciarfuglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE robotics and automation letters</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="25" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DeepVO: Towards endto-end visual odometry with deep recurrent convolutional neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="2043" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DeMoN: Depth and motion network for learning monocular stereo</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">VINet: Visual-Inertial odometry as a sequence-to-sequence learning problem</title>
		<author>
			<persName><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3995" to="4001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Towards visual ego-motion learning in robots</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Leonard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10279</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised CNN for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Is L2 a good loss function for neural networks for image processing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1511</biblScope>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? The KITTI vision benchmark suite</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
