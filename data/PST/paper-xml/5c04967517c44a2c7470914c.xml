<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robustness of conditional GANs to noisy labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kiran</forename><surname>Koshy Thekumparampil</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign, ‡ Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ashish</forename><surname>Khetan</surname></persName>
							<email>ashish.khetan09@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign, ‡ Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zinan</forename><surname>Lin</surname></persName>
							<email>zinanl@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign, ‡ Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sewoong</forename><surname>Oh</surname></persName>
							<email>swoh@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign, ‡ Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Robustness of conditional GANs to noisy labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E380E175F3B1F72C277D8F7B92205BD7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the problem of learning conditional generators from noisy labeled samples, where the labels are corrupted by random noise. A standard training of conditional GANs will not only produce samples with wrong labels, but also generate poor quality samples. We consider two scenarios, depending on whether the noise model is known or not. When the distribution of the noise is known, we introduce a novel architecture which we call Robust Conditional GAN (RCGAN). The main idea is to corrupt the label of the generated sample before feeding to the adversarial discriminator, forcing the generator to produce samples with clean labels. This approach of passing through a matching noisy channel is justified by accompanying multiplicative approximation bounds between the loss of the RCGAN and the distance between the clean real distribution and the generator distribution. This shows that the proposed approach is robust, when used with a carefully chosen discriminator architecture, known as projection discriminator. When the distribution of the noise is not known, we provide an extension of our architecture, which we call RCGAN-U, that learns the noise model simultaneously while training the generator. We show experimentally on MNIST and CIFAR-10 datasets that both the approaches consistently improve upon baseline approaches, and RCGAN-U closely matches the performance of RCGAN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Conditional generative adversarial networks (GAN) have been widely successful in several applications including improving image quality, semi-supervised learning, reinforcement learning, category transformation, style transfer, image de-noising, compression, in-painting, and super-resolution <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b57">58]</ref>. The goal of training a conditional GAN is to generate samples from distributions satisfying certain conditioning on some correlated features. Concretely, given samples from joint distribution of a data point x and a label y, we want to learn to generate samples from the true conditional distribution of the real data P X|Y . A canonical conditional GAN studied in literature is the case of discrete label y <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b31">32]</ref>. Significant progresses have been made in this setting, which are typically evaluated on the quality of the conditional samples. These include measuring inception scores and intra Fréchet inception distances, visual inspection on downstream tasks such as category morphing and super resolution <ref type="bibr" target="#b31">[32]</ref>, and faithfulness of the samples as measured by how accurately we can infer the class that generated the sample <ref type="bibr" target="#b35">[36]</ref>.</p><p>We study the problem of training conditional GANs with noisy discrete labels. By noisy labels, we refer to a setting where the label y for each example in the training set is randomly corrupted. Such noise can result from an adversary deliberately corrupting the data <ref type="bibr" target="#b6">[7]</ref> or from human errors in crowdsourced label collection <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18]</ref>. This can be modeled as a random process, where a clean data point x ∈ X and its label y ∈ [m] are drawn from a joint distribution P X,Y with m classes. For each data point, the label is corrupted by passing through a noisy channel represented by a row-stochastic confusion matrix C ∈ R m×m defined as C ij P( Y = j|Y = i). This defines a joint distribution for the data point x and a noisy label y: P X, Y . If we train a standard conditional GAN on noisy samples, then it solves the following optimization: </p><p>where φ is a function of choice, D and G are the discriminator and the generator respectively optimized over function classes G and F of our choice, and N is the distribution of the latent random vector. For typical choices of φ, for example log(•), and large enough function classes G and F, the optimal conditional generator learns to generate samples from P X| Y , the corrupted conditional distribution. In other words, it generates samples X from classes other than what it is conditioned on. As the learned distribution exhibits such a bias, we call this naive approach the Biased GAN. Under this setting, there is a fundamental question of interest: can we design a novel conditional GAN that can generate samples from the true conditional distribution P X|Y , even when trained on noisy samples?</p><p>Several aspects of this problem make it challenging and interesting. First, the performance of such robust GAN should depend on how noisy the channel C is. If C is rank-deficient, for instance, then there are multiple distributions that result in the same distribution after the corruption, and hence no reliable learning of the true distribution is possible. We would ideally want a theoretical guarantee that shows such trade-off between C and the robustness of GANs. Next, when the noise is from errors in crowdsourced labels, we might have some access to the confusion matrix C from historical data. On other cases of adversarial corruption, we might not have any information of C. We want to provide robust solutions to both. Finally, an important practical challenge in this setting is to correct the noisy labels in the training data. We address all such variations in our approaches and make the following contributions.</p><p>Our contributions. We introduce two architectures to train conditional GANs with noisy samples.</p><p>First, when we have the knowledge of the confusion matrix C, we propose RCGAN (Robust Conditional GAN) in Section 2. We first prove that minimizing the RCGAN loss provably recovers the clean distribution P X|Y (Theorem 2), under certain conditions on the class F of discriminators we optimize over (Assumption 1). We show that such a condition on F is also necessary, as without it, the training loss can be arbitrarily small while the generated distribution can be far from the real (Theorem 4). The assumption leads to our particular choice of the discriminator in RCGAN, called projection discriminator <ref type="bibr" target="#b31">[32]</ref> that satisfies all the conditions (Remark 1). Finally, we provide a finite sample generalization bound showing that the loss minimized in training RCGAN does generalize, and results in the learned distribution being close to the clean conditional distribution P X|Y (Theorem 3). Experimental results in benchmark datasets confirm that RCGAN is robust against noisy samples, and improves significantly over the naive Biased GAN.</p><p>Secondly, when we do not have access to C, we propose RCGAN-U (RCGAN with Unknown noise distribution) in Section 4. We provide experimental results showing that performance gains similar to that of RCGAN can be achieved. Finally, we showcase the practical use of thus learned conditional GANs, by using it to fix the noisy labels in the training data. Numerical experiments confirm that the RCGAN framework provides a more robust approach to correcting the noisy labels, compared to the state-of-the-art methods that rely only on discriminators.</p><p>Related work. Two popular training methods for generative models are variational auto-encoders <ref type="bibr" target="#b21">[22]</ref> and adversarial training <ref type="bibr" target="#b13">[14]</ref>. The adversarial training approach has made significant advances in several applications of practical interest. <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5]</ref> propose new architectures that significantly improve the training in practical image datasets. <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b15">16]</ref> propose new architectures to transfer the style of one image to the other domain. <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b42">43]</ref> show how to enhance a given image with learned generator, by enhancing the resolution or making it more realistic. <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b49">50]</ref> show how to generate videos and <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b0">1]</ref> demonstrate that 3-dimensional models can be generated from adversarial training. <ref type="bibr" target="#b22">[23]</ref> proposes a new architecture encoding causal structures in conditional GANs. <ref type="bibr" target="#b41">[42]</ref> introduces the state-of-the-art conditional independence tester. On a different direction, several recent approaches showcase how the manifold learned by the adversarial training can be used to solve inverse problems <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b52">53]</ref>.</p><p>Conditional GANs have been proposed as a successful tool for various applications, including class conditional image generation <ref type="bibr" target="#b35">[36]</ref>, image to image translation <ref type="bibr" target="#b20">[21]</ref>, and image generation from text <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b54">55]</ref>. Most of the conditional GANs incorporate the class information by naively concatenating it to the input or feature vector at some middle layer <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b54">55]</ref>. AC-GANs <ref type="bibr" target="#b35">[36]</ref> creates an auxiliary classifier to incorporate class information. Projection discriminator GAN <ref type="bibr" target="#b31">[32]</ref> takes an inner product between the embedded class vector and the feature vector. A recent work <ref type="bibr" target="#b30">[31]</ref> which proposes spectral normalization shows that high quality image generation on 1000-class ILSVRC2012 dataset <ref type="bibr" target="#b38">[39]</ref> can be achieved using projection conditional discriminator.</p><p>Robustness of (unconditional) GANs against adversarial or random noise has recently been studied in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b51">52]</ref>. <ref type="bibr" target="#b51">[52]</ref> studies an adversarial attack that perturbs the discriminator output. The proposed architecture of RCGAN is inspired by a closely related work of AmbientGAN in <ref type="bibr" target="#b9">[10]</ref>. AmbientGAN is a general framework addressing any corruption on the image itself (not necessarily just the labels).</p><p>Given corrupted samples with a known corruption, AmbientGAN applies that corruption to the output of the generator before feeding it to the discriminator. Motivated by the success of AmbientGAN in de-noising, we propose RCGAN. An important distinction is that we make specific architectural choices guided by our theoretical analysis that gives a significant gain in practice (Appendix J). Under the scenario of interest with noisy labels, we provide sharp analyses for both the population loss and the finite sample loss. Such sharp characterizations do not exist for the more general AmbientGAN scenarios. Further, our RCGAN-U does not require the knowledge of the confusion matrix, departing from the AmbientGAN approach. Learning classifiers from noisy labels is a closely related problem.</p><p>Recently <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b19">20]</ref> proposed a theoretically motivated classifier which minimizes the modified loss in presence of noisy labels and showed improvement over the robust classifiers <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref>. <ref type="bibr" target="#b46">[47]</ref> proposed adding noise to the classifier output to match the noise distribution.</p><p>Notation. For a vector,</p><formula xml:id="formula_1">x p = ( i |x i | p ) 1/p is the p -norm. For a matrix, let |||A||| p = max x p =1 Ax p denote the operator norm. Then |||A||| ∞ = max i j |A ij |, |||A||| 1 = max j i |A ij | and |||A||| 2 = σ max (A)</formula><p>, the maximum singular value. 1 is all ones vector and I is identity matrix.</p><p>[n] = {1, . . . , n}. For a vector</p><formula xml:id="formula_2">x ∈ R n , x i (i ∈ [n]) is its i-th coordinate.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our first architecture: RCGAN</head><p>Training a conditional GAN with noisy samples results in a biased generator. We propose Robust Conditional GAN (RCGAN) architecture which has the following pre-processing, discriminator update, and generator update steps. We assume in this section that the confusions matrix C is known (and the marginal P Y can easily be inferred), and address the case of unknown C in Section 4. Pre-processing: We train a classifier h * to predict the noisy label y given x under a loss l, trained on</p><formula xml:id="formula_3">h * ∈ arg min h∈H E (x, y)∼ P X, Y [ (h(x), y)],</formula><p>where H is a parametric family of classifiers (typically neural networks) and P X, Y is the joint distribution of real x and corresponding real noisy y. D-step: We train on the following adversarial loss. In the second term below, y is generated according to P Y and corresponding noisy labels are generated by corrupting the y according to the conditional distribution C y which is the y-th row of the confusion matrix (assumed to be known):</p><formula xml:id="formula_4">max D∈F E (x, y)∼ P X, Y [φ (D(x, y))] + E z∼N, y∼P Y y|y∼Cy [φ (1 -D(G(z; y), y))] ,</formula><p>where P Y is the true marginal distribution of the labels, N is the distribution of the latent random vector, and F is a family of discriminators. G-step: We train on the following loss with some λ &gt; 0:</p><formula xml:id="formula_5">min G∈G E z∼N, y∼P Y y|y∼Cy φ (1 -D(G(z; y), y)) + λ (h * (G(z; y)), y) ,<label>(2)</label></formula><p>where G is a family of generators. The idea of using auxiliary classifiers have been used to improve the quality of the image and stability of the training, for example in auxiliary classifier GAN (AC-GAN) <ref type="bibr" target="#b35">[36]</ref>, and improve the quality of clustering in the latent space <ref type="bibr" target="#b32">[33]</ref>. We propose an auxiliary classifiers h, mitigating a permutation error, which we empirically identified on naive implementation of our idea with no regularizers.</p><p>Permutation regularizer (controlled by λ). Permutation error occurs if, when asked to produce samples from a target class, the trained generator produces samples dominantly from a single class but different from the target class. We propose a regularizer h * , which predicts the noisy label y. As long as the confusion matrix is diagonally dominant, which is a necessary condition for identifiability, this regularizer encourages the correct permutation of the labels. More regularizers could potentially provide additonal robustness and we discuss one such regularizer (similar to the InfoGAN loss <ref type="bibr" target="#b10">[11]</ref>) in Appendix K.</p><p>Theoretical motivation for RCGAN. When λ = 0, we get the standard conditional GAN update steps, albeit one which tries to minimize discriminator loss between the noisy real distribution P and the distribution Q of the generator when the label is passed through the same noisy channel parameterized by C. The main idea of RCGAN is to minimize a certain divergence between noisy real data and noisy generated data. For example, the choice of bounded functions </p><formula xml:id="formula_6">F = {D : X × [m] → [0,</formula><formula xml:id="formula_7">( P , Q) (1/2)d KL ( P ( P + Q)/2) + (1/2)d KL ( Q ( P + Q)/2), where d KL (• •)</formula><p>denotes the Kullback-Leibler divergence. The following theorem provides approximation guarantees for some common divergence measures over noisy channel, justifying our proposed practical approach. We refer to Appendix B for a proof.</p><p>Theorem 1. Let P X,Y and Q X,Y be two distributions on X × [m]. Let P X, Y , Q X, Y be the corresponding distributions when samples from P, Q are passed through the noisy channel given by the confusion matrix C ∈ R m×m (as defined in Section 1). If C is full-rank, we get,</p><formula xml:id="formula_8">d TV P , Q ≤ d TV (P, Q) ≤ |||C -1 ||| ∞ d TV P , Q , and<label>(3)</label></formula><formula xml:id="formula_9">d JS P Q ≤ d JS (P Q) ≤ |||C -1 ||| ∞ 8 d JS P Q .<label>(4)</label></formula><p>To interpret this theorem, let Q denote the distribution of the generator. The theorem implies that when the noisy generator distribution Q becomes close to the noisy real distribution P in total variation or in Jensen-Shannon divergence, then the generator distribution Q must be close to the distribution of real data P in the same metric. This justifies the use of the proposed architecture RCGAN. In practice, we minimize the sample divergence of the two distributions, instead of the population divergence as analyzed in the above theorem. However, these standard divergences are known to not generalize in training GANs <ref type="bibr" target="#b2">[3]</ref>. To this end, we provide in Section 3 analyses on neural network distances, which are known to generalize, and provide finite sample bounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Theoretical Analysis of RCGAN</head><p>It was shown in <ref type="bibr" target="#b2">[3]</ref> that standard GAN losses of Jensen-Shannon divergence and Wasserstein distance both fail to generalize with a finite number of samples. On the other hand, more recent advances in analyzing GANs in <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b3">4]</ref> show promising generalization bounds by either assuming Lipschitz conditions on the generator model or by restricting the analysis to certain classes of distributions. Under those assumptions, where JS divergence generalizes, Theorem 1 justifies the use of the proposed RCGAN. However, those require the distribution to be Gaussian, mixture of Gaussians, or output of a neural network generator, for example in <ref type="bibr" target="#b3">[4]</ref>.</p><p>In this section, we provide analyses of RCGAN on a distance that generalizes without any assumptions on the distribution of the real data as proven in <ref type="bibr" target="#b2">[3]</ref>: neural network distance. Formally, consider a class of real-valued functions F and a function φ : [0, 1] → R which is either convex or concave. The neural network distance is defined as</p><formula xml:id="formula_10">d F ,φ (P, Q) sup D∈F E (x,y)∼P [φ (D(x, y))] + E (x,y)∼Q [φ (1 -D(x, y))] -µ φ . (<label>5</label></formula><formula xml:id="formula_11">)</formula><p>where P is the distribution of the real data, Q is that of the generated data, and µ φ is the constant correction term to ensure that d F ,φ (P, P ) = 0. We further assume that F includes three constant functions D(x, y) = 0, D(x, y) = 1/2, and D(x, y) = 1, in order to ensure that d F ,φ (P, Q) ≥ 0 and d F ,φ (P, P ) = 0, as shown in Lemma 1 in the Appendix.</p><p>The proposed RCGAN with λ = 0 approximately minimizes the neural network distance d F ,φ ( P , Q) between the two corrupted distributions. In practice, F is a parametric family of functions from a specific neural network architecture that the designer has chosen. In theory, we aim to identify how the choice of class F provides the desired approximation bounds similar to those in Theorem 1, but for neural network distances. This analysis leads to the choice of projection discriminator <ref type="bibr" target="#b31">[32]</ref> to be used in RCGAN (Remark 1). On the other hand, we show in Theorem 4 that an inappropriate choice of the discriminator architecture can cause non-approximation. Further, we provide the sample complexity of the approximation bounds in Theorem 3.</p><p>We refer to the un-regularized version with λ = 0 as simply RCGAN. In this section, we focus on a class of loss functions called Integral Probability Metrics (IPM) where φ(x) = x <ref type="bibr" target="#b43">[44]</ref>. This is a popular choice of loss in GANs in practice <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b7">8]</ref> and in analyses <ref type="bibr" target="#b3">[4]</ref>. We write the induced neural network distance as d F (P, Q), dropping the φ in the notation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Approximation bounds for neural network distances</head><p>We define an operation • over a matrix T ∈ R m×m and a class F of functions on X × [m] → R as</p><formula xml:id="formula_12">T • F g(x, y) = y∈[m] T y y f (x, y) | f ∈ F .<label>(6)</label></formula><p>This makes it convenient to represent the neural network distance corrupted by noise with a confusion matrix C ∈ R m×m , where C y y is the probability a label y is corrupted as y. Formally, it follows from ( <ref type="formula" target="#formula_10">5</ref>) and ( <ref type="formula" target="#formula_12">6</ref>) that d F ( P , Q) = d C•F (P, Q). We refer to Appendix F for a proof. For d F ( P , Q) to be a good approximation of d F (P, Q), we show that the following condition is sufficient. Assumption 1. We assume that the class of discriminator functions F can be decomposed into three parts</p><formula xml:id="formula_13">F = {f 1 + f 2 + c | f 1 ∈ F 1 , f 2 ∈ F 2 } such that c ∈ R is any constant and</formula><p>• F 1 satisfies the inclusion condition:</p><formula xml:id="formula_14">T • F 1 ⊆ F 1 ,<label>(7)</label></formula><p>for all |||T ||| ∞ max i j |T ij | = 1; and</p><p>• F 2 satisfies the label invariance condition: there exists a class F 2 of functions over only x, such that</p><formula xml:id="formula_15">F 2 = α g(x, y) | g(x, y) = f (x), for any f (x) ∈ F 2 , and α ∈ [0, 1] .<label>(8)</label></formula><p>We discuss the necessity and practical implications of this assumption in Section 3.2, and give examples satisfying these assumptions in Remark 1 and Appendix C. Notice that a trivial class with a single constant zero function satisfies both inclusion and label invariance conditions. For example, we can choose c = 0 and also choose to set either F 1 = {f (x, y) = 0} or F 2 = {f (x, y) = 0}, in which case F only needs to satisfy either one of the conditions in Assumption 1. The flexibility that we gain by allowing the set addition F 1 + F 2 is critical in applying these conditions to practical discriminators, especially in proving Remark 1. Note that in the inclusion condition in Eq. 7, we require the condition to hold for all max-norm bounded set: {T : max i j |T ij | = 1}. The reason a weaker condition of all row-stochastic matrices, {T : j T ij = 1}, does not suffice is that in order to prove the upper bound in Eq. 9, we need to apply the invariance condition to</p><formula xml:id="formula_16">|||C -1 ||| -1 ∞ C -1 • F. This matrix |||C -1 ||| -1</formula><p>∞ C -1 is not row-stochastic, but still max-norm bounded. We first show that Assumption 1 is sufficient for approximability of the neural network distance from corrupted samples. For two distributions P X,Y and Q X,Y on X × [m], let P X, Y and Q X, Y be the corresponding corrupted distributions respectively, where the label Y is passed through the noisy channel defined by the confusion matrix C ∈ R m×m , i.e. P (x, y) = y P (x, y)C y, y . Theorem 2. If a class of functions F satisfies Assumption 1, then</p><formula xml:id="formula_17">d F ( P , Q) ≤ d F (P, Q) ≤ |||C -1 ||| ∞ d F ( P , Q) ,<label>(9)</label></formula><p>where we follow the convention that</p><formula xml:id="formula_18">|||C -1 ||| ∞ = ∞ if C is not full rank.</formula><p>We refer to Appendix F for a proof. This gives a sharp characterization on how two distances are related: the one we can minimize in training RCGAN (i.e. d F ( P , Q)) and the true measure of closeness (i.e. d F (P, Q)). Although the latter cannot be directly evaluated or minimized, RCGAN is approximately minimizing the true neural network distance d F (P, Q) as desired.</p><p>The lower bound proves a special case of the data-processing inequality. Two random variables from P and Q get closer in neural network distance, when passed through a stochastic transformation. The upper bound puts a limit on how much closer P and Q can get, depending on the noise level. This fundamental trade-off is captured by |||C -1 ||| ∞ . Under the noiseless case where C is the identity matrix, we have |||C -1 ||| ∞ = 1 and we recover a trivial fact that the two distances are equal. On the other extreme, if C is rank deficient, we use the convention that |||C -1 ||| ∞ = ∞ and the two distances can be arbitrarily different. The approximation factor of |||C -1 ||| ∞ captures how much the space F can shrink by the noise C. This coincides with Theorem 1, where a similar trade-off was identified for the TV distance. In Remark 3 in Appendix D, we show that these bounds cannot be tightened for general P , Q, and F.</p><p>Theorem 2 shows that (i) RCGAN can learn the true conditional distribution, justifying its use; and</p><p>(ii) performance of RCGAN is determined by how noisy the samples are via |||C -1 ||| ∞ . There are still two loose ends. First, does practical implementation of RCGAN architecture satisfy the inclusion and/or label invariance assumptions? Secondly, in practice we cannot minimize d F ( P , Q) as we only have a finite number of samples. How much do we lose in this finite sample regime? We give precise answers to each question in the following two sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inclusion and label invariance assumptions</head><p>For RCGAN, we propose a popular state-of-the-art discriminator for conditional GANs known as the projection discriminator <ref type="bibr" target="#b31">[32]</ref>, parametrized by</p><formula xml:id="formula_19">V ∈ R m×d V , v ∈ R dv , and θ ∈ R d θ : D V,v,θ (x, y) = vec(y) T V ψ(x; θ) + v T ψ (x; θ) ,<label>(10)</label></formula><p>where ψ(x; θ) ∈ R d V and ψ (x; θ) ∈ R dv are vector valued parametric functions for some integers d V , d v , and vec(y) T = [I y=1 , . . . , I y=m ]. The first term satisfies the inclusion condition, as any operation with T can be absorbed into V . The second term is label invariant as it does not depend on y. This is made precise in the following remark, whose proof is provided in Appendix G. Together with this remark, the approximability result in Theorem 2 justifies the use of projection discriminators in RCGAN, which we use in all our experiments. Remark 1. The class of projection discriminators {D V,v,θ (x, y)} V ∈V1,v∈V2,θ∈Θ defined in Eq. 10 satisfies Assumption 1 for any ψ, ψ , and</p><formula xml:id="formula_20">Θ, if V 1 = V ∈ R m×d V max i |V ij | ≤ 1 for all j ∈ [d V ] , and V 2 = v ∈ R dv v ≤ 1 .</formula><p>Other choices of V 1 and V 2 are also possible. For example,</p><formula xml:id="formula_21">V 1 = {V ∈ R m×d V | j max i |V ij | ≤ 1} or V 1 = {V ∈ R m×d V ||||V ||| ∞ = max i j |V ij | ≤ 1}</formula><p>are also sufficient. We find the proposed choice of V 1 easy to implement, as a column-wise L ∞ -norm normalization via projected gradient descent. We describe implementation details in Appendix L. In Appendix E, we show that Assumption 1 is also necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Finite sample analysis</head><p>In practice, we do not have access to the probability distributions P and Q. Instead, we observe a set of samples of a finite size n, from each of them. In training GAN, we minimize the empirical neural network distance, d F ( P n , Q n ), where P n and Q n denote the empirical distribution of n samples. Inspired from the recent generalization results in <ref type="bibr" target="#b2">[3]</ref>, we show that this empirical distance minimization leads to small d F (P, Q) up to an additive error that vanishes with an increasing sample size. As shown in <ref type="bibr" target="#b2">[3]</ref>, Lipschitz and bounded function classes are critical in achieving sample efficiency for GANs. We follow the same approach over a similar function class. Let</p><formula xml:id="formula_22">F p,L = {D u (x, y) ∈ [0, 1] | D u (x, y) is L-Lipschitz in u and u ∈ U ⊆ R p } ,<label>(11)</label></formula><p>be a class of bounded functions with parameter u ∈ R p . We say that</p><formula xml:id="formula_23">F is L-Lipschitz in u if |D u1 (x, y) -D u2 (x, y)| ≤ L u 1 -u 2 , ∀u 1 , u 2 ∈ U, x ∈ X , y ∈ [m].</formula><p>(12) Theorem 3. For any class F p,L of bounded Lipschitz functions D u (x, y) satisfying Assumption 1, there exists a universal constant c &gt; 0 such that</p><formula xml:id="formula_24">d F p,L ( P n , Q n ) -≤ d F p,L (P, Q) ≤ |||C -1 ||| ∞ d F p,L ( P n , Q n ) + ,<label>(13)</label></formula><p>with probability at least 1e -p for any ε &gt; 0 and n large enough, n ≥ (c p / 2 ) log (pL/ ) .</p><p>We refer to Appendix I for a proof. This justifies the proposed RCGAN which minimizes d F ( P n , Q n ), as it leads to the generator Q being close to the real distribution P in neural network distance, d F (P, Q). These bounds inherit the approximability of the population version from Theorem 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Implementation details are explained in Appendix L. We consider one-coin based models, which are parameterized by their label accuracy probability α. In this model a sample with true label y is flipped uniformly at random to label y in [m] \ {y} with probability 1α. The entries of its confusion matrix C, will then be C ii = α and</p><formula xml:id="formula_25">C i =j = (1 -α)/(m -1)</formula><p>, where m is the number of classes. We call this model uniform flipping model. Code to reproduce our experiments is available at https://github.com/POLane16/Robust-Conditional-GAN.</p><p>Baselines. First is the biased GAN, which is a conditional GAN applied directly on the noisy data.</p><p>The loss is hence biased, and the true conditional distribution is not the optimal solution of this biased loss. Next natural baseline is using de-biased classifier as the discriminator, motivated by the approach of <ref type="bibr" target="#b33">[34]</ref> on learning classifiers from noisy labels. The main insight is to modify the loss function according to C, such that in expectation the loss matches that of the clean data. We refer to this approach as unbiased GAN. Concretely, when training the discriminator, we propose the following (modified) de-biased loss:</p><formula xml:id="formula_26">max D∈F E (x, y)∼ P X, Y y∈[m] (C -1 ) yy φ (D(x, y)) + E z∼N y∼P Y φ (1 -D(G(z; y), y)) .<label>(14)</label></formula><p>This is unbiased, as the first term is equivalent to E (x,y)∼P X,Y [φ(D(x, y))], which is the standard GAN loss with clean samples. However, such de-biasing is sensitive to the condition number of C, and can become numerically unstable for noisy channels as C -1 has large entries <ref type="bibr" target="#b19">[20]</ref>. For both the dataset, we use linear classifiers for permutation regularizer of the RCGAN-U architecture. label recovery accuracy Figure <ref type="figure" target="#fig_5">2</ref>: Noisy MNIST dataset: Our RCGAN models consistently improves upon all competing baseline approaches in generator label accuracy (left). The trend continues in label recovery accuracy (right), where our proposed RCGAN-classifiers improves upon unbiased classifier <ref type="bibr" target="#b33">[34]</ref>, which is one of the state-of-the-art approaches tailored for label recovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">MNIST</head><p>We train five architectures on MNIST dataset corrupted by the uniform flipping noise: RCGAN+y, RCGAN, RCGAN-U, unbiased GAN, and biased GAN. RCGAN+y architecture has the same architecture as RCGAN but the input to the first layer of its discriminator is concatenated with a one-hot representation of the label. We discuss our techniques to overcome the challenges involved in training RCGAN+y in Appendix L.</p><p>Conditional generators can be used to generate samples x from a particular class y, in the classes it learned. We then can use a pre-trained classifier f to compare y to the true class of the sample, f (x) (as perceived by the classifier f ). We compare the generator label accuracy defined as <ref type="figure" target="#fig_5">2</ref>, left panel. We generated 10k labels chosen uniformly at random and corresponding conditional samples from the generators, and calculated the generator label accuracy using a CNN classifier pre-trained on the clean MNIST data to an accuracy of 99.2%. The proposed RCGAN significantly improves upon the competing baselines, and achieves almost perfect label accuracy until a high noise of α = 0.3. RCGAN+y further improves upon RCGAN and to gain very high accuracy even at α = 0.125. The high accuracy of RCGAN-U suggests that robust training is possible without prior knowledge of the confusion matrix C. As expected, biased GAN has an accuracy of approximately 1α.</p><formula xml:id="formula_27">E y∼P Y ,Z∼N [I {y=f (G(z,y))} ], in Figure</formula><p>An immediate application of robust GANs is recovering the true labels of the noisy training data, which is an important and challenging problem in crowdsourcing. We propose a new meta-algorithm, which we call cGAN-label-recovery, which use any conditional generator G(z, y) trained on the noisy samples, to estimate the true label, as ŷ, of a sample x using the following optimization. </p><p>In the right panel of Figure <ref type="figure" target="#fig_5">2</ref> we compare the label recovery accuracy of the meta-algorithm using the five conditional GANs, on 500 randomly chosen noisy training samples. This is also compared to a state-of-the-art method <ref type="bibr" target="#b33">[34]</ref> for label recovery, which proposed minimizing unbiased loss function given the noisy labels and the confusion matrix. This unbiased classifier, was shown to outperforms the robust classifiers <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref> and can be used to predict the true label of the training examples. In Figures 5 of Appendix M, we show example images from all the generators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">CIFAR-10</head><p>In Figure <ref type="figure" target="#fig_6">3</ref>, we show the inception score <ref type="bibr" target="#b39">[40]</ref> and the label accuracy of the conditional generator for the four approaches: our proposed RCGAN and RCGAN-U, against the baselines Unbiased (Section 5) and Biased (Section 1) GANs trained using CIFAR-10 images <ref type="bibr" target="#b23">[24]</ref>, while varying the label accuracy of the real data under uniform flipping model. In RCGAN-U, even with the regularizer, the learned confusion matrix was a permuted version of the true C, possibly because a linear classifier might be too simple to classify CIFAR images. To combat this, we initialized the confusion matrix M to be diagonally dominant (Appendix L). In the left panel of Figure <ref type="figure" target="#fig_6">3</ref>, our RCGAN and RCGAN-U consistently achieve higher inception scores than the other two approaches. The Unbiased GAN is highly unstable and hence produces garbage images for large noise (Fig. <ref type="figure">6</ref>), possibly due to numerical instability of |||C -<ref type="foot" target="#foot_0">1</ref> ||| ∞ , as noted in <ref type="bibr" target="#b19">[20]</ref>. This confirms that robust GANs not only produce images from the correct class, but also produce better quality images. In the right panel of Figure <ref type="figure" target="#fig_6">3</ref>, we report the generator label accuracy (Section 5.1) on 1k samples generated by each GAN. We classify the generator images using a ResNet-110 model 1 trained to an accuracy of 92.3% on the noiseless CIFAR-10 dataset. Biased GAN has significantly lower label accuracy whereas the Unbiased GAN has low inception score. In Figure <ref type="figure">6</ref> in Appendix M, we show example images from the three generators for the different flipping probabilities. We believe that the gain in using the proposed robust GANs will be larger, when we train to higher accuracy with larger networks and extensive hyper parameter tuning, with latest innovations in GAN architectures, for example <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b40">41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Standard conditional GANs can be sensitive to noise in the labels of the training data. We propose two new architectures to make them robust, one requiring the knowledge of the distribution of the noise and another which does not, and demonstrate the robustness on benchmark datasets of CIFAR-10 and MNIST. We further showcase how the learned generator can be used to recover the corrupted labels in the training data, which can potentially be used in practical applications. The proposed architecture combines the noise adding idea of AmbientGAN <ref type="bibr" target="#b9">[10]</ref>, projection discriminator of <ref type="bibr" target="#b31">[32]</ref>, and regularizers similar to those in InfoGAN <ref type="bibr" target="#b10">[11]</ref>. Inspired by AmbientGAN <ref type="bibr" target="#b9">[10]</ref>, the main idea is to pair the generator output image with a label that is passed through a noisy channel, before feeding to the discriminator. We justify this idea of noise adding by identifying a certain class of discriminators that have good generalization properties. In particular, we prove that projection discriminator, introduced in <ref type="bibr" target="#b31">[32]</ref>, has a good generalization property. We showcase that the proposed architecture, when trained with a regularizer, has superior robustness on benchmark datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>y)∼ P X, Y [φ (D(x, y))] + E z∼N ,y∼ P Y [φ (1 -D(G(z; y), y))] ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: The output x of the conditional generator G is paired with a noisy label y corrupted by the channel C. The discriminator D estimates whether a given labeled sample is coming from the real data (x real , ỹreal ) or generated data (x, ỹ). The permutation regularizer h is pre-trained on real data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>1]} and identity map φ(a) = a leads to a total variation minimization; The loss minimized in the G-step is the total variation d TV ( P , Q) sup S∈X ×[m] { P (S) -Q(S)} between the two distributions with corrupted labels, up to some scaling and some shift. If we choose F = {D : X × [m] → [0, 1]} and φ(a) = log(a), then we are minimizing the Jensen-Shannon divergence d JS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4</head><label></label><figDesc>Our second architecture: RCGAN-U In many real world scenarios the confusion matrix C is unknown. We propose RCGAN-Unknown (RCGAN-U) algorithm which jointly estimates the real distribution P and the noise model C. The pre-processing and D steps of the RCGAN-U are the same as those of RCGAN, assuming the current guess M of the confusion matrix. As the G-step in (2) is not differentiable in C, we use the following reparameterized estimator of the loss, motivated by similar technique in training classifiers from noisy labels: min G∈G,M ∈C E z∼N y∼P Y φ M (G(z; y), y, D) + λ l(h * (G(z; y)), y) where C is the set of all transition matrices and φ M (x, y, D) = y∈[m] M y y φ(1 -D(x, y)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>classifier noise in the real data(1α)    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>ŷ 2 .</head><label>2</label><figDesc>∈ arg min y∈[m] min zy |||G(z y , y) -x||| 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Noisy CIFAR-10 dataset: Our RCGAN (red) and RCGAN-U (blue) consistently improves upon Unbiased (magenta) and Biased (black) GANs trained on noisy CIFAR-10 in inception scores (left) and in generator label accuracy (right).</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/wenxinxu/resnet-in-tensorflow</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is supported by NSF awards CNS-1527754, CCF-1553452, CCF-1705007, RI-1815535 and Google Faculty Research Award. This work used the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by National Science Foundation grant number OCI-1053575. Specifically, it used the Bridges system, which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC). This work is partially supported by the generous research credits on AWS cloud computing resources from Amazon.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>. This work used the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by National Science Foundation grant number OCI-1053575. Specifically, it used the Bridges system, which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC).</p><p>32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Panos</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Diamanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02392</idno>
		<title level="m">Representation learning and adversarial generation of 3D point clouds</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gan</forename><surname>Wasserstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Generalization and equilibrium in generative adversarial nets</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00573</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">GANs). arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Risteski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.10586</idno>
		<title level="m">Approximability of discriminators implies diversity in GANs</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">BEGAN: Boundary equilibrium generative adversarial networks</title>
		<author>
			<persName><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Schumm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10717</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><surname>Biau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cadre</surname></persName>
		</author>
		<author>
			<persName><surname>Sangnier</surname></persName>
		</author>
		<author>
			<persName><surname>Tanielian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07819</idno>
		<title level="m">Some theoretical properties of GANs</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Support vector machines under adversarial label noise</title>
		<author>
			<persName><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaine</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Laskov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="97" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Mikołaj</forename><surname>Bińkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dougal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><surname>Demystifying</surname></persName>
		</author>
		<author>
			<persName><surname>Gans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01401</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Bora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajil</forename><surname>Jalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03208</idno>
		<title level="m">Compressed sensing using generative models</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative models from lossy measurements</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Bora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName><surname>Ambientgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Info-GAN: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation of observer error-rates using the em algorithm</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawid</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><forename type="middle">M</forename><surname>Skene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied statistics</title>
		<imprint>
			<biblScope unit="page" from="20" to="28" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep generative image models using a Laplacian pyramid of adversarial networks</title>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Emily L Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improved training of Wasserstein GANs</title>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5769" to="5779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00734</idno>
		<title level="m">The relativistic discriminator: a key element missing from standard GAN</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Iterative learning for reliable crowdsourcing systems</title>
		<author>
			<persName><forename type="first">Sewoong</forename><surname>David R Karger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devavrat</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1953" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning from noisy singly-labeled data</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Khetan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04577</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName><forename type="first">Taeksoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moonsu</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungkwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05192</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Causal-GAN: Learning causal implicit generative models with adversarial training</title>
		<author>
			<persName><forename type="first">Murat</forename><surname>Kocaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriram</forename><surname>Vishwanath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.02023</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<title level="m">The mnist database of handwritten digits</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Dual motion GAN for future-flow embedded video prediction</title>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Zinan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Khetan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giulia</forename><surname>Fanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewoong</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><surname>Pacgan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04086</idno>
		<title level="m">The power of two samples in generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Building text classifiers using positive and unlabeled examples</title>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoli</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wee</forename><surname>Sun Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining, 2003. ICDM 2003. Third IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="179" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05637</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">cGANs with projection discriminator. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Latent space clustering in generative adversarial networks</title>
		<author>
			<persName><forename type="first">Sudipto</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Himanshu</forename><surname>Asnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sreeram</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><surname>Clustergan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03627</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning with noisy labels</title>
		<author>
			<persName><forename type="first">Nagarajan</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><forename type="middle">K</forename><surname>Inderjit S Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambuj</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1196" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Plug &amp; play generative networks: Conditional iterative generation of images in latent space</title>
		<author>
			<persName><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00005</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09585</idno>
		<title level="m">Conditional image synthesis with auxiliary classifier gans</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05396</idno>
		<title level="m">Generative adversarial text to image synthesis</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Solving approximate Wasserstein GANs to stationarity</title>
		<author>
			<persName><forename type="first">Maziar</forename><surname>Sanjabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meisam</forename><surname>Razaviyayn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08249</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Himanshu</forename><surname>Asnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Rahimzamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sreeram</forename><surname>Kannan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09708</idno>
		<title level="m">Mimic and classify: A meta-algorithm for conditional independence testing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenda</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Bharath K Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gert</forename><forename type="middle">Rg</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><surname>Lanckriet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0901.2698</idno>
		<title level="m">On integral probability metrics, φ-divergences and binary classification</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning kernel perceptrons on noisy data using random projections</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Stempfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liva</forename><surname>Ralaivola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Algorithmic Learning Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="328" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning SVMs from sloppily labeled data</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Stempfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liva</forename><surname>Ralaivola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="884" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2080</idno>
		<title level="m">Training convolutional networks with noisy labels</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Dougal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Yu</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumyajit</forename><surname>Strathmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaditya</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ramdas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><surname>Gretton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04488</idno>
		<title level="m">Generative models and model criticism via optimized maximum mean discrepancy</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06759</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="613" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning a probabilistic latent space of object shapes via 3D generative-adversarial modeling</title>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.09700</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Robust GANs against dishonest adversaries. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yian</forename><surname>Teck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh</forename><forename type="middle">N</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName><surname>Do</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.07539</idno>
		<title level="m">Semantic image inpainting with perceptual and contextual losses</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><surname>Stackgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Comput. Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">On the discrimination-generalization tradeoff in GANs</title>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02771</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Generative visual manipulation on the natural image manifold</title>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10593</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
