<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CHARM: Composing Heterogeneous AcceleRators for Matrix Multiply on Versal ACAP Architecture</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jinming</forename><surname>Zhuang</surname></persName>
							<email>jinming.zhuang@pitt.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jason</forename><surname>Lau</surname></persName>
							<email>lau@cs.ucla.edu</email>
						</author>
						<author>
							<persName><forename type="first">Hanchen</forename><surname>Ye</surname></persName>
							<email>hanchen8@illinois.edu</email>
						</author>
						<author>
							<persName><forename type="first">Zhuoping</forename><surname>Yang</surname></persName>
							<email>zhuoping.yang@pitt.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yubo</forename><surname>Du</surname></persName>
							<email>yubo.du@pitt.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jack</forename><surname>Lo</surname></persName>
							<email>jack.lo@amd.com</email>
						</author>
						<author>
							<persName><forename type="first">Kristof</forename><surname>Denolf</surname></persName>
							<email>kristof.denolf@amd.com</email>
						</author>
						<author>
							<persName><forename type="first">Stephen</forename><surname>Neuendorffer</surname></persName>
							<email>stephen.neuendorffer@amd.com</email>
						</author>
						<author>
							<persName><forename type="first">Alex</forename><surname>Jones</surname></persName>
							<email>akjones@pitt.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jingtong</forename><surname>Hu</surname></persName>
							<email>jthu@pitt.edu</email>
						</author>
						<author>
							<persName><forename type="first">Deming</forename><surname>Chen</surname></persName>
							<email>dchen@illinois.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jason</forename><surname>Cong</surname></persName>
							<email>cong@cs.ucla.edu</email>
						</author>
						<author>
							<persName><forename type="first">Peipei</forename><surname>Zhou</surname></persName>
							<email>peipei.zhou@pitt.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Advanced Micro Devices Inc</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Advanced Micro Devices Inc</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">Advanced Micro Devices Inc</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff11">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff12">
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff13">
								<address>
									<settlement>Monterey</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff14">
								<address>
									<addrLine>12 pages</addrLine>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CHARM: Composing Heterogeneous AcceleRators for Matrix Multiply on Versal ACAP Architecture</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3543622.3573210</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Heterogeneous Architecture</term>
					<term>Domain-Specific Accelerator</term>
					<term>Versal ACAP</term>
					<term>Mapping Framework</term>
					<term>Matrix-Multiply</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dense matrix multiply (MM) serves as one of the most heavily used kernels in deep learning applications. To cope with the high computation demands of these applications, heterogeneous architectures featuring both FPGA and dedicated ASIC accelerators have emerged as promising platforms. For example, the AMD/Xilinx Versal ACAP architecture combines general-purpose CPU cores and programmable logic with AI Engine processors optimized for AI/ML. An array of 400 AI Engine processors executing at 1 GHz can provide up to 6.4 TFLOPs performance for 32-bit floating-point (fp32) data. However, machine learning models often contain both large and small MM operations. While large MM operations can be parallelized efficiently across many cores, small MM operations typically cannot. We observe that executing some small MM layers from the BERT natural language processing model on a large, monolithic MM accelerator in Versal ACAP achieved less than 5% of the theoretical peak performance. Therefore, one key question arises: How can we design accelerators to fully use the abundant computation resources under limited communication bandwidth for end-to-end applications with multiple MM layers of diverse sizes?</p><p>We identify the biggest system throughput bottleneck resulting from the mismatch of massive computation resources of one monolithic accelerator and the various MM layers of small sizes in the application. To resolve this problem, we propose the CHARM framework to compose multiple diverse MM accelerator architectures working concurrently towards different layers within one application. CHARM includes analytical models which guide design space exploration to determine accelerator partitions and layer scheduling. To facilitate the system designs, CHARM automatically generates code, enabling thorough onboard design verification. We deploy the CHARM framework on four different deep learning</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Dense matrix multiply (MM) serves as one of the most heavily used kernels in many deep learning workloads, including BERT <ref type="bibr" target="#b0">[1]</ref> for natural language processing, NCF <ref type="bibr" target="#b1">[2]</ref> for recommendations, ViT <ref type="bibr" target="#b2">[3]</ref> for vision classification, and MLP <ref type="bibr" target="#b3">[4]</ref> for multilayer perceptron classification or regression. According to profiling results from Google <ref type="bibr" target="#b4">[5]</ref>, dense matrix multiply tasks occupied 90% of Neural Network (NN) inference workload in Google's data center in 2017. The increasing complexity of these applications leads to extreme demands for computation and data movement.</p><p>According to <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>, the off-chip bandwidth has been a bottleneck for both the performance and energy efficiency of a system and a common trend on current platforms is that the off-chip bandwidth does not scale as fast as the computation resources. Therefore, Two Diverse Accs the first research question arises: How to sustain the faster scaling computation with the slower scaling off-chip bandwidth? A common solution is to increase data reuse by allocating more on-chip storage within an accelerator (acc). As shown in asymptotic analysis in <ref type="bibr" target="#b8">[9]</ref>, the total off-chip communication volume in MM scales as O ( <ref type="formula">1</ref>? ?</p><p>) where M is the on-chip tile size. If we increase the tile size, we can reduce the total communication volume, therefore reducing the pressure on the off-chip bandwidth.</p><p>In this work, we target on the AMD/Xilinx Versal ACAP architecture <ref type="bibr" target="#b9">[10]</ref>, which combines general-purpose CPU cores and programmable logic (PL) with AI Engine processors (AIE) optimized for AI/ML computation. For example, we implemented an MM accelerator on an AMD/Xilinx VCK190 board using 384 AIEs and over 80% on-chip URAM and BRAM resources. The red line in Figure <ref type="figure" target="#fig_0">1</ref> illustrates the performance of this accelerator. This design operates on a native tile size of 1536?128?1024 and achieves 2.8 TFLOPs throughput when carrying a tiled execution of a large square MM (point A). However, when simply mapping different sizes of MM to such a design, the performance decreases significantly as the square MM size drops below 512, since each tile is padded to the native tile size of the accelerator. For instance, at point B, the performance of such a monolithic design goes to 0.41 GFLOPs, which is 6880? lower than point A. Although padding is a common and simple approach to implementing small MM operations on a large accelerator, padding can waste both computation and bandwidth.</p><p>An alternative to padding is implementing multiple accelerators with smaller native tile sizes, potentially executing different tasks on each accelerator in parallel <ref type="bibr" target="#b10">[11]</ref>. We apply this approach using eight independent accelerators with a native tile size of 256?128?256, illustrated by the blue dash line in Figure <ref type="figure" target="#fig_0">1</ref>. For small square MM operations with size 64, this approach achieves 7.2 GFLOPS at point C, approximately 17? speedup compared to point B.</p><p>However, the smaller accelerator size also means less data reuse for large MM, with total throughput almost saturation when the operation size is larger than 256. When the MM size is 3072 (point D), the total throughput from eight duplicate accs is 4.08? smaller than point A in one monolithic design.</p><p>These experiments expose two conflicting design goals. Firstly, we want to implement large MM operations with sufficient data reuse to achieve the highest possible performance on the devices. Secondly, we want to implement small MM operations while minimizing computation and communication overheads. Neither of these simple designs seems able to achieve these design goals simultaneously. Therefore, the second research question arises: How to trade-off between the two design goals for real-world, end-to-end applications where MM layers with large and small sizes coexist?</p><p>To illustrate how these conflicting design goals can affect the performance of practical machine learning models, we consider BERT <ref type="bibr" target="#b0">[1]</ref> as a representative workload containing MM layers with both large and small sizes. In a transformer layer of BERT, there are a total of 8 types of MM kernels where Kernels 0-5 are large MMs and Kernels 6 and 7 are batch dots, i.e., small MMs. The detailed shapes can be referred to Table <ref type="table">5</ref>. Take Kernel 5 and Kernel 6 as examples, Kernel 5 is an MM with the shape 3072?1024?4096, Kernel 6 is a batch dot with the shape 96?512?512?64, which means there are 96 small independent MMs sized at 512?512?64.</p><p>As shown in Figure <ref type="figure" target="#fig_1">2</ref>, when using one monolithic MM accelerator, Kernels 0-5 consume 92% of the total BERT MM computation operations and 12% of the total MM acc time. In contrast, Kernels 6-7 consume 8% of the total operations but take 88% of the total MM acc time. For Kernels 0-5, they lie in Region A (a region that performs similarly to Point A in Figure <ref type="figure" target="#fig_0">1</ref>), where the throughput of acc is more than 2082 GFLOPS. For Kernel 6-7, they lie in Region B, where the throughput of acc is only 23.6 GFLOPS. Given there is a large portion of acc execution underutilized in the timeline, the overall MM acc throughput is only 276 GFLOPS. Can we achieve a design for BERT that lies in region A, i.e., good for large MMs, and also in a region better than point C, i.e., good for small MMs with less or no waste computation/bandwidth? Our answer is "Yes". The key idea is to allocate more portion of the resources to accs dedicated to computing larger MMs and a smaller portion of the resources to other accs to compute smaller MMs at the same time, as shown in Figure <ref type="figure" target="#fig_1">2</ref> where a two-diverse accs system is illustrated. To achieve our design goals, we need to solve these new challenges. First, we need to achieve high computation utilization for every single acc, i.e., use the smaller acc(s) to reduce the waste for small MMs and use the larger acc(s) to maximize the data reuse for large MMs. Second, to maximize overall utilization while maintaining high throughput and low latency, we need to carefully overlap the execution time for these accs by cooptimizing workload and resource partitioning. Third, to facilitate the design space explorations (DSE), we need analytical models to optimize the overall throughput under resource and bandwidth constraints. Fourth, to reduce the programming efforts for the system implementation, we need automatic code generation. Fifth, to resolve the dependency of the kernels within the application graph when running multiple accs we need an accelerator runtime to schedule kernels from different tasks onto the accs.</p><p>To answer the research questions, we propose the CHARM architecture and its corresponding automation framework, the CHARM framework. Our contributions are summarized below: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRIOR WORK</head><p>To achieve high throughput and energy efficiency, NN accelerators usually employ a large number of processing elements (PE) and share a similar memory hierarchy. That is, while the big bulk of data is stored in the off-chip memory, there are multiple levels of on-chip buffers, including the local memory attached to each PE and global shared memory, to further reduce the costly data movement from/to off-chip memory. Several works contribute to NN accelerators by discussing the data reuse opportunities, computation parallelism, and the choice of dataflow. However, many of the prior works apply a one-size-fits-all monolithic design that cannot efficiently handle layers with huge differences in shapes and sizes (Eyeriss <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, ShiDiannao <ref type="bibr" target="#b13">[14]</ref>, NPU <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> and others <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>). AutoSA <ref type="bibr" target="#b21">[22]</ref> is a polyhedral-based compilation framework that generates monolithic systolic array designs for dense matrices. Sextans and Serpens <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> are general-purpose monolithic accelerators for sparse matrices. <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> analyze layout and pipeline efficiency. Other works like AMD DPU <ref type="bibr" target="#b10">[11]</ref>, Mocha <ref type="bibr" target="#b26">[27]</ref> explore task-level parallelism by allocating multiple duplicate accs on the device without </p><formula xml:id="formula_0">? ? ? ? ? Herald [32] ? ? ? ? ? CHARM (Ours) ? ? ? ? ?</formula><p>specializing each acc. DNNBuilder <ref type="bibr" target="#b27">[28]</ref> designs a dedicated acc for each layer according to the number of operations within the layer. DNNExplorer <ref type="bibr" target="#b28">[29]</ref> enhances DNNBuilder by combining dedicated accs for the first several layers and a monolithic acc for the rest of the layers. While it employs multiple accelerators, it lacks a comprehensive exploration of workload assignments. TETRIS <ref type="bibr" target="#b29">[30]</ref> and TANGRAM <ref type="bibr" target="#b30">[31]</ref> propose multiple dataflow optimizations within and across the NN layers to improve performance and energy efficiency. Although they offer diverse accelerator designs, they lack the DSE and workload assignment for high overall throughput. Herald <ref type="bibr" target="#b31">[32]</ref> proposes an architecture with multiple diverse accelerators and explores the workload assignment and resource partition. Still, they choose several existing acc designs from their candidate pool, e.g., ShiDiannao <ref type="bibr" target="#b13">[14]</ref>, NVDLA <ref type="bibr" target="#b32">[33]</ref> without doing DSE for each acc. FPCA <ref type="bibr" target="#b33">[34]</ref> and CHARM'12 <ref type="bibr" target="#b34">[35]</ref> propose a fully pipelined and dynamically composable coarse-grained reconfigurable architecture and compose loosely coupled accelerators for different kernels within an application via permutation network, which costs high in chip area.</p><p>In conclusion, we summarize the differences between our work and prior works in Table <ref type="table" target="#tab_1">1</ref>. Our work is capable of choosing the design from one monolithic, multiple duplicates, and multiple diverse accelerators, and each accelerator is a specialized design considering the different workload assignments, dataflow, and data parallelism strategies that are covered by our DSE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VERSAL ACAP ARCHITECTURE OVERVIEW</head><p>In this section, we first introduce the system architecture of AMD/ Xilinx Versal ACAP architecture in Section 3.1 and then the memory model of AIE array in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Versal ACAP Architecture</head><p>Figure <ref type="figure" target="#fig_2">3</ref> illustrates the overall architecture of the VCK190 <ref type="bibr" target="#b35">[36]</ref> board and highlights the AIE array on the top. The VCK190 board features (1) the first-generation AIE architecture, which has 8 ? 50 1 GHz 7-way VLIW processors supporting vector operations up to 1024 bits <ref type="bibr" target="#b36">[37]</ref>, (2) ARM processors to run Linux and general-purpose applications, and (3) PL to design application-specific hardware with Digital Signal Processors (DSP) available for integration. The AI engine cores and ARM CPUs can be programmed with C/C++ code, while PL can be programmed using both RTL and C/C++ code using High-Level Synthesis (HLS) <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>. These three  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">AIE Memory Model</head><p>Each AIE processor tile contains 32 KB of data and is capable of sharing data with the adjacent AIEs in four directions (AIE?neighbor AIE). In addition to local memory shared with adjacent tiles, each AIE tile also connects to an AXI-Stream (AXIS) switch network, which enables non-local communication between AIE processors (AIE?non-local) and communication with the PL through the PLIOs in the 39 interface tiles (PL?AIEs). The VCK190 board provides 1.2 TB/s (PL?AIEs) / 0.9 TB/s (AIEs?PL) bandwidth between PL and AIEs, which is 46? more than the bandwidth between DDR4 and PL. The AXIS switches support both circuit-switched and packet-switched connections between ports. Circuit-switched connections provide dedicated, deterministic communication and support broadcast, where data from a single input channel is transmitted to multiple output channels simultaneously. Packet-switched connections allow data from an input channel to be dynamically routed to different destinations based on a destination header at the start of each packet. This enables data flows to be time-multiplexed on a single routing path. One situation in which we can use packetswitched connections happens when the computation-to-communication (CTC) ratio of an AIE is more than one. During the computation of AIE 0, the port assigned to this AIE is idle and thus can be used to transfer data to another AIE, say AIE 1, by assigning a different header that matches the destination ID of AIE 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CHARM SINGLE ACCELERATOR DESIGN</head><p>In this section, we describe the dataflow and mapping strategy for a single MM acc using hundreds AIEs in Section 4.1. Then, in Listing 1: Pseudocode of MM loop tiling and dataflow.  </p><formula xml:id="formula_1">AI Engine Packet-Switch Broadcast Broadcast D 0 Packet Switch D 0 D 1 D 2 D 3 D' 0 D' 1 D' 2 D' 3 Scatter Gather Combined D 0 T 1 T 2 T 3 D 0 D 0 D 0 T 0 D 1 D 1 D 1 D 1 D 2 D 2 D 2 D 2 D 3 D 3 D 3 D 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data Reuse in Multiple Levels</head><p>When designing each acc, we adopt a bottom-up strategy and explore data reuse at each level. Firstly, at a single AIE level, we make full use of the seven-way VLIW capability of the AIE vector processor to achieve fully pipelined MAC operations by reusing the AIE local register and the local memory. Secondly, at the PL?AIEs level, when feeding data to tens or hundreds of AIEs, as the number of PLIOs connecting the AIE array and PL is much smaller than the total number of AIE cores, we reduce the number of required PLIO by exploring the data broadcast and packet-switch (described in Section 3.2) mechanism. Figure <ref type="figure" target="#fig_3">4</ref> shows how we reuse one single PLIO port by combing broadcast with packet-switch. Assume that we have a 4?4 AIE array that calculates an MM with size 1?4?4 (1 MAC/AIE), and it takes one cycle for one AIE to get the left-hand-side (LHS) and the right-handside (RHS) operands and four cycles to finish one multiplication which makes the CTC ratio equal to 4. By leveraging the data reuse opportunity in MM (e.g., the row of LHS can be reused by different columns of RHS), we can broadcast the first data from LHS to the first row of AIE arrays at Time 0 utilizing one PLIO port as shown in solid lines. At Time 1, by specifying a different destination header, we can transfer the second data of LHS to the second row of the AIE array by reusing the same PLIO port. At Time 2 and Time 3, the third and fourth data of LHS are sent to the third and fourth rows of AIEs. At Time 4, the first row of AIEs finishes the computation, and the PLIO completes the data transfer to the fourth row of AIEs. Therefore, in this case, we can use one PLIO port to send LHS data to 16 AIEs without any performance degradation.</p><p>Thirdly, in PL?DDR, we further allocate three sets of on-chip buffers for each acc to store the LHS, the RHS, and the output matrices so that a tile of LHS with size (X?A?TI) ? (Y?B?TK) can be reused on-chip for (Z?TJ) times. The buffer size and reuse rate for RHS and output matrices can be calculated in the same way. Besides, the double-buffering technique is applied to three buffers to overlap the off-chip data movement with the computation. By greatly exploring the data reuse opportunities at multiple levels, our system can sustain high computation efficiency under limited off-chip bandwidth, i.e., 25.6 GB/s of DIMM-DDR4 on VCK190.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CHARM ARCHITECTURE AND CHARM FRAMEWORK TO COMPOSE MULTIPLE DIVERSE ACCELERATORS</head><p>In this section, we introduce the CHARM architecture in Section 5.1 and CHARM framework overview in Section 5.2. We then discuss each module within the framework from Section 5.3 to Section 5.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">CHARM Architecture</head><p>Figure <ref type="figure">5</ref> illustrates the CHARM architecture with one or more diverse MM accs in the system and other kernel accs for non-MM kernels within an end-to-end deep learning application. We partition the AIE array for multiple MM accs (two in this example). For each MM acc, we design a specialized DMA module that contains the data transferring control logic and on-chip buffer according to the tiling strategy. The different AIE partitions communicate with their corresponding DMA modules through the PLIO interface and NOC. We refer to the AIE array, its corresponding PLIO, For these communication-bound kernels, the design goal is to achieve near-peak off-chip bandwidth. When running these kernels, as they consume all the off-chip bandwidth, we choose to sequentially launch these non-MM and communication-bound kernels before or after MM acc(s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">CHARM Framework Overview</head><p>We illustrate the proposed CHARM framework overview in Figure <ref type="figure">6</ref>. The CHARM framework takes the application model, platform off-chip bandwidth profiling, and platform hardware resource constraints as input, performs automated optimization and code generation, and launches backend compilers to generate the readyto-run binaries as output. There are several modules in the CHARM framework: (1) On top of CDSE, CDAC finds the optimal design with the highest throughput and outputs the configurable design parameters for each acc. It also generates a runtime config file that specifies which acc should be called for a certain kernel. (2) CACG takes the configurable parameters generated from CDAC as input, implements the design, and generates all the needed source code files for AIEs, PL, and host CPUs. CHARM calls the corresponding backend tools to generate both the hardware bitstream and host binaries.</p><p>(3) CRTS takes the runtime config files from CDAC and kernel dependency graph as input and schedules the kernels in the task pools onto available accs. </p><formula xml:id="formula_2">max Throughput = M ? K ? N ? 2/TIME (1) s.t.? ? ? ? ? ? AIE num (<label>2</label></formula><formula xml:id="formula_3">)</formula><formula xml:id="formula_4">Port in ? PLIO in , Port out ? PLIO out<label>(3)</label></formula><formula xml:id="formula_5">Buff ? On_chip RAM<label>(4)</label></formula><p>AIE-Array Tiling Selection. Since {A, B, C} are fully unrolled and mapped to the AIE Array, the multiplication of the unroll factors A, B, and C should be less than or equal to the total number of AIEs in Equation <ref type="formula" target="#formula_2">2</ref>. The number of packet-switch ports is determined by {A, B, C} and the I/O reuse mechanism described in Section 4.2. They should meet the input and output PLIO resource constraints. The input and output PLIO numbers can be obtained by:</p><formula xml:id="formula_6">Port ?? = ?? ? ?/CTC ? + ?? ? ?/CTC ? Port ??? = ?? ? ?/CTC ?<label>(5)</label></formula><p>PL Tiling Selection. On-chip PL buffers are allocated in order to amortize the 46x bandwidth gap between off-chip to PL and PL to AIE-Array by increasing the data reuse rate. Equation <ref type="formula" target="#formula_7">6</ref>shows the size of LHS, RHS, output buffers, and their off-chip to on-chip communication time. BPD refers to bytes per data and BW_L,R,O are the off-chip bandwidth measured from bandwidth profiling.</p><formula xml:id="formula_7">Buff L = (? ? ? ? ? ? ) ? (? ? B ? TK) ? BPD Buff R = ? ? ? ? ? ? ? ? TK ? TJ ? BPD Buff O = ? ? ? ? ? ? ? ? TI ? TJ ? BPD Buff = 2 ? (Buff ? + Buff ? + Buff ? ) Time L,R,O = Buff ?,?,? /BW ?,?,?<label>(6)</label></formula><p>Performance Modeling. To calculate the overall execution time, the scheduling of data communication between off-chip to on-chip and the AIE array computation should be considered. The computation time for all the on-chip time loops, i.e., Line 6 in Listing 1, can be defined by Equation <ref type="formula" target="#formula_8">7</ref>in which MAC represents the theatrical MAC operation that one AIE engine can do in one cycle, and Eff refers to the real efficiency that the computation kernel achieves. We consider both single AIE and AIE array pipeline efficiency (PL?AIE) here and assign the overall efficiency to 80%.</p><p>For the off-chip to on-chip scheduling, as described in Listing 1, the loop order of the outermost loop is TY?TZ?TX, thus, the memory access time for LHS and RHS will happen TX?TX?TZ times in total. The overall execution TIME can be calculated by Equation <ref type="formula" target="#formula_9">8</ref>. This is an equation for illustration purposes where we leave out the details on the formulation of time spent storing the output and prologue and epilogue time in the pipeline.</p><formula xml:id="formula_8">Time_comp = (? ? ? ? ? ? ? ? ? ? ? ? ? ? /MAC)/Eff<label>(7)</label></formula><formula xml:id="formula_9">TIME = ??? ( [Time L , Time R , Time_comp]) ?(? ? ? ? ? ? ? ? )<label>(8)</label></formula><p>For any specific shape(s), all the possible configurable parameters will be evaluated in an exhaustive fashion. After CDSE, top-ranked optimized design points will be reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">CHARM Diverse Acc. Composer (CDAC)</head><p>Two-step search algorithm in CDAC. To achieve overall optimized throughput when mapping diverse sizes of MM kernels on multiple accs, we propose a sort-based two-step algorithm in CDAC. In the first step of CDAC, we partition the MM kernels of different workloads within an input model to multiple groups. The number of groups equals the number of diverse accs, which is a hyperparameter in CDAC. After the workload partition, in the second step, we generate a resource partition candidate that specifies the resource budget for each accelerator to be proportional to the total number of operations from the assigned MM kernel(s). Under the assigned workload and assigned resource, we search all valid candidates of configurable parameters (A,B,C,X,Y,Z) for each accelerator. We then fine-tune the memory resource partition to generate more resource partition candidates. After the memory fine-tuning, we generate a new workload partition and redo the resource partition and configurable parameter search which further optimize the system throughput of all the accs. We discuss the details of each step as follows. <ref type="table" target="#tab_1">1st</ref> Step: Workload Assignment. To improve the overall throughput of the diverse acc architecture, we need to properly assign the MM kernels to the accs and make them work concurrently with a similar execution time. However, mapping an application with n kernels to num accs suffers from the exponential time complexity as the total mapping search space scales as O (??? ? ) . To better scale larger models that contain more kernels, i.e., a larger n, we propose a sort-based algorithm to partition the workload with reduced time complexity as O ( ?-1 ???-1 ) = ? ?-1 ???-1 . As shown in Algorithm 1, CDAC first sorts the different shapes of the MM kernels by their number of operations (Line 4) so that MMs with larger and smaller sizes can be properly divided. Then we divide the sorted MM kernels into n groups (Lines 5-6). For example, if there are eight different shapes of kernels that need to be mapped to num=2 accs, after sorting the kernels, we put one separator between any two kernels to separate all kernels into two groups. In total, it gives us including the number of AIEs, PLIO, on-chip RAM, and off-chip bandwidth. To minimize the maximum execution time of all the accs, CDAC assigns the number of AIEs and PLIO constraints proportional to the total number of operations assigned to the acc (Lines 7-8). For the number of on-chip RAMs, we first evenly distribute it (Line 2). After sequentially launching CDSE to find the configuration of every acc once (Lines 9-10), we apply a memory fine-tuning step to optimize the memory allocation. It finds the index of the acc that consumes the most time (Line 12) and then tries to explore a better configuration by increasing the memory allocation of this acc while decreasing the memory allocation of others'(Line 13-14). If a better result is found, we update the global optimal execution cycles and corresponding acc configuration settings (Line 15-18). Note that, in the current model, we assume each acc evenly occupies the off-chip bandwidth (Line 1) and leave the discussion of the off-chip bandwidth partition for future work.</p><formula xml:id="formula_10">? 8-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">CHARM Auto. Code Generation (CACG)</head><p>After finding the hardware design parameters of optimized designs from CDAC, we implement CACG, including AIEGen, PLGen, and HostGen, to generate the corresponding source code for AIEs, PL, and host CPU. AIEGen takes the tiling factor of a single AIE (TI,TK,TJ) and AIE Array (A,B,C) as input and instantiates the corresponding number of AIE cores. It leverages the C++-based Adaptive Data Flow (ADF) Graph API <ref type="bibr" target="#b43">[44]</ref> to build connections among AIE cores through the AXI network and connections between AIE Array and PL through PLIOs. Using the PL level (X,Y,Z) design parameters, PLGen generates HLS C/C++ code that allocates on-chip buffers on the PL side and implements the data transferring modules for sending/receiving data to/from the AIE array. HostGen emits the Xilinx runtime library (XRT) API-based host code.</p><p>After code generation, CHARM launches the vendor tools, including the AIE compiler and the V++ compiler to generate the output object files libadf.a and kernel.xo which are linked into one xclbin, i.e., the hardware bitstream of the design. The GCC compiler compiles XRT-API-based host code to host program runs on the ARM CPU for kernel scheduling and system controls. To achieve high throughput while maintaining relatively low latency under dependency constraints within each task, we propose CRTS that runs on the ARM CPU during runtime. Algorithm 2 lists the scheduler algorithm. It takes the dependency graph, number of accelerators, and layer assignment configuration file generated by CDAC as input. There are two parallel processes in CRTS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">CHARM Runtime Scheduler (CRTS)</head><p>The first process keeps tracking to check if there are any idle accs we can assign tasks to (Lines 2-3). CRTS traverses the layers assigned to this acc following a first-in-first-out principle (Lines 5-6). If the layer is still in the task pool, it means that it has not been issued. Suppose all the preceding layer(s) of the current layer have been executed, i.e., dependency resolved. In that case, CRTS assigns this valid layer to the corresponding acc (Lines 7-8) and continues to track other accs (Line 9). The second process keeps track of the status of every acc to see if it has finished the workload (Lines 12-13) and updates the task pool according to the dependency graph, as well as changing the status of the acc (Line 14) to idle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENT RESULTS</head><p>In this section, we first illustrate the single AIE efficiency and single MM acc throughput in Section 6.1 and 6.2. In Section 6.3, we implement different CHARM designs, including one monolithic MM acc, one specialized MM acc, two-diverse MM accs, and eight-duplicate MM accs for four applications: BERT, ViT, NCF, and MLP. All the experiments are conducted on VCK190 with 230MHz on PL and 1GHz on AIE. AMD/Xilinx Vitis version 2021.1 is used as the compilation backend tool. When measuring the power consumption, we iterate each application for more than 60s and report the average value by employing the board evaluation and management tool, AMD/Xilinx BEAM <ref type="bibr" target="#b44">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Single AIE Kernel Efficiency Comparison</head><p>In this section, we showcase our single AIE MM computation efficiency under different matrix sizes for fp32. We leverage the AIE intrinsics <ref type="bibr" target="#b45">[46]</ref> to program the single kernel design and obtain the   <ref type="bibr" target="#b47">[48]</ref>, our single kernel obtains 2.26? average efficiency gain. For the whole system design, we choose 32?32?32 as our single kernel as it achieves high computation efficiency and the total size of LHS, RHS and output matrices are within 16 KB so that they fit in the AIE local memory and can be double buffered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Performance for Square MMs on One Monolithic Accelerator</head><p>We evaluate the throughput of one monolithic acc design and compare the performance between the modeling estimation from CDSE and the on-board measurement. We build the monolithic design by using 384 AIEs and over 83% of on-chip RAM utilization with the AIE running at 1GHz and the PL side at 230MHz. As shown in Table <ref type="table" target="#tab_9">3</ref>, the throughput of the one-acc monolithic design rises as the square MM size increases. While it achieves 3.27 TFLOP/s at size 6144, the throughput at size 64 is only 0.41 GFLOPs. CHARM CDSE is capable of precisely estimating the on-board execution time with an average estimation error rate of only 2.9%. We compare the throughput of the same MM application implemented only in the PL part of VCK190 using the state-of-the-art systolic-array-based framework AutoSA <ref type="bibr" target="#b21">[22]</ref> for fp32 data type. The PL side of VCK190 is featured with 1968 DSP58 IPs. Instead of using five DSP48 to calculate the floating point multiplication in the previous generation board, it only consumes one DSP58.</p><p>As shown in Table <ref type="table" target="#tab_10">4</ref>, the CHARM single MM acc achieves 3.27 TFLOPs throughput, 5.54? throughput and 1.93? energy efficiency gains compared to the PL-only design on VCK190.    the designs of the same application use the same non-MM kernels. Table <ref type="table" target="#tab_11">7</ref> reports the on-board throughput and power consumption under different acc configurations for all the four applications.</p><p>CHARM achieves 1.46 TFLOPs, 1.61 TFLOPs, 1.74 TFLOPs, and 2.94 TFLOPs maximum throughput for the MMs in BERT, ViT, NCF, MLP. Table <ref type="table">6</ref> shows the time breakdown for the MM, the layernorm, the softmax, and the transpose for each end-to-end application. We highlight the best design(s) for each application in Table <ref type="table" target="#tab_11">7</ref>. For BERT and ViT, the two-diverse MM accs designs are the best, whereas for NCF and MLP, one-acc designs are the best. This is because BERT and ViT have both large and small MMs whereas MLP only has large MMs. NCF also has both large and small MMs. However, small MMs consume less than 0.8% of the total computation, and designs favoring the large MMs stand out as the best. The eight-duplicate designs are inferior for all the applications due to insufficient data reuse for each acc.</p><p>For BERT and ViT, when compared to one monolithic design, the customization of using one specialized acc design for a specific application provided by CHARM gives 2.13?, 5.08? gain on energy efficiency (GFLOPs/W), respectively. The additional design spaces explored by using more than one-acc, with heterogeneous and diverse shaped accs provided by CHARM framework, give us 2.25?, 5.24? extra energy efficiency gains for BERT and ViT, respectively. These gains demonstrate the innovative design methodology of CHARM, i.e., composing heterogeneous accelerators.</p><p>We show the implementation layout of the two-diverse MM acc design, i.e., the best design for BERT, in Figure <ref type="figure" target="#fig_5">7</ref>. This is also the layout corresponding to Figure <ref type="figure">5</ref> that contains two MM accs and four non-MM communication-bound accs. The hardware resource utilization for each acc is reported in Table <ref type="table" target="#tab_12">8</ref>. The MM acc 0 provides high data reuse and computation efficiency when calculating large MMs by utilizing 256 AIEs, 53.26% BRAM, and 55.29% URAM. The MM acc 1 utilizes 32 AIEs, 1.96% BRAM, and 3.46% URAM which provides the needed computation and communication without resource over-provisioning for small MMs in BERT. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION OF ARCHITECTURE INSIGHT AND MAPPING INSIGHT</head><p>By leveraging the strong modeling capability provided by the CHARM framework, we explore performance under different hardware architecture changes (number of AIEs, on-chip storage size, off-chip bandwidth) to do pre-silicon architecture explorations and provide architecture design insights that could be helpful for future generation devices. Here, we leverage the CHARM modeling to report throughput for different acc configurations including 1-, 2-, 4-, and 8-accs in the system. For each design (except one-acc), we have two variants, duplicate accs or diverse accs. The explorations help us to understand the following research questions: Q1: Can we benefit from higher off-chip bandwidth? A1: Yes. Versal needs higher off-chip bandwidth.</p><p>We first explore the performance, assuming the platform has more off-chip bandwidth. We increase the DDR bandwidth by 4? to simulate multiple DDR banks and by 16? to simulate the case when we have a high bandwidth memory (HBM). As shown in Figure <ref type="figure">9</ref>, the throughput from the best design for BERT in each bandwidth configuration rises from 1.48 TFLOPs to 3.34 TFLOPs with 4? bandwidth and to 4.80 TFLOPs with 16? bandwidth. The improvement from 1? to 4? DDR is within expectation and implies that the  designs for BERT are bounded by off-chip bandwidth. The maximum throughput for 16? is bounded by the system computation throughput as 4.8 TFLOPs, which is constrained by single kernel computation efficiency (95%) and PL?AIE efficiency(85%). Another observation from Figure <ref type="figure">9</ref> is that the throughput improvement of multiple accs is larger than that of the single acc since when the number of accs increases, each acc has less data reuse and tends to be more bounded by the off-chip bandwidth. Q2: Can we leverage CHARM in future architectures? A2: Yes. The last group in Figure <ref type="figure" target="#fig_7">10</ref> implies that as the computation and communication parallelism further increases in the future, there is a need for more heterogeneous accelerator architectures and CHARM can serve as one of the most promising solutions.</p><p>We explore the performance by varying the number of AIEs, onchip RAM, and off-chip bandwidth. We reduce the number of AIEs to 1/8 of the current AIE array size to simulate the computation capacity of the previous generation FPGA where only PL is equipped with DSPs and has about 1/8 of the theoretical fp32 peak performance of Versal ACAP. As shown in the first group in Figure <ref type="figure" target="#fig_7">10</ref>, the performance difference between the minimum and the maximum under different acc configurations is less than 40%. As the computation parallelism is reduced to 1/8, the waste resulting from the inconsistency between the massive parallelism and the small MM size is mitigated. On the other hand, as shown in the last group in Figure <ref type="figure" target="#fig_7">10</ref>, 4-diverse acc stands out as the best when we increase AIE, on-chip storage, and off-chip bandwidth all by 4?. Simply increasing AIEs does not give significant improvement whereas increasing all the resources as a whole does.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION AND ACKNOWLEDGEMENT</head><p>In this paper, we propose the CHARM architecture and the CHARM framework to provide a novel system-level design methodology for composing heterogeneous accelerators for different MMs within an application and generating end-to-end application solutions. We will explore and extend CHARM for more applications and more data types in our future work. We acknowledge the support from the University of Pittsburgh New Faculty Start-up Grant, NSF awards #2213701, #2217003 and the support from CRISP, one of six SRC JUMP centers. We thank all the reviewers for their valuable feedback and Marci Baun for helping edit the paper. We thank AMD/Xilinx for FPGA and software donation, and support from the AMD/Xilinx Center of Excellence at UIUC, the AMD/Xilinx Heterogeneous Accelerated Compute Cluster at UCLA, and the Center for Research Computing (CRC) at University of Pittsburgh.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Throughput of square MM under different sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Execution timeline of one monolithic MM design vs. two diverse MM accs design for BERT on VCK190.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Versal ACAP architecture. components are integrated with I/O peripherals, such as PCIe and DRAM controllers, into a heterogeneous SoC with a Network-on-Chip (NoC). The VCK190 board is equipped with one DDR4-DIMM off-chip memory with a 25.6 GB/s peak bandwidth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Combining broadcast circuit-switched and packetswitched connections to reduce required I/O to AIE array. Section 4.2, we present the data reuse optimizations to balance the massive computation parallelism and communication among AIEs and between PL?AIEs and PL?DDR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: System architecture of multiple diverse MM accs and other non-MM accs.Model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: System implementation layout of the two-diverse MM accs and four non-MM accs for BERT.</figDesc><graphic url="image-2.png" coords="9,89.68,383.82,166.86,154.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Timeline of 4 tasks scheduled on 2 accs for BERT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Throughput comparison under different AIE, onchip, and off-chip configurations from CHARM for BERT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>?</head><label></label><figDesc>CHARM Systematical Design Methodology on Versal: To achieve high computation and communication efficiency of each acc, in Section 4, we propose a thorough design methodology on Versal heterogeneous platform. We further provide automatic CHARM DSE (CDSE) to find the optimized single acc configuration. To the best of our knowledge, this is the first work that provides a detailed analysis of the systematical data movement and computation on Versal.? CHARM Architecture and Framework: To achieve the design goals of good performance for MMs with both small and large sizes in an application, in Section 5, we propose the CHARM architecture and the CHARM framework to find the optimized design. In the CHARM framework, there are several modules.</figDesc><table><row><cell>First, on top of CDSE, we propose the CHARM diverse accel-</cell></row><row><cell>erator composer (CDAC), which features a sort-based two-step</cell></row><row><cell>search algorithm to find an optimized CHARM design in the</cell></row><row><cell>polynomial time complexity instead of exponential time com-</cell></row><row><cell>plexity. Furthermore, to automate the system implementation,</cell></row><row><cell>CHARM automatic code generation (CACG) is proposed to gen-</cell></row><row><cell>erate source code files for AIEs, PL, and host CPU. Lastly, the</cell></row><row><cell>CHARM runtime system (CRTS) is launched in the host CPU</cell></row><row><cell>that schedules different kernels to the accs for optimizing both</cell></row><row><cell>task latency and overall system throughput.</cell></row><row><cell>? We deploy the CHARM framework to accelerate four applica-</cell></row><row><cell>tions on VCK190 in Section 6. Our on-board experiments demon-</cell></row><row><cell>strate that CHARM achieves 1.46 TFLOPs, 1.61 TFLOPs, 1.74</cell></row><row><cell>TFLOPs, and 2.94 TFLOPs inference throughput for BERT, ViT,</cell></row><row><cell>NCF, MLP, respectively, which obtain 5.29?, 32.51?, 1.00?, and</cell></row><row><cell>1.00?, throughput gains compared to one monolithic accelerator.</cell></row><row><cell>? White-Box Open-Source Tools for Versal. While AMD pro-</cell></row><row><cell>vides users a block-box IP for NN applications called DPU [11],</cell></row><row><cell>we open-sourced our tools completely as a white-box with a de-</cell></row><row><cell>tailed step-by-step guide to reproduce all of the results presented</cell></row><row><cell>in this paper and for the other users to learn and leverage in</cell></row><row><cell>their end-to-end systems. (https:// github.com/ arc-research-</cell></row><row><cell>lab/ CHARM)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison with prior works.</figDesc><table><row><cell>Prior</cell><cell>One</cell><cell>Multi</cell><cell>Multi</cell><cell>Workload</cell><cell>Specializa</cell></row><row><cell>Works</cell><cell>Mono</cell><cell>Duplicate</cell><cell>Diverse</cell><cell>Assignment</cell><cell>-tion for Acc</cell></row><row><cell>Eyeriss etc. [12]-[26]</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>DPU etc. [11, 27]</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>DNN Expl.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>etc. [28, 29]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>5.3 CHARM Design Space Exploration (CDSE) for a Single Acc DSE Configurable parameters: A, B, C, X, Y, Z. In order to attain optimized throughput for each diverse accelerator, we design CDSE, which takes matrix sizes (M, K, and N), optional user-specified hardware constraints, and hardware platform off-chip characterization database as input and perform an analytical model-based search. During CDSE, we set the single AIE workloads to 32?32?32, i.e., TI=TK=TJ=32. We achieve up to 95% of kernel efficiency for MM, utilize 75% of the AIE local memory in this design point, and obtain the CTC ratio of 4. The outputs of CDSE are the configurable parameters, including A, B, C, X, Y, Z that meet all the hardware constraints. The parameters A, B, C determine the number of AIE and PLIO used in the AIE array. X, Y, Z, A, B, C together with prefixed parameters TX, TY, TZ decide the number of utilized on-chip buffers. This optimization problem can be formulated as an integer programming (IP) optimization problem shown as below. AIE num , PLIO in , PLIO out and On_chip RAM represent the user-specified hardware constraints:</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>1  2-1 = 7 grouping design choices. 2nd Step: Hardware Resource Partitioning. For each workload assignment, we perform DSE to find the optimized acc configurable parameters under the partitioned hardware resource constraints,Algorithm 1 Diverse Accelerator Composer AlgorithmInput: layer[n], bw, hw_sr, num, ubound ? layer[n] are n layers in an application. bw refers to bandwidth, hw_sr includes the AIE, PLIO, RAM resources, num refers to the number of accs, ubound is the hyperparameter for memory tuning</figDesc><table><row><cell cols="2">Output: Workload[num], final_Acc[num]</cell><cell></cell></row><row><cell></cell><cell cols="2">? Workload and final_Acc contains the workload assignment and the hardware</cell></row><row><cell cols="2">configuration for each acc respectively</cell><cell></cell></row><row><cell cols="2">1: BW ? bw/num_acc</cell><cell></cell></row><row><cell cols="2">2: HW .RAM [:] ? hw_sr.ram/num_acc</cell><cell></cell></row><row><cell cols="2">3: final_cycle ? inf</cell><cell></cell></row><row><cell cols="2">4: layer_sort [:] ? sort(layer)</cell><cell></cell></row><row><cell cols="2">5: for sche in range(? n-1 num-1 ) do 6: partition[:] ? partition(layer_sort [:], num, sche)</cell><cell>? 1st step</cell></row><row><cell>7:</cell><cell>op_portion[:] ? cnt(partition[:])</cell><cell>? 2nd step</cell></row><row><cell>8:</cell><cell>update(HW .AIE [:], HW .PLIO [:], op_portion[:])</cell><cell></cell></row><row><cell>9:</cell><cell cols="2">Acc [:], cycle [:] ? Acc_search(HW , BW , partition[:])</cell></row><row><cell>10:</cell><cell cols="2">? Sequentially launch CDSE</cell></row><row><cell>11:</cell><cell>while tune_cnt ? ubound do</cell><cell>? Memory tuning</cell></row><row><cell>12:</cell><cell>index ? max(cycle [:])</cell><cell></cell></row><row><cell>13:</cell><cell cols="2">update(HW .RAM [:], index) ? Increase the memory of the slowest acc</cell></row><row><cell>14:</cell><cell></cell><cell></cell></row><row><cell>15:</cell><cell>if max(cycle [:]) &lt; final_cycle then</cell><cell>? Update optimal point</cell></row><row><cell>16:</cell><cell>final_cycle ? max(cycle [:])</cell><cell></cell></row><row><cell>17:</cell><cell>final_Acc [:] ? Acc [:])</cell><cell></cell></row><row><cell>18:</cell><cell>Workload [:] ? partition[:]</cell><cell></cell></row><row><cell>19:</cell><cell>tune_cnt++</cell><cell></cell></row><row><cell cols="2">20: Define Acc_search(HW , BW , partition[:]) :</cell><cell></cell></row><row><cell cols="2">21: for acc in range(num) do</cell><cell></cell></row><row><cell>22:</cell><cell></cell><cell></cell></row></table><note><p>Acc [:], cycle [:] ? Acc_search(HW , BW , partition[:]) CDSE(partition[acc], HW [acc], BW [acc]) 23: return Acc [:], Cycle [; ]</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 2 :</head><label>2</label><figDesc>Single AIE MM comparison under fp32 data type.</figDesc><table><row><cell></cell><cell cols="2">H-GCN[48]</cell><cell cols="3">CHARM (this work)</cell></row><row><cell cols="2">Size: M x K x N MACs/Cyc</cell><cell>Eff</cell><cell>MACs/Cyc</cell><cell>Eff</cell><cell>Eff gain</cell></row><row><cell>16 ? 16 ? 16</cell><cell>2.34</cell><cell>29.30%</cell><cell>6.18</cell><cell>77.22%</cell><cell>2.64x</cell></row><row><cell>32 ? 32 ? 32</cell><cell>3.64</cell><cell>45.50%</cell><cell>7.57</cell><cell>94.70%</cell><cell>2.08x</cell></row><row><cell>64 ? 64 ? 8</cell><cell>3.64</cell><cell>45.50%</cell><cell>7.54</cell><cell>94.29%</cell><cell>2.07x</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison in GFLOPS between onboard measurements and CDSE analytical modeling estimations under different matrix sizes. The error rates in percentage show that CDSE achieves a high prediction accuracy.</figDesc><table><row><cell cols="5">Square MM size On-board Estimation Error Power(W)</cell></row><row><cell>64</cell><cell>0.41</cell><cell>0.40</cell><cell>-2%</cell><cell>32.58</cell></row><row><cell>128</cell><cell>3.36</cell><cell>3.22</cell><cell>-4%</cell><cell>32.86</cell></row><row><cell>256</cell><cell>25.58</cell><cell>25.79</cell><cell>1%</cell><cell>34.66</cell></row><row><cell>512</cell><cell>176.24</cell><cell>178.42</cell><cell>1%</cell><cell>37.95</cell></row><row><cell>1024</cell><cell>1103.46</cell><cell>1123.81</cell><cell>2%</cell><cell>41.78</cell></row><row><cell>1536</cell><cell>1633.13</cell><cell>1649.01</cell><cell>1%</cell><cell>46.02</cell></row><row><cell>2048</cell><cell>1672.76</cell><cell>1688.17</cell><cell>1%</cell><cell>47.87</cell></row><row><cell>3072</cell><cell>2850.13</cell><cell>2895.90</cell><cell>2%</cell><cell>50.65</cell></row><row><cell>4096</cell><cell>2718.42</cell><cell>2773.26</cell><cell>2%</cell><cell>51.97</cell></row><row><cell>6144</cell><cell>3277.99</cell><cell>3363.89</cell><cell>3%</cell><cell>53.57</cell></row></table><note><p><p><p><p><p>execution cycle of our single AIE design by simulating on the Versal ACAP AI Engine System C simulator</p><ref type="bibr" target="#b46">[47]</ref></p>, a cycle-accurate architecture simulator. As shown in Table</p>2</p>, our single AIE can achieve up to 7.57 MACs/cycle and 94.70% peak performance when MM size equals 32?32?32. Compared to the AIE dense MM kernel efficiency reported in H-GCN</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 :</head><label>4</label><figDesc>Comparison between PL only design and PL RAM + AIE design in CHARM on VCK190.</figDesc><table><row><cell></cell><cell>PL [22]</cell><cell>CHARM</cell></row><row><cell>Data Type</cell><cell>Float32</cell><cell>Float32</cell></row><row><cell>Frequency</cell><cell cols="2">PL:200MHz PL:230MHz</cell></row><row><cell>URAM</cell><cell>0</cell><cell>384</cell></row><row><cell>BRAM</cell><cell>923</cell><cell>764</cell></row><row><cell>DSP/AIE</cell><cell>DSP58:1536</cell><cell>AIE:384</cell></row><row><cell>TFLOPs</cell><cell>0.59 (1x)</cell><cell>3.27 (5.54x)</cell></row><row><cell>Power(W)</cell><cell>18.60</cell><cell>53.40</cell></row><row><cell>Energy Eff</cell><cell>1x</cell><cell>1.93x</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>On-board throughput and power comparisons under different MM accs configurations for BERT, ViT, NCF, MLP.</figDesc><table><row><cell cols="2">App CHARM cfg</cell><cell>LUT</cell><cell>BRAM</cell><cell>URAM</cell><cell>DSP</cell><cell>AIE</cell><cell cols="3">GFLOPS Power(W) GFLOPS/W (Ratio)</cell></row><row><cell></cell><cell>One_mono</cell><cell>103959(11.55%)</cell><cell>764 (79.01%)</cell><cell>384 (82.94%)</cell><cell>165 (8.38%)</cell><cell>384 (96%)</cell><cell>276.8</cell><cell>37.0</cell><cell>7.48 (1x)</cell></row><row><cell>BERT</cell><cell cols="6">One_spe Two_diverse 343774(38.20%) 534 (55.22%) 272 (58.75%) 442 (22.46%) 288 (72%) 90351(10.04%) 515 (53.26%) 64 (13.82%) 117 (5.95%) 256 (64%)</cell><cell>515.4 1464.2</cell><cell>32.4 40.7</cell><cell>15.91 (2.13x) 35.98 (4.81x)</cell></row><row><cell></cell><cell>8_duplicate</cell><cell>222956(24.78%)</cell><cell>664 (68.67%)</cell><cell>384 (82.94%)</cell><cell>488 (24.80%)</cell><cell>256 (64%)</cell><cell>534.2</cell><cell>34.2</cell><cell>15.62 (2.09x)</cell></row><row><cell></cell><cell>One_mono</cell><cell>103959(11.55%)</cell><cell>764 (79.01%)</cell><cell>384 (82.94%)</cell><cell>165 (8.38%)</cell><cell>384 (96%)</cell><cell>49.5</cell><cell>32.4</cell><cell>1.53 (1x)</cell></row><row><cell>ViT</cell><cell>One_spe</cell><cell>76661(8.52%)</cell><cell cols="4">275 (28.44%) 590 (61.01%) 320 (69.11%) 299 (15.19%) 264 (72%) 64 (13.82%) 187 (9.50%) 256 (66%)</cell><cell>217.1 1609.0</cell><cell>28.0 39.6</cell><cell>7.75 (5.08x) 40.63 (26.60x)</cell></row><row><cell></cell><cell>8_duplicate</cell><cell>222956(24.78%)</cell><cell>664 (68.67%)</cell><cell>384 (82.94%)</cell><cell>488 (24.80%)</cell><cell>256 (64%)</cell><cell>382.2</cell><cell>32.8</cell><cell>11.65 (7.63x)</cell></row><row><cell></cell><cell>One_mono</cell><cell cols="5">103959(11.55%) 764 (79.01%) 384 (82.94%) 165 (8.38%) 384 (96%)</cell><cell>1736.0</cell><cell>45.2</cell><cell>38.41 (1x)</cell></row><row><cell>NCF</cell><cell>One_spe Two_diverse</cell><cell cols="5">103959(11.55%) 764 (79.01%) 384 (82.94%) 165 (8.38%) 384 (96%) 161597(17.96%) 790 (81.70%) 352 (76.03%) 326 (16.57%) 384 (96%)</cell><cell>1736.0 1730.9</cell><cell>45.2 45.1</cell><cell>38.41 (1.00x) 38.38 (0.99x)</cell></row><row><cell></cell><cell>8_duplicate</cell><cell>222956(24.78%)</cell><cell>664 (68.67%)</cell><cell>384 (82.94%)</cell><cell>488 (24.80%)</cell><cell>256 (64%)</cell><cell>671.0</cell><cell>35.0</cell><cell>19.17 (0.50x)</cell></row><row><cell></cell><cell>One_mono</cell><cell cols="5">103959(11.55%) 764 (79.01%) 384 (82.94%) 165 (8.38%) 384 (96%)</cell><cell>2936.7</cell><cell>51.4</cell><cell>57.13 (1x)</cell></row><row><cell>MLP</cell><cell>One_mono Two_diverse</cell><cell cols="5">103959(11.55%) 764 (79.01%) 384 (82.94%) 165 (8.38%) 384 (96%) 148158(16.46%) 919 (95.04%) 448 (96.76%) 344 (17.48%) 384 (96%)</cell><cell>2936.7 2386.1</cell><cell>51.4 48.8</cell><cell>57.13 (1.00x) 48.90 (0.86x)</cell></row><row><cell></cell><cell>8_duplicate</cell><cell>222956(24.78%)</cell><cell>664 (68.67%)</cell><cell>384 (82.94%)</cell><cell>488 (24.80%)</cell><cell>256 (64%)</cell><cell>696.0</cell><cell>35.2</cell><cell>19.77 (0.35x)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Resource utilization for each acc in the design for BERT with two MM diverse accs, four non-MM accs.</figDesc><table><row><cell>Type</cell><cell cols="2">REG</cell><cell>LUTLogic</cell><cell>LUTMem</cell><cell>BRAM</cell><cell>URAM</cell><cell>DSP</cell><cell>AIE</cell></row><row><cell cols="4">MM0+DMA0+buffer 96790 (5.55%) 91034 (10.41%)</cell><cell>835 (0.19%)</cell><cell cols="4">515 (53.26%) 256(55.29%) 246(12.50%) 256(64%)</cell></row><row><cell cols="5">MM1+DMA1+buffer 62415 (3.58%) 94739 (10.83%) 37668 (8.48%)</cell><cell>19 (1.96%)</cell><cell>16 (3.46%)</cell><cell>196( 9.96%)</cell><cell>32 (8%)</cell></row><row><cell>Layernorm</cell><cell cols="3">45101 (2.58%) 33939 (3.88%)</cell><cell>4234 (0.95%)</cell><cell>15 (1.55%)</cell><cell cols="2">90 (19.44%) 129( 6.55%)</cell><cell>0 (0%)</cell></row><row><cell>Softmax</cell><cell cols="3">34270 (1.96%) 33623 (3.84%)</cell><cell cols="2">2854 (0.64%) 243 (25.13%)</cell><cell>0 (0%)</cell><cell>151( 7.67%)</cell><cell>0 (0%)</cell></row><row><cell>Transpose0</cell><cell cols="2">14217 (0.81%)</cell><cell>6926 (0.79%)</cell><cell>1097 (0.25%)</cell><cell>15 (1.55%)</cell><cell>0 (0%)</cell><cell>94 ( 4.78%)</cell><cell>0 (0%)</cell></row><row><cell>Transpose1</cell><cell cols="4">33967 (1.95%) 58510 (6.69%) 32512 (7.32%)</cell><cell>15 (1.55%)</cell><cell>0 (0%)</cell><cell>19 ( 0.97%)</cell><cell>0 (0%)</cell></row><row><cell>MM0</cell><cell></cell><cell cols="2">MM1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Buffer+DMA0</cell><cell>Softmax</cell><cell cols="2">Buffer+DMA1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Transpose1</cell><cell>Transpose0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Layernorm</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>As shown in Figure8, we map four concurrent tasks on the BERT design with two-diverse accs. Each task has eight MM kernels and there are dependency edges, including 0?6, 1?6, 6?7, 2?7, 7?3?4?5, where x?y means y depends on x. The other non-MM communication-bound kernels are not shown in the figure for illustrative simplicity. It takes 110ms to finish the 1st task and 234ms to finish the 4th task. For one-acc specialized design, each task latency is 162.6ms. Therefore, we have a design tradeoff, i.e., one specialized acc design can process fine-grained tasks whereas two-diverse accs design requires coarse-grained tasks to fill the pipeline of the two accs. Comparing to the one specialized Acc design, with 0.67?, 0.92?, latency for different tasks, we gain 2.8? overall throughput in return. This illustrates that the CHARM framework allows explorations on the latency-throughput tradeoff and users can specify targets to let CHARM generate the designs that optimize throughput while meeting the latency requirement or vice versa.</figDesc><table><row><cell>(GFLOPs)</cell><cell>4000 5000 6000</cell><cell>One_Mono 4 duplicate</cell><cell>One_spe 4 diverse</cell><cell>2 duplicate 8 duplicate</cell><cell>2 diverse 8 diverse</cell></row><row><cell>Throughput</cell><cell>1000 2000 3000</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">1x DDR Figure 9: Throughput comparison under different off-chip 4X DDR 0 16x DDR</cell></row><row><cell cols="6">bandwidth configurations from CHARM for BERT.</cell></row><row><cell cols="6">Explore latency-throughput tradeoff in CHARM. CHARM DSE Efficiency. We use CHARM to perform a sort-based</cell></row><row><cell cols="6">two-step search algorithm in CDAC. For BERT, compared to the</cell></row><row><cell cols="6">exhaustive search, CDAC finds the optimal solution in 170 seconds</cell></row><row><cell cols="6">whereas the exhaustive search takes 33 mins (#search iterations: 2M</cell></row><row><cell cols="6">vs. 58M) with MATLAB R2021b on an Intel Core i9-10900X CPU.</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">End-to-End Performance</head><p>We apply the CHARM framework to four applications, BERT, ViT, NCF, MLP. All the shapes of the MM kernels in these models are listed in Table <ref type="table">5</ref>. We explore the number of accs from 1 to 8 and showcase the representative CHARM designs, including one monolithic MM acc, one specialized MM acc, two-diverse MM accs, and eight-duplicate MM accs, for each application. The one monolithic MM design is described in section 6.2, which stays the same for all four applications. It is set as the baseline design for comparisons. All the other MM acc designs are customized for each application and are designed and implemented using the CHARM framework. All</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th international conference on world wide web</title>
		<meeting>the 26th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Benchmarking TPU, GPU, and CPU platforms for deep learning</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Emma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Gu-Yeon</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Brooks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10701</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Indatacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Norman P Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raminder</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th annual international symposium on computer architecture</title>
		<meeting>the 44th annual international symposium on computer architecture</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A modern primer on processing in memory</title>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saugata</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>G?mez-Luna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachata</forename><surname>Ausavarungnirun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Emerging Computing: From Devices to Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="171" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DAMOV: A new methodology and benchmark suite for evaluating data movement bottlenecks</title>
		<author>
			<persName><forename type="first">Geraldo</forename><forename type="middle">F</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>G?mez-Luna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lois</forename><surname>Orosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saugata</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nandita</forename><surname>Vijaykumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Sadrosadati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="134457" to="134502" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Crow: A low-cost substrate for improving dram performance, energy efficiency, and reliability</title>
		<author>
			<persName><forename type="first">Hasan</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minesh</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremie</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giray Yaglikci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nandita</forename><surname>Vijaykumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nika</forename><surname>Mansouri Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saugata</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th International Symposium on Computer Architecture</title>
		<meeting>the 46th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="129" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Communication avoiding algorithms</title>
		<author>
			<persName><forename type="first">Jim</forename><surname>Demmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 SC Companion: High Performance Computing, Networking Storage and Analysis</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1942" to="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><surname>Amd/Xilinx</surname></persName>
		</author>
		<title level="m">Versal Adaptive Compute Acceleration Platform</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><surname>Amd</surname></persName>
		</author>
		<author>
			<persName><surname>Ip</surname></persName>
		</author>
		<title level="m">Overlays of Deep learning Processing Unit</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Eyeriss v2: A flexible accelerator for emerging deep neural networks on mobile devices</title>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tien-Ju</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Emerging and Selected Topics in Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="292" to="308" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Shidiannao: Shifting vision processing closer to the sensor</title>
		<author>
			<persName><forename type="first">Zidong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Fasthuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Ienne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobing</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunji</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual International Symposium on Computer Architecture</title>
		<meeting>the 42nd Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="92" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Why compete when you can work together: Fpga-asic integration for persistent rnns</title>
		<author>
			<persName><forename type="first">Eriko</forename><surname>Nurvitadhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongup</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Boutros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoong</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Tomson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huseyin</forename><surname>Sumbul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Knag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghavan</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="199" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Beyond peak performance: Comparing the real performance of ai-optimized fpgas and gpus</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Boutros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eriko</forename><surname>Nurvitadhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Gribok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">C</forename><surname>Hoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaughn</forename><surname>Betz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Langhammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Conference on Field-Programmable Technology (ICFPT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A configurable cloud-scale dnn processor for real-time ai</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Fowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalin</forename><surname>Ovtcharov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Papamichael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Massengill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shlomi</forename><surname>Alkalay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Haselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahdi</forename><surname>Ghandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">FBLAS: Streaming linear algebra on FPGA</title>
		<author>
			<persName><forename type="first">Tiziano</forename><surname>De Matteis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>De Fine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Licht</surname></persName>
		</author>
		<author>
			<persName><surname>Hoefler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC20: International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Flexible communication avoiding matrix multiplication on fpga with high-level synthesis</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fine</forename><surname>Licht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grzegorz</forename><surname>Kwasniewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</title>
		<meeting>the 2020 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="244" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Optimizing fpga-based accelerator design for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of FPGA</title>
		<meeting>of FPGA</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="161" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A customizable matrix multiplication framework for the intel harpv2 xeon+fpga platform: A deep learning case study</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srivatsan</forename><surname>Moss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eriko</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Nurvitadhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ratuszniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoong</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asit</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debbie</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchit</forename><surname>Marr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">H W</forename><surname>Subhaschandra</surname></persName>
		</author>
		<author>
			<persName><surname>Leong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, FPGA &apos;18</title>
		<meeting>the 2018 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, FPGA &apos;18</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018-02">Feb 2018</date>
			<biblScope unit="page" from="107" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">AutoSA: A Polyhedral Compiler for High-Performance Systolic Arrays on FPGA</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Licheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2021 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, FPGA &apos;21</title>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021-02">Feb 2021</date>
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sextans: A streaming accelerator for general-purpose sparse-matrix dense-matrix multiplication</title>
		<author>
			<persName><forename type="first">Linghao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuze</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atefeh</forename><surname>Sohrabizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young-Kyu</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, FPGA &apos;22</title>
		<meeting>the 2022 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, FPGA &apos;22<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="65" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Serpens: A high bandwidth memory based accelerator for general-purpose sparse matrix-vector multiplication</title>
		<author>
			<persName><forename type="first">Linghao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuze</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Licheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th ACM/IEEE Design Automation Conference</title>
		<meeting>the 59th ACM/IEEE Design Automation Conference</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="211" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Latte: Locality Aware Transformation for High-Level Synthesis</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cody</forename><surname>Hao Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peipei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 26th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="125" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Energy Efficiency of Full Pipelining: A Case Study for Matrix Multiplication</title>
		<author>
			<persName><forename type="first">Peipei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunseok</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenman</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andr?</forename><surname>Dehon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE 24th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="172" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">MOCHA: Multinode Cost Optimization in Heterogeneous Clouds with Accelerators</title>
		<author>
			<persName><forename type="first">Peipei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cody</forename><surname>Hao Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2021 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, FPGA &apos;21</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="273" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dnnbuilder: an automated tool for building highperformance dnn hardware accelerators for fpgas</title>
		<author>
			<persName><forename type="first">Xiaofan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCAD</title>
		<meeting>ICCAD</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">56</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">DNNExplorer: a framework for modeling and exploring a novel paradigm of FPGA-based DNN accelerator</title>
		<author>
			<persName><forename type="first">Xiaofan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanchen</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Mei</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deming</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Computer-Aided Design</title>
		<meeting>the 39th International Conference on Computer-Aided Design</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tetris: Scalable and efficient neural network acceleration with 3d memory</title>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="751" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Tangram: Optimized coarse-grained dataflow for scalable nn accelerators</title>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="807" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Heterogeneous dataflow accelerators for multi-dnn workloads</title>
		<author>
			<persName><forename type="first">Hyoukjun</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangzhen</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Pellauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="71" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<author>
			<persName><surname>Website</surname></persName>
		</author>
		<ptr target="http://nvdla.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Fully Pipelined and Dynamically Composable Architecture of CGRA</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peipei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE 22nd Annual International Symposium on Field-Programmable Custom Computing Machines</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">CHARM: A Composable Heterogeneous Accelerator-Rich Microprocessor</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Ali Ghodrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beayna</forename><surname>Grigorian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glenn</forename><surname>Reinman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 ACM/IEEE International Symposium on Low Power Electronics and Design, ISLPED &apos;12</title>
		<meeting>the 2012 ACM/IEEE International Symposium on Low Power Electronics and Design, ISLPED &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="379" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><surname>Amd/Xilinx</surname></persName>
		</author>
		<title level="m">Versal AI Core Series VCK190 Evaluation Kit</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title/>
		<author>
			<persName><surname>Amd/Xilinx</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Engine Technology</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">High-level synthesis for FPGAs: From prototyping to deployment</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Neuendorffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanjo</forename><surname>Noguera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kees</forename><surname>Vissers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiru</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="473" to="491" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">FPGA HLS Today: successes, challenges, and opportunities</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Neuendorffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peichen</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kees</forename><surname>Vissers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiru</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Reconfigurable Technology and Systems (TRETS)</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">FCUDA: Enabling efficient compilation of CUDA kernels onto FPGAs</title>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Papakonstantinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Gururaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">A</forename><surname>Stratton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Mei W</forename><surname>Hwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE 7th Symposium on Application Specific Processors</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="35" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multilevel granularity parallelism synthesis on fpgas</title>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Papakonstantinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">A</forename><surname>Stratton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Gururaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Mei W</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE 19th Annual International Symposium on Field-Programmable Custom Computing Machines</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">High-level synthesis: productivity, performance, and software constraints</title>
		<author>
			<persName><forename type="first">Yun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Rupnow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongbo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deming</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electrical and Computer Engineering</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Extending high-level synthesis for task-parallel programs</title>
		<author>
			<persName><forename type="first">Yuze</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Licheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young-Kyu</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 29th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="204" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title/>
		<author>
			<persName><surname>Amd/Xilinx</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adaptive Data Flow API</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><surname>Amd/Xilinx</surname></persName>
		</author>
		<title level="m">Board evaluation and management Tool</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><surname>Amd/Xilinx</surname></persName>
		</author>
		<title level="m">AI Engine API and Intrinsics User Guide</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><surname>Amd/Xilinx</surname></persName>
		</author>
		<title level="m">Versal? ACAP AI Engine System C simulator</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Chengming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiannan</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Herbordt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingwen</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><surname>H-Gcn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.13734</idno>
		<title level="m">A graph convolutional network accelerator on versal acap architecture</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
