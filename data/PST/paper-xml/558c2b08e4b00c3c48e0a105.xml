<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Heterogeneous Block Architecture</title>
				<funder>
					<orgName type="full">Oracle</orgName>
				</funder>
				<funder ref="#_7GPUbFw #_sNDwXNx #_jMfPppT">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_YMQRPEx">
					<orgName type="full">IBM</orgName>
				</funder>
				<funder>
					<orgName type="full">Intel Science and Technology Center</orgName>
				</funder>
				<funder ref="#_VVKMUab #_A8EQS85">
					<orgName type="full">Intel</orgName>
				</funder>
				<funder>
					<orgName type="full">Qualcomm Innovation Fellowship Program</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chris</forename><surname>Fallin</surname></persName>
							<email>cfallin@clf.net</email>
							<affiliation key="aff0">
								<orgName type="department">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chris</forename><surname>Wilkerson</surname></persName>
							<email>chris.wilkerson@intel.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Intel Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The Heterogeneous Block Architecture</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper makes two observations that lead to a new heterogeneous core design. First, we observe that most serial code exhibits fine-grained heterogeneity: at the scale of tens or hundreds of instructions, regions of code fit different microarchitectures better (at the same point or at different points in time). Second, we observe that by grouping contiguous regions of instructions into blocks that are executed atomically, a core can exploit this fine-grained heterogeneity: atomicity allows each block to be executed independently on its own execution backend that fits its characteristics best.</p><p>Based on these observations, we propose afine-grained hetero geneous core design, called the heterogeneous block architecture (BBA), that combines heterogeneous execution backends into one core. HBA breaks the program into blocks of code, determines the best backend for each block, and specializes the block for that backend. As an example HBA design, we combine out-of order, VLIW, and in-order backends, using simple heuristics to choose backends for different dynamic instruction blocks. Our extensive evaluations compare this example HBA design to multiple baseline core designs (including monolithic out of-order, clustered out-of-order, in-order and a state-of-the-art heterogeneous core design) and show that it provides significantly better energy efficiency than all designs at similar performance.</p><p>I.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>General-purpose processor core design faces two competing goals. First, a core should provide high single-thread (serial) peifor mance. This is important for many algorithms and for any application with serialized code sections <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr">15,</ref><ref type="bibr">28,</ref><ref type="bibr">29,</ref><ref type="bibr">59]</ref>. Second, a core should provide high energy efficiency. Energy/power consumption is a primary limiter of system performance and scalability, both in large scale data centers <ref type="bibr" target="#b16">[17]</ref> and in consumer devices <ref type="bibr" target="#b21">[22,</ref><ref type="bibr">41]</ref>.</p><p>Unfortunately, it is difficult to achieve both high performance and high energy efficiency at the same time: no single core microar chitecture is the best design for both metrics for all programs or program phases. Any particular design spends energy on some set of features (e.g., out-of-order instruction scheduling, sophisticated branch prediction, or wide pipeline width), but these features do not always yield improved performance. As a result, a general-purpose core is usually a compromise: it is designed to meet some performance objectives while remaining within a power envelope, but for any given program, it is frequently not the most efficient design.</p><p>Designing a good general-purpose core is difficult because code is heterogeneous at multiple levels: each program has different characteristics, and a single program has different characteristics in different regions of its code. To exploit this diversity, past works have proposed core-level heterogeneity. These heterogeneous designs either combine multiple separate cores (e.g., <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr">10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr">28,</ref><ref type="bibr">33,</ref><ref type="bibr">59,</ref><ref type="bibr">61]</ref>), or combine an in-order and an out-of-order pipeline with a shared frontend in a single core <ref type="bibr">[37]</ref>. Past works demonstrate energy efficiency improvements with usually small impact to performance. This paper makes two key observations that motivate a new way of building a heterogeneous core. Our first observation is that applications have fine-grained heterogeneity. Prior work exploited heterogeneity at the coarser granularity of thousands of instructions: e.g., programs have memory and compute phases, and such phases can be exploited by migrating a thread to "big" cores for compute intensive phases and "little" cores for memory-intensive phases <ref type="bibr">[37,</ref><ref type="bibr">61]</ref>. As we will show, at a much finer granularity (of tens of instructions), adjacent blocks of code often have different properties. For example, one block of code might have a consistent instruction schedule across its dynamic execution instances in an 000 machine, 978-1-4799-6492-511 4/$31.00 ?2014 IEEE 386 whereas a neighboring block might have different execution schedules at different times depending on cache miss behavior of load instruc tions in the block. Such behavior suggests the use of both dynamic and static schedulers within a single core, perhaps even simultaneously for different instructions in flight, so that each instance of a block is executed using the most efficient instruction scheduling mechanism. Migration of execution between separate cores or pipelines cannot easily exploit this fine-grained heterogeneity in code behavior.</p><p>Our second observation is that a core can exploit fine-grained heterogeneity if it splits code into atomic blocks and executes each block on a separate execution backend, including functional units, local storage, and some form of instruction scheduling. To exploit fine-grained heterogeneity, a core will need to (i) have execution backends of multiple types, and (ii) specialize pieces of code for each backend. By enforcing atomicity, or the property that a block of code either executes as a whole or not at all, the core can freely analyze and morph this block of code to fit a particular backend (e.g., atomicity allows the core to reorder instructions freely within the block). Atomic block-based design allows execution backends to operate independently using a well-defined interface (liveins/liveouts) between blocks.</p><p>Based on these two observations, we propose a fine-grained het erogeneous core that dynamically forms code into blocks, specializes those blocks to execute on the most appropriate type of execution backend, and executes blocks on the various backends. This core design serves as a general substrate for fine-grained heterogeneity that can combine many different types of execution backends. As an initial example design, this paper describes and evaluates a core which includes out-oj-order, VLIW, and in-order execution backends, and logic to assign each block to a backend. Our concrete design initially executes each block on the out-of-order execution backend, but monitors schedule stability of the block over time. When a block of code has an unchanging instruction schedule, indicating that instruction latencies are likely not variable, it is converted to a VLIW or in-order block (depending on instruction-level parallelism, ILP), using the instruction schedule recorded during out-of-order execution. When the block again requires dynamic scheduling (determined based on a simple stall-cycle statistic), it is converted to an out-of-order block. At any given time, multiple backend types can be active for different blocks in flight. This paper makes four major contributions:</p><p>1. It introduces the concept and design of the heterogeneous block architecture (HBA). HBA exploits the notions of fine-grained heterogeneity, atomic blocks, and block-based instruction schedul ing/execution in a synergistic manner to adapt each piece of code to the execution backend it is best fit to execute on. ( ?II and ?III) 2. It introduces the implementation of a new fine-grained hetero geneous core, an example HBA design, that forms atomic blocks of code and executes these blocks on out-of-order, VLIW, and in-order backends, depending on the observed instruction schedule stability and ILP of each block, with the goal of maximizing energy efficiency while maintaining high performance. ( ?III-C3 and ?III-D)</p><p>3. It provides simple mechanisms that enable a block of code to be switched between VLIW/in-order and out-of-order execution backends. These mechanisms do not require any support at compile time; they use dynamic heuristics and instruction schedules, and form blocks dynamically. ( ?III-D) 4. It extensively evaluates an example HBA design in comparison to four baselines (out-of-order, clustered <ref type="bibr" target="#b17">[18]</ref>, coarse-grained hetero geneous <ref type="bibr">[37]</ref>, and clustered coarse-grained), showing higher energy efficiency than all previous designs across a wide variety of workloads ( ?V). Our design reduces average core power by 36.4% with 1 % performance loss over the baseline. We show that HBA provides a flexible substrate for future heterogeneous designs, enabling new power-performance tradeoff points in core design ( ?V-C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MOTIVATION: FINE-GRAINED HETEROGENEITY</head><p>Our first major observation is that applications have fine-grained heterogeneity, i.e., heterogeneity across regions of tens or hundreds of instructions. This heterogeneity is distinct from larger program phases <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">56]</ref> that occur because a program switches between wholly different tasks or modes. Fine-grained heterogeneity occurs when small chunks of code have different characteristics due to particular instructions or dataflow within a single task or operation.</p><p>Fig. <ref type="figure" target="#fig_0">1</ref> pictorially represents this distinction. The left half depicts an application that has at least two phases: a regular floating-point phase and a memory-bound pointer-chasing phase. These phases occur at a scale of thousands to millions of instructions. If we focus on one small portion of the first phase, we seefine-grained heterogeneity.</p><p>The right half of Fig. <ref type="figure" target="#fig_0">1</ref> depicts three regions of instructions within the coarse-grained phase. In the first region of instructions, three of the four operations are independent and can issue in parallel, and all instructions have constant, statically-known latencies. Hence, this region has high ILP and a stable (unchanging) dynamic instruction schedule. The second region also has high ILP, but has a variable schedule due to intermittent cache misses. Finally, the third region has low ILP due to a dependence chain. Overall, each small code region within this single "regular floating point" phase has different properties, arising solely from variations in instruction dependences and latencies. Each such region thus benefits from different core features.</p><p>1 To motivate that adjacent regions of code may have different properties that can be exploited, we show the existence of one such property, instruction schedule stability: some regions of code always (or frequently) schedule in the same order in the dynamic out-of order scheduling logic. We also show that this property varies greatly between different nearby regions of code (hence, is fine-grained).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coarse-Grained Heterogeneity</head><p>To do this, we analyze the behavior of 268 workloads on a 4-wide superscalar out-of-order core. I We observe the retired instruction stream and group retired instructions into chunks of up to 16 MOps.</p><p>Chunks are broken at certain boundaries according to the heuristics in ?III-CI. For each chunk, we compare the actual dynamic instruction schedule of that chunk to the schedule of the previous instance of the same (static) code. We record whether the schedule was the same as before. These annotations, called "chunk types," indicate the extent to which each chunk has a stable schedule.</p><p>Fig. <ref type="figure" target="#fig_1">2</ref> shows the fraction of the "same" and "different" chunk types, per benchmark, as a sorted curve. Two observations are in order. First, many chunks repeat instruction schedules in their previous execution (especially in workloads to the left of the graph). Hence, there is significant opportunity to reuse past instruction scheduling order (as also noted by past work <ref type="bibr">[44]</ref>). Second, there are many applications (in the center of the plot) that exhibit a mix of behavior: Moreover, we observe that this heterogeneity exists between near by chunks in the dynamic instruction stream, i.e., there is fine grained heterogeneity. We observe the sequence of chunk types in retire order and group chunks into runs. One run is a consecutive series of chunks with the same instruction schedule stability. We then accumulate the length of all runs. The length of these runs indicates whether the heterogeneity is coarse-or fine-grained. We find that almost 60% of runs are of length 1, and the histogram falls off rapidly thereafter. In other words, the schedule stability of chunks is often different even between temporally-adjacent static regions of code, indicating fine-grained heterogeneity. Fine-grained heterogene ity exists within program phases, and is thus distinct from coarse grained (inter-programlinter-phase) heterogeneity exploited by past works in heterogeneous core/multicore design. Instead, it motivates a new approach: a single core that can execute nearby chunks of instructions with different mechanisms best suited for each dynamic chunk. Our goal in this paper is to provide such a framework for general-purpose core design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. HBA: PRINCIPLES AND EXAMPLE DES IGN</head><p>Based on our observations, we introduce our new design, RBA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. High-Level Overview</head><p>Key Idea #1: Build a core that executes fine-grained blocks of code on heterogeneous backends. As shown in ?II, application code is heterogeneous at a fine granularity. To exploit this, we build a core that contains multiple different execution backends within a single core. The core groups the application's instructions into chunks (called blocks) and determines the best type of execution backend for each block. Multiple execution backends (including multiple of the same type) can be active simultaneously, executing different blocks of code, and these backends communicate with each other directly.</p><p>Key Idea #2: Leverage block atomicity to allow block specializa tion. In order to allow for a block of code to be adapted properly to a particular backend, the block must be considered as a unit, isolated from the rest of the program. Our second key idea is to require atomicity of each block: the core commits all results of the block at once or not at all. Atomicity guarantees the core will always handle an entire block at once, allowing the use of backends that leverage code properties extracted once over the entire block (e.g., by reordering or rewriting instructions) to adapt the block to a particular backend.</p><p>Atomicity thus enables the core to exploit fine-grained heterogeneous backends.</p><p>Key Idea #3: Combine out-of-order and VLIW/in-order execution backends by using out-of-order execution to form stable VLIW schedules. Our final idea leverages dynamically-scheduled (out-of order) execution in order to enable statically-scheduled (VLIW/in order) execution with runtime-informed instruction schedules. The out-of-order backend observes the dynamic schedule and, when it is stable (unchanging) over multiple instances of the same code, records the schedule and uses it for VLIW or in-order execution. If the core later determines that this schedule leads to unnecessary stalls, the schedule is thrown away and the block is again executed by the out of-order backend. Most of the performance of out-of-order execution is retained at much lower energy (as shown in ?V).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Atomicity, Liveins, Liveouts</head><p>We briefly describe terms that are important to understanding our design. First, atomicity of a block means that a block either completes execution and commits all its results, or none at all. This is in contrast to a conventional core, in which the atomic unit of execution is a single instruction, and each instruction commits its results separately. Second, Iiveins and Iiveouts are the inputs and outputs, respectively, to and from a block. A livein is any register that an instruction in a block reads that is not written (produced) by an earlier instruction in the block. A liveout is any register that an instruction in a block writes that is not overwritten by a later instruction in the block.</p><p>C. HBA Design </p><formula xml:id="formula_0">I ROB/Retirei 1_ ---= ? ? ,,? = E ? Global ? 5 Reg File " " '" &gt; 2.</formula><p>Block Execution</p><formula xml:id="formula_1">II Backend I (lYpe 1) I I 8ackend I (l\Ipe 1)</formula><p>II Backend (lYpe 2)</p><formula xml:id="formula_2">II Backend I (lYpe 2)</formula><p>Fig. <ref type="figure" target="#fig_2">3</ref>: HBA (Heterogeneous Block Architecture) overview.</p><p>1) Block Formation and Fetch: HBA core forms blocks dynamically. These blocks are microarchitectural: the block-level interface is not software-visible. In order to avoid storing every block in full, the HBA core uses an instruction cache (as in the baseline),</p><p>and stores only block metadata in a block info cache. This cache is indexed with the start PC of a block and its branch path, just as in a conventional trace cache <ref type="bibr">[45,</ref><ref type="bibr">52]</ref>. The block info cache stores information that the core has discovered about the block ? This information depends on the block type: for example, for a block that executes on a VLIW backend, the information includes the instruction schedule.</p><p>At fetch, the block frontend fetches instructions from the 1cache, using a conventional branch predictor. In parallel, it looks up information in the block info cache. As instructions are fetched, they are not sent to the backend right away, but are kept in a block buffer.</p><p>These instructions become a block and are sent to the backend either when the block name (PC and branch path) hits in the block info cache, and no longer matches exist, or else when the PC and branch path miss in the block info cache. If there is a miss, the block takes on default characteristics: it executes on an 000 backend, which requires no pre-computed information about the block. In this case, the block is terminated whenever any block termination condition holds: when it (i) reaches a maximum length (16 instructions by default), (ii) ends at an indirect branch, or (iii) ends at a difficult-to-predict conditional branch, as determined by a small (lK-entry) table of 2-bit saturating counters incremented whenever a branch is mispredicted <ref type="bibr">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Block Sequencing and Communication:</head><p>The central por tion of the core depicted in Fig. <ref type="figure" target="#fig_2">3</ref> handles sequencing and commu nication: that is, managing block program order and repairing it on branch mispredicts, sending blocks to the appropriate execution units, and communicating program values between those execution units.</p><p>Block Dispatch and Sequencing: Once a block is fetched, the block dispatch logic sends it to an appropriate execution backend. Each execution backend executes one block at a time until all operations within the block are complete. The block dispatch logic maintains one free-list of block execution backends per type, and allocates the appropriate type for each block. In the concrete design point that we evaluate, there are 16 backends, each of which can execute in 000 or VLIW/in-order mode, so there is only one such free-list.</p><p>The block sequencing logic maintains program order among blocks in flight and handles branch misprediction recovery. The logic contains a block-level ROB (reorder buffer), analogous to the ROB in a conventional out-of-order design. There are two types of branch mispredicts: intra-block, due to a conditional branch in the middle of a block, and inter-block, due to a branch that is the last instruction in a block. Inter-block misprediction recoveries squash the blocks that follow the mispredicted branch in program order, roll back state using the ROB, and restart the frontend on the proper path. Intra block mispredicts additionally squash the block that contains the mispredicted branch (due to block-level atomicity) and restart the frontend from the block at the same fetch PC but with a different internal branch-path. Finally, the block sequencing logic handles exceptions by squashing the excepting block and executing in a special single-instruction block mode to reach the exception point. Global Registers and Liveout-Livein Communication: Blocks executing on different backends communicate via global registers that receive liveouts from producer blocks as they execute and provide liveins to consumer blocks. The global register file is centrally located between the block execution backends. In addition to data values, this logic contains subscription bits and a livein wakeup unit, described in more detail below.</p><p>When a block is dispatched, its liveins are renamed by looking up global register pointers in a liveout register alias table (RAT), which contains an entry for each architectural register. Its liveouts are then allocated global registers and the liveout RAT is updated. Liveout to-livein communication between blocks occurs as soon as a given liveout is produced within a backend. The liveout is first written to the global register file. The livein wakeup unit then sends the value to any blocks that consume it as a livein. Thus, values are communicated from producers to consumers as soon as the values become available, and blocks begin executing as soon as any of their instructions has the necessary liveins (avoiding performance loss that would occur if a block were to wait for all liveins first).</p><p>To support this fine-grained livein wakeup, each global register has a corresponding set of subscription bits indicating waiting back ends. When a block is dispatched, it subscribes to its livein registers. At each global writeback, the subscription bits allow wakeups to be sent efficiently to only the consuming backends. This structure is similar to a bit-matrix scheduler <ref type="bibr" target="#b20">[21]</ref>. Note that values produced and consumed internally within a block are never communicated nor written to the global register file.</p><p>3) Block Execution: When a block is sent to a block execution backend, the backend executes the block in its specialized resources. Each backend receives (i) a block specialized for that backend, and (ii) a stream of liveins for that block, as they become available. The backend performs the specified computation and produces (i) a stream of liveouts for its block, (ii) any branch misprediction results, and (iii) a completion signal. When a block completes execution, the backend clears out the block and becomes ready to receive another one.</p><p>In our example HBA design, we implement two types of block execution backend: an out-of-order backend and a VLIW/in-order backend. Both types share a common datapath design, and differ only in the instruction issue logic and pipeline width. Note that these backends represent only two points in a wide design spectrum; more specialized backends are possible and are left for future work. The core will have several such backends (i.e., not simply one of each type); in general, an HBA core could contain an arbitrary pool of backends of many different types.</p><p>Local execution cluster: Both the out-of-order and VLIW/in-order execution backends in our design are built around a local execution cluster that contains simple ALUs, a local register file, and a bypass/writeback bus connecting them. When a block is formed, each instruction in the block is allocated a destination register in the local register file. An instruction that produces block live-outs additionally sends its result to the global register file.</p><p>Shared execution units: In addition to the simple ALUs in each execution backend, the HBA core shares its more expensive execution units (such as floating-point units and load/store pipelines). Execution backends arbitrate for access to these units when instructions are issued (and arbitration conflicts are handled oldest-block-first, with conflicting instructions waiting in skid buffers to retry). Sharing these units between all execution backends amortizes these units' cost <ref type="bibr">[34]</ref>. Memory operations: Execution backends share the Ll cache, the load/store queue (LSQ), and the load/store pipelines. The use of blocks is orthogonal to both the correctness and performance aspects of the memory subsystem: a block preserves memory operation ordering within itself, and allocates loads/stores into the LSQ in original program order. Because our core design achieves similar performance to the baseline core, as we show later, the same memory pipeline throughput as baseline is sufficient. Memory disambiguation does not interact with block atomicity; if a load requires replay, it is sent back to its execution unit as in the baseline. Out-or-order execution backend (Fig. <ref type="figure" target="#fig_3">4a</ref>): This backend imple ments dataflow-order instruction scheduling within a block. The instruction scheduler is bit matrix-based <ref type="bibr" target="#b20">[21,</ref><ref type="bibr">54]</ref>. When a livein is received at the backend, it wakes up dependents as any other value writeback would. Note that because the block execution backend does not need to maintain program order within the block (because blocks are atomic), the backend has no equivalent to a ROB. Rather, it has a simple counter that counts completed instructions and signals block completion when all instructions have executed.</p><p>In order to specialize a block for the out-of-order backend, the block specialization logic (i) pre-renames all instructions in the block, and (ii) pre-computes the scheduling matrix. This information is stored in the block info cache and provided with the block if present.</p><p>Because the out-of-order backend also executes new blocks which have no information in the block info cache, this logic also performs the renaming and computes this information for the first dynamic instance of each new block. Because of this block specialization, the out-of-order backend does not need any renaming logic and does not need any dynamic matrix allocation/setup logic (e.g., the producer tables in Goshima et al. <ref type="bibr" target="#b20">[21]</ref>). These simplifications save energy relative to a baseline out-of-order core. VLIW execution backend (Fig. <ref type="figure" target="#fig_3">4b</ref>): Unlike the out-of-order back end, the VLIW backend has no out-of-order scheduler. Instead, it contains an issue queue populated with pre-formed instruction bundles, and a scoreboard stage that stalls the head of the issue queue until the sources for all of its instructions are ready. The scoreboard implements a stall-on-use policy for long-latency operations such as cache-missing loads and operations on the shared FPU.</p><p>Specialization of blocks for the VLIW backend is more involved than for the out-of-order backend because VLIW execution requires pre-formed bundles of instructions. Rather than require the compiler to form these bundles (which requires a new ISA), the HBA core leverages the existing instruction-scheduling logic in the 000 back end to form bundles dynamically at runtime, as we now describe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Combining Out-oJ-Order and VLIW Execution</head><p>To combine out-of-order and VLIW block execution backends, we propose memoized scheduling. Memoized scheduling exploits the observation (seen in ?ll) that blocks often exhibit schedule stability.</p><p>The key idea is to first use an out-of-order execution backend to execute a block and observe its schedule stability. Each time the block executes on this backend, its instruction schedule (as executed) is recorded. If the instruction schedule of the block remains stable (i.e., changes very little or not at all) over multiple executions of that block, the block of code is converted to use a VLIW backend. (Our evaluations use a threshold of four consecutive executions that use exactly the same schedule.) The recorded instruction schedule is taken as the set of instruction bundles for a VLIW backend. The VLIW backends are designed to have the same issue width and functional units as the out-of-order backends so that the recorded schedule can be used as-is. Thus, the schedule is recorded and replayed, or memoized.</p><p>If the block's schedule remains stable, subsequent executions of the block on the VLIW backend will obtain the same performance as if the out-of-order backend were used, while saving the energy that instruction scheduling would have consumed. However, if the schedule becomes unstable (e.g., due to changing cache-miss behavior or livein timing), subsequent executions may experience false stalls, or cycles in which a VLIW bundle stalls because some of its instructions are not ready to execute, but at least one of the contained instructions is ready and could have executed if it were not bundled. These stalls result in performance loss compared to execution on an out-of-order backend. To minimize this potential loss, the VLIW backend monitors false stall cycles. If the number of such cycles for each block (as a ratio of all execution cycles for that block) exceeds a threshold (5% in our evaluations), the memoized schedule is discarded and the block executes on an out-of-order backend next time it is dispatched.</p><p>Unified OoONLIW Backend: We observe that a VLIW back end's hardware is almost a subset of the out-of-order backend's hardware. The pipeline configurations are identical and only the scheduler is different. Thus, we use a single unified backend that simply turns off its out-of-order scheduling logic (bit-matrix) when it is executing in VLIW mode. (MorphCore [30] exploits a similar observation to combine in-order and out-of-order scheduling.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Reducing Execution Power</head><p>We reduce execution power on the VLIW backend in two ways. Dynamic Pipeline Narrowing: We observe many blocks cannot utilize the full width of a VLIW backend. To exploit this, the VLIW block formation process records the maximum bundle width across all VLIW bundles of a block. When a VLIW backend executes a block, it dynamically narrows its issue width to this maximum, saving static and dynamic energy, similar to <ref type="bibr">[27]</ref>. (An "in-order" backend is simply a VLIW backend that has reduced its width to one.) These savings occur via clock and power gating to unused execution units. Dead Write Elision: In a block executing on a VLIW backend, any values that are used only while on the bypass network need not be written to their local destination register <ref type="bibr">[49]</ref>. Similarly, any values that are never used while on the bypass network need not be written to the bypass network, but only to the register file. The VLIW block formation process detects values with such unnecessary writes and marks them as such, saving energy during execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHODOLOGY</head><p>System Configuration: We evaluate our example HBA design against several baseline core designs, modeling each core and memory faithfully. Ta ble I shows the main system parameters we model. Power Model: To model core power/energy, we use a modified version of <ref type="bibr">McPAT [36]</ref>. To model HBA's energy, we use McPAT's component models to construct a faithful model. We assume 2GHz operation for all core designs. We replaced McPAT's ALU model with a custom, more accurate, Veri log model we developed and synthesized for a commercial process with Synopsys tools. We provide all numbers and formulas of our model in a technical report <ref type="bibr" target="#b15">[16]</ref>.</p><p>One parameter of our model is the sensitivity of design to static power. The parameters in our model are based on a 28nm FDSOI (fully depleted silicon on insulator) process technology as described in <ref type="bibr" target="#b18">[19]</ref>. Depending on operating conditions and the choice of low  Vi (fast, leaky) or regular Vi (slow, less leaky) devices, the relative contribution of static and dynamic power may vary. For example, leakage can be 15% of total power for a processor implemented with fast, low Vi devices operating at nominal voltage (0.9V) <ref type="bibr" target="#b18">[19]</ref>. The use of regular leakage devices will reduce leakage power by about an order of magnitude but will reduce performance by about 10-15%.</p><p>Results will change depending on the characteristics of the underlying process technology and choice of operating voltage. We focus on two evaluation points: worst-case leakage (all fast low-Vi devices at 0.9V), resulting in 15% of total power, and more realistic leakage with a 50%/50% mix of low-Viand high-Vi devices, resulting in 10% of total power. A real design <ref type="bibr" target="#b11">[12]</ref> might use both types by optimizing critical path logic with fast transistors while reducing power in non critical logic with less leaky transistors. Our main analysis assumes 10% leakage but we summarize key results for 15% leakage in ?V-A.</p><p>Our power gating mechanism gates (i) scheduling logic in back ends when they are in VLIW mode, (ii) superscalar ways when backends execute narrow VLIW blocks, and (iii) shared execution units (FPUs and the multiplier) in both HBA and in the baseline. Workloads: We evaluate IS4 distinct checkpoints, collected from the SPEC CPU2006, Olden [50], <ref type="bibr">MediaBench [35]</ref> suites, and many real workloads: Firefox, FFmpeg, Adobe Flash player, VS Javascript engine, GraphChi graph-analysis framework, MySQL, the lighttpd web server, I5f E X, Octave (a MATLAB replacement), and an xS6 simulator. Many of these workloads have multiple checkpoints at multiple representative regions as provided by PinPoints [4S]. All checkpoints are listed in <ref type="bibr" target="#b15">[16]</ref>, along with their individual performance and energy consumption on each of the evaluated core models. Baselines: We compare HBA to four core designs. First, we compare to two variants of a high-performance out-of-order core: (i) one with a monolithic backend (scheduler, register file, and execution units), (ii) one with a clustered microarchitecture that splits these structures into separate clusters and copies values between them as necessary (e.g., <ref type="bibr">[IS]</ref>). The clusters have equivalent scheduler size and issue width as the block execution backends in the HBA core. Second, we compare to two variants of a coarse-grained heterogeneous design that combines an out-of-order and an in-order core [37] (iii) without clustering and (iv) with clustering. We model an ideal controller for this coarse-grained design, thus providing an upper bound on efficiency and performance relative to the real controller-based mech anism of [37].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATION</head><p>Summary: We will show that three main conclusions hold: (i) HBA has nearly the same performance as a baseline 4-wide out of-order core, with only 1 % performance degradation on average; (ii) HBA saves 36% of average core power relative to this baseline; (iii) HBA is the most energy-efficient design among a large set of evaluated core designs ( ?V-C summarizes this result by evaluating 390 a variety of core designs that fall into different power-performance tradeoff points).</p><p>We analyzed HBA and other baselines extensively but can report only some analyses below due to space constraints. Our technical report <ref type="bibr" target="#b15">[16]</ref> provides additional results, including sensitivity studies, power model details, individual benchmark results and more analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Power</head><p>The main benefit of HBA is that it saves significant core energy (i.e., average core power). Ta ble II shows average core power and Energy Per Instruction (EPI) for six core designs: baseline out-of order, clustered out-of-order <ref type="bibr">[IS]</ref>, coarse-grained heterogeneous [37], coarse-grained heterogeneous combined with clustered out-of-order, HBA with only out-of-order backends, and HBA with heterogeneous backends. HBA (row 6) reduces core power by 36.4% and EPI by 31.9% over a baseline out-of-order core (row 1). HBA is also the most energy-efficient core design in both core power and EPI.  To provide insight into these numbers, Fig. <ref type="figure" target="#fig_5">5</ref> shows a breakdown of the EPI. We make several major observations:</p><p>1. Energy reductions in HBA occur for three major reasons: (i) decoupled execution backends, (ii) block atomicity and (iii) hetero geneity. The clustered out-of-order core, which has execution clusters configured equivalently to HBA (item (i), saves S.3% in EPI over the baseline monolithic core (first to second bar). Leveraging block atomicity (item (ii), the HBA design that uses only out-of-order execution backends reduces energy by a further 17.2% (second to fifth bar). Making use of all heterogeneous execution backends (item (iii) reduces energy by an additional 6.4% (fifth to sixth bar). 2. Decoupled execution backends: the clustered core saves instruction scheduling energy because each cluster has its own scheduling logic operating independently of the other clusters. Thus, the RS (scheduler) power reduces by 71 % from the first to second bar in Fig. <ref type="figure" target="#fig_5">5</ref>.</p><p>3. Block atomicity: Even without heterogeneous backends, HBA saves energy in renaming (RAT), program-order sequencing/retire (ROB), and global register file as it tracks blocks rather than instructions. Savings are because: (i) the block core renames only liveouts, rather than all written values, so RAT accesses reduce by 62%; (ii) the block core dispatches/retires whole blocks at a time and stores information about only liveouts in the ROB, reducing ROB accesses by 74%; and (iii) only 60% of register file accesses go to the global register file.</p><p>? 2 r---------------?--,-?r--r--? ?--, 4. Heterogeneity: the HBA design with all mechanisms enabled saves energy in (i) instruction scheduling, due to the use of VLIW backends for 61 % of blocks, (ii) the register file and the bypass network: dynamic pipeline narrowing for narrow blocks causes 21 % of allj.wps to execute on a narrow pipe, and dead write elision eliminates 44% of local RF writes and 19% of local bypass network writes.</p><p>5. The state-of-the-art coarse-grained heterogeneous core [37] saves energy in both the out-of-order logic (RAT, ROB, and RS) and execution resources (bypass buses and register file) as it can use the in-order backend a portion of the time. However, when using the out-of-order backend, it cannot improve energy-efficiency. HBA saves additional energy because it can exploit finer-grained heterogeneity.</p><p>6. Using a clustered out-of-order backend in the coarse-grained heterogeneous core (Coarse, Clustered) reduces EPIIpower more than either the clustered core or coarse-grained core alone. However, this comes with higher performance degradation than any of the designs (2.S%, row 4 of Ta ble II). HBA outperforms this coarse-grained, clustered design in IPC, power and EPI, as Table <ref type="table" target="#tab_3">II</ref> and Fig. <ref type="figure" target="#fig_5">5</ref> show.</p><p>Overall, these results show that HBA reduces core energy signif icantly compared to all baseline designs, including the non-clustered and clustered versions of a state-of-the-art heterogeneous core <ref type="bibr">[37]</ref>, by leveraging block atomicity and heterogeneity synergistically.</p><p>Sensitivity: These savings are robust to power modeling assump tions. The above evaluation assumes leakage comprises lO% of total power (see ?IV). If we assume 15% leakage power, worst case in our process, HBA still reduces average core power by 21.9% and EPI by 21.1 % over the baseline out-of-order core.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance</head><p>1) Performance of HBA VS. Baselines: Table <ref type="table" target="#tab_3">II</ref> shows nor malized average (geometric mean) performance for our baseline and HBA evaluation points. Several major conclusions are in order:</p><p>1. The HBA design (row 6) reduces average performance by only 1.0% over the out-of-order core. This is a result of (i) equivalent instruction window size, yielding similar MLP for memory-bound programs, and (ii) performance gain due to higher available issue width that balances performance loss due to inter-block communica tion latency, as explained in ?V-B2.</p><p>2. When all blocks are executed on out-of-order backends only (row 5), the HBA design improves average performance by 0.4% over baseline, and 1.4% over nominal HBA (row 6). Thus, memoized scheduling has some performance penalty (as it sometimes sends a block to VLIW backends although dynamic scheduling would be better), but this penalty is modest (1.0%) for the energy reduction it obtains.</p><p>3. HBA provides similar performance to the coarse-grained heteroge neous core (row 3), but has much lower power/EPI. HBA saves more energy as it uses VLIW backends for fine-grained blocks, exposing more opportunity, and uses memoized scheduling to enable more blocks to use these backends. The coarse-grained design executes a longer chunk of the program on only one core at a time, and with strict in-order (as opposed to memoized) scheduling, so it must limit its use of the efficient in-order core to maintain performance.</p><p>4. Using a clustered out-of-order backend in the coarse-grained heterogeneous core (row 4) degrades performance over either the clustered or coarse-grained core alone (rows 2,3) due to the additive overheads of clustering <ref type="bibr">[IS]</ref> and coarse-grained heterogeneity <ref type="bibr">[37]</ref>. HBA (rows 5,6) has higher performance and efficiency than this design.</p><p>2) Additional Analysis: To understand HBA's performance and potential, we perform several limit and control studies. Table <ref type="table" target="#tab_5">III</ref> shows an out-of-order design as (i) its issue width is widened, and (ii) its fetchlretire width bottlenecks are removed. It also shows HBA (without heterogeneous backends) as inter-block communication latency is removed and the fetch width bottleneck is alleviated. We make several conclusions:</p><p>1. Inter-block communication latency penalizes performance. The "instant inter-block communication" HBA design (row 5) has 6.2% higher performance than the baseline (row 1).  2. Higher aggregate issue rate increases HBA's performance. This arises because each block execution backend has its own scheduler and ALUs that work independently, extracting higher ILP than in the baseline. This effect is especially visible when inter-block latency is removed, and is responsible for HBA's 6.2% IPC increase above baseline.</p><p>3. Higher issue rate alone cannot account for all of the idealized HBA's performance. To see this, we evaluate a 64-wide out-of-order core with a monolithic scheduler (row 2). Such a design performs only 2.1 % better than baseline (row 2), in contrast to 6.2% for idealized HBA (row 5). Thus, other effects are present that make HBA better.</p><p>4. In particular, the remaining advantage of HBA is due to block wide dispatch and retire: as HBA tracks precise state only at block boundaries, it achieves high instruction retire throughput when the backend executes a region with high ILP. Allowing for block-wide (16 /top-wide) fetch/dispatch/retire in both the out-of-order and HBA designs, we observe 23.6% (000, row 3) and 23.1% (HBA, row 6) performance above baseline, respectively. Hence, HBA is capable of harnessing nearly all ILP discovered by an ideal out-of-order design, subject only to inter-block communication latencies and fetch/retire bottlenecks.</p><p>3) Per-Application Energy and Performance: C. Power-Performance Tradeoff Space Fig. <ref type="figure" target="#fig_7">7</ref> shows multiple HBA and baseline core configurations in the 2D power-performance tradeoff space. HBA variants are labeled as "HBA(number of block backends, other options)," with options including 000 (out-of-order backends only) and 2-wide (all block backends are 2-wide). The plot compares HBA configurations against several out-of-order baselines with various window sizes, 1-, 2-, and 4-wide in-order baselines, and several coarse-grained heterogeneous cores. We conclude that (i) HBA is the most energy-efficient design (closest to the bottom-right corner), (ii) HBA's power-performance tradeoff is widely configurable, and (iii) HBA enables new points in the power-performance tradeoff space, not achievable by past designs.</p><p>3 Two types of code perform poorly on HBA: code with hard-to-predict branches, leading to block squashes, and code with long dependence chains, leading to high inter-block communication.</p><p>4 Workloads that perform best on HBA are largely those with regular code that can execute independent chunks in each backend. D. Symbiotic Out-of-Order and VLIW Execution Fig. <ref type="figure">8</ref> shows a sorted curve of per-benchmark fraction of out of-order vs. VLIW blocks. Most benchmarks execute both types of blocks often, with few having either all out-of-order or all VLIW blocks. In a few benchmarks on the left, almost all blocks (greater than 90%) execute as VLIW: for such benchmarks, learning one instruction schedule per block is sufficient to capture most of the benefit of out-of-order execution. Of the VLIW blocks, 32.7% are 2-wide, 3.3% are I-wide. Thus, dynamic pipeline narrowing yields significant energy savings.</p><p>? "" () ? I ,------------------------, ""@ 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Core Area</head><p>Like past heterogeneous designs (e.g., <ref type="bibr" target="#b23">[24,</ref><ref type="bibr">33,</ref><ref type="bibr">62]</ref>), HBA opti mizes for a future in which energy and power (and not core area) are key performance limiters <ref type="bibr" target="#b13">[14]</ref>. Similar to such designs, HBA's power reduction comes at the cost of chip area. We use McPAT, which provides a rough area estimate by estimating growth in key structures in the core. McPAT reports that our our initial HBA implementation increases core area by 62.5% over the baseline out-of-order core. For comparison, McPAT estimates 20% area overhead for <ref type="bibr">[37]</ref>. Note that we did not aim to optimize for area in our HBA implementation in order to freely explore the power/performance tradeoff space.</p><p>Although HBA comes with area overhead, cores actually occupy a relatively small area in a modern mobile SoC (e.g., 17% in Apple's A7 <ref type="bibr" target="#b10">[11]</ref>). Hence, the investment of additional core area to improve system energy efficiency can be a good tradeoff. By trading off die area for specialization, HBA: (i) achieves large energy savings that were not attainable in a smaller core ( ?V-A), and (ii) enables new power-performance tradeoff points not previously achievable ( ?V-C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RELATED WORK</head><p>HBA is the first heterogeneous core design that enables the concurrent execution of very fine-grained code blocks (tens of in structions long) on the most efficient backend for each block. It combines several concepts, including heterogeneity, atomic block based execution, and instruction schedule memoization, in a holistic manner to provide a new heterogeneous core design framework for energy-efficient execution. Though these concepts have been applied individually or in some combination (as we discuss briefly below), no past work applied them in the manner HBA does to dynamically find the best execution backend for each fine-grained code block. Other novel contributions were discussed at the end of ?I.</p><p>Forming and Reusing Instruction Schedules: Several works use one execution engine to schedule/format instructions within a code block, cache the scheduled/formatted code block, and reuse that schedule on a simpler execution engine when the same code block is encountered later. An early example is DIF (Dynamic Instruction Formation) <ref type="bibr">[43]</ref>, which uses a simple in-order engine and a hardware-based instruction scheduler to schedule instructions in a code block the first time it is encountered. Later instances of the same code block are always executed on a prima ry VLIW engine.</p><p>DIF thus uses an in-order engine to format the code to be executed on a VLIW engine. Similarly, Transmeta processors [32] use Code Morphing software to translate code for execution on a VLIW engine. <ref type="bibr">Banerjia et al. [5]</ref> propose a similar high-level approach that pre schedules instructions and places them in a "schedule cache" for later execution. A later example, ReLaSch [44], moves the out-of order instruction scheduler to the commit stage. The first time a code block is encountered, its schedule is formed by this commit-stage scheduler. Later instances of the same code block are always executed in the primary in-order scheduler. ReLaSch thus uses an out-of-order instruction scheduler to format the code to be executed on an in order scheduler. Several other works combine a "cold pipeline" and "hot pipeline" that execute infrequent (cold) and frequent (hot) code traces respectively <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr">51]</ref>, and use various mechanisms to form the traces to be executed by the "hot pipeline". No ne of the above designs dynamically switch the backend the code block executes on: once a code block is formatted/scheduled, it is always executed on the prima ry backend/engine, regardless of whether or not the schedule is effective. In contrast, HBA dynamically determines which backend is likely the best for any given instance of a code block: e.g., a code block may execute on the VLIW backend in one instance and in the 000 backend in the next. In other words, there is no "primary" or "hot" backend in HBA, but rather, the backends are truly equal and the best one is chosen depending on code characteristics.</p><p>Yoga <ref type="bibr">[63]</ref>, developed concurrently with HBA, can switch its backend between out-of-order and VLIW modes. HBA can exploit heterogeneity, and therefore adapt to characteristics of code blocks, at a finer granularity than Yoga as it can concurrently execute blocks in different (VLIW and 000) backends whereas Yoga uses only one type of backend at a time. In addition, HBA's heuristic for switching a block from VLIW to 000 mode takes into account the "goodness" of the VLIW schedule (hence, the potential performance loss of staying in VLIW mode) whereas Yoga switches to 000 mode when an optimized VLIW frame does not exist for the code block.</p><p>Coarse-grained Heterogeneous Cores: Several works propose the use of statically heterogeneous cores <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr">10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr">28,</ref><ref type="bibr">33,</ref><ref type="bibr">58,</ref><ref type="bibr">59]</ref>, dynamically heterogeneous cores <ref type="bibr" target="#b25">[26,</ref><ref type="bibr">30,</ref><ref type="bibr">31,</ref><ref type="bibr">60]</ref>, one core with heterogeneous backends <ref type="bibr">[37]</ref>, or a core with variable parameters <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23]</ref> to adapt to the running application at a coarse granularity for better efficiency. We quantitatively compared to a coarse-grained heterogeneous approach [37] in ?V and showed that although coarse-grained designs can achieve good energy-efficiency, HBA does better by exploiting much finer-grained heterogeneity. However, combining these two levels of heterogeneity might lead to further gains, which is a promising path for future work to explore.</p><p>Atomic Block-Based Execution: Many past works (e.g., <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr">38,</ref><ref type="bibr">39,</ref><ref type="bibr">42,</ref><ref type="bibr">47,</ref><ref type="bibr">53,</ref><ref type="bibr">57]</ref>) exploited the notion of large atomic code blocks to improve performance, efficiency and design simplicity. HBA borrows the notion of block atomicity and uses it as a mechanism to exploit fine-grained heterogeneity.</p><p>Other Related Works: <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">40]</ref> propose pre-scheduling instruc tions to save complexity or improve parallelism. HBA takes advantage of the benefits of prescheduling by reusing instruction schedules. We adapt the ideas of dynamic pipeline narrowing and "software-based dead write elision" respectively from <ref type="bibr">[27] and [49]</ref> to HBA. Note that none of these past works exploit heterogeneity as HBA does.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>This paper introduces the Heterogeneous Block Ar chitecture (HBA) to improve energy efficiency of modern cores while maintain ing performance. HBA combines fine-grained heterogeneity, atomic code blocks, and block-based instruction scheduling to adapt each fine-grained (tens of instructions long) code block to the execution backend that best fits its characteristics. Our extensive evaluations of an initial HBA design that can dynamically schedule atomic code blocks to out-of-order and VLIW/in-order execution backends using simple heuristics demonstrate that this HBA design (i) greatly improves energy efficiency compared to four state-of-the-art core designs and (ii) enables new power-performance tradeoff points in core design. We believe HBA provides a flexible execution substrate for exploiting fine-grained heterogeneity in core design, and hope that future work will investigate other, more aggressive, HBA designs with more specialized backends (e.g., SIMD, fine-grained reconfigurable, and coarse-grained reconfigurable logic), leading to new core designs that are even more energy-efficient and higher-performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Coarse-grained vs. fine-grained heterogeneity.</figDesc><graphic url="image-4.png" coords="2,93.13,339.83,150.72,134.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>IFig. 2 :</head><label>2</label><figDesc>Fig. 2: Fraction of chunks in the instruction stream that have a different schedule than their previous instance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3</head><label>3</label><figDesc>Fig.3illustrates the basic HBA design. The core consists of three major parts: (i) block fetch, (ii) block sequencing and communication, and (iii) block execution. We discuss each in turn.</figDesc><graphic url="image-9.png" coords="3,86.40,247.67,177.60,79.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Block execution backend designs.</figDesc><graphic url="image-12.png" coords="4,70.08,431.03,210.24,52.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Simulator: We employ an in-house cycle-accurate simulator that is execution-driven. We faithfully model all major structures and algorithms within the HBA core and baseline cores, carry values through the model, and check against a functional model to ensure correctness. The model implements the user-mode x86-64 ISA by cracking instructions into /-tops (using a modified version of the PTLsim [64] decoder).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Energy-per-Instruction (EPI) of six core designs.</figDesc><graphic url="image-16.png" coords="5,366.73,567.36,187.20,105.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 Fig. 6 :</head><label>66</label><figDesc>Fig. 6: HBA performance and EPI relative to baseline 000.</figDesc><graphic url="image-29.png" coords="6,370.93,470.27,160.08,53.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Power-performance tradeoff space of core designs.</figDesc><graphic url="image-31.png" coords="7,72.96,50.87,223.68,108.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>8 .Fig. 8 :</head><label>88</label><figDesc>Fig. 8: Per-benchmark block type breakdown.</figDesc><graphic url="image-33.png" coords="7,125.76,306.23,125.76,51.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Major system parameters used in evaluation.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>I</head><label></label><figDesc>Row I Configuration II LS: Power I LS: EPI I LS: IPC I</figDesc><table><row><cell>1 2 3 4 5 6</cell><cell>4-wide 000 (Baseline) 4-wide Clustered 000 [I8] Coarse-grained [37] Coarse-grained, Clustered HBA, 000 Backends Only HBA, OoONLIW</cell><cell>--1l.5% -5.4% -16.9% -28.7% -36.4%</cell><cell>--8.3% -8.9% -17.3% -2 5. 5% -3 1. 9%</cell><cell>--1.4% -l.2% -2.8% +0.4% -1.0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc></figDesc><table /><note><p>Power, EPI, and performance vs. baseline out-of-order execution core.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Limit studies and control experiments.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>The use of a block info cache in parallel with an instruction cache, rather than a full trace cache[45, 52], allows the HBA core to approximate the best of both worlds: it achieves the space efficiency of the instruction cache while retaining the learned information about code blocks.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMEN TS</head><p>We especially thank the members of the SAFARI research group for helpful feedback while developing this work. We thank many of the current and previous reviewers of this paper as well as <rs type="person">David Hansquine</rs> for their valuable feedback and suggestions. This work was supported by the <rs type="funder">Intel</rs> <rs type="grantName">URO Swiss Army Processor Program grant</rs>, the <rs type="funder">Qualcomm Innovation Fellowship Program</rs>, gifts from <rs type="funder">Oracle</rs>, and <rs type="funder">NSF</rs> Awards <rs type="grantNumber">CCF-1l47397</rs> and <rs type="grantNumber">CCF-1212962</rs>. We also thank our industrial partners and the <rs type="funder">Intel Science and Technology Center</rs> for the support they provide. <rs type="person">Chris Fallin</rs> was supported by an <rs type="funder">NSF</rs> <rs type="grantName">Graduate Fellowship</rs>. <rs type="person">Onur Mutlu</rs> is supported by an <rs type="funder">Intel</rs> <rs type="grantName">Early Career Faculty Honor Program Award</rs> and an <rs type="funder">IBM</rs> <rs type="grantName">Faculty Partnership Award</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_VVKMUab">
					<orgName type="grant-name">URO Swiss Army Processor Program grant</orgName>
				</org>
				<org type="funding" xml:id="_7GPUbFw">
					<idno type="grant-number">CCF-1l47397</idno>
				</org>
				<org type="funding" xml:id="_sNDwXNx">
					<idno type="grant-number">CCF-1212962</idno>
				</org>
				<org type="funding" xml:id="_jMfPppT">
					<orgName type="grant-name">Graduate Fellowship</orgName>
				</org>
				<org type="funding" xml:id="_A8EQS85">
					<orgName type="grant-name">Early Career Faculty Honor Program Award</orgName>
				</org>
				<org type="funding" xml:id="_YMQRPEx">
					<orgName type="grant-name">Faculty Partnership Award</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Validity of the sin ,g le processor approach to achieving large scale computing capabilities</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Amdahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A FlPS</title>
		<imprint>
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mitigating Amdahl&apos;s law through EPI throttling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Annavaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grochowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Shen</surname></persName>
		</author>
		<idno>ISCA-32</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">White paper: Big.LITTLE processing with ARM Cortex AI5 &amp; Cortex-A7</title>
		<author>
			<persName><surname>Arm Ltd</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Sept 201 I</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Power and energy reduction via pipeline balancing</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I</forename><surname>Bahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Manne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA-28</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">MPS: Miss-path scheduling for multiple-issue processors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sathaye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Menezes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Conte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TC</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic thread assignment on heteroge neous multiprocessor architectures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Becchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Crowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JlLP</title>
		<imprint>
			<date type="published" when="2008-06">June 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Turboscalar: A high frequency high IPC microarchitecture</title>
		<author>
			<persName><forename type="first">B</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>WCED</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scaling to the end of silicon with EDGE architectures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Kckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dahlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">BulkSC: bulk enforcement of sequential consistency</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tuck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Montesinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>To</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>ISCA-34</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Efficient program scheduling for heteroge neous multi-core architectures</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">46</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Inside the Apple A7 from the iPhone 5s -Updated</title>
		<ptr target="http://www.chipworks.com!enltechnical-competitive-analysis/resources/bloglinside-the-a7/" />
		<imprint/>
	</monogr>
	<note>Chip Works</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Designing a 3 GHz, 130 nm, Intel Pentium 4 processor</title>
		<author>
			<persName><forename type="first">D</forename><surname>Deleganes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Sym. on VLSI Circuits</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Working sets past and present</title>
		<author>
			<persName><forename type="first">P</forename><surname>Denning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TSE</title>
		<imprint>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Dark silicon and the end of multicore scaling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Es Maeilzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Blem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Amant</surname></persName>
		</author>
		<idno>ISCA-38</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling critical sections in Amdahl&apos;s law and its implications for multicore design</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The Heterogenous Block Architecture</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fallin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><surname>Mutlu</surname></persName>
		</author>
		<idno>No. 2014-001</idno>
		<imprint>
			<date type="published" when="2014-03">Mar 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">SAFARI Te chnical Report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Power provisioning for a warehouse-sized computer</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W-D</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The multicIuster architecture: Reducing cycle time through partitioning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Vranesic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note>MICRO-30</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Planar fully depleted silicon technology to design competitive SOC at 28nm and beyond</title>
		<author>
			<persName><forename type="first">P</forename><surname>F1atresse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cesana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cauchy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">STMi croelectronics White Paper</title>
		<imprint>
			<date type="published" when="2012-02">Feb. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scheduling for heterogeneous processors in server systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rawson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CF</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A high-speed dynamic instruction scheduling scheme for superscalar processors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Goshima</surname></persName>
		</author>
		<imprint>
			<publisher>MICRO-34, 200 I</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Power Considerations in the Design of the Alpha 21264 Microprocessor</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Gowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DAC</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Best of both latency and throughput</title>
		<author>
			<persName><forename type="first">E</forename><surname>Grochowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ronen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCD</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Bundled execution of recurring traces for energy efficient general purpose processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>MICRO-44</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Increasing the instruction fetch rate via block-structured instruction set architectures</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P. -Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Evers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Core fusion: Accommodating software diversity in chip multiprocessors</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ipek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Conte</surname></persName>
		</author>
		<idno>38] [39] [40] [4 1] [42] [43] [44] [45] [46] [47] [48] [49] [SO] [5 1] [52] [53] [54] [55] [56] [57] [58] [59] [60] [61] [62] [63</idno>
		<imprint>
			<date type="published" when="2007">2007. 2009</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
	<note>ISCA-34. Length adapative processors: A solution for energy/performance dilemma in embedded systems,&quot; INTERA CT</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">MorphCore: An energy-efficient microarchitecture for high performance IL P and high throughput TLP</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Joao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Suleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Joao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Suleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Khubaib et aI</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sethumadhavan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Govindan</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007">2012. 2013. 2007</date>
		</imprint>
	</monogr>
	<note>MICRO-45, 2012. et aI. , &quot;Composable lightweight processors,&quot; MICRO-40</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The technology behind Crusoe processors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Klaiber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Single-IS A heterogeneous multI-core architectures: The potential for processor power reduction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<idno>MICRO-36</idno>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Conjoined-core chip multiprocessing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>MICRO-37</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">MediaBench: A tool for evaluating and synthesizing multimedia and communications systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note>MICRO-30</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">McPAT: An integrated power, area, and timing modeling framework for multi core and manycore architectures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<idno>MICRO-42</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Composite cores: Pushing heterogeneity into a core</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lukefahr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Padmanabha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Sleiman</surname></persName>
		</author>
		<idno>MICRO-45</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Hardware support for large atomic units in dynamically scheduled machines</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Melvin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Enhancing instruction scheduling with a block-structured ISA</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Melvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">UPP</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Data-How prescheduling for large instruc tion windows in out-of-order processors</title>
		<author>
			<persName><forename type="first">P</forename><surname>Michaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>HPCA-7, 200l</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Power: a first-class architectural design constraint</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Exploiting instruction level parallelism in processors by caching scheduled groups</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sankaralingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">R</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hopkins</surname></persName>
		</author>
		<idno>ISCA-24</idno>
		<imprint>
			<date type="published" when="1997">2001. 1997</date>
		</imprint>
	</monogr>
	<note>A design space evaluation of grid processor architectures</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Reusing cached schedules in an out-of-order processor with in-order issue logic</title>
		<author>
			<persName><forename type="first">O</forename><surname>Palomar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCD</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Trace cache design for wide issue superscalar processors</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Patel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>University of Michigan</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Increasing the size of atomic instruction blocks using control How assertions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Patel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>MICRO-33</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">re PLay: a hardware framework for dynamic optimization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Lumetta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TC</title>
		<imprint>
			<date>June 200l</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Pinpointing representative portions of large Intel Itanium programs with dynamic instrumentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Patil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>MICRO-37</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficient code genera tion for horizontal architectures: Compiler techniques and architectural support</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Rau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Glaeser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Supporting dynamic data structures on distributed memory machines</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carlisle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Reppy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hendren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>ACM TOPLAS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Power awareness through selective dynamically optimized traces</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rosner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA-3J</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Trace cache: A low latency approach to high bandwidth instruction fetching</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rotenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">E</forename><surname>Rotenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sazeides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996. 1997</date>
		</imprint>
	</monogr>
	<note>MICRO-29. Trace processors,&quot; MICRO-30</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Matrix scheduler reloaded</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sassone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ruple)'</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brekelbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>ISCA-34</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">A 64 Kbytes ISL-TAGE branch predictor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2, 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Automatically characterizing large scale program behavior</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Facilitating superscalar processing via a combined static/dynamic register renaming scheme</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sprangle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Patt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>MICRO-27</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">ACMP: Balancing hardware efficiency and programmer efficiency</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Suleman</surname></persName>
		</author>
		<idno>TR-H PS-2007-001</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="report_type">HPS Tech. Report</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Acceleratin ?, critical section execution with asymmetric multi-core architectures</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Suleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Palt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>ASPLOS-XIV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Scheduling hetero geneous multi-cores through Performance Impact Es timation (PIE)</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tarj An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Skadron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
		<idno>ISCA-39</idno>
	</analytic>
	<monogr>
		<title level="m">DAC, 2008. K. van Craeynest, A. Ja1eel</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>Federation: Repurposing scalar cores for out-of-order instructIOn Issue</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Conservation cores: reducing the energy of mature computations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goulding</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>ASPLOS-XV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Yoga: A hybrid dynamic VLIW/OoO processor</title>
		<author>
			<persName><forename type="first">C</forename><surname>Viuavieja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Joao</surname></persName>
		</author>
		<idno>TR-H PS-2014-00 1</idno>
		<imprint/>
	</monogr>
	<note type="report_type">HPS Tech. Report</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">PTLsim: A cycle accurate full system x86-64 microarchi tectural simulator</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPASS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
