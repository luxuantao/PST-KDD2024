<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Island Loss for Learning Discriminative Features in Facial Expression Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-10-23">23 Oct 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jie</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of South Carolina</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zibo</forename><surname>Meng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of South Carolina</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ahmed</forename><surname>Shehab Khan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of South Carolina</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of South Carolina</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><surname>O'reilly</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of South Carolina</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Tong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of South Carolina</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Island Loss for Learning Discriminative Features in Facial Expression Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-10-23">23 Oct 2017</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1710.03144v3[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Over the past few years, Convolutional Neural Networks (CNNs) have shown promise on facial expression recognition. However, the performance degrades dramatically under real-world settings due to variations introduced by subtle facial appearance changes, head pose variations, illumination changes, and occlusions.</p><p>In this paper, a novel island loss is proposed to enhance the discriminative power of the deeply learned features. Specifically, the IL is designed to reduce the intra-class variations while enlarging the inter-class differences simultaneously. Experimental results on four benchmark expression databases have demonstrated that the CNN with the proposed island loss (IL-CNN) outperforms the baseline CNN models with either traditional softmax loss or the center loss and achieves comparable or better performance compared with the state-of-the-art methods for facial expression recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>As one of the most expressive parts of human, the face has been extensively studied in various active research fields. Automatic recognition of facial expression has attracted significant attention because of its importance and wide range of applications in human-computer interaction (HCI), such as interactive games, intelligent transportation, and animation. However, facial expression recognition in the wild is still a challenging problem because of high intra-class variations and high inter-class similarities caused by diversity in head pose, illumination, occlusions, and personal attributes.</p><p>As one of the major steps of facial expression recognition, features are extracted from either static images or videos to capture the appearance/geometry changes related to a target facial expression. In the past decades, various human crafted features have been adopted for facial expression recognition. Most recently, deep features learned by deep convolutional neural networks (CNNs) have achieved promising results on facial expression recognition especially in more challenging settings <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b2">[3]</ref>.</p><p>Traditional CNNs are optimized using a softmax loss, which penalizes the misclassified samples and thus forces the features of different classes staying apart. As illustrated in Fig. <ref type="figure">1 (a)</ref>, the learned features form clusters corresponding to different expressions in the feature space. However, due to high intra-class variations, the features in each cluster are often scattered. Furthermore, the clusters overlap because of high inter-class similarities. Most recently, an additional center loss was introduced into CNNs <ref type="bibr" target="#b43">[44]</ref> to reduce the intra-class variations of the learned features for face recognition. As shown in Fig. <ref type="figure">1</ref>  An illustration of deep features learned by (a) a softmax loss, (b) a softmax loss + a center loss, and (c) a softmax loss + an island loss in the feature space. Inclusion of a center loss pulls the features of the same expression towards their centers denoted by a cross; while the island loss not only compresses the clusters individually, but also pushes the two clusters apart. Best viewed in color.</p><p>their corresponding centers with smaller intra-class variations compared to those learned only using softmax loss. However, the inter-class similarity was not considered in the center loss. This motivates us to further enhance the discriminative power of the learned deep features by increasing the differences between different expressions. Specifically, as depicted in Fig. <ref type="figure">1(c</ref>), we propose an island loss to simultaneously compress each cluster and push cluster centers apart as isolated "islands".</p><p>To demonstrate the effectiveness of the proposed island loss, a CNN with the island loss (IL-CNN) is developed for facial expression recognition. As illustrated in Fig. <ref type="figure" target="#fig_2">2</ref>, the IL-CNN architecture includes three convolutional layers, each of which is followed by a PReLU layer and a batch normalization (BN) layer. A max pooling layer is employed after each of the first two BN layers. Following the third convolutional layer, two fully-connected layers are used to generate the representation for each input sample. An island loss is calculated at the second fully-connected layer. Finally, a softmax loss is calculated at the decision layer to produce the distribution over the target expressions and to calculate the classification errors. The island loss and the softmax loss are jointly minimized to drive the fine-tuning process in the CNN training.</p><p>In summary, our major contributions are:</p><p>-Proposing a novel loss function with the island loss, which aims to learn representations with lower intraclass variations and higher inter-class distances; and -Developing an IL-CNN with the proposed island loss to learn discriminative features for facial expression recognition. The proposed IL-CNN was evaluated on three wellknown posed facial expression databases, i.e. Extended Cohn-Kanade database (CK+) <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b23">[24]</ref>, Oulu-CASIA database <ref type="bibr" target="#b49">[50]</ref> and MMI database <ref type="bibr" target="#b30">[31]</ref>. More importantly, it was also evaluated on a spontaneous facial expression dataset, i.e. Static Facial Expressions in the Wild (SFEW) <ref type="bibr" target="#b5">[6]</ref>, which contains face images with large head pose variations and different illuminations and is widely used for benchmarking facial expression recognition in the wild. Experimental results on these four databases have shown that the proposed IL-CNN outperforms the baseline CNN models using the traditional softmax loss or the center loss, thanks to the increased inter-class distances and further reduced intraclass variations compared to that using the center loss. It also achieves comparable or better performance compared to the state-of-the-art facial expression recognition methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Facial activity analysis has been widely studied as detailed in the recent surveys <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b24">[25]</ref>. One of the major steps is to extract the most discriminative features that characterize facial appearance and geometry changes caused by facial behavior. These features can be roughly divided into two categories: human designed and learned features.</p><p>Gabor wavelets <ref type="bibr" target="#b3">[4]</ref>, Scale Invariant Feature Transform (SIFT) features <ref type="bibr" target="#b48">[49]</ref>, histogram of Oriented Gradients (HOG) <ref type="bibr" target="#b1">[2]</ref>, histograms of Local Binary Patterns (LBP) <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b14">[15]</ref>, histograms of Local Phase Quantization (LPQ) <ref type="bibr" target="#b10">[11]</ref>, histograms of Local Gabor Binary Patterns (LGBP) <ref type="bibr" target="#b28">[29]</ref> have been demonstrated to be the most successful human designed features. In addition to the spatiotemporal extensions of the aforementioned 2D features <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b49">[50]</ref>, features are deliberately designed to utilize both spatial and temporal information in an image sequence, such as temporal modeling of shapes (TMS) <ref type="bibr" target="#b9">[10]</ref>, interval temporal Bayesian network <ref type="bibr" target="#b42">[43]</ref>, expressionlets on spatiotemporal manifold (STM-ExpLet) <ref type="bibr" target="#b20">[21]</ref>, selective transfer machine (STM) <ref type="bibr" target="#b4">[5]</ref>, Gabor phase shifts (F-Bases) <ref type="bibr" target="#b35">[36]</ref>, Latent Ordinal Model (LOMo) <ref type="bibr" target="#b37">[38]</ref>, spatiotemporal covariance descriptors (Cov3D) <ref type="bibr" target="#b33">[34]</ref>.</p><p>Benefiting from the advance in feature learning, features can be learned either unsupervised by sparse coding <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b26">[27]</ref> or supervised by deep learning <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b2">[3]</ref>. Among them, the deep CNNs have achieved promising recognition performance under real-world conditions as demonstrated in the recent EmotiW2015 <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b53">[54]</ref> and EmotiW2016 challenge <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b2">[3]</ref>.</p><p>Most of the aforementioned CNN-based approaches adopted the softmax loss as the supervision signal to train the CNN models. In real-world scenarios, facial expression recognition suffers from high intra-class variations and interclass similarities. On one hand, CNNs may generate similar representations for images containing the different expressions, especially for the same person. On the other hand, the deeply learned features may be different for images containing the same expression resulted from the aforementioned challenges. Recently, an Identity-Aware CNN (IACNN) <ref type="bibr" target="#b25">[26]</ref> was developed to alleviate variations introduced by personal attributes using an expression-sensitive contrastive loss and an identity-sensitive contrastive loss. However, the variations caused by other factors such as head pose and illumination were not considered. In addition, the contrastive loss suffers from drastic data expansion when constructing image pairs from the training set. Wen et al. <ref type="bibr" target="#b43">[44]</ref> introduced a center loss for face recognition, which targets directly on one of the learning objectives, i.e., the intra-class compactness. As a result, the learned features from the same subject will be more similar. Rather than minimizing the distance to the cluster center as the center loss, Li et al. <ref type="bibr" target="#b17">[18]</ref> proposed a Deep Locality-Preserving CNN (DLP-CNN), which preserves the locality proximity by minimizing the distance to the K-nearest neighbors within the same class. While the center loss pulls the samples towards their corresponding cluster centers and DLP-CNN pushes the samples to their Knearest neighbors, the proposed island loss further enhances the discriminative power of the deep features by simultaneously reducing intra-class variations and augmenting interclass differences as demonstrated in the experiments on four expression databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>In this section, we will first give a brief review of the center loss, and then introduce the proposed island loss followed by the forward and backward propagation processes of the IL-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. A Brief Review of Center Loss</head><p>As illustrated in Fig. <ref type="figure">1</ref>(b), the center loss <ref type="bibr" target="#b43">[44]</ref> explicitly reduces the intra-class variations by pushing samples towards their corresponding class centers in the feature space during training. The centers will be updated in each iteration using Stochastic Gradient Descent (SGD) as part of the CNN training.</p><p>1) Forward propagation: The center loss denoted as L C is defined in Eq. 1 <ref type="bibr" target="#b43">[44]</ref> as the summation of squared distances between samples and their corresponding centers in the feature space:</p><formula xml:id="formula_0">L C = 1 2 m i=1 ||x i − c yi || 2<label>(1)</label></formula><p>where y i is the class label of the i th sample; x i denotes the feature vector of the i th sample taken from the fullyconnected layer before the decision layer; c yi ∈ R d denotes the center of all samples with the same class label as y i ; and m is the number of samples in the mini-batch. By minimizing the center loss, the samples of the same class will be pulled towards their corresponding centers and thus, the overall intra-class variations can be reduced.</p><p>During forward propagation, a joint loss is calculated as the weighted sum of the softmax loss and the center loss, which is used in the backward propagation to drive the finetuning process:</p><formula xml:id="formula_1">L = L S + λL C<label>(2)</label></formula><p>where L S is the softmax loss; and a scalar λ is used for balancing the softmax loss and the center loss.</p><p>2) Backward propagation: During backward propagation, the partial derivative of the center loss L C with respect to the input sample x i can be calculated as</p><formula xml:id="formula_2">∂L c ∂x i = x i − c yi<label>(3)</label></formula><p>In addition, the centers will be updated in the iterative optimization of the CNN using SGD as</p><formula xml:id="formula_3">∆c j = m i=1 δ(y i , j)(c j − x i ) 1 + m i=1 δ(y i , j)<label>(4)</label></formula><p>where δ(y i , j) is defined as</p><formula xml:id="formula_4">δ(y i , j) = 1, y i = j 0, y i = j<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. An Island Loss for Facial Expression Recognition</head><p>As shown in Fig. <ref type="figure">1</ref>(b), minimizing the center loss tends to reduce the intra-class variations of the deep features, while the clusters from different classes may be overlapped with each other. To cope with this problem, an island loss is proposed to reduce the intra-class variations and increase the inter-class differences simultaneously.</p><p>1) Forward propagation: The island loss denoted as L IL is defined as the summation of the center loss and the pairwise distances between class centers in the feature space:</p><formula xml:id="formula_5">L IL = L C + λ 1 cj ∈N ck∈N ck =cj c k • c j c k 2 c j 2 + 1 (6)</formula><p>where N is the set of expression labels; c k and c j denote the k th and j th center with L 2 norm c k 2 and c j 2 , respectively; (•) represents the dot product. Specifically, the first term penalizes the distance between the sample and its corresponding center and the second term penalizes the similarity between expressions. λ 1 is used for balancing the two terms. By minimizing the island loss, the samples of the same expression will get closer to each other and those of different expressions will be pushed apart.</p><p>The overall loss function of CNN training is given by Eq. 7:</p><formula xml:id="formula_6">L = L S + λL IL (<label>7</label></formula><formula xml:id="formula_7">)</formula><p>where a hyper parameter λ is used to balance the two losses.</p><p>2) Backward propagation: The partial derivative of the island loss L IL with respect to the input sample x i can be calculated as</p><formula xml:id="formula_8">∂L IL ∂x i = x i − c yi<label>(8)</label></formula><p>which is actually the same as the one only using the center loss as in Eq. 3. ∂LIL ∂xi will be further backpropagated to the lower fully-connected layer and the convolutional layers to drive the fine-tuning process of CNNs.</p><p>Updating the cluster center: Based on SGD, the update of the j th class center can be calculated as</p><formula xml:id="formula_9">∆c j = m i=1 δ(y i , j)(c j − x i ) 1 + m i=1 δ(y i , j) + λ 1 |N | − 1 ck∈N ck =cj c k c k 2 c j 2 − c k • c j c k 2 c j 3 2 c j (9)</formula><p>where |N | is the total number of expressions.</p><p>In this manner, the class centers can be updated iteratively in each mini-batch with a learning rate α<ref type="foot" target="#foot_0">1</ref> :</p><formula xml:id="formula_10">c t+1 j = c t j − α∆c t j (<label>10</label></formula><formula xml:id="formula_11">)</formula><p>The forward and backward learning process in the IL-CNN is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>A series of experiments have been conducted on four benchmark expression databases including three posed facial expression databases, i.e. the CK+ database <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b23">[24]</ref>, the MMI database <ref type="bibr" target="#b30">[31]</ref>, and the Oulu-CASIA database <ref type="bibr" target="#b49">[50]</ref>, and more importantly, on a spontaneous facial expression Algorithm 1 Forward-backward learning algorithm of IL-CNN Input: Training data {x i }.</p><p>1: Given: mini-batch size m, number of iterations T , learning rates µ and α, and hyper-parameters λ and λ 1 . 2: Initialize: t = 1, network layer parameters W, softmax loss parameters θ, and island loss parameters c j . 3: for t = 1 to T do 4:</p><p>Calculate the joint loss as in Eq. 7:</p><p>5:</p><formula xml:id="formula_12">L = L S + λL IL 6:</formula><p>Update the softmax loss parameters:</p><formula xml:id="formula_13">7: θ t+1 = θ t − µ ∂L t S ∂θ t 8:</formula><p>Update the island loss parameters (i.e. centers) as in Eq. 10:</p><p>9:</p><formula xml:id="formula_14">c t+1 j = c t j − α∆c t j 10:</formula><p>Update the backpropagation error:</p><p>11:</p><formula xml:id="formula_15">∂L t ∂x t i = ∂L t S ∂x t i + λ ∂L t IL ∂x t i 12:</formula><p>Update the network layer parameters:</p><p>13:</p><formula xml:id="formula_16">W t+1 = W t − µ ∂L t ∂W t = W t − µ ∂L t ∂x t i ∂x t i ∂W t</formula><p>14: end for Output: Network layer parameters W, island loss parameters c j , and softmax loss parameters θ.</p><p>database, i.e., the SFEW dataset <ref type="bibr" target="#b5">[6]</ref> to evaluate the proposed IL-CNN for facial expression recognition. Furthermore, to demonstrate the effectiveness of the proposed island loss, the IL-CNN is compared with two baseline CNNs, which have the same network structure as the IL-CNN, but are under the supervision of (1) softmax loss and (2) softmax loss + center loss, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Preprocessing</head><p>To reduce the variations in face scale and in-plane rotation, face alignment is employed on each image based on the facial landmarks extracted by Discriminative Response Map Fitting (DRMF) <ref type="bibr" target="#b0">[1]</ref>. Specifically, given the 66 extracted facial landmarks, face regions are aligned based on three key points: centers of two eyes and mouth. The aligned facial images are then resized to 60 × 60. In addition, histogram equalization is utilized to improve the contrast in facial images. Because of the limited number of images in the facial expression databases, a data augmentation strategy is adopted to produce more data for training. Specifically, 48 × 48 patches are randomly cropped from the 60 × 60 images, and then rotated by a random degree between -10 • and 10 • . The rotated images are randomly horizontally flipped as the input of all CNNs, resulting in a new dataset 5,760 times larger than the original one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Datasets</head><p>1) CK+ Dataset: The CK+ database <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b23">[24]</ref> consists of 327 videos collected from 118 subjects, each of which is associated with one of 7 expression labels, i.e. anger, contempt, disgust, fear, happiness, sadness, and surprise. Each video starts with a neutral face, and reaches the peak in the last frame. To collect more data, the last three frames of each sequence are collected associated with the provided expression label. Thus, an experimental dataset consisting of 981 images is built.</p><p>2) MMI dataset: The MMI database <ref type="bibr" target="#b30">[31]</ref> contains 213 image sequences, from which 208 sequences with frontalview faces of 31 subjects are used in our experiment. Each sequence is labeled as one of six basic expressions, i.e. anger, disgust, fear, happiness, sadness, and surprise, starting from a neutral expression, through a peak phase in the middle, and back to a neutral face at the end. Since the actual location of the peak frame is not provided, three frames in the middle of each image sequence are collected as peak frames associated with the provided label. Hence, there are a total of 624 images used in our experiments.</p><p>3) Oulu-CASIA dataset: The Oulu-CASIA database [50] contains 2,880 videos, each of which contains one of the six basic expressions (happiness, sadness, surprise, anger, fear, and disgust), collected from 80 subjects. Each of the videos is captured with one of two imaging systems, i.e. near infrared (NIR) and visible light (VIS), under one of three different illumination conditions: normal indoor illumination, weak illumination or dim illumination. Following the previous work evaluated on the Oulu-CASIA database <ref type="bibr" target="#b12">[13]</ref>, only the 480 videos collected by the VIS System under normal indoor illumination are employed in our experiments. For each video, the last three frames are collected as the peak frames of the labeled expression. Thus, the Oulu-CASIA dataset contains 1,440 images for our experiments.</p><p>Training/testing strategy: The baseline CNNs and the proposed IL-CNN are trained and tested on static images. A 10-fold cross-validation strategy is employed for CK+, MMI, and Oulu-CASIA datasets, where each dataset is further split into 10 subsets, and the subjects in any two subsets are mutually exclusive. For each run, data from 8 sets are used for training, the remaining two subsets are used for validation and testing, respectively. The final sequence-level decision is made by choosing the label of the class with the highest average score over the three images from the same sequence. The results are reported as the average of the 10 runs.</p><p>4) SFEW dataset: The SFEW database <ref type="bibr" target="#b5">[6]</ref> is the most widely used benchmark database for facial expression recognition in the wild. It is composed of 1,766 images, i.e. 958 for training, 436 for validation, and 372 for testing. Each of the images has been assigned to one of seven expression categories, i.e., anger, disgust, fear, neutral, happy, sad, and surprise. The expression labels of the training and validation sets are provided, while those of the testing set is held back by the challenge organizer. Thus, the performance on the testing set is evaluated and provided by the challenge organizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. CNN Implementation Details</head><p>For experiments on each benchmark dataset, a CNN with the softmax loss is pre-trained using the Facial Expression Recognition (FER-2013) dataset <ref type="bibr" target="#b8">[9]</ref> and the other three datasets. Starting from the same pre-trained CNN, the IL-CNN and the two baseline CNNs are fine-tuned on each of the four benchmark expression datasets, respectively. Stochastic gradient descent with a momentum of 0.9, a minibatch size of 300, and a weight decay parameter of 0.05, is used for training the CNNs. The learning rate µ starts from 0.001 and is reduced by a factor of 0.1 for every 2500 iterations. A dropout rate of 0.6 is employed for the last two fully-connected layers, i.e. zeroing out the output of a neuron with a probability of 0.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experimental Results</head><p>The confusion matrices of the proposed IL-CNN are reported for the four datasets, where entries along the diagonal represent the recognition accuracy for each expression. In addition to the two baseline CNNs, the proposed IL-CNN is compared with the state-of-the-art methods evaluated on the four databases such as methods using human crafted features (HOG 3D <ref type="bibr" target="#b16">[17]</ref>, TMS <ref type="bibr" target="#b9">[10]</ref>, Cov3D <ref type="bibr" target="#b33">[34]</ref>, STM <ref type="bibr" target="#b4">[5]</ref>, STM-ExpLet <ref type="bibr" target="#b20">[21]</ref>, LOMo <ref type="bibr" target="#b37">[38]</ref>, ITBN <ref type="bibr" target="#b42">[43]</ref> and F-Bases <ref type="bibr" target="#b35">[36]</ref>), a method using sparse coding (MSR <ref type="bibr" target="#b31">[32]</ref>), and CNN-based methods (3DCNN and 3DCNN-DAP <ref type="bibr" target="#b19">[20]</ref>, Inception <ref type="bibr" target="#b27">[28]</ref>, IACNN <ref type="bibr" target="#b25">[26]</ref>, DLP-CNN <ref type="bibr" target="#b17">[18]</ref>, FN2EN <ref type="bibr" target="#b6">[7]</ref>, PPDN <ref type="bibr" target="#b51">[52]</ref> and DTAGN <ref type="bibr" target="#b12">[13]</ref>).</p><p>1) Results on CK+ dataset: The confusion matrix of the proposed IL-CNN is reported in Table <ref type="table" target="#tab_0">I</ref> and the comparison results in terms of average recognition accuracy are reported in Table <ref type="table" target="#tab_1">II</ref>. The IL-CNN achieves an average recognition accuracy of 94.35% on the CK+ dataset for the 7 expressions.</p><p>2) Results on MMI dataset: The confusion matrix of the proposed IL-CNN is given by Table <ref type="table" target="#tab_1">III</ref> and the comparison results are presented in Table <ref type="table" target="#tab_0">IV</ref> for the MMI dataset.</p><p>3) Results on Oulu-CASIA dataset: Table <ref type="table">V</ref> gives the confusion matrix of the IL-CNN and Table <ref type="table" target="#tab_0">VI</ref> summarizes the comparison results on the Oulu-CASIA dataset. Result analysis on the three posed facial expression datasets: As shown in Table <ref type="table" target="#tab_1">II</ref>, Table <ref type="table" target="#tab_0">IV</ref>, and Table VI, the IL-CNN consistently outperforms the two baseline CNNs using either softmax loss or center loss by reducing the intraclass variations and inter-class similarities. Furthermore, the IL-CNN also achieves better or at least comparable performance compared to the state-of-the-art methods. Note that, while most of the state-of-the-art methods utilized dynamic features extracted from image sequences, the proposed IL-CNN is trained on static images, which is more favorable  <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b23">[24]</ref> IN TERMS OF THE AVERAGE RECOGNITION ACCURACY OF 7 EXPRESSIONS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Acc. Classes Feature Strategy 3DCNN <ref type="bibr" target="#b19">[20]</ref> 85.9 7 Dynamic 15 folds ITBN <ref type="bibr" target="#b42">[43]</ref> 86.3 7 Dynamic 15 folds F-Bases <ref type="bibr" target="#b35">[36]</ref> 89.01 7 Dynamic LOSO MSR <ref type="bibr" target="#b31">[32]</ref> 91.4 7 Static LOSO HOG 3D <ref type="bibr" target="#b16">[17]</ref> 91.44 7 Dynamic 10 folds TMS <ref type="bibr" target="#b9">[10]</ref> 91.89 6 Dynamic 4 folds Cov3D <ref type="bibr" target="#b33">[34]</ref> 92.3 7 Dynamic 5 folds 3DCNN-DAP <ref type="bibr" target="#b19">[20]</ref> 92.4 7 Dynamic 15 folds Inception <ref type="bibr" target="#b27">[28]</ref> 93.2 6 Static 5 folds STM-ExpLet <ref type="bibr" target="#b20">[21]</ref> 94.19 7 Dynamic 10 folds F-Bases <ref type="bibr" target="#b35">[36]</ref> 94.81 7 Static LOSO LOMo <ref type="bibr" target="#b37">[38]</ref> 95.1 7 Dynamic 10 folds IACNN <ref type="bibr" target="#b25">[26]</ref> 95.37 7 Static 8 folds DLP-CNN <ref type="bibr" target="#b17">[18]</ref> 95.78 6 Static 5 folds STM <ref type="bibr" target="#b4">[5]</ref> 96.40 7 Dynamic N/A FN2EN <ref type="bibr" target="#b6">[7]</ref> 96.80 8 Static 10 folds DTAGN <ref type="bibr" target="#b12">[13]</ref> 97.25 7 Dynamic 10 folds PPDN <ref type="bibr" target="#b51">[52]</ref> 97.   <ref type="bibr" target="#b42">[43]</ref> 59.7 6 Dynamic 20 folds HOG 3D <ref type="bibr" target="#b16">[17]</ref> 60.89 6 Dynamic 10 folds 3DCNN-DAP <ref type="bibr" target="#b19">[20]</ref> 63.4 6 Dynamic 20 folds DTAGN <ref type="bibr" target="#b12">[13]</ref> 70.24 6 Dynamic 10 folds IACNN <ref type="bibr" target="#b25">[26]</ref> 71.55 6 Static 10 folds STM-ExpLet <ref type="bibr" target="#b20">[21]</ref> 75.12 6 Dynamic 10 folds F-Bases <ref type="bibr" target="#b35">[36]</ref> 57.56 6 Static LOSO F-Bases <ref type="bibr" target="#b35">[36]</ref> 73.66 6 Dynamic LOSO Inception <ref type="bibr" target="#b27">[28]</ref> 77.   for online applications or snapshots where per frame labels are preferred. We are aware that the Inception model <ref type="bibr" target="#b27">[28]</ref> has the best performance on the MMI database owning to a much more complex and deeper network structure. The proposed island loss can be adopted by these advanced network structures by replacing the softmax loss. 4) Results on SFEW dataset: The confusion matrices of IL-CNN are given by Table <ref type="table" target="#tab_8">VII</ref>    set. Furthermore, the IL-CNN, which uses a single CNN with a shallow architecture, is ranked at the third place for the testing set among all the methods compared. Note that, Kim et al. <ref type="bibr" target="#b15">[16]</ref> and Yu et al. <ref type="bibr" target="#b47">[48]</ref>, who are ranked the 1 st and 2 nd , utilized an ensemble of CNNs. In addition, Yu et al. <ref type="bibr" target="#b47">[48]</ref> also employed a combination of different network structures. Thus, an ensemble of IL-CNNs has been constructed and achieves comparable performance as the best methods <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b47">[48]</ref> on the SFEW dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Visualization Study</head><p>A visualization study is performed to demonstrate that the island loss is effective in reducing intra-class variations while increasing the inter-class differences. Specifically, the features learned by the two baseline CNNs and the IL-CNN are visualized using t-SNE <ref type="bibr" target="#b41">[42]</ref>, which is widely employed to visualize high dimensional data. As illustrated in Fig. <ref type="figure">3</ref>, the learned features are clustered according to 7 expressions, where the training, validation, and testing samples are denoted by dots, stars and diamonds, respectively.</p><p>By employing the center loss, samples of the same expression are closer to each other in Fig. <ref type="figure">3b</ref> compared with those learned with only the softmax loss (Fig. <ref type="figure">3a</ref>). As the interclass similarity is not handled in the center loss, overlap can still be observed between clusters as shown in Fig. <ref type="figure">3b</ref>. In contrast, the proposed island loss deals with the intra-class variations and the inter-class similarities simultaneously. Thus, as depicted in Fig. <ref type="figure">3c</ref>, the features learned using the island loss form more compact clusters, which are better separated in the feature space, compared with those learned using the softmax loss and the center loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. A Study of the Distances</head><p>To further demonstrate the discriminative power of the proposed island loss, the cosine distances between centers are analyzed for the four databases. For the model using the softmax loss, the cosine distance between the means of each pair of expressions is calculated; while for the model using the center loss or the proposed island loss, the distance between each pair of the learned centers is computed. The final cosine distance is obtained by averaging the distances between all pairs. As shown in Table <ref type="table" target="#tab_11">X</ref>, the island loss consistently achieves the largest between-centerdistance indicating the enlarged inter-class differences on the four databases.</p><p>Furthermore, the cosine distance between each sample and its corresponding center is calculated. The final sample-to- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this work, a novel island loss is proposed for CNNs to enhance the discriminative power of learned deep features. Specifically, the proposed island loss pulls the samples towards their corresponding class centers to achieve intraclass compactness and at the same time, pushes the centers away from each other to make the clusters as isolated "islands". Experimental results on three posed facial expression datasets and, more importantly, a spontaneous facial expression dataset have demonstrated that the proposed IL-CNN outperforms the baseline CNNs with the traditional softmax loss or the center loss and achieves better or at least comparable performance compared with the state-of-the-art methods for facial expression recognition. As shown in the experiments, the proposed island loss increases the inter-class differences consistently on the four databases as indicated by the increased cosine distance between the class centers. Meanwhile, the intra-class variations are further reduced as compared to the one using the center loss.</p><p>Note that the proposed island loss is a general loss function for CNNs and is ready to be adopted by other advanced network structures for various computer vision and machine learning problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(b), the samples are pulled towards * indicates equal contribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig. 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The proposed IL-CNN for facial expression recognition. An island loss calculated at the second fully-connected layer and the softmax loss calculated at the decision layer are responsible to fine-tuning the CNN parameters. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I CONFUSION</head><label>I</label><figDesc>MATRIX OF THE PROPOSED IL-CNN EVALUATED ON THE CK+ DATASET<ref type="bibr" target="#b13">[14]</ref>,<ref type="bibr" target="#b23">[24]</ref>. THE GROUND TRUTH AND THE PREDICTED LABELS ARE GIVEN BY THE FIRST COLUMN AND THE FIRST ROW, RESPECTIVELY.</figDesc><table><row><cell></cell><cell>An</cell><cell>Co</cell><cell>Di</cell><cell>Fe</cell><cell>Ha</cell><cell>Sa</cell><cell>Su</cell></row><row><cell>An Co Di Fe Ha Sa Su</cell><cell cols="7">95.6% 2.2% 0% 11.1% 74.1% 0% 3.7% 3.7% 7.4% 0% 0% 0% 2.2% 0% 0% 0% 100% 0% 0% 0% 0% 0% 4% 0% 84.0% 12.0% 0% 0% 0% 0% 0% 0% 100% 0% 0% 14.3% 0% 3.6% 0% 0% 82.1% 0% 0% 1.2% 0% 0% 0% 0% 98.8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II PERFORMANCE</head><label>II</label><figDesc>COMPARISON ON THE CK+ DATASET</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Fig. 3. A visualization study of the deep features learned by CNNs using (a) softmax loss, (b) softmax + center loss, and (c) softmax + island loss on the CK+ database. There are a total of 981 samples including 263 × 3 training data from 8 subsets, 32 × 3 validation data, and 32 × 3 testing data. The dots, stars, and diamonds represent training, validation, and testing data, respectively. Note that the features learned by the IL-CNN are well separated according to expressions. Best viewed in color.</figDesc><table><row><cell>Angry Contempt</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Disgust</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fear</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Happy</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sad</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Surprise</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(a) Softmax Loss</cell><cell>(b) Center Loss</cell><cell></cell><cell cols="2">(c) Island Loss</cell></row><row><cell></cell><cell>softmax loss center loss IL-CNN</cell><cell>6 66.35 69.23 70.67</cell><cell>6 6 6 6</cell><cell>Static Static Static Static</cell><cell>5 folds 10 folds 10 folds 10 folds</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>and Table VIII for the validation and testing sets of SFEW, respectively. As illustrated in Table IX, the proposed IL-CNN outperforms the baseline CNNs for both validation set and the testing</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII CONFUSION</head><label>VII</label><figDesc>MATRIX OF THE PROPOSED IL-CNN EVALUATED ON THE SFEW [6] VALIDATION SET. THE GROUND TRUTH AND THE PREDICTED LABELS ARE GIVEN BY THE FIRST COLUMN AND THE FIRST ROW, RESPECTIVELY.</figDesc><table><row><cell></cell><cell>An</cell><cell>Di</cell><cell>Fe</cell><cell>Ha</cell><cell>Ne</cell><cell>Sa</cell><cell>Su</cell></row><row><cell>An Di Fe Ha Ne Sa Su</cell><cell cols="7">61.0% 0% 1.3% 1.3% 22.1% 11.7% 2.6% 4.4% 0% 0% 13.0% 30.4% 43.5% 8.7% 6.4% 2.1% 6.4% 6.4% 55.3% 8.5% 14.9% 0% 0% 1.4% 89.0% 5.5% 4.1% 0% 8.1% 0% 1.2% 1.2% 66.2% 22.1% 1.2% 4.1% 0% 0% 0% 43.8% 48.0% 4.1% 14.0% 0% 1.8% 5.3% 38.6% 7.0% 33.3%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE X AVERAGE</head><label>X</label><figDesc>BETWEEN-CENTER-DISTANCE ON FOUR DATASETS. THE BIGGER VALUE MEANS THE FURTHER DISTANCE.center distance is obtained by averaging the distances of all samples. As shown in Table XI, the proposed island loss consistently produces the smallest sample-to-center distance on the four datasets, which demonstrate the effectiveness of the proposed island loss in reducing the intra-class variation.</figDesc><table><row><cell>Dataset</cell><cell>CK+</cell><cell>MMI</cell><cell cols="2">CASIA SFEW Val</cell></row><row><cell>Softmax loss Center loss Island loss</cell><cell>0.3418 0.3665 0.4396</cell><cell>0.2934 0.3165 0.5250</cell><cell>0.3537 0.3668 0.4562</cell><cell>0.3012 0.3274 0.3778</cell></row><row><cell></cell><cell></cell><cell>TABLE XI</cell><cell></cell><cell></cell></row><row><cell cols="5">AVERAGE SAMPLE-TO-CENTER DISTANCE ON FOUR DATASETS.</cell></row><row><cell cols="5">THE SMALLER VALUE MEANS THE CLOSER DISTANCE.</cell></row><row><cell>Dataset</cell><cell>CK+</cell><cell>MMI</cell><cell cols="2">CASIA SFEW Val</cell></row><row><cell>Softmax loss Center loss Island loss</cell><cell>0.0301 0.0233 0.0194</cell><cell>0.0804 0.0634 0.0618</cell><cell>0.0536 0.0395 0.0307</cell><cell>0.0797 0.0694 0.0689</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">In our experiments, we set α = 1, λ = 0.01, λ 1 = 10 empirically.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust discriminative response map fitting with constrained local models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3444" to="3451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cross-dataset learning and person-specific normalisation for automatic action unit detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Emotion recognition in the wild from videos using images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barsoum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C F</forename></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="433" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recognizing facial expression: Machine learning and application to spontaneous behavior</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Littlewort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lainscsek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="568" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Selective transfer machine for personalized facial expression analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video and image based emotion recognition challenges in the wild: Emotiw</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="423" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Facenet2expnet: Regularizing a deep face recognition net for expression recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Video-based emotion recognition using CNN-RNN and C3D hybrid networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="445" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Challenges in representation learning: A report on three machine learning contests</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Carrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hamner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cukierski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="117" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Facial expression recognition with temporal modeling of shapes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1642" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Decision level fusion of domain specific regions for facial action recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1776" to="1781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Action unit detection using sparse appearance descriptors in space-time video volumes</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<editor>FG</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint fine-tuning in deep neural networks for facial expression recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2983" to="2991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Comprehensive database for facial expression analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
				<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="46" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Contrasting and combining least squares based learners for emotion recognition in the wild</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gürpinar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Afshar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Salah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="459" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hierarchical committee of deep cnns with exponentially-weighted decision fusion for static facial expression recognition</title>
		<author>
			<persName><forename type="first">B.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A spatio-temporal descriptor based on 3d-gradients</title>
		<author>
			<persName><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="275" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reliable crowdsourcing and deep localitypreserving learning for expression recognition in the wild</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">AU-aware deep networks for facial expression recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deeply learning deformable facial action parts model for dynamic expression analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning expressionlets on spatio-temporal manifold for dynamic facial expression recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1749" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Facial expression recognition via a boosted deep belief network</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1805" to="1812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving facial expression analysis using histograms of log-transformed nonnegative sparse representation with a spatial pyramid structure</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The extended cohn-kanade dataset (ck+): A complete expression dataset for action unit and emotion-specified expression</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic analysis of facial actions: A survey</title>
		<author>
			<persName><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Identity-aware convolutional neural network for facial expression recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="558" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Simultaneous recognition of facial expression and identity via sparse representation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fatemizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1066" to="1073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Going deeper in facial expression recognition using deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In WACV</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Local binary patterns for multi-view facial expression recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="541" to="558" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning for emotion recognition on small datasets using transfer learning</title>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vonikakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="443" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Web-based database for facial expression analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rademaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Manifold based sparse representation for robust expression recognition without neutral subtraction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ptucha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tsagkatakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Savakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2136" to="2143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Disentangling factors of variation for facial expression recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="808" to="822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spatiotemporal covariance descriptors for action and gesture recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="103" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Automatic analysis of facial affect: A survey of registration, representation and recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sariyanidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>IEEE</publisher>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1113" to="1133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning bases of activity for facial expression recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sariyanidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T-IP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1965" to="1978" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A 3-dimensional sift descriptor and its application to action recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="357" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Lomo: Latent ordinal model for facial analysis in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5580" to="5589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Combining multimodal features within a fusion network for emotion recognition in the wild</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="497" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep learning using linear support vector machines</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Metaanalysis of the first facial expression recognition challenge</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mehu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T-SMC-B</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="966" to="979" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Capturing complex spatio-temporal relations among facial muscles for facial expression recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3422" to="3429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Boosting encoded dynamic features for facial expression recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="132" to="139" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Holonet: towards robust emotion recognition in the wild</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="472" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Capturing AU-aware facial features and their latent relations for emotion recognition in the wild</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="451" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Image based static facial expression recognition with multiple deep network learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="435" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Discriminant multi-label manifold embedding for facial action unit detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yuce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thiran</surname></persName>
		</author>
		<editor>FG</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Facial expression recognition from near-infrared videos</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. IVC</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="607" to="619" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Dynamic texture recognition using local binary patterns with an application to facial expressions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietiäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T-PAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="915" to="928" />
			<date type="published" when="2007-06">June 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Peak-piloted deep network for facial expression recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="425" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning multiscale active facial patches for expression analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1499" to="1510" />
			<date type="published" when="2015-08">August 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Transductive transfer lda with riesz-based volume lbp for emotion recognition in the wild</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="491" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Emotion recognition in the wild via sparse transductive transfer linear discriminant analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. on Multimodal User Interfaces</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="163" to="172" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
