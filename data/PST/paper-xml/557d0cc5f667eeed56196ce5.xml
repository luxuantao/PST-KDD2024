<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BD47B46C315EDFD4222F8DB7C19A2518</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>positivity, and thus will be submanifolds of codimension one. Applying the proposition leads to the conclusion that vI and v2 divide one another, or equivalently, are equal. Consequently, p and q are equal.</p><p>In fact, all that is necessary for uniqueness is that the scale-space functions vl and v2 should be irreducible. To extend the results, it would be interesting to characterize all factorable polynomials that satisfy the Heat Equation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reconstructions from Zero Crossings in Scale Space</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROBERT HUMMEL, MEMBER, IEEE, AND ROBERT MONIOT</head><p>Abstract-A useful representation of signal data, besides being a complete and stable transformation of the information, should make explicit important features in the data. In computer vision, the oneparameter family of images obtained from the Laplacian-of-a-Gaussian-filtered version of the image, parameterized by the width of the Gaussian, has proven to be a useful data structure for the extraction of feature data. In particular, the zero crossings of these so-called scalespace data are associated with edges, and were proposed by Marr and others as the basis of a representation of the image data. The question arises as to whether the representation is complete and stable. We survey some of the results and studies related to these questions, and survey several papers that attempt reconstructions based on this or related representations. We then formulate a new method for the reconstruction from zero crossings in scale space, based on minimizing equation error, and present results showing that the reconstruction is possible, but can be unstable. We further show that the method applies when gradient data along the zero crossings are included in the representation, and demonstrate empirically that the reconstruction is then stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. NOTIONS A. Representations</head><p>N the fields of signal analysis and image processing, I the first stage of a complete pattern recognition system typically applies some numerical process to the digital data. In image processing, for example, it is generally considered useful to extract edges, comers, and textured regions in the image. In other words, the features and salient symbolic information about signal and image data are dynamically associated with groups or regions of the data, and typically depend more explicitly on primitive features such as discontinuities, extrema, and local statistical properties of the spatially sampled data. It seems reasonable, therefore, to transform the initial data to intermediate representations to make the primitive features more accessible to the algorithms for signal analysis. The collection of all intermediate representations, which might include binary edge images, texture measures, and other feature detectors , comprise a representation that replaces the original signal for analysis purposes. This is the central idea in vision processing, for example, of Marr's "Primal Sketch" [l] or Tenenbaum and Barrow's "Intrinsic Images" <ref type="bibr" target="#b1">[2]</ref>. Since all analysis is done on the intermediate represen- tation, the representation should carry all the information necessary for the interpretation of the data. Of course, the main idea is that the relevant information should be more explicit than the original samples, and that data redundancies should be eliminated. The representation may constitute a data compressed version of the original sampling, but might as easily contain more bulk data, in the attempt to represent different features. It should be possible to reconstruct a version of the original signal from the intermediate representation since the representation should contain all the essential information. While the reconstruction need not be mathematically identical, it should have the same subjective interpretation as the original signal.</p><p>It is important to emphasize that our interest, from the computer vision standpoint, is independent of data compression concerns. An intermediate representation such as the zero crossings together with the gradients along the zero crossings, discussed in detail later in this paper, may contain redundant information and fail to be compressive at all. Instead, our concern is with the quality of an intermediate representation for the purposes of building a computer vision system. Such a system will probably never actually perform a reconstruction. Accordingly, we do not care if the representation is bulky and if reconstruction is expeiisive. We do care that the construction of the representation should be easy, and that stable reconstruction is at least theoretically possible.</p><p>How can we evaluate the quality of an intermediate representation? The usual method is to show that algorithms making use of the representation are effective. While it is difficult to fault success, this approach is limited by a lack of generality. That is, the range of algorithms that can be demonstrated is necessarily limited, and the reasons for their effectiveness and limitations are always varied. For example, we can never be sure if a partly successful vision system is limited because the features are inadequate or because the pattern matching method is inextensible.</p><p>Alternatively, we can study a representation mathematically, to understand 1) the dependence of the representation on the initial data, 2) the fibers of the representation, and 3) the stability of the transformation. For the first issue, the dependence, it is desirable to show some form of continuity, so that small changes in the original data result in small changes in the representation. By "fibers,'' as studied in the second issue, we mean the sets of signals that map to a single representation. If the mapping is one-to-one, then the fibers are singleton sets, and the representation is complete. Finally, stability of the representation concerns continuity of the inverse map. We would like to show that small changes in the representation, or inaccuracies induced by numerical implementations, result in small changes in the fibers mapping to the respective representations.</p><p>Mathematical analysis is complicated by the need to define topologies on the spaces. For example, the notion of a small change in the preimage sets (the fibers) is not a priori defined. The metrics should correspond to an intuitive notion of similarity in the data, as observed by human subjects. Further, analysis of a representation is especially difficult if there are serious nonlinearities present in the transformation. The intent of the results presented in later sections of this work is to contribute to the mathematical analysis of representations involving zero crossings in scale space.</p><p>As a side benefit of a mathematical analysis of a representation, it is often possible to devise a reconstruction (or approximate reconstruction) algorithm. A reconstruction scheme is a right inverse, in the sense that the transformation applied to a reconstruction should yield the same representation to which the reconstruction was applied [see Fig. <ref type="figure">l(a)</ref>]. Thus, the purpose of the reconstruction is to construct an element in the representation's fiber. A reconstruction can be used in a subjective or psychophysical evaluation of a representation. Specifically, a comparison can be made between an original signal and a reconstruction of its representation. Two outcomes are possible. If one discovers an instance when the reconstruction and the original are visually distinguishable, then the representation does not encode the perceptual content of the scene, and fails to encode the differences that are observed. In particular, any recognition system based on the representation will respond identically to the different scenes. Alternatively, we may discover that the reconstructions are generally indistinguishable from the original, at least in terms of the content for recognizability of the objects or for the intended tasks. Note that it is not necessarily expected that the reconstructions are numerically identical. Indeed, if the reconstructions are numerically different but perceptually the same, then this behavior forms strong support for the utility of the! representation for building a vision system that mimics some portion of human visual capabilities. That is, patterns of indistinguishability are evidence, albeit inconclusive, that the representation carries information essential to the interpretation of the associated signals.</p><p>Ideally, one would like to discover and computationally simulate the same representation as is used by the human system. Unfortunately, despite extensive study by neuroscientists of the visual pathway, complete computational models are still elusive. Suppose we were to find a representation that behaves well mathematically, and yields patterns of indistinguishability similar to those of the human visual system. This would then constitute evidence, but not conclusive evidence, that the representation is used by the human visual system. Given the variety of possible features that could be used in a representation, we would have to be very lucky to pick precisely the correct representation without considerable physiological evidence. For representations based on zero crossings, to be considered later in this paper, there is some minor evidence that there might be cortical cells that respond to zero crossings [3], and considerable evidence (particularly psychophysical evidence) that zero crossings are not used by the human visual system. Accordingly, our results, which include instances of stable reconstructions, provide interesting information about potential intermediate representations for machine vision systems, but cannot be used to make conclusions about the computations of the human visual system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Scale Space</head><p>Early work in image processing and signal pattern recognition emphasized the development of simple local operators for the detection of features. For example, dozens of edge detectors have been developed for image analysis applications. Many of the operators assumed a certain resolution level, and apply the operations to a neighborhood of samples that is independent of the data. Thus, many edge detectors assume that the edge can be discerned within a three-by-three pixel neighborhood. Such operators frequently fail to find features when the data are sampled too finely, and sometimes fail when the resolution is too coarse. Nonetheless, in controlled cases, feature operators can perform useful preprocessing for the analysis of signal data.</p><p>In order to build into the process a certain degree of scale invariance, and to make the feature detection operate dynamically on the data, the use of multiresolution data structures has become an important aspect of signal processing. One of the earliest uses of multiresolution methods in computer vision was described by Rosenfeld and Thurston <ref type="bibr" target="#b3">[4]</ref>. A simple construction is to sample the image at multiple resolution levels, forming a "pyramid" of images. Each layer represents a different scale for the same image. Algorithms are then developed that incorporate feature-detector responses obtained by applying the same operation to each level of the pyramid. From a single high-resolution image, this pyramid can be obtained numerically by successively blurring and subsampling the original image. Each operation of blurring and subsampling produces an antialiased version of the same image at a coarser resolution. The resulting structure is called a ' 'Gaussian pyramid. " Computational methods for building Gaussian pyramids, and other pyramids, are described by Burt and Adelson [5].</p><p>We will describe a continuous formulation of these multiresolution pyramids, in order to assist in the mathematical analysis, and to formulate scale space precisely.</p><p>We begin with data f ( x ) , where x E RIn. For n = 1, f ( x ) could be an acoustical signal; for n = 2, we usually regardf(x) as an image. The case n = 3 arises with seismographic data, time-varying imagery, or tomographic data. Higher dimensional domains for the data are possible. Scale space refers to a domain ( x , t ) for a set of data, u ( x , t), parameterized by a variable t E R, giving variations on the dataf(x). In particular, we will always assume that t 1 0, and that u ( x , 0) = f ( x ) . Further, u ( x , t ) will be continuous in t, and that for any to &gt; 0, ~( x , t o ) gives a variant off ( x ) . The idea of scale space, parameterized by a continuous variable obtained as the standard deviation of a Gaussian, is due to Witkin <ref type="bibr" target="#b5">[6]</ref>.</p><p>The natural framework for the analysis of scale-space formulations of multiresolution representations is in terms of the heat equation <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr">[8]</ref>. We define ~( x , t) to be a bounded solution to the heat equation:</p><formula xml:id="formula_0">a M at -= A M , M ( X , 0 ) = f ( x ) .</formula><p>(Heat Equation)</p><p>The solution is given by convolution against the fundamental solution to the Heat Equation, which for the do-main R" is given by where</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K ( x , t ) = ( 4 ~t ) -" / ~e -~~' ~/ ~' ,</head><p>We see that u ( x , t ) is obtained by blurring f ( x ) by increasingly diffuse Gaussians, parameterized by t &gt; 0, with standard deviations a satisfying 2a2 = 4t. In computer vision, scale space sometimes refers to the (x, a ) variables that can be used to reparameterize the domain of U. We retain the ( x , t ) parameterization to keep the linear Heat Equation relation for the function U .</p><p>Convolution by Gaussians is considered special for many reasons <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr">[lo]</ref>. We see from the above analysis a relationship between Gaussian convolution, the Heat Equation, and the Laplacian operator. Of course, Gaussian convolution enjoys other properties; for example, the central limit theorem implies that Gaussian convolution is easy to implement by an iterative procedure.</p><p>In computer vision, in an idea that dates back to 1955 [ll], the image data f ( x ) are often filtered with the Laplacian of a Gaussian, instead of filtering by a Gaussian. The use of a Laplacian-of-Gaussian-type filter is in part motivated by the existence of center-surround receptive fields, which exhibit excitatoryhnhibitory centedsurround response patterns [ 121, but can also be motivated on a predictive encoding basis <ref type="bibr">[13]</ref>. Experiences with Laplacian-of-Gaussian convolutions have demonstrated their utility for computer vision applications. Filtering by the Laplacian of a Gaussian can be written in three ways:</p><formula xml:id="formula_1">A K * f = K * Af = A ( K * f ) .</formula><p>If we denote the result by v ( x , t), we see that 1) u ( x , t ) is the f ( x ) data filtered by the Laplacian of a Gaussian, 2) u ( x , t ) is the solution to the Heat Equation with initial data A f , and 3) u ( x , t ) is A u ( x , t), where U is the solution to the Heat Equation with initial data f ( x ) .</p><p>We will refer to u as the scale-space function of the data f . The scale-space function u provides a continuous-domain analog [ 141 of the Laplacian-pyramid data structure A difference of two Gaussians is often used in place of the Laplacian-of-Gaussian. We see from the Heat Equation formulation the basis for the approximation. Namely, since K ( x , t ) is itself a solution to the Heat Equation, PI.</p><formula xml:id="formula_2">= 7 -0 lim ( K ( x , t + 7 ) -K ( x , t ) ) / ~.</formula><p>That is, the difference of Gaussians is a good approximation to A K as the separation between the spread of the two Gaussians approaches zero (and the difference is scaled). In actual use, it is more frequent to use a difference of Gaussians where the ratio of the two scales in the a-variable is given by 1.6, but the rationale for this choice is not the degree of approximation to the Laplacian-of-Gaussian [ 151.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Zero Crossings</head><p>The zero set of ~( x , t ) is the point set in ( x , t ) where U = 0. The set might be empty (for instance, iff is subharmonic or superharmonic <ref type="bibr">[16]</ref>) everything (iff is harmonic <ref type="bibr" target="#b16">[17]</ref>), or a proper subset of ( x , t ) space. In the latter case, zeros can be isolated points, lines, and surfaces (but never regions). We distinguish components of the zero set which form manifolds of codimension one.</p><p>DeBnition: The zero crossings of v ( x , t ) refer to the point set where as refers to the boundary of the set S .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H</head><p>That is, the zero crossings are surfaces in scale space that separate regions where the scale-space function U is positive from regions where U is negative. If we intersect these zero-crossing surfaces with a plane { t = to } , fixing a single level, we obtain the "zero-crossing curves" at that level [see Fig. <ref type="figure">l(b)</ref>]. These curves also separate positive and negative regions, but in this case the regions lie in the plane determined by the fixed level. The zero-crossing curves at the continuum of levels trace, and thus develop, the zero-crossing surfaces, which we refer to simply as the zero crossings. Of course, our terminology of curves," "surfaces," and the fixed "plane" are motivated by the case of a two-dimensional spatial domain (i.e., x E RI2); however, the concepts are valid for any spatial dimension.</p><p>Zero-crossings curves have been used for segmentation of imagery by edge detection <ref type="bibr" target="#b14">[15]</ref>, and for stereo matching and motion correspondence between pairs of images (e.g., <ref type="bibr">[lS]</ref>). It has also been suggested <ref type="bibr">[lo]</ref> that they form a nearly complete representation of A$ Marr also suggested that the zero crossings might be a complete representation [l], but further thought that perhaps the zerocrossing information would need to be supplemented with gradient data along the zero crossings. These suggestions form the basis of the interest in zero crossings as an intermediate representation for images.</p><p>It should be noted that there is a long and distinguished history of the analysis of zero crossings in mathematics. Kedem supplies a survey together with some advances <ref type="bibr">[19]</ref>. In this and related work, analysis is generally restricted to signals of one variable, and the representation is based on the local statistics of zero crossings (the number of crossings per unit length). Issues such as the local density of zero crossings become very important when they are used for stereopsis computation with image pairs. Presumably, this body of mathematical work is quite relevant to image representation, but has not been applied to computer vision in a major way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>"</head><p>Witkin <ref type="bibr" target="#b5">[6]</ref> observes that zero crossings in scale space evolve as t increases, and are never created at some nonzero t. This property, discussed in <ref type="bibr" target="#b8">[9]</ref> and in <ref type="bibr">[ lo]</ref>, ensures that zero-crossing surfaces are nested, one within another, enclosing regions containing the face { t = 0 } , or forming a sheet meeting the face { t = 0} and extending to t = 03. In Fig. <ref type="figure">l(b</ref>), this property is reflected in the fact that the zero-crossing surfaces close at the top-there are no surfaces that hang from the top, closing below. The property can be given a precise statement.</p><p>Evolution Property of Zero Crossings: Let C be a connected component of the set of zero crossings in the domain { ( x , t ) ( x E RIn, TI I t I T 2 } , where 0 I TI &lt; H When first observed, the evolution property sparked a great deal of interest in scale-space methods. It was thought that the property gave great justification to the use of Laplacian-of-Gaussian filtering, and indeed, that the Gaussian was the only filter that would yield this property <ref type="bibr" target="#b8">[9]</ref>, [ 101. Under appropriate restrictions, it is true that the Gaussian is special in terms of yielding the evolution property. However, as we will see in Section 11-A, the evolution property is essentially an expression of the Maximum principle for the Heat Equation.</p><p>The practical import of the evolution property is that the zero-crossing information can be simplified, and represented in a symbolic (or approximate) way by describing the nesting and some other simple features of the zerocrossing surfaces. The hope is that the simplified or symbolic representation will also suffice to form a complete representation of the original data, at least for image analysis purposes. For instance, Johansen considers the representation of signals by the "toppoints" of the zero crossings in scale space (with t extended to -00) <ref type="bibr" target="#b19">[20]</ref>. To date, the hope of simplifying the representation of the fingerprint of zero crossings has borne little practical fruit, but has nonetheless sparked a wide range of interesting mathematical analysis.</p><p>In the next section, we consider some of the mathematical results related to the evolution property and the issue of completeness of the representation by zero crossings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">ANALYTICAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evolution Property</head><p>In this section, we show that the evolution property, applied to level crossings (and not just zero crossings), is equivalent to the classical Maximum Principle for parabolic partial differential equations. That is, we will show that the evolution property can be derived simply from the maximum principle, and moreover, that any scale-space construction that obeys the evolution property for all level crossings, subject to minor restrictions, will satisfy a maximum principle.</p><p>The classical maximum principle for the solution to the parabolic Heat Equation &amp; / a t = Au states (see, e.g., Maximum Principle: Let D E R" be open and bounded.</p><p>Suppose U is a solution to the Heat Equation in T = { ( x , T2. Then C n { ( x , t ) ( t = T , } # 0 .</p><p>[21]- <ref type="bibr" target="#b22">[23]</ref>):</p><formula xml:id="formula_3">t ) I x E D , 0 &lt; t &lt; T } of class C 2</formula><p>and is continuous in the closure T. Then U assumes its maximum at some point ( x , t ) for which either x E aD or t = 0.</p><p>The maximum principle holds for more general parabolic partial differential operators, but certainly not for all parabolic equations. It is not known (to the authors' knowledge) how to characterize parabolic equations giving a maximum principle. For example, the maximum principle does not hold when the biharmonic operator is used: &amp; / a t = A2u. Generally, the elliptic operator will be second order in order for a maximum principle to hold.</p><p>We will assume that whenever a maximum principle holds, a similar minimum principle also is given. For linear parabolic operators, such as the Heat Equation, a minimum principle follows from the maximum principle. However, since we can envision more general operators, we will henceforth assume that the statement that a maximum principle holds means that both a maximum and minimum principle hold.</p><p>Next, let us denote the construction of the representation of the image f in scale space by the operator v = Sf.</p><p>In the previous section, we described the construction of U, givenf E L" ( W2), according to However, we now wish to permit more general constructions, allowing, for example, f t o be defined on an irregular or bounded domain, or allowing constructions involving differential operators with nonconstant coefficients. However, we will need to place restrictions on the growth of functions v = Sf as 1 x 1 -+ 00 and on the boundary of the domains. Specifically, we will assume that the domain o f f is D (which may or may not be bounded), and that the corresponding scale space will have</p><formula xml:id="formula_4">( x , t ) E D X [0, 00).</formula><p>There is a class of permissible images f for which the scale-space operator v = Sf is defined, and we assume that for U'S constructed in this fashion:</p><formula xml:id="formula_5">i) v E C ( D x (0, a)) ii) v(x, t ) = 0 forx E 8 0 , iii) v(x, t ) -+ o as I x 1 -+ 00 uniformly for t 2 0. t 1 0;</formula><p>Note that this means for the Laplacian-of-Gaussian construction given above, we must limit application to functionsfthat tend to zero as I x I + 00. The spacefe L2 ( RI2)</p><p>n L"(W2) suffices, for example. This restriction is stronger than necessary, but not tembly objectionable.</p><p>Our main result in this section is that a scale-space construction v = Sf, given the boundary restrictions, satisfies the evolution property for level crossings if and only if a maximum principle holds (meaning both a maximum and minimum principle). The result is Theorem 1.</p><p>fieorem 1: Consider a scale-space construction v = Sf satisfying i), ii), and iii) above for all admissible functions f. Then the following are equivalent.</p><p>1) The maximum principle holds for functions v = Sf obtained from admissible f's.</p><p>2) The evolution property holds for level crossings of solutions v = Sf obtained from admissible f's.</p><p>Proof: We first show that the maximum principle implies the evolution property. For if the evolution property fails for a level crossing 1 of ~( x , t ) , v = S' , then there is a component C, and bounds t , , t2, where C is a To show that the evolution property for level crossings yields the maximum principle, an even simpler argument suffices. For, suppose that D X [ t , , t 2 ] is a bounded cylinder, with a maximum (or minimum) at a point (xo, t o ) in the interior or top of the cylinder, in violation of the maximum principle. Either way, there is a value 1 less than the maximum (or greater than the minimum) at (xo, to), but greater (or less than) the values of v on the sides and bottom of the cylinder. Thus, a component of the level4 crossing within the wedge t , I t I t2 lies entirely within the interior of the cylinder, and thus does not meet the plane { t = t , }. Thus, the evolution property fails if H As a result of the theorem, we have an evolution property for scale-space constructions more general than Laplacian-of-Gaussian filtering. For example, let D be a bounded domain, and consider scale space T = D X [0, 00 ) with functions constructed as follows.</p><formula xml:id="formula_6">connected component of { v ( x , t ) &gt; 1 It, I t I .t2} or { v(x, t ) &lt; 1 I tl I t I t2 ] such that C does not meet the plane { t = t , }. If 1 # 0,</formula><p>ForfE L " ( D ) , solve for U E C ( T ) , the maximum principle is violated.</p><formula xml:id="formula_7">au Au = -i n D at x (0, a)</formula><p>Then let</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>v(x, t ) =au ( x , t ) . at</head><p>We observe that U = 0 on the sides of the scale-space cylinder, and that the maximum principle holds since v satisfies the Heat Equation. Thus, level crossings of v evolve as t increases, and always meet the base { t = 0 } .</p><p>Under more restrictive assumptions, one can show that the evolution property requires Laplacian-of-Gaussian scale-space construction <ref type="bibr" target="#b8">[9]</ref>. The assumptions, however, require that the domain be all of Euclidean space, and that the underlying equation have constant coefficients. Further, proofs of the evolution property have frequently assumed that a level-crossing component not meeting { t = 0} will have an extrema1 lower point, which is an invalid restriction. Given the equivalence of the evolution property to the maximum principle, any proof that does not cite the maximum principle or essentially redo the proof is suspect. Since the proof of the maximum principle is slightly delicate, especially in the absence of strong regularity assumptions, the former course seems more appropriate.</p><p>A version of the evolution property is easy to establish when the scale space is a discrete domain. For simplicity, let us treat the case of one space dimension, and assume that we are given dataf;, i = , -1, 0, 1, * * -. We obtain scale-space data q k , where k is the scale, or level, k = 0 , 1 ; . * . The method by which v is constructed is unimportant at this point; a maximum principle states that in any rectangular grid of lattice points { (i, k ) I i , I i I i 2 , kl I k I k 2 } , the maximum of q k must occur on either the bottom k = k , or the sides, i = i , or i = i 2 .</p><p>The maximum principle will surely hold true if U satisfies a construction where Given the maximum principle for discrete data, we can easily obtain an evolution property. The evolution property will state that for any 4-connected component of grid points of { ( i , k) I q k &gt; 0, kl 5 k 5 k2 }, this component will include points on the bottom level k = k,.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Completeness when Gradients are Included</head><p>We now show that the zero crossings, when supplemented with gradient data along the zero crossing boundaries, are sufficient (in theory) to reconstruct the original functionf(x). Actually, the method is used to reconstruct A f ( x ) . However, if we assume that f ( x ) -+ 0 as I x I -+ 00, then f ( x ) can be reconstructed by solving Poisson's equation. Details of these ideas were reported earlier in a technical report <ref type="bibr" target="#b23">[24]</ref>. In the same report, it is observed that completeness of the representation involving gradients along the zero crossings is easily established, by a nonconstructive proof, using the Hopf maximum principle for the Heat Equation. We instead provide a constructive proof below, although it should be noted that the constructive method is unstable.</p><p>The use of gradient data for the representation also appears in <ref type="bibr" target="#b6">[7]</ref>, but the gradient data there are not limited to the zero crossings. The use of gradient data along zero crossings is discussed in <ref type="bibr" target="#b24">[25]</ref>. Many researchers have noted from a casual observation of zero crossings of image data that zero crossings with large gradient magnitudes are of greater significance than those with low gra- . ~ ~~~ Theorem 2: Suppose that the data g (x), x E D, are contained under a bounded zero crossing surface r of the scale-space function U . We require that g ( x ) belongs to the class S' of tempered distributions (and thus has a Fourier transform from which g can be recovered) [26]. Then b ( x ) can be reconstructed from the data I? and V u along r. Thus, g(x), forx E D , is theoretically reconstructible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Continuous Case:</head><formula xml:id="formula_8">Specifically, let Q be a bounded connected component of { (x, t ) It 1 0 , ~( x , t ) f 0}, anddenotebyDtheset { x ~~~I ( x , O ) ~n ) , a n d b y r the zero crossings an n { t &gt; 0 } . Let T be a value such that 7 &gt; sup { t I (x, t ) E Q } . Next, denoting A f ( x ) by</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof: Observe that aw at</head><formula xml:id="formula_9">-+ AW = 0 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Using this fact and Green's theorem, we have O = j o ( $ --A u ) * w d x d t + s , ( $ + A w )</head><formula xml:id="formula_10">-v dx dt r ( v w ) ~ + (VVW -W V V ) n] da = Jan [</formula><p>where ( n , c) (x, t ) is the outward pointing surface normal to I' at (x, t ) and da is surface area measure. The gradient V is with respect to spatial coordinates only. Using the factsthat (n, c) = (0, -1) whent = Oandv = O o n r , the equation reduces to </p><formula xml:id="formula_11">= -1 w ( x , t ) ( V v -n ) ( x , t ) do.</formula><formula xml:id="formula_12">( y ) = J, K ( x -y , 7 -t ) ( V v -n ) ( x , t ) da.</formula><p>Thus, given the zero crossing r and V V ( X , t ) for (x, t ) E r , the blurred data b (x) can be constructed by a simple linear process. Since the blurring operation taking g(x)</p><p>to b (x) is one-to-one, g (x) for x E D can theoretically be Deblurring is, of course, a classic unstable process. The situation is not hopeless, however, since g (x) has known comPact sumort. which might be used to advantage, and reconstructed.</p><p>H also since errors that occur are predominantly in high-frequency components, which might not be as essential to visual interpretability. Further, one might hope that approximation methods would suffice <ref type="bibr" target="#b25">[27]</ref>. Nonetheless, the method is provably unstable. However, the foregoing analysis applies only to the case where Q is bounded. Next consider an unbounded component Q of (x, t ) space with the zero crossings removed.</p><p>Let T be greater than the range of t in all of the bounded components of that space. Finally, let</p><formula xml:id="formula_13">Q, = Q n { (x, t ) IO I t I 7}.</formula><p>We repeat the previous argument, replacing integration over Q by integration over a, . There is now in addition to</p><formula xml:id="formula_14">D = { x E Rfl I ( x , 0) E Q,}, another boundary portion D, = { x E Rn 1 ( x , T ) E Q , } to consider</formula><p>in the surface 'integral. Thus, we obtain</p><formula xml:id="formula_15">+ w ( x , t ) ( V v -n ) ( x , t ) du r n ( t &lt; 7 ) 4 Y , 7) (Y, 7 ) E D7 = [O otherwise.</formula><p>Here we have used the fact that w( y , The lesson of this section, ultimately, is that even for bounded zero crossings supplemented with gradient data along zero crossing, reconstruction by the indicated method is still unstable.</p><p>2) Discrete Data: It is interesting that the analytical result above can be converted to a discrete form, without involving any discrete approximations. This is because Green's theorem can be converted to a discrete sum. For a signal or image defined on a discrete lattice, we first define a notion of a Gaussian pyramid (actually, a "monolith"), and a Laplacian pyramid, and then the concept of zero crossings in the discrete scale space. Knowledge of the zero crossings together with gradient data, in the formulation below, will translate to knowledge of the values of the scale-space function on a discrete set of points, with the points located on either side of a zero crossing. Since values are given on both sides of a zero crossing, the gradient values are implicitly encoded in the representation.</p><p>For simplicity, we treat the case of one unbounded space dimension, although the results extend easily. We are given dataf;, i = --, -1, 0, 1, -, and define</p><formula xml:id="formula_16">g. = ifi--i f i + 'f. 4 1 + 1 .</formula><p>We define the filtered data U i , k recursively Ui,o = gi, U ; , k + l = $ U ; -l , k + hO;,k + $ U ; + l , k .</p><p>We also define the blurring kernel, using binomial coefficients and Ki,k = 0 elsewhere, for k L 0. Both U and K satisfy a discrete version of the Heat Equation, namely,</p><formula xml:id="formula_17">u i , k + l -Vi,k = i u i -l , k -3Ui.k + i U i + l , k .</formula><p>It is not hard to prove a discrete analog of the evolution property for zero crossings: The key, as indicated in Section 11-A, is a discrete version of the maximum principle, which is easy to establish.</p><p>To formulate the reconstruction method, let Q be a bounded 4-connected collection of pixels (i, k ) with a non- </p><formula xml:id="formula_18">empty set D = { i I (i, 0 ) E Q } . Let Tbe an upper bound T &gt; max { k l ( i , k) E Q } ,</formula><formula xml:id="formula_19">{(i, k ) E Q ( ( i zk 1, k ) $ a } , a,o,,,Q = {(i, k ) E Q K i , k + 1 ) @ Q } , a(o,-\)Q = ((i, k ) E Q ( k &gt; 0, (i, k -1 ) $ a } .</formula><p>Then simple but messy algebra allows us to show Theorem 3.</p><p>Theorem 3: <ref type="figure">j ,</ref><ref type="figure" target="#fig_21">l (i,</ref> In order to make the computations feasible, it is necessary to modify the formulas for a bounded spatial domain. For example, a common approach to constructing a discrete scale-space function involves solving a bounded domain problem, with -N I i I N , by setting = 0 for i = + N . The blurring kernel K is changed by this modification, but the proposition carries over with little change.</p><formula xml:id="formula_20">[ Ui,k +2Ui+e,k 4bj = ( K i + e -</formula><p>There is another version of the formula for bJ above, and thus for the reconstruction of the image data, where the stored information consists of the precise locations of the zero crossings, assuming a linear interpolation of values between grid points. The gradient data, in the form of the difference of the neighboring grid values, still need to be included. With this information, the formulas simplify some, but the information in the representation is clearly equivalent, so we omit the details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">COMPLETENESS AND A BRIEF SURVEY OF</head><p>RECONSTRUCTIONS Under certain restrictions on the class of signals, it can be shown that the zero crossings in scale space form a complete representation of the signal data. There have been many different studies and theorems along these lines for applications to computer vision. An oft-cited study, for the case of signals in one dimension, is provided by Logan [28]. Logan's theorem requires that the function be bandpass, and satisfy certain technical conditions concerning the signal's Hilbert transform. Further, the result is intended for the reconstruction of a function represented by the zero crossings at exactly one level of resolution. While feasible as a complex analytic result, our interest here is in the more practical possibility of reconstruction given zero crossings at multiple scales of resolution in scale space.</p><p>More complex analytic results can be used when the image data are polynomial. In fact, when f ( x ) and thus Af(x) is a polynomial in x E R", then v(x, t ) is a polynomial in (x, t ) E R" + I . Accordingly, the zero crossings are part of the analytic varieties of the polynomial v as studied in algebraic geometry. It is well known that the varieties in C" determine the complex polynomial defined on n complex variables. It is not as commonly used, but nonetheless true, that an n-dimensional subportion of the intersection of the analytic variety with R" + determines an irreducible polynomial. A proof of this result, supplied by Mumford, is given in the Appendix. Related proofs are given by Huang and Sanz [29] and Curtis, Oppenheim, and Lim [30]. Thus, the case of polynomial data can be settled with algebraic geometry.</p><p>However, these results depend heavily on the assumption that the data are polynomial. It is obvious that there are many different functions, even smooth functions, that have identical zero crossings. Interestingly, the completeness in the case of polynomial data applies equally well to the scale-space function v(x, t ) as to the original data f ( x ) or, for that matter, any single level of the scale-space function v(x, to). That is, the polynomial f ( x ) is determined by its zero crossings (providing it is irreducible as a polynomial), a level of the scale-space function u(x, to) is determined by its zero crossings, providing it is irreducible, or the scale-space function u(x, t ) is determined by its zero crossings, providing it is irreducible. In any of the above cases, f (x) can theoretically be reconstructed. However, it is in some sense more likely that v ( x , t ) will be irreducible, and we expect the reconstruction offfrom the function u to be more stable than reconstruction from a single level of U , or from the zero-crossing curves off alone.</p><p>Yuille and Poggio [31] prove a similar result: showing that when Af(x) and hencef(x) is polynomial in x, and if n = 1, then reconstruction from zero crossings is theoretically possible. They refer to the validity of the observation for larger n. Their method relies on an expansion of the analytic structure of a zero-crossing contour, using arbitrarily high derivatives of the contour, at two points of the zero crossing in scale space. While the method is constructive, it is not likely to lead to stable reconstructions.</p><p>Curtis, Shitz, and Oppenheim <ref type="bibr">[32]</ref> and Sanz and Huang [29] extend the above results to the case whenf(x) is a band-limited function which is irreducible as an entire function. These results are likewise of an algebraic geometry nature, and rely on the fact that there is a finite Fourier series of the band-limited function that provides a polynomial of several complex variables for representing f.</p><p>Since the determination of a polynomial by its varieties is essentially an analytic continuation result, stability of the reconstruction is unlikely. That is, small errors in measurement of the zero crossings could lead to arbitrarily large errors in the determination of A f ( x ) . Put differently, there can be widely different initial data leading to nearly identical zero-crossing data.</p><p>Worse, settling the case for polynomial data, or even irreducible entire band-limited data, says little about the general case of continuous initial data. Although the Stone-Weierstrass theorem says that a continuous function can be uniformly approximated by a polynomial on a compact set, the zero crossings depend on the initial data globally, and the dependence cannot be localized. Further, the lack of stability means that the approximation is irrelevant. The situation is similar to the fact that a poly-nomial of a single variable with all real roots is determined by its zeros, but that given all the zeros of a continuous function, one knows nothing more than the zeros.</p><p>Based on these results of completeness under certain restrictions, there have been a number of attempts at reconstructions from zero crossings. The hope is that natural image data will sufficiently restrict the class of admissible functions that some stable reconstruction method can be found. We briefly survey below three such attempts. In Section IV, we provide a formulation for a new attempt at reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Curtis-Oppenheim</head><p>Curtis and Oppenheim, using results that show that band-limited images are completely represented by their zero crossings under certain conditions, present some experiments with reconstruction [33], In their formulation, they begin with a band-limited imagef(x, y), and record a large number of locations in the ( x , y ) domain wheref crosses some threshold (such as zero, assuming thatfhas both positive and negative regions). These locations are recorded with great precision, to ultra-subpixel accuracy. The image is reconstructed from this information. Thus, in their work, the image is reconstructed from a number of level-crossing contours. Clearly, the assumption that the image is band-limited is crucial, for otherwise there are an infinity of images having precisely the same level crossings. In essence, the work constitutes reconstruction from thresholded imagery, but with the understanding that the image is band-limited, and that the level-crossing contours are recorded with great accuracy.</p><p>The method begins by writing the discrete image f as the inverse Fourier transform of its discrete Fourier transform F</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>~</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N -1 N -1</head><p>This function is interpolated to an entire band-limited periodic function, f ( x , y), given by . N -1 N -1</p><formula xml:id="formula_21">I u = o v = o</formula><p>It is the locations of zeros off(x, y ) that are recorded, say ( x n , Y n ) , n = 1, --* , K. We can then write the K equations</p><formula xml:id="formula_22">N -1 N -1 F(u, v ) e 2 r i u n / N e 2*ivYn/N = 0 9 u = o v = o n = 1 , 2 , --e , K .</formula><p>If the mean value p off(x, y ) is not zero, then the additional equation F ( 0 , 0) = p N 2 is included. Otherwise, one more equation is needed, giving the location and value off(x, y ) at a single location wherefis nonzero. Assuming the former case, we have K linear equations in the unknowns F( U , v), ( U , U ) f (0, 0), where ( U , U ) ranges over the known (band-limited) support of F. By using many more equations than unknowns, and computing the least squares solution, reasonable reconstructions are obtained. However, in the examples shown, the number of independent nonzero spectral components is roughly 200. Further, the locations of these spectral components were chosen carefully, in advance, and the image to be reconstructed was obtained from a full-resolution image by setting all except the 200 Fourier coefficients to zero. By tiling Fourier space (as Rotem and Zeevi do; see below), a general image could be reconstructed by this method using the zero crossings of multiple images, each obtained by filtering the initial image. However, since a 256by-256 image contains 64K independent spectral components, it is not clear how many levels will be needed. Nonetheless, the method provides a theoretically possible means for representation and reconstruction from multiple images of zero crossings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Rotem-Zeevi</head><p>Rotem and Zeevi, in a paper in which they extend the Logan theorem for application to two-dimensional images, experiment with reconstructions from zero-crossing data [34]. Their method is based on first decomposing the given imagef(x, y ) into a sum of imagesfk(x, y), with each fk obtained f r m f b y a filtering operation in Fourier space. Eachfk has the property that its discrete Fourier transform has nonzero coefficients in a region that is bandpass of one octave or less in at least one of the two spectral dimensions. Further, the low-frequency components are omitted from the reconstruction process, and simply stored as part of the representation, and an attempt is made to reconstruct only a low-pass version off, where all frequencies above half of the maximum are omitted. Thus, for a 256-by-256 image, where spectral frequencies range from -128 to 127, only the frequencies in the range -64 to 64 are included in the reconstruction, and the spectral frequencies where both components U and v satisfy I U I I 8, I v I I 8, are stored separately, and excluded from the reconstruction. The remaining spectral domain is decomposed into 19 regions, so that there result 19 images, f k , k = 1, ---, 19. Iffo denotes the lowpass image composed of frequencies below 8, and if g(x, y ) is the high-pass image composed of frequencies above 64 (in either dimension), then</p><formula xml:id="formula_23">19 f = f o + f k + g. k = O</formula><p>By keeping the filters yielding the decomposition symmetric (i.e., including the component for <ref type="figure">( -U, -U</ref> ) if the coefficient for ( U , v ) is included), we can be sure that if f i s a real-valued image, then each of the images in the decomposition will be real valued.</p><p>Rotem and Zeevi reconstruct each of thefk from the information sgn( fk), k = 1, --, 19. Since eachfk is a bandpass off, they correspond, crudely, to the levels of the scale-space function v that we use to reconstruct f.</p><p>However, the analogy is poor, since thefk are true bandpass images, with one octave or less in frequencies in at least one of the two spectral dimensions, whereas a level of u is only an approximate isotropic bandpass filter off. Further, the number of free variables in any givenfk is small-the greatest number of spectral components used in any of the fk is 1024, whereas the sgn( fk) have 64K bits of information, for each k. In any case, the reconstruction of anfk works as follows.</p><p>Suppose that fk is bandpass in the horizontal spectral dimension. It is then easily shown that each row offk is a one-dimensional bandpass signal. (Alternatively, iffk is bandpass in the vertical spectral dimension, then columns offk are bandpass.) The spectral support of the rows or columns is the corresponding band in the horizontal or vertical dimension of the spectral domain of fk. The reconstruction begins by finding individual rows or columns that can be reconstructed from the sign data along the row or column. For a one-dimensional row S which is bandpass, denote by Bp the bandpass filter; thus, S = Bp(S). Given the data sgn ( S ), let So = Bp ( sgn ( S ) ). The algorithm to recover S proceeds iteratively, beginning with So, and iteratively setting</p><formula xml:id="formula_24">S,+, = S , -c[Bp(sgn(S,)) -So].</formula><p>The iteration is not guaranteed to converge, but if it does, one generally has that sgn ( S, ) = sgn (S ). Providing all the conditions for uniqueness of the representation of the one-dimensional signal by its zero crossing are met (i.e., Logan's conditions), then this will imply that S, and S differ only by a multiplicative constant. Having reconstructed a number of rows (or columns), modulo multiplicative constants , the one-dimensional signals must be individually scaled. By reconstructing a one-dimensional signal in a transverse direction, ratios can be obtained to scale the individual rows (or columns) modulo a single multiplicative constant, relative to the whole image. If the image is bandpass in the vertical dimension, then columns can be reconstructed. If the image is only low-pass in this dimension, Zeevi and Rotem show that it will be bandpass in a diagonal direction, and so some diagonal can be reconstructed. In practice, they reconstruct several transverse one-dimensional signals, and use averages to best scale the individual rows. Reconstruction along columns that do not initially converge can be assisted by initialization with data obtained from diagonals or rows, and from horizontally blurring the partially reconstructed image.</p><p>Once eachfk is reconstructed to within a multiplicative scale factor, a scale factor is applied, based on the assumption that the variance of eachfk is known in advance. The final reconstruction off is performed using the reconstructed fk, the storedfo, and by omitting g . The result shown in their paper is visually good, and they show that errors are mostly concentrated on the edges of the original image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Sanz-Huang</head><p>More recently, Sanz and Huang have shown examples of reconstructions of images using zero crossings of the scale-space function recorded at four levels <ref type="bibr">[29]</ref>.</p><p>First, four levels of the scale-space function v(x, y, ti) are chosen, i = 1, 2, 3, 4, where U is the scale-space function of the initial image f. The information that is stored in the representation is sgn( ~( x , y, ti)) for i = 1, -* * , 4. The operator that takesf(x, y ) to C U ( X , y, ti) is linear, and a pseudoinverse is computed. This operator, which cannot restore the very low spectral components or the very high spectral components, is nonetheless a convolution operator (assuming an unbounded domain), given by convolution against a function 4. That is, f = 4 * c U ( . , -, t;).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I</head><p>The reconstruction algorithm proceeds iteratively , beginning with so = 4 * C sgn(u(*, * , t;)).</p><p>Given S,, the next iteration is obtained by first computing the scale-space levels of the estimate S, 1 U , ( . , * , t;) = AK( * , * , t;) * S,,, and then modifying U, so that the data have the right sign %(x, y , ti)</p><formula xml:id="formula_25">I U&amp;, y , ti) I if sgn( 4 x 9 Y , ti)) &gt; 0</formula><p>Finally, the B's are summed and filtered to obtain the next estimate S , , , = 4 * c f i n ( -, -, t;).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I</head><p>This procedure is not guaranteed to converge, but was used for experimental purposes anyway. When applied to large, natural gray scale images, they resulted in reconstructions that had scale-space functions whose signs agreed, on the four levels, with the original sign data of the representation, at roughly 80 percent of the pixels. However, the reconstructions, while clearly related to the originals, had a rather stylized or "impressionistic" appearance, and many perceptual differences from the originals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comments</head><p>Each of the above reconstruction techniques attempts to find a function that satisfies the given zero-crossing information by taking some measurement of the error of the zero crossings of the reconstruction with the desired zero crossings. We might call this generic approach one of minimizing data error: the error in realizing the zerocrossing data. As yet another version of minimizing data error, we could take as a measure of the error the square integral of the L2 difference between the sign of the scalespace function and the desired sign of the scale-space function:</p><p>where sgn( vo) is the given data in scale space, and U = AK *f. This functional is not differentiable with respect to the reconstructed imagefdue to the discontinuous signum function. However, if we replace the signum function sgn with a C" approximate, say $, then the error measure becomes a smooth function of the f data. Suppose that instead of being given the sign of the scale-space data, we are instead given S o ( x , y, t ) = $ ( v o ( x , y , t)). Suppose thatfis the current estimate of the original image. Then the gradient of the error measure with respect to the estimate f is proportional to where U = A K *f. Thus, a simple gradient descent procedure can be formulated to minimize the error. The result is a particularly simple linear iterative process. Of course, the representation is now the approximate signum of the vo data, 4 ( vo), rather than the simple boolean information about the sign of the filtered data or the location of the zero crossings. Further, convergence is not guaranteed and can be very slow. If the true signum data are used as the target data in place of So, then convergence of the iterative process is much worse.</p><p>Instead of pursuing this approach, we instead develop the idea of minimizing equation error, discussed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. FORMULATION AND METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Equation Error Minimization</head><p>In this section, we describe a method for reconstructing an image from its zero crossings based on minimizing equation error. The method was reported for a different application (deblurring) in conference proceedings [35], and preliminary results on reconstructions from zero crossings were shown at a different conference [36]. The use of minimization of equation error to solve ill-conditioned inverse problems is motivated by a related technique that has been applied to identification problems for elliptic equations [37], such as in porous-media flow problems (i.e., oil recovery). In these applications, the method proved more favorable than more typical identification problem methods, related to minimization of data error. Although one can provide heuristic arguments motivating the selection of equation error minimization for inverse problems, a thorough mathematical justification of the stability and advantages of the method is still lacking.</p><p>Our approach to reconstruction from zero crossings is to reconstruct the function v in scale space, and make use of the fact that ZI satisfies the Heat Equation. The discretized data can be viewed as a multilevel grid of units communicating locally. The essence of the idea is that the units should achieve values satisfying the given zerocrossing constraints, and also satisfying, to the extent possible, a discrete version of the Heat Equation. As in a network, the values are updated iteratively using information from local values to minimize a measure of error. As such, the method is amenable to implementation using networks of analog processors connected in a mesh by tunable resistive links. We first sketch the formulation of the approach in a continuous domain. Consider the initial image f ( x ) and its scale-space Laplacian-of-Gaussian filtered data vo (x, t ) . We assume that the zero crossings in scale space of vo are given, in the form of knowledge of sgn [ vo(x, t ) ] at all points. We then pose the problem:</p><formula xml:id="formula_26">Find v minimizing AV --, / I :I2 subject to sgn[v(x, t ) ] = sgn[vo(x, t ) ] .</formula><p>Here, the norm 11 -11 is the L2 norm over the scale space R" x R+.</p><p>We in fact advocate a slightly different measure of the equation error. We transform the Heat Equation, a second-order partial differential equation, into a first-order system These problems can be given a more precise formulation using the theory of variational inequalities [38]. The spaces of admissible functions (functions satisfying the inequality constraints) for the unknowns U and U should be defined as convex subsets of appropriate Sobolev spaces. When this is done, the problem becomes a standard "obstacle problem," for which existence and uniqueness of a solution can easily be shown. Further, the only local minimum of the functional is also the global minimum. However, we can only be sure that the Heat Equation is satisfied if the error is actually zero. Accordingly, we know that if we minimize the equation error, we (at least in theory) will have no problems with local minima, and will find a single global minimum; further, we know that we can always find a global minimum, but that we have a solution to our reconstruction problem (that is, a solution to the Heat Equation with the given constaints) only if the error at that minimum is zero. In general, existence of a solution to the Heat Equation with given sign constraints is not guaranteed.</p><formula xml:id="formula_27">v u = U ,</formula><p>Numerical methods for the solution of constrained quadratic optimization problems, such as this one, are much studied in numerical analysis. Related optimization problems frequently occur in computer vision [39]. In the next two sections, we discuss some of the implementation details for the experiments described in Section V, concentrating on the discrete formulation peculiar to the equation error minimization problem for reconstructions from zero crossings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Discrete Formulation</head><p>We now describe the discrete formulation of these ideas. The case of a finite-difference discretization for n = 1 will be described in some detail. Extension to two spatial dimensions involves some complications which will be summarized more briefly. A finite-element formulation is also given for the case n = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) One Spatial Dimension:</head><p>From the given initial data 5, we recursively construct the scale-space function vi,k from the formulas:</p><p>To make the computation feasible, the space and scale domains must be bounded. Various approaches to handling the spatial border points are possible. A particularly convenient one, analogous to Neumann boundary conditions, is defined as follows. Let us assume that 5 is defined for 0 I i I Nl. We then extend5 to be periodic with period N, according to the formulas f ~+ ; <ref type="figure">=fN-,-;</ref>,</p><formula xml:id="formula_28">i = 0, 1, ---, N -1, f Z N + ; = A , all i .</formula><p>Then the values q k are defined for all i and all k 1 0.</p><p>The scale-space function vl, k is periodic with period 2 N in the variable i , and has the same structure as the ex-tendedA data. Specifically, the data from i = 0, ---, N -1 are repeated in reverse order from i = N , --* , 2 N</p><p>-1, and then extended as a 2 N block periodically (see Fig. <ref type="figure" target="#fig_7">2</ref>).</p><p>The U data approximate the Laplacian applied to the Gaussian-blurred image data. Associated with the computed scale-space data ui,k are the sign data Si,k, which are defined to be 1, 0, or -1 according to whether vi, is positive, zero, or negative, respectively. In practice, the U data and resulting S data are calculated only for 0 I i I N -1, and for 0 5 k I T, for some arbitrary cutoff T.</p><p>Having computed the representation in the form of the S,,k data, we proceed to the reconstruction. The discrete reformulation of the Heat Equation as a first-order system involves the introduction of a second grid of data, 0 i . k which ideally satisfies Note that the values at the endpoints, e.g., the datumfN_ ,, appears twice in succession.</p><p>Then the equation etror is defined by the quadratic functional</p><p>We minimize E subject to the constraints</p><formula xml:id="formula_29">when Si,&amp; = 1 u;,k 2 0 Vi,k 5 0 when Si,k = -1.</formula><p>A further constraint is needed to ensure that the solution ZI = 0 is not obtained. In principle, this could be done by specifying a single value for v at some node. In practice, such a constraint would propagate only very slowly through the grid, making convergence difficult. We chose instead to specify U at the maximum level k = T , where it is ordinarily quite smooth. Little is added to the representation by inclusion of the ut, data, since these data could be quite accurately represented by a very few floating-point values specifying the low-order discrete Fourier coefficients. Recall that a similar approach is utilized by Zeevi and Rotem (Section 111-B). (In experiments involving fixed gradient data along the zero crossings, the zero solution is no longer admissible, and the vi, data are not included in the representation.) As in the continuous case, the minimization of the (coercive), quadratic functional on a convex set results in a unique local minimum which is also the global minimum. The difficulty comes about because of the large number of variables. Without care, convergence can be extremely slow. Further, we do not know in advance how "shallow" the minimum will be on the convex set. When the minimum is shallow, then the problem is unstable, and we expect to see dependence on initial conditions in the iterative minimization algorithms. Such is the case for the experiments with reconstructions from zero crossings alone. When the minimum is sharper, convergence should be faster and independent of the initial conditions, as is the case in the experiments with reconstructions from zero crossings with gradient information.</p><p>In the experiments described in Section V, we used a conjugate-gradient approach to minimizing the error functional. The inequality constraints were imposed by means of a penalization method, adding to E a term that grows quadratically in the variable whenever a constraint is violated. By placing a large weight on the penalization term, a functional suitable for minimizing is obtained that will ensure, upon convergence, that the constraints hold approximately. Specifically, we use a penalization term of the form</p><p>The gradient of the equation error with respect to the unknowns can be represented as a sum of discrete convolutions applied to the data v and U , which may be regarded as image arrays. Then for E equal to the equation error as above, the values of aE/av,, k and aE/aui, k form image arrays also, which we will denote by V U E and V,E.</p><p>The formulas (valid for k &gt; 0) are</p><formula xml:id="formula_30">0 -4 V U E = -1 10 -1 * U + 0 -4 [ 0 -4 1 1 [ : : -B1. V U E = [ -2 : -: 2 0 : ] * U + [ -! ! -! ] * U .</formula><p>Here, the three-by-three arrays are masks that are employed in three-by-three local convolutions against the array data v and U. The boundaries can be conveniently handled by placing borders one pixel wide around the data arrays. These border pixels are set so as to satisfy the chosen boundary conditions. The sign penalty adds a term</p><formula xml:id="formula_31">A(sgn ( v ) -SI -v to V U E .</formula><p>The minimization by conjugate gradients [40] is implemented as follows. Observe that the equation error E as formulated above is a quadratic functional of the unknowns v and U . Combining the unknowns v and U into a single vector, say x , the equation error is thus equivalent to the quadratic functional</p><formula xml:id="formula_32">E = $xTAX -hTx + Eo</formula><p>for some positive definite matrix A , vector h, and scalar constant Eo. Adding the penalization terms to E maintains the same basic form, although the matrix A becomes a function of v . Nonzero values for the quantities h and E,, arise from fixed grid values on the boundaries. The method of conjugate gradients is ordinarily formulated as the minimization of a functional of exactly this form. The computation involves forming the inner products of certain vectors with each other, and products of vectors with the matrix A . However, there is no need to find A , h , and Eo explicitly. Instead, we will make use of the fact that their products with relevant vectors have known forms. Observe that</p><formula xml:id="formula_33">V E = AX -h.</formula><p>Hence, if p is an arbitrary vector (e.g., one of the conjugate gradients), the product Ap can be computed by evaluating the expression of VE, according to the convolutions given above, with p substituted for x, which is the combined v and U data, and h set to zero. Setting h to zero is achieved simply by replacing all constants on the boundaries by zeros. This also has the effect of setting EO to zero. The other matrix product of interest, pTAp7 is computed by evaluating directly the expression for E, again substituting p for n and setting boundary constants to zero. Note that if reflected boundary conditions are in effect, then values forp must be reflected in the same way. Thus, we see that all the matrix products necessary for a conjugate gradient calculation can be computed without knowing A explicitly.</p><p>It is worth noting that this formulation of the conjugate gradient minimization is readily adapted to heterogeneous constraints according to which v or U are held fixed at arbitrary locations in the grid. One simply sets the values of p to 0 at the corresponding positions. In the iterative updating, those points are skipped over, although they enter into the calculation of the equation error and the matrix products.</p><p>2) Two Spatial Dimensions: We now turn to the finitedifference discretization for the case n = 2 . We define the scale-space grid as v ; , ~, , ~, k 1 0, and formulate the Heat Equation as</p><formula xml:id="formula_34">v i , j , k + l = i $ j [ v i -l , j -l , k + 2 v i -l , j , k + V i -l , j + l , k + 2 v i , j -l , k + 4 v i , j , k + 2 v i , j + l , k + V i + l , j -l , k + 2 U i + l , j , k + v i + l , j + l , k l ( 2 -D Discrete Heat Equation)</formula><p>Reformulation of this equation as a first-order system is not so straightforward as in the one-dimensional case. A fairly natural approach proceeds with introduction of variables U/,;!$ fs$;k, and wi,j,k which ideally satisfy</p><formula xml:id="formula_35">( 1 ) = vi,j,k -v i -l , j , k ui,j,k 9 2</formula><p>and When these equations hold exactly, the data U are blurred from level k to level k + 1 by convolution against the twodimensional discrete Heat Equation. Analogously to the n = 1 case, the equations above can be converted to a quadratic measure of equation error in the unknown variables v , w , U ( ' ) , and U ( ' ) . The sign constraints on v can again be implemented using quadratic penalty terms.</p><p>The resulting error functional can then be minimized by a conjugate-gradient procedure in the same way as described for n = l . It is worth noting that this approach results in equations for the gradient of the equation error which are linear in the unknowns, and that a component of the gradient with respect to an unknown at ( i , j , k) depends only on values at neighboring grid points. Thus, this formulation is well suited to a hardware implementation involving only small-kernel convolutions and (for the conjugate-gradient method) accumulation of simple global sums.</p><p>3) A Finite Element Approach: The discretization of the Heat Equation can alternatively proceed by a finiteelement approach <ref type="bibr">[41]</ref>. For the case n = 1, we assume a regular rectangular discrete grid, ( i * h, k s), i = 0, * --Nl a n d k = O , 1, -. . . Here h and s specify the scaling of the two axes relative to each other. (In the finite-difference method, we implicitly set h = 1/2, s = 1 .) The simplest elements can be formed by splitting each grid square into two triangular regions, along one of the two chosen diagonals (standard triangular elements); see Fig. <ref type="figure" target="#fig_11">3</ref>. The unknown functions, v and U, can be represented as continuous piecewise linear functions, linear within each triangular region, by simply specifying the value of the functions at each grid point. Accordingly, the unknowns are the values of v ( i h , ks ) and (T ( i h , k s ) , for all i and k, and will be denoted by vi, k and ai, k . The simple piecewise linear elements suffice since the equation was converted to a first-order system. It is then a simple (albeit tedious) matter to compute the equation error for such functions in terms of the unknowns. Like the finite-difference formulation, it turns out that the equation error is a quadratic function in the unknowns, and that there is locality of dependence.</p><p>The case h = 1/2, s = 1 is very common, and so we will give the gradient equations for this case. As was done for the finite-difference case, the result can be represented as a sum of discrete convolutions, with the data v and U considered as image arrays. Once again, except near border pixels, these gradient values &amp;?/dui, k and aE/aui, k can be viewed as images obtained by convolutions against the grid arrays vi,k and q k . The formulas are and on the borders. Note that a certain amount of asymmetry results simply from choosing triangular elements based on one of two possible diagonal cuts through each rectangular grid. The sign penalty adds the same term to V U E as for the finite-difference formulation. </p><formula xml:id="formula_36">V U E = [ -1 O -4 10 -1 : ] * U + -;[-; -: 0 -4 2 -2 and V U E = : [ -: -6 2 ' ] * v + t [ -2 1 1 12 -!]*U.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Image Reconstruction from Scale-Space Data</head><p>Once the approximate scale-space function 0 has been reconstructed by the above minimization pr?cess, there remains the problem of obtaining the image f to which it corresponds. First, the discrete reconstructed data are summed over the scale dimension to give</p><formula xml:id="formula_37">T P = -c Bk. k = O</formula><p>The J data can be seen to be the discrete approximation to f -uT+ where the U data are defined in terms of convolution off against a discrete Gaussian kernel, Ki, k , as defined in Sectio! 11-B-2. The problem of computing the approximation f to f thus reduces to inverting I -GT+ where GT+ is the operator yieldhg T + 1 levels of Gaussian blur. That is, (I -GT+ ) f = J. The sim- plest and cheapest method is simply to incorporate the uT+ data into the representation, so that an approximation to the original f can be obtained by computing P and adding uT+ Since the uT+ data are quite blurry, including them does not add much information to the representation.</p><p>However, since the operator Z -GT+ is well behaved, it is possible to reconstruct f modulo an additive constant and a multiplicative constant, without inclusion of the uT+ data in the representation, by computing the pseudoinverse of Z -G T + I using a singular-value decomposition (SVD). The resulting falways has an average value of zero; therefore, the average value of the original image is added to f to complete the reconstruction. Unfortunately, computing the SVD can be prohibitively expensive for images larger than about 16 by 16.</p><p>Providing the operator Z -GT+ can be viewed as a convolution, the singular-value decomposition can be computed efficiently, even for large images, using the discrete Fourier transform of I -G T + I or, equivalently, the inversion may be done by means of the discrete convolution theorem. Depending upon the method of handling borders, I -GT+ may or may not be a convolution. If a Fourier method is applied when the borders are handled in such a way that the operator is not a convolution, then even though border effects are confined to a neighborhood of a few pixels from the border, the inversion yields poor results. However, some important border methods can be viewed as convolutions. In particular, for the Neumann boundary conditions model, as discussed in the previous section, the operator is equivalent to a convolution operation against a doubly periodic image that is twice the size of the original in each dimension, obtained by reflecting the image in each dimension. When the Fourier method is used to invert the operator Z -G T + I computed using the doubly reflected periodic image, the results are excellent.</p><p>The inversion is complicated by the singularity that arises out of the additive-constant ambiguity: the "DCcomponent" of Z -G T + I has zero amplitude. Further, the Fourier coefficients belonging to low frequencies will be small in value. A cutoff must be chosen judiciously in order to avoid amplifying low-frequency errors in the inversion process.</p><p>Whether the operator Z -G T + is a convolution or not, provided it is linear, and provided all eigenvalues of G are strictly less than one, the inversion may be obtained from the series expansion which can usually be computed quite easily, if somewhat intensively.</p><p>The results obtained by using the appropriate method for the given boundary conditions are, in all cases, very similar to those obtained using the simpler method by adding uT+ to P as described earlier, assuming that uT+ is part of the representation. Consequently, the reconstructions shown below were performed by the latter method.</p><p>32 64 % 128 Fig. <ref type="figure">6</ref>. A reconstructionfof a signal from the reconstructed Laplacian-of-Gaussian filtered data U. This function should be the same as Fig. <ref type="figure">4</ref> if stable reconstruction from zero crossings were possible.</p><p>V. RESULTS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. One-Dimensional Experiments</head><p>Our first experiments were done with one-dimensional data taken from a scanline of a digitized image. Fig. <ref type="figure">4</ref> shows a plot of the original data. The Laplacian monolith u0 of the original image was computed, as described in Section IV-B-1, to a depth of 40 levels of blurring by the "1 2 1" mask. Fig. <ref type="figure">5</ref> shows the sign of the function u0, using black for negative regions, white for positive regions, and gray for pixels whose values lie within machine-precision zero. Reconstruction proceeded by minimization of equation error, using conjugate gradients with a penalty term for values of U which differ in sign from the corresponding points of u0, and holding the topmost (most blurred) level fixed equal to the corresponding level of u0. The methods for the calculation of the gradients and of the construction were described in Sections IV-B-1 and IV-C. The initial estimate for U was sgn ( u 0 ) , and the initial estimate for U was the gradient of this initial U.</p><p>The computation was run for over 40 000 iterations, achieving an equation error plus penalty of less than lo-'' with only 77 sign violations out of 5120 grid points. (It should be noted that the sign violations are small: the rms average value of the violations is only low4 of the rms average value of U. ) The sum of levels was used to obtain the reconstructed function, by the pseudoinverse method described in Section IV-C. This reconstructed function is shown in Fig. <ref type="figure">6</ref>; and Fig. <ref type="figure" target="#fig_13">7</ref> shows its sign of Laplacian monolith.</p><p>It is evident that the zero crossings of the original and reconstructed functions are essentially identical. Also, major features of the signal are reproduced, although there are significant differences in the small-scale details. We conclude that while the reconstruction from zero crossings preserves some of the essential information about the signal, it is unstable with respect to high-frequency components.</p><p>Next, the reconstruction process was applied to the same data, but this time holding fixed the values of U along the zero crossings of u0, and leaving free the values of U at maximum blur. In this way, we have specified the zero crossings and the gradient information at the zero crossings through 40 levels in scale space. After approximately 6000 iterations, the equation error plus penalty fell below</p><p>The reconstructed function is not shown because it is visually indistinguishable from the original signal. The reconstruction function differed from the original by less than at all points (on an intensity scale of 0 to 1 ). This result can be taken as empirical evidence that the representation based on zero crossings plus gradient data for one-dimensional images is complete and stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Two-Dimensional Experiments</head><p>We next turn our attention to two-dimensional images. The original images are 64 X 64 pixels with 8-bit intensity resolution. Because of computing constraints, only 20 levels of scale were used in the reconstructions. Each successive level is obtained from the previous by means of the 2-D discrete Heat Equation, given in Section IV-B-2. Fig. <ref type="figure" target="#fig_14">8</ref> shows an original image of a pagoda and the result of reconstruction from zero crossings. The equation-error minimization was run for approximately 2000 iterations. The reconstruction was computed by subtracting the sum of levels of the reconstructed Laplacian monolith from the blurred original image, as described in Section IV-C. As can be seen, the two images are perceptually different. Mainly, the high-frequency content is not well reconstructed, so that the reconstruction appears as a slightly blurred version of the original. Despite the evident differences between the two images, however, the zero crossings of the two images are nearly identical. There are sign differences at less than 2 percent of the points in the 20-layer monolith, concentrated in the bottom (high-frequency) layers. Virtually all of these sign differences are adjacent to zero crossings, causing at most a one-pixel shift in the location of the zero crossing.</p><p>The reconstruction by equation error minimization is sensitive to the initial guess used to start the conjugategradient iteration. For the reconstruction in Fig. <ref type="figure" target="#fig_14">8(b)</ref>, the initial Laplacian monolith contained values of f 1 everywhere, with the sign matching the required sign at each point. The input image magnitude was in the range [0, 2551. Fig. <ref type="figure" target="#fig_15">9</ref> shows the results obtained using initial values of f 256 instead. Even after running for nearly 20 000 iterations, this reconstruction shows considerable noise and artifacts. These artifacts take the form of strong positive values side-by-side with strong negative values, which largely cancel each other out in the blurring process. Once again, the zero crossings of the reconstructed image in Fig. <ref type="figure" target="#fig_15">9</ref> are nearly the same as those of the original image. The differences between the reconstructed images (Figs. 8(b) and 9) and the original [Fig. <ref type="figure" target="#fig_14">8(a)</ref>] can be taken as an empirical demonstration of the lack of stability of the representation based on zero crossings alone.</p><p>To show the similarities of the zero-crossing representations, we show the sign of the Laplacian-of-Gaussian for the original image and the reconstructed images, in Fig. <ref type="figure" target="#fig_0">10</ref>, at a sampling of levels. The zero crossings are  consequently at the borders between black and white regions. As can be seen, there are some differences between the zero crossings, especially at the bottom levels. However, as noted before, the differences consist almost exclusively of a one-pixel displacement in the location of the zero crossings. This is more easily seen when one displays the zero crossings as curves, and flickers between the two images.</p><p>It is worth emphasizing that the sign of Laplacian-of-Gaussian in Fig. <ref type="figure" target="#fig_0">10(b</ref>) was computed from the reconstructed image, and is not the sign of the reconstructed monolith. The solution method involves the reconstruction of a function in scale space, U, which approximately satisfies the heat equation, subject to a penalty term which tends to make sgn ( U ) agree with the given sgn ( vo). The resulting U, summed ovqr scale space, is used to reconstruct an approximationfto the origin_al imagef. The Laplacian-of-Gaussian computed from f , call it vi, should approximate vo, the Laplacian-of-Gaussian of the original image f. However, each of the scale-space functions vo, D , and vi, is different. It is the sign of vj which is shown in Fig. <ref type="figure" target="#fig_0">10</ref>, fepresenting the true locations of the zero crossings off. Accordingly, Figs. <ref type="figure" target="#fig_14">8(a</ref>) and (b) and 9 represent three images whose zero crossings are nearly the same at all levels, and yet have substantially different numerical and perceptual values. Thus, similarly to the onedimensional case described earlier, these results empirically establish the instability of the zero-crossing representation, when zero crossings are used alone. Of course, one should modify this broad statement with the note that a particular method of handling borders has been specified, and the image comes from a particular class of images.</p><p>The image in Fig. <ref type="figure" target="#fig_14">8</ref>(a) contains a large number of sharp edges, and thus high-frequency components. It is known  that in the conjugate-gradient method of minimizing equation error, the low-fequency components will converge before the high frequencies. In Fig. <ref type="figure" target="#fig_17">11</ref>  Fourier components (nearly a factor of 10, on average) at the higher fequencies as compared to the image of the pagoda, in Fig. <ref type="figure" target="#fig_14">8</ref>. The reconstruction from zero crossings of this image works quite well, as shown in Fig. <ref type="figure">ll(b)</ref>. Again there are in fact some significant numerical differences between the original and reconstruction in Fig. <ref type="figure" target="#fig_0">1</ref> 1, but the reconstructed image is perceptually similar to the original, except for fine details. The signs of the Laplacian-of-Gaussians of the images are shown in Fig. <ref type="figure" target="#fig_18">12</ref>, and are essentially the same, as in Fig. <ref type="figure" target="#fig_0">10</ref>. Once again, there are minor differences at the low levels (high frequencies). In another experiment, not shown, using an even softer original image in which fine detail was relatively lacking, the reconstructed image was perceptually almost identical to the original. Further, the iterative process for reconstructing U converged somewhat more quickly to a low equation error with this image than with the image of the pagoda. In Fig. <ref type="figure" target="#fig_11">13</ref>, we show the result of a reconstruction of Fig. <ref type="figure" target="#fig_14">8(a)</ref>, where the representation by zero crossings has been enhanced with the value of the gradients along the zero crossing. The reconstruction is now stable, accurate, and fast. There are minor numerical differences, but no major perceptual differences between the original and the reconstruction. Of course, the amount of data in the entire representation exceeds the information content in the original image, since at every zero-crossing point on every level, a value of the magnitude of the gradient of the filtered image must be stored. Since these values are stored as floating-point numbers, the entire data structure is in no way a compaction of the original image. However, a compact code is not the objective of the representation by zero crossings, so that at this junction, we can conclude that a representation that includes, with the zero crossings, the gradient data along the zero crossings is empirically complete and stable.</p><p>In other experiments using gradient data, we have found that reconstruction is stable for all kinds of images. Noise can be added to the gradient data, and stable reconstruction is still not difficult. For Fig. <ref type="figure" target="#fig_0">14</ref>, multiplicative noise in the range of 1 k 1/256 was applied to the gradient values along the zero crossings before reconstruction. The noise simulates storing the gradient values with 8-bit precision, and still yields a good representation. However, even with 8-bit values for the gradients, the total amount of data in the typical representation exceeds the data content of the original image.</p><p>In Fig. <ref type="figure" target="#fig_0">15</ref>, we show the same image reconstructed from a representation using the zero crossings together with 10 percent of the gradient values. The locations of the gra- dient values were chosen at random from the set of all zero-crossing pixels. Reconstruction is much improved from the zero crossings alone. However, using only 10 percent of the gradient values, some noise is present in the reconstruction. If the percentage of gradient values retained is increased, then the reconstruction is improved. Indeed, with 50 percent of the gradient values, reconstruction is approximately as good as in Fig. <ref type="figure" target="#fig_11">13</ref>, where 100 percent of the gradient values were retained. Good reconstructions can also be obtained by concentrating the gradient data in the higher-frequency levels. Fig. <ref type="figure" target="#fig_20">16</ref> shows a reconstruction for which gradients were specified on all zero crossings in the bottom two levels, and nowhere else. The number of zero crossings at which gradients were specified came to about 22 percent of the total. The reconstruction is nearly as good as that for which gradients were completely specified at all levels. This result shows how the instability is greatest for the high frequencies: when these are suitably controlled, the reconstruction proceeds stably.</p><p>VI. CONCLUSIONS Despite the enthusiasm for zero crossings in multiresolution representations that was prevalent in the computer vision research community in the early 1980's, their use has declined considerably, in favor of other edge detection and feature detection techniques. Where zero crossings are still used, the system generally incorporates some measure of the gradient along the zero crossings, such as a thresholding operation to eliminate zero crossings with low gradients.</p><p>The analyses and results in this paper tend to corroborate the wisdom of this trend. Specifically, we have seen that there exist distinct images with quite similar zero-crossing surfaces (at least through a large range of scale space), giving an empirical example of the instability of zero crossings alone as an intermediate representation. We have also demonstrated, again empirically, that reconstruction from zero crossings together with gradient values along the zero crossings is possible and stable. The chief disadvantage of this representation is that there is much redundancy in the bulk of data included in the representation.</p><p>The many mathematical results showing that the zero crossings form a complete representation under analytical assumptions, we have seen, are primarily based on analytic continuation results, and thus suggest an unstable representation. The likely situation is that the zero crossings alone are enough to be complete, in most cases, but insufficient to provide stability. One might hope that the inclusion of a little additional information will make the entire representation stable, but short of adding all the gradient information along the zero crossings, this remains to be seen. Preliminary experiments are not encouraging.</p><p>The mathematical analysis of a representation frequently will occur after the technique has been proven inthe-field, or has been largely discarded. Such is probably the case with the analysis of zero crossings for computer vision applications. We could hope to establish a mathematical analysis of the stability of these representations, rather than merely the empirical results cited here, but for vision applications these theorems would be largely irrelevant.</p><p>The technique of minimizing equation error for solving the inverse problem of reconstruction from a representation, however, seems to yield far better results than methods involving the minimization of data error, at least for the applications studied here. We expect that the technique will be useful for other inverse problems, and presumably some in the computer vision field. The general methodology of studying a representation in terms of its mathematical properties, and developing reconstruction methods to evaluate the stability and variations in the fi- bers, in analogy with the study undertaken here, is highly recommended.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>We show that if two irreducible polynomials with real coefficients in n real variables share in common the same ( nl-dimensional ) zero crossing (even for just an open patch), then the two polynomials are the same. This proof was supplied by Mumford. We first prove an analog of the decomposition theorem for real algebraic varieties.</p><p>-, x,] be a prime ideal and let h = the height of a (i.e., the length of a maximal chain of prime ideals: Then if V (a) is the locus of zeros in Rfl shared by all polynomials in the ideal a (the real variety of the ideal a), then V (a) is the union of real submanifolds of Rfl of dimension nh and less.</p><p>Proof: By the standard decomposition theorem, the complex variety V ( a ) C C", which is the locus of complex zeros of the polynomials in ?r viewed as polynomials of n complex variables, breaks up into a smooth part, which is always an nh-dimensional complex submanifold of C", and a singular part, which is a finite union of subsets of the form V ( a ' ) , with a' being a prime ideal satisfying a $ a' (and thus a' is of greater height). The smooth part intersects W" in an nh-dimensional real submanifold of R", and so the proposition follows by decreasing induction on the height of the prime ideal. The main result is Proposition 2.</p><p>-, x,] be an irreducible polynomial. Thus, the ideal a = ( p ) generated by p has height one. Let x E V ( p ) be a smooth point (i.e., a point where V ( p ) is an nl-dimensional submanifold ), and let U be an open neighborhood of x in R ".</p><p>Then for any real polynomial q of n variables, if Proof: Let I be the ideal of all polynomials in R [x,, x2, * * , x,,] that vanish on U n V ( a), i.e., the ideal defined by this subset. The radical of I, denoted h, consists of all polynomials such that some power of the polynomial lies in I. A standard theorem in algebraic geometry, used in the proof of a strong form of the Nullstellensatz, shows that the radical of I is the finite intersection of prime ideals containing I, so that $I = n a2 nn ak, a, prime ideals. Further, p E I C $I, so that p E ai for all i . Suppose that ( p) $ 7ri for every i. Then the height of each ar is at least two, and thus by proposition 1, the variety of each V (a,) consists of submanifolds of dimension n -2 and less. Thus, the union of the V ( a,)'~ cannot contain an n -1dimensional patch, which is a contradiction. So for some i , we must have (p) = a,, and so ( p ) c I c al n --. n ?Tk c T , = ( P ) .</p><formula xml:id="formula_38">q = O on U n V ( a ) ,</formula><p>Recalling that q is in I, we have therefore that q E ( p ) , As a result of the proposition, we have a uniqueness theorem. Suppose that p and q are two polynomials with domain R". Suppose furthermore that A p and A q are irreducible polynomials. The corresponding scale-space functions U , and v2 will also be polynomials (of n + 1 variables) and must be irreducible, for otherwise a factorization of either would induce (at t = 0) a factorization of the Laplacian of the initial polynomial. Suppose that v 1 and v2 share in common an open patch of some zero crossing. Recall that the zero crossings are defined in terms of the boundaries of the regions of negativity and i.e., thatp divides q, as required.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>(a) Although a good representation need not be complete, it should be a right inverse, in that the representation of a reconstruction should be the same as the original representation. (b) Zero-crossing surfaces in scale space, and a zero-crossing curve at a fixed scale (the dashed level). (c) Figure for Theorem 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>then the component C must be bounded, since I v I -+ 0 uniformly as I x I -+ 00.Thus, the supremum (or infemum) of v ( x , t ) is attained at a finite point (xo, t o ) in C. If 1 = 0, then the supremum (or infemum) in C is nonzero, and once again attained at a point (xo, to) of C. Since (xo, t o ) cannot lie on the levelcrossing surface (assuming v is nonconstant), it must either be in the interior of C, or lie on the top surface { t = t2 } within C. In either case, we can place a bounded cylinder within C with (xo, to) in the interior or top, in violation of the maximum principle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2116</head><label></label><figDesc>IEEE TRANSACTIONS ON ACOUSTICS, SPEECH. AND SIGNAL PROCESSING, VOL. 37, NO.<ref type="bibr" target="#b11">12</ref></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>g</head><label></label><figDesc>(x), we set g(x) = g(x) forx E D , and g(x) = 0 elsewhere. Let b ( x ) be the g(x) data blurred to the level T: [see Fig. l(c)]. Finally, for fixed y and 7 define w ( x , t ) = K ( xy , 7 -t ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>r</head><label></label><figDesc>Using the definitions of g, b , and w above, the equation becomes r b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( 4 7 )</head><label>47</label><figDesc>7 ) = K ( xy , 0 ) is a delta function centered at x = y. With g (x) and b (x) defined as before, we have P $D,. Since U ( y, T ) is unknown, the equation gives no useful information when ( y, 7 ) E D,. The missing data could be supplied by analytic continuation of b ( y ) from ( y , 7 ) $ D,, assisted by the fact that the above equation holds with the unknown but (hopefully) small error U ( y , T ) when ( y, 7) E D,. Once b( y ) has been extended in this way, g ( x ) is once again found by deblurring.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>and defineb; to be thedatag, i E D, blurred to level T: Finally, denote the set of pixels on the boundary of Q by a,,l,o,Q =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>) - 2</head><label>2</label><figDesc>To reconstruct data by the above equation, choose a connected component of { (t, k) I U (i, k ) &gt; 0 } (or, respectively, &lt;O). If the component extends to infinity in either coordinate, truncate the domain to become a convenient bounded collection of pixels, and denote the result by Q . We store the sets a,,,,,,Q, d,,,,,,Q, and D as defined above. For pixels (i, k) in8, ,,,,Q (respectively, d(-l,o,Q), we store the information v,,k and U , + I , k (respectively, v,,k and v, -I , k ) . For pixels ( i , k ) in a,,,,,Q, we store the data u,,k + I and for a(o, -I , Q pixels we store ut, k. Using the above equation, we choose a T and reconstruct the blurred data b, . To reconstruct the data g, for i E D, it suffices to deblur the b, data by solving for g, in the linear equations defining b,. In fact, the system is overdetermined, although still poorly conditioned, especially if 1 D I or T is large.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>now reformulated as the search for a pair of functions ( U , a) minimizing subject to sgn [U(., 03 = sgn [vob, I ) ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Borders are handled by extending the original data f. The data&amp;, . . . , fN ~ I are repeated alternately from left to right, and then from right to left, to form a doubly infinite signal that is periodic with period 2 N .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>needed for k = 0 ( JE/dvi, and J E / ~U ~, ~)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Triangular, piecewise-linear, finite elements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. A scanline taken from a digitized natural image. There are 128 pixels in the scanline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The sign of the scale-space filtered data shown in Fig. 4. The data are nearly identical to those of Fig. 5, showing that the signals in Figs. 4 and 6 have virtually the same zero crossings for their Laplacian-of-Gaussian filtered data at all scales of resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. An original image, of a Japanese pagoda, and a reconstruction from the zero crossings in scale space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Another reconstruction of the pagoda image of Fig. 8(a), using a different initialization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig</head><label></label><figDesc>Fig. IO. Levels 0, 3, 6, 9, 12, 15, and 18 of the sign of the Laplacian-of-Gaussian "monolith" of (left column) the original image, (middle column) the reconstructed image Fig. 8(b), and (right column) the reconstructed image Fig. 9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. An original image, and the reconstruction using the zero crossings. The original image is relatively free of high-frequency components.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Levels 0, 3, 6, 9, 12, 15, and 18 of the sign of the Laplacian monolith of (left column) the original image, (second column) the reconstructed image of Fig. 11.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 13 .Fig. 15 .Fig. 14 .</head><label>131514</label><figDesc>Fig. 13. A reconstruction of the pagoda image [Fig. 8(a)] using zero crossings and gradient data along the zero crossings. Fig. 15. A reconstruction of the pagoda image [Fig. 8(a)] using zero crossings and approximately 10 percent of the gradient data along the zero crossings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. A reconstruction of the pagoda image [Fig. 8(a)] using zero crossings together with data along zero crossings in the bottom two levels only.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Proposition I :</head><label>I</label><figDesc>Let a C R [ x I , x2, * (0) = no 5 5 a 2 5 . . * 5 ah = a.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Proposition 2 :</head><label>2</label><figDesc>Let p E R [ x I , x2,then p divides q.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>Now, the patch U n V (a) is contained in V (h), by the definition of I . So, U n v (a) c v ( h ) = v ( a l ) U I/ ( 8 2 ) U U I/ ( a k ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, we show a standard test image. The image has significantly smaller</figDesc><table><row><cell>girl. 64</cell><cell>girl-pegu.64</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Marr</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Vision</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Recovering intrinsic scene characteristics from images,&apos;&apos; in Computer Vision Systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Barrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Tenenbaum</surname></persName>
		</author>
		<editor>A. Hanson and E. Riseman</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Non-lineanties in cortical simple cells and the possible detection of zero crossings</title>
		<author>
			<persName><forename type="first">J</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybern</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Edge detection and curve detection for visual scene analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thurston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="562" to="569" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Laplacian pyramid as a compact image code</title>
		<author>
			<persName><forename type="first">P</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">532</biblScope>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scale space filtering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Witkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Int. Joint Conf. Artificial Intell</title>
		<meeting>8th Int. Joint Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="1983">1983</date>
			<biblScope unit="page">1019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The structure of images</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Koenderink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybern</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Receptive fields and the representation of visual information</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Zucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Hummel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Int. Conf. Pattern Recogn</title>
		<meeting>7th Int. Conf. Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="1984-07">July 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Uniqueness of the Gaussian kernel for scale-space filtering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Babaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">0</forename><surname>Duda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="26" to="33" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scaling theorems for zero crossings</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image processing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kovasznay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Joseph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IRE</title>
		<meeting>IRE</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The contrast sensitivity of retinal ganglion cells of the cat</title>
		<author>
			<persName><forename type="first">C</forename><surname>Enroth-Cugell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Robson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J . Physiol</title>
		<imprint>
			<biblScope unit="page">517</biblScope>
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Matching coding to scenes to enahnce efficiency</title>
		<author>
			<persName><forename type="first">S</forename><surname>Laughlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Physical and Biological Processing of Images</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Braddick</surname></persName>
		</editor>
		<editor>
			<persName><surname>Sleigh</surname></persName>
		</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The scale-space formulation of pyramid data structures</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hummel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Computer Vision, L. Uhr</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="107" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Theory of edge detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hildreth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Roy. Soc. London (B)</title>
		<imprint>
			<biblScope unit="page">187</biblScope>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
	<note>see appendix</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Six formal properties of anisotropic visual fibers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Daugman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="882" to="887" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stereopsis in the absence of zero-crossings in the bandpass filtered images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Prazdny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memo, Lab. AI Res., Fairchild, Schlumberger</title>
		<imprint>
			<biblScope unit="volume">4001</biblScope>
			<pubPlace>Palo Alto, CA 94304</pubPlace>
		</imprint>
	</monogr>
	<note>Miranda Ave.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A computational theory of human stereo vision</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Roy. Soc. London (B)</title>
		<imprint>
			<biblScope unit="page">301</biblScope>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Zero-crossings analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kedem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">20742. Nov. 1985</date>
			<pubPlace>College Park, MD</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dep. Math., Univ. Maryland</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. TR85-38</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Representing signals by their toppoints in scale space</title>
		<author>
			<persName><forename type="first">P</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skelboe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Int. Conf. Pattern Recogn</title>
		<meeting>8th Int. Conf. Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="215" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Bers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schechter</surname></persName>
		</author>
		<title level="m">Partial Differential Equations</title>
		<meeting><address><addrLine>Providence, RI</addrLine></address></meeting>
		<imprint>
			<publisher>American Mathematical Society</publisher>
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Maximum Principles in Differential Equations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>John</surname></persName>
		</author>
		<title level="m">Partial Differential Equations</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Zero crossings and the heat equation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Hummel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Gidas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NYU Robotics Rep</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="1984-03">Mar. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fingerprints theorems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Amer. Assoc. Artifcial Intell</title>
		<imprint>
			<biblScope unit="page" from="362" to="365" />
			<date type="published" when="1261">1984. 1261</date>
		</imprint>
	</monogr>
	<note>Rudin, Functional Analysis</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deblurring Gaussian blur</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Hummel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kimia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis., Graphics, Image Processing</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="66" to="80" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Information in the zero-crossings of band pass signals</title>
		<author>
			<persName><forename type="first">B</forename><surname>Logan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page">510</biblScope>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">San</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Freeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1955">1982. 1978. 1986. 1984. 15-25, 1986. 43. 1955. 1983</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="560" to="570" />
			<pubPlace>New York; New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Signal reconstruction from Fourier transform sign information</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">S</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oppenheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust. , Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="643" to="657" />
			<date type="published" when="1985">1989. 1985</date>
		</imprint>
	</monogr>
	<note>IEEE Trans. Pattern Anal. Machine Intell.</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fingerprints theorems for zero crossings</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="683" to="692" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reconstruction of nonperiodic two-dimensional signals from zero crossings</title>
		<author>
			<persName><forename type="first">S</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oppenheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName><surname>Oppenheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="230" />
			<date type="published" when="1987">1987. 1987</date>
		</imprint>
	</monogr>
	<note>J . Opt. Soc. Amer.</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image reconstruction from zero crossings</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Zeevi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rotem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1269" to="1277" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Solving ill-conditional problems by minimizing equation error</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hummel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moniot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Ist Int. Conf. Comput. Vision</title>
		<meeting>IEEE Ist Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="527" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A network approach to reconstructions from zero-crossings</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Workshop Comput. Vision</title>
		<meeting>IEEE Comput. Soc. Workshop Comput. Vision</meeting>
		<imprint>
			<date type="published" when="1986-10">Nov. 1987. Oct. 1986</date>
		</imprint>
		<respStmt>
			<orgName>New York Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
	<note>A variational method for parameter identification</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">An Introduction to Variational Inequalities and Their Applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kinderlehrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stampacchia</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Regularization of inverse visual problems involving discontinuities</title>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Conjugate Direction Methods in Optimization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Hestenes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">An Analysis of the Finite Element Method</title>
		<author>
			<persName><forename type="first">G</forename><surname>Strang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fix</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973">1973</date>
			<publisher>Prentice-Hall</publisher>
			<biblScope unit="page" from="8" to="13" />
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
