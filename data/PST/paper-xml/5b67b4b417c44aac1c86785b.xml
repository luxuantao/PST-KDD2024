<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MetaAnchor: Learning to Detect Objects with Customized Anchors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tong</forename><surname>Yang</surname></persName>
							<email>yangtong@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="department">Megvii Inc (Face++)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
							<email>zhangxiangyu@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="department">Megvii Inc (Face++)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zeming</forename><surname>Li</surname></persName>
							<email>lizeming@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="department">Megvii Inc (Face++)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenqiang</forename><surname>Zhang</surname></persName>
							<email>wqzhang@fudan.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
							<email>sunjian@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="department">Megvii Inc (Face++)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MetaAnchor: Learning to Detect Objects with Customized Anchors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FF902BD2886AA64717298F0A6B1783B6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel and flexible anchor mechanism named MetaAnchor for object detection frameworks. Unlike many previous detectors model anchors via a predefined manner, in MetaAnchor anchor functions could be dynamically generated from the arbitrary customized prior boxes. Taking advantage of weight prediction, MetaAnchor is able to work with most of the anchor-based object detection systems such as RetinaNet. Compared with the predefined anchor scheme, we empirically find that MetaAnchor is more robust to anchor settings and bounding box distributions; in addition, it also shows the potential on transfer tasks. Our experiment on COCO detection task shows that MetaAnchor consistently outperforms the counterparts in various scenarios.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The last few years have seen the success of deep neural networks in object detection task <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b1">2]</ref>. In practice, object detection often requires to generate a set of bounding boxes along with their classification labels associated with each object in the given image. However, it is nontrivial for convolutional neural networks (CNNs) to directly predict an orderless set of arbitrary cardinality <ref type="foot" target="#foot_0">1</ref> . One widely-used workaround is to introduce anchor, which employs the thought of divide-and-conquer and has been successfully demonstrated in the state-of-the-art detection frameworks <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b1">2]</ref>. In short, anchor method suggests dividing the box space (including position, size, class, etc.) into discrete bins (not necessarily disjoint) and generating each object box via the anchor function defined in the corresponding bin. Denote x as the feature extracted from the input image, then anchor function for i-th bin could be formulated as follows:</p><formula xml:id="formula_0">F bi (x; θ i ) = F cls bi (x; θ cls i ), F reg bi (x; θ reg i )<label>(1)</label></formula><p>where b i ∈ B is the prior (also named anchor box in <ref type="bibr" target="#b32">[32]</ref>), which describes the common properties of object boxes associated with i-th bin (e.g. averaged position/size and classification label); while F cls bi (•) discriminates whether there exists an object box associated with the i-th bin, and F reg bi (•) regresses the relative location of the object box (if any) to the prior b i ; θ i represents the parameters for the anchor function.</p><p>To model anchors with deep neural networks, one straight-forward strategy is via enumeration, which is adopted by most of the previous work <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22]</ref>. First, a number of predefined priors (or anchor boxes) B is chosen by handcraft <ref type="bibr" target="#b32">[32]</ref> or statistical methods like clustering <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b31">31]</ref>. Then for each b i ∈ B the anchor function F bi is usually implemented by one or a few neural network layers respectively. Weights for different anchor functions are independent or partially shared. Obviously in this framework anchor strategies (i.e. anchor box choices and the definition of corresponding anchor functions) are fixed in both training and inference. In addition, the number of available anchors is limited by the predefined B.</p><p>In this paper, we propose a flexible alternative to model anchors: instead of enumerating every possible bounding box prior b i and modeling the corresponding anchor functions respectively, in our framework anchor functions are dynamically generated from b i . It is done by introducing a novel MetaAnchor module which is defined as follows:</p><formula xml:id="formula_1">F bi = G (b i ; w)<label>(2)</label></formula><p>where G(•) is called anchor function generator which maps any bounding box prior b i to the corresponding anchor function F bi ; and w represents the parameters. Note that in MetaAnchor the prior set B is not necessarily predefined; instead, it works as a customized manner -during inference, users could specify any anchor boxes, generate the corresponding anchor functions and use the latter to predict object boxes. In Sec. 3, we present that with weight prediction mechanism <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref> anchor function generator could be elegantly implemented and embedded into existing object detection frameworks for joint optimization.</p><p>In conclusion, compared with traditional predefined anchor strategies, we find our proposed MetaAnchor has the following potential benefits (detailed experiments are present in Sec. 4):</p><p>• MetaAnchor is more robust to anchor settings and bounding box distributions. In traditional approaches, the predefined anchor box set B often needs careful design -too few anchors may be insufficient to cover rare boxes, or result in coarse predictions; however, more anchors usually imply more parameters, which may suffer from overfitting. In addition, many traditional strategies use independent weights to model different anchor functions, so it is very likely for the anchors associated with few ground truth object boxes in training to produce poor results. In contrast, for MetaAnchor anchor boxes of any shape could be randomly sampled during training so as to cover different kinds of object boxes, meanwhile, the number of parameters keeps constant. Furthermore, according to Equ. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Anchor methodology in object detection. Anchors (maybe called with other names, e.g. "default boxes" in <ref type="bibr" target="#b25">[25]</ref>, "priors" in <ref type="bibr" target="#b39">[39]</ref> or "grid cells" in <ref type="bibr" target="#b30">[30]</ref>) are employed in most of the state-of-the-art detection systems <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b14">15]</ref>. The essential of anchors includes position, size, class label or others. Currently most of the detectors model anchors via enumeration, i.e. predefining a number of anchor boxes with all kinds of positions, sizes and class labels, which leads to the following issues. First, anchor boxes need careful design, e.g. via clustering <ref type="bibr" target="#b31">[31]</ref>, which is especially critical on specific detection tasks such as anchor-based face <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b43">43]</ref> and pedestrian <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b26">26]</ref> detections. Specially, some papers suggest multi-scale anchors <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> to handle different sizes of objects. Second, predefined anchor functions may cause too many parameters. A lot of work addresses the issue by weight sharing. For example, in contrast to earlier work like <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b30">30]</ref>, detectors like <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b31">31]</ref> and their follow-ups <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23]</ref> employ translationinvariant anchors produced by fully-convolutional network, which could share parameters across different positions. Two-stage frameworks such as <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b1">2]</ref> share weights across various classes. And <ref type="bibr" target="#b22">[23]</ref> shares weights for multiple detection heads. In comparison, our approach is free of the issues, as anchor functions are customized and generated dynamically.</p><p>Weight prediction. Weight prediction means a mechanism in neural networks where weights are predicted by another structure rather than directly learned, which is mainly used in the fields of learning to learn <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b42">42]</ref>, few/zero-shot learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b42">42]</ref> and transfer learning <ref type="bibr" target="#b27">[27]</ref>. For object detection there are a few related works, for example, <ref type="bibr" target="#b14">[15]</ref> proposes to predict mask weights from box weights. There are mainly two differences from ours: first, in our MetaAnchor the purpose of weight prediction is to generate anchor functions, while in <ref type="bibr" target="#b14">[15]</ref> it is used for domain adaption (from object box to segmentation mask); second, in our work weights are generated almost "from scratch", while in <ref type="bibr" target="#b14">[15]</ref> the source is the learned box weights.</p><p>3 Approach </p><formula xml:id="formula_2">F bi (x; θ i ) = F(x; θ bi )<label>(3)</label></formula><p>Then, since each anchor function is distinguished only by its parameters θ bi , anchor function generator could be formulated to predict θ bi as follows:</p><formula xml:id="formula_3">θ bi = G(b i ; w) = θ * + R(b i ; w)<label>(4)</label></formula><p>where θ * stands for the shared parameters (independent to b i and also learnable), and the residual term R(b i , w) depends on anchor box b i .</p><p>In the paper we implement R(•) with a simple two-layer network:</p><formula xml:id="formula_4">R(b i , w) = W 2 σ (W 1 b i )<label>(5)</label></formula><p>Here, W 1 and W 2 are the learnable parameters and σ(•) is the activation function (i.e. ReLU in our work). Denote the number of hidden neurons by m. In practice m is usually much smaller than the dimension of θ bi , which causes the weights predicted by R(•) lie in a significantly low-rank subspace.</p><p>That is why we formulate G(•) as a residual form in Equ 4 rather than directly use R(•). We also survey more complex designs for G(•), however, which results in comparable benchmarking results.</p><p>In addition, we introduce a data-dependent variant of anchor function generator, which takes the input feature x into the formulation:</p><formula xml:id="formula_5">θ bi = G(b i ; x, w) = θ * + W 2 σ (W 11 b i + W 12 r(x))<label>(6)</label></formula><p>where r(•) is used to reduce the dimension of the feature x; we empirically find that for convolutional feature x, using global averaged pooling <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b38">38]</ref> operation for r(•) usually produces good results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Architecture Details</head><p>Theoretically MetaAnchor could work with most of the existing anchor-based object detection frameworks <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b1">2]</ref>. Among them, for the two-stage detectors <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19]</ref> anchors are usually used to model "objectness" and generate box proposals, while fine results are predicted by RCNN-like modules <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8]</ref> in the second stage. We try to use MetaAnchor in these frameworks and observe some improvements on the box proposals (e.g. improved recalls), however, it seems no use to the final predictions, whose quality we believe is mainly determined by the second stage. Therefore, in the paper we mainly study the case of single-stage detectors <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>We choose the state-of-the-art single-stage detector RetinaNet <ref type="bibr" target="#b22">[23]</ref> to apply MetaAnchor for instance. Note that our methodology is also applicable to other single-stage frameworks such as <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b35">35]</ref>.  each detection head. Referring to the settings in <ref type="bibr" target="#b22">[23]</ref>, anchor functions are implemented by a 3 × 3 convolutional layer; and for each detection head, there are 3 × 3 × 80 types of anchor boxes (3 scales, 3 aspect ratios and 80 classes) are predefined. Thus for each anchor function, there should be 720 filters for the classification term and 36 filters for the regression term (3 × 3 × 4, as regression term is class-agnostic).</p><p>In order to apply MetaAnchor, we need to redesign the original anchor functions so that their parameters are generated from the customized anchor box b i . First of all, we consider how to encode b i . According to the definition in Sec. 1, b i should be a vector which includes the information such as position, size and class label. In RetinaNet, thanks to the fully-convolutional structure, position could be naturally encoded by the coordinate of feature maps thus no need to be involved in b i . As for class label, there are two alternatives: A) directly encode it in b i , or B) let G(•) predict weights for each class respectively. We empirically find that Option B is easier to optimize and usually results in better performance than Option A. So, in our experiment b i is mainly related to anchor size. Motivated by the bounding box encoding method introduced in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b32">32]</ref>, b i is represented as follows:</p><formula xml:id="formula_6">b i = log ah i AH , log aw i AW<label>(7)</label></formula><p>where ah i and aw i are the height and width of the corresponding anchor box; and (AH, AW ) is the size of "standard anchor box", which is used as a normalization term. We also survey a few other alternatives, for example, using the scale and aspect ratio to represent the size of anchor boxes, which results in comparable results with that of Equ. 7. It is also worth noting that in RetinaNet <ref type="bibr" target="#b22">[23]</ref> corresponding layers in all levels of detection heads share the same weights, even including the last layers which stand for anchor functions. However, the definitions of anchors differ from layer to layer: for example, in l-th level suppose an anchor function associated to the anchor box of size (ah, aw); while in (l + 1)-th level (with 50% smaller resolution), the same anchor function should detect with 2x larger anchor box, i.e. (2ah, 2aw). So, in order to keep consistent with the original design, in MetaAnchor we use the same anchor generator function G(•, w cls ) and G(•, w reg ) for each level of detection head; while the "standard boxes" (AH, AW ) in Equ. 7 are different between levels: suppose the standard box size in l-th level is (AH l , AW l ), then for (l + 1)-th level we set (AH l+1 , AW l+1 ) = (2AH l , 2AW l ). In our experiment, the size of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>In this section we mainly evaluate our proposed MetaAnchor on COCO object detection task <ref type="bibr" target="#b24">[24]</ref>. The basic detection framework is RetinaNet <ref type="bibr" target="#b22">[23]</ref> as introduced in 3.2, whose backbone feature extractor we use is ResNet-50 <ref type="bibr" target="#b12">[13]</ref> pretrained on ImageNet classification dataset <ref type="bibr" target="#b34">[34]</ref>. For MetaAnchor, we use the data-independent variant of anchor function generator (Equ. 4) unless specially mentioned.</p><p>MetaAnchor subnets are jointly optimized with the backbone detector during training. We do not use Batch Normalization <ref type="bibr" target="#b16">[17]</ref> in MetaAnchor.</p><p>Dataset. Following the common practice <ref type="bibr" target="#b22">[23]</ref> in COCO detection task, for training we use two different dataset splits: COCO-all and COCO-mini; while for test, all results are evaluated on the minival set which contains 5000 images. COCO-all includes all the images in the original training and validation sets excluding minival images, while COCO-mini is a subset of around 20000 images.</p><p>Results are mainly evaluated with COCO standard metrics such as mmAP.</p><p>Training and evaluation configurations. For fair comparison, we follow most of the settings in <ref type="bibr" target="#b22">[23]</ref> (image size, learning rate, etc.) for all the experiments, except for a few differences as follows.</p><p>In <ref type="bibr" target="#b22">[23]</ref>, 3 × 3 anchor boxes (i.e. 3 scales and 3 aspect ratios) are predefined for each level of detection head. In the paper, more anchor boxes are employed in some experiments. Table <ref type="table" target="#tab_2">1</ref> lists the anchor box configurations for feature level P 3 , where the 3 × 3 case is identical to that in <ref type="bibr" target="#b22">[23]</ref>. Settings for other feature levels could also be derived (see Sec. 3.2). As for MetaAnchor, since predefined anchors are not needed, we suggest to use the strategy as follows. In training, first we select a sort of anchor box configuration from Table <ref type="table" target="#tab_2">1</ref> (e.g. 5 × 5), then generate 25 b i s according to Equ. 7; for each iteration, we randomly augment each b i within ±0.5, calculating the corresponding ground truth and use them to optimize. We call the methodology "training with 5 × 5 anchors". While in test, b i s are also set by a certain anchor box configuration without augmentation (not necessary the same as used in training). We argue that with that training/inference scheme, it is possible to make direct comparisons between MetaAnchor and the counterpart baselines.</p><p>In the following subsections, first we study the performances of MetaAnchor by a series of controlled experiments on COCO-mini. Then we report the fully-equipped results on COCO-full dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Comparison with RetinaNet baselines</head><p>Table <ref type="table" target="#tab_3">2</ref> compares the performances of MetaAnchor and RetinaNet baseline on COCO-mini dataset.</p><p>Here we use the same anchor box settings for training and test. In the column "Threshold" t 1 /t 2 means the intersection-over-union (IoU) thresholds for positive/negative anchor boxes respectively in training (the detailed definition are introduced in <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b22">23]</ref>).</p><p>To analyze, first we compare the rows with the threshold of 0.5/0.4. It is clear that MetaAnchor outperforms the counterpart baselines on each of anchor configurations and evaluation metrics, for instance, 0.2 ∼ 0.8% increase for mmAP and 0.8 ∼ 1.5% for AP 50 . We suppose the improvements may come from two aspects: first, in MetaAnchor the sizes of anchor boxes could be augmented and make the anchor functions to generate a wider range of predictions, which may enhance the model capability (especially important for the case with smaller number of anchors, e.g. 3 × 3); second, rather than predefined anchor functions with independent parameters, MetaAnchor allows all the training boxes to contribute to the shared generators, which seems beneficial to the robustness over the different configurations or object box distributions.</p><p>For further investigating, we try using stricter IoU threshold (0.6/0.5) for training to encourage more precise anchor box association, however, statistically there are fewer chances for each anchor to be assigned with a positive ground truth. Results are also presented in Table <ref type="table" target="#tab_3">2</ref>. We find results of all the baseline models suffer from significantly drops especially on AP 50 , which implies the degradation of anchor functions; furthermore, simply increasing the number of anchors works little on the performance. For MetaAnchor, in contrast, 3 out of 4 configurations are less affected (for the case of 9 × 9 anchors even 0.3% improved mmAP are obtained). The only exception is the 3 × 3 case; however, according to Table <ref type="table">3</ref> we believe the degradation is mainly because of too few anchor boxes for inference rather than poor training. So, the comparison supports our hypothesis: MetaAnchor helps to use training samples in a more efficient and robust way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Comparison of various anchor configurations in inference</head><p>Unlike the traditional fixed or predefined anchor strategy, one of the major benefits of MetaAnchor is able to use flexible anchor scheme during inference time. Table <ref type="table">3</ref> compares a variety of anchor box configurations (refer to Table <ref type="table" target="#tab_2">1</ref>; note that the normalization coefficient (AH, AW ) should be consistent with what used in training) for inference along with their scores on COCO-mini. For each experiment IoU threshold in training is set to 0.6/0.5. From the results we find that more anchor boxes in inference usually produce higher performances, for instance, results of 9 × 9 inference anchors are 0.7 ∼ 1.1% better than that of 3 × 3 for a variety of training configurations.</p><p>Table <ref type="table">3</ref> also implies that the improvements are quickly saturated with the increase of anchor boxes, e.g. ≥ 7 × 7 anchors only bring minor improvements, which is also observed in Table <ref type="table" target="#tab_3">2</ref>. We revisit the anchor configurations in Table <ref type="table" target="#tab_2">1</ref> and find 7 × 7 and 9 × 9 cases tend to involve too "dense" anchor boxes, thus predicting highly overlapped results which might contribute little to the final performance.</p><p>Inspired by the phenomenon, we come up with an inference approach via greedy search: each step we randomly select one anchor box b i , generate the predictions and evaluate the combined results with the previous step (performed on a subset of training data); if the score improves, we update the current predictions with the combined results, otherwise discard the predictions in the current step. Final anchor configuration is obtained after a few steps. Improved results are shown in the last column (named "search") of Table <ref type="table">3</ref>. Though domain adaption or transfer learning <ref type="bibr" target="#b29">[29]</ref> is out of the design purpose of MetaAnchor, recently the technique of weight prediction <ref type="bibr" target="#b9">[10]</ref>, which is also employed in the paper, has been successfully applied in those tasks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14]</ref>. So, for MetaAnchor it is interesting to evaluate whether it is able to bridge the distribution gap between two dataset. More specifically, what about the performance if the detection model is trained with another dataset which has the same class labels but different distributions of object box sizes?</p><p>We perform the experiment on COCO-mini, in which we "drop" some boxes in the training set. However, it seems nontrivial to directly erase the objects in image; instead, during training, once we use an ground truth box which falls in a certain range (in our experiment the range is {(h, w)|50 &lt; √ hw &lt; 100, -1 &lt; log w h &lt; 1}, around 1/6 of the whole boxes), we manually assign the corresponding loss to 0. As for test, we use all the data in the validation set. Therefore, the distributions of the boxes we used in training and test are very different. Table <ref type="table" target="#tab_4">4</ref> shows the evaluation results. Obviously after some ground truth boxes are erased, all the scores drop significantly; however, compared with the RetinaNet baseline, MetaAnchor suffers from smaller degradations and generates much better predictions, which shows the potential on the transfer tasks.</p><p>In addition, we train models only with COCO-full dataset and evaluate the transfer performace on VOC2007 test set <ref type="bibr" target="#b5">[6]</ref>. We use two models: Baseline(RetianNet) and MetaAnchor, which achieve the best performace on COCO-full dataset with different architectures. In this experiment, we achieve 83.3% mAP on VOC 2007 test set, with 0.8% improvement in mAP compared with Baseline and 0.2% better than MetaAnchor, as shown in Table <ref type="table">5</ref>. Therefore, MetaAnchor shows a better tansfer ability than the RetinaNet baseline on this task. Note that the result is evaluated without sofa class, because there is no sofa annotation in COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Data-independent vs. data-dependent anchor function generators</head><p>In Sec. 3.2 we introduce two variants of anchor function generators: data-independent (Equ. 4) and data-dependent (Equ. 6). In the above subsections we mainly evaluate the data-independent ones. Table <ref type="table" target="#tab_5">6</ref> compares the performance of the two alternatives. For simplicity, we use the same training and test anchor configurations; the IoU threshold is 0.6/0.5. Results shows that in most cases data-dependent variant is slight better, however, the difference is small. We also report the scores after anchor configuration search (described in Sec. 4.1.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on COCO Object Detection</head><p>Finally, we compare our fully-equipped MetaAnchor models with RetinaNet <ref type="bibr" target="#b22">[23]</ref> baselines on COCOfull dataset (also called trainval35k in <ref type="bibr" target="#b22">[23]</ref>). As mentioned at the begin of Sec. 4, we follow the same evaluation protocol as <ref type="bibr" target="#b22">[23]</ref>. The input resolution is 600× in both training and test. The backbone feature extractor is ResNet-50 <ref type="bibr" target="#b12">[13]</ref>. Performances are benchmarked with COCO standard mmAP in the minival dataset. We also evaluate our method on PASCAL VOC 2007 and get preliminary resluts that MetaAnchor achieves ∼ 0.3% more mAP than RetinaNet baseline (80.3-&gt;80.6% mAP@0.5). The gain is less significant compared with that on COCO, as we find the distribution of boxes on PASCAL VOC is much simpler than COCO.</p><p>To validate our method further, we implement MetaAnchor on YOLOv2 <ref type="bibr" target="#b31">[31]</ref>, which also use a two-layer network to predict detector parameters. For YOLOv2 baseline, we use anchors showed on open source project<ref type="foot" target="#foot_4">4</ref> to detect objects. In MetaAnchor, the "standard box" (AH, AW ) is (4.18, 4.69). For training, we follow the strategy used in <ref type="bibr" target="#b31">[31]</ref> and use the COCO-full dataset. For the results, we report mmAP and mAP@0.5 on COCO minival. Table <ref type="table">7</ref> illustrates the results. Obviously, MetaAnchor is better than YOLOv2 baseline and boosts the performace with greedy search method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a novel and flexible anchor mechanism named MetaAnchor for object detection frameworks, in which anchor functions could be dynamically generated from the arbitrary customized prior boxes. Thanks to weight prediction, MetaAnchor is able to work with most of the anchor-based object detection systems such as RetinaNet. Compared with the predefined anchor scheme, we empirically find that MetaAnchor is more robust to anchor settings and bounding box distributions; in addition, it also shows the potential on transfer tasks. Our experiment on COCO detection task shows that MetaAnchor consistently outperforms the counterparts in various scenarios. (2 0 , 1/3), (2 0 , 1/2), (2 0 , 1), (2 0 , 2) and (2 0 , 3) respectively. Note that for each picture we aggregate the predictions of all the 5 levels of detection heads, so the differences of boxes mainly lie in aspect ratios. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig 1(a)  gives the overview of RetinaNet. In short, 5 levels of features {P l |l ∈ {3, 4, 5, 6, 7}} are extracted from a "U-shaped" backbone network, where P 3 stands for the finest feature map (i.e. with largest resolution) and P 7 is the coarsest. For each level of feature, a subnet named "detection head" in Fig 1 is attached to generate detection results. Anchor functions are defined at the tail of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration to applying MetaAnchor on RetinaNet [23]. (a) RetinaNet overview. (b) Detection heads in RetinaNet equipped with MetaAnchor. F cls (•) and F reg (•) compose the anchor function (defined in Equ 1), which are implemented by a convolutional layer respectively here. G(•, w cls ) and G(•, w reg ) are anchor function generators defined in Equ 4 (or Equ 6). b i is the customized box prior (or called anchor box); and "cls" and "reg" represent the prediction results associated to b i .</figDesc><graphic coords="4,113.79,72.91,169.52,104.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig 1 (</head><label>1</label><figDesc>Fig 1(b)  illustrates the usage of MetaAnchor in each detection head of RetinaNet. In the original design<ref type="bibr" target="#b22">[23]</ref>, the classification and box regression parts of anchor functions are attached to separated feature maps (x cls and x reg ) respectively; so in MetaAnchor, we also use two independent anchor function generators G(•, w cls ) and G(•, w reg ) to predict their weights respectively. The design of G(•) follows Equ. 4 (data-independent variant) or Equ. 6 (data-dependent variant), in which the number of hidden neurons m is set to 128. In addition, recall that in MetaAnchor anchor functions are dynamically derived from b i rather than predefined by enumeration; so, the number of filters for F cls (•) reduces to 80 (80 classes, for example) and 4 for F reg (•).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Detection results at a variety of customized anchor boxes. From (a) to (e) the anchor box sizes (scale, ratio) are: (2 0 , 1/3), (2 0 , 1/2), (2 0 , 1), (2 0 , 2) and (2 0 , 3) respectively. Note that for each picture we aggregate the predictions of all the 5 levels of detection heads, so the differences of boxes mainly lie in aspect ratios.</figDesc><graphic coords="9,267.65,231.89,76.71,51.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>(see Equ. 2), which maps b i to the corresponding anchor function F bi , plays a key role in the framework. In order to model G(•) with neural work, inspired by<ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b9">10]</ref>, first we assume that for different b i anchor functions F bi share the same formulation F(•) but have different parameters, which means:</figDesc><table><row><cell>3.1 Anchor Function Generator</cell></row><row><cell>In MetaAnchor framework, anchor function is dynamically generated from the customized box prior</cell></row><row><cell>(or anchor box) b i rather than fixed function associated with predefined anchor box. So, anchor</cell></row><row><cell>function generator G(•)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">: Anchor box configurations</cell><cell></cell></row><row><cell># of Anchors</cell><cell>Scales 2</cell><cell>Aspect Ratios</cell><cell>(AH, AW )</cell></row><row><cell>3 × 3</cell><cell>{2 k/3 |k &lt; 3}</cell><cell>{1/2, 1, 2}</cell><cell>(44, 44)</cell></row><row><cell>5 × 5</cell><cell>{2 k/5 |k &lt; 5}</cell><cell>{1/3, 1/2, 1, 2, 3}</cell><cell>(45, 47)</cell></row><row><cell>7 × 7</cell><cell>{2 k/7 |k &lt; 7}</cell><cell>{1/4, 1/3, 1/2, 1, 2, 3, 4}</cell><cell>(48, 50)</cell></row><row><cell>9 × 9</cell><cell cols="2">{2 k/9 |k &lt; 9} {1/5, 1/4, 1/3, 1/2, 1, 2, 3, 4, 5}</cell><cell>(53, 53)</cell></row><row><cell cols="4">standard box in the lowest level (i.e. P 3 , which has the largest resolution) is set to the average of all</cell></row><row><cell cols="3">the anchor box sizes (shown in the last column in Table 1).</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of RetinaNets with/without MetaAnchor.</figDesc><table><row><cell cols="2">Threshold # of Anchors</cell><cell cols="3">Baseline (%)</cell><cell cols="2">MetaAnchor (%)</cell></row><row><cell></cell><cell></cell><cell cols="5">mmAP AP 50 AP 75 mmAP AP 50 AP 75</cell></row><row><cell>0.5/0.4</cell><cell>3 × 3</cell><cell>26.5</cell><cell cols="2">43.1 27.6</cell><cell>26.9</cell><cell>44.2 28.2</cell></row><row><cell>0.5/0.4</cell><cell>5 × 5</cell><cell>26.9</cell><cell cols="2">43.7 28.1</cell><cell>27.1</cell><cell>44.5 28.1</cell></row><row><cell>0.5/0.4</cell><cell>7 × 7</cell><cell>26.4</cell><cell cols="2">43.0 27.7</cell><cell>27.2</cell><cell>44.4 28.5</cell></row><row><cell>0.5/0.4</cell><cell>9 × 9</cell><cell>26.3</cell><cell cols="2">42.8 27.5</cell><cell>27.1</cell><cell>44.3 28.2</cell></row><row><cell>0.6/0.5</cell><cell>3 × 3</cell><cell>25.7</cell><cell cols="2">41.1 27.3</cell><cell>26.0</cell><cell>42.0 27.2</cell></row><row><cell>0.6/0.5</cell><cell>5 × 5</cell><cell>26.1</cell><cell cols="2">41.4 27.8</cell><cell>27.3</cell><cell>44.2 28.8</cell></row><row><cell>0.6/0.5</cell><cell>7 × 7</cell><cell>26.2</cell><cell cols="2">41.3 27.9</cell><cell>27.0</cell><cell>43.1 28.3</cell></row><row><cell>0.6/0.5</cell><cell>9 × 9</cell><cell>26.1</cell><cell cols="2">41.0 27.9</cell><cell>27.4</cell><cell>43.7 29.2</cell></row><row><cell cols="7">Table 3: Comparison of various anchors in inference (mmAP, %)</cell></row><row><cell></cell><cell>Training</cell><cell></cell><cell></cell><cell>Inference</cell><cell></cell></row><row><cell></cell><cell cols="6"># of Anchors 3 × 3 5 × 5 7 × 7 9 × 9 search</cell></row><row><cell></cell><cell>3 × 3</cell><cell>26.0</cell><cell>26.6</cell><cell>26.8</cell><cell>26.7</cell><cell>27.0</cell></row><row><cell></cell><cell>5 × 5</cell><cell>26.7</cell><cell>27.3</cell><cell>27.5</cell><cell>27.5</cell><cell>27.7</cell></row><row><cell></cell><cell>7 × 7</cell><cell>26.1</cell><cell>26.9</cell><cell>27.0</cell><cell>27.1</cell><cell>27.3</cell></row><row><cell></cell><cell>9 × 9</cell><cell>26.3</cell><cell>27.2</cell><cell>27.4</cell><cell>27.4</cell><cell>27.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison in the scenarios of different training/test distributions (mmAP, %) # of Anchors Baseline (all) MetaAnchor (all) Baseline (drop) MetaAnchor (drop)</figDesc><table><row><cell>3 × 3</cell><cell>26.5</cell><cell>26.9</cell><cell>21.2</cell><cell>22.2</cell></row><row><cell>5 × 5</cell><cell>26.9</cell><cell>27.1</cell><cell>20.8</cell><cell>23.0</cell></row><row><cell>7 × 7</cell><cell>26.4</cell><cell>27.2</cell><cell>21.8</cell><cell>22.8</cell></row><row><cell>9 × 9</cell><cell>26.3</cell><cell>27.1</cell><cell>20.8</cell><cell>22.8</cell></row><row><cell cols="5">Table 5: Transfer evaluation on VOC 2007 test set from COCO-full dataset</cell></row><row><cell></cell><cell>Method</cell><cell cols="3">Baseline MetaAnchor Search</cell></row><row><cell></cell><cell>mAP @0.5(%)</cell><cell>82.5</cell><cell>83.1</cell><cell>83.3</cell></row><row><cell cols="5">4.1.3 Cross evaluation between datasets of different distributions</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison of anchor function generators (mmAP, %) # of Anchors Data-independent Data-dependent Table8lists the results. Interestingly, our reimplemented RetinaNet model is 1.8% better than the counterpart reported in<ref type="bibr" target="#b22">[23]</ref>. For better understanding, we further investigate a lot of anchor box configurations (including those in Table1) and retrain the baseline model, the best of which is named "RetinaNet * " and marked with "search" in Table8. In comparison, our MetaAnchor model achieves 37.5% mmAP on COCO minival, which is 1.7% better than the original RetinaNet (our implemented) and 0.6% better than the best searched entry of RetinaNet. Our data-dependent variant (Equ. 6) further boosts the performance by 0.4%. In addition, we argue that for MetaAnchor the configuration for inference could be easily obtained by greedy search introduced in 4.1.2 without retraining. Specifically, the scales and aspects of greedy search anchors are {2 k/5 | -2 &lt; k &lt; 6} and {1/3, 1/t, 1, t, 3|t = 1.1, 1.2, ..., 2} respectively. Fig 2 visualizes some detection results predicted by MetaAnchor. It is clear that the shapes of detected boxes vary according to the customized anchor box b i .</figDesc><table><row><cell>3 × 3</cell><cell>26.0</cell><cell></cell><cell>26.5</cell></row><row><cell>5 × 5</cell><cell>27.3</cell><cell></cell><cell>27.3</cell></row><row><cell>7 × 7</cell><cell>27.0</cell><cell></cell><cell>27.4</cell></row><row><cell>9 × 9</cell><cell>27.4</cell><cell></cell><cell>27.3</cell></row><row><cell>search 3</cell><cell>27.6</cell><cell></cell><cell>28.0</cell></row><row><cell cols="4">Table 7: Results of YOLOv2 on COCO minival (%)</cell></row><row><cell>Method</cell><cell cols="3">Baseline MetaAnchor Search</cell></row><row><cell>mmAP</cell><cell>18.9</cell><cell>21.2</cell><cell>21.2</cell></row><row><cell>mAP @0.5</cell><cell>35.2</cell><cell>39.4</cell><cell>39.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Results on COCO minival</figDesc><table><row><cell>Model</cell><cell>Training</cell><cell>Inference</cell><cell></cell></row><row><cell></cell><cell cols="3"># of Anchors # of Anchors mmAP (%)</cell></row><row><cell>RetinaNet [23]</cell><cell>3 × 3</cell><cell>3 × 3</cell><cell>34.0</cell></row><row><cell>RetinaNet (our impl.)</cell><cell>3 × 3</cell><cell>3 × 3</cell><cell>35.8</cell></row><row><cell>RetinaNet  *  (our impl.)</cell><cell>search</cell><cell>search</cell><cell>36.9</cell></row><row><cell>MetaAnchor (ours)</cell><cell>3 × 3</cell><cell>3 × 3</cell><cell>36.8</cell></row><row><cell>MetaAnchor (ours)</cell><cell>9 × 9</cell><cell>search</cell><cell>37.5</cell></row><row><cell>MetaAnchor (ours, data-dependent)</cell><cell>9 × 9</cell><cell>search</cell><cell>37.9</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>There are a few recent studies on the topic, such as<ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b37">37]</ref>.32nd Conference on Neural Information Processing Systems (NeurIPS</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2018), Montréal, Canada.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>Here we follow the same definition of scale and aspect ratio as in<ref type="bibr" target="#b22">[23]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>Based on the models with 7 × 7 anchor configuration in training.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>https://github.com/pjreddie/darknet</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This work is supported by National Key R&amp;D Program No. 2017YFA0700800, China.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3981" to="3989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="743" to="761" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Write a classifier: Zero-shot learning using purely textual descriptions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2584" to="2591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scalable object detection using deep neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2147" to="2154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010-06">June 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<title level="m">Dssd: Deconvolutional single shot detector</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.08083</idno>
		<title level="m">Fast r-cnn</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Hypernetworks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09106</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lsda: Large scale detection through adaptation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3536" to="3544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10370</idno>
		<title level="m">Learning to segment every thing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Densebox: Unifying landmark localization with end to end object detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04874</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07264</idno>
		<title level="m">Light-head r-cnn: In defense of two-stage object detector</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Detnet: A backbone network for object detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06215</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fssd: Feature fusion single shot multibox detector</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00960</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">T.-Y</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Dollár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02002</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">What can help pedestrian detection?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">From red wine to red tomato: Composition with context</title>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ssh: Single stage headless face detector</title>
		<author>
			<persName><forename type="first">M</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4875" to="4884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">Yolo9000: better, faster, stronger</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deep perm-set net: Learn to predict sets with unknown permutation and cardinality using deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kaskman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">T</forename><surname>Motlagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00613</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dsod: Learning deeply supervised object detectors from scratch</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Beyond trade-off: Accelerate fcn-based face detector with higher accuracy</title>
		<author>
			<persName><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.05197</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2325" to="2333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Cvpr</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1441</idno>
		<title level="m">Scalable, high-quality object detection</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Sface: An efficient network for face detection in large scale variations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06559</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07752</idno>
		<title level="m">Repulsion loss: Detecting pedestrians in a crowd</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to learn: Model regression networks for easy small sample learning</title>
		<author>
			<persName><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="616" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Is faster r-cnn doing well for pedestrian detection?</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="443" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05237</idno>
		<title level="m">S3fd: Single shot scale-invariant face detector</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
