<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatically Discovering User Consumption Intents in Meituan</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yinfeng</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chen</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Huazhou</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Depeng</forename><surname>Jin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Meituan</forename><surname>†chen Gao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yinfeng</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaoyi</forename><surname>Du</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hengliang</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName><surname>De-</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Xiaoyi Du Meituan Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Meituan Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country>China Hengliang</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Luo Meituan Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department">KDD &apos;22</orgName>
								<address>
									<addrLine>August 14-18</addrLine>
									<postCode>2022</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automatically Discovering User Consumption Intents in Meituan</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3534678.3539122</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Consumption Intents Discovery</term>
					<term>Graph Neural Networks</term>
					<term>Selfsupervised Learning</term>
					<term>Disentangled Representation Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Consumption intent, defined as the decision-driven force of consumption behaviors, is crucial for improving the explainability and performance of user-modeling systems, with various downstream applications like recommendation and targeted marketing. However, consumption intent is implicit, and only a few known intents have been explored from the user consumption data in Meituan. Hence, discovering new consumption intents is a crucial but challenging task, which suffers from two critical challenges: 1) how to encode the consumption intent related to multiple aspects of preferences, and 2) how to discover the new intents with only a few known ones. In Meituan, we designed the AutoIntent system, consisting of the disentangled intent encoders and intent discovery decoders, to address the above challenges. Specifically, for the disentangled intent encoders, we construct three groups of dual hypergraphs to capture the high-order relations under the three aspects of preferences and then utilize the designed hypergraph neural networks to extract disentangled intent features. For the intent discovery decoders, we propose to build intent-pair pseudo labels based on the denoised feature similarities to transfer knowledge from known intents to new ones. Extensive evaluations verify that AutoIntent can effectively discover unknown consumption intents. Moreover, experiments also demonstrate that AutoIntent can effectively enhance the downstream recommendation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Most existing models in industrial recommendation engines work as a black-box, without explicit modeling of why a user behavior occurs. To address it, a possible solution is to detect why a user makes that decision, which can be defined as intent. For example, in local life service platforms such as Meituan <ref type="foot" target="#foot_0">1</ref> , an intent of family gathering may leads to a consumption behavior of movie ticket. In other words, a consumption intent can be understood as a group or a typical pattern of user consumption behaviors. Hence, consumption intents can support many downstream applications, such as item recommendation, target-user marketing, supply chain optimization, etc. In the real-world scenarios of Meituan, practitioners can obtain a small fraction of intents based on expert knowledge and user reviews <ref type="bibr" target="#b27">[28]</ref>. A user may have written a short review including "family gathering" after a consumption behavior of movie ticket, and then practitioners can obtain an intent label for that behavior. However, the reviewing data is always sparse, and the behaviors along with reviews only take a tiny percentage of all behaviors, as revealed by industrial practice and academic datasets <ref type="bibr" target="#b24">[25]</ref>.</p><p>In this work, we approach the problem of intent discovery, which aims to assign intent labels (known + new) for the unlabeled consumption behavior data with a tiny fraction of labeled data. There are two critical challenges when solving this problem.</p><p>• Consumption intent is related to multiple aspects of user preferences (how to encode). Consumption is not only determined by users' intrinsic preferences like price and brand but also largely affected by spatial and temporal factors, especially for local life service platforms like Meituan. For example, the intent of family gathering tends to occur at night and not to occur around the office area. • Learning from the unlabeled data along with only a few labeled data (how to discover). Since there is only a few labeled data, the intent discovery problem is faced with the challenge of transferring the knowledge of labeled data into unlabeled data and extracting self-supervision signal from unlabeled data. To address the above challenges, we propose a system named AutoIntent (short for Automatically Consumption Intent Discovery), consisting of two main parts, 1) disentangled intent encoders and 2) intent discovery decoders. Specifically, we first construct three groups of dual hypergraphs to represent the relations among user intrinsic preferences, spatial factor, and temporal factor, respectively. We then deploy a dual-hypergraph neural networks model to extract high-order relations and obtain disentangled intent representations. With the disentangled intent encoders, we address the first challenge. As for the second challenge, we propose to first warm up the intent discovery decoders with intent number estimation and feature fine-tuning. We then propose to build intentpair pseudo labels based on the denoised feature similarities, which can be regarded as a self-supervision signal. Finally, We design a joint-learning framework to discover new intents, which can well transfer knowledge from known intents to unknown ones. With the discovered intents, we further explored the possible applications in Meituan. AutoIntent can serve as an essential component in Meituan's user-modeling system, with various downstream applications like recommendation and targeted marketing. Hence, we deploy AutoIntent in the recommendation engine of the Meituan APP to further verify the effectiveness of the AutoIntent system. The contributions of this work can be summarized as follows.</p><p>• To the best of our knowledge, we take the pioneering step to approach the problem of consumption intent discovery, which is critical for various industrial user personalized services, such as recommendation, targeting marketing, etc. • We develop an AutoIntent system, which consists of two parts:</p><p>1) disentangled intent encoders to learn the disentangled intent representations with and 2) intent discovery decoders to discover the new intents with knowledge transfer. • We evaluate our system on both intent discovery and downstream recommendation tasks. Experimental results on two real-world datasets verify that AutoIntent can effectively discover unknown intents. The downstream evaluations further confirm that Au-toIntent can enhance the recommendation in Meituan APP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM STATEMENT</head><p>The 𝑖-th record of user consumption behavior can be denoted as 𝑥 𝑖 = (𝑢 𝑖 , 𝑙 𝑖 , 𝑡 𝑖 , 𝑐 𝑖 ) , which means user 𝑢 𝑖 bought the item with category Given the user consumption data D = D 𝑙 ∪D 𝑢 , intent discovery aims to automatically cluster the unlabeled data D 𝑢 into a number of intents classes (I 𝑘 ∪ I 𝑢 ) by transferring knowledge from the labeled data D 𝑙 . In other words, we assign each consumption behavior a label from known intents I 𝑘 or unknown ones I 𝑢 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE AUTOINTENT SYSTEM</head><p>Figure <ref type="figure">1</ref> illustrates the intent discovery and its applications in Meituan. In this section, we first introduce our proposed AutoIntent model, and then introduce how to deploy it in the recommendation engine of the Meituan APP. To address the challenges in the introduction, we propose AutoIntent, illustrated in Figure <ref type="figure" target="#fig_2">3</ref> (a), which consists of the following parts: 1) Disentangled Intent Encoders to sufficiently model consumption intent in multiple aspects and 2) Intent Discovery Decoders to discover the new intents by transferring knowledge from known intents to unknown ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Disentangled Intent Encoders</head><p>The user consumption behaviors in life-service platforms, such as Meituan and Yelp, are driven by complex heterogeneous factors. Specifically, user consumption intents are determined by the following three key factors, 1) intrinsic preference<ref type="foot" target="#foot_1">2</ref> : the users preference towards the attributes of the item such as taste, price, brand, etc.; 2) location-aware preference: the user may have location-related consumption behaviors such as consuming quick food when the user is around office building; 3) time-aware preference: the user's consumption behaviors are relevant to the time, such as consuming at Bar at night.  To sufficiently utilize the above three preferences, from the disentangled view (ensuring we can learn three-aspect disentangled representations of users), we decompose the quadruple user consumption data into three types of bipartite relations, i.e. user-location (UL), user-time (UT) and user-category (UC). For example, a user consumption data 𝑥 𝑖 = (𝑢 𝑖 , 𝑙 𝑖 , 𝑡 𝑖 , 𝑐 𝑖 ) contains three types of bipartite relations, (𝑢 𝑖 , 𝑙 𝑖 ) for location preference, (𝑢 𝑖 , 𝑡 𝑖 ) for time preference, and (𝑢 𝑖 , 𝑐 𝑖 ) for category preference. Given that the dual hypergraphs can naturally match the bipartite relations and have the better capability on high-order relations than a normal graph, inspired by the recent advances in dual-hypergraph based bipartite-relation learning <ref type="bibr" target="#b35">[36]</ref>, we construct three groups of dual hypergraphs to model the above bipartite relations. Embedding layer. We create four learnable embedding matrices</p><formula xml:id="formula_0">E 𝑈 ∈ R | U |×𝑑 , E 𝐿 ∈ R | L |×𝑑 , E 𝑇 ∈ R | T |×𝑑 , E 𝐶 ∈ R | C |×𝑑</formula><p>for all the users, locations, time-slots and item categories, where 𝑑 denotes the embedding size. Note that G 𝐿 𝑈 , G 𝑇 𝑈 , G 𝐶 𝑈 share the same nodes but reveals completely different information and semantics (e.g. node embeddings in G 𝐿 𝑈 represent users' location preference). Thus, we further transform the original user embedding matrix E 𝑢 into three disentangled sub-spaces to represent users' preferences on location, time and item category, respectively. The above transformation operation on user embedding matrix can be formulated as E 𝑈 ,𝑠 = E 𝑈 W 𝑠 ∈ R 𝑑×𝑑 , where 𝑠 ∈ {𝐿,𝑇 , 𝐶} and W 𝑠 denote the sub-spaces and transformation matrix in sub-space 𝑠, respectively.</p><p>Joint HyperGraph Convolution (Joint-HGC). As for the dual homogeneous hypergraphs constructed from bipartite relations, an intuitive approach of representation learning is the traditional hypergraph convolution networks <ref type="bibr" target="#b7">[8]</ref>. However, although it can capture the high-order relations among nodes in each hypergraph (intra-graph view), it neglects the relations between dual hypergraphs (inter-graph view), which reveals the important interaction information. For example, in the user-category (UC) relation, the inter-graph propagation can directly fuse the user embedding and category embedding from distinct hypergraphs (G 𝐶 𝑈 , G 𝐶 ) to naturally capture the interaction relations. To address it, we combine both the intra-and inter-graph propagation by the proposed Joint-HGC. Given the dual hypergraphs {G 𝑈 , G 𝑉 }, the aggregation of Joint-HGC in the (ℓ + 1)-th layer can be formulated as follows,</p><formula xml:id="formula_1">X (ℓ+1) 𝑈 = D − 1 2 𝑈 H 𝑈 B −1 𝑈 H ⊤ 𝑈 D − 1 2 𝑈 X (ℓ) 𝑈 + B −1 𝑉 H ⊤ 𝑉 X (ℓ) 𝑉 , X (ℓ+1) 𝑉 = D − 1 2 𝑉 H 𝑉 B −1 𝑉 H ⊤ 𝑉 D − 1 2 𝑉 X (ℓ) 𝑉 intra-graph + B −1 𝑈 H ⊤ 𝑈 X (ℓ) 𝑈 inter-graph ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">H 𝑈 , D 𝑈 , B 𝑈 , X<label>(ℓ)</label></formula><p>𝑈 and</p><formula xml:id="formula_3">H 𝑉 , D 𝑉 , B 𝑉 , X<label>(ℓ)</label></formula><p>𝑉 denote the incidence matrix, node degree matrix, hyperedge degree matrix, and node features of G 𝑈 , G 𝑉 , respectively. Here we remove the nonlinear feature transformations by following <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>Since D, B can be calculated with H, we simplify the formulation Equation ( <ref type="formula" target="#formula_1">2</ref>) by defined a function of Joint-HGC(•) as follows,</p><formula xml:id="formula_4">X (ℓ+1) 𝑈 , X (ℓ+1) 𝑉 = Joint-HGC(H 𝑈 , H 𝑉 , X (ℓ) 𝑈 , X (ℓ) 𝑉 ).</formula><p>(</p><formula xml:id="formula_5">)<label>3</label></formula><p>Then the information aggregation on the three groups of dual hypergraphs can be further formulated as follows, where {H * }, {E (ℓ) * } ( * represents subscript) denote the incidence matrices and embedding features at ℓ-th layer. Note that we initialize the node features at 0-th layer as E (0) * = E * . After propagating through 𝐿 layers, we combine the embeddings learned from each layer with average pooling to obtain final embeddings E * .</p><formula xml:id="formula_6">E (ℓ+1) 𝑈 ,𝐿 , E (ℓ+1) 𝐿 = Joint-HGC(H 𝐿 𝑈 , H 𝐿 , E (ℓ) 𝑈 ,𝐿 , E (ℓ) 𝐿 ), E (ℓ+1) 𝑈 ,𝑇 , E (ℓ+1) 𝑇 = Joint-HGC(H 𝑇 𝑈 , H 𝑇 , E (ℓ) 𝑈 ,𝑇 , E (ℓ) 𝑇 ), E (ℓ+1) 𝑈 ,𝐶 , E (ℓ+1) 𝐶 = Joint-HGC(H 𝐶 𝑈 , H 𝐶 , E (ℓ) 𝑈 ,𝐶 , E (ℓ) 𝐶 ),<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Intent Feature Generation.</head><p>With the learned embeddings, for each user consumption behavior 𝑥 𝑖 = (𝑢 𝑖 , 𝑙 𝑖 , 𝑡 𝑖 , 𝑐 𝑖 ), we can generate the disentangled intent features z 𝐿 𝑖 , z 𝑇 𝑖 , z 𝐶 𝑖 in the location-aware, time-aware and category-aware spaces as follows,</p><formula xml:id="formula_7">z 𝐿 𝑖 = MLP 𝐿 ( [e 𝐿 𝑢 𝑖 , e 𝑙 𝑖 ]), z 𝑇 𝑖 = MLP 𝑇 ( [e 𝑇 𝑢 𝑖 , e 𝑡 𝑖 ]), z 𝐶 𝑖 = MLP 𝐶 ( [e 𝐶 𝑢 𝑖 , e 𝑐 𝑖 ]),<label>(5)</label></formula><p>where [] denote concatenation operation. Here MLP 𝑠 and e 𝑠 𝑢 𝑖 denote the multilayer perceptron and user embedding of 𝑢 𝑖 in each subspace 𝑠 ∈ {𝐿,𝑇 , 𝐶}, respectively, and e 𝑙 𝑖 , e 𝑡 𝑖 , e 𝑐 𝑖 denote the final embedding of 𝑙 𝑖 , 𝑡 𝑖 , 𝑐 𝑖 , respectively. In this way, we obtain the disentangled intent encoders Φ 𝑠 : 𝑥 𝑖 ↦ → (𝑥 𝑖 ) ∈ R 𝑑 to generate intent features z 𝑠 𝑖 in each disentangled subspace 𝑠. 3.1.4 Independence-constraint Loss. Since the disentangled intent features should reflect different aspects of user preferences, we add the independence constrain on them. Specifically, following the recent advances of disentangled representation learning <ref type="bibr" target="#b33">[34]</ref>, we regard the distance correlation of any two intent features among three subspaces S = {𝐿,𝑇 , 𝐶} as an independence loss to ensure independence, formulated as follows,</p><formula xml:id="formula_8">L IND = 1 𝑀 + 𝑁 𝑀+𝑁 ∑︁ 𝑖=1 ∑︁ 𝑠,𝑠 ′ ∈S,𝑠≠𝑠 ′ dCov(z 𝑠 𝑖 , z 𝑠 ′ 𝑖 ) √︃ dVar(z 𝑠 𝑖 ) • dVar(z 𝑠 ′ 𝑖 ) ,<label>(6)</label></formula><p>where dCov(•) and dVar(•) denote the distance covariance and the distance variance, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5">Encoder Pre-training.</head><p>To enhance the supervision signal in the feature learning, we conduct a pre-training based on the fact that the observed behavior 𝑥 𝑖 = (𝑢 𝑖 , 𝑙 𝑖 , 𝑡 𝑖 , 𝑐 𝑖 ) reflect the similarity of embeddings of 𝑢 𝑖 , 𝑙 𝑖 , 𝑡 𝑖 , and 𝑐 𝑖 . Furthermore, we can use both the labeled and unlabeled data for pre-training, and thus the representation learning can aid the knowledge transfer process from known intents to unknown intents. Specifically, for each consumption behavior 𝑥 𝑖 = (𝑢 𝑖 , 𝑙 𝑖 , 𝑡 𝑖 , 𝑐 𝑖 ), we first calculate the prediction score with its disentangled intent features z 𝑠 𝑖 , 𝑠 ∈ S = {𝐿,𝑇 , 𝐶}, denoted as, ŷ</p><formula xml:id="formula_9">(𝑥 𝑖 ) = ∑︁ 𝑠 ∈S 𝑓 𝑠 (z 𝑠 𝑖 ),<label>(7)</label></formula><p>where 𝑓 𝑠 : R 𝑑 ↦ → R denotes the score function in the disentangled subspace 𝑠. Then, we adopts BPR loss <ref type="bibr" target="#b28">[29]</ref> to ensure that the the observed behaviors can be assigned a higher score than the unobserved ones when pre-train the encoders, formulated as follows,</p><formula xml:id="formula_10">L BPR = 1 |O| ∑︁ (𝑥 𝑖 ,𝑥 * 𝑗 ) ∈ O − ln 𝜎 ( ŷ(𝑥 𝑖 ) − ŷ(𝑥 * 𝑗 ) ),<label>(8)</label></formula><p>where O = {(𝑥 𝑖 , 𝑥 * 𝑗 )|𝑥 𝑖 ∈ D, 𝑥 * 𝑗 ∉ D} denotes the pairwise training set built with negative sampling, 𝜎 (•) is the sigmoid function. Combining the BPR loss and independence loss, the loss function in the pre-training stage can be formulated as follows,</p><formula xml:id="formula_11">L PRE = L BPR + 𝜆L IND ,<label>(9)</label></formula><p>where 𝜆 denotes the hyperparameter to control the influence of independence constraints among the disentangled intent features.</p><p>To sum up, we obtain the disentangled intent feature in each aspect with the disentangled intent encoders to capture the user preferences in distinct aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Intent Discovery Decoders</head><p>To discover the new intents with only a small amount of labeled data, we propose to transfer knowledge from known intents to unknown ones with intent discovery decoders, which consist of three stages, i.e., warm-up stage, main stage, and output stage. In real-world business scenarios, we may not know the number of unknown intents in the unlabeled user consumption data. Hence, we first propose a simple yet effective method to estimate the number of intents. Given that the designed intent encoders in section 3.1 have sufficiently capture users' core preferences in the disentangled aspects, we first generate intent features with the pre-trained intent encoders, denoted as z 𝑖 = 𝑠 ∈S z 𝑠 𝑖 , where z 𝑠 𝑖 is the intent feature of record 𝑥 𝑖 in the subspace 𝑠. Then, following <ref type="bibr" target="#b38">[39]</ref>, we conduct k-means <ref type="bibr" target="#b23">[24]</ref> on the extracted intent features to estimate the intent number. Specifically, we assign 𝐾 ′ (e.g., three times of the known intent classes 𝐾 𝑘 ) as the number of all intents (known and unknown) and cluster all samples into 𝐾 ′ clusters with k-means. Similar to <ref type="bibr" target="#b38">[39]</ref>, we drop the low confidence clusters (the size smaller than the expected cluster mean size 𝑀+𝑁 𝐾 ′ ) and obtain the total intent number 𝐾 as follows,</p><formula xml:id="formula_12">𝐾 = 𝐾 ′ ∑︁ 𝑖=1 𝛿 (|𝑆 𝑖 | ≥ 𝑀 + 𝑁 𝐾 ′ ),<label>(10)</label></formula><p>where 𝛿 (•) is the indicator function (1 if condition satisfied else 0). 𝑀 and 𝑁 denote the number of the labeled and unlabeled data, respectively. In this way, we estimate the total intent number 𝐾 and the number of unknown intents 𝐾 𝑢 = 𝐾 − 𝐾 𝑘 . After estimating the intent number in user consumption behavior data, we will introduce how to discover the new consumption intents. Since we do not reveal intent labels in the pre-training stage (section 3.1.5), we first fine-tune the pre-trained encoders with the labeled (known intents) data.</p><p>b) Feature fine-tuning. With the pre-trained intent encoder Φ 𝑠 in the disentangled subspace 𝑠, following <ref type="bibr" target="#b11">[12]</ref>, we further extent it with a classification head 𝜂 𝑠 𝑘 : R 𝑑 ↦ → R 𝐾 𝑘 (a linear layer with softmax function) to learn a classifier for the 𝐾 𝑘 known intents in the subspace 𝑠. Specifically, we first generate the disentangled intent features 𝑧 𝑠 𝑖 = Φ 𝑠 (𝑥 𝑖 ), 𝑠 ∈ S = {𝐿,𝑇 , 𝐶} for each consumption behavior 𝑥 𝑖 to capture the intent features that reveal distinct aspects of user preferences. Then, we calculate the classification probabilities in each disentangled subspaces as 𝜂 𝑠 𝑘 (𝑧 𝑠 𝑖 ). Given that user consumption intents may be more relevant to one or more aspects among location-, time-and category-aware preferences, we attentively fuse the classification probabilities in distinct disentangled subspaces with typical attention modules <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b41">42]</ref>, and further optimize the model with the cross-entropy (CE) loss as follows,</p><formula xml:id="formula_13">L FT = − 1 𝑀 𝑀 ∑︁ 𝑖=1 𝑦 𝑙 𝑖 log[ ∑︁ 𝑠 ∈S 𝛼 𝑠 • 𝜂 𝑠 𝑘 (z 𝑠 𝑖 )], 𝛼 𝑠 = softmax(q ⊤ z 𝑠 𝑖 ),<label>(11)</label></formula><p>where 𝑀 and 𝑦 𝑙 𝑖 denotes the number and the intent label of the labeled data D 𝑙 , respectively. q ∈ R 𝑑 is the learnable attention vector. Note that we froze the weights of encoders Φ 𝑠 and only update the parameters of classifiers (i.e. 𝜂 𝑠 𝑘 and q) to avoid overfitting when fine-tuning on the labeled data D 𝑙 .</p><p>After the feature fine-tuning, we next introduce how to transfer knowledge from known intents to unknown ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Main Stage: Knowledge Transferring from Labeled Intents.</head><p>Once the intent encoder Φ 𝑠 and the classifier for known intents 𝜂 𝑠 𝑘 in each subspace 𝑠 has been well trained, we next introduce how to discover the new intents by transferring knowledge from known intents to unknown ones. Given that we have estimated the class number of unknown intents 𝐾 𝑢 in section 3.2.1 a), similar to known intents, we also extend the encoder Φ 𝑠 with classification head 𝜂 𝑠 𝑢 : R 𝑑 ↦ → R 𝐾 𝑢 for 𝐾 𝑢 unknown intents. With the classifiers 𝜂 𝑠 𝑘 (for known intents) and 𝜂 𝑠 𝑢 (for unknown intents), we can obtain the final classifier 𝜂 𝑠 = [𝜂 𝑠 𝑘 , 𝜂 𝑠 𝑢 ] : R 𝑑 ↦ → R 𝐾 to classify any unlabeled sample into 𝐾 intents (𝐾 𝑘 known and 𝐾 𝑢 unknown). Hence, the problem turns to how to train the final classifiers 𝜂 𝑠 with both the labeled data D 𝑙 and unlabeled data D 𝑢 .</p><p>To address the above problem, we propose the intent-pair pseudo-label learning for intent discovery, which consists of two steps, a) label construction based on denoised similarity and b) pseudo-label enhanced joint learning.</p><p>a) Label construction based on denoised similarity. The key assumption of intent discovery is that the similar user consumption behaviors on the Meituan platform should belong to the same intent classes. Hence, similar to <ref type="bibr" target="#b11">[12]</ref>, we first define a relation among pairs of unlabeled samples (𝑥 𝑢 𝑖 , 𝑥 𝑢 𝑗 ). Since the well-trained intent encoders can obtain the transferable intent features for both known intents and unknown ones, we can generate the intent features z 𝑠 𝑖,𝑢 , z 𝑠 𝑗,𝑢 for the above pairwise data in subspace 𝑠. Given that consumption intents may be more relevant in one aspect but less in other ones (e.g. have lunch and have dinner are more relevant in category-aspect but less in time-aspect), for the pair (𝑥 𝑢 𝑖 , 𝑥 𝑢 𝑗 ), we generate the pseudo-label 𝑟 𝑠 𝑖 𝑗 in each disentangled subspace 𝑠 based on the similarity of the corresponding intent features z 𝑠 𝑖,𝑢 , z 𝑠 𝑗,𝑢 . Calculating denoised similarity: Since the robust pseudolabels are more suitable for training the classifiers of unknown intents, instead of directly calculating the similarity of z 𝑠 𝑖,𝑢 and z 𝑠 𝑗,𝑢 , we propose a more robust method by denoising the intent features with Low-Pass Fast Fourier Transform (LPFFT). Since Fast Fourier Transform (FFT) can convert signals into the frequency domain, it is widely used for denoising in signal processing area <ref type="bibr" target="#b1">[2]</ref>. In this paper, we first conduct FFT for each dimension of intent features, and then we remove the higher-frequency half of signals via Low-Pass Filter (LPF). Finally, we perform inverse FFT (IFFT) to generate the robust features. The above operations can be denoted as LPFFT : IFFT(LPF(FFT(•))). With the denoised features, we further calculate the cosine similarity 𝑟 𝑠 𝑖 𝑗 for the pairwise samples (𝑥 𝑢 𝑖 , 𝑥 𝑢 𝑗 ) in subspace 𝑠, formulated as, 𝑟 𝑠 𝑖 𝑗 = COSINE(LPFFT(z 𝑠 𝑖,𝑢 ), LPFFT(z 𝑠 𝑗,𝑢 )), 𝑠 ∈ S = {𝐿,𝑇 , 𝐶}, (12) where COSINE(•) denotes the cosine similarity function. In this way, we obtain the pairwise pseudo-labels in each subspace 𝑠.</p><p>b) Pseudo-label enhanced joint learning. Given that we have learned 𝜂 𝑠 𝑘 in section 3.2.1 b), we only randomly initialize the parameters for the new classes in 𝜂 𝑠 and further train 𝜂 𝑠 with the pseudo-label enhanced joint learning. For the labeled data D 𝑙 , the extended classifier 𝜂 𝑠 should also classify the known intents correctly. Hence, we extend the cross-entropy (CE) loss in eq. ( <ref type="formula" target="#formula_13">11</ref>) to 𝜂 𝑠 in three subspaces S = {𝐿,𝑇 , 𝐶} as follows,</p><formula xml:id="formula_14">L CE = − 1 𝑀 𝑀 ∑︁ 𝑖=1 𝑦 𝑙 𝑖 * log[ ∑︁ 𝑠 ∈S 𝛼 𝑠 • 𝜂 𝑠 (z 𝑠 𝑖 )], 𝛼 𝑠 = softmax(q ⊤ z 𝑠 𝑖 ),<label>(13)</label></formula><p>where 𝑦 𝑙 𝑖 * is the extended one-hot intent label (the dimensions of new classes are set to 0) for the labeled data.</p><p>For the unlabeled data, we use the obtained pairwise pseudolabels in each subspace 𝑠 to train the corresponding classifier 𝜂 𝑠 . Specifically, in each disentangled subspace 𝑠, we first calculate the ). Then, we optimize 𝜂 𝑠 with the binary cross-entropy (BCE) loss, denoted as,</p><formula xml:id="formula_15">L BCE = − 1 𝑁 2 𝑁 ∑︁ 𝑖=1 𝑁 ∑︁ 𝑗=1 ∑︁ 𝑠 ∈S [𝑟 𝑠 𝑖 𝑗 logŝ 𝑠 𝑖 𝑗 + (1 − 𝑟 𝑠 𝑖 𝑗 )log(1 − ŝ𝑠 𝑖 𝑗 )], (<label>14</label></formula><formula xml:id="formula_16">)</formula><p>where 𝑁 is the number of unlabeled data D 𝑢 . S = {𝐿,𝑇 , 𝐶} is the set of three disentangled subspaces and 𝑟 𝑠 𝑖 𝑗 denotes the pseudo-label of pairwise samples (𝑥 𝑢 𝑖 , 𝑥 𝑢 𝑗 ) in subspace 𝑠. To ensure the independence among disentangled intent features, we further introduce the independence loss in eq. ( <ref type="formula" target="#formula_8">6</ref>) and define the overall loss function as follows,</p><formula xml:id="formula_17">L = L CE + L BCE + 𝜆L IND ,<label>(15)</label></formula><p>where 𝜆 denotes the hyperparameter to control the influence of independence constraints. Note that we also froze the encoders Φ 𝑠 (𝑠 ∈ S) to avoid over-fitting during the joint-learning process.</p><p>In this way, we successfully transfer the knowledge from known intents to unknown intents with the joint-learning framework. The joint-learning framework also creates a feedback loop that refines the intent features with the well-trained classifier 𝜂 𝑠 , which in turn generates better pairwise pseudo-labels for the training of 𝜂 𝑠 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Output Stage.</head><p>With the intent discovery decoders in three disentangled aspects S = {𝐿,𝑇 , 𝐶}, we can adapt the importance of each aspect to the final intent discovery results. Specifically, we first conduct aspect-level prediction and then leverage attentive fusion to fuse the results from different aspects. a) Aspect-level prediction. With the well-trained encoder Φ 𝑠 and classifier 𝜂 𝑠 (for both known intents and unknown ones) in each disentangled aspect 𝑠, we calculate the prediction score 𝑝 𝑠 𝑖 for any unlabeled data 𝑥 𝑢 𝑖 , denoted as 𝑝 𝑠 𝑖 = 𝜂 𝑠 (Φ 𝑠 (𝑥 𝑢 𝑖 )). b) Attentive Fusion. We assume that the well-trained intent feature in each disentangled aspect should reveal the importance of the corresponding result. In other words, our model should pay more attention to the aspect that generates more important intent feature. Hence, we first calculate the attention score 𝛼 𝑠 with intent features and then combine the predicted results from three aspects to obtain the final predicted intent class, formulated as follows, where q denotes attention vector. In this way, we can assign any unlabeled sample to a certain known or unknown intent class.</p><formula xml:id="formula_18">ŷ𝑢 𝑖 = argmax( ∑︁ 𝑠 ∈S 𝛼 𝑠 • 𝑝 𝑠 𝑖 ), 𝛼 𝑠 = softmax(q ⊤ Φ 𝑠 (𝑥 𝑢 𝑖 )),<label>(16)</label></formula><p>To summarize, with the designed decoders, our AutoIntent can estimate the number of new intents and can further assign the unlabeled data to a certain intent class with the well-trained intent classifier. In other words, AutoIntent can assign the data into distinct intent clusters. Hence, we can obtain the intent-category relation from the intent clusters and further define the semantic information of the new intents by combining the popular categories in the corresponding clusters, which will further contribute to the downstream applications (i.e. recommendation) in Meituan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Industrial Deployment of AutoIntent</head><p>In this section, we will introduce how to deploy our AutoIntent model in the recommendation engine for Meituan APP's homepage. As Figure <ref type="figure">4</ref> shows, the recommendation engine consists of two stages, i.e., recall stage and ranking stage. For a certain user on Meituan platform, according to his/her historical consumption data, the recommendation engine first produces an candidate item set (recall pool) with distinct recall methods in the recall stage. Then, the above candidates (recall pool) are passed through multi-stages of ranking (i.e., pre-ranking, ranking, and re-ranking) to generate the final recommendation list. We deploy AutoIntent in the recommendation engine by introducing an additional intent discovery stage, which is benefitial to both the recall and ranking stage.</p><p>With the user consumption data (only few data with intent label and most data without label), our AutoIntent can assign any unlabeled sample to a certain known or unknown intent class. Moreover, for each discovered new intent, AutoIntent can generate the corresponding intent-category relation according to the item category information in the user consumption data samples that belong to the current new intent. As illustrated in Figure <ref type="figure">4</ref>, the generated intent labels and intent-category relation for new intents can enhance the intent-based recall method in the recall stage. For the ranking stage, the intent features captured by AutoIntent can contribute to the better personalized modeling for user consumption behaviors. At a high level, the system in Figure <ref type="figure">4</ref> is in a positive feedback loop. The intent discovery stage can constantly discover new intents from the user consumption data, which contributes to better recommendation and user growth. In return, better recommendation and user growth can provide more user consumption data to enhance intent discovery. We conduct evaluation of downstream recommendation in section 5 to verify the effectiveness of the deployment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION OF INTENT DISCOVERY 4.1 Experimental Settings</head><p>4.1.1 Datasets. We collect two large-scale user consumption data from Meituan APP in the two cities of China (Beijing and Shanghai), from Jan. 1st to Mar. 1st, 2021 (60 days), which contain 13 locations and 96 time-slots. To evaluate the model performance, we split the first 48 days' data as the training set, the following 6 days' data as validation set, and the last 6 days' data as testing set. Moreover, we select the first 10 intents as known intents and treat the remaining 9 intents as unknown ones. The details of datasets are provided in Table <ref type="table" target="#tab_3">1</ref> and Appendix A.1.</p><p>4.1.2 Metrics. Following <ref type="bibr" target="#b38">[39]</ref>, we adopt three widely used clustering metrics, ACC<ref type="foot" target="#foot_2">3</ref> (Accuracy), ARI (Adjusted Rand Index), and NMI (Normalized Mutual Information), for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Baselines.</head><p>The two key designs of AutoIntent are 1) disentangled intent encoders and 2) intent discovery decoders, we compare AutoIntent with two categories of baselines. 1) Three SOTA feature generating methods <ref type="foot" target="#foot_3">4</ref> (DeepFM <ref type="bibr" target="#b9">[10]</ref>, LightGCN <ref type="bibr" target="#b13">[14]</ref>, and HAN <ref type="bibr" target="#b32">[33]</ref>) to verify the effectiveness of our encoders. 2) Four SOTA deep clustering methods for intent discovery (CDAC+ <ref type="bibr" target="#b22">[23]</ref> and DeepAligned <ref type="bibr" target="#b38">[39]</ref>) and new category discovery (DTC <ref type="bibr" target="#b12">[13]</ref> and RankStat <ref type="bibr" target="#b11">[12]</ref>) <ref type="foot" target="#foot_4">5</ref> . We provide the details of baselines in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Overall performance</head><p>We compare our AutoIntent with SOTA baselines on two datasets. From the results in Table <ref type="table" target="#tab_4">2</ref>, we have the following observations. • Our proposed AutoIntent achieves the best performance.</p><p>Owing to the disentangled intent encoders and conducting intent discovery from the disentangled view, AutoIntent can capture the preferences of distinct aspects (i.e., location-, time-and category-aspect) and achieves the best performance. On average, AutoIntent outperforms the best baseline by 14.14% on ACC, 13.93% on ARI, and 14.30% on NMI, respectively. The significant performance gains verify the effectiveness of our AutoIntent. • Modeling the preferences with the relations from distinct aspects is essential. For the encoder baselines, HAN-KM and LightGCN-KM achieve better performance than DeepFM-KM, which demonstrates modeling the relations in user consumption behavior is necessary. Moreover, HAN-KM (separately modeling the location-, time-and category-aspect performance with distinct meta-paths) achieves the best which verifies the necessity of modeling the user preferences from the disentangled views with dual hypergraphs in our AutoIntent. • For intent discovery, pairwise labeling is easier to optimize than the clustering methods HAN-RankStat, using the pairwise pseudo-labels to train the classifiers for unlabeled data,  achieves a better performance than clustering-based methods (HAN-CDAC+, HAN-DeepAligned, and HAN-DTC), which verifies that learning the classifiers for the unlabeled data is a better choice for intent discovery. Hence, it is necessary to sufficiently model the pairwise similarity from disentangled perspectives for better pseudo-labels in our proposed AutoIntent.  <ref type="table" target="#tab_5">3</ref>, removing any component leads to a significant performance drop, which demonstrates the effectiveness of the above key components. Among them, removing the BCE loss causes a dramatic drop, which verifies that our denoised feature comparison with Low-Pass Fast Fourier Transform (LPFFT) can indeed generate reliable pairwise pseudo labels to train the classifiers for unlabeled data. Moreover, if we replace the designed Disentangled Intent Encoders with HAN (the SOTA baseline encoder), the performance also suffers a significant drop, which demonstrates the necessity of modeling the user preferences in distinct aspects with dual hypergraphs in Disentangled Intent Encoders. We further provide the ablation study of Denoised Similarity Methods and Training Scheme in Appendix A.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Studies of AutoIntent</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Hyper-parameter Study.</head><p>In this part, we study the impact of two essential hyper-parameters in our proposed AutoIntent, i.e., the model depth 𝐿 and the independent coefficient 𝜆. 1) Impact of Model Depth 𝐿. To study the impact of depth of dual hypergraphs in encoders, we vary 𝐿 in {1, 2, 3, 4}. From the results in Figure <ref type="figure" target="#fig_4">5</ref> (a), the model achieves the best performance with one layer on both datasets. The possible reason is that the hypergraph can capture the high-order relations without stacking multiple layers, which means our hypergraph-based encoders are more efficient. Hence, we set 𝐿 as 1 for both datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Guess You Like</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ski Resort</head><p>The quality of the snow in this ski resort is so great.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Guess You Like</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ski Resort</head><p>The quality of the snow in this ski resort is so great.  2) Impact of Independent Coefficient 𝜆. To evaluate the impact of 𝜆, we vary it in {1𝑒 −6 , 1𝑒 −5 , 1𝑒 −4 , 1𝑒 −3 , 1𝑒 −2 , 1𝑒 −1 }. According to the results in Figure <ref type="figure" target="#fig_4">5</ref> (b), our AutoIntent is not sensitive to 𝜆 (the performance remains relatively stable in a certain interval 1𝑒 −5 ∼ 1𝑒 −3 ) and achieves the best performance when 𝜆 = 1𝑒 −4 . Hence, we set 𝜆 as 1𝑒 −4 for both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION OF DOWNSTREAM RECOMMENDATION</head><p>In this section, we further evaluate whether our proposed AutoIntent can enhance the downstream recommendation in Meituan.</p><p>Referring to the proposed deployment scheme in section 3.3, we conduct the downstream evaluations on the recommendation engine for the Meituan APP homepage (including recall and ranking stage) to evaluate the effectiveness of the deployment. Specifically, we obtain the recall pool by merging the results from all the three strategies (popularity-based, model-based, and intent-based). In the ranking stage, we further generate recommendation list from the recall pool. To evaluate the effectiveness intent discovery module in the deployment system, we compare the recommendation performance with (strategy A) and without (strategy B) AutoIntent. In strategy A (with AutoIntent), as shown in Figure <ref type="figure">4</ref>, the discovered new intents and intent features are used in recall and ranking stage, respectively. The evaluation of recommendation are conducted in the Meituan APP homepage, as shown in Figure <ref type="figure" target="#fig_8">6</ref>, involving about 8 million users. We compare the recommendation performance among the users in Beijing and Shanghai with two ranking-based metrics, recall@10 (R@10) and NDCG@10 (N@10). In Meituan APP, there are many different Business Units (BUs), such as Takeaway and Pets in Figure <ref type="figure" target="#fig_8">6</ref>. Given that the user intents in different BUs may be different, we conduct the experiments with two settings,    <ref type="table" target="#tab_8">4</ref>, the recommendation model with AutoIntent achieves the performance gains of 14.70% on Recall and 17.55% on NDCG, which is a more significant improvement than in the known BUs. The possible reason is that AutoIntent can capture the common features in distinct intents and transfer the knowledge from the intents (in known BUs) to the new intents (in new BUs).</p><p>In short, the results demonstrate that a) AutoIntent can enhance the recommendation performance with the discovered intents and b) our proposed AutoIntent can transfer knowledge among BUs to achieve more significant improvement in New BUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Intent Discovery Intent discovery aims to discover new intents by transferring the knowledge from the known intents to the new ones and has been explored in dialogue systems <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. CDAC+ <ref type="bibr" target="#b22">[23]</ref> propose to discover new intents via deep adaptive clustering with cluster refinement. DeepAligned <ref type="bibr" target="#b38">[39]</ref> enhance the deep clustering with an alignment strategy to tackle the label inconsistency problem. Another research problem that is highly related to intent discovery is the new visual categories discovery <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b39">40]</ref>. DTC <ref type="bibr" target="#b12">[13]</ref> extends the Deep Embedded Clustering to a transfer learning setting by introducing the temporal ensemble and consistency. RankStat <ref type="bibr" target="#b11">[12]</ref> propose to generate the pairwise pseudo-label with rank statistics to transform the clustering task to a binary classification task. Zhao et al <ref type="bibr" target="#b39">[40]</ref> further extent RankStat in a twobranch learning framework. Different from the above works, we aim to discover intents from user consumption data, which is more challenging and needs to sufficiently capture the user preferences.</p><p>Hypergraph Learning Hypergraph <ref type="bibr" target="#b0">[1]</ref> introduces hyperedge, a special edge to connect more than two nodes, to naturally capture high-order relations, which has widely used in recommendation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38]</ref>. The learning on hypergraph can be regarded as a two-stage process and has been well-explored. HGNN <ref type="bibr" target="#b7">[8]</ref> introduces graph convolution to hypergraph to learn the graph embedding. HyperGAT <ref type="bibr" target="#b6">[7]</ref> attentively aggregates the node information and extends GAT to hypergraph. HGC-RNN <ref type="bibr" target="#b36">[37]</ref> combines the hypergraph learning and RNN to learn temporal dependency among different hypergraphs. In this work, we use dual hypergraphs to model the user preferences.</p><p>Disentangled Representation Learning Disentangled representations can independently model a certain object from multiple aspects or factors <ref type="bibr" target="#b2">[3]</ref>. The earlier works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15]</ref> learn the disentangled features via the regularized variational auto-encoders <ref type="bibr" target="#b18">[19]</ref>. With the development of GNNs, there exist some works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41]</ref> that explore how to learn the disentangled embeddings on the graphs. In this work, we learn the disentangled intent features from distinct aspects and further conduct intent discovery in a disentangled manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this work, we approach the new problem of user consumption intents discovery that is highly related to the recommendation in Meituan and develop a system named AutoIntent to automatically discover intents from the consumption data. AutoIntent first leverages dual hypergraph neural networks to learn the disentangled intent features with the disentangled intent encoders. Then, the intent discovery decoders transfer the knowledge from the known intents to discover new ones. Finally, we deploy AutoIntent in the Meituan recommendation engine for downstream evaluation. Experiments verify that AutoIntent can effectively discover unknown intents and enhance recommendation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The Construction of Dual Hypergraphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The General architecture of the proposed AutoIntent model (a) and the details of intent decoder (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3. 2 . 1</head><label>21</label><figDesc>Warm-up Stage. The warm-up stage includes a) intents number estimation and b) feature fine-tuning on the labeled data. a) Intents Number Estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Impact of model depth 𝐿 (a) and independent coefficient 𝜆 (b) on two datasets from Meituan.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4. 3 . 1</head><label>31</label><figDesc>Ablation Study. AutoIntent has the following key designs: 1) Disentangled Intent Encoders (DisenEncoder), 2) Denoised Similarity (DS), and 3) Loss Functions in Intent Discovery Decoders (i.e., L CE , L BCE , and L IND ). To evaluate the effectiveness of the above components, we compare the performance of the model variants that without (w/o) a certain component. From the results in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>for couple￥129 ￥381 66% off ￥129 ￥381 66% offThe ingredients are very fresh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Illustration of Meituan App's homepage.</figDesc><graphic url="image-1.png" coords="8,98.11,4.74,148.36,329.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>i.e. intent discovery in known BUs and intent discovery in new BUs. a) Intent Discovery in Known BUs. We first compare the recommendation performance with (w) and without (w/o) AutoIntent in the known BUs. We use the data from all BUs in Beijing and Shanghai datasets to train our AutoIntent model. AutoIntent discovers 11 new intents with 19 known intents on both offline datasets. Then, we deploy the AutoIntent model on all BUs to evaluate the recommendation performance. From the results in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>𝑐 𝑖 on location 𝑙 𝑖 at time-slot 𝑡 𝑖 , where 𝑢 𝑖 , 𝑙 𝑖 , 𝑡 𝑖 , 𝑐 𝑖 denote the user ID, location ID, time-slot ID and item category ID, respectively. Let U, L, T , C denote the sets of users, locations, time-slots and categories, of which the sizes are denoted as 𝑁 𝑈 , 𝑁 𝐿 , 𝑁 𝑇 , and 𝑁 𝐶 , respectively. As mentioned in the introduction, the user makes a consumption behavior due to a specific intent. For each consumption behavior 𝑥 𝑖 , we use 𝑦 𝑖 to denote the associated intent. In real world, we can only manually define a limited set of intents, with a small amount of labeled consumption behaviors D 𝑙 = {(𝑥 𝑙 𝑖 , 𝑦 𝑙 𝑖 )| 𝑀 𝑖=1 }, where 𝑦 𝑙 𝑖 ∈ I 𝑘 (I 𝑘 denotes the known intents with 𝐾 𝑘 classes). For the unlabeled data D 𝑢 = {𝑥 𝑢 𝑖 | 𝑁 𝑖=1 }, the intents of D 𝑢 may belong to the unknown intents I 𝑢 or the known ones I 𝑘 .</figDesc><table><row><cell cols="3">User Consumption Data</cell><cell cols="2">Consumption Intent Discovery</cell><cell>Downstream Applications</cell></row><row><cell cols="2">User Location Time Category</cell><cell>Intents</cell><cell cols="2">Intent Discovery</cell></row><row><cell></cell><cell cols="2">Gathering</cell><cell cols="2">System</cell><cell>System Recommender</cell></row><row><cell>...</cell><cell cols="2">Cooking at Home ？ ...</cell><cell>AutoIntent Home Cooking at</cell><cell>Intent Known</cell><cell>User Understanding Targeted ...</cell></row><row><cell></cell><cell></cell><cell>？</cell><cell>Exercise Doing</cell><cell>Intent New</cell><cell>Marketing</cell></row><row><cell cols="6">Figure 1: Illustration of the intent discovery and its applica-</cell></row><row><cell>tions in Meituan.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>on how to represent the bipartite relations with dual homogeneous hypergraphs. Take user-category bipartite relations in Figure 2 as an example, the user 𝑢 1 purchases items with categories 𝑐 1 , 𝑐 3 , 𝑐 4 , which corresponds to a hyperedge {𝑐 1 , 𝑐 3 , 𝑐 4 } ∈ E 𝐶 in G 𝐶 . From the perspective of items, the item with category𝑐 2 is bought by 𝑢 2 , 𝑢 3 , 𝑢 4 , which forms a hyperedge {𝑢 2 , 𝑢 3 , 𝑢 4 } ∈ E 𝐶 𝑈 in</head><label></label><figDesc>G 𝑈 . In this way, we construct the dual hypergraphs {G 𝐶 𝑈 , G 𝐶 } to capture the UC bipartite relation. Similarly, we construct other two groups of dual hypergraphs {G 𝐿 𝑈 , G 𝐿 } and {G 𝑇 𝑈 , G 𝑇 } to model the bipartite relations of UL and UT, respectively. 𝐿 , H 𝑇 , H 𝐶 ) for other homogeneous hypergraphs (G 𝐿 𝑈 , G 𝑇 𝑈 , G 𝐿 , G 𝑇 , G 𝐶 ) in a similar way.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Obviously, we can easily generate the incidence matrix (H 𝐿 𝑈 , H 𝑇 𝑈 , H 3.1.2 Embedding Propagation on Dual Hypergraphs. With the</cell></row><row><cell></cell><cell></cell><cell></cell><cell>constructed dual hypergraphs, to learn representations that capture</cell></row><row><cell></cell><cell></cell><cell></cell><cell>the high-order relations, we propose hypergraph convolutional</cell></row><row><cell></cell><cell></cell><cell></cell><cell>layers based on the embedding propagation. Specifically, we first</cell></row><row><cell></cell><cell></cell><cell></cell><cell>introduce the embedding layer and then conduct the proposed joint</cell></row><row><cell></cell><cell></cell><cell></cell><cell>hypergraph convolution (Joint-HGC) for aggregation.</cell></row><row><cell cols="4">3.1.1 Dual Hypergraphs Construction. Let {G 𝐿 𝑈 = (U, E 𝐿 𝑈 ), G 𝐿 =</cell></row><row><cell cols="4">(L, E 𝐿 )}, {G 𝑇 𝑈 = (U, E 𝑇 𝑈 ), G 𝑇 = (T , E 𝑇 )}, {G 𝐶 𝑈 = (U, E 𝐶 𝑈 ), G 𝐶 =</cell></row><row><cell cols="4">(C, E 𝐶 )} denote the dual hypergraphs groups to capture the bipar-</cell></row><row><cell cols="4">tite relations of user-location (UL), user-time (UT) and user-category</cell></row><row><cell cols="4">(UC), respectively. Note that the hypergraphs G 𝐿 𝑈 , G 𝑇 𝑈 , G 𝐶 𝑈 share</cell></row><row><cell cols="4">the same node set U but have distinct hyperedges (E 𝐿 𝑈 , E 𝑇 𝑈 , E 𝐶 𝑈 ).</cell></row><row><cell cols="4">Then we elaborate Hypergraph extends the concept of adjacency matrix in normal</cell></row><row><cell cols="4">graph to incidence matrix, H, to represent the connections among</cell></row><row><cell cols="4">more than two nodes. For the constructed homogeneous hyper-</cell></row><row><cell cols="4">graph G 𝐶 𝑈 , each entry of the incidence matrix H 𝐶 𝑈 ∈ R | U |× | E 𝐶 𝑈 | can</cell></row><row><cell>be defined as follows,</cell><cell></cell><cell></cell></row><row><cell>H 𝐶 𝑈 (𝑢, 𝑒) =</cell><cell cols="2">1 if 𝑢 is connected by 𝑒, 𝑒 ∈ E 𝐶 𝑈 , 0 otherwise.</cell><cell>(1)</cell></row><row><cell cols="4">Further, we use diagonal matrices D 𝐶 𝑈 ∈ R | U |× | U | and B 𝐶 𝑈 ∈</cell></row><row><cell cols="4">R | E 𝐶 𝑈 |×| E 𝐶 𝑈 | to represent the node degrees and hyperedge degrees,</cell></row><row><cell cols="2">where D 𝐶 𝑈 (𝑢, 𝑢) = 𝑒 ∈ E 𝐶 𝑈</cell><cell cols="2">H 𝐶 𝑈 (𝑢, 𝑒) and B 𝐶 𝑈 (𝑒, 𝑒) = 𝑢 ∈U H 𝐶 𝑈 (𝑢, 𝑒).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Statistics of Two Datasets from Meituan.</figDesc><table><row><cell cols="6">Dataset #Users #Locations #Time #Category #Intents #Records</cell></row><row><cell>Beijing 38,702</cell><cell>13</cell><cell>96</cell><cell>748</cell><cell>19</cell><cell>7,075,926</cell></row><row><cell>Shanghai 44,186</cell><cell>13</cell><cell>96</cell><cell>792</cell><cell>19</cell><cell>8,634,379</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Performance comparisons on two datasets.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Beijing</cell><cell></cell><cell></cell><cell>Shanghai</cell><cell></cell></row><row><cell></cell><cell>ACC</cell><cell>ARI</cell><cell>NMI</cell><cell>ACC</cell><cell>ARI</cell><cell>NMI</cell></row><row><cell>DeepFM-KM</cell><cell>54.41</cell><cell>35.83</cell><cell>29.17</cell><cell>51.98</cell><cell>33.69</cell><cell>37.63</cell></row><row><cell>LightGCN-KM</cell><cell>58.84</cell><cell>36.16</cell><cell>31.48</cell><cell>55.46</cell><cell>35.87</cell><cell>40.82</cell></row><row><cell>HAN-KM</cell><cell>60.32</cell><cell>38.59</cell><cell>33.74</cell><cell>58.76</cell><cell>36.65</cell><cell>42.29</cell></row><row><cell>HAN-CDAC+</cell><cell>67.34</cell><cell>42.53</cell><cell>36.82</cell><cell>66.18</cell><cell>40.47</cell><cell>45.86</cell></row><row><cell>HAN-DeepAligned</cell><cell>69.89</cell><cell>46.48</cell><cell>36.75</cell><cell>67.56</cell><cell>43.28</cell><cell>46.50</cell></row><row><cell>HAN-DTC</cell><cell>68.35</cell><cell>47.72</cell><cell>38.36</cell><cell>67.13</cell><cell>43.34</cell><cell>46.91</cell></row><row><cell>HAN-RankStat</cell><cell>70.24</cell><cell>49.45</cell><cell>40.29</cell><cell>68.58</cell><cell>44.92</cell><cell>47.46</cell></row><row><cell>AutoIntent</cell><cell cols="3">81.07 57.34 46.81</cell><cell cols="3">77.39 50.27 53.35</cell></row><row><cell>Improv.</cell><cell cols="3">15.42% 15.96% 16.18%</cell><cell cols="3">12.85% 11.91% 12.41%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Ablation Study of the key designs in AutoIntent.</figDesc><table><row><cell>Dataset</cell><cell>Beijing</cell><cell>Shanghai</cell></row><row><cell>Model Variants</cell><cell cols="2">ACC ARI NMI ACC ARI NMI</cell></row><row><cell cols="3">w/o DisenEncoder (HAN) 74.23 53.42 43.45 70.67 47.83 49.69</cell></row><row><cell>w/o DS</cell><cell cols="2">78.88 50.09 42.08 76.28 45.62 50.67</cell></row><row><cell>w/o L CE</cell><cell cols="2">76.28 51.47 42.96 74.19 46.08 50.83</cell></row><row><cell>w/o L BCE</cell><cell cols="2">36.26 26.45 20.37 32.41 24.19 27.32</cell></row><row><cell>w/o L IND</cell><cell cols="2">79.36 55.88 45.95 76.79 49.47 52.34</cell></row><row><cell>AutoIntent</cell><cell cols="2">81.07 57.34 46.81 77.39 50.27 53.35</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Special Local Shop Mutton Barbecue</head><label></label><figDesc></figDesc><table><row><cell>Beijing</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cloudy 4℃</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Takeaway</cell><cell>Food</cell><cell cols="2">Entertainment</cell><cell>Ticket</cell><cell>Market</cell><cell>Movie</cell></row><row><cell>Medicine</cell><cell cols="2">Life Services</cell><cell>Run Errends</cell><cell>Medicine</cell><cell>Pets</cell><cell>Bar</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Today's Food service areas is</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>kept clean and</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>hygienic.</cell><cell></cell><cell></cell></row><row><cell cols="2">start from ￥80</cell><cell></cell><cell cols="2">￥8.8 ￥20 56% off</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc></figDesc><table /><note>, we can observe that the recommendation performance with AutoIntent</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Evaluations of Downstream Recommendation (in %). 54% on Recall and 11.23% on NDCG, respectively. Such a significant gain verifies that AutoIntent can enhance the recommendation with the discovered new intents. b) Intent Discovery in New BUs. Another important task in Meituan is the intent discovery in new BUs, which is a more challenging task with no known intents in new BUs. Here, we regard Pets as new BU and remove the known intents in Pets BU. Hence, the number of known intents reduce to 15 after removing the intent labels in Pets BU. In the offline training of AutoIntent, we only use the 15 known intents for training and discover 7 new intents in Pets BU. Then, we deploy the well-trained AutoIntent model in the Pets BU and evaluate the recommendation performance. As the results in Table</figDesc><table><row><cell></cell><cell></cell><cell>Beijing</cell><cell>Shanghai</cell></row><row><cell></cell><cell>Model</cell><cell>R@10 N@10 #Intent R@10 N@10 #Intent</cell></row><row><cell>Known BUs</cell><cell cols="2">w/o AutoIntent 14.25 11.43 w AutoIntent 15.57 12.68 19+(11) 14.21 11.24 19+(11) 19 12.94 10.08 19 Imp. 9.26% 10.94% -9.81% 11.51% -</cell></row><row><cell>New BUs</cell><cell cols="2">w/o AutoIntent 13.27 10.57 w AutoIntent 15.08 12.32 15+(7) 13.45 10.74 15+(7) 15 11.62 9.06 15 Imp. 13.64% 16.56% -15.75% 18.54% -</cell></row><row><cell cols="2">improves by 9.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://about.meituan.com/en</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Note that we capture the intrinsic preference with the user-category (UC) relation in the proposed AutoIntent system.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">When calculating ACC, we first match the predicted intent label and the ground-truth label with the Hungarian algorithm<ref type="bibr" target="#b19">[20]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">We use the K-means (KM)<ref type="bibr" target="#b23">[24]</ref> to cluster new intents for all encoder baselines.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">To ensure better performance, we adopt the best encoder in the SOTA feature generating methods (HAN<ref type="bibr" target="#b32">[33]</ref>) as the feature encoder of the above deep clustering methods.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This work is supported in part by National Key Research and Development Program of China under 2020YFA0711403. This work is supported in part by National Natural Science Foundation of China under 61971267, 61972223, and U1936217. This work is also supported by Meituan.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX FOR REPRODUCIBILITY A.1 Datasets and Evaluation Setting</head><p>We collect two large-scale user consumption data from Meituan APP in the two cities of China (Beijing and Shanghai), from Jan. 1st to Mar. 1st, 2021 (60 days). According to the business logic of Meituan, there are 13 locations and 96 time-slots (dividing a day into 48 time-slots for weekends and weekdays) in the collected datasets. We only have 19 known intents among a small amount of data (∼ 17.4%), and most of the user consumption behaviors lack intent labels. To evaluate the model performance in the offline intent discovery task, we split the first 48 days' data as the training set, the following 6 days' data as validation set, and the last 6 days' data as testing set. Specifically, we select the first 10 intents as known intents and treat the remaining 9 intents as unknown ones (10 known + 9 unknown intents). We regard the data with known intents and part of the unknown intents data as training set 6 . For the evaluation, we test all the methods on the remaining unknown intents data to evaluate the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Baseline Models</head><p>In this section, we give the detailed descriptions of all compared methods as follows.</p><p>(1) Feature generating methods:</p><p>• DeepFM <ref type="bibr" target="#b9">[10]</ref> combines the FM and deep neural networks to capture the feature interactions. We concatenate the user ID, location ID, time-slot ID, and category ID as input.</p><p>• LightGCN <ref type="bibr" target="#b13">[14]</ref> is the state-of-the-art GCN model for recommendation. We adopt it to our task by construct a graph with 6 types of edges among the four types of nodes (i.e., user, location, time and category). We add the learned node embeddings for each record as the intent feature. • HAN <ref type="bibr" target="#b32">[33]</ref> is a general state-of-the-art heterogeneous graph learning method. We adapt it to the graph with four types of nodes (user, location, time, and category) by designing three types of meta-path (user-location, user-time, user-category) to represent user's preferences on distinct aspects. We combine those feature generating baselines with K-means <ref type="bibr" target="#b23">[24]</ref> to obtain DeepFM-KM, LightGCN-KM, and HAN-KM for the consumption intent discovery task in Meituan.</p><p>(2) Deep clustering methods:</p><p>• CDAC+ <ref type="bibr" target="#b22">[23]</ref> refines the cluster results by forcing the model to learn from the high confidence assignments for intent discovery. • DeepAligned <ref type="bibr" target="#b38">[39]</ref> is the SOTA method for intent discovery in dialogue systems, which proposes an alignment strategy to tackle the label inconsistency problem during clustering assignments. • DTC <ref type="bibr" target="#b12">[13]</ref> extends the Deep Embedded Clustering to a transfer learning setting by introducing the temporal ensemble and consistency for the new category discovery. • RankStat <ref type="bibr" target="#b11">[12]</ref> is the SOTA method for the new category discovery, which uses pairwise labeling and rank statistics to transfer knowledge of the labeled classes to the unlabeled data. 6 We conduct sampling strategy when obtaining training set to ensure the proportion of known intents data (∼ 17.4%) is consistent with the distribution of the original data.</p><p>We replace the feature encoder of the deep clustering methods with HAN <ref type="bibr" target="#b32">[33]</ref> (the encoder with best performance) to obtain HAN-CDAC+, HAN-DeepAligned, HAN-DTC, and HAN-RankStat for the consumption intent discovery task in Meituan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Metrics</head><p>A.3.1 Metrics for the intent discovery task. Following <ref type="bibr" target="#b38">[39]</ref>, we adopt three widely used clustering metrics, ACC (Accuracy), ARI (Adjusted Rand Index), and NMI (Normalized Mutual Information), to evaluate the performance of the intent discovery task. Note that we first match the predicted intent label and the ground-truth label with the Hungarian algorithm <ref type="bibr" target="#b19">[20]</ref> when calculating ACC.</p><p>A.3.2 Metrics for the recommendation task. Following <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17]</ref>, we use two widely used ranking-based metrics, Recall@K and NDCG@K (we set K as 10 by following <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22]</ref>), to evaluate the performance of the recommendation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Implementation Details</head><p>A.4.1 Efficient Implementation for Large-scale Industrial Datasets. Given that the user number is huge in large scale datasets from Meituan, for all of the graph-based encoders (i.e., LightGCN, HAN, and the dual-hypergraphs in our AutoIntent), the propagation on the graph is very time-consuming. Inspired by GraphSAGE <ref type="bibr" target="#b10">[11]</ref>, instead of training node embedding with its all neighbors, we propose to sample local neighborhoods for each node and update its embedding by aggregating features from its local neighborhoods. Specifically, we conduct the sample strategy in a mini-batch manner. Given B nodes in a mini-batch and the sample number is N, the cost of message passing is O (𝐵𝑁 ), which is more efficient and only related to the mini-batch size and sample number. Hence, with the above sample strategy, the graph-based methods can be used in large-scale industrial datasets. Following GraphSAGE <ref type="bibr" target="#b10">[11]</ref>, we set the sample number 𝑁 as 25 in our AutoIntent model for the two datasets from Meituan.</p><p>A.4.2 Hyper-parameter Settings. For all the models, the embedding size and hidden state size are set as 64, the batch size is set to 2048. We optimize all the methods with Adam <ref type="bibr" target="#b17">[18]</ref> optimizer, which initializes learning rate as 0.001 and will decay it by 0.1 after every three epochs. We also utilize early stopping to detect over-fitting, and the training process will be stopped if ACC on the validation set does not increase for five epochs. For the baseline methods, we initialize the hyper-parameters as the original papers and carefully tune them to get optimal performance. For AutoIntent, we set the initial number of all intents 𝐾 ′ as 3𝐾 𝑘 when estimating intent number. We further study the impact of other essential hyper-parameters (the layer number of dual hypergraphs in intent encoders 𝐿 and the independent coefficient 𝜆) in section 4.3.2. For all methods, we run ten times with the same partition and report the average results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Additional Ablation Study</head><p>In this section, we further provide the additional ablation study of Denoised Similarity Methods and Training Scheme.   A.5.1 Ablation Study of Denoised Similarity Methods. We combine the Low-Pass Fast Fourier Transform (LPFFT) and Cosine similarity to calculate the pairwise pseudo labels in eq. <ref type="bibr" target="#b11">(12)</ref>. In this section, we further compare the LPFFT with other alternative denoised similarity methods, i.e., ranking statistics proposed in <ref type="bibr" target="#b11">[12]</ref> (RS), cosine similarity without denoising (Cosine), High-Pass Fast Fourier Transform (HPFFT), Band-Pass Fast Fourier Transform (BPFFT). From the results in Table <ref type="table">5</ref>, the denoised methods (RS, LPFFT) achieve the better performance. Among the methods with filter algorithms, HPFFT achieves the worst performance while LPFFT achieves the best, which verifies that the higher-frequency signals are more likely to be noisy and filtering out them contributes to better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5.2 Ablation Study of Training Scheme.</head><p>As for the training scheme of AutoIntent, we first pre-train the disentangled intent encoders with both the labeled and unlabeled data (don't use intent label). Then, we fine-tune the classifier for known intents on the labeled data. Finally, we transfer the knowledge form the known intents to unknown ones with the peseudo-label enhanced joint learning. To verify the effectiveness of the each step in training scheme, we compare the performance of model variants that without pre-training (w/o PRE), without fine-tuning (w/o FT), without peseudo-label enhanced joint learning (w/o JL) and AutoIntent. From the results in Table <ref type="table">6</ref>, we can observe that removing any training step will cause significant performance drop. Among them, removing the pre-training of disentangled intent encoders causes a dramatic drop , which verifies that the pre-training without intent label indeed captures the user preference without bias and can generate the transferable intent features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Case Study</head><p>We assume that user consumption behaviors are related to multiple aspects (i.e., location, time, and category). Hence, we design disentangled intent encoders to obtain the intent feature in each aspect and further conduct intent discovery in a disentangled manner. To verify the effectiveness of the disentangled setting in the user consumption intent discovery task, we select two kinds of discovered new intents (i.e., snack and doing exercise) for case study. Specifically, we conduct the attention distribution analysis with a box plot for the attention weights calculated from the disentangled intent features in Eq. <ref type="bibr" target="#b15">(16)</ref>. From the results in Figure <ref type="figure">7</ref>, we can observe that the attention distributes in different intents are quite different. For the intent snack, the attention values in the category aspect are larger than other aspects, which means that the consumption intent snack is more related to users' intrinsic preference, such as taste and brand. For the intent doing exercise, the attention values in location and category aspects are larger than the time aspect, which means the user who wants to do exercise can be more likely to be influenced by the location factor (where he/she is) and intrinsic preference (which sport he/she likes). In short, the results of the case study further verify the effectiveness of the disentangled setting in our AutoIntent model.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Higher order learning with graphs</title>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristin</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A model for the shape of the Fourier amplitude spectrum of acceleration at high frequencies</title>
		<author>
			<persName><forename type="first">G</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">E</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><surname>Hough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the Seismological Society of America</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<date type="published" when="1984">1984. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Irina</forename><surname>Christopher P Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arka</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><surname>Lerchner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03599</idno>
		<title level="m">Understanding disentangling in beta-VAE</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Ricky Tq Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><surname>Duvenaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04942</idno>
		<title level="m">Isolating sources of disentanglement in variational autoencoders</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenghan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Feature-Level Attentive ICF for Recommendation</title>
				<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Kaize</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.00387</idno>
		<title level="m">Be More with Less: Hypergraph Attention Networks for Inductive Text Classification</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hypergraph neural networks</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingrong</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghua</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhan</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Depeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.12843</idno>
		<title level="m">Graph Neural Networks for Recommender Systems: Challenges, Methods, and Directions</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">DeepFM: a factorization-machine based neural network for CTR prediction</title>
		<author>
			<persName><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04247</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Automatically discovering and learning new visual categories with ranking statistics</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvestre-Alvise</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastien</forename><surname>Ehrhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05714</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to discover novel visual categories via deep transfer clustering</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lightgcn: Simplifying and powering graph convolution network for recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multi-class classification without multi-class labels</title>
		<author>
			<persName><forename type="first">Yen-Chang</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Schlosser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Odom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00544</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural input search for large scale recommendation models</title>
		<author>
			<persName><forename type="first">Cong</forename><surname>Manas R Joglekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The Hungarian method for the assignment problem</title>
		<author>
			<persName><surname>Harold W Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval research logistics quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955">1955. 1955</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Enhancing Hypergraph Neural Networks with Intent Disentanglement for Session-based Recommendation</title>
		<author>
			<persName><forename type="first">Yinfeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengliang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Depeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DisenHCN: Disentangled Hypergraph Convolutional Networks for Spatiotemporal Activity Prediction</title>
		<author>
			<persName><forename type="first">Yinfeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Depeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE 38th International Conference on Data Engineering (ICDE)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discovering new intents via constrained deep adaptive clustering with cluster refinement</title>
		<author>
			<persName><forename type="first">Hua</forename><surname>Ting-En Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanlei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName><forename type="first">James</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth Berkeley symposium on mathematical statistics and probability</title>
				<meeting>the fifth Berkeley symposium on mathematical statistics and probability<address><addrLine>Oakland, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1967">1967</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Addressing complex and subjective productrelated queries with customer reviews</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="625" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Dialogue state induction using neural latent variable models</title>
		<author>
			<persName><forename type="first">Libo</forename><surname>Qingkai Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05666</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Hugh</forename><surname>Perkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.11487</idno>
		<title level="m">Dialog intent induction with deep multi-view clustering</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">User Consumption Intention Prediction in Meituan</title>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taichi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengliang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Depeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">BPR: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1205.2618</idno>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Auto-dialabel: Labeling dialogue data with unsupervised learning</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 conference on empirical methods in natural language processing</title>
				<meeting>the 2018 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="684" to="689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Automatic discovery of novel intents &amp; domains from text utterances</title>
		<author>
			<persName><forename type="first">Nikhita</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><surname>Alok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mukund</forename><surname>Sridhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.01208</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Next-item recommendation with sequential hypergraphs</title>
		<author>
			<persName><forename type="first">Jianling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaize</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjie</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Caverlee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Heterogeneous graph attention network</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Disentangled graph collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongye</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">DisenHAN: Disentangled Heterogeneous Graph Attention Network for Recommendation</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntong</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multiplex Bipartite Network Embedding using Dual Hypergraph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Hansheng</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaibhav</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hypergraph Convolutional Recurrent Neural Network</title>
		<author>
			<persName><forename type="first">Jaehyuk</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinkyoo</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3366" to="3376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graph Convolutional Network for Recommendation with Low-pass Collaborative Filters</title>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Discovering new intents with deep aligned clustering</title>
		<author>
			<persName><forename type="first">Hanlei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Ting-En Lin</surname></persName>
		</author>
		<author>
			<persName><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Novel visual category discovery with dual ranking statistics and mutual knowledge distillation</title>
		<author>
			<persName><forename type="first">Bingchen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Disentangling User Interest and Conformity for Recommendation with Causal Embedding</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Depeng</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Atrank: An attention-based user behavior modeling framework for recommendation</title>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinze</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junshuai</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengchao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiusi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
